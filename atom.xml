<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-01-05T10:06:41Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6351</id>
    <link href="https://windowsontheory.org/2018/12/13/ising-perceptron-under-gaussian-disorder-and-k-nae-sat/" rel="alternate" type="text/html"/>
    <title>Ising Perceptron under Gaussian Disorder, and k-NAE-SAT</title>
    <summary>Blog Post By: Patrick Guo, Vinh-Kha Le, Shyam Narayanan, and David Stoner Methods in statistical physics are known to be extremely useful for understanding certain problems in theoretical computer science. Physical observations can motivate the underlying theoretical models, which in turn explain some of the physical phenomena. This post is based on Professor Nike Sun’s […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Blog Post By: Patrick Guo, Vinh-Kha Le, Shyam Narayanan, and David Stoner</strong></p>
<p>Methods in statistical physics are known to be extremely useful for understanding certain problems in theoretical computer science. Physical observations can motivate the underlying theoretical models, which in turn explain some of the physical phenomena.</p>
<p>This post is based on Professor Nike Sun’s guest lecture on the Ising Perceptron model and regular NAE-SAT for CS 229R: Physics and Computation. These are both examples of random constraint satisfaction problems, where we have <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> variables <img alt="x_1, ..., x_n \in \{-1, 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C+...%2C+x_n+%5Cin+%5C%7B-1%2C+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1, ..., x_n \in \{-1, 1\}"/> and certain relations, or constraints, between the <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, and we wish to approximate the number of solutions or visualize the geometry of the solutions. For both problems, the problem instance can be random: for example, the linear constraints in the Ising perceptron model are random, and the clauses in the NAE-SAT instance are chosen at random. As in the previous blog posts, to understand the geometry of solutions, statistical physicists think of sampling random solutions from <img alt="\{-1, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{-1, 1\}^n"/>, which introduces a second type of randomness.</p>
<p>This post is meant to be expository. For interested readers, we point to useful references at the end for more rigorous treatments of these topics.</p>
<p><strong>1. Perceptron Model</strong></p>
<p>The Ising Perceptron under Gaussian disorder asks how many points in <img alt="\{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}^n"/> survive <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> half-plane bisections (i.e. are satisfying assignments for <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> constraints), where the planes’ coefficients are drawn from standard Gaussians. Like many problems in statistical physics, there is likely a critical capacity of constraints where having more constraints yields no survivors with high probability, and having fewer constraints yields survivors with high probability. This lecture gives an overview of a proof for one side of this physics prediction, i.e. the existence of a lower bound critical capacity, where having fewer constraints yields survivors with positive probability. Briefly, we use the <em>Cavity method</em> to approximate the distribution of the number of satisfying assignments, then attempt to use the second moment method on that distribution to get our lower bound. A direct application fails, however, due to the variance of the Gaussian constraints. The solution is to carefully choose exactly what to condition on without destroying the model. Specifically, we iteratively compute values related to the Gaussian disorder, after which we are able remove enough variance for the second moment method to work and thus establish the lower bound for the Ising Perceptron’s critical capacity. The result holds subject to an analytical condition which is detailed in the paper (Condition 1.2 in [6]) and which remains to be rigorously verified.</p>
<p><strong>1.1. Problem</strong></p>
<p>We pick a random direction in <img alt="\mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{R}^n"/>, and delete all vertices in the hypercube <img alt="\{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}^n"/> which are in the half-space negatively correlated with that direction. We repeat this process of picking a random half space and deleting points <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> times, and see if any points in the hypercube survive. Formally, we define the perceptron model as follows:</p>
<p><strong>Definition 1:</strong> Let <img alt="G = (g_{\mu{}i})_{\mu\ge 1, i\ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=G+%3D+%28g_%7B%5Cmu%7B%7Di%7D%29_%7B%5Cmu%5Cge+1%2C+i%5Cge+1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G = (g_{\mu{}i})_{\mu\ge 1, i\ge 1}"/> array of i.i.d. standard Gaussians. Let <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> be the largest integer such that:</p>
<p><img alt="\left\{J\in \{\pm 1\}^N:\sum_{i=1}^N\frac{g_{\mu{}i}J_i}{\sqrt{N}}\ge 0 \hspace{0.1cm} \forall \mu\le M\right\}\neq \emptyset." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7BJ%5Cin+%5C%7B%5Cpm+1%5C%7D%5EN%3A%5Csum_%7Bi%3D1%7D%5EN%5Cfrac%7Bg_%7B%5Cmu%7B%7Di%7DJ_i%7D%7B%5Csqrt%7BN%7D%7D%5Cge+0+%5Chspace%7B0.1cm%7D+%5Cforall+%5Cmu%5Cle+M%5Cright%5C%7D%5Cneq+%5Cemptyset.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left\{J\in \{\pm 1\}^N:\sum_{i=1}^N\frac{g_{\mu{}i}J_i}{\sqrt{N}}\ge 0 \hspace{0.1cm} \forall \mu\le M\right\}\neq \emptyset."/></p>
<p>More compactly, <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> is the largest integer such that</p>
<p><img alt="\left\{J\in \{\pm 1\}^N:\frac{GJ}{\sqrt{N}}\ge 0\right\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7BJ%5Cin+%5C%7B%5Cpm+1%5C%7D%5EN%3A%5Cfrac%7BGJ%7D%7B%5Csqrt%7BN%7D%7D%5Cge+0%5Cright%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left\{J\in \{\pm 1\}^N:\frac{GJ}{\sqrt{N}}\ge 0\right\}"/></p>
<p>is nonempty, where the inequality is taken pointwise, <img alt="G \in \mathbb{R}^{M \times N}" class="latex" src="https://s0.wp.com/latex.php?latex=G+%5Cin+%5Cmathbb%7BR%7D%5E%7BM+%5Ctimes+N%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G \in \mathbb{R}^{M \times N}"/> is an array of i.i.d. std. Gaussians, and <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J"/> is treated as a vector in <img alt="\{\pm 1\}^N \subset \mathbb{R}^N" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5EN+%5Csubset+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}^N \subset \mathbb{R}^N"/>.</p>
<p><img alt="CS229 Pic 1" class="alignnone size-full wp-image-6353" src="https://windowsontheory.files.wordpress.com/2018/12/CS229-Pic-1.png?w=600"/></p>
<p>In the late 80’s, physicists conjectured that there is a critical capacity <img alt="\alpha_{*}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_{*}"/> such that <img alt="\frac{M}{N} \overset{P}{\to} \alpha_{*}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BM%7D%7BN%7D+%5Coverset%7BP%7D%7B%5Cto%7D+%5Calpha_%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{M}{N} \overset{P}{\to} \alpha_{*}"/> where <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> is a function of <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/>. The predicted critical capacity has been studied, for example in [7,9]. Our goal is to establish a lower bound on <img alt="\alpha_*" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_*"/> for the Ising Perceptron under Gaussian disorder. To do this, let <img alt="Z(G)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)"/> denote the random variable which measures how many choices of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> variables survive <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> Gaussian constraints in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> (this is the partition function). We want to show <img alt="Z &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=Z+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z &gt; 0"/> with high probability when there are <img alt="M &lt; \alpha_{*}N" class="latex" src="https://s0.wp.com/latex.php?latex=M+%3C+%5Calpha_%7B%2A%7DN&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M &lt; \alpha_{*}N"/> constraints. To this end, the second moment method seems promising:</p>
<p><strong>Second Moment Method: </strong>If <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> is a nonnegative random variable with finite variance, then</p>
<p><img alt="P(Z &gt; 0) \ge \frac{\left(\mathbb{E}[Z]\right)^2}{\mathbb{E}[Z^2]}" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z+%3E+0%29+%5Cge+%5Cfrac%7B%5Cleft%28%5Cmathbb%7BE%7D%5BZ%5D%5Cright%29%5E2%7D%7B%5Cmathbb%7BE%7D%5BZ%5E2%5D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z &gt; 0) \ge \frac{\left(\mathbb{E}[Z]\right)^2}{\mathbb{E}[Z^2]}"/></p>
<p>However, this method actually fails due to various sources of variance in the perceptron model. We will briefly sketch the fix as given in [6].</p>
<p>Before we can start, however, what does the distribution of <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> even look like? This is actually quite computationally intensive; we will use the <strong>cavity equations</strong>, a technique developed in [12,13], to approximate <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/>‘s distribution.</p>
<p><strong>1.2. Cavity Method</strong></p>
<p>The goal of the cavity method is to see how the solution space changes as we remove a row or a column of the matrix <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> with i.i.d. standard Gaussian entries, assuming that <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is fixed. Since our matrix <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is big and difficult to deal with, we try to see how the solution space changes as we add one row or one column at a time. Why is this valuable? We can think of the system of variables and constraints as an interaction between the rows (constraints) and columns (variables), so the number of solutions should behave proportionally to the product of the solutions attributed to each variable and each constraint. With this as motivation, define <img alt="G_{-\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=G_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{-\mu}"/> as the matrix obtained by removing row <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/> and <img alt="G^{-i}" class="latex" src="https://s0.wp.com/latex.php?latex=G%5E%7B-i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G^{-i}"/> as the matrix obtained by removing column <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>. We can approximate</p>
<p><img alt="Z(G) \approx \prod\limits_{\mu = 1}^{M} \frac{Z(G)}{Z(G_{-\mu})} \cdot \prod\limits_{i = 1}^{N} \frac{Z(G)}{Z(G^{-i})}" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29+%5Capprox+%5Cprod%5Climits_%7B%5Cmu+%3D+1%7D%5E%7BM%7D+%5Cfrac%7BZ%28G%29%7D%7BZ%28G_%7B-%5Cmu%7D%29%7D+%5Ccdot+%5Cprod%5Climits_%7Bi+%3D+1%7D%5E%7BN%7D+%5Cfrac%7BZ%28G%29%7D%7BZ%28G%5E%7B-i%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G) \approx \prod\limits_{\mu = 1}^{M} \frac{Z(G)}{Z(G_{-\mu})} \cdot \prod\limits_{i = 1}^{N} \frac{Z(G)}{Z(G^{-i})}"/></p>
<p>since we can think of the partition function as receiving a multiplicative factor from each addition of a row and each addition of a column. Thus, the cavity method seeks to compute <img alt="Z(G)/Z(G_{-\mu})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G_{-\mu})"/> and <img alt="Z(G)/Z(G^{-i})." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})."/></p>
<p><strong>1.2.1. Removing a constraint</strong></p>
<p>Our goal in computing <img alt="Z(G)/Z(G_{-\mu})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G_{-\mu})"/> is to understand how the solution space <img alt="SOL(G_{-\mu})" class="latex" src="https://s0.wp.com/latex.php?latex=SOL%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="SOL(G_{-\mu})"/> changes when we add in the constraint <img alt="\mu." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu."/> Recalling that <img alt="J \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=J+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J \in \{\pm 1\}^n"/> is our vector that is potentially in the solution space, if we define</p>
<p><img alt="\Delta_\mu := \sum_{i = 1}^{N}\frac{g_{\mu{}i}J_i}{\sqrt{N}}," class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%3A%3D+%5Csum_%7Bi+%3D+1%7D%5E%7BN%7D%5Cfrac%7Bg_%7B%5Cmu%7B%7Di%7DJ_i%7D%7B%5Csqrt%7BN%7D%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu := \sum_{i = 1}^{N}\frac{g_{\mu{}i}J_i}{\sqrt{N}},"/></p>
<p>then adding the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/> is equivalent to forcing <img alt="\Delta_\mu \ge 0." class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%5Cge+0.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu \ge 0."/> We will try to understand the distribution of <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> under the Gibbs measure <img alt="\nu_{-\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\nu_{-\mu}"/>, which is the uniform measure on the solution space without the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/>. This way, we can determine the probability of <img alt="\Delta_\mu \ge 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%5Cge+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu \ge 0"/> under the Gibbs measure, which will equal <img alt="Z(G)/Z(G_{-\mu})." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G_{-\mu})."/> To do so, we will use the <strong>Replica Symmetric Cavity Assumption</strong> (which we will abbreviate as RS cavity assumption). The RS cavity assumption lets us assume that the <img alt="J_i" class="latex" src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i"/>‘s for <img alt="1 \le i \le N" class="latex" src="https://s0.wp.com/latex.php?latex=1+%5Cle+i+%5Cle+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1 \le i \le N"/> are independent under <img alt="\nu_{-\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\nu_{-\mu}"/>. As the <img alt="J_i" class="latex" src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i"/>‘s are some discrete sample space that depend on our constraints, we do not actually have full independence, but the RS cavity assumption tells us there is very little dependence between the <img alt="J_i" class="latex" src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i"/>‘s, so we pretend they are independent.</p>
<p>Note that <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> should be approximately normally distributed, since it is the sum of many “independent” terms <img alt="g_{\mu i} J_i" class="latex" src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu+i%7D+J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_{\mu i} J_i"/> by the RS Cavity assumption. Now, define <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/> as the expectation of <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> under the Gibbs measure without the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/>, and <img alt="v_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=v_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v_\mu"/> as the variance of <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> under the Gibbs measure without the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/>. It will also turn out that the variance <img alt="v_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=v_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v_\mu"/> will concentrate around a constant <img alt="\sigma^2." class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma^2."/> Thus,</p>
<p><img alt="\frac{Z(G)}{Z(G_{-\mu})} \approx \nu_{-\mu}(\Delta_{\mu} \ge 0) = \overline{\Phi} (\frac{-h_\mu}{\sigma})." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BZ%28G%29%7D%7BZ%28G_%7B-%5Cmu%7D%29%7D+%5Capprox+%5Cnu_%7B-%5Cmu%7D%28%5CDelta_%7B%5Cmu%7D+%5Cge+0%29+%3D+%5Coverline%7B%5CPhi%7D+%28%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{Z(G)}{Z(G_{-\mu})} \approx \nu_{-\mu}(\Delta_{\mu} \ge 0) = \overline{\Phi} (\frac{-h_\mu}{\sigma})."/></p>
<p>Here, <img alt="\overline{\Phi}(\frac{-h_\mu}{\sigma})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Coverline%7B%5CPhi%7D%28%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\overline{\Phi}(\frac{-h_\mu}{\sigma})"/> equals the probability that a random <img alt="\mathcal{N}(h_\mu, \sigma^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%28h_%5Cmu%2C+%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(h_\mu, \sigma^2)"/> Gaussian distribution is positive, or equivalently, the probability that a standard Gaussian <img alt="\mathcal{N}(0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(0, 1)"/> distribution is greater than <img alt="\frac{-h_\mu}{\sigma}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{-h_\mu}{\sigma}."/></p>
<p><strong>1.2.2. Removing a spin</strong></p>
<p>To calculate the cavity equation for removing one column, we think of removing a column as deleting one spin from <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J"/>. Define <img alt="J^{-i}" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E%7B-i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J^{-i}"/> as the vector resulting from removing spin <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> from <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J"/>. We want to calculate <img alt="Z(G)/Z(G^{-i})." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})."/> Note that it is possible for <img alt="J^{-i} \not\in SOL(G^{-i})" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E%7B-i%7D+%5Cnot%5Cin+SOL%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J^{-i} \not\in SOL(G^{-i})"/> but <img alt="J \in SOL(G)" class="latex" src="https://s0.wp.com/latex.php?latex=J+%5Cin+SOL%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J \in SOL(G)"/> now, which complicates this calculation. It is possible to overcome this difficulty by passing to positive temperature, though this makes the calculations incredibly difficult. We do not worry about these issues here, and just briefly sketch how <img alt="Z(G)/Z(G^{-i})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})"/> is computed.</p>
<p>To compute <img alt="Z(G)/Z(G^{-i})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})"/>, we will split the numerator into a sum of two terms based on the sign of <img alt="J_i," class="latex" src="https://s0.wp.com/latex.php?latex=J_i%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i,"/> which will allow us to compute not only <img alt="Z(G)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)"/> but also <img alt="\langle J_i \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+J_i+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\langle J_i \rangle"/>, which represents the magnetization at spin <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>. A series of complicated calculations (see the lecture notes for the details) will give us</p>
<p><img alt="\frac{Z(G)}{Z(G^{-i})} = \frac{\exp(H_i)+\exp(-H_i)}{exp(c)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BZ%28G%29%7D%7BZ%28G%5E%7B-i%7D%29%7D+%3D+%5Cfrac%7B%5Cexp%28H_i%29%2B%5Cexp%28-H_i%29%7D%7Bexp%28c%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{Z(G)}{Z(G^{-i})} = \frac{\exp(H_i)+\exp(-H_i)}{exp(c)}"/></p>
<p>for some constant <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/>, where <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/> is a quantity that compares how much more correlated spin <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> is to the constraints than all other spins. The <img alt="\exp(+H_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28%2BH_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(+H_i)"/> will come from the solutions for <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> with <img alt="J_i = 1" class="latex" src="https://s0.wp.com/latex.php?latex=J_i+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i = 1"/> and the <img alt="\exp(-H_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-H_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(-H_i)"/> will come from the solutions with <img alt="J_i = -1" class="latex" src="https://s0.wp.com/latex.php?latex=J_i+%3D+-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i = -1"/>.</p>
<p>The above equations allow us to deduce that</p>
<p><img alt="Z(G) \approx \prod\limits_{\mu = 1}^{M} \overline{\Phi}\left(-\frac{h_\mu}{\sigma}\right) \cdot \prod\limits_{i = 1}^{N} \frac{\exp(H_i) + \exp(-H_i)}{\exp(c)}." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29+%5Capprox+%5Cprod%5Climits_%7B%5Cmu+%3D+1%7D%5E%7BM%7D+%5Coverline%7B%5CPhi%7D%5Cleft%28-%5Cfrac%7Bh_%5Cmu%7D%7B%5Csigma%7D%5Cright%29+%5Ccdot+%5Cprod%5Climits_%7Bi+%3D+1%7D%5E%7BN%7D+%5Cfrac%7B%5Cexp%28H_i%29+%2B+%5Cexp%28-H_i%29%7D%7B%5Cexp%28c%29%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G) \approx \prod\limits_{\mu = 1}^{M} \overline{\Phi}\left(-\frac{h_\mu}{\sigma}\right) \cdot \prod\limits_{i = 1}^{N} \frac{\exp(H_i) + \exp(-H_i)}{\exp(c)}."/></p>
<p><strong>1.3. The Randomness of G</strong></p>
<p>In the previous section, we regarded <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> as fixed. We now use the these results but allow for <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> to be random again. Recall <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>‘s entries were i.i.d. Gaussians. We thus get that <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/>, as a linear combination of the <img alt="g_{\mu{}i}" class="latex" src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu%7B%7Di%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_{\mu{}i}"/>‘s, is a Gaussian. We thus can write <img alt="h_\mu \sim \mathcal{N}(0, q)." class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu+%5Csim+%5Cmathcal%7BN%7D%280%2C+q%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu \sim \mathcal{N}(0, q)."/> Similarly, <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/> is a linear combination of the <img alt="g_{\mu{}i}" class="latex" src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu%7B%7Di%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_{\mu{}i}"/>‘s and must be Gaussian. We thus write <img alt="H_i \sim \mathcal{N}(0, \psi)" class="latex" src="https://s0.wp.com/latex.php?latex=H_i+%5Csim+%5Cmathcal%7BN%7D%280%2C+%5Cpsi%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i \sim \mathcal{N}(0, \psi)"/>.</p>
<p>With some calculations detailed in the lecture notes and [12], we get <em>Gardner’s Formula</em>:</p>
<p><img alt="\frac{\ln Z}{N} \rightarrow \alpha \int \ln \overline{\Phi}\left(-\frac{\sqrt{q} z}{\sigma}\right) \varphi(z) dz + \int \ln \left(2 \cosh(\sqrt{\psi z})\right)\varphi(z) dz" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cln+Z%7D%7BN%7D+%5Crightarrow+%5Calpha+%5Cint+%5Cln+%5Coverline%7B%5CPhi%7D%5Cleft%28-%5Cfrac%7B%5Csqrt%7Bq%7D+z%7D%7B%5Csigma%7D%5Cright%29+%5Cvarphi%28z%29+dz+%2B+%5Cint+%5Cln+%5Cleft%282+%5Ccosh%28%5Csqrt%7B%5Cpsi+z%7D%29%5Cright%29%5Cvarphi%28z%29+dz&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{\ln Z}{N} \rightarrow \alpha \int \ln \overline{\Phi}\left(-\frac{\sqrt{q} z}{\sigma}\right) \varphi(z) dz + \int \ln \left(2 \cosh(\sqrt{\psi z})\right)\varphi(z) dz"/></p>
<p>where <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/> is our capacity, <img alt="\frac{M}{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BM%7D%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{M}{N}"/>. It turns out that <img alt="\sigma^2 = 1-q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2+%3D+1-q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma^2 = 1-q"/> and <img alt="c = \psi(1-q)/2," class="latex" src="https://s0.wp.com/latex.php?latex=c+%3D+%5Cpsi%281-q%29%2F2%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c = \psi(1-q)/2,"/> so the above equation only depends on two parameters, <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> and <img alt="\psi." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi."/> It will turn out that <img alt="q, \psi" class="latex" src="https://s0.wp.com/latex.php?latex=q%2C+%5Cpsi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q, \psi"/> have relations dependent on each other based on our definitions of <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/> and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>, and these will give us a fixed point equation for <img alt="(q, \psi)," class="latex" src="https://s0.wp.com/latex.php?latex=%28q%2C+%5Cpsi%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(q, \psi),"/> which has two solutions: one near <img alt="\alpha \approx 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Capprox+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \approx 1"/> and one near <img alt="\alpha \approx 0.83." class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Capprox+0.83.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \approx 0.83."/> It is believed that the second point is correct, meaning that the critical capacity should equal <img alt="\alpha_* = 0.83." class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%2A+%3D+0.83.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_* = 0.83."/></p>
<p><strong>1.4. Second Moment Method</strong></p>
<p>Now that we have Gardner’s formula, solving for <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> gives us an approximation for its distribution as</p>
<p><img alt="Z \sim \exp\{N\mathcal{G}(\alpha) + \mathcal{N}(0, N\varepsilon^2)\}" class="latex" src="https://s0.wp.com/latex.php?latex=Z+%5Csim+%5Cexp%5C%7BN%5Cmathcal%7BG%7D%28%5Calpha%29+%2B+%5Cmathcal%7BN%7D%280%2C+N%5Cvarepsilon%5E2%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z \sim \exp\{N\mathcal{G}(\alpha) + \mathcal{N}(0, N\varepsilon^2)\}"/></p>
<p>where <img alt="\mathcal{G}(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BG%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{G}(\alpha)"/> is Gardner’s formula, and the Gaussian noise <img alt="\mathcal{N}(0,N\varepsilon^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%280%2CN%5Cvarepsilon%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(0,N\varepsilon^2)"/> arises from the Gaussian distribution of the <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/>‘s and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>‘s. Again, we are interested in the probability that <img alt="Z &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=Z+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z &gt; 0"/>, but due to the approximate nature of the above equation, we cannot work with this distribution directly, and instead can try the second moment method. However, the exponentiated Gaussian makes <img alt="\mathbb{E}[Z^2] \gg \mathbb{E}[Z]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5E2%5D+%5Cgg+%5Cmathbb%7BE%7D%5BZ%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[Z^2] \gg \mathbb{E}[Z]^2"/> and the moment method just gives <img alt="P(Z&gt;0) \ge 0" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29+%5Cge+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0) \ge 0"/>, whereas we need <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/> with high probability.</p>
<p>The fault here lies with Gaussian noise due to the <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/>‘s and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>‘s, so it is natural to consider what happens when we condition on them, getting rid of the noise. We denote</p>
<p><img alt="\underline{h} = (h_\mu)_{\mu = 1}^M \text{ and } \underline{H} = (H_i)_{i=1}^N." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D+%3D+%28h_%5Cmu%29_%7B%5Cmu+%3D+1%7D%5EM+%5Ctext%7B+and+%7D+%5Cunderline%7BH%7D+%3D+%28H_i%29_%7Bi%3D1%7D%5EN.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h} = (h_\mu)_{\mu = 1}^M \text{ and } \underline{H} = (H_i)_{i=1}^N."/></p>
<p>Then, we can compute that</p>
<p><img alt="\frac{1}{N}\log \mathbb{E}[Z | \underline{H},\underline{h}] \to \mathcal{G}(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D%5Clog+%5Cmathbb%7BE%7D%5BZ+%7C+%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D+%5Cto+%5Cmathcal%7BG%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{N}\log \mathbb{E}[Z | \underline{H},\underline{h}] \to \mathcal{G}(\alpha)"/></p>
<p>in probability as <img alt="N\to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\to \infty"/>. (This is not an exact equality becacuse of the approximations made in the derivation of Gardner’s formula.) Moreover, we will have <img alt="\mathbb{E}[Z^2|\underline{H},\underline{h}] \approx \mathbb{E}[Z|\underline{H},\underline{h}]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5E2%7C%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D+%5Capprox+%5Cmathbb%7BE%7D%5BZ%7C%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[Z^2|\underline{H},\underline{h}] \approx \mathbb{E}[Z|\underline{H},\underline{h}]^2"/>. The second moment method gives us the desired lower bound on <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/>. However, note that these <img alt="\underline{H},\underline{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H},\underline{h}"/> must satisfy the equations that defined them. In vector form, we have</p>
<p><img alt="\frac{1}{\sqrt{N}}G\tanh \underline{H} = \underline{h} + b_*F(\underline{h})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7BN%7D%7DG%5Ctanh+%5Cunderline%7BH%7D+%3D+%5Cunderline%7Bh%7D+%2B+b_%2AF%28%5Cunderline%7Bh%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{\sqrt{N}}G\tanh \underline{H} = \underline{h} + b_*F(\underline{h})"/><br/>
<img alt="\frac{1}{\sqrt{N}}G^T\tanh F(\underline{h}) = \underline{H} + d_*\tanh\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7BN%7D%7DG%5ET%5Ctanh+F%28%5Cunderline%7Bh%7D%29+%3D+%5Cunderline%7BH%7D+%2B+d_%2A%5Ctanh%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{\sqrt{N}}G^T\tanh F(\underline{h}) = \underline{H} + d_*\tanh\underline{H}"/></p>
<p>where <img alt="b_*" class="latex" src="https://s0.wp.com/latex.php?latex=b_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b_*"/>, <img alt="F" class="latex" src="https://s0.wp.com/latex.php?latex=F&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F"/>, and <img alt="d_*" class="latex" src="https://s0.wp.com/latex.php?latex=d_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d_*"/> are constants and functions that appear in the rigorous definitions of <img alt="h_{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{\mu}"/> and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>.<br/>
Here arises a problem, however. We cannot solve these equations easily, and furthermore when we condition on <img alt="\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}"/> and <img alt="\underline{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}"/>, the <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>‘s that satisfy the above are not necessarily representative of matrices of standard Gaussians. Hence, simply conditioning on knowing the values of <img alt="\underline{h},\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%2C%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h},\underline{H}"/> destroys the model and will not prove the lower bound on <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/> for Gaussian disorder.</p>
<p>To solve this problem, we introduce iteration: first, initialize <img alt="\underline{h}^{(0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%280%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}^{(0)}"/> and <img alt="\underline{H}^{(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%281%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}^{(1)}"/> (initialized to specific values detailed in [6]. Then, a simplified version of each time step’s update looks like</p>
<p><img alt="\underline{h}^{(t)} = \frac{G\tanh'{\underline{H}^{(t)}}}{\sqrt{N}} - b_*F(\underline{h}^{(t - 1)})," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%28t%29%7D+%3D+%5Cfrac%7BG%5Ctanh%27%7B%5Cunderline%7BH%7D%5E%7B%28t%29%7D%7D%7D%7B%5Csqrt%7BN%7D%7D+-+b_%2AF%28%5Cunderline%7Bh%7D%5E%7B%28t+-+1%29%7D%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}^{(t)} = \frac{G\tanh'{\underline{H}^{(t)}}}{\sqrt{N}} - b_*F(\underline{h}^{(t - 1)}),"/><br/>
<img alt="\underline{H}^{(t + 1)} = \frac{G^TF(\underline{h}^t)}{\sqrt{N}} - d_*\tanh{\underline{H}^{(t + 1)}}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%28t+%2B+1%29%7D+%3D+%5Cfrac%7BG%5ETF%28%5Cunderline%7Bh%7D%5Et%29%7D%7B%5Csqrt%7BN%7D%7D+-+d_%2A%5Ctanh%7B%5Cunderline%7BH%7D%5E%7B%28t+%2B+1%29%7D%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}^{(t + 1)} = \frac{G^TF(\underline{h}^t)}{\sqrt{N}} - d_*\tanh{\underline{H}^{(t + 1)}}."/></p>
<p>It has been proven (see [2,4]) that <img alt="\underline{h}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}^{(t)}"/> and <img alt="\underline{H}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}^{(t)}"/> converge as <img alt="t\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=t%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t\to\infty"/>, and moreover the convergent values are distributed by what looks like a Gaussian at each time step. Since this sequence of <img alt="\underline{h},\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%2C%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h},\underline{H}"/> converge (at a rate that is independent of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>) and are representative of Gaussian <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>, for a special kind of truncated partition function <img alt="\tilde Z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tilde Z"/>, conditioning on these iteration values allows the second moment method to work and gives</p>
<p><img alt="\mathbb{E}[\tilde Z | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]^2 \approx \mathbb{E}[\tilde Z^2 | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Ctilde+Z+%7C+%5Cunderline%7Bh%7D%5E%7B%280%29%7D%2C%5Cunderline%7BH%7D%5E%7B%280%29%7D%2C%5Cdots%2C%5Cunderline%7Bh%7D%5E%7B%28t%29%7D%2C%5Cunderline%7BH%7D%5E%7B%28t%29%7D%5D%5E2+%5Capprox+%5Cmathbb%7BE%7D%5B%5Ctilde+Z%5E2+%7C+%5Cunderline%7Bh%7D%5E%7B%280%29%7D%2C%5Cunderline%7BH%7D%5E%7B%280%29%7D%2C%5Cdots%2C%5Cunderline%7Bh%7D%5E%7B%28t%29%7D%2C%5Cunderline%7BH%7D%5E%7B%28t%29%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[\tilde Z | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]^2 \approx \mathbb{E}[\tilde Z^2 | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]"/></p>
<p>which is then enough to establish the lower bound <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/> for the non-truncated partition function <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> (see [6] for details, see also [3] for closely related computations).</p>
<p><strong>2. <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAE-SAT</strong></p>
<p>This lecture also gave a brief overview of results in <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT. In the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT problem, we ask whether a boolean formula in <img alt="CNF" class="latex" src="https://s0.wp.com/latex.php?latex=CNF&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="CNF"/> form, with each clause containing exactly <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> literals, has a satisfying solution. For <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT, we instead require that the satisfying solution is not uniform on any clause; equivalently, each clause must contain at least one true and at least one false value. Finally, we will restrict our set of possible boolean formulae to those for which every variable is contained in exactly <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> clauses; we call this model <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-regular <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT. We briefly outline the critical capacities of clauses where the solution space changes. For more background on the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT probem and its variants, see [10].</p>
<p>As with the perceptron model, we are concerned with the expected number of solutions <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> of this system where the <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-regular formula is chosen uniformly at random. It turns out that for any fixed <img alt="\alpha=\frac{d}{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3D%5Cfrac%7Bd%7D%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha=\frac{d}{k}"/>, the expected proportion of satisfiable solutions as <img alt="n\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n\to\infty"/> converges to some <img alt="f(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\alpha)"/>. More on this, the model in general and the critical <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/> constants mentioned below can be found in [15]. Via an application of Markov’s inequality and Jensen’s Inequality for the convex function <img alt="x^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x^n"/>, <img alt="f(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\alpha)"/> may be bounded above by <img alt="f^{RS}(\alpha)=(\mathbb{E} Z)^{\frac{1}{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7BRS%7D%28%5Calpha%29%3D%28%5Cmathbb%7BE%7D+Z%29%5E%7B%5Cfrac%7B1%7D%7Bn%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f^{RS}(\alpha)=(\mathbb{E} Z)^{\frac{1}{n}}"/>, shown graphically below.</p>
<p><img alt="CS 229 Pic 2" class="alignnone size-full wp-image-6352" src="https://windowsontheory.files.wordpress.com/2018/12/CS-229-Pic-2.png?w=600"/><br/>
The diagram also shows the nature of the expected assortment of the solutions of a random <img alt="d-" class="latex" src="https://s0.wp.com/latex.php?latex=d-&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d-"/>regular <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT for ranges of values of <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/>. Namely, physics analysis suggests the following:</p>
<ul>
<li>For <img alt="0&lt;\alpha&lt;\alpha_d" class="latex" src="https://s0.wp.com/latex.php?latex=0%3C%5Calpha%3C%5Calpha_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0&lt;\alpha&lt;\alpha_d"/>, w.h.p. the solutions are concentrated in a single large cluster.</li>
<li>For <img alt="\alpha_d&lt;\alpha&lt;\alpha_{\text{cond}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_d%3C%5Calpha%3C%5Calpha_%7B%5Ctext%7Bcond%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_d&lt;\alpha&lt;\alpha_{\text{cond}}"/>, w.h.p. the solutions are distributed among a large number of clusters.</li>
<li>For <img alt="\alpha_{\text{cond}}&lt;\alpha_{\text{sat}}&lt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%7B%5Ctext%7Bcond%7D%7D%3C%5Calpha_%7B%5Ctext%7Bsat%7D%7D%3C0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_{\text{cond}}&lt;\alpha_{\text{sat}}&lt;0"/>, the function <img alt="f(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\alpha)"/> breaks away from <img alt="f^{RS}(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7BRS%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f^{RS}(\alpha)"/>, and w.h.p. the solutions are concentrated in a small number of clusters.</li>
<li>For <img alt="\alpha&gt;\alpha_{\text{sat}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3E%5Calpha_%7B%5Ctext%7Bsat%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha&gt;\alpha_{\text{sat}}"/>, w.h.p. there are no satisfiable solutions.</li>
</ul>
<p>One final note for this model: the solutions to satisfiability problems tend to be more clumpy than in the perceptron model, so correlation decay methods won’t immediately work. See [15] for how this is handled.</p>
<p><strong>References</strong></p>
<ol>
<li>N. Bansal. Constructive algorithms for discrepancy minimization. In <em>Proc. FOCS</em> 2010, pages 3–10.</li>
<li>M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. <em>IEEE Trans. Inform. Theory</em>, 57(2):764-785, 2011.</li>
<li>Bolthausen, E., 2018. A Morita type proof of the replica-symmetric formula for SK. arXiv preprint arXiv:1809.07972.</li>
<li>E. Bolthausen. An iterative construction of solutions of the TAP equations for the Sherrington-Kirkpatrick model. <em>Commun. Math. Phys.</em>, 325(1):333-366, 2014.</li>
<li>T. Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. <em>IEEE Trans. Electron. Comput.</em> 14(3):326-334.</li>
<li>J. Ding and N. Sun. Capacity lower bound for the Ising perceptron. <a href="https://arxiv.org/pdf/1809.07742.pdf" rel="nofollow">https://arxiv.org/pdf/1809.07742.pdf</a></li>
<li>E. Gardner and B. Derrida. Optimal storage properties of neural network models. <em>J. Phys. A.</em>, 21(1): 271–284, 1988.</li>
<li>J. H. Kim and J. R. Roche. Covering cubes by random half cubes, with applications to binary neural networks. <em>J. Comput. Syst. Sci.</em>, 56(2):223–252, 1998</li>
<li>W. Krauth and M. Mézard. Storage capacity of memory networks with binary couplings. <em>J. Phy.</em> 50(20): 3057-3066, 1989.</li>
<li>F. Krzakala et al. “Gibbs states and the set of solutions of random constraint satisfaction problems.” <i>Proc. Natl. Acad. Sci.</i> 104.25 (2007): 10318-10323.</li>
<li>S. Lovett and R. Meka. Constructive discrepancy minimization by walking on the edges. <em>SIAM J. Comput.</em>, 44(5):1573-1582.</li>
<li>M. Mézard. The space of interactions in neural networks: Gardner’s computation with the cavity method. <em>J. Phys. A.</em>, 22(12):2181, 1989</li>
<li>M. Mézard and G. Parisi and M. Virasoro. SK Model: The Replica Solution without Replicas. <em>Europhys. Lett.</em>, 1(2): 77-82, 1986.</li>
<li>M. Shcherbina and B. Tirozzi. Rigorous solution of the Gardner problem. <em>Commun. Math. Phys.</em>, 234(3):383-422, 2003.</li>
<li>A. Sly and N. Sun and Y. Zhang. The Number of Solutions for Random Regular NAE-SAT. In <em>Proc. FOCS</em> 2016, pages 724-731.</li>
<li>J. Spencer. Six standard deviations suffice. <em>Trans. Amer. Math. Soc.</em> 289 (1985), 679-706</li>
<li>M. Talagrand. Intersecting random half cubes. <em>Random Struct. Algor.</em>, 15(3-4):436–449, 1999.</li>
</ol></div>
    </content>
    <updated>2018-12-14T04:44:40Z</updated>
    <published>2018-12-14T04:44:40Z</published>
    <category term="physics"/>
    <author>
      <name>degeneratetriangle</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-17T05:29:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6311</id>
    <link href="https://windowsontheory.org/2018/12/07/quantum-error-correction/" rel="alternate" type="text/html"/>
    <title>Peter Shor on Quantum Error Correction</title>
    <summary>[Guest post by Annie Wei who scribed Peter Shor’s lecture in our physics and computation seminar. See here for all the posts of this seminar. –Boaz] On October 19, we were lucky enough to have Professor Peter Shor give a guest lecture about quantum error correcting codes. In this blog post, I (Annie Wei) will […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Annie Wei who scribed Peter Shor’s lecture in our <a href="https://www.boazbarak.org/fall18seminar/">physics and computation seminar</a>. See <a href="https://windowsontheory.org/category/physics/">here</a> for all the posts of this seminar. –Boaz]</em></p>
<p>On October 19, we were lucky enough to have Professor Peter Shor give a guest lecture about quantum error correcting codes. In this blog post, I (Annie Wei) will present a summary of this guest lecture, which builds up quantum error correcting codes starting from classical coding theory. We will start by reviewing an example from classical error correction to motivate the similarities and differences when compared against the quantum case, before moving into quantum error correction and quantum channels. Note that we do assume a very basic familiarity with quantum mechanics, such as that which might be found <a href="https://people.eecs.berkeley.edu/~vazirani/f16quantum/lec1.pdf">here</a> or <a href="http://www.theory.caltech.edu/people/preskill/ph229/notes/chap2.pdf">here</a>.</p>
<p><strong>1. Motivation</strong><br/>
We are interested in quantum error correction, ultimately, because any real-world computing device needs to be able to tolerate noise. Theoretical work on quantum algorithms has shown that quantum computers have the potential to offer speedups for a variety of problems, but in practice we’d also like to be able to eventually build and operate real quantum computers. We need to be able to protect against any decoherence that occurs when a quantum computer interacts with the environment, and we need to be able to protect against the accumulation of small gate errors since quantum gates need to be unitary operators.</p>
<p>In error correction the idea is to protect against noise by encoding information in a way that is resistant to noise, usually by adding some redundancy to the message. The redundancy then ensures that enough information remains, even after noise corruption, so that decoding will allow us to recover our original message. This is what is done in classical error correction schemes.</p>
<p>Unfortunately, it’s not obvious that quantum error correction is possible. One obstacle is that errors are continuous, since a continuum of operations can be applied to a qubit, so a priori it might seem like identifying and correcting an error would require infinite resources. In a later section we show how this problem, that of identifying quantum errors, can be overcome. Another obstacle is the fact that, as we’ve stated, classical error correction works by adding redundancy to a message. This might seem impossible to perform in a quantum setting due to the No Cloning Theorem, which states the following:</p>
<p><strong>Theorem</strong> (<em>No Cloning Theorem</em>): Performing the mapping</p>
<p><img alt="|\psi\rangle|0\rangle\mapsto|\psi\rangle|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle%7C0%5Crangle%5Cmapsto%7C%5Cpsi%5Crangle%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle|0\rangle\mapsto|\psi\rangle|\psi\rangle"/></p>
<p>is not a permissible quantum operation.</p>
<p><strong>Proof</strong>: We will use unitarity, which says that a quantum operation specified by a unitary matrix <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> must satisfy</p>
<p><img alt="\langle\phi|U^{\dagger}U|\psi\rangle = \langle\phi|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle%5Cphi%7CU%5E%7B%5Cdagger%7DU%7C%5Cpsi%5Crangle+%3D+%5Clangle%5Cphi%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\langle\phi|U^{\dagger}U|\psi\rangle = \langle\phi|\psi\rangle"/>.</p>
<p>(This ensures that the normalization of the state <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/> is always preserved, i.e. that <img alt="|\langle\psi|\psi\rangle|^2=1" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Clangle%5Cpsi%7C%5Cpsi%5Crangle%7C%5E2%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\langle\psi|\psi\rangle|^2=1"/>, which is equivalent to the conservation of probability.)</p>
<p>Now suppose that we can perform the operation</p>
<p><img alt="U(|\psi\rangle|0\rangle)=|\psi\rangle|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=U%28%7C%5Cpsi%5Crangle%7C0%5Crangle%29%3D%7C%5Cpsi%5Crangle%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U(|\psi\rangle|0\rangle)=|\psi\rangle|\psi\rangle"/>.</p>
<p>Then, letting</p>
<p><img alt="(\langle\phi|\langle 0|)(|\psi\rangle|0\rangle)=\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clangle%5Cphi%7C%5Clangle+0%7C%29%28%7C%5Cpsi%5Crangle%7C0%5Crangle%29%3D%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\langle\phi|\langle 0|)(|\psi\rangle|0\rangle)=\alpha"/>,</p>
<p>we note that by unitarity</p>
<p><img alt="(\langle\phi|\langle 0|)(|\psi\rangle |0\rangle)=\alpha(\langle\phi|\langle 0|)U^{\dagger}U(|\psi\rangle|0\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clangle%5Cphi%7C%5Clangle+0%7C%29%28%7C%5Cpsi%5Crangle+%7C0%5Crangle%29%3D%5Calpha%28%5Clangle%5Cphi%7C%5Clangle+0%7C%29U%5E%7B%5Cdagger%7DU%28%7C%5Cpsi%5Crangle%7C0%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\langle\phi|\langle 0|)(|\psi\rangle |0\rangle)=\alpha(\langle\phi|\langle 0|)U^{\dagger}U(|\psi\rangle|0\rangle)"/>.</p>
<p>But</p>
<p><img alt="(\langle\phi|\langle 0|)U^{\dagger}U(|\psi\rangle|0\rangle)=(\langle\phi|\langle\phi|)(|\psi\rangle|\psi\rangle)=\alpha^2" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clangle%5Cphi%7C%5Clangle+0%7C%29U%5E%7B%5Cdagger%7DU%28%7C%5Cpsi%5Crangle%7C0%5Crangle%29%3D%28%5Clangle%5Cphi%7C%5Clangle%5Cphi%7C%29%28%7C%5Cpsi%5Crangle%7C%5Cpsi%5Crangle%29%3D%5Calpha%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\langle\phi|\langle 0|)U^{\dagger}U(|\psi\rangle|0\rangle)=(\langle\phi|\langle\phi|)(|\psi\rangle|\psi\rangle)=\alpha^2"/>,</p>
<p>and in general <img alt="\alpha\neq\alpha^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%5Cneq%5Calpha%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha\neq\alpha^2"/>, so we have a contradiction.</p>
<p>How do we get around this apparent contradiction? To do so, note that the no-cloning theorem only prohibits the copying of non-orthogonal quantum states. With orthogonal quantum states, either <img alt="\alpha=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha=0"/> or <img alt="\alpha=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha=1"/>, so we don’t run into a contradiction. This also explains why it is possible to copy classical information, which we can think of as orthogonal quantum states.</p>
<p>So how do we actually protect quantum information from noise? In the next section we first review classical error correction, as ideas from the classical setting re-appear in the quantum setting, and then we move into quantum error correction.</p>
<p><strong>2. Review of Classical Error Correction</strong><br/>
First we start by reviewing classical error correction. In classical error correction we generally have a message that we encode, send through a noisy channel, and then decode, in the following schematic process:</p>
<p><img alt="fig1.png" class="alignnone size-full wp-image-6312" src="https://windowsontheory.files.wordpress.com/2018/12/fig1.png?w=600"/><br/>
In an effective error correction scheme, the decoding process should allow us to identify any errors that occurred when our message passed through the noisy channel, which then tells us how to correct the errors. The formalism that allows us to do so is the following: we first define a <img alt="r\times n" class="latex" src="https://s0.wp.com/latex.php?latex=r%5Ctimes+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r\times n"/> encoding matrix <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> that takes a message <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> of length <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> and converts it to a codeword <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/> of length <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>, where the codewords make up the span of the rows of <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>. An example of such a matrix is</p>
<p><img alt="G=\left(\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\\1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=G%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bccccccc%7D0%260%260%261%261%261%261%5C%5C1%260%261%260%261%260%261%5C%5C0%261%261%260%260%261%261%5C%5C1%261%261%261%261%261%261%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G=\left(\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\\1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1\end{array}\right)"/>,</p>
<p>corresponding to the 7-bit Hamming codes, which encodes a 4-bit message as a 7-bit codeword. Note that this code has distance 3 since each of the rows in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> differ in at most 3 spots, which means that it can correct at most 1 error (the number of errors that can be corrected is given by half the code distance).</p>
<p>We also define the parity check matrix <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> to be the matrix that satisfies</p>
<p><img alt="GH^T=0" class="latex" src="https://s0.wp.com/latex.php?latex=GH%5ET%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="GH^T=0"/>.</p>
<p>For example, to define <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> corresponding to the <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> we defined for the 7-bit Hamming code, we could take</p>
<p><img alt="H=\left(\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=H%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bccccccc%7D0%260%260%261%261%261%261%5C%5C1%260%261%260%261%260%261%5C%5C0%261%261%260%260%261%261%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H=\left(\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\end{array}\right)"/>.</p>
<p>Then we may decode <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>, a 7-bit string, in the following manner. Say that</p>
<p><img alt="x=c+e" class="latex" src="https://s0.wp.com/latex.php?latex=x%3Dc%2Be&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x=c+e"/>,</p>
<p>where <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/> is a codeword and <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/> is the 1-bit error we wish to correct. Then</p>
<p><img alt="xH^T=(c+e)H^T=eH^T" class="latex" src="https://s0.wp.com/latex.php?latex=xH%5ET%3D%28c%2Be%29H%5ET%3DeH%5ET&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="xH^T=(c+e)H^T=eH^T"/>.</p>
<p>Here <img alt="eH^T" class="latex" src="https://s0.wp.com/latex.php?latex=eH%5ET&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="eH^T"/> uniquely identifies the error and is known as the <em>error syndrome</em>. Having it tells us how to correct the error. Thus our error correction scheme consists of the following steps:</p>
<ol>
<li>Encode a <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/>-bit message <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> by multiplying by <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> to obtain codeword <img alt="mG=c" class="latex" src="https://s0.wp.com/latex.php?latex=mG%3Dc&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="mG=c"/>.</li>
<li>Send the message through channel generating error <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/>, resulting in the string <img alt="x=c+e" class="latex" src="https://s0.wp.com/latex.php?latex=x%3Dc%2Be&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x=c+e"/>.</li>
<li>Multiply by <img alt="H^T" class="latex" src="https://s0.wp.com/latex.php?latex=H%5ET&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^T"/> to obtain the <em>error syndrome</em> <img alt="eH^T" class="latex" src="https://s0.wp.com/latex.php?latex=eH%5ET&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="eH^T"/>.</li>
<li>Correct error <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/> to obtain <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/>.</li>
</ol>
<p>Having concluded our quick review of classical error correction, we now look at the theory of quantum error correction.</p>
<p><strong>3. Quantum Error Correction</strong><br/>
In this section we introduce quantum error correction by directly constructing the 9-qubit code and the 7-qubit code. Then we introduce the more general formalism of CSS codes, which encompasses both the 9-qubit and 7-qubit codes, before introducing the stabilizer formalism, which tells us how we might construct a CSS code.</p>
<p><strong>3.1. Preliminaries</strong><br/>
First we introduce some tools that we will need in this section.</p>
<p><strong>3.1.1. Pauli Matrices</strong><br/>
The Pauli matrices are a set of 2-by-2 matrices that form an orthogonal basis for the 2-by-2 Hermitian matrices, where a Hermitian matrix <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> satisfies <img alt="H^{\dagger}=H" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cdagger%7D%3DH&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\dagger}=H"/>. Note that we can form larger Hilbert spaces by taking the tensor product of smaller Hilbert spaces, so in particular taking the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-fold tensor product of Pauli matrices gives us a basis for the <img alt="2^k" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^k"/>-by-<img alt="2^k" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^k"/> Hermitian matrices. Note also that generally, in quantum mechanics, we are interested in Hermitian matrices because they can be used to represent measurements, and because unitary matrices, which can be used to represent probability-preserving quantum operations, can be obtained by exponentiating Hermitian matrices (that is, every unitary matrix <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> can be written in the form <img alt="U=e^{iH}" class="latex" src="https://s0.wp.com/latex.php?latex=U%3De%5E%7BiH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U=e^{iH}"/> for <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> a Hermitian matrix).</p>
<p>The Pauli matrices are given by</p>
<p><img alt="\sigma_x\equiv X\equiv\left(\begin{array}{cc}0&amp;1\\1&amp;0\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x%5Cequiv+X%5Cequiv%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D0%261%5C%5C1%260%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x\equiv X\equiv\left(\begin{array}{cc}0&amp;1\\1&amp;0\end{array}\right)"/></p>
<p><img alt="\sigma_y\equiv Y\equiv\left(\begin{array}{cc}0&amp;-i\\i&amp;0\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y%5Cequiv+Y%5Cequiv%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D0%26-i%5C%5Ci%260%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y\equiv Y\equiv\left(\begin{array}{cc}0&amp;-i\\i&amp;0\end{array}\right)"/></p>
<p><img alt="\sigma_z\equiv Z\equiv\left(\begin{array}{cc}1&amp;0\\0&amp;-1\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z%5Cequiv+Z%5Cequiv%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D1%260%5C%5C0%26-1%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z\equiv Z\equiv\left(\begin{array}{cc}1&amp;0\\0&amp;-1\end{array}\right)"/>.</p>
<p>By direction computation we can show that they satisfy the relations</p>
<p><img alt="X^2=Y^2=Z^2=I" class="latex" src="https://s0.wp.com/latex.php?latex=X%5E2%3DY%5E2%3DZ%5E2%3DI&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X^2=Y^2=Z^2=I"/></p>
<p><img alt="ZX=-XZ=iY" class="latex" src="https://s0.wp.com/latex.php?latex=ZX%3D-XZ%3DiY&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="ZX=-XZ=iY"/></p>
<p><img alt="YZ=-ZY=iX" class="latex" src="https://s0.wp.com/latex.php?latex=YZ%3D-ZY%3DiX&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="YZ=-ZY=iX"/></p>
<p><img alt="XY=-YX=iZ" class="latex" src="https://s0.wp.com/latex.php?latex=XY%3D-YX%3DiZ&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="XY=-YX=iZ"/>.</p>
<p><strong>3.1.2. Von Neumann Measurements</strong><br/>
We will also need the concept of a Von Neumann measurement. A Von Neumann measurement is given by a set of projection matrices <img alt="\{E_1, E_2, ..., E_k\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BE_1%2C+E_2%2C+...%2C+E_k%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{E_1, E_2, ..., E_k\}"/> satisfying</p>
<p><img alt="\sum_{i=1}^k E_i=I" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5Ek+E_i%3DI&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{i=1}^k E_i=I"/>.</p>
<p>That is, the projectors partition a Hilbert space <img alt="{\cal H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+H%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{\cal H}"/> into <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> subspaces. Then, given any state <img alt="|\psi\rangle\in{\cal H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle%5Cin%7B%5Ccal+H%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle\in{\cal H}"/>, when we perform a measurement using these projectors we obtain the measurement result corresponding to <img alt="E_i" class="latex" src="https://s0.wp.com/latex.php?latex=E_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_i"/>, with corresponding post-measurement state</p>
<p><img alt="\frac{E_i|\psi\rangle}{||E_i|\psi\rangle||}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BE_i%7C%5Cpsi%5Crangle%7D%7B%7C%7CE_i%7C%5Cpsi%5Crangle%7C%7C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{E_i|\psi\rangle}{||E_i|\psi\rangle||}"/>,</p>
<p>with probability <img alt="\langle\psi|E_i|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle%5Cpsi%7CE_i%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\langle\psi|E_i|\psi\rangle"/>.</p>
<p><strong>3.2. First Attempt at a Quantum Code</strong><br/>
Now we make a first attempt at coming up with a quantum code, noting that our efforts and adjustments will ultimately culminate in the 9-qubit code. Starting with the simplest possible idea, we take inspiration from the classical repetition code, which maps</p>
<p><img alt="0\mapsto 000" class="latex" src="https://s0.wp.com/latex.php?latex=0%5Cmapsto+000&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0\mapsto 000"/></p>
<p><img alt="1\mapsto 111" class="latex" src="https://s0.wp.com/latex.php?latex=1%5Cmapsto+111&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1\mapsto 111"/></p>
<p>and decodes by taking the majority of the 3 bits. We consider the quantum analog of this, which maps</p>
<p><img alt="|0\rangle\mapsto|000\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle%5Cmapsto%7C000%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle\mapsto|000\rangle"/></p>
<p><img alt="|1\rangle\mapsto|111\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle%5Cmapsto%7C111%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle\mapsto|111\rangle"/>.</p>
<p>We will take our quantum errors to be the Pauli matrices <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/>, <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/>, and <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/>. Then the encoding process, where our message is a quantum state <img alt="\alpha|0\rangle+\beta|1\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle%2B%5Cbeta%7C1%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle+\beta|1\rangle"/>, looks like the following:</p>
<p><img alt="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|000\rangle+\beta|111\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle%2B%5Cbeta%7C1%5Crangle%5Cmapsto%5Calpha%7C000%5Crangle%2B%5Cbeta%7C111%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|000\rangle+\beta|111\rangle"/>.</p>
<p>We claim that this code can correct bit errors but not phase errors, which makes it equivalent to the original classical repetition code for error correction. To see this, note that applying an <img alt="X_1" class="latex" src="https://s0.wp.com/latex.php?latex=X_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_1"/> error results in the mapping</p>
<p><img alt="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|100\rangle+\beta|011\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle%2B%5Cbeta%7C1%5Crangle%5Cmapsto%5Calpha%7C100%5Crangle%2B%5Cbeta%7C011%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|100\rangle+\beta|011\rangle"/>.</p>
<p>This can be detected by the von Neumann measurement which projects onto the subspaces</p>
<p><img alt="\{|000\rangle,|111\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C000%5Crangle%2C%7C111%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|000\rangle,|111\rangle\}"/></p>
<p><img alt="\{|011\rangle,|100\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C011%5Crangle%2C%7C100%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|011\rangle,|100\rangle\}"/></p>
<p><img alt="\{|010\rangle,|101\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C010%5Crangle%2C%7C101%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|010\rangle,|101\rangle\}"/></p>
<p><img alt="\{|110\rangle,|001\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C110%5Crangle%2C%7C001%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|110\rangle,|001\rangle\}"/></p>
<p>We could then apply <img alt="\sigma_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x"/> to the first qubit to correct the error. To see that this doesn’t work for phase errors, note that applying a <img alt="Z_2" class="latex" src="https://s0.wp.com/latex.php?latex=Z_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_2"/> error results in the mapping</p>
<p><img alt="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|000\rangle-\beta|111\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle%2B%5Cbeta%7C1%5Crangle%5Cmapsto%5Calpha%7C000%5Crangle-%5Cbeta%7C111%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|000\rangle-\beta|111\rangle"/>.</p>
<p>This is a valid encoding of the state <img alt="\alpha|0\rangle-\beta|1\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle-%5Cbeta%7C1%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle-\beta|1\rangle"/>, so the error is undetectable.</p>
<p>What adjustments can we make so that we’re able to also correct <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> errors? For this we will introduce the Hadamard matrix, defined as</p>
<p><img alt="H=\frac{1}{\sqrt{2}}\left(\begin{array}{cc}1&amp;1\\1&amp;-1\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=H%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D1%261%5C%5C1%26-1%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H=\frac{1}{\sqrt{2}}\left(\begin{array}{cc}1&amp;1\\1&amp;-1\end{array}\right)"/></p>
<p>and satisfying</p>
<p><img alt="HX=ZH" class="latex" src="https://s0.wp.com/latex.php?latex=HX%3DZH&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="HX=ZH"/>.</p>
<p>Note in particular that, because <img alt="HX=ZH" class="latex" src="https://s0.wp.com/latex.php?latex=HX%3DZH&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="HX=ZH"/>, the Hadamard matrix turns bit errors into phase errors, and vice versa. This allows us to come up with a code that corrects phase errors by mapping</p>
<p><img alt="H|0\rangle\mapsto H^{\otimes 3}|000\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=H%7C0%5Crangle%5Cmapsto+H%5E%7B%5Cotimes+3%7D%7C000%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H|0\rangle\mapsto H^{\otimes 3}|000\rangle"/></p>
<p><img alt="H|1\rangle\mapsto H^{\otimes 3}|111\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=H%7C1%5Crangle%5Cmapsto+H%5E%7B%5Cotimes+3%7D%7C111%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H|1\rangle\mapsto H^{\otimes 3}|111\rangle"/></p>
<p>or equivalently,</p>
<p><img alt="|0\rangle\mapsto\frac{1}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle%5Cmapsto%5Cfrac%7B1%7D%7B2%7D%28%7C000%5Crangle%2B%7C011%5Crangle%2B%7C101%5Crangle%2B%7C110%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle\mapsto\frac{1}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)"/></p>
<p><img alt="|1\rangle\mapsto\frac{1}{2}(|111\rangle+|100\rangle+|010\rangle+|001\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle%5Cmapsto%5Cfrac%7B1%7D%7B2%7D%28%7C111%5Crangle%2B%7C100%5Crangle%2B%7C010%5Crangle%2B%7C001%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle\mapsto\frac{1}{2}(|111\rangle+|100\rangle+|010\rangle+|001\rangle)"/></p>
<p>Now we can concatenate our bit flip code with our phase flip code to take care of both errors. This gives us the 9-qubit code, also known as the Shor code.</p>
<p><strong>3.3. 9-Qubit Code</strong><br/>
In the previous section, we went through the process of constructing the 9-qubit Shor code by considering how to correct both bit flip errors and phase flip errors. Explicitly, the 9-qubit Shor code is given by the following mapping:</p>
<p><img alt="|0\rangle\mapsto|0\rangle_L\equiv\frac{1}{2}(|000000000\rangle+|000111111\rangle+|111000111\rangle+|111111000\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle%5Cmapsto%7C0%5Crangle_L%5Cequiv%5Cfrac%7B1%7D%7B2%7D%28%7C000000000%5Crangle%2B%7C000111111%5Crangle%2B%7C111000111%5Crangle%2B%7C111111000%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle\mapsto|0\rangle_L\equiv\frac{1}{2}(|000000000\rangle+|000111111\rangle+|111000111\rangle+|111111000\rangle)"/></p>
<p><img alt="|1\rangle\mapsto|1\rangle_L\equiv\frac{1}{2}(|111111111\rangle+|111000000\rangle+|000111000\rangle+|000000111\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle%5Cmapsto%7C1%5Crangle_L%5Cequiv%5Cfrac%7B1%7D%7B2%7D%28%7C111111111%5Crangle%2B%7C111000000%5Crangle%2B%7C000111000%5Crangle%2B%7C000000111%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle\mapsto|1\rangle_L\equiv\frac{1}{2}(|111111111\rangle+|111000000\rangle+|000111000\rangle+|000000111\rangle)"/>.</p>
<p>Here <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> and <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/> are known as <em>logical qubits</em>; note that our 9-qubit code essentially represents 1 logical qubit with 9 physical qubits.</p>
<p>Note that by construction this code can correct <img alt="\sigma_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x"/>, <img alt="\sigma_y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y"/>, and <img alt="\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z"/> errors on any one qubit (we’ve already shown by construction that it can correct <img alt="\sigma_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x"/> and <img alt="\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z"/> errors, and <img alt="\sigma_y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y"/> can be obtained as a product of the two). This is also equivalent to the statement that the states <img alt="\sigma_x^{(i)}|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x%5E%7B%28i%29%7D%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x^{(i)}|0\rangle_L"/>, <img alt="\sigma_y^{(i)}|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y%5E%7B%28i%29%7D%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y^{(i)}|0\rangle_L"/>, <img alt="\sigma_z^{(i)}|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z%5E%7B%28i%29%7D%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z^{(i)}|0\rangle_L"/>, <img alt="\sigma_x^{(i)}|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x%5E%7B%28i%29%7D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x^{(i)}|1\rangle_L"/>, <img alt="\sigma_y^{(i)}|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y%5E%7B%28i%29%7D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y^{(i)}|1\rangle_L"/>, and <img alt="\sigma_z^{(i)}|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z%5E%7B%28i%29%7D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z^{(i)}|1\rangle_L"/> are all orthogonal.</p>
<p>Now we have a 1-error quantum code. We claim that such a code can in fact correct any error operation, and that this is a property of all 1-error quantum codes:</p>
<p><strong>Theorem: </strong>Given any possible unitary, measurement, or quantum operation on a one-error quantum code, the code can correct it.</p>
<p><strong>Proof: </strong><img alt="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BI%2C+%5Csigma_x%2C+%5Csigma_y%2C+%5Csigma_z%5C%7D%5E%7B%5Cotimes+t%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}"/> forms a basis for the <img alt="2\times 2" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ctimes+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2\times 2"/> matrices. For errors on <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> qubits, the code can correct these errors if it can individually correct errors <img alt="\sigma_{w_i}^{(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_%7Bw_i%7D%5E%7B%28i%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_{w_i}^{(i)}"/> for <img alt="w_i\in\{x,y,z\}" class="latex" src="https://s0.wp.com/latex.php?latex=w_i%5Cin%5C%7Bx%2Cy%2Cz%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_i\in\{x,y,z\}"/>, <img alt="i\in\{1,...,t\}" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cin%5C%7B1%2C...%2Ct%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i\in\{1,...,t\}"/>, since <img alt="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BI%2C+%5Csigma_x%2C+%5Csigma_y%2C+%5Csigma_z%5C%7D%5E%7B%5Cotimes+t%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}"/> forms a basis for <img alt="\mathbb{C}^{2t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D%5E%7B2t%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{C}^{2t}"/>.</p>
<p><strong>Example: Phase Error</strong> Next we’ll do an example where we consider how we might correct an arbitrary phase error applied to the <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> state. Since quantum states are equivalent up to phases, the error operator is given by</p>
<p><img alt="\left(\begin{array}{cc}1&amp;0\\0&amp;e^{i\theta}\end{array}\right)\equiv\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D1%260%5C%5C0%26e%5E%7Bi%5Ctheta%7D%5Cend%7Barray%7D%5Cright%29%5Cequiv%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7De%5E%7B-i%5Ctheta%2F2%7D%260%5C%5C0%26e%5E%7Bi%5Ctheta%2F2%7D%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left(\begin{array}{cc}1&amp;0\\0&amp;e^{i\theta}\end{array}\right)\equiv\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)"/>.</p>
<p>Note that this can be rewritten in the <img alt="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BI%2C+%5Csigma_x%2C+%5Csigma_y%2C+%5Csigma_z%5C%7D%5E%7B%5Cotimes+t%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}"/> basis as</p>
<p><img alt="\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)=\cos\frac{\theta}{2}I-i\sin\frac{\theta}{2}\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7De%5E%7B-i%5Ctheta%2F2%7D%260%5C%5C0%26e%5E%7Bi%5Ctheta%2F2%7D%5Cend%7Barray%7D%5Cright%29%3D%5Ccos%5Cfrac%7B%5Ctheta%7D%7B2%7DI-i%5Csin%5Cfrac%7B%5Ctheta%7D%7B2%7D%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)=\cos\frac{\theta}{2}I-i\sin\frac{\theta}{2}\sigma_z"/>.</p>
<p>Now, applying this error to <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/>, we get</p>
<p><img alt="\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)\frac{1}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)=\frac{1}{2}\cos\frac{\theta}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)-\frac{i}{2}\sin\frac{\theta}{2}(|000\rangle-|011\rangle+|101\rangle-|110\rangle)=\cos\frac{\theta}{2}|0\rangle_L-i\sin\frac{\theta}{2}\sigma_z|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7De%5E%7B-i%5Ctheta%2F2%7D%260%5C%5C0%26e%5E%7Bi%5Ctheta%2F2%7D%5Cend%7Barray%7D%5Cright%29%5Cfrac%7B1%7D%7B2%7D%28%7C000%5Crangle%2B%7C011%5Crangle%2B%7C101%5Crangle%2B%7C110%5Crangle%29%3D%5Cfrac%7B1%7D%7B2%7D%5Ccos%5Cfrac%7B%5Ctheta%7D%7B2%7D%28%7C000%5Crangle%2B%7C011%5Crangle%2B%7C101%5Crangle%2B%7C110%5Crangle%29-%5Cfrac%7Bi%7D%7B2%7D%5Csin%5Cfrac%7B%5Ctheta%7D%7B2%7D%28%7C000%5Crangle-%7C011%5Crangle%2B%7C101%5Crangle-%7C110%5Crangle%29%3D%5Ccos%5Cfrac%7B%5Ctheta%7D%7B2%7D%7C0%5Crangle_L-i%5Csin%5Cfrac%7B%5Ctheta%7D%7B2%7D%5Csigma_z%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)\frac{1}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)=\frac{1}{2}\cos\frac{\theta}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)-\frac{i}{2}\sin\frac{\theta}{2}(|000\rangle-|011\rangle+|101\rangle-|110\rangle)=\cos\frac{\theta}{2}|0\rangle_L-i\sin\frac{\theta}{2}\sigma_z|0\rangle_L"/>.</p>
<p>After performing a projective measurement, we get state <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> with probability <img alt="\cos^2\frac{\theta}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ccos%5E2%5Cfrac%7B%5Ctheta%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\cos^2\frac{\theta}{2}"/>, in which case we do not need to perform any error correction, and we get <img alt="\sigma_z|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z|0\rangle_L"/> with probability <img alt="\sin^2\frac{\theta}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csin%5E2%5Cfrac%7B%5Ctheta%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sin^2\frac{\theta}{2}"/>, in which case we would know to correct the <img alt="\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z"/> error.</p>
<p><strong>3.4. 7-Qubit Code</strong><br/>
Now that we’ve constructed the 9-qubit code and shown that quantum error correction is possible, we might wonder whether it’s possible to do better. For example, we’d like a code that requires fewer qubits. We’ll construct a 7-qubit code that corrects 1 error, defining a mapping to <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> and <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/> by taking inspiration from a classical code, as we did for the 9-qubit case.</p>
<p>For this we will need to go back to the example we used to illustration classical error correction. Recall that in classical error correction, we have an encoding matrix <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> and a parity check matrix <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> satisfying <img alt="GH^T=0" class="latex" src="https://s0.wp.com/latex.php?latex=GH%5ET%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="GH^T=0"/>, with <img alt="\text{rank}(G)+\text{rank}(H)=n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Brank%7D%28G%29%2B%5Ctext%7Brank%7D%28H%29%3Dn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{rank}(G)+\text{rank}(H)=n"/>. We encode a message <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> to obtain codeword <img alt="mG=c" class="latex" src="https://s0.wp.com/latex.php?latex=mG%3Dc&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="mG=c"/>. After error <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/> is applied, this becomes <img alt="c+e" class="latex" src="https://s0.wp.com/latex.php?latex=c%2Be&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c+e"/>, from which we can extract the error syndrome <img alt="(c+e)H^T=eH^T" class="latex" src="https://s0.wp.com/latex.php?latex=%28c%2Be%29H%5ET%3DeH%5ET&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(c+e)H^T=eH^T"/>. We can then apply the appropriate correction to extract <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/> from <img alt="c+e" class="latex" src="https://s0.wp.com/latex.php?latex=c%2Be&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c+e"/>.</p>
<p>Now we will use the encoding matrix from our classical error correction example, and we will divide our codewords into two sets, <img alt="C_1" class="latex" src="https://s0.wp.com/latex.php?latex=C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1"/> and <img alt="C_1'" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1'"/>, given by</p>
<p><img alt="C_1=\left\{\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\end{array}\right." class="latex" src="https://s0.wp.com/latex.php?latex=C_1%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bccccccc%7D0%260%260%261%261%261%261%5C%5C1%260%261%260%261%260%261%5C%5C0%261%261%260%260%261%261%5Cend%7Barray%7D%5Cright.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1=\left\{\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\end{array}\right."/></p>
<p>and</p>
<p><img alt="C_1'=C_1+1111111" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%27%3DC_1%2B1111111&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1'=C_1+1111111"/>.</p>
<p>Similar to how we approached the 9-qubit case, we will start by defining our code as follows:</p>
<p><img alt="|0\rangle_L\equiv\frac{1}{\sqrt{8}}\sum_{v\in C_1}|v\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L%5Cequiv%5Cfrac%7B1%7D%7B%5Csqrt%7B8%7D%7D%5Csum_%7Bv%5Cin+C_1%7D%7Cv%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L\equiv\frac{1}{\sqrt{8}}\sum_{v\in C_1}|v\rangle"/></p>
<p><img alt="|1\rangle_L\equiv\frac{1}{\sqrt{8}}\sum_{w\in C_1'}|w\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L%5Cequiv%5Cfrac%7B1%7D%7B%5Csqrt%7B8%7D%7D%5Csum_%7Bw%5Cin+C_1%27%7D%7Cw%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L\equiv\frac{1}{\sqrt{8}}\sum_{w\in C_1'}|w\rangle"/>.</p>
<p>Note that this corrects bit flip errors by construction. How can we ensure that we are also able to correct phase errors? For this we again turn to the Hadamard matrix, which allows us to toggle between bit and phase errors. We claim that</p>
<p><img alt="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L+|1\rangle_L)" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7C0%5Crangle_L%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C0%5Crangle_L%2B%7C1%5Crangle_L%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L+|1\rangle_L)"/></p>
<p><img alt="H^{\otimes 7}|1\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L-|1\rangle_L)" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7C1%5Crangle_L%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C0%5Crangle_L-%7C1%5Crangle_L%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|1\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L-|1\rangle_L)"/>.</p>
<p><strong>Proof: </strong>We will show that</p>
<p><img alt="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L+|1\rangle_L)" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7C0%5Crangle_L%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C0%5Crangle_L%2B%7C1%5Crangle_L%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L+|1\rangle_L)"/>,</p>
<p>noting that the argument for <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/> is similar. First we will need the fact that</p>
<p><img alt="H^{\otimes 7}|v\rangle=\frac{1}{2^{7/2}}\sum_{w\in\{0,1\}^7}(-1)^{w\cdot v}|w\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7Cv%5Crangle%3D%5Cfrac%7B1%7D%7B2%5E%7B7%2F2%7D%7D%5Csum_%7Bw%5Cin%5C%7B0%2C1%5C%7D%5E7%7D%28-1%29%5E%7Bw%5Ccdot+v%7D%7Cw%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|v\rangle=\frac{1}{2^{7/2}}\sum_{w\in\{0,1\}^7}(-1)^{w\cdot v}|w\rangle"/>.</p>
<p>To see that this fact is true, note that</p>
<p><img alt="H=\frac{1}{\sqrt{2}}(|0\rangle\langle 0|+|0\rangle\langle 1|+|1\rangle\langle 0|-|1\rangle\langle 1|)" class="latex" src="https://s0.wp.com/latex.php?latex=H%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C0%5Crangle%5Clangle+0%7C%2B%7C0%5Crangle%5Clangle+1%7C%2B%7C1%5Crangle%5Clangle+0%7C-%7C1%5Crangle%5Clangle+1%7C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H=\frac{1}{\sqrt{2}}(|0\rangle\langle 0|+|0\rangle\langle 1|+|1\rangle\langle 0|-|1\rangle\langle 1|)"/></p>
<p>and that <img alt="w\cdot v" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Ccdot+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\cdot v"/> is equal to the number of bits in which <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w"/> and <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> are both 1. Now we can start by directly calculating</p>
<p><img alt="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{8}}\frac{1}{\sqrt{128}}\sum_{v\in C_1}\sum_{w\in\{0,1\}^7}(-1)^{v\cdot w}|w\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7C0%5Crangle_L%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B8%7D%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B128%7D%7D%5Csum_%7Bv%5Cin+C_1%7D%5Csum_%7Bw%5Cin%5C%7B0%2C1%5C%7D%5E7%7D%28-1%29%5E%7Bv%5Ccdot+w%7D%7Cw%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{8}}\frac{1}{\sqrt{128}}\sum_{v\in C_1}\sum_{w\in\{0,1\}^7}(-1)^{v\cdot w}|w\rangle"/>.</p>
<p>Note that for <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> two codewords, assuming that <img alt="w\cdot y=1" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Ccdot+y%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\cdot y=1"/>, we must have that <img alt="x\cdot w=0" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Ccdot+w%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x\cdot w=0"/> iff <img alt="(x+y)\cdot w=1" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2By%29%5Ccdot+w%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x+y)\cdot w=1"/>. Thus we can break the codespace up into an equal number of codewords <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> satisfying <img alt="x\cdot w=0" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Ccdot+w%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x\cdot w=0"/> and <img alt="x\cdot w=1" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Ccdot+w%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x\cdot w=1"/>. This means that we must have that the sum <img alt="\sum_{v\in C_1}\sum_{w\in\{0,1\}^7}(-1)^{w\cdot v}|w\rangle=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bv%5Cin+C_1%7D%5Csum_%7Bw%5Cin%5C%7B0%2C1%5C%7D%5E7%7D%28-1%29%5E%7Bw%5Ccdot+v%7D%7Cw%5Crangle%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{v\in C_1}\sum_{w\in\{0,1\}^7}(-1)^{w\cdot v}|w\rangle=0"/> unless we have <img alt="w\perp C_1" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Cperp+C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\perp C_1"/>. But those <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w"/> that satisfy <img alt="w\perp C_1" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Cperp+C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\perp C_1"/> are exactly all the codewords by definition, so we must have that</p>
<p><img alt="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}|0\rangle_L+\frac{1}{\sqrt{2}}|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7C0%5Crangle_L%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%7C0%5Crangle_L%2B%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}|0\rangle_L+\frac{1}{\sqrt{2}}|1\rangle_L"/></p>
<p>as the sum in <img alt="|0\rangle_L+|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L%2B%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L+|1\rangle_L"/> runs equally over all codewords.</p>
<p>Thus we have constructed a 7-qubit quantum code that corrects 1 error, and moreover we see that for both the 9-qubit and 7-qubit codes, both of which are 1-error quantum codes, the fact that they can correct 1-error comes directly from the fact that the original classical codes we used to construct them can themselves correct 1 error. This suggests that we should be able to come up with a more general procedure for constructing quantum codes from classical codes.</p>
<p><strong>3.5. CSS Codes</strong><br/>
<em>CSS (Calderbank-Shor-Steane) codes</em> generalize the process by which we constructed the 9-qubit and 7-qubit codes, and they give us a general framework for constructing quantum codes from classical codes. In a CSS code, we require groups <img alt="C_1" class="latex" src="https://s0.wp.com/latex.php?latex=C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1"/>, <img alt="C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2"/> satisfying</p>
<p><img alt="&#xA0;C_1\subseteq C_2" class="latex" src="https://s0.wp.com/latex.php?latex=%C2%A0C_1%5Csubseteq+C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="&#xA0;C_1\subseteq C_2"/></p>
<p><img alt="C_2^{\perp}\subseteq C_1^{\perp}" class="latex" src="https://s0.wp.com/latex.php?latex=C_2%5E%7B%5Cperp%7D%5Csubseteq+C_1%5E%7B%5Cperp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2^{\perp}\subseteq C_1^{\perp}"/></p>
<p>Then we can define codewords to correspond to cosets of <img alt="C_1" class="latex" src="https://s0.wp.com/latex.php?latex=C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1"/> in <img alt="C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2"/>, so that the number of codewords is equal to <img alt="2^{\text{dim}(C_2)-\text{dim}(C_1)}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Ctext%7Bdim%7D%28C_2%29-%5Ctext%7Bdim%7D%28C_1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{\text{dim}(C_2)-\text{dim}(C_1)}"/>. Thus by this definition we can say that codewords <img alt="w_1, w_2\in C_2" class="latex" src="https://s0.wp.com/latex.php?latex=w_1%2C+w_2%5Cin+C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_1, w_2\in C_2"/> are in the same coset if <img alt="w_1-w_2\in C_1" class="latex" src="https://s0.wp.com/latex.php?latex=w_1-w_2%5Cin+C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_1-w_2\in C_1"/>. Explicitly, the codeword for coset <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w"/> is given by the state</p>
<p><img alt="\frac{1}{|C_1|^{1/2}}\sum_{x\in C_1}|x+w\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%7CC_1%7C%5E%7B1%2F2%7D%7D%5Csum_%7Bx%5Cin+C_1%7D%7Cx%2Bw%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{|C_1|^{1/2}}\sum_{x\in C_1}|x+w\rangle"/>,</p>
<p>and under the Hadamard transformation applied to each qubit this state is in turn mapped to the state</p>
<p><img alt="\frac{1}{|C_1^{\perp}|^{1/2}}\sum_{x\in C_1^{\perp}}|x+w\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%7CC_1%5E%7B%5Cperp%7D%7C%5E%7B1%2F2%7D%7D%5Csum_%7Bx%5Cin+C_1%5E%7B%5Cperp%7D%7D%7Cx%2Bw%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{|C_1^{\perp}|^{1/2}}\sum_{x\in C_1^{\perp}}|x+w\rangle"/>.</p>
<p>That is to say, the Hadamard “dualizes” our original code, toggling bit errors to phase errors and vice versa. (This can be seen by direct calculation, as in the case of the 7-qubit code, where we used the fact that <img alt="\sum_{v\in C_1}(-1)^{v\cdot w}=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bv%5Cin+C_1%7D%28-1%29%5E%7Bv%5Ccdot+w%7D%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{v\in C_1}(-1)^{v\cdot w}=0"/> for <img alt="w\not\in C_1^{\perp}" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Cnot%5Cin+C_1%5E%7B%5Cperp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\not\in C_1^{\perp}"/>.)</p>
<p>Note also that this code can correct a number of bit errors equal to the minimum weight of <img alt="\{v\in C_2-C_1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bv%5Cin+C_2-C_1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{v\in C_2-C_1\}"/>.</p>
<p>With the CSS construction we have thus reduced the problem of finding a quantum error correcting code to the problem of finding appropriate <img alt="C_1" class="latex" src="https://s0.wp.com/latex.php?latex=C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1"/>, <img alt="C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2"/>. Note that the special case of <img alt="C_2^{\perp}=C_1=C" class="latex" src="https://s0.wp.com/latex.php?latex=C_2%5E%7B%5Cperp%7D%3DC_1%3DC&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2^{\perp}=C_1=C"/> corresponds to weakly self-dual codes, which are well studied classically. Doubly even, weakly self-dual codes additionally have the requirement that all codewords have Hamming weights that are multiples of 4; they satisfy the requirement</p>
<p><img alt="1^n\subseteq C^{\perp}\subseteq C\subseteq\mathbb{Z}_2^n" class="latex" src="https://s0.wp.com/latex.php?latex=1%5En%5Csubseteq+C%5E%7B%5Cperp%7D%5Csubseteq+C%5Csubseteq%5Cmathbb%7BZ%7D_2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1^n\subseteq C^{\perp}\subseteq C\subseteq\mathbb{Z}_2^n"/></p>
<p>and are also well studied classically.</p>
<p><strong>3.6. Gilbert-Varshamov Bound</strong><br/>
In the previous section we introduced CSS codes and demonstrated that the problem of constructing a quantum code could be reduced to the problem of finding two groups <img alt="C_1" class="latex" src="https://s0.wp.com/latex.php?latex=C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1"/>, <img alt="C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2"/> satisfying</p>
<p><img alt="C_1\subseteq C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%5Csubseteq+C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1\subseteq C_2"/></p>
<p><img alt="C_2^{\perp}\subseteq C_1^{\perp}" class="latex" src="https://s0.wp.com/latex.php?latex=C_2%5E%7B%5Cperp%7D%5Csubseteq+C_1%5E%7B%5Cperp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2^{\perp}\subseteq C_1^{\perp}"/>.</p>
<p>The next natural question is to ask whether such groups can in fact be found.</p>
<p>The Gilbert-Varshamov bound answers this question in the affirmative, ensuring that there do exist good CSS codes (the bound applies to both quantum and classical codes). It can be stated in the following way:</p>
<p><strong>Theorem </strong>(<em>Gilbert-Varshamov Bound</em>): There exist CSS codes with rate <img alt="R=" class="latex" src="https://s0.wp.com/latex.php?latex=R%3D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R="/>(number of encoded bits)/(length of code) given by</p>
<p><img alt="R\geq 1-2H_2(d/n)" class="latex" src="https://s0.wp.com/latex.php?latex=R%5Cgeq+1-2H_2%28d%2Fn%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R\geq 1-2H_2(d/n)"/>,</p>
<p>where <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> is the minimum distance of the code, <img alt="d/2" class="latex" src="https://s0.wp.com/latex.php?latex=d%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d/2"/> is the number of errors it can correct, and <img alt="H_2(x)" class="latex" src="https://s0.wp.com/latex.php?latex=H_2%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_2(x)"/> is the Shannon entropy, defined as</p>
<p><img alt="H_2(x)=-x\log_2x-(1-x)\log_2(1-x)" class="latex" src="https://s0.wp.com/latex.php?latex=H_2%28x%29%3D-x%5Clog_2x-%281-x%29%5Clog_2%281-x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_2(x)=-x\log_2x-(1-x)\log_2(1-x)"/>.</p>
<p><strong>Proof:</strong> Note that we can always take a code, apply a random linear transformation to it, and get another code. Thus each vector is equally likely to appear in a random code. Given this fact, we can estimate the probability that a code of dimension <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> contains a word of weight <img alt="\leq d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\leq d"/> using the union bound:</p>
<p><img alt="P(" class="latex" src="https://s0.wp.com/latex.php?latex=P%28&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P("/>code of dimension <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> has word of weight <img alt="\leq d)\leq(" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+d%29%5Cleq%28&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\leq d)\leq("/>number of words<img alt=")\times P(" class="latex" src="https://s0.wp.com/latex.php?latex=%29%5Ctimes+P%28&amp;bg=ffffff&amp;fg=333333&amp;s=0" title=")\times P("/>word has weight <img alt="\leq d)=2^k\cdot\frac{\sum_{i=0}^d \binom{n}{i}}{2^n}\approx \frac{2^k\cdot 2^{nH(d/n)}}{2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+d%29%3D2%5Ek%5Ccdot%5Cfrac%7B%5Csum_%7Bi%3D0%7D%5Ed+%5Cbinom%7Bn%7D%7Bi%7D%7D%7B2%5En%7D%5Capprox+%5Cfrac%7B2%5Ek%5Ccdot+2%5E%7BnH%28d%2Fn%29%7D%7D%7B2%5En%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\leq d)=2^k\cdot\frac{\sum_{i=0}^d \binom{n}{i}}{2^n}\approx \frac{2^k\cdot 2^{nH(d/n)}}{2^n}"/></p>
<p>For this to be a valid probability we need to have</p>
<p><img alt="(k/n)+H(d/n)&lt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%28k%2Fn%29%2BH%28d%2Fn%29%3C+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(k/n)+H(d/n)&lt; 1"/>.</p>
<p>We can calculate rate by noting that for a CSS code, given by <img alt="C_1\subseteq C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%5Csubseteq+C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1\subseteq C_2"/>, <img alt="C_2^{\perp}\subseteq C_1^{\perp}" class="latex" src="https://s0.wp.com/latex.php?latex=C_2%5E%7B%5Cperp%7D%5Csubseteq+C_1%5E%7B%5Cperp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2^{\perp}\subseteq C_1^{\perp}"/>, with <img alt="\text{dim}(C_1)=n-k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bdim%7D%28C_1%29%3Dn-k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{dim}(C_1)=n-k"/>, <img alt="\text{dim}(C_2)=k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bdim%7D%28C_2%29%3Dk&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{dim}(C_2)=k"/>, the expression for rate is given by</p>
<p><img alt="R=\frac{n-2k}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=R%3D%5Cfrac%7Bn-2k%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R=\frac{n-2k}{n}"/>.</p>
<p>Combining this with the bound we obtained by considering probabilities, we get that</p>
<p><img alt="R\geq 1-2H(d/n)" class="latex" src="https://s0.wp.com/latex.php?latex=R%5Cgeq+1-2H%28d%2Fn%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R\geq 1-2H(d/n)"/>.</p>
<p>Thus there exist good CSS codes.</p>
<p><strong>3.7. Stabilizer Codes</strong><br/>
Having discussed and constructed some examples of CSS codes, we will now discuss the stabilizer formalism. Note that this formalism allows us to construct codes without having to work directly with the states representing <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> and <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/>, as this can quickly get unwieldy. Instead, we will work with stabilizers, operators that leave these states invariant.</p>
<p>To see how working directly with states can get unwieldy, we can consider the 5-qubit code. We can define it the way we defined the 9-qubit and 7-qubit codes, by directly defining the basis vectors <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> and <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/>,</p>
<p><img alt="|0\rangle_L\equiv\frac{1}{4}(|00000\rangle-|01100\rangle+|00101\rangle+|01010\rangle-|01111\rangle+(" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L%5Cequiv%5Cfrac%7B1%7D%7B4%7D%28%7C00000%5Crangle-%7C01100%5Crangle%2B%7C00101%5Crangle%2B%7C01010%5Crangle-%7C01111%5Crangle%2B%28&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L\equiv\frac{1}{4}(|00000\rangle-|01100\rangle+|00101\rangle+|01010\rangle-|01111\rangle+("/>symmetric under cyclic permutations<img alt="))" class="latex" src="https://s0.wp.com/latex.php?latex=%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="))"/>,</p>
<p>with <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/> defined similarly. But we can also define this code more succinctly using the stabilizer formalism. To do so, we start by choosing a commutative subgroup of the Pauli group, with generators <img alt="g_i" class="latex" src="https://s0.wp.com/latex.php?latex=g_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_i"/> satisfying</p>
<p><img alt="g_i^2=I" class="latex" src="https://s0.wp.com/latex.php?latex=g_i%5E2%3DI&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_i^2=I"/></p>
<p><img alt="g_ig_j=g_jg_i" class="latex" src="https://s0.wp.com/latex.php?latex=g_ig_j%3Dg_jg_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_ig_j=g_jg_i"/></p>
<p>For example, for the 5-qubit code, the particular choice of generators we would need is given by</p>
<p><img alt="g_1\equiv IXZZX" class="latex" src="https://s0.wp.com/latex.php?latex=g_1%5Cequiv+IXZZX&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_1\equiv IXZZX"/></p>
<p><img alt="g_2\equiv XIXZZ" class="latex" src="https://s0.wp.com/latex.php?latex=g_2%5Cequiv+XIXZZ&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_2\equiv XIXZZ"/></p>
<p><img alt="g_3\equiv ZXIXZ" class="latex" src="https://s0.wp.com/latex.php?latex=g_3%5Cequiv+ZXIXZ&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_3\equiv ZXIXZ"/></p>
<p><img alt="g_4\equiv ZZXIX" class="latex" src="https://s0.wp.com/latex.php?latex=g_4%5Cequiv+ZZXIX&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_4\equiv ZZXIX"/>.</p>
<p>Now we consider states <img alt="\{|\psi\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|\psi\rangle\}"/> that are stabilized by the <img alt="\{g_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bg_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{g_i\}"/>. That is, they satisfy</p>
<p><img alt="g_i|\psi\rangle=|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=g_i%7C%5Cpsi%5Crangle%3D%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_i|\psi\rangle=|\psi\rangle"/>.</p>
<p>Note that the eigenvalues of <img alt="\sigma_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x"/>, <img alt="\sigma_y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y"/>, and <img alt="\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z"/> are <img alt="\pm 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpm+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\pm 1"/>, so in the case of the 5-qubit code, there exists a <img alt="2^5/2=16" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E5%2F2%3D16&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^5/2=16"/>-dimensional space of <img alt="\{|\psi\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|\psi\rangle\}"/> satisfying <img alt="g_1|\psi\rangle=|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=g_1%7C%5Cpsi%5Crangle%3D%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_1|\psi\rangle=|\psi\rangle"/>. Recalling that two commuting matrices are simultaneously diagonalizable, there exists a <img alt="16/2=8" class="latex" src="https://s0.wp.com/latex.php?latex=16%2F2%3D8&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="16/2=8"/>-dimensional space of <img alt="\{|\psi\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|\psi\rangle\}"/> satisfying <img alt="g_1|\psi\rangle=g_2|\psi\rangle=|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=g_1%7C%5Cpsi%5Crangle%3Dg_2%7C%5Cpsi%5Crangle%3D%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_1|\psi\rangle=g_2|\psi\rangle=|\psi\rangle"/>, and so on, where we cut the dimension of the subspace in half each time we add a generator. Finally, there exists a <img alt="2^5/2^4=2" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E5%2F2%5E4%3D2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^5/2^4=2"/>-dimensional space of <img alt="\{|\psi\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|\psi\rangle\}"/> satisfying <img alt="g_i|\psi\rangle=|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=g_i%7C%5Cpsi%5Crangle%3D%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_i|\psi\rangle=|\psi\rangle"/> for all <img alt="i=1,...,4" class="latex" src="https://s0.wp.com/latex.php?latex=i%3D1%2C...%2C4&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i=1,...,4"/>. This 2-dimensional space is exactly the subspace spanned by <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> and <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/>. Thus fixing the stabilizers is enough to give us our code.</p>
<p>Next we will consider all elements in the Pauli group that commute with all elements in our stabilizer group <img alt="G=\{g_1,...,g_4\}" class="latex" src="https://s0.wp.com/latex.php?latex=G%3D%5C%7Bg_1%2C...%2Cg_4%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G=\{g_1,...,g_4\}"/>. As we shall see, this will give us our logical operators, where a <em>logical operator</em> performs an operation on a logical qubit (for example, the logical <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/> operator, <img alt="X_L" class="latex" src="https://s0.wp.com/latex.php?latex=X_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_L"/>, would act on the logical qubit <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> by mapping <img alt="X_L|0\rangle_L=|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=X_L%7C0%5Crangle_L%3D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_L|0\rangle_L=|1\rangle_L"/>, and so on). In the 5-qubit case we end up with a 6-dimensional nonabelian group <img alt="\tilde{G}=\langle g_1,...,g_4, h_1, h_2\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BG%7D%3D%5Clangle+g_1%2C...%2Cg_4%2C+h_1%2C+h_2%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tilde{G}=\langle g_1,...,g_4, h_1, h_2\rangle"/> by adding the following two elements to those elements that are in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>:</p>
<p><img alt="h_1=XXXXX" class="latex" src="https://s0.wp.com/latex.php?latex=h_1%3DXXXXX&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_1=XXXXX"/></p>
<p><img alt="h_2=ZZZZZ" class="latex" src="https://s0.wp.com/latex.php?latex=h_2%3DZZZZZ&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_2=ZZZZZ"/></p>
<p>These will be our logical operators</p>
<p><img alt="X_L\equiv h_1" class="latex" src="https://s0.wp.com/latex.php?latex=X_L%5Cequiv+h_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_L\equiv h_1"/></p>
<p><img alt="Z_L\equiv h_2" class="latex" src="https://s0.wp.com/latex.php?latex=Z_L%5Cequiv+h_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_L\equiv h_2"/></p>
<p>so that</p>
<p><img alt="X_L|0\rangle_L=|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=X_L%7C0%5Crangle_L%3D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_L|0\rangle_L=|1\rangle_L"/></p>
<p><img alt="X_L|1\rangle_L=|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=X_L%7C1%5Crangle_L%3D%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_L|1\rangle_L=|0\rangle_L"/></p>
<p><img alt="Z_L|1\rangle_L=-|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=Z_L%7C1%5Crangle_L%3D-%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_L|1\rangle_L=-|1\rangle_L"/></p>
<p><img alt="Z_L|0\rangle_L=|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=Z_L%7C0%5Crangle_L%3D%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_L|0\rangle_L=|0\rangle_L"/>.</p>
<p>Note that this code has distance 3 and corrects 1 error because 3 is the minimum Hamming weight in the group <img alt="\tilde{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BG%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tilde{G}"/>. (To see this, note that <img alt="XXXXX\cdot IXZZX=XIYYI" class="latex" src="https://s0.wp.com/latex.php?latex=XXXXX%5Ccdot+IXZZX%3DXIYYI&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="XXXXX\cdot IXZZX=XIYYI"/> has Hamming weight 3.)</p>
<p>Why is Hamming weight 2 not enough to correct one error? If we had, for example, <img alt="XZIII\in\tilde{G}" class="latex" src="https://s0.wp.com/latex.php?latex=XZIII%5Cin%5Ctilde%7BG%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="XZIII\in\tilde{G}"/>, then we would have</p>
<p><img alt="\sigma_x^{(1)}|\psi_1\rangle=\sigma_z^{(2)}|\psi_2\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x%5E%7B%281%29%7D%7C%5Cpsi_1%5Crangle%3D%5Csigma_z%5E%7B%282%29%7D%7C%5Cpsi_2%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x^{(1)}|\psi_1\rangle=\sigma_z^{(2)}|\psi_2\rangle"/></p>
<p>for <img alt="|\psi_1\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi_1%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi_1\rangle"/>, <img alt="|\psi_2\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi_2%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi_2\rangle"/> both in the code, which means that we wouldn’t be able to distinguish an <img alt="X_1" class="latex" src="https://s0.wp.com/latex.php?latex=X_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_1"/> error from a <img alt="Z_2" class="latex" src="https://s0.wp.com/latex.php?latex=Z_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_2"/> error.</p>
<p>Note that, in general, when <img alt="x\in\tilde{G}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin%5Ctilde%7BG%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x\in\tilde{G}"/>, <img alt="x|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=x%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x|\psi\rangle"/> will be in the code, so elements of <img alt="\tilde{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BG%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tilde{G}"/> map codewords to codewords. We can prove this fact by noting that</p>
<p><img alt="xg_i|\psi\rangle=x|\psi\rangle=g_ix|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=xg_i%7C%5Cpsi%5Crangle%3Dx%7C%5Cpsi%5Crangle%3Dg_ix%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="xg_i|\psi\rangle=x|\psi\rangle=g_ix|\psi\rangle"/>.</p>
<p>Note also that in the examples we’ve been dealing with so far, where we have a commuting subgroup of the Pauli group, our codes correspond to classical, additive, weakly self-dual codes over <img alt="GF(4)" class="latex" src="https://s0.wp.com/latex.php?latex=GF%284%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="GF(4)"/>. Here <img alt="GF(4)=\{0,1,\omega,\bar{\omega}\}" class="latex" src="https://s0.wp.com/latex.php?latex=GF%284%29%3D%5C%7B0%2C1%2C%5Comega%2C%5Cbar%7B%5Comega%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="GF(4)=\{0,1,\omega,\bar{\omega}\}"/> (with group elements <img alt="\{\omega, \bar{\omega},1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Comega%2C+%5Cbar%7B%5Comega%7D%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\omega, \bar{\omega},1\}"/> corresponding to the third roots of unity) is the finite field on 4 elements, and multiplying Pauli matrices corresponds to group addition. Specifically,</p>
<p><img alt="X\equiv 1" class="latex" src="https://s0.wp.com/latex.php?latex=X%5Cequiv+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X\equiv 1"/></p>
<p><img alt="Y\equiv \omega" class="latex" src="https://s0.wp.com/latex.php?latex=Y%5Cequiv+%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y\equiv \omega"/></p>
<p><img alt="Z\equiv \bar{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=Z%5Cequiv+%5Cbar%7B%5Comega%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z\equiv \bar{\omega}"/></p>
<p><img alt="I\equiv 0" class="latex" src="https://s0.wp.com/latex.php?latex=I%5Cequiv+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="I\equiv 0"/></p>
<p>satisfying</p>
<p><img alt="H\omega=\bar{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=H%5Comega%3D%5Cbar%7B%5Comega%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H\omega=\bar{\omega}"/></p>
<p><img alt="2X=2Y=2Z=0" class="latex" src="https://s0.wp.com/latex.php?latex=2X%3D2Y%3D2Z%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2X=2Y=2Z=0"/>.</p>
<p>We have now concluded our discussion of quantum error-correcting codes. In the next section we will shift gears and look at quantum channels and channel capacities.</p>
<p><strong>4. Quantum Channels</strong><br/>
In this final section we will look at quantum channels and channel capacities.</p>
<p><strong>4.1. Definition and Examples</strong></p>
<p><strong>4.1.1. Definition</strong><br/>
We know that we want to define a quantum channel to take a quantum state as input. What should the output be? As a first attempt we might imagine having the output be a probability distribution <img alt="\{p_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bp_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{p_i\}"/> over states <img alt="\{|\psi_i\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi_i%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|\psi_i\rangle\}"/>. It turns out that for a more succinct description, we can have both the input and output be a density matrix.</p>
<p>Recall that a density matrix takes the form</p>
<p><img alt="\rho=\sum_i p_i|\psi_i\rangle\langle\psi_i|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%3D%5Csum_i+p_i%7C%5Cpsi_i%5Crangle%5Clangle%5Cpsi_i%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho=\sum_i p_i|\psi_i\rangle\langle\psi_i|"/></p>
<p>representing a probability distribution over pure states <img alt="|\psi_i\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi_i%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi_i\rangle"/>. <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> must also be Hermitian, and it must satisfy <img alt="\text{Tr}(\rho)=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BTr%7D%28%5Crho%29%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{Tr}(\rho)=1"/> (equivalently, we must have <img alt="\sum_i p_i=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_i+p_i%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_i p_i=1"/>).</p>
<p>Now we may define a quantum channel as the map <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta"/> that takes</p>
<p><img alt="\eta:\rho\mapsto\sum_i E_i\rho E_i^{\dagger}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3A%5Crho%5Cmapsto%5Csum_i+E_i%5Crho+E_i%5E%7B%5Cdagger%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta:\rho\mapsto\sum_i E_i\rho E_i^{\dagger}"/>,</p>
<p>where</p>
<p><img alt="\sum_i E_i^{\dagger}E_i=I" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_i+E_i%5E%7B%5Cdagger%7DE_i%3DI&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_i E_i^{\dagger}E_i=I"/>.</p>
<p>To see that the output is in fact a density matrix, note that the output expression is clearly Hermitian and can be shown to have unit trace using the cyclical property of traces. Note also that the decomposition into <img alt="\{E_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BE_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{E_i\}"/> need not be unique.</p>
<p><strong>4.1.2. Example Quantum Channels</strong><br/>
Next we give a few examples of quantum channels. The dephasing channel is given by the map</p>
<p><img alt="\rho\mapsto(1-p)\rho+p\sigma_z\rho\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%5Cmapsto%281-p%29%5Crho%2Bp%5Csigma_z%5Crho%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho\mapsto(1-p)\rho+p\sigma_z\rho\sigma_z"/>.</p>
<p>It maps</p>
<p><img alt="\left(\begin{array}{cc}\alpha&amp;\beta\\\gamma&amp;\delta\end{array}\right)\mapsto \left(\begin{array}{cc}\alpha&amp;(1-2p)\beta\\(1-2p)\gamma&amp;\delta\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%5Calpha%26%5Cbeta%5C%5C%5Cgamma%26%5Cdelta%5Cend%7Barray%7D%5Cright%29%5Cmapsto+%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%5Calpha%26%281-2p%29%5Cbeta%5C%5C%281-2p%29%5Cgamma%26%5Cdelta%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left(\begin{array}{cc}\alpha&amp;\beta\\\gamma&amp;\delta\end{array}\right)\mapsto \left(\begin{array}{cc}\alpha&amp;(1-2p)\beta\\(1-2p)\gamma&amp;\delta\end{array}\right)"/></p>
<p><img alt="\left(\begin{array}{cc}\alpha&amp;\beta\\\gamma&amp;\delta\end{array}\right)\mapsto \left(\begin{array}{cc}\alpha&amp;(1-2p)\beta\\(1-2p)\gamma&amp;\delta\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%5Calpha%26%5Cbeta%5C%5C%5Cgamma%26%5Cdelta%5Cend%7Barray%7D%5Cright%29%5Cmapsto+%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%5Calpha%26%281-2p%29%5Cbeta%5C%5C%281-2p%29%5Cgamma%26%5Cdelta%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left(\begin{array}{cc}\alpha&amp;\beta\\\gamma&amp;\delta\end{array}\right)\mapsto \left(\begin{array}{cc}\alpha&amp;(1-2p)\beta\\(1-2p)\gamma&amp;\delta\end{array}\right)"/>,</p>
<p>so it multiplies off-diagonal elements by a factor that is less than 1. Note that when <img alt="p=1/2" class="latex" src="https://s0.wp.com/latex.php?latex=p%3D1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p=1/2"/>, it maps</p>
<p><img alt="\alpha|0\rangle+\beta|1\rangle\mapsto|\alpha|^2|0\rangle\langle 0|+|\beta|^2|1\rangle\langle 1|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle%2B%5Cbeta%7C1%5Crangle%5Cmapsto%7C%5Calpha%7C%5E2%7C0%5Crangle%5Clangle+0%7C%2B%7C%5Cbeta%7C%5E2%7C1%5Crangle%5Clangle+1%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle+\beta|1\rangle\mapsto|\alpha|^2|0\rangle\langle 0|+|\beta|^2|1\rangle\langle 1|"/>,</p>
<p>which means that it turns superpositions into classical mixtures (hence the name “dephasing”).</p>
<p>Another example is the amplitude damping channel, which models an excited state decaying to a ground state. It is given by</p>
<p><img alt="E_1=\left(\begin{array}{cc}0&amp;\sqrt{p}\\0&amp;0\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=E_1%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D0%26%5Csqrt%7Bp%7D%5C%5C0%260%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_1=\left(\begin{array}{cc}0&amp;\sqrt{p}\\0&amp;0\end{array}\right)"/></p>
<p><img alt="E_2=\left(\begin{array}{cc}1&amp;0\\0&amp;\sqrt{1-p}\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=E_2%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D1%260%5C%5C0%26%5Csqrt%7B1-p%7D%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_2=\left(\begin{array}{cc}1&amp;0\\0&amp;\sqrt{1-p}\end{array}\right)"/></p>
<p>Here we let the vector <img alt="|0\rangle=(1, 0)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle%3D%281%2C+0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle=(1, 0)"/> denote the ground state, and we let the vector <img alt="|1\rangle=(0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle%3D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle=(0, 1)"/> denote the excited state. Thus we can see that the channel maps the ground state to itself, <img alt="|0\rangle\mapsto|0\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle%5Cmapsto%7C0%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle\mapsto|0\rangle"/>, while the excited state <img alt="|1\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle"/> gets mapped to <img alt="|0\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle"/> with probability <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> and stays at <img alt="|1\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle"/> with probability <img alt="1-p" class="latex" src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1-p"/>.</p>
<p><strong>4.2 Quantum Channel Capacities</strong><br/>
Now we consider the capacity of quantum channels, where the capacity quantifies how much information can make it through the channel. We consider classical channels, classical information sent over quantum channels, and quantum information sent over quantum channels. First we start off with the example of the quantum erasure channel to demonstrate that quantum channels behave differently from classical channels, and then we give the actual expressions for the channel capacities before revisiting the example of the quantum erasure channel.</p>
<p><strong>4.2.1 Example: Quantum Erasure Channel</strong><br/>
First we start with the example of the quantum erasure channel, which given a state <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/> replaces it by an orthogonal state <img alt="|E\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7CE%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|E\rangle"/> with probability <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> and returns the same state <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/> with probability <img alt="1-p" class="latex" src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1-p"/>. We claim that the erasure channel can’t transmit quantum information when <img alt="p\geq 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cgeq+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p\geq 0.5"/>, behavior that is markedly different from that of classical information. That is to say, for <img alt="p\geq 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cgeq+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p\geq 0.5"/>, there is no way to encode quantum information to send it through the channel and then decode it so the receiver gets a state close to the state that was sent.</p>
<p>To see why this is the case, assume the contrary, that there do exist encoding and decoding protocols that send quantum information through quantum erasure channels with erasure rate <img alt="p\geq 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cgeq+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p\geq 0.5"/>. We will show that this violates the no-cloning theorem. Now, suppose that <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> does the following: For each qubit in the encoded state, she tosses a fair coin. If the coin lands heads, she send <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> the state <img alt="|E\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7CE%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|E\rangle"/> and sends <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> the channel input with probability <img alt="2p-1" class="latex" src="https://s0.wp.com/latex.php?latex=2p-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2p-1"/> and the erasure state <img alt="|E\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7CE%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|E\rangle"/> otherwise. If the coin lands tails, she sends <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> the state <img alt="|E\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7CE%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|E\rangle"/> and sends <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> the channel input with probability <img alt="2p-1" class="latex" src="https://s0.wp.com/latex.php?latex=2p-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2p-1"/> and the erasure state otherwise. This implements a <img alt="p\geq 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cgeq+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p\geq 0.5"/> channel to both receivers <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> and <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/>, which means that <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> can use this channel to transmit an encoding of <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/> to both receivers, which in turn means that both receivers will be able to decode <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/>. But this means that <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> has just used this channel to clone the quantum state <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/>, resulting in a contradiction. Thus no quantum information can be transmitted through a channel with <img alt="p\geq 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cgeq+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p\geq 0.5"/>. Note, however, that we can send classical information over this channel, so the behavior of quantum and classical information is markedly different.</p>
<p>It turns out that the rate of quantum information sent over the erasure channel, as a function of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>, is given by the following graph:</p>
<p><img alt="fig4.png" class="alignnone size-full wp-image-6318" src="https://windowsontheory.files.wordpress.com/2018/12/fig4.png?w=600"/><br/>
while the rate of classical information sent over the erasure channel, as a function of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>, is given by the following graph:</p>
<p><img alt="fig5.png" class="alignnone size-full wp-image-6319" src="https://windowsontheory.files.wordpress.com/2018/12/fig5.png?w=600"/></p>
<p>Next we will formally state the definition of channel capacity, and then we will return to the quantum erasure channel example and derive the curve that plots rate against <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>.</p>
<p><strong>4.2.2. Definition of Channel Capacities</strong><br/>
Channel capacity is defined as the maximum rate at which information can be communicated over many independent uses of a channel from sender to receiver. Here we list the expressions for channel capacity for classical channels, classical information over a quantum channel, and quantum information over a quantum channel.</p>
<p><strong>Classical Channel Capacity</strong> For a classical channel this expression is just the maximum mutual information over all input-output pairs,</p>
<p><img alt="\max_X H(\eta(X))-H(\eta(X)|X)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_X+H%28%5Ceta%28X%29%29-H%28%5Ceta%28X%29%7CX%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\max_X H(\eta(X))-H(\eta(X)|X)"/>,</p>
<p>where <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/> is the input information and <img alt="\eta(X)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%28X%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta(X)"/> is the output information after having gone through the channel <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta"/>.</p>
<p><strong>Classical Information Over a Quantum Channel </strong>The capacity for classical information sent over a quantum channel is given by</p>
<p><img alt="\max_{\{p_i,\rho_i\}} H(\eta(\rho))-\sum_i p_iH(\eta(\rho_i))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%7B%5C%7Bp_i%2C%5Crho_i%5C%7D%7D+H%28%5Ceta%28%5Crho%29%29-%5Csum_i+p_iH%28%5Ceta%28%5Crho_i%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\max_{\{p_i,\rho_i\}} H(\eta(\rho))-\sum_i p_iH(\eta(\rho_i))"/></p>
<p>up to regularization, where <img alt="\rho=\sum_i p_i\rho_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%3D%5Csum_i+p_i%5Crho_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho=\sum_i p_i\rho_i"/> is the average input state, and <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta"/> is the channel.</p>
<p>Note that we would regularize this by using <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> copies of the state (that is to say, we want the output of <img alt="\eta^{\otimes n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%5E%7B%5Cotimes+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta^{\otimes n}"/>) and then dividing by <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>, to get an expression like the following for the regularized capacity of classical information over a quantum channel:</p>
<p><img alt="\lim_{n\rightarrow\infty}\max_{\{p_i,\rho_i\}} [H(\eta(\rho)^{\otimes n})-\sum_i p_i H(\eta(\rho_i)^{\otimes n})]/n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn%5Crightarrow%5Cinfty%7D%5Cmax_%7B%5C%7Bp_i%2C%5Crho_i%5C%7D%7D+%5BH%28%5Ceta%28%5Crho%29%5E%7B%5Cotimes+n%7D%29-%5Csum_i+p_i+H%28%5Ceta%28%5Crho_i%29%5E%7B%5Cotimes+n%7D%29%5D%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lim_{n\rightarrow\infty}\max_{\{p_i,\rho_i\}} [H(\eta(\rho)^{\otimes n})-\sum_i p_i H(\eta(\rho_i)^{\otimes n})]/n"/>.</p>
<p><strong>Quantum Information</strong> The capacity for quantum information is given by the expression</p>
<p><img alt="\max_\rho H(\eta(\rho))-H((\eta\otimes I)\Phi_\rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%5Crho+H%28%5Ceta%28%5Crho%29%29-H%28%28%5Ceta%5Cotimes+I%29%5CPhi_%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\max_\rho H(\eta(\rho))-H((\eta\otimes I)\Phi_\rho)"/>,</p>
<p>also up to regularization. Here <img alt="\eta(\rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%28%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta(\rho)"/> is the output when channel <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> acts on input state <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/>, while <img alt="\Phi_\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPhi_%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Phi_\rho"/> is the purification of <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> (that is, it is a pure state containing <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> that we can obtain by enlarging the Hilbert space). The regularized capacity for quantum information looks like the following:</p>
<p><img alt="&#xA0;\lim_{n\rightarrow\infty}\max_\rho [H(\eta(\rho)^{\otimes n})-H((\eta\otimes I)(\Phi_\rho)^{\otimes n})]/n" class="latex" src="https://s0.wp.com/latex.php?latex=%C2%A0%5Clim_%7Bn%5Crightarrow%5Cinfty%7D%5Cmax_%5Crho+%5BH%28%5Ceta%28%5Crho%29%5E%7B%5Cotimes+n%7D%29-H%28%28%5Ceta%5Cotimes+I%29%28%5CPhi_%5Crho%29%5E%7B%5Cotimes+n%7D%29%5D%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="&#xA0;\lim_{n\rightarrow\infty}\max_\rho [H(\eta(\rho)^{\otimes n})-H((\eta\otimes I)(\Phi_\rho)^{\otimes n})]/n"/>.</p>
<p>Now that we have the exact expression that allows us to calculate the quantum channel capacity, we will revisit our example of the quantum erasure channel and reproduce the plot of channel rate vs erasure probability.</p>
<p><strong>4.2.3. Example Revisited: Quantum Erasure Channel</strong><br/>
Recall that, up to regularization, the capacity of a quantum channel is given by</p>
<p><img alt="\max_\rho H(\eta(\rho))-H((\eta\otimes I)\Phi_\rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%5Crho+H%28%5Ceta%28%5Crho%29%29-H%28%28%5Ceta%5Cotimes+I%29%5CPhi_%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\max_\rho H(\eta(\rho))-H((\eta\otimes I)\Phi_\rho)"/>.</p>
<p>We will directly calculate this expression for the example of the quantum erasure channel. Let the input <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> be given by the density matrix for the completely mixed state,</p>
<p><img alt="\rho=\left(\begin{array}{cc}\frac{1}{2}&amp;0\\0&amp;\frac{1}{2}\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%5Cfrac%7B1%7D%7B2%7D%260%5C%5C0%26%5Cfrac%7B1%7D%7B2%7D%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho=\left(\begin{array}{cc}\frac{1}{2}&amp;0\\0&amp;\frac{1}{2}\end{array}\right)"/>,</p>
<p>so that the purification of <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> is given by the state</p>
<p><img alt="\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C00%5Crangle%2B%7C11%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)"/>.</p>
<p>Recall that the erasure channel replaces our state with <img alt="|E\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7CE%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|E\rangle"/> with probability <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>, while with probability <img alt="1-p" class="latex" src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1-p"/> it leaves the input state unchanged. Then, in the basis <img alt="\{|0\rangle, |1\rangle, |E\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C0%5Crangle%2C+%7C1%5Crangle%2C+%7CE%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|0\rangle, |1\rangle, |E\rangle\}"/>, the matrix corresponding to <img alt="\eta(\rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%28%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta(\rho)"/> is given by</p>
<p><img alt="\eta(\rho)=\left(\begin{array}{ccc}\frac{1-p}{2}&amp;0&amp;0\\0&amp;\frac{1-p}{2}&amp;0\\0&amp;0&amp;p\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%28%5Crho%29%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bccc%7D%5Cfrac%7B1-p%7D%7B2%7D%260%260%5C%5C0%26%5Cfrac%7B1-p%7D%7B2%7D%260%5C%5C0%260%26p%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta(\rho)=\left(\begin{array}{ccc}\frac{1-p}{2}&amp;0&amp;0\\0&amp;\frac{1-p}{2}&amp;0\\0&amp;0&amp;p\end{array}\right)"/></p>
<p>while in the basis <img alt="\{|00\rangle, |01\rangle, |10\rangle, |11\rangle, |0E\rangle, |1E\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C00%5Crangle%2C+%7C01%5Crangle%2C+%7C10%5Crangle%2C+%7C11%5Crangle%2C+%7C0E%5Crangle%2C+%7C1E%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|00\rangle, |01\rangle, |10\rangle, |11\rangle, |0E\rangle, |1E\rangle\}"/>, the matrix corresponding to <img alt="(\eta\otimes I)\Phi_\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Ceta%5Cotimes+I%29%5CPhi_%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\eta\otimes I)\Phi_\rho"/> is given by</p>
<p><img alt="(\eta\otimes I)\Phi_\rho=\left(\begin{array}{cccccc}\frac{1-p}{2}&amp;0&amp;0&amp;\frac{1-p}{2}&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;0\\\frac{1-p}{2}&amp;0&amp;0&amp;\frac{1-p}{2}&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;\frac{p}{2}&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;\frac{p}{2}\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Ceta%5Cotimes+I%29%5CPhi_%5Crho%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcccccc%7D%5Cfrac%7B1-p%7D%7B2%7D%260%260%26%5Cfrac%7B1-p%7D%7B2%7D%260%260%5C%5C0%260%260%260%260%260%5C%5C0%260%260%260%260%260%5C%5C%5Cfrac%7B1-p%7D%7B2%7D%260%260%26%5Cfrac%7B1-p%7D%7B2%7D%260%260%5C%5C0%260%260%260%26%5Cfrac%7Bp%7D%7B2%7D%260%5C%5C0%260%260%260%260%26%5Cfrac%7Bp%7D%7B2%7D%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\eta\otimes I)\Phi_\rho=\left(\begin{array}{cccccc}\frac{1-p}{2}&amp;0&amp;0&amp;\frac{1-p}{2}&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;0\\\frac{1-p}{2}&amp;0&amp;0&amp;\frac{1-p}{2}&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;\frac{p}{2}&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;\frac{p}{2}\end{array}\right)"/></p>
<p>We can directly calculate that</p>
<p><img alt="H(\eta(\rho))=H_2(p)+(1-p)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28%5Ceta%28%5Crho%29%29%3DH_2%28p%29%2B%281-p%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H(\eta(\rho))=H_2(p)+(1-p)"/></p>
<p><img alt="H((\eta\otimes I)\Phi_\rho)=H_2(p)+p" class="latex" src="https://s0.wp.com/latex.php?latex=H%28%28%5Ceta%5Cotimes+I%29%5CPhi_%5Crho%29%3DH_2%28p%29%2Bp&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H((\eta\otimes I)\Phi_\rho)=H_2(p)+p"/>.</p>
<p>Then, subtracting the two entropies, we can calculate the rate to be</p>
<p><img alt="R=1-2p" class="latex" src="https://s0.wp.com/latex.php?latex=R%3D1-2p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R=1-2p"/>,</p>
<p>which corresponds exactly to the line we saw on the diagram that plotted rate as a function of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> for the quantum erasure channel.</p>
<p><strong>References</strong></p>
<ol>
<li>Bennett, C. H., DiVencenzo, D. P., and Smolin, J. A. Capacities of quantum erasure channels. <em>Phys. Rev. Lett</em>., 78:3217-3220 (1997). quant-ph/9701015.</li>
<li>Bennett, C. H., DiVencenzo, D. P., Smolin, J. A., and Wootters, W. K. Mixed state entanglement and quantum error correction. <em>Phys. Rev. A,</em> 54:3824 (1996). quant-ph/9604024.</li>
<li>Calderbank, A. R. and Shor, P. W. Good quantum error-correcting codes exist. <em>Phys. Rev. A</em>, 54:1098 (1996). quant-ph/9512032.</li>
<li>Devetak, I. The Private Classical Capacity and Quantum Capacity of a Quantum Channel. <em>IEEE Trans. Inf. Theor</em>., 51:44-45 (2005). quant-ph/0304127</li>
<li>Devetak, I. and Winter, A. Classical data compression with quantum side information. <em>Phys. Rev. A</em>, 68(4):042301 (2003).</li>
<li>Gottesman, D. Class of quantum error-correcting codes saturating the quantum Hamming bound. <em>Phys. Rev. A</em>, 54:1862 (1996).</li>
<li>Laflamme, R., Miquel, C., Paz, J.-P., and Zurek, W. H. Perfect quantum error correction code. <em>Phys. Rev. Lett</em>., 77:198 (1996). quant-ph/9602019.</li>
<li>Lloyd, S. Capacity of the noisy quantum channel. <em>Phys. Rev. A</em>., 55:3 (1997). quant-ph/9604015.</li>
<li>Nielsen, M. A. and Chuang, I. L. <em>Quantum Computation and Quantum Information</em>., Cambridge University Press, New York (2011).</li>
<li>Shor, P. W. Scheme for reducing decoherence in quantum computer memory. <em>Phys. Rev. A</em>., 52:2493 (1995).</li>
<li>Shor, P. W. The quantum channel capacity and coherent information. <em>MSRI Workshop on Quantum Computation</em> (2002).</li>
<li>Steane, A. M. Error correcting codes in quantum theory. <em>Phys. Rev. Lett</em>., 77:793 (1996).</li>
<li>Steane, A. M. Multiple particle interference and quantum error correction. <em>Proc. R. Soc. London A</em>, 452:2551-76 (1996).</li>
</ol></div>
    </content>
    <updated>2018-12-07T15:12:52Z</updated>
    <published>2018-12-07T15:12:52Z</published>
    <category term="physics"/>
    <author>
      <name>anniewei</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-17T05:29:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6305</id>
    <link href="https://windowsontheory.org/2018/11/21/highlights-beyond-ec-call-for-nominations/" rel="alternate" type="text/html"/>
    <title>Highlights beyond EC: Call for nominations</title>
    <summary>[Guest post by Moshe Babaioff –Boaz] “Highlights Beyond EC” Session at EC 2019: Call for Nominations Committee: Mohammad Akbarpour, Moshe Babaioff, Shengwu Li and Ariel Procaccia Following a new tradition started last year, the 2019 ACM Conference on Economics and Computation (EC’19) will host a special session highlighting some of the best work in economics […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Moshe Babaioff –Boaz]</em></p>
<p style="font-weight: 400;"><strong>“Highlights Beyond EC” Session at EC 2019: Call for Nominations</strong></p>
<p style="font-weight: 400;"><strong><u>Committee</u></strong>: Mohammad Akbarpour, Moshe Babaioff, Shengwu Li and Ariel Procaccia</p>
<p style="font-weight: 400;">Following a new tradition started last year, the 2019 ACM Conference on Economics and Computation (EC’19) will host a special session highlighting some of the best work in economics and computation that appears in conferences and journals other than EC. The intention of this session is to expose EC attendees to related work just beyond the boundary of their current awareness.</p>
<p style="font-weight: 400;">We seek nominations for papers on Economics and Computation that have made breakthrough advances, opened up new questions or areas, made unexpected connections, or had significant impact on practice or other sciences. Examples of conferences and journals that publish papers relevant for the special session include STOC/FOCS/SODA/ITCS, AAAI/IJCAI/AAMAS, NIPS/ICML/COLT, WWW/KDD, AER/Econometrica/JPE/QJE/RESTUD/TE/AEJ Micro/JET/GEB, and Math of OR/Management Science/Operations Research. Please email nominations to<u><a href="mailto:HighlightsBeyondEC@gmail.com">HighlightsBeyondEC@gmail.com</a></u>. Anyone is welcome to contact us, but we especially invite members of PCs or editorial boards in various venues to send us suggestions. Nominations should include:</p>
<ol>
<li style="font-weight: 400;"> Name of paper and authors.</li>
<li style="font-weight: 400;">Publication venue or online working version. Preference will be given to papers that have appeared in a related conference or journal within the past two years, or have a working version circulated within the past two years.</li>
<li style="font-weight: 400;">Short (2-3 paragraph) explanation of the paper and its importance.</li>
<li style="font-weight: 400;">(Optional) Names of 1-3 knowledgeable experts on the area of the paper.</li>
</ol>
<p style="font-weight: 400;">Note that at least one of the authors of a selected paper will be required to present their paper at EC 2019 and so should be available to travel to the conference, which is taking place in Phoenix, AZ on June 25-27, 2019.</p>
<p style="font-weight: 400;">To ensure maximum consideration, please send all nominations by December 15, 2018.</p>
<p style="font-weight: 400;"/></div>
    </content>
    <updated>2018-11-21T19:14:58Z</updated>
    <published>2018-11-21T19:14:58Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-17T05:29:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6301</id>
    <link href="https://windowsontheory.org/2018/11/16/halg-2019-call-for-nominations/" rel="alternate" type="text/html"/>
    <title>HALG 2019 Call for nominations</title>
    <summary>[Guest post by Piotr Sankowski –Boaz] Call for Nominations – 4th Highlights of Algorithms conference (HALG 2019) Copenhagen, June 14-16, 2019 http://www.halgdiku.dk/ The HALG 2019 conference seeks high-quality nominations for invited talks that will highlight recent advances in algorithmic research. Similarly to previous years, there are two categories of invited talks: A. survey (60 minutes): a […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Piotr Sankowski –Boaz]</em></p>
<p><strong>Call for Nominations – </strong><strong>4th Highlights of Algorithms conference (HALG 2019)</strong></p>
<p><strong>Copenhagen, June 14-16, 2019</strong></p>
<p><a href="http://www.halgdiku.dk/" rel="noopener noreferrer" target="_blank">http://www.halgdiku.dk/</a></p>
<p>The HALG 2019 conference seeks high-quality nominations for invited talks that will highlight recent advances in algorithmic research. Similarly to previous years, there are two categories of invited talks:</p>
<p><strong>A. survey (60 minutes):</strong> a survey of an algorithmic topic that has seen exciting developments in last couple of years.</p>
<p><strong>B. paper (30 minutes):</strong> a significant algorithmic result appearing in a paper in 2018 or later.</p>
<p>To nominate, please email  <a href="mailto:halg2019.nominations@gmail.com" rel="noopener" target="_blank">halg2019.nominations@gmail.com</a>  the following information:</p>
<p><strong>1. Basic details:</strong> speaker name + topic (for survey talk) or paper’s title, authors, conference/arxiv + preferable speaker (for paper talk).</p>
<p><strong>2. Brief justification:</strong> Focus on the benefits to the audience, e.g., quality of results, importance/relevance of topic, clarity of talk, speaker’s presentation skills.</p>
<p>All nominations will be reviewed by the Program Committee (PC) to select speakers that will be invited to the conference.</p>
<p>Nominations deadline: <strong>December 9, 2018</strong> (for full consideration).</p>
<p>Please keep in mind that the conference does not provide financial support for the speakers.</p>
<p>Best regards,<br/>
Piotr Sankowski<br/>
HALG 2019 PC Chair</p></div>
    </content>
    <updated>2018-11-16T14:39:20Z</updated>
    <published>2018-11-16T14:39:20Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-17T05:29:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6299</id>
    <link href="https://windowsontheory.org/2018/11/12/wheres-that-paper/" rel="alternate" type="text/html"/>
    <title>Where’s that paper?</title>
    <summary>[Guest post by Eylon Yogev about a Chrome extension he wrote, of which I am a happy user –Boaz] Hi fellow researchers, I’m writing to share a little tool that I have developed with the ambitious goal of boosting research productivity. The tool is a Chrome extension named “Where’s that paper?”. Before I tell you […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Eylon Yogev about a <a href="https://chrome.google.com/webstore/detail/wheres-that-paper/dkjnkdmoghkbkfkafefhbcnmofdbfdio">Chrome extension</a> he wrote, of which I am a happy user –Boaz]</em></p>
<p dir="ltr">Hi fellow researchers,</p>
<p dir="ltr">I’m writing to share a little tool that I have developed with the ambitious goal of boosting research productivity. The tool is a Chrome extension named “Where’s that paper?”. Before I tell you more about what it does, let me touch upon the largest obstacle of any new tool, the learning curve. Rest assured that “Where’s that paper?” requires  zero training – just install it and continue working as usual – it will guide you without further effort on your end.</p>
<p dir="ltr">After building much suspense, I will elaborate. If you’re like me, you find yourself frequently searching the web for papers that you have browsed / opened in the past on your computer – basically paper browsing history. Whether it is in the process of writing a paper or searching for existing ideas, you may search for authors of titles of papers that you recall know to have once glimpsed at. At some point, the paper has either been downloaded or added  to the favorites bar. In any case, this process is manual and takes up much time (often including frustration). As such, I thought we could all use some code to automate it.</p>
<h3><strong>Functionality.</strong></h3>
<p dir="ltr">The extension is very simple: it identifies when you are reading a scientific paper (according to the domain) and then automatically adds this paper, with the proper author list, year etc., to your favorites bar under a designated folder. Then, when you type a search in the Chrome’s search bar for an author or a title, the relevant results from the favorites pop up. One might also explicitly browse in the favorites to see all papers read.</p>
<p dir="ltr">See an example of the results shown when searching for “short” in Chrome’s address bar. The first three links are from the favorite and the rest are general Google suggestions from the web.</p>
<p dir="ltr"><img height="156" src="https://lh3.googleusercontent.com/as_kKo13e_1V6xYViPzmZ0OEAoLbv1H8yJn_4v0efz-4MM7opHtLX9rNd5bRNY2qJbumaoPsIP2jNUr2FrkSyJ1u1nwj2iAldENR9CHyE3JV5QOYzR3NEVdGHpKJ77V-TzPy5uVooE5YS8nudA" width="529"/></p>
<p dir="ltr">The extension automatically works on a list of specified domains that include:</p>
<p dir="ltr"><a href="http://eprint.iacr.org/" rel="noopener" target="_blank">eprint.iacr.org</a>, <a href="http://arxiv.org/" rel="noopener" target="_blank">arxiv.org</a>, <a href="http://eccc.weizmann.ac.il/" rel="noopener" target="_blank">eccc.weizmann.ac.il</a>, <a href="http://epubs.siam.org/" rel="noopener" target="_blank">epubs.siam.org</a>, <a href="http://research.microsoft.com/" rel="noopener" target="_blank">research.microsoft.com</a>, <a href="http://citeseerx.ist.psu.edu/" rel="noopener" target="_blank">citeseerx.ist.psu.edu</a>, <a href="http://ac.elscdn.com/" rel="noopener" target="_blank">ac.elscdn.com</a>, <a href="http://www.sciencedirect.com/" rel="noopener" target="_blank">www.sciencedirect.com</a>, <a href="http://download.springer.com/" rel="noopener" target="_blank">download.springer.com</a>, <a href="http://link.springer.com/" rel="noopener" target="_blank">link.springer.com</a>, <a href="http://delivery.acm.org/" rel="noopener" target="_blank">delivery.acm.org</a>, proceedings.mlr.press and <a href="http://journals.aps.org/" rel="noopener" target="_blank">journals.aps.org</a>.</p>
<p dir="ltr">It is not too difficult to customize the list and additional domains. Reach out to me and I’ll add them upon your request!</p>
<p dir="ltr">A bonus feature (thanks Boaz for the suggestion): Click the extension’s icon and you can download a bib file containing (a nice formatting of) the DBLP bibtex records of all papers added to the favorites.</p>
<p dir="ltr">Download “Where’s that paper?” from the Chrome Web Store: <a href="https://chrome.google.com/webstore/detail/wheres-that-paper/dkjnkdmoghkbkfkafefhbcnmofdbfdio" rel="noopener" target="_blank">https://chrome.google.com/webstore/detail/wheres-that-paper/dkjnkdmoghkbkfkafefhbcnmofdbfdio</a></p>
<h3><strong>Security.</strong></h3>
<p dir="ltr">Most who work in our domain of expertise would be quite concerned with installing extensions and with good reason. The extension I wrote does not leak any information, not to me or to another third party. I have no ulterior motive in developing the extension, originally helping myself, I now see the benefit of sharing it with our community.  I do not know how to reassure you that this is indeed the case other than giving you my word and publishing the source code online. It is available here:</p>
<p dir="ltr"><a href="https://github.com/eylonyogev/Where-s-That-Paper-" rel="noopener" target="_blank">https://github.com/eylonyogev/Where-s-That-Paper-</a></p>
<p dir="ltr">I’d be glad to hear any feedback.</p>
<p dir="ltr">Thanks,</p>
<p dir="ltr">Eylon.</p></div>
    </content>
    <updated>2018-11-12T23:38:01Z</updated>
    <published>2018-11-12T23:38:01Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>windowsontheory</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-17T05:29:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6294</id>
    <link href="https://windowsontheory.org/2018/11/11/approximating-partition-functions/" rel="alternate" type="text/html"/>
    <title>Approximating Partition Functions</title>
    <summary>[Guest post by Alex Kelser, Alex Lin, Amil Merchant, and Suproteem Sarkar, scribing for a lecture by Andrej Risteski.] Andrej Risteski: Approximating Partition Functions via Variational Methods and Taylor Series For a probability distribution defined up to a constant of proportionality, we have already seen the partition function. To refresh your memory, given a probability […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Alex Kelser, Alex Lin, Amil Merchant, and Suproteem Sarkar, scribing for a lecture by Andrej Risteski.]</em></p>
<h1 id="andrej-risteski-approximating-partition-functions-via-variational-methods-and-taylor-series">Andrej Risteski: Approximating Partition Functions via Variational Methods and Taylor Series</h1>
<p>For a probability distribution defined up to a constant of proportionality, we have already seen the <strong>partition function</strong>. To refresh your memory, given a probability mass function <img alt="p(\vec{x}) \propto f(\vec{x})" src="https://latex.codecogs.com/png.latex?p%28%5Cvec%7Bx%7D%29%20%5Cpropto%20f%28%5Cvec%7Bx%7D%29" style="vertical-align: middle;" title="p(\vec{x}) \propto f(\vec{x})"/> over all <img alt="\vec{x} \in \mathcal{X}" src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D%20%5Cin%20%5Cmathcal%7BX%7D" style="vertical-align: middle;" title="\vec{x} \in \mathcal{X}"/>, the partition function is simply the <em>normalization constant</em> of the distribution, i.e.<br/>
<img alt="Z = \sum_{\vec{x} \in \mathcal{X}} f(\vec{x})" src="https://latex.codecogs.com/png.latex?Z%20%3D%20%5Csum_%7B%5Cvec%7Bx%7D%20%5Cin%20%5Cmathcal%7BX%7D%7D%20f%28%5Cvec%7Bx%7D%29" style="vertical-align: middle;" title="Z = \sum_{\vec{x} \in \mathcal{X}} f(\vec{x})"/></p>
<p>At first glance, the partition function may appear to be uninteresting. However, upon taking a deeper look, this single quantity holds a wide array of intriguing and complex properties. For one thing, its significance in the world of thermodynamics cannot be overstated. The partition function is at the heart of relating the microscopic quantities of a system – such as the individual energies of each probabilistic state <img alt="\vec{x} \in \mathcal{X}" src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D%20%5Cin%20%5Cmathcal%7BX%7D" style="vertical-align: middle;" title="\vec{x} \in \mathcal{X}"/> – to macroscopic entities describing the entire system: the total energy, the energy fluctuation, the heat capacity, and the free energy. For explicit formulas, consult <a href="https://pdfs.semanticscholar.org/456b/b94e824c2637f763233dd3110ba71c900011.pdf">this source</a> for reference. In machine learning the partition function also holds much significance, since it’s intimately linked to computing marginals in the model.</p>
<p>Although there exists an explicit formula for the partition function, the challenge lies in computing the quantity in polynomial time. Suppose <img alt="\vec{x}" src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D" style="vertical-align: middle;" title="\vec{x}"/> is a vector of dimension <img alt="n" src="https://latex.codecogs.com/png.latex?n" style="vertical-align: middle;" title="n"/> where each <img alt="x_i \in \{-1, 1\}" src="https://latex.codecogs.com/png.latex?x_i%20%5Cin%20%5C%7B-1%2C%201%5C%7D" style="vertical-align: middle;" title="x_i \in \{-1, 1\}"/> for <img alt="i \in \{1, \ldots, n\}" src="https://latex.codecogs.com/png.latex?i%20%5Cin%20%5C%7B1%2C%20%5Cldots%2C%20n%5C%7D" style="vertical-align: middle;" title="i \in \{1, \ldots, n\}"/>. Specifically, we would like a smarter way of finding <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/> than simply adding up <img alt="f(x)" src="https://latex.codecogs.com/png.latex?f%28x%29" style="vertical-align: middle;" title="f(x)"/> over all <img alt="2^n" src="https://latex.codecogs.com/png.latex?2%5En" style="vertical-align: middle;" title="2^n"/> combinations of <img alt="x" src="https://latex.codecogs.com/png.latex?x" style="vertical-align: middle;" title="x"/>.</p>
<p>Andrej Risteski’s lecture focused on using two general techniques – variational methods and Taylor series approximations – to find provably approximate estimates of <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/>.</p>
<h1 id="motivation">Motivation</h1>
<p>The general setup addresses undirected graphical models, also known as a Markov random fields (MRF), where the probability mass function <img alt="p : \mathcal{X} \to [0, 1]" src="https://latex.codecogs.com/png.latex?p%20%3A%20%5Cmathcal%7BX%7D%20%5Cto%20%5B0%2C%201%5D" style="vertical-align: middle;" title="p : \mathcal{X} \to [0, 1]"/> has the form<br/>
<img alt=" p(\vec{x}) \propto \exp \left( \sum_{i \sim j} \phi_{ij}(x_i,x_j) \right) " src="https://latex.codecogs.com/png.latex?%20p%28%5Cvec%7Bx%7D%29%20%5Cpropto%20%5Cexp%20%5Cleft%28%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cphi_%7Bij%7D%28x_i%2Cx_j%29%20%5Cright%29%20" style="vertical-align: middle;" title=" p(\vec{x}) \propto \exp \left( \sum_{i \sim j} \phi_{ij}(x_i,x_j) \right) "/><br/>
for some random, <img alt="n" src="https://latex.codecogs.com/png.latex?n" style="vertical-align: middle;" title="n"/>-dimensional vector <img alt="\vec{x} \in \mathcal{X}" src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D%20%5Cin%20%5Cmathcal%7BX%7D" style="vertical-align: middle;" title="\vec{x} \in \mathcal{X}"/> and some set of parameterized functions <img alt="\{\phi_{ij}: \mathcal{X} \to \mathcal{R}\}" src="https://latex.codecogs.com/png.latex?%5C%7B%5Cphi_%7Bij%7D%3A%20%5Cmathcal%7BX%7D%20%5Cto%20%5Cmathcal%7BR%7D%5C%7D" style="vertical-align: middle;" title="\{\phi_{ij}: \mathcal{X} \to \mathcal{R}\}"/>. Note that the notation <img alt="i \sim j" src="https://latex.codecogs.com/png.latex?i%20%5Csim%20j" style="vertical-align: middle;" title="i \sim j"/> denotes all pairs <img alt="(i, j)" src="https://latex.codecogs.com/png.latex?%28i%2C%20j%29" style="vertical-align: middle;" title="(i, j)"/> where <img alt="i, j \in \{1, 2, \ldots, n\}" src="https://latex.codecogs.com/png.latex?i%2C%20j%20%5Cin%20%5C%7B1%2C%202%2C%20%5Cldots%2C%20n%5C%7D" style="vertical-align: middle;" title="i, j \in \{1, 2, \ldots, n\}"/> and the edge <img alt="e = (i, j)" src="https://latex.codecogs.com/png.latex?e%20%3D%20%28i%2C%20j%29" style="vertical-align: middle;" title="e = (i, j)"/> exists in the graph.</p>
<p>The talk considered the specific setup where each <img alt="x_i \in \{-1, 1\}" src="https://latex.codecogs.com/png.latex?x_i%20%5Cin%20%5C%7B-1%2C%201%5C%7D" style="vertical-align: middle;" title="x_i \in \{-1, 1\}"/>, so <img alt="\mathcal{X} = \{-1, 1\}^n" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D%20%3D%20%5C%7B-1%2C%201%5C%7D%5En" style="vertical-align: middle;" title="\mathcal{X} = \{-1, 1\}^n"/>. Also, we fix</p>
<p><img alt="\phi_{ij}(x_i, x_j) = J_{ij} x_i x_j" src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bij%7D%28x_i%2C%20x_j%29%20%3D%20J_%7Bij%7D%20x_i%20x_j" style="vertical-align: middle;" title="\phi_{ij}(x_i, x_j) = J_{ij} x_i x_j"/><br/>
for some set of coefficients <img alt="\{J_{ij}\}" src="https://latex.codecogs.com/png.latex?%5C%7BJ_%7Bij%7D%5C%7D" style="vertical-align: middle;" title="\{J_{ij}\}"/>, thereby giving us the well-known <a href="https://en.wikipedia.org/wiki/Ising_model">Ising model</a>. Note, the interaction terms could be more complicated and made “less local” if desired, but that was not discussed in the lecture.</p>
<p>These graphical models are common in machine learning, where there are two common tasks of interest:</p>
<ol type="1">
<li><strong>Learning</strong>: given samples, models try to learn <img alt="\phi_{ij}(x_i, x_j)" src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bij%7D%28x_i%2C%20x_j%29" style="vertical-align: middle;" title="\phi_{ij}(x_i, x_j)"/> or <img alt="\{J_{ij}\}" src="https://latex.codecogs.com/png.latex?%5C%7BJ_%7Bij%7D%5C%7D" style="vertical-align: middle;" title="\{J_{ij}\}"/></li>
<li><strong>Inference</strong>: given <img alt="\{\phi_{ij}\}" src="https://latex.codecogs.com/png.latex?%5C%7B%5Cphi_%7Bij%7D%5C%7D" style="vertical-align: middle;" title="\{\phi_{ij}\}"/> or the potentials <img alt="\{J_{ij}\}" src="https://latex.codecogs.com/png.latex?%5C%7BJ_%7Bij%7D%5C%7D" style="vertical-align: middle;" title="\{J_{ij}\}"/> as input, we want to calculate the marginal probabilities, such as<br/>
<img alt="p(x_i = -1) \text{ or }  p(x_i = 1, x_j = -1) " src="https://latex.codecogs.com/png.latex?p%28x_i%20%3D%20-1%29%20%5Ctext%7B%20or%20%7D%20%20p%28x_i%20%3D%201%2C%20x_j%20%3D%20-1%29%20" style="vertical-align: middle;" title="p(x_i = -1) \text{ or }  p(x_i = 1, x_j = -1) "/></li>
</ol>
<p>We focus on the latter task. The problem of inference is closely related to calculating the <strong>partition function</strong>. This value is often used as the normalization constant in many methods, and it is classically defined as<br/>
<img alt=" Z = \sum_{x \in \mathcal{X}} \exp \left( \sum_{i \sim j} \phi_{ij}(x_i, x_j) \right)" src="https://latex.codecogs.com/png.latex?%20Z%20%3D%20%5Csum_%7Bx%20%5Cin%20%5Cmathcal%7BX%7D%7D%20%5Cexp%20%5Cleft%28%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cphi_%7Bij%7D%28x_i%2C%20x_j%29%20%5Cright%29" style="vertical-align: middle;" title=" Z = \sum_{x \in \mathcal{X}} \exp \left( \sum_{i \sim j} \phi_{ij}(x_i, x_j) \right)"/></p>
<p>Although we can write down the aforementioned closed-form expression for the partition function, it is difficult to calculate this quantity in polynomial time. There are two broad approaches to solving inference problems:</p>
<ol type="1">
<li><strong>Randomized</strong>: Methods based on Markov chain Monte Carlo (MCMC), such as Gibbs sampling, that construct a Markov chain whose stationary distribution is the distribution of interest. These have been more extensively studied in theoretical computer science.</li>
<li><strong>Deterministic</strong>: Variational methods, self-avoiding walk trees, Taylor expansion methods</li>
</ol>
<p>Although more often used in practice, randomized algorithms such as MCMC are notoriously hard to debug, and it is often unclear at what point the chain reaches stationarity.</p>
<p>In contrast, variational methods are often more difficult to rigorously analyze but have the added benefit of turning inference problems into optimization problems. Risteski’s talk considered some provable instantiations of variational methods for calculating the partition function.</p>
<h1 id="variational-methods">Variational Methods</h1>
<p>Let us start with a basic observation, known as the Gibbs variational principle. This can be stated as the following.</p>
<p><strong>Lemma 1</strong>: Let <img alt="p(\vec{x}) \propto \exp{\sum_{i \sim j} J_{ij} x_i x_j}" src="https://latex.codecogs.com/png.latex?p%28%5Cvec%7Bx%7D%29%20%5Cpropto%20%5Cexp%7B%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20x_i%20x_j%7D" style="vertical-align: middle;" title="p(\vec{x}) \propto \exp{\sum_{i \sim j} J_{ij} x_i x_j}"/>. Then, we can show that the corresponding partition function satisfies</p>
<p><img alt=" \log Z = \max_{q \in \mathcal{Q}} \left \{ \sum_{i \sim j} \underbrace{J_{ij}\mathbb{E}_q[x_i x_j]}_{\text{energy term}} + \underbrace{\mathbb{H}(q)}_{\text{entropy term}} \right \}  " src="https://latex.codecogs.com/png.latex?%0A%5Clog%20Z%20%3D%20%5Cmax_%7Bq%20%5Cin%20%5Cmathcal%7BQ%7D%7D%20%5Cleft%20%5C%7B%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cunderbrace%7BJ_%7Bij%7D%5Cmathbb%7BE%7D_q%5Bx_i%20x_j%5D%7D_%7B%5Ctext%7Benergy%20term%7D%7D%20%2B%20%5Cunderbrace%7B%5Cmathbb%7BH%7D%28q%29%7D_%7B%5Ctext%7Bentropy%20term%7D%7D%20%5Cright%20%5C%7D%20%0A" style="vertical-align: middle;" title=" \log Z = \max_{q \in \mathcal{Q}} \left \{ \sum_{i \sim j} \underbrace{J_{ij}\mathbb{E}_q[x_i x_j]}_{\text{energy term}} + \underbrace{\mathbb{H}(q)}_{\text{entropy term}} \right \}  "/></p>
<p>where <img alt="q : \mathcal{X} \to [0, 1]" src="https://latex.codecogs.com/png.latex?q%20%3A%20%5Cmathcal%7BX%7D%20%5Cto%20%5B0%2C%201%5D" style="vertical-align: middle;" title="q : \mathcal{X} \to [0, 1]"/> is defined to be a valid distribution over <img alt="\mathcal{X}" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D" style="vertical-align: middle;" title="\mathcal{X}"/> and <img alt="\mathcal{Q}" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BQ%7D" style="vertical-align: middle;" title="\mathcal{Q}"/> is the set of all such distributions.</p>
<p><em>Note that we use <img alt="\mathbb{E}_q(\cdot)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_q%28%5Ccdot%29" style="vertical-align: middle;" title="\mathbb{E}_q(\cdot)"/> to denote the expectation of the inner argument with respect to the distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> and <img alt="\mathbb{H}(q)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D%28q%29" style="vertical-align: middle;" title="\mathbb{H}(q)"/> to denote the Shannon entropy of <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/>.</em></p>
<p><strong>Proof</strong>: For any <img alt="q \in \mathcal{Q}" src="https://latex.codecogs.com/png.latex?q%20%5Cin%20%5Cmathcal%7BQ%7D" style="vertical-align: middle;" title="q \in \mathcal{Q}"/>, the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> between <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> and <img alt="p" src="https://latex.codecogs.com/png.latex?p" style="vertical-align: middle;" title="p"/> must be greater than or equal to 0.</p>
<p><img alt="    \mathbb{KL} (q || p) \geq 0 " src="https://latex.codecogs.com/png.latex?%20%20%20%20%5Cmathbb%7BKL%7D%20%28q%20%7C%7C%20p%29%20%5Cgeq%200%20" style="vertical-align: middle;" title="    \mathbb{KL} (q || p) \geq 0 "/></p>
<p><img alt=" \underbrace{\mathbb{E}_q [\log q(\vec{x})}_{-\mathbb{H}(q)}] - \mathbb{E}_q \left[ \sum_{i \sim j} J_{ij} x_i x_j \right] + \log Z \geq 0 " src="https://latex.codecogs.com/png.latex?%20%5Cunderbrace%7B%5Cmathbb%7BE%7D_q%20%5B%5Clog%20q%28%5Cvec%7Bx%7D%29%7D_%7B-%5Cmathbb%7BH%7D%28q%29%7D%5D%20-%20%5Cmathbb%7BE%7D_q%20%5Cleft%5B%20%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20x_i%20x_j%20%5Cright%5D%20%2B%20%5Clog%20Z%20%5Cgeq%200%20" style="vertical-align: middle;" title=" \underbrace{\mathbb{E}_q [\log q(\vec{x})}_{-\mathbb{H}(q)}] - \mathbb{E}_q \left[ \sum_{i \sim j} J_{ij} x_i x_j \right] + \log Z \geq 0 "/></p>
<p><img alt="\log Z \geq \sum_{i \sim j} J_{ij} \mathbb{E}_q[x_i x_j] + \mathbb{H}(q)" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%20%5Cgeq%20%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20%5Cmathbb%7BE%7D_q%5Bx_i%20x_j%5D%20%2B%20%5Cmathbb%7BH%7D%28q%29" style="vertical-align: middle;" title="\log Z \geq \sum_{i \sim j} J_{ij} \mathbb{E}_q[x_i x_j] + \mathbb{H}(q)"/><br/>
Equality holds if <img alt="q = p" src="https://latex.codecogs.com/png.latex?q%20%3D%20p" style="vertical-align: middle;" title="q = p"/>, leading directly to the lemma above.</p>
<p><em>Note that the proof would have also held in the more generic exponential family with <img alt="\phi_{ij}" src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bij%7D" style="vertical-align: middle;" title="\phi_{ij}"/> instead of <img alt="J_{ij} x_i x_j" src="https://latex.codecogs.com/png.latex?J_%7Bij%7D%20x_i%20x_j" style="vertical-align: middle;" title="J_{ij} x_i x_j"/>. Also, at most temperatures, one of the 2 terms, either the energy or the entropy will often dominate.</em></p>
<p><strong>As a result of this lemma, we have framed the original inference problem of calculating <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/> as an optimization problem over <img alt="\mathcal{Q}" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BQ%7D" style="vertical-align: middle;" title="\mathcal{Q}"/>. This is the crux of variational inference.</strong></p>
<h2 id="optimization-difficulties">Optimization Difficulties</h2>
<p>The issue with this approach is that it is difficult to optimize over the polytope of distributions due to the values of each <img alt="x_i" src="https://latex.codecogs.com/png.latex?x_i" style="vertical-align: middle;" title="x_i"/> coming from <img alt="\pm 1" src="https://latex.codecogs.com/png.latex?%5Cpm%201" style="vertical-align: middle;" title="\pm 1"/>. In general, this is <em>NOT</em> tractable. We can imagine two possible solutions:</p>
<ol type="1">
<li>Inner Approximation
<p>Instead of optimizing over all <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/>, pick a “nice” subfamily of distributions to constrain <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/>. The prototypical example is the mean-field approximation in which we let <img alt="q(\vec{x}) = \prod_{i=1}^n q_i(x_i)" src="https://latex.codecogs.com/png.latex?q%28%5Cvec%7Bx%7D%29%20%3D%20%5Cprod_%7Bi%3D1%7D%5En%20q_i%28x_i%29" style="vertical-align: middle;" title="q(\vec{x}) = \prod_{i=1}^n q_i(x_i)"/>, where each <img alt="q_i : \{-1, 1\} \to [0, 1]" src="https://latex.codecogs.com/png.latex?q_i%20%3A%20%5C%7B-1%2C%201%5C%7D%20%5Cto%20%5B0%2C%201%5D" style="vertical-align: middle;" title="q_i : \{-1, 1\} \to [0, 1]"/> is a univariate distribution. Thus, we approximate <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/> with<br/>
<img alt="\max_{q = \prod_i q_i} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{q_i}[x_i] \cdot \mathbb{E}_{q_j}[x_j] + \sum_i \mathbb{H}(q_i) \right \}" src="https://latex.codecogs.com/png.latex?%5Cmax_%7Bq%20%3D%20%5Cprod_i%20q_i%7D%20%5Cleft%20%5C%7B%20%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20%5Ccdot%20%5Cmathbb%7BE%7D_%7Bq_i%7D%5Bx_i%5D%20%5Ccdot%20%5Cmathbb%7BE%7D_%7Bq_j%7D%5Bx_j%5D%20%2B%20%5Csum_i%20%5Cmathbb%7BH%7D%28q_i%29%20%5Cright%20%5C%7D" style="vertical-align: middle;" title="\max_{q = \prod_i q_i} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{q_i}[x_i] \cdot \mathbb{E}_{q_j}[x_j] + \sum_i \mathbb{H}(q_i) \right \}"/></p>
<p>This would provide a <em>lower</em> bound on <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/>. There are a number of potential issues with this method. For one thing, the functions are typically non-convex. Even ignoring this problem, it is difficult to quantify how good the approximation is.</p></li>
<li>Outer Approximation
<p><em>Note: Given a distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/>, we use <img alt="q_S" src="https://latex.codecogs.com/png.latex?q_S" style="vertical-align: middle;" title="q_S"/> to denote the marginal <img alt="q(\vec{x}_S) = q(x_{s_1}, \ldots, x_{s_k})" src="https://latex.codecogs.com/png.latex?q%28%5Cvec%7Bx%7D_S%29%20%3D%20q%28x_%7Bs_1%7D%2C%20%5Cldots%2C%20x_%7Bs_k%7D%29" style="vertical-align: middle;" title="q(\vec{x}_S) = q(x_{s_1}, \ldots, x_{s_k})"/> for some set <img alt="S = \{s_1, \ldots, s_k\} \subseteq \{1, \ldots, n\}" src="https://latex.codecogs.com/png.latex?S%20%3D%20%5C%7Bs_1%2C%20%5Cldots%2C%20s_k%5C%7D%20%5Csubseteq%20%5C%7B1%2C%20%5Cldots%2C%20n%5C%7D" style="vertical-align: middle;" title="S = \{s_1, \ldots, s_k\} \subseteq \{1, \ldots, n\}"/>.</em></p>
<p>In the outer approximation, we relax the polytope we are optimizing over using convex hierarchies. For instance, we can define <img alt="\mathcal{M}^k" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%5Ek" style="vertical-align: middle;" title="\mathcal{M}^k"/> as the polytope containing valid marginals <img alt="q_S" src="https://latex.codecogs.com/png.latex?q_S" style="vertical-align: middle;" title="q_S"/> over subsets <img alt="S" src="https://latex.codecogs.com/png.latex?S" style="vertical-align: middle;" title="S"/> of size at most <img alt="k" src="https://latex.codecogs.com/png.latex?k" style="vertical-align: middle;" title="k"/>. We can then reformulate our objective as a two-part problem of (1) finding a set of pairwise marginals <img alt="\widetilde{q} = \{\widetilde{q}_S \text{ s.t. } \lvert S \rvert \leq k\}\in \mathcal{M}^k" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D%20%3D%20%5C%7B%5Cwidetilde%7Bq%7D_S%20%5Ctext%7B%20s.t.%20%7D%20%5Clvert%20S%20%5Crvert%20%5Cleq%20k%5C%7D%5Cin%20%5Cmathcal%7BM%7D%5Ek" style="vertical-align: middle;" title="\widetilde{q} = \{\widetilde{q}_S \text{ s.t. } \lvert S \rvert \leq k\}\in \mathcal{M}^k"/> that optimizes the energy term and (2) finding a distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> with pairwise marginals matching <img alt="\widetilde{q}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D" style="vertical-align: middle;" title="\widetilde{q}"/> that optimizes the entropy term. For simplicity of notation, we use <img alt="\widetilde{q}_{ij}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D_%7Bij%7D" style="vertical-align: middle;" title="\widetilde{q}_{ij}"/> to denote <img alt="\widetilde{q}_{S}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D_%7BS%7D" style="vertical-align: middle;" title="\widetilde{q}_{S}"/>, where <img alt="S = \{i, j\}" src="https://latex.codecogs.com/png.latex?S%20%3D%20%5C%7Bi%2C%20j%5C%7D" style="vertical-align: middle;" title="S = \{i, j\}"/>. It follows that we can rewrite the Gibbs equation as</p>
<p><img alt="\log Z = \max_{\widetilde{q} \in \mathcal{M}^k} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{\widetilde{q}_{ij}}[x_i x_j] + \max_{q \given q_S = \widetilde{q}_S \forall S, \lvert S \rvert \leq k} \mathbb{H}(q) \right \}" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%20%3D%20%5Cmax_%7B%5Cwidetilde%7Bq%7D%20%5Cin%20%5Cmathcal%7BM%7D%5Ek%7D%20%5Cleft%20%5C%7B%20%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20%5Ccdot%20%5Cmathbb%7BE%7D_%7B%5Cwidetilde%7Bq%7D_%7Bij%7D%7D%5Bx_i%20x_j%5D%20%2B%20%5Cmax_%7Bq%20%5Cgiven%20q_S%20%3D%20%5Cwidetilde%7Bq%7D_S%20%5Cforall%20S%2C%20%5Clvert%20S%20%5Crvert%20%5Cleq%20k%7D%20%5Cmathbb%7BH%7D%28q%29%20%5Cright%20%5C%7D" style="vertical-align: middle;" title="\log Z = \max_{\widetilde{q} \in \mathcal{M}^k} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{\widetilde{q}_{ij}}[x_i x_j] + \max_{q \given q_S = \widetilde{q}_S \forall S, \lvert S \rvert \leq k} \mathbb{H}(q) \right \}"/></p>
<p>The first term here represents the energy on pairwise marginals, whereas the second term is the maximum entropy subject to a constraint about matching the energy distribution’s pairwise marginals. The goal of this procedure is to enlarge the polytope such that <img alt="\mathcal{M}^k" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%5Ek" style="vertical-align: middle;" title="\mathcal{M}^k"/> is a tractable set, where we can impose a polynomial number of constraints satisfied by real marginals.</p>
<p>One specific convex hierarchy that is commonly used for relaxation is the <a href="http://www.cs.princeton.edu/courses/archive/spr05/cos598B/liftproj.pdf">Sherali-Adams (SA) hierarchy</a>. Sherali-Adams will allow us to formulate the optimization of the energy term (and an approximation of the entropy term) as a convex program. We introduce the polytope <img alt="\text{SA}(k)" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%28k%29" style="vertical-align: middle;" title="\text{SA}(k)"/> for <img alt="k \geq 2" src="https://latex.codecogs.com/png.latex?k%20%5Cgeq%202" style="vertical-align: middle;" title="k \geq 2"/>, which will relax the constraints on <img alt="\vec{x}" src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D" style="vertical-align: middle;" title="\vec{x}"/> to allow for values outside of <img alt="\{-1, 1\}^n" src="https://latex.codecogs.com/png.latex?%5C%7B-1%2C%201%5C%7D%5En" style="vertical-align: middle;" title="\{-1, 1\}^n"/> in order to generate a polynomial-time solution for <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/>.</p>
<p>The Sherali-Adams hierarchy will take care of the energy term, but it remains unclear how to rewrite the entropy term in the context of the convex program. In fact, we will need approximations for <img alt="\mathbb{H}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D" style="vertical-align: middle;" title="\mathbb{H}"/> in order to accomplish this task.</p>
<p>In this talk, we’ll consider two approximations: one classical one – the <strong>Bethe entropy</strong> <img alt="\mathbb{H}_\text{BETHE}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{BETHE}"/> and one more recent one – the <strong>augmented mean-field pseudo-entropy</strong> <img alt="\mathbb{H}_\text{aMF}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{aMF}"/>.</p></li>
</ol>
<h2 id="bethe-entropy-approximation">Bethe Entropy Approximation</h2>
<p>The Bethe entropy works by pretending that the Markov random field is a tree. In fact, we can show that if the graph <em>is</em> a tree, then using the Bethe entropy approximation in the convex program defined by <img alt="\text{SA}(2)" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%282%29" style="vertical-align: middle;" title="\text{SA}(2)"/> will yield an <em>exact calculation</em> of <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/>.</p>
<p>Specifically, the Bethe entropy is defined as<br/>
<img alt="\mathbb{H}_\text{BETHE} (\widetilde{q}) = \sum_{i \sim j} \mathbb{H}(\widetilde{q}_{ij}) - \sum_i \mathbb{H}(\widetilde{q}_i) (d_i - 1)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D%20%28%5Cwidetilde%7Bq%7D%29%20%3D%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cmathbb%7BH%7D%28%5Cwidetilde%7Bq%7D_%7Bij%7D%29%20-%20%5Csum_i%20%5Cmathbb%7BH%7D%28%5Cwidetilde%7Bq%7D_i%29%20%28d_i%20-%201%29" style="vertical-align: middle;" title="\mathbb{H}_\text{BETHE} (\widetilde{q}) = \sum_{i \sim j} \mathbb{H}(\widetilde{q}_{ij}) - \sum_i \mathbb{H}(\widetilde{q}_i) (d_i - 1)"/><br/>
where <img alt="d_i" src="https://latex.codecogs.com/png.latex?d_i" style="vertical-align: middle;" title="d_i"/> is defined to be the degree of the particular vertex <img alt="i" src="https://latex.codecogs.com/png.latex?i" style="vertical-align: middle;" title="i"/>. Note that there are no marginals over sets of dimension greater than two in this expression; thus, we have a valid convex program over the polytope <img alt="\text{SA}(2)" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%282%29" style="vertical-align: middle;" title="\text{SA}(2)"/>.</p>
<p>This lemma is well-lnown, but it will be a useful warmup:</p>
<p><strong>Lemma 2</strong> Define the output of the convex program<br/>
<img alt="\log Z_{\text{BETHE}} =      \max_{\widetilde{q} \in \text{SA}(2)} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{\widetilde{q}_{ij}} [x_i x_j] + \mathbb{H}_{\text{BETHE}} (\widetilde{q}) \right \}" src="https://latex.codecogs.com/png.latex?%5Clog%20Z_%7B%5Ctext%7BBETHE%7D%7D%20%3D%20%0A%20%20%20%20%5Cmax_%7B%5Cwidetilde%7Bq%7D%20%5Cin%20%5Ctext%7BSA%7D%282%29%7D%20%5Cleft%20%5C%7B%20%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20%5Ccdot%20%5Cmathbb%7BE%7D_%7B%5Cwidetilde%7Bq%7D_%7Bij%7D%7D%20%5Bx_i%20x_j%5D%20%2B%20%5Cmathbb%7BH%7D_%7B%5Ctext%7BBETHE%7D%7D%20%28%5Cwidetilde%7Bq%7D%29%20%5Cright%20%5C%7D" style="vertical-align: middle;" title="\log Z_{\text{BETHE}} =      \max_{\widetilde{q} \in \text{SA}(2)} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{\widetilde{q}_{ij}} [x_i x_j] + \mathbb{H}_{\text{BETHE}} (\widetilde{q}) \right \}"/><br/>
On a tree, <img alt="\log Z_{\text{BETHE}} = \log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z_%7B%5Ctext%7BBETHE%7D%7D%20%3D%20%5Clog%20Z" style="vertical-align: middle;" title="\log Z_{\text{BETHE}} = \log Z"/> and the optimization objective is concave with respect to the <img alt="\text{SA}(2)" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%282%29" style="vertical-align: middle;" title="\text{SA}(2)"/> variables, so it can be solved in polynomial time.</p>
<p><strong>Proof sketch</strong> We will prove 3 claims:</p>
<ol type="1">
<li><img alt="\log Z_{\text{BETHE}} \geq \log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z_%7B%5Ctext%7BBETHE%7D%7D%20%5Cgeq%20%5Clog%20Z" style="vertical-align: middle;" title="\log Z_{\text{BETHE}} \geq \log Z"/>
<p>Since the energy term is exact, it suffices to show that for valid marginals <img alt="\widetilde{q}_S" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D_S" style="vertical-align: middle;" title="\widetilde{q}_S"/>, <img alt="\mathbb{H}_\text{BETHE}(\widetilde{q}) \geq \max_{q \given q_S = \widetilde{q}_S} \mathbb{H}(q)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D%28%5Cwidetilde%7Bq%7D%29%20%5Cgeq%20%5Cmax_%7Bq%20%5Cgiven%20q_S%20%3D%20%5Cwidetilde%7Bq%7D_S%7D%20%5Cmathbb%7BH%7D%28q%29" style="vertical-align: middle;" title="\mathbb{H}_\text{BETHE}(\widetilde{q}) \geq \max_{q \given q_S = \widetilde{q}_S} \mathbb{H}(q)"/>. This can be done by re-writing <img alt="\mathbb{H}_\text{BETHE}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{BETHE}"/> and using a property of conditional entropy, namely that <img alt="\mathbb{H}(X,Y) = \mathbb{H}(X) + \mathbb{H}(Y|X)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D%28X%2CY%29%20%3D%20%5Cmathbb%7BH%7D%28X%29%20%2B%20%5Cmathbb%7BH%7D%28Y%7CX%29" style="vertical-align: middle;" title="\mathbb{H}(X,Y) = \mathbb{H}(X) + \mathbb{H}(Y|X)"/>.</p></li>
<li>For trees, <img alt="\log Z_\text{BETHE}" src="https://latex.codecogs.com/png.latex?%5Clog%20Z_%5Ctext%7BBETHE%7D" style="vertical-align: middle;" title="\log Z_\text{BETHE}"/> is concave in the <img alt="\text{SA}(2)" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%282%29" style="vertical-align: middle;" title="\text{SA}(2)"/> variables.
<p>This proof was only sketched in class but is similar to the usual proof of concavity of entropy.</p></li>
<li>For trees, we can <em>round</em> a solution to a proper distribution over <img alt="\{-1, 1\}^n" src="https://latex.codecogs.com/png.latex?%5C%7B-1%2C%201%5C%7D%5En" style="vertical-align: middle;" title="\{-1, 1\}^n"/> which attains the same value for the original <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/> optimization.
<p>The distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> that we will produce is<br/>
<img alt="q(\vec{x}) = \prod_i q(x_i \given x_{\text{pa}(i)})" src="https://latex.codecogs.com/png.latex?q%28%5Cvec%7Bx%7D%29%20%3D%20%5Cprod_i%20q%28x_i%20%5Cgiven%20x_%7B%5Ctext%7Bpa%7D%28i%29%7D%29" style="vertical-align: middle;" title="q(\vec{x}) = \prod_i q(x_i \given x_{\text{pa}(i)})"/><br/>
where we start at the root and keep sampling down the tree. Based on the tree structure, <img alt="\mathbb{H}(q) = \mathbb{H}_\text{BETHE}(q)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D%28q%29%20%3D%20%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D%28q%29" style="vertical-align: middle;" title="\mathbb{H}(q) = \mathbb{H}_\text{BETHE}(q)"/>. The energy is also the same since <img alt="q_{ij}(x_i, x_j) = \widetilde{q}_{ij}(x_i, x_j)" src="https://latex.codecogs.com/png.latex?q_%7Bij%7D%28x_i%2C%20x_j%29%20%3D%20%5Cwidetilde%7Bq%7D_%7Bij%7D%28x_i%2C%20x_j%29" style="vertical-align: middle;" title="q_{ij}(x_i, x_j) = \widetilde{q}_{ij}(x_i, x_j)"/>. Therefore, since both terms in the equation are equal to the respective terms in the partition function, we get that the equation must equal the partition function.</p></li>
</ol>
<p><strong>Remarks:</strong></p>
<ol type="1">
<li><img alt="\mathbb{H}_\text{BETHE}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{BETHE}"/> is commonly used on graphs that are not trees.</li>
<li>However, for non-trees, the optimization is often no longer concave.</li>
<li>Therefore, the value obtained by the output equation is neither an upper or lower bound.</li>
<li>The fixed points of the <a href="https://en.wikipedia.org/wiki/Belief_propagation">belief propagation algorithm</a> give a 1-1 correspondence with the stationary points of the Bethe objective. (See a classical paper by <a href="http://qwone.com/~jason/trg/papers/yedidia-belief-01.pdf">Yedidia et al.</a>.)</li>
</ol>
<h2 id="augmented-mean-field-pseudo-entropy">Augmented Mean Field Pseudo-Entropy</h2>
<p>For this approximation, we will focus on dense graphs: namely graphs in which each vertex has degree <img alt="\geq \Delta \cdot n" src="https://latex.codecogs.com/png.latex?%5Cgeq%20%5CDelta%20%5Ccdot%20n" style="vertical-align: middle;" title="\geq \Delta \cdot n"/> for some <img alt="\Delta &gt; 0" src="https://latex.codecogs.com/png.latex?%5CDelta%20%3E%200" style="vertical-align: middle;" title="\Delta &gt; 0"/>. To simplify things, we focus on distributions of the form<br/>
<img alt=" p(\vec{x}) \propto \exp \left(\sum_{i \sim j} \pm J x_i x_j\right) " src="https://latex.codecogs.com/png.latex?%0Ap%28%5Cvec%7Bx%7D%29%20%5Cpropto%20%5Cexp%20%5Cleft%28%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cpm%20J%20x_i%20x_j%5Cright%29%0A" style="vertical-align: middle;" title=" p(\vec{x}) \propto \exp \left(\sum_{i \sim j} \pm J x_i x_j\right) "/><br/>
where <img alt="J" src="https://latex.codecogs.com/png.latex?J" style="vertical-align: middle;" title="J"/> is some positive constant parameter. The Bethe entropy approximation is undesirable in the dense case, because we cannot bound its error on non-trees. Instead, we proved the following theorem.</p>
<p><strong>Theorem</strong> <a href="http://arxiv.org/abs/1607.03183">(Risteski ’16)</a></p>
<p>For a dense graph with parameter <img alt="\Delta" src="https://latex.codecogs.com/png.latex?%5CDelta" style="vertical-align: middle;" title="\Delta"/>, there exists an outer approximation based on <img alt="\text{SA}(\frac{1}{\epsilon^2\Delta})" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%28%5Cfrac%7B1%7D%7B%5Cepsilon%5E2%5CDelta%7D%29" style="vertical-align: middle;" title="\text{SA}(\frac{1}{\epsilon^2\Delta})"/> and an entropy proxy which achieves a <img alt="O(J \epsilon n^2 \Delta)" src="https://latex.codecogs.com/png.latex?O%28J%20%5Cepsilon%20n%5E2%20%5CDelta%29" style="vertical-align: middle;" title="O(J \epsilon n^2 \Delta)"/> additive approximation to <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/> for some <img alt="\epsilon &gt; 0" src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%3E%200" style="vertical-align: middle;" title="\epsilon &gt; 0"/>. There is an algorithm with runtime <img alt="O(\frac{1}{\epsilon^2\Delta})" src="https://latex.codecogs.com/png.latex?O%28%5Cfrac%7B1%7D%7B%5Cepsilon%5E2%5CDelta%7D%29" style="vertical-align: middle;" title="O(\frac{1}{\epsilon^2\Delta})"/>.</p>
<p>To parse the theorem statement, consider the potential regimes for <img alt="J" src="https://latex.codecogs.com/png.latex?J" style="vertical-align: middle;" title="J"/>:</p>
<ol type="1">
<li><img alt="J &gt;&gt; \frac{1}{n} \rightarrow" src="https://latex.codecogs.com/png.latex?J%20%3E%3E%20%5Cfrac%7B1%7D%7Bn%7D%20%5Crightarrow" style="vertical-align: middle;" title="J &gt;&gt; \frac{1}{n} \rightarrow"/> One can ignore the entropy portion and solve for the energy portion that dominates. (So the guarantee is not that interesting.)</li>
<li><img alt="J &lt;&lt; \frac{1}{n} \rightarrow" src="https://latex.codecogs.com/png.latex?J%20%3C%3C%20%5Cfrac%7B1%7D%7Bn%7D%20%5Crightarrow" style="vertical-align: middle;" title="J &lt;&lt; \frac{1}{n} \rightarrow"/> MCMC methods give a <img alt="(1+\epsilon)" src="https://latex.codecogs.com/png.latex?%281%2B%5Cepsilon%29" style="vertical-align: middle;" title="(1+\epsilon)"/>-factor approximation in time poly<img alt="(n, \frac{1}{\epsilon})" src="https://latex.codecogs.com/png.latex?%28n%2C%20%5Cfrac%7B1%7D%7B%5Cepsilon%7D%29" style="vertical-align: middle;" title="(n, \frac{1}{\epsilon})"/>. (It’s not clear if the methods suggested here can give such a guarantee – this is an interesting problem to explore.)</li>
<li><img alt="J = \Theta(\frac{1}{n}) \rightarrow" src="https://latex.codecogs.com/png.latex?J%20%3D%20%5CTheta%28%5Cfrac%7B1%7D%7Bn%7D%29%20%5Crightarrow" style="vertical-align: middle;" title="J = \Theta(\frac{1}{n}) \rightarrow"/> MCMC mixes slowly, but there is no other way to get a comparable guarantee. This is the interesting regime!</li>
</ol>
<p><strong>Proof Sketch</strong> The proof strategy is as follows: We will formulate a convex program under the SA(<img alt="k" src="https://latex.codecogs.com/png.latex?k" style="vertical-align: middle;" title="k"/>) relaxation that can return a solution <img alt="\widetilde{q}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D" style="vertical-align: middle;" title="\widetilde{q}"/> in polynomial time with value <img alt="\log \tilde{Z}" src="https://latex.codecogs.com/png.latex?%5Clog%20%5Ctilde%7BZ%7D" style="vertical-align: middle;" title="\log \tilde{Z}"/>. Note that this value of the relaxation will be an upperbound on the true partition function value <img alt="\log Z^*" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%5E%2A" style="vertical-align: middle;" title="\log Z^*"/>. The convex program solution <img alt="\widetilde{q}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D" style="vertical-align: middle;" title="\widetilde{q}"/> may not be a valid probability distribution, so from this, we will construct a “rounded solution’’ – an actual distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> with value <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/>. It follows that<br/>
<img alt="\log \tilde{Z} \geq \log Z^* \geq \log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20%5Ctilde%7BZ%7D%20%5Cgeq%20%5Clog%20Z%5E%2A%20%5Cgeq%20%5Clog%20Z" style="vertical-align: middle;" title="\log \tilde{Z} \geq \log Z^* \geq \log Z"/></p>
<p>We will then aim to put a bound on the gap between <img alt="\log \tilde{Z}" src="https://latex.codecogs.com/png.latex?%5Clog%20%5Ctilde%7BZ%7D" style="vertical-align: middle;" title="\log \tilde{Z}"/> and <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/>; namely, that<br/>
<img alt="    \log Z \geq \log \tilde{Z} - \epsilon \cdot n^2 \cdot J \cdot \Delta " src="https://latex.codecogs.com/png.latex?%20%20%20%20%5Clog%20Z%20%5Cgeq%20%5Clog%20%5Ctilde%7BZ%7D%20-%20%5Cepsilon%20%5Ccdot%20n%5E2%20%5Ccdot%20J%20%5Ccdot%20%5CDelta%0A" style="vertical-align: middle;" title="    \log Z \geq \log \tilde{Z} - \epsilon \cdot n^2 \cdot J \cdot \Delta "/><br/>
or equivalently,<br/>
<img alt="    \sum_{i \sim j} \mathbb{E}_q[x_i x_j] \cdot J + \mathbb{H}(q) \geq \sum_{i \sim j} \mathbb{E}_{\widetilde{q}}[x_i x_j] \cdot J + \mathbb{H}_\text{aMF}(\widetilde{q}) - \epsilon n^2 J \Delta " src="https://latex.codecogs.com/png.latex?%20%20%20%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cmathbb%7BE%7D_q%5Bx_i%20x_j%5D%20%5Ccdot%20J%20%2B%20%5Cmathbb%7BH%7D%28q%29%20%5Cgeq%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cmathbb%7BE%7D_%7B%5Cwidetilde%7Bq%7D%7D%5Bx_i%20x_j%5D%20%5Ccdot%20J%20%2B%20%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D%28%5Cwidetilde%7Bq%7D%29%20-%20%5Cepsilon%20n%5E2%20J%20%5CDelta%20" style="vertical-align: middle;" title="    \sum_{i \sim j} \mathbb{E}_q[x_i x_j] \cdot J + \mathbb{H}(q) \geq \sum_{i \sim j} \mathbb{E}_{\widetilde{q}}[x_i x_j] \cdot J + \mathbb{H}_\text{aMF}(\widetilde{q}) - \epsilon n^2 J \Delta "/><br/>
This equivalently places a bound on the optimality gap between <img alt="\log \tilde{Z}" src="https://latex.codecogs.com/png.latex?%5Clog%20%5Ctilde%7BZ%7D" style="vertical-align: middle;" title="\log \tilde{Z}"/> and <img alt="\log Z^*" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%5E%2A" style="vertical-align: middle;" title="\log Z^*"/>, thereby proving the theorem. Here are the main components of the proof:</p>
<p><strong>1. Entropy Proxy</strong></p>
<p>To approximate <img alt="\mathbb{H}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D" style="vertical-align: middle;" title="\mathbb{H}"/>, we will use the pseudo-augmented mean field entropy approximation <img alt="\mathbb{H}_\text{aMF}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{aMF}"/>. This is defined as</p>
<p><img alt=" \mathbb{H}_\text{aMF}(q) = \mathbb{I}n_{|S| \le k} \{ \overbrace{\mathbb{H}(q_S)}^\text{entropy of $S$} + \overbrace{\sum_{i \notin S} \mathbb{H}(q_i|q_S)}^\text{entropy of everything else} \} " src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D%28q%29%20%3D%20%5Cmathbb%7BI%7Dn_%7B%7CS%7C%20%5Cle%20k%7D%20%5C%7B%20%5Coverbrace%7B%5Cmathbb%7BH%7D%28q_S%29%7D%5E%5Ctext%7Bentropy%20of%20%24S%24%7D%20%2B%20%5Coverbrace%7B%5Csum_%7Bi%20%5Cnotin%20S%7D%20%5Cmathbb%7BH%7D%28q_i%7Cq_S%29%7D%5E%5Ctext%7Bentropy%20of%20everything%20else%7D%20%5C%7D%0A" style="vertical-align: middle;" title=" \mathbb{H}_\text{aMF}(q) = \mathbb{I}n_{|S| \le k} \{ \overbrace{\mathbb{H}(q_S)}^\text{entropy of $S$} + \overbrace{\sum_{i \notin S} \mathbb{H}(q_i|q_S)}^\text{entropy of everything else} \} "/></p>
<p>Using <img alt="\mathbb{H}_\text{aMF}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{aMF}"/>, we can write a convex program under SA(<img alt="k" src="https://latex.codecogs.com/png.latex?k" style="vertical-align: middle;" title="k"/>) that provides a relaxation for optimizing <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/>. Specifically, we will let <img alt="k = O(1 / (\Delta \cdot \epsilon^2))" src="https://latex.codecogs.com/png.latex?k%20%3D%20O%281%20%2F%20%28%5CDelta%20%5Ccdot%20%5Cepsilon%5E2%29%29" style="vertical-align: middle;" title="k = O(1 / (\Delta \cdot \epsilon^2))"/>. Let <img alt="\widetilde{q}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D" style="vertical-align: middle;" title="\widetilde{q}"/> be the outputed solution to this relaxation. We define the <em>rounded solution</em> as<br/>
<img alt="q(\vec{x}) = \widetilde{q}_S(\vec{x}_S) \cdot \prod_{i \notin S} \widetilde{q}_S(x_i \given \vec{x}_S)" src="https://latex.codecogs.com/png.latex?q%28%5Cvec%7Bx%7D%29%20%3D%20%5Cwidetilde%7Bq%7D_S%28%5Cvec%7Bx%7D_S%29%20%5Ccdot%20%5Cprod_%7Bi%20%5Cnotin%20S%7D%20%5Cwidetilde%7Bq%7D_S%28x_i%20%5Cgiven%20%5Cvec%7Bx%7D_S%29" style="vertical-align: middle;" title="q(\vec{x}) = \widetilde{q}_S(\vec{x}_S) \cdot \prod_{i \notin S} \widetilde{q}_S(x_i \given \vec{x}_S)"/><br/>
Using the chain rule for entropy, we can show that<br/>
<img alt="\mathbb{H}(q) \geq \mathbb{H}_\text{aMF}(\widetilde{q})" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D%28q%29%20%5Cgeq%20%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D%28%5Cwidetilde%7Bq%7D%29" style="vertical-align: middle;" title="\mathbb{H}(q) \geq \mathbb{H}_\text{aMF}(\widetilde{q})"/></p>
<p><strong>2. Rounding Scheme</strong> The above distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> is actually the same distribution that correlation rounding, as introduced by a seminal paper of <a href="https://arxiv.org/abs/1104.4680">Barak, Raghavendra, and Steurer</a>, produces. By using definition of mutual information <img alt="\mathbb{I}(X, Y)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BI%7D%28X%2C%20Y%29" style="vertical-align: middle;" title="\mathbb{I}(X, Y)"/> and Pinsker’s Theorem showing that <img alt="\mathrm{\mathbb{C}ov}(X, Y) = O(\sqrt{\mathbb{I}(X, Y)})" src="https://latex.codecogs.com/png.latex?%5Cmathrm%7B%5Cmathbb%7BC%7Dov%7D%28X%2C%20Y%29%20%3D%20O%28%5Csqrt%7B%5Cmathbb%7BI%7D%28X%2C%20Y%29%7D%29" style="vertical-align: middle;" title="\mathrm{\mathbb{C}ov}(X, Y) = O(\sqrt{\mathbb{I}(X, Y)})"/>, we can work through some algebra to show that</p>
<p><img alt="\sum_{i \sim j} \mathbb{E}_q[x_i x_j] \cdot J \geq \sum_{i \sim j} \mathbb{E}_{\widetilde{q}}[x_i x_j] \cdot J - \epsilon n^2 J \Delta" src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cmathbb%7BE%7D_q%5Bx_i%20x_j%5D%20%5Ccdot%20J%20%5Cgeq%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cmathbb%7BE%7D_%7B%5Cwidetilde%7Bq%7D%7D%5Bx_i%20x_j%5D%20%5Ccdot%20J%20-%20%5Cepsilon%20n%5E2%20J%20%5CDelta" style="vertical-align: middle;" title="\sum_{i \sim j} \mathbb{E}_q[x_i x_j] \cdot J \geq \sum_{i \sim j} \mathbb{E}_{\widetilde{q}}[x_i x_j] \cdot J - \epsilon n^2 J \Delta"/></p>
<p>Then, putting together the entropy and energy effects, we can prove the main theorem.</p>
<p><strong>Remarks:</strong></p>
<ol type="1">
<li>The above proof easily extends to a more general notion of density, as well as to sparse graphs with “low-rank’’ spectral structure. See <a href="http://arxiv.org/abs/1607.03183">the paper</a> for more information.</li>
<li>In fact, with more work, one can extract guarantees on the the best <strong>mean field</strong> approximation to the Gibbs distribution in the dense regime. (This intuitively can be done as the distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> produced above is almost mean-field, except for the set <img alt="S" src="https://latex.codecogs.com/png.latex?S" style="vertical-align: middle;" title="S"/>). This is interesting, since as we mentioned, the quality of the mean-field approximation is usually difficult to characterize. (Moreover, the procedure is algorithmic.) See the recent work of <a href="https://arxiv.org/abs/1808.07226">Jain, Koehler and Risteski ’18</a> for more details and results.</li>
</ol>
<h1 id="taylor-series-approximation">Taylor Series Approximation</h1>
<p>Another method of calculating the partition function involves Taylor expanding its logarithm around a cleverly selected point. This approach was first developed by <a href="https://arxiv.org/abs/1405.1974">Barvinok ’14</a>, for the purpose of computing the partition function of cliques in a graph. The mathematical techniques developed in this paper can be naturally extended to evaluate a variety of partition functions, including that of the ferromagnetic Ising model. We will use this example to illustrate the general idea of the Taylor expansion technique.</p>
<p>The goal here is to devise a deterministic, fully polynomial-time approximation scheme (an FPTAS) for evaluating the partition function of the ferromagnetic Ising model. This means that the the algorithm must run in poly(<img alt="n, \frac{1}{\epsilon}" src="https://latex.codecogs.com/png.latex?n%2C%20%5Cfrac%7B1%7D%7B%5Cepsilon%7D" style="vertical-align: middle;" title="n, \frac{1}{\epsilon}"/>) and be correct within a multiplicative factor of <img alt="1 + \epsilon" src="https://latex.codecogs.com/png.latex?1%20%2B%20%5Cepsilon" style="vertical-align: middle;" title="1 + \epsilon"/>. We will work in the general case where the Hamiltonian includes an external magnetic field term, as well as the neighboring spin interaction terms. Using logarithmic identities, we can re-write the partition function slightly differently from last time:</p>
<p><img alt=" Z(\lambda) = \sum_{x \in \{ \pm 1 \}^n} \lambda^{|{1: x_i = 1}|} \beta^{|E(x)|}" src="https://latex.codecogs.com/png.latex?%20Z%28%5Clambda%29%20%3D%20%5Csum_%7Bx%20%5Cin%20%5C%7B%20%5Cpm%201%20%5C%7D%5En%7D%20%5Clambda%5E%7B%7C%7B1%3A%20x_i%20%3D%201%7D%7C%7D%20%5Cbeta%5E%7B%7CE%28x%29%7C%7D" style="vertical-align: middle;" title=" Z(\lambda) = \sum_{x \in \{ \pm 1 \}^n} \lambda^{|{1: x_i = 1}|} \beta^{|E(x)|}"/><br/>
Here, <img alt="\lambda" src="https://latex.codecogs.com/png.latex?%5Clambda" style="vertical-align: middle;" title="\lambda"/> is called the vertex activity (or external field), and characterizes the likelihood of a vertex to be in the + configuration. Additionally, <img alt="\beta \geq 0" src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Cgeq%200" style="vertical-align: middle;" title="\beta \geq 0"/> is referred to as the edge activity, and characterizes the propensity of a vertex to agree with its neighbors. The ferromagnetic regime (where agreement of spins is favored) corresponds to <img alt="\beta &lt; 1" src="https://latex.codecogs.com/png.latex?%5Cbeta%20%3C%201" style="vertical-align: middle;" title="\beta &lt; 1"/>. <img alt="|E(x)|" src="https://latex.codecogs.com/png.latex?%7CE%28x%29%7C" style="vertical-align: middle;" title="|E(x)|"/> denotes the number of edge cut in the graph, which is equivalent to the number of neighboring pairs with opposite spins.</p>
<p>As one final disclaimer, the approximation scheme presented below is not valid for <img alt="| \lambda| = 1" src="https://latex.codecogs.com/png.latex?%7C%20%5Clambda%7C%20%3D%201" style="vertical-align: middle;" title="| \lambda| = 1"/>, which corresponds to the zero-magnetic field case. Randomized algorithms exist which can handle this case.</p>
<p>The general idea here is to hold one parameter fixed (<img alt="\beta" src="https://latex.codecogs.com/png.latex?%5Cbeta" style="vertical-align: middle;" title="\beta"/> in our case), and express the logarithm of the partition function as a Taylor series in the other parameter (<img alt="\lambda" src="https://latex.codecogs.com/png.latex?%5Clambda" style="vertical-align: middle;" title="\lambda"/>). From a physics standpoint, the Taylor expansion around <img alt="\lambda = 0" src="https://latex.codecogs.com/png.latex?%5Clambda%20%3D%200" style="vertical-align: middle;" title="\lambda = 0"/> tells us how the partition function is changing as a function of the magnetic field. Without loss of generality, we focus on the case where <img alt="| \lambda | &lt; 1" src="https://latex.codecogs.com/png.latex?%7C%20%5Clambda%20%7C%20%3C%201" style="vertical-align: middle;" title="| \lambda | &lt; 1"/>. This simplification can be justified by a simple symmetry argument. Consider <img alt="\bar{x}" src="https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D" style="vertical-align: middle;" title="\bar{x}"/> as the inverse of <img alt="x" src="https://latex.codecogs.com/png.latex?x" style="vertical-align: middle;" title="x"/> where all the 1’s (spin-up) are flipped to -1s (spin-down) and vice-versa. The partition function of this flipped system can be related to the original system by a constant factor:</p>
<p><img alt="\begin{align*} \sum_x \beta^{|E(x)|} \lambda^{|\{1: x_i = 1\}|} &amp;= \sum_x \beta^{|E(\bar{x})|} \lambda^{n - |\{1: \bar{x}_i = 1\}|} \\ &amp;= \lambda^{n} \sum_x \beta^{|E(\bar{x})|} \left(\frac{1}{\lambda}\right)^{|\{1: \bar{x}_i = 1\}|} \end{align*}" src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%2A%7D%0A%5Csum_x%20%5Cbeta%5E%7B%7CE%28x%29%7C%7D%20%5Clambda%5E%7B%7C%5C%7B1%3A%20x_i%20%3D%201%5C%7D%7C%7D%20%26%3D%20%5Csum_x%20%5Cbeta%5E%7B%7CE%28%5Cbar%7Bx%7D%29%7C%7D%20%5Clambda%5E%7Bn%20-%20%7C%5C%7B1%3A%20%5Cbar%7Bx%7D_i%20%3D%201%5C%7D%7C%7D%20%5C%5C%0A%26%3D%20%5Clambda%5E%7Bn%7D%20%5Csum_x%20%5Cbeta%5E%7B%7CE%28%5Cbar%7Bx%7D%29%7C%7D%20%5Cleft%28%5Cfrac%7B1%7D%7B%5Clambda%7D%5Cright%29%5E%7B%7C%5C%7B1%3A%20%5Cbar%7Bx%7D_i%20%3D%201%5C%7D%7C%7D%0A%5Cend%7Balign%2A%7D" style="vertical-align: middle;" title="\begin{align*} \sum_x \beta^{|E(x)|} \lambda^{|\{1: x_i = 1\}|} &amp;= \sum_x \beta^{|E(\bar{x})|} \lambda^{n - |\{1: \bar{x}_i = 1\}|} \\ &amp;= \lambda^{n} \sum_x \beta^{|E(\bar{x})|} \left(\frac{1}{\lambda}\right)^{|\{1: \bar{x}_i = 1\}|} \end{align*}"/></p>
<p>This holds because the number of cut edges remains constant when the values are flipped. Since these two partition functions are related by this factor constant in <img alt="\lambda" src="https://latex.codecogs.com/png.latex?%5Clambda" style="vertical-align: middle;" title="\lambda"/>, for any model with <img alt="\lambda &gt; 1" src="https://latex.codecogs.com/png.latex?%5Clambda%20%3E%201" style="vertical-align: middle;" title="\lambda &gt; 1"/>, we can simply consider the flipped graph <img alt="\bar{x}" src="https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D" style="vertical-align: middle;" title="\bar{x}"/> and use the above relation.</p>
<p>For our purposes, it will be more convenient to approximate the logarithm of the partition function, because a multiplicative <img alt="(1 + \epsilon)" src="https://latex.codecogs.com/png.latex?%281%20%2B%20%5Cepsilon%29" style="vertical-align: middle;" title="(1 + \epsilon)"/> approximation of <img alt="Z(\lambda)" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29" style="vertical-align: middle;" title="Z(\lambda)"/> corresponds to an <img alt="\mathcal{O}(\epsilon)" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D%28%5Cepsilon%29" style="vertical-align: middle;" title="\mathcal{O}(\epsilon)"/> additive approximation of <img alt="\log Z(\lambda)" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%28%5Clambda%29" style="vertical-align: middle;" title="\log Z(\lambda)"/>. In fact, if we allow the partition function to be complex, then we can easily show that an additive bound of <img alt="\frac{\epsilon}{4}" src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cepsilon%7D%7B4%7D" style="vertical-align: middle;" title="\frac{\epsilon}{4}"/> for <img alt="\log Z(\lambda)" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%28%5Clambda%29" style="vertical-align: middle;" title="\log Z(\lambda)"/> guarantees a multiplicative bound of <img alt="(1+\epsilon)" src="https://latex.codecogs.com/png.latex?%281%2B%5Cepsilon%29" style="vertical-align: middle;" title="(1+\epsilon)"/> for <img alt="Z(\lambda)" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29" style="vertical-align: middle;" title="Z(\lambda)"/>.</p>
<p>For notational convenience we define:<br/>
<img alt=" \log Z(\lambda) = f(\lambda)" src="https://latex.codecogs.com/png.latex?%20%5Clog%20Z%28%5Clambda%29%20%3D%20f%28%5Clambda%29" style="vertical-align: middle;" title=" \log Z(\lambda) = f(\lambda)"/><br/>
Thus, the Taylor expansion of <img alt="f(\lambda)" src="https://latex.codecogs.com/png.latex?f%28%5Clambda%29" style="vertical-align: middle;" title="f(\lambda)"/> around <img alt="\lambda = 0" src="https://latex.codecogs.com/png.latex?%5Clambda%20%3D%200" style="vertical-align: middle;" title="\lambda = 0"/> if given by:<br/>
<img alt="f(\lambda) = \sum_j f^{(j)} (0) \frac{\lambda^j}{j!}" src="https://latex.codecogs.com/png.latex?f%28%5Clambda%29%20%3D%20%5Csum_j%20f%5E%7B%28j%29%7D%20%280%29%20%5Cfrac%7B%5Clambda%5Ej%7D%7Bj%21%7D" style="vertical-align: middle;" title="f(\lambda) = \sum_j f^{(j)} (0) \frac{\lambda^j}{j!}"/></p>
<p>The big question now is: Can we get a good approximation from just the lower order terms of the Taylor expansion for <img alt="f(\lambda)" src="https://latex.codecogs.com/png.latex?f%28%5Clambda%29" style="vertical-align: middle;" title="f(\lambda)"/>? Equivalently, can we can bound the sum of the higher order terms at <img alt="\frac{\epsilon}{4}" src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cepsilon%7D%7B4%7D" style="vertical-align: middle;" title="\frac{\epsilon}{4}"/>? Additionally, we still must address the question of how to actually calculate the lower order terms.</p>
<p>To answer these questions, we make simple observations about the derivatives of <img alt="f" src="https://latex.codecogs.com/png.latex?f" style="vertical-align: middle;" title="f"/> in relation to the derivatives of <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/>.<br/>
<img alt="\begin{align*}     f'(\lambda) &amp;= \frac{1}{Z(\lambda)} Z'(\lambda) \\     Z'(\lambda) &amp;= f'(\lambda) Z(\lambda) \\     Z^{(m)} (\lambda) &amp;= \sum_{j=0}^{m-1} {m-1 \choose j} Z^{(j)} (\lambda) f^{(m-j)} (\lambda) \end{align*}" src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%2A%7D%0A%20%20%20%20f%27%28%5Clambda%29%20%26%3D%20%5Cfrac%7B1%7D%7BZ%28%5Clambda%29%7D%20Z%27%28%5Clambda%29%20%5C%5C%0A%20%20%20%20Z%27%28%5Clambda%29%20%26%3D%20f%27%28%5Clambda%29%20Z%28%5Clambda%29%20%5C%5C%0A%20%20%20%20Z%5E%7B%28m%29%7D%20%28%5Clambda%29%20%26%3D%20%5Csum_%7Bj%3D0%7D%5E%7Bm-1%7D%20%7Bm-1%20%5Cchoose%20j%7D%20Z%5E%7B%28j%29%7D%20%28%5Clambda%29%20f%5E%7B%28m-j%29%7D%20%28%5Clambda%29%0A%5Cend%7Balign%2A%7D" style="vertical-align: middle;" title="\begin{align*}     f'(\lambda) &amp;= \frac{1}{Z(\lambda)} Z'(\lambda) \\     Z'(\lambda) &amp;= f'(\lambda) Z(\lambda) \\     Z^{(m)} (\lambda) &amp;= \sum_{j=0}^{m-1} {m-1 \choose j} Z^{(j)} (\lambda) f^{(m-j)} (\lambda) \end{align*}"/><br/>
The last equation just comes from repeated application of the product rule. Using these relations, we can solve for the first <img alt="m" src="https://latex.codecogs.com/png.latex?m" style="vertical-align: middle;" title="m"/> derivatives of <img alt="f" src="https://latex.codecogs.com/png.latex?f" style="vertical-align: middle;" title="f"/> if we have the first <img alt="m" src="https://latex.codecogs.com/png.latex?m" style="vertical-align: middle;" title="m"/> derivatives of <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/> using a triangular linear system of equations. As <img alt="Z(0) = 1" src="https://latex.codecogs.com/png.latex?Z%280%29%20%3D%201" style="vertical-align: middle;" title="Z(0) = 1"/>, we see that the system is non-degenerate.</p>
<p>Note, <img alt="Z(\lambda)" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29" style="vertical-align: middle;" title="Z(\lambda)"/> is a n-degree polynomial with leading coefficient of 1 and constant coefficient of 1 (corresponding to the configurations where all vertices are positive / negative respectively). Using this form, we can re-write the partition function in terms of its <img alt="n" src="https://latex.codecogs.com/png.latex?n" style="vertical-align: middle;" title="n"/> (possibly complex) roots <img alt="r_i" src="https://latex.codecogs.com/png.latex?r_i" style="vertical-align: middle;" title="r_i"/>:</p>
<p><img alt="Z(\lambda) = \prod_{i=1}^n \left(1-\frac{\lambda}{r_i} \right) \text{ for some } r_i" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29%20%3D%20%5Cprod_%7Bi%3D1%7D%5En%20%5Cleft%281-%5Cfrac%7B%5Clambda%7D%7Br_i%7D%20%5Cright%29%20%5Ctext%7B%20for%20some%20%7D%20r_i" style="vertical-align: middle;" title="Z(\lambda) = \prod_{i=1}^n \left(1-\frac{\lambda}{r_i} \right) \text{ for some } r_i"/></p>
<p><img alt="f(\lambda) = \log Z(\lambda) = \sum_{i=1}^n \log \left(1-\frac{\lambda}{r_i}\right)" src="https://latex.codecogs.com/png.latex?f%28%5Clambda%29%20%3D%20%5Clog%20Z%28%5Clambda%29%20%3D%20%5Csum_%7Bi%3D1%7D%5En%20%5Clog%20%5Cleft%281-%5Cfrac%7B%5Clambda%7D%7Br_i%7D%5Cright%29" style="vertical-align: middle;" title="f(\lambda) = \log Z(\lambda) = \sum_{i=1}^n \log \left(1-\frac{\lambda}{r_i}\right)"/></p>
<p><img alt="= - \sum_{i=1}^n \sum_{j=1}^{\infty} \frac{1}{j} \left(\frac{\lambda}{r_i} \right)^j" src="https://latex.codecogs.com/png.latex?%3D%20-%20%5Csum_%7Bi%3D1%7D%5En%20%5Csum_%7Bj%3D1%7D%5E%7B%5Cinfty%7D%20%5Cfrac%7B1%7D%7Bj%7D%20%5Cleft%28%5Cfrac%7B%5Clambda%7D%7Br_i%7D%20%5Cright%29%5Ej" style="vertical-align: middle;" title="= - \sum_{i=1}^n \sum_{j=1}^{\infty} \frac{1}{j} \left(\frac{\lambda}{r_i} \right)^j"/></p>
<p>Supposing we keep the first <img alt="m" src="https://latex.codecogs.com/png.latex?m" style="vertical-align: middle;" title="m"/> terms, the error is given bounded by:</p>
<p><img alt="|f(\lambda) + \sum_{i=1}^n \sum_{j=1}^{m} \frac{1}{j} (\frac{\lambda}{r_i})^j| \leq  \sum_{i=1}^n \sum_{j=m+1}^{\infty} \frac{1}{|r_i|}\frac{|\lambda|^j}{j} \leq \sum_{i=1}^n \frac{|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}\frac{1}{|r_i|}" src="https://latex.codecogs.com/png.latex?%7Cf%28%5Clambda%29%20%2B%20%5Csum_%7Bi%3D1%7D%5En%20%5Csum_%7Bj%3D1%7D%5E%7Bm%7D%20%5Cfrac%7B1%7D%7Bj%7D%20%28%5Cfrac%7B%5Clambda%7D%7Br_i%7D%29%5Ej%7C%20%5Cleq%20%20%5Csum_%7Bi%3D1%7D%5En%20%5Csum_%7Bj%3Dm%2B1%7D%5E%7B%5Cinfty%7D%20%5Cfrac%7B1%7D%7B%7Cr_i%7C%7D%5Cfrac%7B%7C%5Clambda%7C%5Ej%7D%7Bj%7D%20%5Cleq%20%5Csum_%7Bi%3D1%7D%5En%20%5Cfrac%7B%7C%5Clambda%7C%5E%7Bm%2B1%7D%7D%7B%28m%2B1%29%281-%7C%5Clambda%7C%29%7D%5Cfrac%7B1%7D%7B%7Cr_i%7C%7D" style="vertical-align: middle;" title="|f(\lambda) + \sum_{i=1}^n \sum_{j=1}^{m} \frac{1}{j} (\frac{\lambda}{r_i})^j| \leq  \sum_{i=1}^n \sum_{j=m+1}^{\infty} \frac{1}{|r_i|}\frac{|\lambda|^j}{j} \leq \sum_{i=1}^n \frac{|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}\frac{1}{|r_i|}"/></p>
<p>Here we can invoke the Lee-Yang theorem, which tells us that the zeroes of <img alt="Z(\lambda)" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29" style="vertical-align: middle;" title="Z(\lambda)"/> for a system with ferromagnetic interactions lie on the unit circle of the complex plane. So the Lee-Yang theorem guarantees that <img alt="|r_i| = 1" src="https://latex.codecogs.com/png.latex?%7Cr_i%7C%20%3D%201" style="vertical-align: middle;" title="|r_i| = 1"/>, and we see that the error due to truncation is ultimately bounded by:</p>
<p><img alt="\frac{n|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}" src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bn%7C%5Clambda%7C%5E%7Bm%2B1%7D%7D%7B%28m%2B1%29%281-%7C%5Clambda%7C%29%7D" style="vertical-align: middle;" title="\frac{n|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}"/></p>
<p>Now by the previous symmetry argument, we can assume that <img alt="\lambda &lt; 1" src="https://latex.codecogs.com/png.latex?%5Clambda%20%3C%201" style="vertical-align: middle;" title="\lambda &lt; 1"/>. Thus, to achieve an error bound of <img alt="\frac{\epsilon}{4}" src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cepsilon%7D%7B4%7D" style="vertical-align: middle;" title="\frac{\epsilon}{4}"/>, we must have:<br/>
<img alt="\frac{\epsilon}{4} &gt; \frac{n|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}" src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cepsilon%7D%7B4%7D%20%3E%20%5Cfrac%7Bn%7C%5Clambda%7C%5E%7Bm%2B1%7D%7D%7B%28m%2B1%29%281-%7C%5Clambda%7C%29%7D" style="vertical-align: middle;" title="\frac{\epsilon}{4} &gt; \frac{n|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}"/><br/>
Rearranging terms and taking the natural log of both sides (which is justified given that <img alt="|\lambda| &lt; 1" src="https://latex.codecogs.com/png.latex?%7C%5Clambda%7C%20%3C%201" style="vertical-align: middle;" title="|\lambda| &lt; 1"/>), we see that the inequality is satisfied if:</p>
<p><img alt="m \geq \frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))" src="https://latex.codecogs.com/png.latex?m%20%5Cgeq%20%5Cfrac%7B1%7D%7B%5Clog%28%5Cfrac%7B1%7D%7B%7C%5Clambda%7C%7D%29%7D%28%5Clog%28%5Cfrac%7B4n%7D%7B%5Cepsilon%7D%29%20%2B%20%5Clog%28%5Cfrac%7B1%7D%7B1-%7C%5Clambda%7C%7D%29%29" style="vertical-align: middle;" title="m \geq \frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))"/></p>
<p>Thus, we need only retain the first <img alt="\frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))" src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B%5Clog%28%5Cfrac%7B1%7D%7B%7C%5Clambda%7C%7D%29%7D%28%5Clog%28%5Cfrac%7B4n%7D%7B%5Cepsilon%7D%29%20%2B%20%5Clog%28%5Cfrac%7B1%7D%7B1-%7C%5Clambda%7C%7D%29%29" style="vertical-align: middle;" title="\frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))"/> terms of the Taylor series expansion of <img alt="f(\lambda)" src="https://latex.codecogs.com/png.latex?f%28%5Clambda%29" style="vertical-align: middle;" title="f(\lambda)"/>. This will involve calculating the first <img alt="m = \lceil \frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))\rceil" src="https://latex.codecogs.com/png.latex?m%20%3D%20%5Clceil%20%5Cfrac%7B1%7D%7B%5Clog%28%5Cfrac%7B1%7D%7B%7C%5Clambda%7C%7D%29%7D%28%5Clog%28%5Cfrac%7B4n%7D%7B%5Cepsilon%7D%29%20%2B%20%5Clog%28%5Cfrac%7B1%7D%7B1-%7C%5Clambda%7C%7D%29%29%5Crceil" style="vertical-align: middle;" title="m = \lceil \frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))\rceil"/> derivatives of <img alt="Z(\lambda)" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29" style="vertical-align: middle;" title="Z(\lambda)"/>, which naively can be done in quasi-polynomial time. This is done by running over all <img alt="S" src="https://latex.codecogs.com/png.latex?S" style="vertical-align: middle;" title="S"/>, where <img alt="|S| = m" src="https://latex.codecogs.com/png.latex?%7CS%7C%20%3D%20m" style="vertical-align: middle;" title="|S| = m"/>. This takes <img alt="O(n \log n + \log (\frac{1}{\epsilon}))" src="https://latex.codecogs.com/png.latex?O%28n%20%5Clog%20n%20%2B%20%5Clog%20%28%5Cfrac%7B1%7D%7B%5Cepsilon%7D%29%29" style="vertical-align: middle;" title="O(n \log n + \log (\frac{1}{\epsilon}))"/> time, which is quasi-polynomial.</p>
<p>Recent work by <a href="https://arxiv.org/abs/1707.05186">Patel and Regts</a>, as well as <a href="https://arxiv.org/abs/1704.06493">Liu, Sinclair and Srivastiva</a> has focused on evaluating these coefficients more efficiently (in polynomial time), but is outside the scope of this lecture. Clever counting arguments aside, to trivially calculate the <img alt="k" src="https://latex.codecogs.com/png.latex?k" style="vertical-align: middle;" title="k"/>-th derivative, we need only sum over the vectors with <img alt="k" src="https://latex.codecogs.com/png.latex?k" style="vertical-align: middle;" title="k"/> spin-up components.</p>
<h2 id="lee-yang-theorem">Lee-Yang Theorem</h2>
<p>Most of the heavy lifting in the above approach is done by the Lee-Yang theorem. In this section, we sketch how it is proved.</p>
<p>First, let’s define the Lee-Yang property, which we will refer to as the LYP. Let <img alt="P(\lambda_1, \dots, \lambda_n)" src="https://latex.codecogs.com/png.latex?P%28%5Clambda_1%2C%20%5Cdots%2C%20%5Clambda_n%29" style="vertical-align: middle;" title="P(\lambda_1, \dots, \lambda_n)"/> be some multilinear polynomial. <img alt="P" src="https://latex.codecogs.com/png.latex?P" style="vertical-align: middle;" title="P"/> has the Lee-Yang property if for any complex numbers <img alt="\lambda_1, \dots, \lambda_n" src="https://latex.codecogs.com/png.latex?%5Clambda_1%2C%20%5Cdots%2C%20%5Clambda_n" style="vertical-align: middle;" title="\lambda_1, \dots, \lambda_n"/> such that <img alt="|\lambda_i| &gt; 1" src="https://latex.codecogs.com/png.latex?%7C%5Clambda_i%7C%20%3E%201" style="vertical-align: middle;" title="|\lambda_i| &gt; 1"/> for all i, then <img alt="P(\lambda_1, \dots, \lambda_n) \neq 0" src="https://latex.codecogs.com/png.latex?P%28%5Clambda_1%2C%20%5Cdots%2C%20%5Clambda_n%29%20%5Cneq%200" style="vertical-align: middle;" title="P(\lambda_1, \dots, \lambda_n) \neq 0"/>.</p>
<p>We can then show that the partition function for the ferromagnetic Ising model, which we wrote as<br/>
<img alt="Z(\lambda_1, \dots, \lambda_n) = \sum_x \beta^{|E(x)|} \prod_{i: x_i = 1} \lambda_i" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda_1%2C%20%5Cdots%2C%20%5Clambda_n%29%20%3D%20%5Csum_x%20%5Cbeta%5E%7B%7CE%28x%29%7C%7D%20%5Cprod_%7Bi%3A%20x_i%20%3D%201%7D%20%5Clambda_i" style="vertical-align: middle;" title="Z(\lambda_1, \dots, \lambda_n) = \sum_x \beta^{|E(x)|} \prod_{i: x_i = 1} \lambda_i"/><br/>
must have this LYP. In the antiferromagnetic case it turns out that all zeroes lie on the negative real axis, but we will focus on the ferromagnetic case.</p>
<p>Proof (sketch):</p>
<p>For the proof that the partition function has the LYP, we will use Asano’s contraction argument. This relies on the fact that certain operations preserve LYP and we can “build” up to the partition function of the full graph by these operations.</p>
<ul>
<li><strong>Disjoint union</strong>: This is fairly simple to prove since the partition function for the disjoint union of <img alt="G" src="https://latex.codecogs.com/png.latex?G" style="vertical-align: middle;" title="G"/> and <img alt="H" src="https://latex.codecogs.com/png.latex?H" style="vertical-align: middle;" title="H"/> would just factor into the multiplication of the two partition function of the original graphs <img alt="Z_{G \sqcup H} = Z_{G} Z_{H}" src="https://latex.codecogs.com/png.latex?Z_%7BG%20%5Csqcup%20H%7D%20%3D%20Z_%7BG%7D%20Z_%7BH%7D" style="vertical-align: middle;" title="Z_{G \sqcup H} = Z_{G} Z_{H}"/>. Where <img alt="Z_{G}" src="https://latex.codecogs.com/png.latex?Z_%7BG%7D" style="vertical-align: middle;" title="Z_{G}"/> and <img alt="Z_{H}" src="https://latex.codecogs.com/png.latex?Z_%7BH%7D" style="vertical-align: middle;" title="Z_{H}"/> have the LYP, then it is clear that <img alt="Z_{G \sqcup H}" src="https://latex.codecogs.com/png.latex?Z_%7BG%20%5Csqcup%20H%7D" style="vertical-align: middle;" title="Z_{G \sqcup H}"/> has the same LYP.</li>
<li><strong>Contraction</strong>: Suppose we produce a graph <img alt="G'" src="https://latex.codecogs.com/png.latex?G%27" style="vertical-align: middle;" title="G'"/> from <img alt="G" src="https://latex.codecogs.com/png.latex?G" style="vertical-align: middle;" title="G"/> by contracting 2 vertices <img alt="z_1, z_2" src="https://latex.codecogs.com/png.latex?z_1%2C%20z_2" style="vertical-align: middle;" title="z_1, z_2"/> in <img alt="G" src="https://latex.codecogs.com/png.latex?G" style="vertical-align: middle;" title="G"/>, which means merging them into one vertex. It can be shown that if <img alt="G" src="https://latex.codecogs.com/png.latex?G" style="vertical-align: middle;" title="G"/> has the LYP, then <img alt="G'" src="https://latex.codecogs.com/png.latex?G%27" style="vertical-align: middle;" title="G'"/> also has the LYP. We write the partition function for G as<br/>
<img alt="A\lambda_1 \lambda_2 + B \lambda_1 + C \lambda_2 + D" src="https://latex.codecogs.com/png.latex?A%5Clambda_1%20%5Clambda_2%20%2B%20B%20%5Clambda_1%20%2B%20C%20%5Clambda_2%20%2B%20D" style="vertical-align: middle;" title="A\lambda_1 \lambda_2 + B \lambda_1 + C \lambda_2 + D"/><br/>
The “contracted” graph amounts to deleting the middle 2 terms so the partition function for <img alt="G'" src="https://latex.codecogs.com/png.latex?G%27" style="vertical-align: middle;" title="G'"/> can be written as<br/>
<img alt="\underbrace{A \lambda'}_{\text{both plus}} + \underbrace{D}_{\text{both minus}}" src="https://latex.codecogs.com/png.latex?%5Cunderbrace%7BA%20%5Clambda%27%7D_%7B%5Ctext%7Bboth%20plus%7D%7D%20%2B%20%5Cunderbrace%7BD%7D_%7B%5Ctext%7Bboth%20minus%7D%7D" style="vertical-align: middle;" title="\underbrace{A \lambda'}_{\text{both plus}} + \underbrace{D}_{\text{both minus}}"/><br/>
We want to show that the partition function for <img alt="G'" src="https://latex.codecogs.com/png.latex?G%27" style="vertical-align: middle;" title="G'"/> has the LYP. Because the partition function of <img alt="G" src="https://latex.codecogs.com/png.latex?G" style="vertical-align: middle;" title="G"/> has the LYP, we can consider the case where <img alt="\lambda_1 = \lambda_2 = \lambda" src="https://latex.codecogs.com/png.latex?%5Clambda_1%20%3D%20%5Clambda_2%20%3D%20%5Clambda" style="vertical-align: middle;" title="\lambda_1 = \lambda_2 = \lambda"/>. By the LYP,<br/>
<img alt="A\lambda^2 + (B+C)\lambda + D \neq  0" src="https://latex.codecogs.com/png.latex?A%5Clambda%5E2%20%2B%20%28B%2BC%29%5Clambda%20%2B%20D%20%5Cneq%20%200" style="vertical-align: middle;" title="A\lambda^2 + (B+C)\lambda + D \neq  0"/><br/>
if <img alt="|\lambda| &gt; 1" src="https://latex.codecogs.com/png.latex?%7C%5Clambda%7C%20%3E%201" style="vertical-align: middle;" title="|\lambda| &gt; 1"/>. By Vieta’s formulas we can find a relation between A and D,<br/>
<img alt=" \frac{|D|}{|A|} = \prod |zeroes| \leq 1" src="https://latex.codecogs.com/png.latex?%20%5Cfrac%7B%7CD%7C%7D%7B%7CA%7C%7D%20%3D%20%5Cprod%20%7Czeroes%7C%20%5Cleq%201" style="vertical-align: middle;" title=" \frac{|D|}{|A|} = \prod |zeroes| \leq 1"/><br/>
Now, to show that the partition function of G’ has the LYP, we assume there is a <img alt="\lambda'" src="https://latex.codecogs.com/png.latex?%5Clambda%27" style="vertical-align: middle;" title="\lambda'"/> such that <img alt="A\lambda' + D = 0" src="https://latex.codecogs.com/png.latex?A%5Clambda%27%20%2B%20D%20%3D%200" style="vertical-align: middle;" title="A\lambda' + D = 0"/>. For this to be true,<br/>
<img alt="|\lambda'| = \frac{|D|}{|A|} " src="https://latex.codecogs.com/png.latex?%7C%5Clambda%27%7C%20%3D%20%5Cfrac%7B%7CD%7C%7D%7B%7CA%7C%7D%20" style="vertical-align: middle;" title="|\lambda'| = \frac{|D|}{|A|} "/><br/>
However, this means that <img alt="|\lambda'| \leq 1" src="https://latex.codecogs.com/png.latex?%7C%5Clambda%27%7C%20%5Cleq%201" style="vertical-align: middle;" title="|\lambda'| \leq 1"/> so there is no solution where <img alt="|\lambda'| &gt; 1" src="https://latex.codecogs.com/png.latex?%7C%5Clambda%27%7C%20%3E%201" style="vertical-align: middle;" title="|\lambda'| &gt; 1"/> such that the partition function of G’ is 0 and so it has the LYP.</li>
</ul>
<p>The final part of this proof is to construct the original graph. We observe is that for a single edge, the partition function has the LYP. In this case, we easily write out the partition function for 2 vertices:<br/>
<img alt="z_1z_2 + B z_1 + B z_2 + 1" src="https://latex.codecogs.com/png.latex?z_1z_2%20%2B%20B%20z_1%20%2B%20B%20z_2%20%2B%201" style="vertical-align: middle;" title="z_1z_2 + B z_1 + B z_2 + 1"/><br/>
Suppose <img alt="|z_1| &gt; 1" src="https://latex.codecogs.com/png.latex?%7Cz_1%7C%20%3E%201" style="vertical-align: middle;" title="|z_1| &gt; 1"/>: then <img alt="z_1z_2 + Bz_1 + Bz_2 + 1=0" src="https://latex.codecogs.com/png.latex?z_1z_2%20%2B%20Bz_1%20%2B%20Bz_2%20%2B%201%3D0" style="vertical-align: middle;" title="z_1z_2 + Bz_1 + Bz_2 + 1=0"/> would imply <img alt="|z_2| = \frac{|1+Bz_1|}{|z_1 + B|}" src="https://latex.codecogs.com/png.latex?%7Cz_2%7C%20%3D%20%5Cfrac%7B%7C1%2BBz_1%7C%7D%7B%7Cz_1%20%2B%20B%7C%7D" style="vertical-align: middle;" title="|z_2| = \frac{|1+Bz_1|}{|z_1 + B|}"/>. However, this is just the Möbius transform mapping the exterior of the unit disk to the interior, so <img alt="|z_2| \leq 1" src="https://latex.codecogs.com/png.latex?%7Cz_2%7C%20%5Cleq%201" style="vertical-align: middle;" title="|z_2| \leq 1"/>. Thus, it cannot be the case that both <img alt="z_1" src="https://latex.codecogs.com/png.latex?z_1" style="vertical-align: middle;" title="z_1"/> and <img alt="z_2" src="https://latex.codecogs.com/png.latex?z_2" style="vertical-align: middle;" title="z_2"/> have absolute value greater than one.</p>
<p>Since single edges have the LYP. We break the graph into single edges, with copies of vertices. These copies are then contracted and we build up the graph to show that the partition function has the LYP.</p>
<p>Knowing that the partition function has the LYP, direct application of the Lee-Yang theorem guarantees that the roots are on the unit circle in the complex plane.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Although there exists an explicit formula for the partition function, the challenge lies in computing the quantity in polynomial time. Andrej Risteski’s lecture focused on two less-explored methods in theoretical computer science – namely variational inference and Taylor series approximations – to find provably approximate estimates. Both of these approaches are relatively new and replete with open problems.</p></div>
    </content>
    <updated>2018-11-11T02:24:14Z</updated>
    <published>2018-11-11T02:24:14Z</published>
    <category term="physics"/>
    <author>
      <name>Suproteem Sarkar</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-17T05:29:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6292</id>
    <link href="https://windowsontheory.org/2018/10/24/rabin-postdocs-ad-is-out/" rel="alternate" type="text/html"/>
    <title>Rabin postdocs ad is out</title>
    <summary>[Guest blog from Yaron Singer who is heading the selection committee for the Rabin fellowship this year. In addition to the Rabin fellowship and other postdoc opportunities, this year we also have a new postdoc opportunity in quantum computation and information via the Harvard Quantum Initiative. –Boaz.]   Michael O. Rabin Postdoctoral Fellowship in Theoretical […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest blog from Yaron Singer who is heading the selection committee for the Rabin fellowship this year. In addition to the Rabin fellowship and other postdoc opportunities, this year we also have a new postdoc opportunity in quantum computation and information via the <a href="https://academicjobsonline.org/ajo/jobs/12308">Harvard Quantum Initiative</a>. –Boaz.]</em></p>
<p> </p>
<div>
<div><b><span style="font-family: arial, helvetica, sans-serif;">Michael O. Rabin Postdoctoral Fellowship in Theoretical Computer Science</span></b></div>
<div><span style="font-family: arial, helvetica, sans-serif;"> </span></div>
<div><span style="font-family: arial, helvetica, sans-serif;">Deadline for full consideration: <b>December 3, 2018</b>.  Applications can be submitted here::<br/>
</span></div>
<div><span style="font-family: arial, helvetica, sans-serif;"> </span></div>
<div><span style="font-family: arial, helvetica, sans-serif;"><a href="https://academicpositions.harvard.edu/postings/8512" rel="noopener" target="_blank">https://academicpositions.harvard.edu/postings/8512</a><br/>
</span></div>
</div>
<div><span style="font-family: arial, helvetica, sans-serif;"> </span></div>
<div><span style="font-family: arial, helvetica, sans-serif;">The Harvard John A. Paulson School of Engineering and Applied Sciences at Harvard University seeks applicants for the Michael O. Rabin Postdoctoral Fellowship in Theoretical Computer Science. The standard duration of the Rabin Fellowship is two years. Rabin Fellows will receive a generous salary as well as an annual allocation for research and travel expenses.</span></div>
<div><span style="font-family: arial, helvetica, sans-serif;"> </span></div>
<div><span style="font-family: arial, helvetica, sans-serif;">Past fellows are Mika Goose and Aviad Rubinstein and the current fellow is Alexander Golovnev.</span></div>
<div><span style="font-family: arial, helvetica, sans-serif;"> </span></div>
<div><span style="font-family: arial, helvetica, sans-serif;">We are looking for exceptional junior scientists in theoretical computer science, broadly construed. Rabin Fellows will be provided with the opportunity to pursue their research agenda in an intellectually vibrant environment with ample mentorship. While interaction with Harvard faculty, students, and visitors is encouraged, Rabin Fellows are free to pursue their own interests. Candidates are required to have a doctorate or terminal degree in Computer Science or a related area by the expected start date.</span></div>
<div><span style="font-family: arial, helvetica, sans-serif;"> </span></div>
<div><span style="font-family: arial, helvetica, sans-serif;">Required application documents include a cover letter, research statement, CV (including a list of publications), and names and contact information for three references. We recommend that papers mentioned in the CV be available online on your homepage or on electronic archives. Applicants will apply on-line at the above address. We encourage candidates to apply by December 3, 2018, but applications will be accepted until the position is filled.</span></div>
<div><span style="font-family: arial, helvetica, sans-serif;"> </span></div>
<div><span style="font-family: arial, helvetica, sans-serif;">Harvard is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.</span></div>
<div><span style="font-family: arial, helvetica, sans-serif;"> </span></div>
<div><span style="font-family: arial, helvetica, sans-serif;">*The fellowship is named after Michael O. Rabin, pioneer in Computer Science research and winner of numerous awards including the A. M. Turing award in 1976. Michael Rabin has been on the faculty at Harvard since 1981, and currently is the Thomas J. Watson, Sr. Research Professor of Computer Science in the Harvard Paulson School. The fellowship is aimed at researchers in all areas of theoretical computer science, including fellows that, like Rabin, might create new areas that do not yet exist.</span></div></div>
    </content>
    <updated>2018-10-24T16:33:41Z</updated>
    <published>2018-10-24T16:33:41Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>windowsontheory</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-17T05:29:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6268</id>
    <link href="https://windowsontheory.org/2018/10/20/belief-propagation-and-the-stochastic-block-model/" rel="alternate" type="text/html"/>
    <title>Belief Propagation and the Stochastic Block Model</title>
    <summary>[Guest post by Thomas Orton who presented a lecture on this in our  physics and computation seminar –Boaz] Introduction This blog post is a continuation of the CS229R lecture series. Last week, we saw how certain computational problems like 3SAT exhibit a thresholding behavior, similar to a phase transition in a physical system. In this post, […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Thomas Orton who presented a lecture on this in our  <a href="https://www.boazbarak.org/fall18seminar/">physics and computation seminar</a> –Boaz]</em></p>
<h2><strong>Introduction</strong></h2>
<p>This blog post is a continuation of the <a href="https://windowsontheory.org/category/physics/">CS229R lecture series</a>. <a href="https://windowsontheory.org/2018/09/15/statistical-physics-an-introduction-in-two-parts/">Last week</a>, we saw how certain computational problems like 3SAT exhibit a thresholding behavior, similar to a phase transition in a physical system. In this post, we’ll continue to look at this phenomenon by exploring a heuristic method, belief propagation (and the cavity method), which has been used to make hardness conjectures, and also has thresholding properties. In particular, we’ll start by looking at belief propagation for approximate inference on sparse graphs as a purely computational problem. After doing this, we’ll switch perspectives and see belief propagation motivated in terms of Gibbs free energy minimization for physical systems. With these two perspectives in mind, we’ll then try to use belief propagation to do inference on the the stochastic block model. We’ll see some heuristic techniques for determining when BP succeeds and fails in inference, as well as some numerical simulation results of belief propagation for this problem. Lastly, we’ll talk about where this all fits into what is currently known about efficient algorithms and information theoretic barriers for the stochastic block model.</p>
<h1/>
<h2>(1) Graphical Models and Belief Propagation</h2>
<h3>(1.0) Motivation</h3>
<p>Suppose someone gives you a probabilistic model on <img alt="\vec{x}=(x_1,...,x_n) \in \chi^{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D%3D%28x_1%2C...%2Cx_n%29+%5Cin+%5Cchi%5E%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\vec{x}=(x_1,...,x_n) \in \chi^{N}"/> (think of <img alt="\chi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\chi"/> as a discrete set) which can be decomposed in a special way, say</p>
<p><img alt="P(\vec{x})\propto \prod_{a \in F} f_{a}(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29%5Cpropto+%5Cprod_%7Ba+%5Cin+F%7D+f_%7Ba%7D%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})\propto \prod_{a \in F} f_{a}(\vec{x})"/></p>
<p>where each <img alt="f_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{a}"/> only depends on the variables <img alt="V_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=V_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V_{a}"/>. Recall from last week that we can express constraint satisfaction problems in these kinds of models, where each <img alt="f_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{a}"/> is associated with a particular constraint. For example, given a 3SAT formula <img alt="\phi=\land_{a} c_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%3D%5Cland_%7Ba%7D+c_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi=\land_{a} c_{a}"/>, we can let <img alt="f_{a}(\vec{x})=1" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Ba%7D%28%5Cvec%7Bx%7D%29%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{a}(\vec{x})=1"/> if <img alt="c_{a}(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Ba%7D%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{a}(\vec{x})"/> is satisfied, and 0 otherwise. Then each <img alt="f_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{a}"/> only depends on 3 variables, and <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/> only has support on satisfying assignments of <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/>.</p>
<p> </p>
<p>A central problem in computer science is trying to find satisfying assignments to constraint satisfaction problems, i.e. finding values <img alt="\vec{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\vec{x}"/> in the support of <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>. Suppose that we knew that the value of <img alt="P(x_1=1)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_1%3D1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_1=1)"/> were <img alt="&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="&gt;0"/>. Then we would know that there exists some satisfying assignment where <img alt="x_1=1" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1=1"/>. Using this knowledge, we could recursively try to find <img alt="\vec{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\vec{x}"/>‘s in the support of <img alt="P(1,x_2,...,x_n)" class="latex" src="https://s0.wp.com/latex.php?latex=P%281%2Cx_2%2C...%2Cx_n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(1,x_2,...,x_n)"/>, and iteratively come up with a satisfying assignment to our constraint satisfaction problem. In fact, we could even sample uniformally from the distribution as follows: randomly assign <img alt="x_1" class="latex" src="https://s0.wp.com/latex.php?latex=x_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1"/> to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1"/> with probability <img alt="P(x_1=1)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_1%3D1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_1=1)"/>, and assign it to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> otherwise. Now iteratively sample from <img alt="P(x_2|x_1)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_2%7Cx_1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_2|x_1)"/> for the model where <img alt="x_1" class="latex" src="https://s0.wp.com/latex.php?latex=x_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1"/> is fixed to the value we assigned to it, and repeat until we’ve assigned values to all of the <img alt="\{x_i\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bx_i%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{x_i\}_{i}"/>. A natural question is therefore the following: When can we try to efficiently compute the marginals</p>
<p><img alt="P(x_i):=\sum_{\vec{x}-x_i} P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_i%29%3A%3D%5Csum_%7B%5Cvec%7Bx%7D-x_i%7D+P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_i):=\sum_{\vec{x}-x_i} P(\vec{x})"/></p>
<p>for each <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>?</p>
<p> </p>
<p>A well known efficient algorithm for this problem exists when the corresponding graphical model of <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/> (more in this in the next section) is a tree. Even though belief propagation is only guaranteed to work exactly for trees, we might hope that if our factor graph is “tree like”, then BP will still give a useful answer. We might even go further than this, and try to analyze exactly when BP fails for a random constraint satisfaction problem. For example, you can do this for k-SAT when <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> is large, and then learn something about the solution threshold for k-SAT. It therefore might be natural to try and study when BP succeeds and fails for different kinds of problems.</p>
<h3>(1.1) Deriving BP</h3>
<p>We will start by making two simplifying assumptions on our model <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>.</p>
<p> </p>
<p>First, we will assume that <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/> can be written in the form <img alt="P(\vec{x})\propto \prod_{(i,j) \in E} f_{i,j}(x_i,x_j) \prod_{i \in [n]} f_i(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29%5Cpropto+%5Cprod_%7B%28i%2Cj%29+%5Cin+E%7D+f_%7Bi%2Cj%7D%28x_i%2Cx_j%29+%5Cprod_%7Bi+%5Cin+%5Bn%5D%7D+f_i%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})\propto \prod_{(i,j) \in E} f_{i,j}(x_i,x_j) \prod_{i \in [n]} f_i(x_i)"/> for some functions <img alt="\{f_{i,j}\}_{i,j}, \{f_i(x_i)\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bf_%7Bi%2Cj%7D%5C%7D_%7Bi%2Cj%7D%2C+%5C%7Bf_i%28x_i%29%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{f_{i,j}\}_{i,j}, \{f_i(x_i)\}_{i}"/> and some “edge set” <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E"/> (where the edges are undirected). In other words, we will only consider pairwise constraints. We will see later that this naturally corresponds to a physical interpretation, where each of the “particles” <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> interact with each other via pairwise forces. Belief propagation actually still works without this assumption (which is why we can use it to analyze <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT for <img alt="k&gt;2" class="latex" src="https://s0.wp.com/latex.php?latex=k%3E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k&gt;2"/>), but the pairwise case is all we need for the stochastic block model.</p>
<p> </p>
<p>For the second assumption, notice that there is a natural correspondence between <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/> and the graphical model <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> on <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> vertices, where <img alt="(i,j)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j)"/> forms an edge in <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> iff <img alt="(i,j) \in E" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29+%5Cin+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j) \in E"/>. In order words, edges <img alt="(i,j)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j)"/> in <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> correspond to factors of the form <img alt="f_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{i,j}"/> in <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>, and vertices in <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> correspond to variables in <img alt="\vec{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\vec{x}"/>. Our second assumption is that the graphical model <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> is a tree.</p>
<p><img alt="graphical_model_fig_1" class="alignnone size-full wp-image-6270" src="https://windowsontheory.files.wordpress.com/2018/10/graphical_model_fig_1.png?w=600"/></p>
<p>Now, suppose we’re given such a tree <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> which represents our probabilistic model. How to we compute the marginals? Generally speaking, when computer scientists see trees, they begin to get very excited [reference]. “I know! Let’s use recursion!” shouts the student in <a href="http://sites.fas.harvard.edu/~cs124/cs124/">CS124</a>, their heart rate noticeably rising. Imagine that we arbitrarily rooted our tree at vertex <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. Perhaps, if we could somehow compute the marginals of the children of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, we could somehow stitch them together to compute the marginal <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. In order words, we should think about computing the marginals of roots of subtrees in our graphical model. A quick check shows that the base case is easy: suppose we’re given a graphical model which is a tree consisting of a single node <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. This corresponds to some PDF <img alt="P(\vec{x})=P(x_i) \propto f_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29%3DP%28x_i%29+%5Cpropto+f_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})=P(x_i) \propto f_{i}(x_i)"/>. So to compute <img alt="P(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_i)"/>, all we have to do is compute the marginalizing constant <img alt="Z=\sum_{x_i \in \chi} f_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%3D%5Csum_%7Bx_i+%5Cin+%5Cchi%7D+f_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z=\sum_{x_i \in \chi} f_{i}(x_i)"/>, and then we have <img alt="P(x_i)=\frac{1}{Z}f(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_i%29%3D%5Cfrac%7B1%7D%7BZ%7Df%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_i)=\frac{1}{Z}f(x_i)"/>. With the base case out of the way, let’s try to solve the induction step: given a graphical model which is a tree rooted at <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, and where we’re given the marginals of the subtrees rooted at the children of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, how do we compute the marginal of the tree rooted at <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>? Take a look at figure 2 to see what this looks like graphically. To formalize the induction step, we’ll define some notation that will also be useful to us later on. The main pieces of notation are <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>, which is the subtree rooted at <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> with parent <img alt="x_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_j"/>, and the “messages” <img alt="\psi^{k \rightarrow i}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{k \rightarrow i}_{x_i}"/>, which can be thought of as information which is passed from the child subtrees of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> to the vertex <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> in order to compute the marginals correctly.</p>
<ol>
<li>We let <img alt="\partial i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpartial+i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\partial i"/> denote the neighbours of vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> in <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/>. In general, I’ll interchangeably switch between vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> and the variable <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> represented by vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>.</li>
<li>We define <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/> to be the subtree of <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> rooted at <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>, where <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>‘s parent is <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j"/>. We need to specify <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>‘s parent in order to give an orientation to our tree (so that we know in which direction we need to do recursion down the tree). Likewise, we let <img alt="V_{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=V_%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V_{i \rightarrow j}"/> be the set of vertices/variables which occur in subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>.</li>
<li>We define the function <img alt="T^{i \rightarrow j}(V_{i \rightarrow j})" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D%28V_%7Bi+%5Crightarrow+j%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}(V_{i \rightarrow j})"/>, which is a function of the variables in <img alt="V_{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=V_%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V_{i \rightarrow j}"/>, to be equal to the product of <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>‘s edges and vertices. Specifically, <img alt="T^{i \rightarrow j}(V_{i \rightarrow j})=\prod_{(a,b) \in E'} f_{a,b}(x_a,x_b) \prod_{a \in V_{i \rightarrow j}} f_{a}(x_a)" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D%28V_%7Bi+%5Crightarrow+j%7D%29%3D%5Cprod_%7B%28a%2Cb%29+%5Cin+E%27%7D+f_%7Ba%2Cb%7D%28x_a%2Cx_b%29+%5Cprod_%7Ba+%5Cin+V_%7Bi+%5Crightarrow+j%7D%7D+f_%7Ba%7D%28x_a%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}(V_{i \rightarrow j})=\prod_{(a,b) \in E'} f_{a,b}(x_a,x_b) \prod_{a \in V_{i \rightarrow j}} f_{a}(x_a)"/>, where <img alt="E'" class="latex" src="https://s0.wp.com/latex.php?latex=E%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E'"/> is the set of edges which occur in subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>. We can think of <img alt="T^{i \rightarrow j}(V_{i \rightarrow j})" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D%28V_%7Bi+%5Crightarrow+j%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}(V_{i \rightarrow j})"/> as being the pdf (up to normalizing constant) of the “model” of the subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>.</li>
<li>Lastly, we define <img alt="\psi^{i \rightarrow j}_{x_i}:=\frac{1}{Z^{i \rightarrow j}} \sum_{V_{i \rightarrow j}-x_i} T(V_{i \rightarrow j})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D%3A%3D%5Cfrac%7B1%7D%7BZ%5E%7Bi+%5Crightarrow+j%7D%7D+%5Csum_%7BV_%7Bi+%5Crightarrow+j%7D-x_i%7D+T%28V_%7Bi+%5Crightarrow+j%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}:=\frac{1}{Z^{i \rightarrow j}} \sum_{V_{i \rightarrow j}-x_i} T(V_{i \rightarrow j})"/>, where <img alt="Z^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=Z%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z^{i \rightarrow j}"/> is a normalizing constant chosen such that <img alt="\sum_{x_i \in \chi} \psi^{i \rightarrow j}_{x_i}=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx_i+%5Cin+%5Cchi%7D+%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{x_i \in \chi} \psi^{i \rightarrow j}_{x_i}=1"/>. In particular, <img alt="\psi^{i \rightarrow j}_{x_i}: \chi \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D%3A+%5Cchi+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}: \chi \rightarrow \mathbb{R}"/> is a function defined for each possible value of <img alt="x_i \in \chi" class="latex" src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5Cchi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i \in \chi"/>. We can interpret <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> in two ways: firstly, it is the marginal of the root of the subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>. Secondly, we can think of it as a “message” from vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> to vertex <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j"/>. We’ll see why this is a valid interpretation shortly.</li>
</ol>
<p><img alt="graphical_model_recursion" class="alignnone  wp-image-6269" height="465" src="https://windowsontheory.files.wordpress.com/2018/10/graphical_model_recursion.png?w=507&amp;h=465" width="507"/></p>
<p>Phew! That was a lot of notation. Now that we have that out of the way, let’s see how we can express the marginal of the root of a tree as a function of the marginals of its subtrees. Suppose we’re considering the subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>, so that vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> has children <img alt="(\partial i)-j" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cpartial+i%29-j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\partial i)-j"/>. Then we can compute the marginal <img alt="\psi^{i \rightarrow j}_{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Br%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{r}"/> directly:</p>
<p><img alt="eqs_1" class="alignnone  wp-image-6279" height="238" src="https://windowsontheory.files.wordpress.com/2018/10/eqs_1.png?w=746&amp;h=238" width="746"/></p>
<p>The non-obvious step in the above is that we’re able to switch around summations and products: we’re able to do this because each of the trees are functions on disjoint sets of variables. So we’re able to express <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> as a function of the children values <img alt="\{\psi^{k \rightarrow i}_{x_k}\}_{k \in \partial i-j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D%5C%7D_%7Bk+%5Cin+%5Cpartial+i-j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\psi^{k \rightarrow i}_{x_k}\}_{k \in \partial i-j}"/>. Looking at the update formula we have derived, we can now see why the <img alt="\{\psi^{k \rightarrow i}_{x_k}\}_{k \in \partial i-j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D%5C%7D_%7Bk+%5Cin+%5Cpartial+i-j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\psi^{k \rightarrow i}_{x_k}\}_{k \in \partial i-j}"/> are called “messages” to vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>: they send information about the child subtrees to their parent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>.</p>
<p>The above discussion is a purely algebraic way of deriving belief propagation. A more intuitive way to get this result is as follows: imagine fixing the value of <img alt="x_i=a" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%3Da&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i=a"/> in the the subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>, and then drawing from each of the marginals of the children of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> conditioned on the value <img alt="x_i=a" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%3Da&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i=a"/>. We can consider the marginals of each of the children independently, because the children are independent of each other when conditioned on the value of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. Converting words to equations, this means that if <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> has children <img alt="x_{k_{1}},...,x_{k_{d}}" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bk_%7B1%7D%7D%2C...%2Cx_%7Bk_%7Bd%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_{k_{1}},...,x_{k_{d}}"/>, then the marginal probability of <img alt="(x_i,x_{k_{1}},...,x_{k_{d}})" class="latex" src="https://s0.wp.com/latex.php?latex=%28x_i%2Cx_%7Bk_%7B1%7D%7D%2C...%2Cx_%7Bk_%7Bd%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x_i,x_{k_{1}},...,x_{k_{d}})"/> in the subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/> is proportional to <img alt="f_{i}(x_i) \prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bi%7D%28x_i%29+%5Cprod_%7B%5Csubstack%7Bk%3A%28i%2Ck%29+%5Cin+E%2C+%5C%5C+k+%5Cnot+%3D+j%7D%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D+f_%7Bi%2Ck%7D%28x_i%2Cx_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{i}(x_i) \prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)"/>. We can then write</p>
<p><img alt="\psi_{x_i}^{i \rightarrow j} \propto \sum_{(\partial i)-j} f_{i}(x_i) \prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi+%5Crightarrow+j%7D+%5Cpropto+%5Csum_%7B%28%5Cpartial+i%29-j%7D+f_%7Bi%7D%28x_i%29+%5Cprod_%7B%5Csubstack%7Bk%3A%28i%2Ck%29+%5Cin+E%2C+%5C%5C+k+%5Cnot+%3D+j%7D%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D+f_%7Bi%2Ck%7D%28x_i%2Cx_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i \rightarrow j} \propto \sum_{(\partial i)-j} f_{i}(x_i) \prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)"/></p>
<p><img alt="= f_{i}(x_i)\prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \sum_{x_k \in \chi} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=%3D+f_%7Bi%7D%28x_i%29%5Cprod_%7B%5Csubstack%7Bk%3A%28i%2Ck%29+%5Cin+E%2C+%5C%5C+k+%5Cnot+%3D+j%7D%7D+%5Csum_%7Bx_k+%5Cin+%5Cchi%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D+f_%7Bi%2Ck%7D%28x_i%2Cx_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="= f_{i}(x_i)\prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \sum_{x_k \in \chi} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)"/></p>
<p>And we get back what we had before. We’ll call this last equation our “update” or “message passing” equation. The key assumption we used was that if we condition on <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, then the children of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> are independent. It’s useful to keep this assumption in mind when thinking about how BP behaves on more general graphs.</p>
<p>A similar calculation yields that we can calculate the marginal of our original probability distribution <img alt="\psi_{x_i}^{i}:=P(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi%7D%3A%3DP%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i}:=P(x_i)"/> as the marginal of the subtree with no parent, i.e.</p>
<p><img alt="\psi_{x_i}^{i} \propto f_{i}(x_i)\prod_{k:(i,k) \in E} \sum_{x_k \in \chi} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi%7D+%5Cpropto+f_%7Bi%7D%28x_i%29%5Cprod_%7Bk%3A%28i%2Ck%29+%5Cin+E%7D+%5Csum_%7Bx_k+%5Cin+%5Cchi%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D+f_%7Bi%2Ck%7D%28x_i%2Cx_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i} \propto f_{i}(x_i)\prod_{k:(i,k) \in E} \sum_{x_k \in \chi} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)"/></p>
<p> </p>
<p>Great! So now we have an algorithm for computing marginals: recursively compute <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> for each <img alt="(i,j) \in E" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29+%5Cin+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j) \in E"/> in a dynamic programming fashion with the “message passing” equations we have just derived. Then, compute <img alt="\psi_{x_i}^{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i}"/> for each <img alt="i \in [n]" class="latex" src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i \in [n]"/>. If the diameter of our tree <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> is <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>, then the recursion depth of our algorithm is at most <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>.</p>
<p>However, instead of computing every <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> neatly with recursion, we might try something else: let’s instead randomly initialize each <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> <img alt="(x_i \in \chi)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x_i+%5Cin+%5Cchi%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x_i \in \chi)"/> with anything we want. Then, let’s update each <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> in parallel with our update equations. We will keep doing this in successive steps until each <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> has converged to a fixed value. By looking at belief propagation as a recursive algorithm, it’s easy to see that all of the <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/>‘s will have their correct values after at most <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> steps. This is because (after arbitrarily rooting our tree at any vertex) the leaves of our recursion will initialize to the correct value after 1 step. After two steps, the parents of the leaves will be updated correctly as functions of the leaves, and so they will have the correct values as well. Specifically:<br/>
<strong>Proposition:</strong> Suppose we initialize messages <img alt="\psi_{x_i}^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i \rightarrow j}"/> arbitrarily, and update them in parallel according to our update equations. If <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> has diameter <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>, then after <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> steps each <img alt="\psi_{r}^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Br%7D%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{r}^{i \rightarrow j}"/> converges, and we recover the correct marginals.</p>
<p>Why would anyone want to do things in this way? In particular, by computing everything in parallel in steps instead of recursively, we’re computing a lot of “garbage” updates which we never use. However, the advantage of doing things in this way is that this procedure is now well defined for general graphs. In particular, suppose <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/> violated assumption (2), so that the corresponding graph were not a tree. Then we could still try to compute the messages <img alt="\psi_{x_i}^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i \rightarrow j}"/> with parallel updates. We are also able to do this in a local “message passing” kind of way, which some people may find physically intuitive. Maybe if we’re lucky, the messages will converge after a reasonable amount of iterations. Maybe if we’re even luckier, they will converge to something which gives us information about the marginals <img alt="\{P(x_i)\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BP%28x_i%29%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{P(x_i)\}_{i}"/>. In fact, we’ll see that just such a thing happens in the stochastic block model. More on that later. For now, let’s shift gears and look at belief propagation from a physics perspective.</p>
<p> </p>
<h2>(2) Free Energy Minimization</h2>
<h3>(2.0) Potts model (Refresh of last week)</h3>
<p>We’ve just seen a statistical/algorithmic view of how to compute marginals in a graphical model. It turns out that there’s also a physical way to think about this, which leads to a qualitatively similar algorithm. Recall from last week that another interpretation of a pairwise factor-able PDF is that of particles interacting with each other via pairwise forces. In particular, we can imagine each particle <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> interacting with <img alt="x_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_j"/> via a force of strength</p>
<p><img alt="J_{(i,j)}(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=J_%7B%28i%2Cj%29%7D%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_{(i,j)}(x_i,x_j)"/></p>
<p>and in addition, interacting with an external field</p>
<p><img alt="h_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{i}(x_i)"/></p>
<p>We imagine that each of our particles take values from a discrete set <img alt="\chi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\chi"/>. When <img alt="\chi=\{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%3D%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\chi=\{0,1\}"/>, we recover the Ising model, and in general we have a Potts model. The energy function of this system is then</p>
<p><img alt="E[\vec{x}]=-\sum_{(i,j)}J_{(i,j)}(x_i,x_j)-\sum_{i}h_{i}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%5B%5Cvec%7Bx%7D%5D%3D-%5Csum_%7B%28i%2Cj%29%7DJ_%7B%28i%2Cj%29%7D%28x_i%2Cx_j%29-%5Csum_%7Bi%7Dh_%7Bi%7D%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E[\vec{x}]=-\sum_{(i,j)}J_{(i,j)}(x_i,x_j)-\sum_{i}h_{i}(x)"/></p>
<p>with probability distribution given by</p>
<p><img alt="P(\vec{x})=\frac{1}{Z} e^{-E[\vec{x}]/T}" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29%3D%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-E%5B%5Cvec%7Bx%7D%5D%2FT%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})=\frac{1}{Z} e^{-E[\vec{x}]/T}"/></p>
<p>Now, for <img alt="\chi=\{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%3D%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\chi=\{0,1\}"/>, computing the marginals <img alt="P(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_i)"/> corresponds to the equivalent physical problem of computing the “magnetizations” <img alt="P(x_i=1)-P(x_i=0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_i%3D1%29-P%28x_i%3D0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_i=1)-P(x_i=0)"/>.</p>
<p>How does this setup relate to the previous section, where we thought about constraint satisfaction problems and probability distributions? If we could set <img alt="e^{-J_{(i,j)}(x_i,x_j)/T}=f_{i,j}(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B-J_%7B%28i%2Cj%29%7D%28x_i%2Cx_j%29%2FT%7D%3Df_%7Bi%2Cj%7D%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e^{-J_{(i,j)}(x_i,x_j)/T}=f_{i,j}(x_i,x_j)"/> and <img alt="e^{-h_{i}(x_i)/T}=f_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B-h_%7Bi%7D%28x_i%29%2FT%7D%3Df_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e^{-h_{i}(x_i)/T}=f_{i}(x_i)"/>, we would recover exactly the probability distribution <img alt="P(\vec{x})=\prod_{i,j} f_{i,j}(\vec{x}) \prod_{i} f_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29%3D%5Cprod_%7Bi%2Cj%7D+f_%7Bi%2Cj%7D%28%5Cvec%7Bx%7D%29+%5Cprod_%7Bi%7D+f_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})=\prod_{i,j} f_{i,j}(\vec{x}) \prod_{i} f_{i}(x_i)"/> from the previous section. From a constraint satisfaction perspective, if we set <img alt="J_{(i,j)}(x_i,x_j)=1" class="latex" src="https://s0.wp.com/latex.php?latex=J_%7B%28i%2Cj%29%7D%28x_i%2Cx_j%29%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_{(i,j)}(x_i,x_j)=1"/> if constraint <img alt="(i,j)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j)"/> is satisfied and <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> otherwise, then as <img alt="T \rightarrow 0" class="latex" src="https://s0.wp.com/latex.php?latex=T+%5Crightarrow+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T \rightarrow 0"/> (our system becomes colder), <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>‘s probability mass becomes concentrated only on the satisfying assignments of the constraint satisfaction problem.</p>
<h3>(2.1) Free energy minimization</h3>
<p>We’re now going to try a different approach to computing the marginals: let’s define a distribution <img alt="b(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=b%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(\vec{x})"/>, which we will hope to be a good approximation to <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>. If you like, you can think about the marginal <img alt="b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)"/> as being the “belief” about the state of variable <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. We can measure the “distance” between <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> and <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/> by the KL divergence</p>
<p> </p>
<p><img alt="KL(b(\vec{x})||p(\vec{x})) =\sum_{\vec{x}}b(\vec{x})\ln\frac{b(\vec{x})}{P(\vec{x})}" class="latex" src="https://s0.wp.com/latex.php?latex=KL%28b%28%5Cvec%7Bx%7D%29%7C%7Cp%28%5Cvec%7Bx%7D%29%29+%3D%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29%5Cln%5Cfrac%7Bb%28%5Cvec%7Bx%7D%29%7D%7BP%28%5Cvec%7Bx%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="KL(b(\vec{x})||p(\vec{x})) =\sum_{\vec{x}}b(\vec{x})\ln\frac{b(\vec{x})}{P(\vec{x})}"/></p>
<p><img alt="=\sum_{\vec{x}}b(\vec{x})E(\vec{x})+\sum_{\vec{x}}b(\vec{x})\ln b(\vec{x})+\ln Z" class="latex" src="https://s0.wp.com/latex.php?latex=%3D%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29E%28%5Cvec%7Bx%7D%29%2B%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29%5Cln+b%28%5Cvec%7Bx%7D%29%2B%5Cln+Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="=\sum_{\vec{x}}b(\vec{x})E(\vec{x})+\sum_{\vec{x}}b(\vec{x})\ln b(\vec{x})+\ln Z"/></p>
<p>which equals 0 iff the two distributions are equal. Let’s define the Gibbs free energy as<br/>
<img alt="G(b(\vec{x}))=\sum_{\vec{x}}b(\vec{x})E(\vec{x})+\sum_{\vec{x}}b(\vec{x})\ln b(\vec{x})=:U(b(\vec{x}))-S(b(\vec{x}))" class="latex" src="https://s0.wp.com/latex.php?latex=G%28b%28%5Cvec%7Bx%7D%29%29%3D%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29E%28%5Cvec%7Bx%7D%29%2B%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29%5Cln+b%28%5Cvec%7Bx%7D%29%3D%3AU%28b%28%5Cvec%7Bx%7D%29%29-S%28b%28%5Cvec%7Bx%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G(b(\vec{x}))=\sum_{\vec{x}}b(\vec{x})E(\vec{x})+\sum_{\vec{x}}b(\vec{x})\ln b(\vec{x})=:U(b(\vec{x}))-S(b(\vec{x}))"/></p>
<p> </p>
<p>So the minimum value of <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is <img alt="F=-\ln Z" class="latex" src="https://s0.wp.com/latex.php?latex=F%3D-%5Cln+Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F=-\ln Z"/> which is called the Helmholz free energy, <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> is the “average energy” and <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> is the “entropy”.</p>
<p> </p>
<p>Now for the “free energy minimization part”. We want to choose <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/> to minimize <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>, so that we can have that <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/> is a good approximation of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/>. If this happens, then maybe we can hope to “read out” the marginals of <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/> directly. How do we do this in a way which makes it easy to “read out” the marginals? Here’s one idea: let’s try to write <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> as a function of only the marginals <img alt="b(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i,x_j)"/> and <img alt="b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)"/> of <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/>. If we could do this, then maybe we could try to minimize <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> by only optimizing over values for “variables” <img alt="\{b(x_i,x_j)\}_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%2Cx_j%29%5C%7D_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i,x_j)\}_{i,j}"/> and <img alt="\{b(x_i)\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%29%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i)\}_{i}"/>. However, we need to remember that <img alt="\{b(x_i,x_j)\}_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%2Cx_j%29%5C%7D_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i,x_j)\}_{i,j}"/> and <img alt="\{b(x_i)\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%29%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i)\}_{i}"/> are actually meant to represent marginals for some real probability distribution <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/>. So at the very least, we should add the consistency constraints <img alt="b(x_i)=\sum_{x_j \in \chi} b(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29%3D%5Csum_%7Bx_j+%5Cin+%5Cchi%7D+b%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)=\sum_{x_j \in \chi} b(x_i,x_j)"/> and <img alt="\sum_{x_i \in \chi} b(x_i)=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx_i+%5Cin+%5Cchi%7D+b%28x_i%29%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{x_i \in \chi} b(x_i)=1"/> to our optimization problem. We can then think of <img alt="b(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i,x_j)"/> and <img alt="b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)"/> as “pseudo-marginals” which obey degree-2 Sherali-Adams constraints.</p>
<p> </p>
<p>Recall that we’ve written <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> as a sum of both the average energy <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> and the entropy <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/>. It turns out that we <em>can</em> actually write <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> as only a function of the pariwise marginals of <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/>:</p>
<p><img alt="U(b(\vec{x})) =-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)J_{i,j}(x_i,x_j)-\sum_{i}\sum_{x_i}b(x_i) h_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=U%28b%28%5Cvec%7Bx%7D%29%29+%3D-%5Csum_%7Bi%2Cj%7D%5Csum_%7Bx_i%2Cx_j%7D+b%28x_i%2Cx_j%29J_%7Bi%2Cj%7D%28x_i%2Cx_j%29-%5Csum_%7Bi%7D%5Csum_%7Bx_i%7Db%28x_i%29+h_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U(b(\vec{x})) =-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)J_{i,j}(x_i,x_j)-\sum_{i}\sum_{x_i}b(x_i) h_{i}(x_i)"/></p>
<p>which follows just because the sums marginalize out the variables which don’t form part of the pairwise interactions:<br/>
<img alt="\sum_{\vec{x}}b(\vec{x})\left(-\sum_{i,j}J_{i,j}(x_i,x_j)-\sum_{i} h_{i}(x_i)\right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29%5Cleft%28-%5Csum_%7Bi%2Cj%7DJ_%7Bi%2Cj%7D%28x_i%2Cx_j%29-%5Csum_%7Bi%7D+h_%7Bi%7D%28x_i%29%5Cright%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{\vec{x}}b(\vec{x})\left(-\sum_{i,j}J_{i,j}(x_i,x_j)-\sum_{i} h_{i}(x_i)\right) "/></p>
<p><img alt="=-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)J_{i,j}(x_i,x_j)-\sum_{i}\sum_{x_i}b(x_i) h_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%3D-%5Csum_%7Bi%2Cj%7D%5Csum_%7Bx_i%2Cx_j%7D+b%28x_i%2Cx_j%29J_%7Bi%2Cj%7D%28x_i%2Cx_j%29-%5Csum_%7Bi%7D%5Csum_%7Bx_i%7Db%28x_i%29+h_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="=-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)J_{i,j}(x_i,x_j)-\sum_{i}\sum_{x_i}b(x_i) h_{i}(x_i)"/></p>
<p>This is good news: since <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> only depends on pairwise interactions, the average energy component of <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> only depends on <img alt="b(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i,x_j)"/> and <img alt="b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)"/>. However, it is not so clear how to express the entropy as a function of one node and two node beliefs. However, maybe we can try to pretend that our model is really a “tree”. In this case, the following is true:</p>
<p> </p>
<p><strong>Claim:</strong> If our model is a tree, and <img alt="\{b(x_i,x_j)\}_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%2Cx_j%29%5C%7D_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i,x_j)\}_{i,j}"/> and <img alt="\{b(x_i)\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%29%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i)\}_{i}"/> are the associated marginals of our probabilistic model <img alt="b(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=b%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(\vec{x})"/>, then we have<br/>
<img alt="b(\vec{x})=\frac{\prod_{(i,j) \in E}b(x_i,x_j)}{\prod_{i \in [n]} b(x_i)^{q_i-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=b%28%5Cvec%7Bx%7D%29%3D%5Cfrac%7B%5Cprod_%7B%28i%2Cj%29+%5Cin+E%7Db%28x_i%2Cx_j%29%7D%7B%5Cprod_%7Bi+%5Cin+%5Bn%5D%7D+b%28x_i%29%5E%7Bq_i-1%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(\vec{x})=\frac{\prod_{(i,j) \in E}b(x_i,x_j)}{\prod_{i \in [n]} b(x_i)^{q_i-1}}"/></p>
<p>where <img alt="q_i" class="latex" src="https://s0.wp.com/latex.php?latex=q_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_i"/> is the degree of vertex <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> in the tree.</p>
<p> </p>
<p>It’s not too difficult to see why this is the case: imagine a tree rooted at <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, with children <img alt="\partial i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpartial+i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\partial i"/>. We can think of sampling from this tree as first sampling from <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> via its marginal <img alt="b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)"/>, and then by recursively sampling the children conditioned on <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. Associate with <img alt="\{T_k\}_{k \in \partial i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BT_k%5C%7D_%7Bk+%5Cin+%5Cpartial+i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{T_k\}_{k \in \partial i}"/> the subtrees of the children of <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>, i.e. <img alt="T_j(V_{j})" class="latex" src="https://s0.wp.com/latex.php?latex=T_j%28V_%7Bj%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T_j(V_{j})"/> is equal to the probability of the occurrence <img alt="V_{j}" class="latex" src="https://s0.wp.com/latex.php?latex=V_%7Bj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V_{j}"/> on the probabilistic model of the tree rooted at vertex <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j"/>. Then we have</p>
<p><img alt="b(\vec{x}) =b(x_i) \prod_{k \in \partial i} b(x_k|x_i) \prod_{k \in \partial i} T_k(V_{k}|x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28%5Cvec%7Bx%7D%29+%3Db%28x_i%29+%5Cprod_%7Bk+%5Cin+%5Cpartial+i%7D+b%28x_k%7Cx_i%29+%5Cprod_%7Bk+%5Cin+%5Cpartial+i%7D+T_k%28V_%7Bk%7D%7Cx_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(\vec{x}) =b(x_i) \prod_{k \in \partial i} b(x_k|x_i) \prod_{k \in \partial i} T_k(V_{k}|x_k)"/></p>
<p><img alt="=b(x_i)\frac{1}{b(x_i)^{q_i}} \prod_{k \in \partial i} b(x_i,x_k) \prod_{k \in \partial i}\frac{1}{b(x_k)} \prod_{k \in \partial i}T_k(V_{k})" class="latex" src="https://s0.wp.com/latex.php?latex=%3Db%28x_i%29%5Cfrac%7B1%7D%7Bb%28x_i%29%5E%7Bq_i%7D%7D+%5Cprod_%7Bk+%5Cin+%5Cpartial+i%7D+b%28x_i%2Cx_k%29+%5Cprod_%7Bk+%5Cin+%5Cpartial+i%7D%5Cfrac%7B1%7D%7Bb%28x_k%29%7D+%5Cprod_%7Bk+%5Cin+%5Cpartial+i%7DT_k%28V_%7Bk%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="=b(x_i)\frac{1}{b(x_i)^{q_i}} \prod_{k \in \partial i} b(x_i,x_k) \prod_{k \in \partial i}\frac{1}{b(x_k)} \prod_{k \in \partial i}T_k(V_{k})"/><br/>
<img alt="=\frac{\prod_{(i,j) \in E}b(x_i,x_j)}{\prod_{i \in [N]}b(x_i)^{q_i-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%3D%5Cfrac%7B%5Cprod_%7B%28i%2Cj%29+%5Cin+E%7Db%28x_i%2Cx_j%29%7D%7B%5Cprod_%7Bi+%5Cin+%5BN%5D%7Db%28x_i%29%5E%7Bq_i-1%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="=\frac{\prod_{(i,j) \in E}b(x_i,x_j)}{\prod_{i \in [N]}b(x_i)^{q_i-1}}"/></p>
<p>where the last line follows inductively, since each <img alt="T_k" class="latex" src="https://s0.wp.com/latex.php?latex=T_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T_k"/> only sees <img alt="q_{k}-1" class="latex" src="https://s0.wp.com/latex.php?latex=q_%7Bk%7D-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_{k}-1"/> edges of <img alt="x_k" class="latex" src="https://s0.wp.com/latex.php?latex=x_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_k"/>.</p>
<p> </p>
<p>If we make the assumption that our model is a tree, then we can write the Bethe approximation entropy as</p>
<p><img alt="S_{Bethe}=-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)\ln(b(x_i,x_j))+\sum_{i}(q_i-1)\sum_{x_i}b(x_i)\ln b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=S_%7BBethe%7D%3D-%5Csum_%7Bi%2Cj%7D%5Csum_%7Bx_i%2Cx_j%7D+b%28x_i%2Cx_j%29%5Cln%28b%28x_i%2Cx_j%29%29%2B%5Csum_%7Bi%7D%28q_i-1%29%5Csum_%7Bx_i%7Db%28x_i%29%5Cln+b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S_{Bethe}=-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)\ln(b(x_i,x_j))+\sum_{i}(q_i-1)\sum_{x_i}b(x_i)\ln b(x_i)"/></p>
<p>Where the <img alt="\{q_i\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bq_i%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{q_i\}_{i}"/>‘s are the degrees of the variables <img alt="\{x_i\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bx_i%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{x_i\}_{i}"/> in the graphical model defined by <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>. We then define the Bethe free energy as <img alt="U+S_{Bethe}" class="latex" src="https://s0.wp.com/latex.php?latex=U%2BS_%7BBethe%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U+S_{Bethe}"/>. The Bethe free energy is in general not an upper bound on the true free energy. Note that if we make the assignments <img alt="E_{i}(x_i)=h_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=E_%7Bi%7D%28x_i%29%3Dh_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_{i}(x_i)=h_{i}(x_i)"/>, <img alt="E_{i,j}(x_i,x_j)=-J_{i,j}(x_i,x_j)-h_{i}(x_i)-h_{j}(x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=E_%7Bi%2Cj%7D%28x_i%2Cx_j%29%3D-J_%7Bi%2Cj%7D%28x_i%2Cx_j%29-h_%7Bi%7D%28x_i%29-h_%7Bj%7D%28x_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_{i,j}(x_i,x_j)=-J_{i,j}(x_i,x_j)-h_{i}(x_i)-h_{j}(x_j)"/>, then we can rewrite <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> as<br/>
<img alt="U=\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)E_{i,j}(x_i,x_j)+\sum_{i}(q_i-1)\sum_{x_i}b(x_i)E_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=U%3D%5Csum_%7Bi%2Cj%7D%5Csum_%7Bx_i%2Cx_j%7D+b%28x_i%2Cx_j%29E_%7Bi%2Cj%7D%28x_i%2Cx_j%29%2B%5Csum_%7Bi%7D%28q_i-1%29%5Csum_%7Bx_i%7Db%28x_i%29E_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U=\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)E_{i,j}(x_i,x_j)+\sum_{i}(q_i-1)\sum_{x_i}b(x_i)E_{i}(x_i)"/></p>
<p> </p>
<p>which is similar in form to the Bethe approximation entropy. In general, we have</p>
<p><img alt="G_{Bethe}(b(x_i),b(x_i,x_j))=" class="latex" src="https://s0.wp.com/latex.php?latex=G_%7BBethe%7D%28b%28x_i%29%2Cb%28x_i%2Cx_j%29%29%3D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{Bethe}(b(x_i),b(x_i,x_j))="/></p>
<p><img alt="\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)(E_{i,j}(x_i,x_j)+\ln(b(x_i,x_j)))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%2Cj%7D%5Csum_%7Bx_i%2Cx_j%7D+b%28x_i%2Cx_j%29%28E_%7Bi%2Cj%7D%28x_i%2Cx_j%29%2B%5Cln%28b%28x_i%2Cx_j%29%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)(E_{i,j}(x_i,x_j)+\ln(b(x_i,x_j)))"/></p>
<p><img alt="-\sum_{i}(q_i-1)\sum_{x_i}b(x_i)( E_{i}(x_i)+\ln b(x_i))" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Csum_%7Bi%7D%28q_i-1%29%5Csum_%7Bx_i%7Db%28x_i%29%28+E_%7Bi%7D%28x_i%29%2B%5Cln+b%28x_i%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="-\sum_{i}(q_i-1)\sum_{x_i}b(x_i)( E_{i}(x_i)+\ln b(x_i))"/></p>
<p>which is exactly the Gibbs free energy for a probabilistic model whose associated graph is a tree. Since BP gives the correct marginals on trees, we can say that the BP beliefs are the global minima of the Bethe free energy. However, the following is also true:</p>
<p><strong>Proposition:</strong> A set of beliefs gives a BP fixed point in any graph (not necessarily a tree) iff they correspond to local stationary points of the Bethe free energy.</p>
<p>(For a proof, see e.g. page 20 of [4])</p>
<p>So trying to minimize the Bethe free energy is in some sense the same thing as doing belief propagation. Apparently, one typically finds that when belief propagation fails to converge on a graph, the optimization program which is trying to minimize <img alt="G_{Bethe}" class="latex" src="https://s0.wp.com/latex.php?latex=G_%7BBethe%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{Bethe}"/> also runs into problems in similar parameter regions, and vice versa.</p>
<p> </p>
<h2>(3) The Block Model</h2>
<h3>(3.0) Definitions</h3>
<p>Now that we’ve seen Belief Propagation from two different perspectives, let’s try to apply this technique of computing marginals to analyzing the behavior of the stochastic block model. This section will heavily follow the paper [2].</p>
<p>The stochastic block model is designed to capture a variety of interesting problems, depending on its settings of parameters. The question we’ll be looking at is the following: suppose we generate a random graph, where each vertex of the graph comes from one of <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> groups each with probability <img alt="n_{1},...,n_{q}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7B1%7D%2C...%2Cn_%7Bq%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{1},...,n_{q}"/>. We add an edge between vertices <img alt="(i,j)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j)"/> in groups <img alt="a,b" class="latex" src="https://s0.wp.com/latex.php?latex=a%2Cb&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a,b"/> resp. with probability <img alt="p_{a,b}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Ba%2Cb%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{a,b}"/>. For sparse graphs, we define <img alt="c_{i,j}:=Np_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bi%2Cj%7D%3A%3DNp_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{i,j}:=Np_{i,j}"/>, where we think of <img alt="p_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{i,j}"/> as <img alt="\mathcal{O}(\frac{1}{N})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%5Cfrac%7B1%7D%7BN%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{O}(\frac{1}{N})"/>. The problem is the following: given such a random graph, can you label the vertices so that, up to permutation, the labels you choose have high correlation to the true hidden labels which were used to generate the graph? Here are some typical settings of parameters which represent different problems:</p>
<ol>
<li> Community detection, where we have <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> groups. We set <img alt="n_{i}=\frac{1}{q}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Bi%7D%3D%5Cfrac%7B1%7D%7Bq%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{i}=\frac{1}{q}"/>, and <img alt="c_{i,j}=c_{in}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bi%2Cj%7D%3Dc_%7Bin%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{i,j}=c_{in}"/> if <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i=j"/> and <img alt="c_{out}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bout%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{out}"/> otherwise, with <img alt="c_{in}&gt;c_{out}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bin%7D%3Ec_%7Bout%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{in}&gt;c_{out}"/> (assortative structure).</li>
<li>Planted graph partitioning, where <img alt="p_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{i,j}"/> may not necessarily be <img alt="\mathcal{O}(\frac{1}{N})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%5Cfrac%7B1%7D%7BN%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{O}(\frac{1}{N})"/>.</li>
<li>Planted graph colouring, where <img alt="p_{in}=0" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bin%7D%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{in}=0"/>, <img alt="p_{a,b}=p_{out}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Ba%2Cb%7D%3Dp_%7Bout%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{a,b}=p_{out}"/> and <img alt="n_{a}=\frac{1}{q}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Ba%7D%3D%5Cfrac%7B1%7D%7Bq%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{a}=\frac{1}{q}"/> for all groups. We know how to efficiently find a planted coloring which is strongly correlated with the true coloring when the average degree <img alt="c=(q-1)p_{out}N/q" class="latex" src="https://s0.wp.com/latex.php?latex=c%3D%28q-1%29p_%7Bout%7DN%2Fq&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c=(q-1)p_{out}N/q"/> is greater than <img alt="\mathcal{O}(q^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28q%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{O}(q^2)"/>.</li>
</ol>
<p>We’ll concern ourselves with the case where our graph is sparse, and we need to try and come up with an assignment for the vertices such that we have high correlation with the true labeling of vertices. How might we measure how well we solve this task? Ideally, a labeling which is identical to the true labeling (up to permutation) should get a score of 1. Conversely, a labeling which naively guesses that every vertex comes from the largest group <img alt="\arg \max_{i \in [q]} n_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Carg+%5Cmax_%7Bi+%5Cin+%5Bq%5D%7D+n_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\arg \max_{i \in [q]} n_{i}"/> should get a score of 0.  Here’s one metric which satisfies these properties: if we come up with a labeling <img alt="\{t_{i}\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bt_%7Bi%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{t_{i}\}"/>, and the true labeling is <img alt="\{q_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bq_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{q_i\}"/>, then we’ll measure our performance by</p>
<p> </p>
<p><img alt="Q(\{t_{i}\},\{q_{i}\}):=\max_{\pi \in S_q} \frac{\frac{1}{N}\sum_{i \in [N]} \delta_{t_i,\pi(q_i)}-max_{a \in [q]} n_a}{1-\max_{a \in [q]} n_a}" class="latex" src="https://s0.wp.com/latex.php?latex=Q%28%5C%7Bt_%7Bi%7D%5C%7D%2C%5C%7Bq_%7Bi%7D%5C%7D%29%3A%3D%5Cmax_%7B%5Cpi+%5Cin+S_q%7D+%5Cfrac%7B%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi+%5Cin+%5BN%5D%7D+%5Cdelta_%7Bt_i%2C%5Cpi%28q_i%29%7D-max_%7Ba+%5Cin+%5Bq%5D%7D+n_a%7D%7B1-%5Cmax_%7Ba+%5Cin+%5Bq%5D%7D+n_a%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q(\{t_{i}\},\{q_{i}\}):=\max_{\pi \in S_q} \frac{\frac{1}{N}\sum_{i \in [N]} \delta_{t_i,\pi(q_i)}-max_{a \in [q]} n_a}{1-\max_{a \in [q]} n_a}"/></p>
<p> </p>
<p>where we maximize over all permutations <img alt="\pi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\pi"/>. When we choose a labeling which (up to permutation) agrees with the true labeling, then the numerator of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/> will equal the denominator, and <img alt="Q=1" class="latex" src="https://s0.wp.com/latex.php?latex=Q%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q=1"/>. Likewise, when we trivially guess that every vertex belongs to the largest group, then the numerator of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/> is <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> and <img alt="Q=0" class="latex" src="https://s0.wp.com/latex.php?latex=Q%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q=0"/>.</p>
<p>Given <img alt="\{c_{a,b}\},\{n_a\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bc_%7Ba%2Cb%7D%5C%7D%2C%5C%7Bn_a%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{c_{a,b}\},\{n_a\}"/> and a set of observed edges <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E"/>, we can write down the probability of a labeling <img alt="\{q_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bq_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{q_i\}"/> as<br/>
<img alt="P(\{q_i\}_{i})=\prod_{\substack{(i,j) \not \in E \\ i \not = j}}(1-p_{q_{i},q_{j}})\prod_{\substack{(i,j) \in E }}p_{q_{i},q_{j}} \prod_{i \in [q]}n_{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5C%7Bq_i%5C%7D_%7Bi%7D%29%3D%5Cprod_%7B%5Csubstack%7B%28i%2Cj%29+%5Cnot+%5Cin+E+%5C%5C+i+%5Cnot+%3D+j%7D%7D%281-p_%7Bq_%7Bi%7D%2Cq_%7Bj%7D%7D%29%5Cprod_%7B%5Csubstack%7B%28i%2Cj%29+%5Cin+E+%7D%7Dp_%7Bq_%7Bi%7D%2Cq_%7Bj%7D%7D+%5Cprod_%7Bi+%5Cin+%5Bq%5D%7Dn_%7Bq_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\{q_i\}_{i})=\prod_{\substack{(i,j) \not \in E \\ i \not = j}}(1-p_{q_{i},q_{j}})\prod_{\substack{(i,j) \in E }}p_{q_{i},q_{j}} \prod_{i \in [q]}n_{q_i}"/></p>
<p>How might we try to infer <img alt="\{q_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bq_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{q_i\}"/> such that we have maximum correlation (up to permutation) with the true labeling? It turns out that the answer is to use the maximum likelihood estimator of the marginal distribution of each <img alt="q_i" class="latex" src="https://s0.wp.com/latex.php?latex=q_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_i"/>, up to a caveat. In particular, we should label <img alt="q_i" class="latex" src="https://s0.wp.com/latex.php?latex=q_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_i"/> with the <img alt="r \in [q]" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cin+%5Bq%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \in [q]"/> such that <img alt="P(q_i=r)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28q_i%3Dr%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(q_i=r)"/> is maximized. The caveat comes in when <img alt="P(\{q_i\}_{i})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5C%7Bq_i%5C%7D_%7Bi%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\{q_i\}_{i})"/> is invariant under permutations of the labelings <img alt="\{q_i\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bq_i%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{q_i\}_{i}"/>, so that each marginal <img alt="P(q_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28q_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(q_i)"/> is actually the uniform distribution. For example, this happens in community detection, when all the group sizes <img alt="n_{1},...,n_{q}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7B1%7D%2C...%2Cn_%7Bq%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{1},...,n_{q}"/> are equal. In this case, the correct thing to do is to still use the marginals, but only after we have “broken the symmetry” of the problem by randomly fixing certain values of the vertices to have particular labels. There’s actually a way belief propagation does this implicitly: recall that we start belief propagation by randomly initializing the messages. This random initialization can be interpreted as “symmetry breaking” of the problem, in a way that we’ll see shortly.</p>
<h2/>
<h3>(3.1) Belief Propagation</h3>
<p>We’ve just seen from the previous section that in order to maximize the correlation of the labeling we come up with, we should pick the labelings which maximize the marginals of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/>. So we have some marginals that we want to compute. Let’s proceed by applying BP to this problem in the “sparse” regime where <img alt="c_{a,b}=Np_{a,b}=\mathcal{O}(1)" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Ba%2Cb%7D%3DNp_%7Ba%2Cb%7D%3D%5Cmathcal%7BO%7D%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{a,b}=Np_{a,b}=\mathcal{O}(1)"/> (other algorithms, like approximate message passing, can be used for “dense” graph problems). Suppose we’re given a random graph with edge list <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E"/>. What does does graph associated with our probabilistic model look like? Well, in this case, every variable is actually connected to every other variable because <img alt="P(\{q_i\}_{i})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5C%7Bq_i%5C%7D_%7Bi%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\{q_i\}_{i})"/> includes a factor <img alt="f_{i,j}(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bi%2Cj%7D%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{i,j}(x_i,x_j)"/> for every <img alt="(i,j) \in [n] \times [n]" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29+%5Cin+%5Bn%5D+%5Ctimes+%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j) \in [n] \times [n]"/>, so we actually have a complete graph. However, some of the connections between variables are much weaker than others. In full, our BP update equations are</p>
<p><img alt="eqs_2" class="alignnone size-full wp-image-6280" src="https://windowsontheory.files.wordpress.com/2018/10/eqs_2.png?w=600"/></p>
<p>Likewise<br/>
<img alt="\psi^{i}_{t_{i}}=\frac{1}{Z^{i}} n_{t_{i}} \prod_{\substack{(i,k) \not \in E}}\left[ 1-\frac{1}{N}\sum_{t_k \in [q]}c_{t_1,t_k}\psi^{k \rightarrow i}_{t_k} \right]\prod_{\substack{(i,k) \in E}}\left[\sum_{t_k \in [q]}c_{t_i,t_k} \psi^{k \rightarrow i}_{t_k} \right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi%7D_%7Bt_%7Bi%7D%7D%3D%5Cfrac%7B1%7D%7BZ%5E%7Bi%7D%7D+n_%7Bt_%7Bi%7D%7D+%5Cprod_%7B%5Csubstack%7B%28i%2Ck%29+%5Cnot+%5Cin+E%7D%7D%5Cleft%5B+1-%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bt_k+%5Cin+%5Bq%5D%7Dc_%7Bt_1%2Ct_k%7D%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bt_k%7D+%5Cright%5D%5Cprod_%7B%5Csubstack%7B%28i%2Ck%29+%5Cin+E%7D%7D%5Cleft%5B%5Csum_%7Bt_k+%5Cin+%5Bq%5D%7Dc_%7Bt_i%2Ct_k%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bt_k%7D+%5Cright%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i}_{t_{i}}=\frac{1}{Z^{i}} n_{t_{i}} \prod_{\substack{(i,k) \not \in E}}\left[ 1-\frac{1}{N}\sum_{t_k \in [q]}c_{t_1,t_k}\psi^{k \rightarrow i}_{t_k} \right]\prod_{\substack{(i,k) \in E}}\left[\sum_{t_k \in [q]}c_{t_i,t_k} \psi^{k \rightarrow i}_{t_k} \right]"/></p>
<p> </p>
<p>what we want to do is approximate these equations so that we only have to pass messages along the edges <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E"/>, instead of the complete graph. This will make our analysis simpler, and also allow the belief propagation algorithm to run more efficiently. The first observation is the following: Suppose we have two nodes <img alt="j,j' \in [N]" class="latex" src="https://s0.wp.com/latex.php?latex=j%2Cj%27+%5Cin+%5BN%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j,j' \in [N]"/> such that <img alt="(i,j),(i,j') \not \in E" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29%2C%28i%2Cj%27%29+%5Cnot+%5Cin+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j),(i,j') \not \in E"/>. Then we see that <img alt="\psi^{i \rightarrow j}_{t_{i}}=\psi^{i \rightarrow j'}_{t_{i}}+\mathcal{O}\left(\frac{1}{N}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bt_%7Bi%7D%7D%3D%5Cpsi%5E%7Bi+%5Crightarrow+j%27%7D_%7Bt_%7Bi%7D%7D%2B%5Cmathcal%7BO%7D%5Cleft%28%5Cfrac%7B1%7D%7BN%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{t_{i}}=\psi^{i \rightarrow j'}_{t_{i}}+\mathcal{O}\left(\frac{1}{N}\right)"/>, since the only difference between these two variables are two factors of order <img alt="(1-\mathcal{O}\left(\frac{1}{N}\right))" class="latex" src="https://s0.wp.com/latex.php?latex=%281-%5Cmathcal%7BO%7D%5Cleft%28%5Cfrac%7B1%7D%7BN%7D%5Cright%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(1-\mathcal{O}\left(\frac{1}{N}\right))"/> which appear in the first product of the BP equations. Thus, we send essentially the same messages to non-neighbours <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j"/> of <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> in our random graph. In general though, we have:</p>
<p><img alt="eqs_3.png" class="alignnone size-full wp-image-6281" src="https://windowsontheory.files.wordpress.com/2018/10/eqs_3.png?w=600"/></p>
<p>The first approximation comes from dropping non-edge constraints on the first product, and is reasonable because we expect the number of neighbours of <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> to be constant. We’ve also defined a variable</p>
<p><img alt="h_{t_i}:=\frac{1}{N}\sum_{k \in [N]}\sum_{t_k \in [q]}c_{t_i,t_k} \psi^{k \rightarrow i}_{t_k}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bt_i%7D%3A%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bk+%5Cin+%5BN%5D%7D%5Csum_%7Bt_k+%5Cin+%5Bq%5D%7Dc_%7Bt_i%2Ct_k%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bt_k%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{t_i}:=\frac{1}{N}\sum_{k \in [N]}\sum_{t_k \in [q]}c_{t_i,t_k} \psi^{k \rightarrow i}_{t_k}"/></p>
<p>and we’ve used the approximation <img alt="e^{-x} \approx 1-x" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B-x%7D+%5Capprox+1-x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e^{-x} \approx 1-x"/> for small <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>. We think of the term <img alt="h_{t_i}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bt_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{t_i}"/> as defining an “auxiliary external field”. We’ll use this approximate BP equation to find solutions for our problem. This has the advantage that the computation time is <img alt="\mathcal{O}(|E|)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%7CE%7C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{O}(|E|)"/> instead of <img alt="\mathcal{O}(N^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28N%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{O}(N^2)"/>, so we can deal with large sparse graphs computationally. It also allows us to see how a large dense graphical model with only sparse strong connections still behaves like a sparse tree-like graphical model from the perspective of Belief Propagation. In particular, we might have reason to hope that the BP equations will actually converge and give us good approximations to the marginals.</p>
<p>From now on, we’ll only consider factored block models, which in some sense represent a “hard” setting of parameters. These are models which satisfy the condition that each group has the same average degree <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/>. In particular, we require</p>
<p><img alt="\sum_{d=1}^{q} c_{a,d} n_d = \sum_{d=1}^{q} c_{b,d}n_d =c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bd%3D1%7D%5E%7Bq%7D+c_%7Ba%2Cd%7D+n_d+%3D+%5Csum_%7Bd%3D1%7D%5E%7Bq%7D+c_%7Bb%2Cd%7Dn_d+%3Dc&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{d=1}^{q} c_{a,d} n_d = \sum_{d=1}^{q} c_{b,d}n_d =c"/></p>
<p>An important observation for this setting of parameters is that</p>
<p><img alt="\psi^{i \rightarrow j}_{t_i}=n_{t_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bt_i%7D%3Dn_%7Bt_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{t_i}=n_{t_i}"/></p>
<p>is always a fixed point of our BP equations, which is known as a <em>factored fixed point</em> (this can be seen by inspection by plugging the fixed point conditions into the belief propagation equations we derived). When BP ever reaches such a fixed point, we get that <img alt="Q=0" class="latex" src="https://s0.wp.com/latex.php?latex=Q%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q=0"/> and the algorithm fails. However, we might hope that if we randomly initialize <img alt="\{\psi^{i \rightarrow j}\}_{(i,j) \in E}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpsi%5E%7Bi+%5Crightarrow+j%7D%5C%7D_%7B%28i%2Cj%29+%5Cin+E%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\psi^{i \rightarrow j}\}_{(i,j) \in E}"/>, then BP might converge to some non-trivial fixed point which gives us some information about the original labeling of the vertices.</p>
<h3>(3.2) Numerical Results</h3>
<p>Now that we have our BP equations, we can run numerical simulations to try and get a feel of when BP works. Let’s consider the problem of community detection. In particular, we’ll set our parameters with all group sizes <img alt="n_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{a}"/> being equal, and with <img alt="c_{a,a}=c_{in}&gt;c_{out}=c_{a,b}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Ba%2Ca%7D%3Dc_%7Bin%7D%3Ec_%7Bout%7D%3Dc_%7Ba%2Cb%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{a,a}=c_{in}&gt;c_{out}=c_{a,b}"/> for <img alt="a \not = b" class="latex" src="https://s0.wp.com/latex.php?latex=a+%5Cnot+%3D+b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a \not = b"/> and vary the ratio <img alt="\epsilon:=c_{out}/c_{in}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3A%3Dc_%7Bout%7D%2Fc_%7Bin%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon:=c_{out}/c_{in}"/>, and see when BP finds solutions which are correlated “better than guessing” to the original labeling used to generate the graph. When we do this, we get images which look like this:</p>
<p><img alt="convergence_and_overlap.png" class="alignnone size-full wp-image-6274" src="https://windowsontheory.files.wordpress.com/2018/10/convergence_and_overlap.png?w=600"/></p>
<p>It should be mentioned that the point at which the dashed red line occurs depends on the parameters of the stochastic block model. We get a few interesting observations from numerical experiments:</p>
<ol>
<li>The point at which BP fails (<img alt="Q=0" class="latex" src="https://s0.wp.com/latex.php?latex=Q%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q=0"/>) happens at a certain value of <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/> independent of <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/>. In particular, when <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/> gets too large, BP converges to the trivial factored fixed point. We’ll see in the next section that this is a “stable” fixed point for certain settings of parameters.</li>
<li>Using the approximate BP equations gives the same performance as using the exact BP equations. This numerically justifies the approximations we made.</li>
<li>If we try to estimate the marginals with other tools from our statistical inference toolbox, we find (perhaps surprisingly) that Markov Chain Monte Carlo methods give the same performance as Belief Propagation, even though they take significantly longer to run and have strong asymptotic guarantees on their correctness (if you run MCMC long enough, you will eventually get the correct marginals, but this may take exponential time). This naturally suggests the conjecture that belief propagation is optimal for this task.</li>
<li>As we get closer to the region where BP fails and converges to the trivial fixed point, the number of iterations required for BP to converge increases significantly, and diverges at the critical point <img alt="\epsilon_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_{c}"/> where BP fails and converges to the factored fixed point. The same kind of behavior is seen when using Gibbs sampling.</li>
</ol>
<h3>(3.3) Heuristic Stability Calculations</h3>
<p>How might we analytically try to determine when BP fails for certain settings of <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> and <img alt="\{c_{a,b}\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bc_%7Ba%2Cb%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{c_{a,b}\}"/>? One way we might heuristically try to do this, is to calculate the stability of the factored fixed point. If the fixed point is stable, this suggests that BP will converge to a factored point. If however it is unstable, then we might hope that BP converges to something informative. In particular, suppose we run BP, and we converge to a factored fixed point, so we have that for all our messages <img alt="\psi_{t_i}^{i \rightarrow j}=n_{t_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bt_i%7D%5E%7Bi+%5Crightarrow+j%7D%3Dn_%7Bt_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{t_i}^{i \rightarrow j}=n_{t_i}"/>. Suppose we now add a small amount of noise to some of the <img alt="\psi_{t_i}^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bt_i%7D%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{t_i}^{i \rightarrow j}"/>‘s (maybe think of this as injecting a small amount of additional information about the true marginals). We (heuristically) claim that if we now continue to run more steps of BP, either the messages will converge back to the fixed point <img alt="\psi_{t_i}^{i \rightarrow j}=n_{t_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bt_i%7D%5E%7Bi+%5Crightarrow+j%7D%3Dn_%7Bt_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{t_i}^{i \rightarrow j}=n_{t_i}"/>, or they will diverge to something else, and whether or not this happens depends on the eigenvalue of some matrix of partial derivatives.</p>
<p>Following this idea, here’s a heuristic way of calculating the stability of the factored fixed point. Let’s pretend that our BP equations occur on a tree, which is a reasonable approximation in the sparse graph case. Let our tree be rooted at node <img alt="k_0" class="latex" src="https://s0.wp.com/latex.php?latex=k_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k_0"/> and have depth <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>. Let’s try to approximately calculate the influence on <img alt="k_0" class="latex" src="https://s0.wp.com/latex.php?latex=k_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k_0"/> of perturbing a leaf <img alt="k_d" class="latex" src="https://s0.wp.com/latex.php?latex=k_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k_d"/> from its factored fixed point. In particular, let the path from the leaf to the root be <img alt="k_d,...,k_0" class="latex" src="https://s0.wp.com/latex.php?latex=k_d%2C...%2Ck_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k_d,...,k_0"/>. We’re going to apply a perturbation <img alt="\psi^{k_d}_{t} = n_t+\epsilon_{t}^{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bk_d%7D_%7Bt%7D+%3D+n_t%2B%5Cepsilon_%7Bt%7D%5E%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{k_d}_{t} = n_t+\epsilon_{t}^{k}"/> for each <img alt="t \in [q]" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Cin+%5Bq%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t \in [q]"/>. In vector notation, this looks like <img alt="\psi^{k_d} = \mathbf{1}n_t+\epsilon^{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bk_d%7D+%3D+%5Cmathbf%7B1%7Dn_t%2B%5Cepsilon%5E%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{k_d} = \mathbf{1}n_t+\epsilon^{k}"/>, where <img alt="\psi^{k_d} \in \mathbb{R}^{q \times 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bk_d%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bq+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{k_d} \in \mathbb{R}^{q \times 1}"/> is a column vector. The next thing we’ll do is define the matrix of partial derivatives</p>
<p><img alt="eqs_4.png" class="alignnone size-full wp-image-6282" src="https://windowsontheory.files.wordpress.com/2018/10/eqs_4.png?w=600"/></p>
<p>Up to first order (and ignoring normalizing constants), the perturbation effect on <img alt="\epsilon^{k_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_0}"/> is then (by chain rule) <img alt="\epsilon^{k_0} = \left[\prod_{i=0}^{d-1} T_{i} \right]\epsilon^{k_d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_0%7D+%3D+%5Cleft%5B%5Cprod_%7Bi%3D0%7D%5E%7Bd-1%7D+T_%7Bi%7D+%5Cright%5D%5Cepsilon%5E%7Bk_d%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_0} = \left[\prod_{i=0}^{d-1} T_{i} \right]\epsilon^{k_d}"/>. Since <img alt="T_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=T_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T_{i}"/> does not depend on <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>, we can write this as <img alt="\epsilon^{k_0}=T^{d} \epsilon^{k_d} \approx \lambda^{d} \epsilon^{k_d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_0%7D%3DT%5E%7Bd%7D+%5Cepsilon%5E%7Bk_d%7D+%5Capprox+%5Clambda%5E%7Bd%7D+%5Cepsilon%5E%7Bk_d%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_0}=T^{d} \epsilon^{k_d} \approx \lambda^{d} \epsilon^{k_d}"/>, where <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> is the largest eigenvalue of <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/>. Now, on a random tree, we have approximately <img alt="c^{d}" class="latex" src="https://s0.wp.com/latex.php?latex=c%5E%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c^{d}"/> leaves. If we assume that the perturbation effect from each leaf is independent, and that <img alt="\epsilon^{k_d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_d%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_d}"/> has 0 mean, then the net mean perturbation from all the leaves will be 0. The variance will be</p>
<p><img alt="eqs_5.png" class="alignnone  wp-image-6283" height="152" src="https://windowsontheory.files.wordpress.com/2018/10/eqs_5.png?w=307&amp;h=152" width="307"/></p>
<p>if we assume that the cross terms vanish in expectation.</p>
<p>(Aside: You might want to ask: why are we assuming that <img alt="\epsilon^{k_{d}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_%7Bd%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_{d}}"/> has mean zero, and that (say) the noise at each of the leaves are independent, so that the cross terms vanish? If we want to maximize the variance, then maybe choosing the <img alt="\epsilon^{k_{d}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_%7Bd%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_{d}}"/>‘s to be correlated or have non-zero mean would give us a better bound. The problem is that we’re neglecting the effects of normalizing constants in this analysis: if we perturbed all the <img alt="\psi^{k_d}_{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bk_d%7D_%7Bt%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{k_d}_{t}"/> in the same direction (e.g. non-zero mean), our normalization conditions would cancel out our perturbations.)</p>
<p>We Therefore end up with the stability condition <img alt="c \lambda^{2} =1" class="latex" src="https://s0.wp.com/latex.php?latex=c+%5Clambda%5E%7B2%7D+%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c \lambda^{2} =1"/>. When <img alt="c \lambda^{2}&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=c+%5Clambda%5E%7B2%7D%3E1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c \lambda^{2}&gt;1"/>, a small perturbation will be magnified as we move up the tree, leading to the messages moving away from the factored fixed point after successive iterations of BP (the fixed point is unstable). If <img alt="c \lambda^{2}&lt;1" class="latex" src="https://s0.wp.com/latex.php?latex=c+%5Clambda%5E%7B2%7D%3C1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c \lambda^{2}&lt;1"/>, the effect of a small perturbation will vanish as we move up the tree, we expect the factored fixed point to be stable. If we restrict our attention to graphs of the form <img alt="c_{a,a}=c_{in}&gt;c_{a,b}=c_{out}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Ba%2Ca%7D%3Dc_%7Bin%7D%3Ec_%7Ba%2Cb%7D%3Dc_%7Bout%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{a,a}=c_{in}&gt;c_{a,b}=c_{out}"/> for <img alt="a \not = b" class="latex" src="https://s0.wp.com/latex.php?latex=a+%5Cnot+%3D+b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a \not = b"/>, and have all our groups with size <img alt="n_{a}=\frac{1}{q}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Ba%7D%3D%5Cfrac%7B1%7D%7Bq%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{a}=\frac{1}{q}"/>, then <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> is known to have eigenvalues <img alt="\lambda_{1}=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_%7B1%7D%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_{1}=0"/> with eigenvector <img alt="(1,...,1)" class="latex" src="https://s0.wp.com/latex.php?latex=%281%2C...%2C1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(1,...,1)"/>, and <img alt="\lambda_{2}=(c_{in}-c_{out})/qc" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_%7B2%7D%3D%28c_%7Bin%7D-c_%7Bout%7D%29%2Fqc&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_{2}=(c_{in}-c_{out})/qc"/>. The stability threshold then becomes <img alt="|c_{in}-c_{out}|=q\sqrt{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cc_%7Bin%7D-c_%7Bout%7D%7C%3Dq%5Csqrt%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|c_{in}-c_{out}|=q\sqrt{c}"/>. This condition is known as the Almeida-Thouless local stability condition for spin glasses, and the Kesten-Stigum bound on reconstruction on trees. It is also observed empirically that BP and MCMC succeed above this threshold, and converge to factored fixed points below this threshold. The eigenvalues of <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> are related to the belief propagation equations and the backtracking matrix. For more details, see [3]</p>
<h3>(3.4) Information Theoretic Results, and what we know algorithmically</h3>
<p>We’ve just seen a threshold for when BP is able to solve the community detection problem. Specifically, when <img alt="c&lt;\frac{1}{\lambda^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=c%3C%5Cfrac%7B1%7D%7B%5Clambda%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c&lt;\frac{1}{\lambda^{2}}"/>, BP doesn’t do better than chance. It’s natural to ask whether this is because BP is not powerful enough, or whether there really isn’t enough information in the random graph to recover the true labeling. For example, if <img alt="c_{out}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bout%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{out}"/> is very close to <img alt="c_{in}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bin%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{in}"/>, it might be impossible to distinguish between group boundaries up to random fluctuations in the edges.</p>
<p>It turns out that for <img alt="q=2" class="latex" src="https://s0.wp.com/latex.php?latex=q%3D2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q=2"/>, there is not enough information below the threshold <img alt="c&lt;\frac{1}{\lambda^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=c%3C%5Cfrac%7B1%7D%7B%5Clambda%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c&lt;\frac{1}{\lambda^{2}}"/> to find a labeling which is correlated with the true labeling [3]. However, it can be shown information-theoretically [1] that the threshold at which one can find a correlated labeling is <img alt="c_{c} = \Theta \left(\frac{\log(q)}{q \lambda^2} \right)" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bc%7D+%3D+%5CTheta+%5Cleft%28%5Cfrac%7B%5Clog%28q%29%7D%7Bq+%5Clambda%5E2%7D+%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{c} = \Theta \left(\frac{\log(q)}{q \lambda^2} \right)"/>. In particular, when <img alt="q&gt;11" class="latex" src="https://s0.wp.com/latex.php?latex=q%3E11&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q&gt;11"/>, there exists exponential time algorithms which recover a correlated labeling below the Kesten-Stigum threshold. This is interesting, because it suggests an information-computational gap: we observe empirically that heuristic belief propagation seems to perform as well as any other inference algorithm at finding a correlated labeling for the stochastic block model. However, belief propagation fails at a “computational” threshold below the information theoretic threshold for this problem. We’ll talk more about these kinds of information-computation gaps in the coming weeks.</p>
<p> </p>
<p> </p>
<h2>References</h2>
<p>[1] Jess Banks, Cristopher Moore, Joe Neeman, Praneeth Netrapalli,<br/>
<em>Information-theoretic thresholds for community detection in sparse</em><br/>
<em>networks</em>. AJMLR: Workshop and Conference Proceedings vol 49:1–34, 2016.<br/>
<a href="http://proceedings.mlr.press/v49/banks16.pdf">Link</a></p>
<p>[2] Aurelien Decelle, Florent Krzakala, Cristopher Moore, Lenka Zdeborová,<br/>
<em>Asymptotic analysis of the stochastic block model for modular networks and its</em><br/>
<em>algorithmic applications</em>. 2013.<br/>
<a href="https://arxiv.org/pdf/1109.3041.pdf">Link</a></p>
<p>[3] Cristopher Moore, <em>The Computer Science and Physics</em><br/>
<em>of Community Detection: </em><em>Landscapes, Phase Transitions, </em><em>and Hardness</em>. 2017.<br/>
<a href="https://arxiv.org/pdf/1702.00467.pdf">Link</a></p>
<p>[4] Jonathan Yedidia, William Freeman, Yair Weiss, U<em>nderstanding Belief Propogation and its Generalizations<br/>
</em><a href="http://people.csail.mit.edu/billf/publications/Understanding_Belief_Propogation.pdf">Link</a></p>
<p>[5] Afonso Banderia, Amelia Perry, Alexander Wein, <em>Notes on computational-to-statistical gaps: predictions using statistical physics</em>. 2018.<br/>
<a href="https://arxiv.org/pdf/1803.11132.pdf">Link</a></p>
<p>[6] Stephan Mertens, Marc Mézard, Riccardo Zecchina, <em>Threshold Values of Random K-SAT from the </em><em>Cavity Method</em>. 2005.<br/>
<a href="https://arxiv.org/pdf/cs/0309020.pdf">Link</a></p>
<p>[7] Andrea Montanari, Federico Ricci-Tersenghi, Guilhem Semerjian, <em>Clusters of solutions and replica symmetry breaking in </em><em>random k-satisfiability</em>. 2008.<br/>
<a href="https://arxiv.org/pdf/0802.3627.pdf">Link</a></p>
<p>A big thanks to Tselil for all the proof reading and recommendations, and to both Boaz and Tselil for their really detailed post-presentation feedback.</p></div>
    </content>
    <updated>2018-10-20T12:30:18Z</updated>
    <published>2018-10-20T12:30:18Z</published>
    <category term="physics"/>
    <author>
      <name>7omj0</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-17T05:29:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6266</id>
    <link href="https://windowsontheory.org/2018/10/04/be-a-program-director-at-nsf/" rel="alternate" type="text/html"/>
    <title>Be a program director at NSF!</title>
    <summary>Guest post by Shuchi Chawla One of the best ways to serve the US-based TCS community is to take up a position at the NSF. Beginning as early as 2019, NSF/CCF is seeking at least one program director for the Algorithmic Foundations core program. This is a rotator position, which is generally two or three years in […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Guest post by Shuchi Chawla</em></p>
<p style="font-weight: 400;">One of the best ways to serve the US-based TCS community is to take up a position at the NSF. Beginning as early as 2019, NSF/CCF is seeking at least one program director for the Algorithmic Foundations core program. This is a rotator position, which is generally two or three years in duration. Please consider applying!</p>
<p style="font-weight: 400;">Besides service to the community, there are many other benefits from serving:</p>
<p><span style="font-weight: 400;">– It’s an opportunity to meet a lot of people in one’s own field and others, and to become more well-known in research communities. Some institutions place value on the experience. Many rotators are able to use it to enhance career options.<br/>
</span><br style="font-weight: 400;"/><span style="font-weight: 400;">– A rotator can typically spend 20% (NSF-paid) time on research, including visits back to the home institution. The impact on research and advising may be considerable, but does not have to be a complete hiatus.<br/>
</span><br style="font-weight: 400;"/><span style="font-weight: 400;">– There is a wealth of opportunities for cultural and educational experiences for families who relocate to the area for a few years, which some find to offset the very considerable impacts associated with such a move.</span></p>
<p>The official posting for AF won’t appear until later, but postings for similar positions can be found here: <a href="https://www.nsf.gov/careers/openings/">https://www.nsf.gov/careers/openings/</a>. For further information, please reach out to Tracy Kimbrel (<a href="mailto:tkimbrel@nsf.gov">tkimbrel@nsf.gov</a>) or Shuchi Chawla (<a href="mailto:shuchi@cs.wisc.edu">shuchi@cs.wisc.edu</a>).</p></div>
    </content>
    <updated>2018-10-04T00:15:43Z</updated>
    <published>2018-10-04T00:15:43Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-17T05:29:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6204</id>
    <link href="https://windowsontheory.org/2018/09/15/statistical-physics-an-introduction-in-two-parts/" rel="alternate" type="text/html"/>
    <title>Statistical Physics: an introduction in two parts</title>
    <summary>Statistical physics has deep connections to many computational problems, including statistical inference, counting and sampling, and optimization. Perhaps especially compelling are the field’s insights and intuitions regarding “average-case complexity” and information-computation gaps. These are topics for which the traditional theoretical CS approaches seem ill-suited, while on the other hand statistical physics has supplied a rich […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Statistical physics has deep connections to many computational problems, including statistical inference, counting and sampling, and optimization. Perhaps especially compelling are the field’s insights and intuitions regarding “average-case complexity” and information-computation gaps. These are topics for which the traditional theoretical CS approaches seem <a href="http://www.cse.cuhk.edu.hk/~andrejb/pubs/average-now.pdf">ill-suited</a>, while on the other hand statistical physics has supplied a rich (albeit not always mathematically rigorous) theory.</p>
<p>Statistical physics is the first topic in the <a href="https://www.boazbarak.org/fall18seminar/">seminar course</a> I am co-teaching with Boaz this fall, and one of our primary goals is to explore this theory. This blog post is a re-working of a lecture I gave in class this past Friday. It is meant to serve as an introduction to statistical physics, and is composed of two parts: in the first part, I introduce the basic concepts from statistical physics in a hands-on manner, by demonstrating a phase transition for the Ising model on the complete graph. In the second part, I introduce random k-SAT and the satisfiability conjecture, and give some moment-method based proofs of bounds on the satisfiability threshold.</p>
<p><span style="color: #999999;"><em>Update on September 16, 3:48pm: the first version of this post contained an incorrect plot of the energy density of the Ising model on the complete graph, which I have amended below.</em></span></p>
<h2>I. From particle interactions to macroscopic behaviors</h2>
<p>In statistical physics, the goal is to understand how materials behave on a macroscopic scale based on a simple model of particle-particle interactions.</p>
<p>For example, consider a block of iron. In a block of iron, we have many iron particles, and each has a net <img alt="\{\pm 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}"/>-polarization or “spin” which is induced by the quantum spins of its unpaired electrons. On the microscopic scale, nearby iron atoms “want” to have the same spin. From what I was able to gather on <a href="https://en.wikipedia.org/wiki/Ferromagnetism#Explanation" title="Explanation of Ferromagnetism">Wikipedia</a>, this is because the unpaired electrons in the distinct iron atoms repel each other, and if two nearby iron atoms have the same spins, then this allows them to be in a physical configuration where the atoms are further apart in space, which results in a <strong>lower energy state</strong> (because of the repulsion between electrons).</p>
<p>When most of the particles in a block of iron have correlated spins, then on a macroscopic scale we observe this correlation as the phenomenon of magnetism (or ferromagnetism if we want to be technically correct).</p>
<p>In the 1890’s, Pierre Curie showed that if you heat up a block of iron (introducing energy into the system), it eventually loses its magnetization. In fact, magnetization exhibits a <strong>phase transition</strong>: there is a critical temperature, <img alt="T_c" class="latex" src="https://s0.wp.com/latex.php?latex=T_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T_c"/>, below which a block of iron will act as a magnet, and above which it will suddenly lose its magnetism. This is called the “Curie temperature”. This phase transition is in contrast to the alternative, in which the iron would gradually lose its magnetization as it is heated.</p>
<p><img alt="trans" class="alignnone size-full wp-image-6217" src="https://windowsontheory.files.wordpress.com/2018/09/trans.png?w=600"/></p>
<h3>The Ising Model</h3>
<p>We’ll now set up a simple model of the microscopic particle-particle interactions, and see how the global phenomenon of the magnetization phase transition emerges. This is called the <em>Ising model</em>, and it is one of the more canonical models in statistical physics.</p>
<p>Suppse that we have <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> iron atoms, and that their interactions are described by the (for simplicity unweighted) graph <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> with adjacency matrix <img alt="A_G" class="latex" src="https://s0.wp.com/latex.php?latex=A_G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A_G"/>. For example, we may think of the atoms as being arranged in a 3D cubic lattice, and then <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> would be the 3D cubic lattice graph. We give each atom a label in <img alt="[n] = \{1,\ldots,n\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bn%5D+%3D+%5C%7B1%2C%5Cldots%2Cn%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[n] = \{1,\ldots,n\}"/>, and we associate with each atom <img alt="i \in [n]" class="latex" src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i \in [n]"/> a spin <img alt="x_i \in \{\pm 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i \in \{\pm 1\}"/>.</p>
<p>For each choice of spins or <em>state</em> <img alt="x \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \{\pm 1\}^n"/> we associate the <em>total energy</em></p>
<p><img alt="E(x) = - x^\top A_G x= \sum_{(i,j) \in G} - x_i x_j" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29+%3D+-+x%5E%5Ctop+A_G+x%3D+%5Csum_%7B%28i%2Cj%29+%5Cin+G%7D+-+x_i+x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E(x) = - x^\top A_G x= \sum_{(i,j) \in G} - x_i x_j"/>.</p>
<p>If two interacting particles have the same spin, then they are in a “lower energy” configuration, and then they contribute <img alt="-1" class="latex" src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="-1"/> to the total energy. If two neighboring particles have opposite spins, then they are in a “higher energy” configuration, and they contribute <img alt="+1" class="latex" src="https://s0.wp.com/latex.php?latex=%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="+1"/> to the total energy.</p>
<p>We also introduce a <em>temperature</em> parameter <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/>. At each <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/>, we want to describe what a “typical” configuration for our block of iron looks like. When <img alt="T=0" class="latex" src="https://s0.wp.com/latex.php?latex=T%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T=0"/>, there is no kinetic energy in the system, so we expect the system to be in the lowest-energy state, i.e. all atoms have the same spin. As the temperature increases, the kinetic energy also increases, and we will begin to see more anomalies.</p>
<p>In statistical physics, the “description” takes the form of a probability distribution over states <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>. To this end we define the <em>Boltzmann distribution</em>, with density function <img alt="P_{\beta}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_{\beta}(x)"/>:</p>
<p><img alt="P_{\beta}(x) \propto \exp(-\beta E(x)), \qquad \text{where } \beta = \frac{1}{T}." class="latex" src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D%28x%29+%5Cpropto+%5Cexp%28-%5Cbeta+E%28x%29%29%2C+%5Cqquad+%5Ctext%7Bwhere+%7D+%5Cbeta+%3D+%5Cfrac%7B1%7D%7BT%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_{\beta}(x) \propto \exp(-\beta E(x)), \qquad \text{where } \beta = \frac{1}{T}."/></p>
<p>As <img alt="T\to 0" class="latex" src="https://s0.wp.com/latex.php?latex=T%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T\to 0"/>, <img alt="\beta\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta\to\infty"/>, <img alt="P_{\beta}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_{\beta}(x)"/> becomes supported entirely on the <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> that minimize <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E(x)"/>; we call these the <em>ground states</em> (for connected <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> these are exactly <img alt="x = \pm \vec{1}" class="latex" src="https://s0.wp.com/latex.php?latex=x+%3D+%5Cpm+%5Cvec%7B1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x = \pm \vec{1}"/>). On the other hand as <img alt="T \to \infty, \beta \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=T+%5Cto+%5Cinfty%2C+%5Cbeta+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T \to \infty, \beta \to 0"/>, all <img alt="x \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \{\pm 1\}^n"/> are weighted equally according to <img alt="P_{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_{\beta}"/>.</p>
<p>Above we have defined the Boltzmann distribution to be proportional to <img alt="\exp(-\beta E(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+E%28x%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(-\beta E(x))"/>. To spell it out,</p>
<p><img alt="P_{\beta}(x) = \frac{1}{Z(\beta)} \exp(-\beta E(x)), \qquad \text{where } Z(\beta) = \sum_{x \in \{\pm 1\}^n} \exp(-\beta E(x))." class="latex" src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D%28x%29+%3D+%5Cfrac%7B1%7D%7BZ%28%5Cbeta%29%7D+%5Cexp%28-%5Cbeta+E%28x%29%29%2C+%5Cqquad+%5Ctext%7Bwhere+%7D+Z%28%5Cbeta%29+%3D+%5Csum_%7Bx+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En%7D+%5Cexp%28-%5Cbeta+E%28x%29%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_{\beta}(x) = \frac{1}{Z(\beta)} \exp(-\beta E(x)), \qquad \text{where } Z(\beta) = \sum_{x \in \{\pm 1\}^n} \exp(-\beta E(x))."/></p>
<p>The normalizing quantity <img alt="Z(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(\beta)"/> is referred to as the <em>partition function</em>, and is interesting in its own right. For example, from <img alt="Z(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(\beta)"/> we can compute the <em>free energy</em> <img alt="F(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=F%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F(\beta)"/> of the system, as well as the <em>internal energy</em> <img alt="U(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=U%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U(\beta)"/> and the <em>entropy</em> <img alt="S(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=S%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S(\beta)"/>:</p>
<p><img alt="F(\beta) = -\frac{1}{\beta} \ln Z(\beta), \qquad \qquad U(\beta) = \frac{\partial}{\partial \beta} (\beta F(\beta)), \qquad \qquad S(\beta) = \beta^2 \frac{\partial}{\partial \beta} F(\beta)." class="latex" src="https://s0.wp.com/latex.php?latex=F%28%5Cbeta%29+%3D+-%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln+Z%28%5Cbeta%29%2C+%5Cqquad+%5Cqquad+U%28%5Cbeta%29+%3D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cbeta%7D+%28%5Cbeta+F%28%5Cbeta%29%29%2C+%5Cqquad+%5Cqquad+S%28%5Cbeta%29+%3D+%5Cbeta%5E2+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cbeta%7D+F%28%5Cbeta%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F(\beta) = -\frac{1}{\beta} \ln Z(\beta), \qquad \qquad U(\beta) = \frac{\partial}{\partial \beta} (\beta F(\beta)), \qquad \qquad S(\beta) = \beta^2 \frac{\partial}{\partial \beta} F(\beta)."/></p>
<p>Using some straightforward calculus, we can then see that <img alt="S(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=S%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S(\beta)"/> is the Shannon entropy,</p>
<p><img alt="S(\beta) = - \sum_{x} P_{\beta}(x)\ln (P_{\beta}(x))," class="latex" src="https://s0.wp.com/latex.php?latex=S%28%5Cbeta%29+%3D+-+%5Csum_%7Bx%7D+P_%7B%5Cbeta%7D%28x%29%5Cln+%28P_%7B%5Cbeta%7D%28x%29%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S(\beta) = - \sum_{x} P_{\beta}(x)\ln (P_{\beta}(x)),"/></p>
<p>that <img alt="U(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=U%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U(\beta)"/> is the average energy in the system,</p>
<p><img alt="U(\beta) = \sum_{x} P_{\beta}(x) \cdot E(x)," class="latex" src="https://s0.wp.com/latex.php?latex=U%28%5Cbeta%29+%3D+%5Csum_%7Bx%7D+P_%7B%5Cbeta%7D%28x%29+%5Ccdot+E%28x%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U(\beta) = \sum_{x} P_{\beta}(x) \cdot E(x),"/></p>
<p>and that the free energy is the difference of the internal energy and the product of the temperature <img alt="T = \frac{1}{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=T+%3D+%5Cfrac%7B1%7D%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T = \frac{1}{\beta}"/> and the entropy,</p>
<p><img alt="F(\beta) = U(\beta) - T \cdot S(\beta)," class="latex" src="https://s0.wp.com/latex.php?latex=F%28%5Cbeta%29+%3D+U%28%5Cbeta%29+-+T+%5Ccdot+S%28%5Cbeta%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F(\beta) = U(\beta) - T \cdot S(\beta),"/></p>
<p>just like the <a href="https://en.wikipedia.org/wiki/Gibbs_free_energy">classical thermodynamic definitons</a>!</p>
<h3>Why these functions?</h3>
<p>The free energy, internal energy, and entropy encode information about the typical behavior of the system at temperature <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta"/>. We can get some intuition by considering the extremes, <img alt="\beta \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta \to \infty"/> and <img alt="\beta \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta \to 0"/>.</p>
<p>In cold systems with <img alt="\beta \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta \to \infty"/>, if we let <img alt="E_0" class="latex" src="https://s0.wp.com/latex.php?latex=E_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_0"/> be the energy of the ground state, <img alt="N_0 = |\{x : E(x) = E_0\}|" class="latex" src="https://s0.wp.com/latex.php?latex=N_0+%3D+%7C%5C%7Bx+%3A+E%28x%29+%3D+E_0%5C%7D%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N_0 = |\{x : E(x) = E_0\}|"/> be the number of ground state configurations, and <img alt="\Delta_E = \min_{x : E(x) \textgreater E_0} E(x) - E_0" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_E+%3D+%5Cmin_%7Bx+%3A+E%28x%29+%5Ctextgreater+E_0%7D+E%28x%29+-+E_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_E = \min_{x : E(x) \textgreater E_0} E(x) - E_0"/> be the energy gap, then</p>
<p><img alt="Z(\beta) = N_0 \exp(-\beta E_0)\cdot\left(1 + O\left(\exp(-\beta \Delta_E)\right)\right)," class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29+%3D+N_0+%5Cexp%28-%5Cbeta+E_0%29%5Ccdot%5Cleft%281+%2B+O%5Cleft%28%5Cexp%28-%5Cbeta+%5CDelta_E%29%5Cright%29%5Cright%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(\beta) = N_0 \exp(-\beta E_0)\cdot\left(1 + O\left(\exp(-\beta \Delta_E)\right)\right),"/></p>
<p>where the <img alt="O(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Ccdot%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(\cdot)"/> notation hides factors that do not depend on <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta"/>. From this it isn’t hard to work out that</p>
<p><img alt="\begin{aligned} F(\beta) &amp;= E_0 -\frac{1}{\beta} \ln(N_0) + O\left(\exp(-\beta \Delta_E)\right),\\ E(\beta) &amp;= E_0 + O\left(\exp(-\beta \Delta_E)\right)\\ S(\beta) &amp;= \ln N_0 + O\left(\exp(-\beta \Delta_E)\right). \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+F%28%5Cbeta%29+%26%3D+E_0+-%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln%28N_0%29+%2B+O%5Cleft%28%5Cexp%28-%5Cbeta+%5CDelta_E%29%5Cright%29%2C%5C%5C+E%28%5Cbeta%29+%26%3D+E_0+%2B+O%5Cleft%28%5Cexp%28-%5Cbeta+%5CDelta_E%29%5Cright%29%5C%5C+S%28%5Cbeta%29+%26%3D+%5Cln+N_0+%2B+O%5Cleft%28%5Cexp%28-%5Cbeta+%5CDelta_E%29%5Cright%29.+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\begin{aligned} F(\beta) &amp;= E_0 -\frac{1}{\beta} \ln(N_0) + O\left(\exp(-\beta \Delta_E)\right),\\ E(\beta) &amp;= E_0 + O\left(\exp(-\beta \Delta_E)\right)\\ S(\beta) &amp;= \ln N_0 + O\left(\exp(-\beta \Delta_E)\right). \end{aligned}"/></p>
<p>We can see that the behavior of the system is dominated by the few ground states. As <img alt="\beta \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta \to \infty"/>, all of the free energy can be attributed to the internal energy term.</p>
<p>On the other hand, as <img alt="\beta \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta \to 0"/>,</p>
<p><img alt="\begin{aligned} F(\beta) &amp;= \mathbb{E}_{x\sim\{\pm 1\}^n} E(x) - \frac{n}{\beta} + O(\beta),\\ U(\beta) &amp;= \mathbb{E}_{x\sim\{\pm 1\}^n} E(x) + O(\beta),\\ S(\beta) &amp;= n + O(\beta), \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+F%28%5Cbeta%29+%26%3D+%5Cmathbb%7BE%7D_%7Bx%5Csim%5C%7B%5Cpm+1%5C%7D%5En%7D+E%28x%29+-+%5Cfrac%7Bn%7D%7B%5Cbeta%7D+%2B+O%28%5Cbeta%29%2C%5C%5C+U%28%5Cbeta%29+%26%3D+%5Cmathbb%7BE%7D_%7Bx%5Csim%5C%7B%5Cpm+1%5C%7D%5En%7D+E%28x%29+%2B+O%28%5Cbeta%29%2C%5C%5C+S%28%5Cbeta%29+%26%3D+n+%2B+O%28%5Cbeta%29%2C+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\begin{aligned} F(\beta) &amp;= \mathbb{E}_{x\sim\{\pm 1\}^n} E(x) - \frac{n}{\beta} + O(\beta),\\ U(\beta) &amp;= \mathbb{E}_{x\sim\{\pm 1\}^n} E(x) + O(\beta),\\ S(\beta) &amp;= n + O(\beta), \end{aligned}"/></p>
<p>and the behavior of the system is chaotic, with the free energy dominated by the entropy term.</p>
<h3>Phase transition at the critical temperature.</h3>
<p>We say that the system undergoes a <em>phase transition</em> at <img alt="\beta_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta_c"/> if the <em>energy density</em> <img alt="f(\beta) = \lim_{n\to\infty}\frac{1}{n}F(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn%5Cto%5Cinfty%7D%5Cfrac%7B1%7D%7Bn%7DF%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta) = \lim_{n\to\infty}\frac{1}{n}F(\beta)"/> is not analytic at <img alt="\beta = \beta_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+%5Cbeta_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta = \beta_c"/>. Often, this comes from a shift in the relative contributions of the internal energy and entropy terms to <img alt="F(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=F%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F(\beta)"/>. Phase transitions are often associated as well with a <strong>qualitative</strong> change in system behavior.</p>
<p>For example, we’ll now show that for the Ising model with <img alt="G = K_n" class="latex" src="https://s0.wp.com/latex.php?latex=G+%3D+K_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G = K_n"/> the complete graph with self loops, the system has a phase transition at <img alt="\beta_c = \frac{1}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta_c+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta_c = \frac{1}{2}"/> (the self-loops don’t make much physical sense, but are convenient to work with). Furthermore, we’ll show that this phase transition corresponds to a qualitative change in the system, i.e. the loss of magnetism.</p>
<p>Define the <em>magnetization</em> of the system with spins <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> to be <img alt="m(x) = \frac{1}{n}\sum_i x_i" class="latex" src="https://s0.wp.com/latex.php?latex=m%28x%29+%3D+%5Cfrac%7B1%7D%7Bn%7D%5Csum_i+x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m(x) = \frac{1}{n}\sum_i x_i"/>. If <img alt="\lim_{n \to \infty} m(x) \not\to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Cto+%5Cinfty%7D+m%28x%29+%5Cnot%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lim_{n \to \infty} m(x) \not\to 0"/>, then we say the system is <em>magnetized</em>.</p>
<p>In the complete graph, normalized so that the total interaction of each particle is <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1"/>, there is a direct relationship between the energy and the magnetization:</p>
<p><img alt="E(x) = \frac{1}{n}\sum_{i, j} - x_i x_j = - n\cdot m(x)^2." class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29+%3D+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%2C+j%7D+-+x_i+x_j+%3D+-+n%5Ccdot+m%28x%29%5E2.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E(x) = \frac{1}{n}\sum_{i, j} - x_i x_j = - n\cdot m(x)^2."/></p>
<h3>Computing the energy density.</h3>
<p>The magnetization takes values <img alt="\frac{k}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bk%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{k}{n}"/> for <img alt="k\in\{-n,\ldots, n\}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Cin%5C%7B-n%2C%5Cldots%2C+n%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k\in\{-n,\ldots, n\}"/>. So, letting <img alt="N_k" class="latex" src="https://s0.wp.com/latex.php?latex=N_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N_k"/> be the number of states with magnetization <img alt="k/n" class="latex" src="https://s0.wp.com/latex.php?latex=k%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k/n"/>, we have that</p>
<p><img alt="Z(\beta) = \sum_{x} \exp(-\beta(- n\cdot m(x)^2)) = \sum_{k = -n}^{n} N_k \cdot \exp\left(\beta n \left(\frac{k}{n}\right)^2\right)." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29+%3D+%5Csum_%7Bx%7D+%5Cexp%28-%5Cbeta%28-+n%5Ccdot+m%28x%29%5E2%29%29+%3D+%5Csum_%7Bk+%3D+-n%7D%5E%7Bn%7D+N_k+%5Ccdot+%5Cexp%5Cleft%28%5Cbeta+n+%5Cleft%28%5Cfrac%7Bk%7D%7Bn%7D%5Cright%29%5E2%5Cright%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(\beta) = \sum_{x} \exp(-\beta(- n\cdot m(x)^2)) = \sum_{k = -n}^{n} N_k \cdot \exp\left(\beta n \left(\frac{k}{n}\right)^2\right)."/></p>
<p>Now, <img alt="N_k" class="latex" src="https://s0.wp.com/latex.php?latex=N_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N_k"/> is just the number of strings with Hamming weight <img alt="\frac{1}{2}(n+k)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D%28n%2Bk%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{2}(n+k)"/>, so <img alt="N_k = \binom{n}{(n+k)/2}" class="latex" src="https://s0.wp.com/latex.php?latex=N_k+%3D+%5Cbinom%7Bn%7D%7B%28n%2Bk%29%2F2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N_k = \binom{n}{(n+k)/2}"/>. By Stirling’s approximation <img alt="N_k \approx \exp(n \cdot H(\frac{1+k/n}{2}))" class="latex" src="https://s0.wp.com/latex.php?latex=N_k+%5Capprox+%5Cexp%28n+%5Ccdot+H%28%5Cfrac%7B1%2Bk%2Fn%7D%7B2%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N_k \approx \exp(n \cdot H(\frac{1+k/n}{2}))"/>, where <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> is the entropy function, so up to lower-order terms</p>
<p><img alt="Z(\beta) \approx \sum_{\substack{k\in [\pm n]\\ k+n \equiv_2 0}} \exp\left( \beta n \left(\frac{k}{n}\right)^2 + H\left(\frac{1 + \frac{k}{n}}{2}\right) n\right)." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29+%5Capprox+%5Csum_%7B%5Csubstack%7Bk%5Cin+%5B%5Cpm+n%5D%5C%5C+k%2Bn+%5Cequiv_2+0%7D%7D+%5Cexp%5Cleft%28+%5Cbeta+n+%5Cleft%28%5Cfrac%7Bk%7D%7Bn%7D%5Cright%29%5E2+%2B+H%5Cleft%28%5Cfrac%7B1+%2B+%5Cfrac%7Bk%7D%7Bn%7D%7D%7B2%7D%5Cright%29+n%5Cright%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(\beta) \approx \sum_{\substack{k\in [\pm n]\\ k+n \equiv_2 0}} \exp\left( \beta n \left(\frac{k}{n}\right)^2 + H\left(\frac{1 + \frac{k}{n}}{2}\right) n\right)."/></p>
<p>Now we apply the following simplification: for <img alt="x \in \mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \mathbb{R}^n"/>, <img alt="\|x\|_{\infty} \le \|x\|_1 \le n \|x\|_{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Cx%5C%7C_%7B%5Cinfty%7D+%5Cle+%5C%7Cx%5C%7C_1+%5Cle+n+%5C%7Cx%5C%7C_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\|x\|_{\infty} \le \|x\|_1 \le n \|x\|_{\infty}"/>, and then <img alt="\log(\|x\|_1) = \log \|x\|_{\infty} + O(\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog%28%5C%7Cx%5C%7C_1%29+%3D+%5Clog+%5C%7Cx%5C%7C_%7B%5Cinfty%7D+%2B+O%28%5Clog+n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\log(\|x\|_1) = \log \|x\|_{\infty} + O(\log n)"/>. Treating our summands as the entries of the vector <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>, from this we have,</p>
<p><img alt="\log(Z(\beta)) = \max_{k \in \{-n,\ldots, n\}} \beta n \left(\frac{k}{n}\right)^2 + H\left(\frac{1}{2}\left(1+\frac{k}{n}\right)\cdot n\right) + O\left(\log n\right)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog%28Z%28%5Cbeta%29%29+%3D+%5Cmax_%7Bk+%5Cin+%5C%7B-n%2C%5Cldots%2C+n%5C%7D%7D+%5Cbeta+n+%5Cleft%28%5Cfrac%7Bk%7D%7Bn%7D%5Cright%29%5E2+%2B+H%5Cleft%28%5Cfrac%7B1%7D%7B2%7D%5Cleft%281%2B%5Cfrac%7Bk%7D%7Bn%7D%5Cright%29%5Ccdot+n%5Cright%29+%2B+O%5Cleft%28%5Clog+n%5Cright%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\log(Z(\beta)) = \max_{k \in \{-n,\ldots, n\}} \beta n \left(\frac{k}{n}\right)^2 + H\left(\frac{1}{2}\left(1+\frac{k}{n}\right)\cdot n\right) + O\left(\log n\right)."/></p>
<p>By definition of the energy density,</p>
<p><img alt="f(\beta) = \lim_{n \to \infty} \left(- \frac{1}{\beta n} \log Z(\beta)\right) = -\lim_{n\to\infty} \max_{k \in \{-n,\ldots, n\}} \left(\frac{k}{n}\right)^2 + \frac{1}{\beta}H\left(\frac{1}{2}(1+\frac{k}{n})\right)," class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn+%5Cto+%5Cinfty%7D+%5Cleft%28-+%5Cfrac%7B1%7D%7B%5Cbeta+n%7D+%5Clog+Z%28%5Cbeta%29%5Cright%29+%3D+-%5Clim_%7Bn%5Cto%5Cinfty%7D+%5Cmax_%7Bk+%5Cin+%5C%7B-n%2C%5Cldots%2C+n%5C%7D%7D+%5Cleft%28%5Cfrac%7Bk%7D%7Bn%7D%5Cright%29%5E2+%2B+%5Cfrac%7B1%7D%7B%5Cbeta%7DH%5Cleft%28%5Cfrac%7B1%7D%7B2%7D%281%2B%5Cfrac%7Bk%7D%7Bn%7D%29%5Cright%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta) = \lim_{n \to \infty} \left(- \frac{1}{\beta n} \log Z(\beta)\right) = -\lim_{n\to\infty} \max_{k \in \{-n,\ldots, n\}} \left(\frac{k}{n}\right)^2 + \frac{1}{\beta}H\left(\frac{1}{2}(1+\frac{k}{n})\right),"/></p>
<p>and since <img alt="n \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n \to \infty"/> independently of <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta"/>, we also have</p>
<p><img alt="f(\beta) = -\left(\max_{\delta \in [-1,1]} \delta^2 + \frac{1}{\beta}H\left(\frac{1+\delta}{2}\right)\right)," class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+-%5Cleft%28%5Cmax_%7B%5Cdelta+%5Cin+%5B-1%2C1%5D%7D+%5Cdelta%5E2+%2B+%5Cfrac%7B1%7D%7B%5Cbeta%7DH%5Cleft%28%5Cfrac%7B1%2B%5Cdelta%7D%7B2%7D%5Cright%29%5Cright%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta) = -\left(\max_{\delta \in [-1,1]} \delta^2 + \frac{1}{\beta}H\left(\frac{1+\delta}{2}\right)\right),"/></p>
<p>because the error from rounding <img alt="\delta \in [-1,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cin+%5B-1%2C1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta \in [-1,1]"/> to the nearest factor of <img alt="2/n" class="latex" src="https://s0.wp.com/latex.php?latex=2%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2/n"/> is <img alt="O(1/n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%2Fn%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(1/n)"/>.</p>
<p>We can see that the first term in the expression for <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> corresponds to the square of the magnetization (and therefore the energy); the more magnetized the system is, the larger the contribution from the first term. The second term corresponds to the entropy, or the number of configurations in the support; the larger the support, the larger the contribution of the second term. As <img alt="T = \frac{1}{\beta}\to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=T+%3D+%5Cfrac%7B1%7D%7B%5Cbeta%7D%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T = \frac{1}{\beta}\to \infty"/>, the contribution of the entropy term overwhelms the contribution of the energy term; this is consistent with our physical intuition.</p>
<h3>A phase transition.</h3>
<p>We’ll now demonstrate that there is indeed a phase transition in <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta"/>. To do so, we solve for this maximum. Taking the derivative with respect to <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta"/>, we have that</p>
<p><img alt="\frac{\partial}{\partial \delta} \left( \delta^2 + \frac{1}{\beta}H\left(\frac{1+\delta}{2}\right)\right) = 2\delta + \frac{1}{2\beta} \ln \left(\frac{1-\delta}{1+\delta}\right)," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cdelta%7D+%5Cleft%28+%5Cdelta%5E2+%2B+%5Cfrac%7B1%7D%7B%5Cbeta%7DH%5Cleft%28%5Cfrac%7B1%2B%5Cdelta%7D%7B2%7D%5Cright%29%5Cright%29+%3D+2%5Cdelta+%2B+%5Cfrac%7B1%7D%7B2%5Cbeta%7D+%5Cln+%5Cleft%28%5Cfrac%7B1-%5Cdelta%7D%7B1%2B%5Cdelta%7D%5Cright%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{\partial}{\partial \delta} \left( \delta^2 + \frac{1}{\beta}H\left(\frac{1+\delta}{2}\right)\right) = 2\delta + \frac{1}{2\beta} \ln \left(\frac{1-\delta}{1+\delta}\right),"/></p>
<p>so the derivative is <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> whenever <img alt="\delta = \frac{1}{4\beta} \ln\left(\frac{1+\delta}{1-\delta}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cfrac%7B1%7D%7B4%5Cbeta%7D+%5Cln%5Cleft%28%5Cfrac%7B1%2B%5Cdelta%7D%7B1-%5Cdelta%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta = \frac{1}{4\beta} \ln\left(\frac{1+\delta}{1-\delta}\right)"/>. From this, we can check the maxima. When <img alt="\beta \textgreater \frac{1}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Ctextgreater+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta \textgreater \frac{1}{2}"/>, there are two maxima equidistant from the origin, corresponding to negatively or positively-magnetized states. When <img alt="\beta \textless \frac{1}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Ctextless+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta \textless \frac{1}{2}"/>, the maximizer is <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/>, corresponding to an unmagnetized state.</p>
<p><img alt="exponent" class="alignnone size-full wp-image-6210" src="https://windowsontheory.files.wordpress.com/2018/09/exponent.png?w=600"/></p>
<p>Given the maximizers, we now have the energy density. When we plot the energy density, the phase transition at <img alt="\beta = \frac{1}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta = \frac{1}{2}"/> is subtle (an earlier version of this post contained a mistaken plot):</p>

<a href="https://windowsontheory.org/energy-density-2/"><img alt="" class="attachment-thumbnail size-thumbnail" height="100" src="https://windowsontheory.files.wordpress.com/2018/09/energy-density1.png?w=150&amp;h=100" width="150"/></a>
<a href="https://windowsontheory.org/energy-density-zoom-3/"><img alt="" class="attachment-thumbnail size-thumbnail" height="100" src="https://windowsontheory.files.wordpress.com/2018/09/energy-density-zoom2.png?w=150&amp;h=100" width="150"/></a>

<p>But when we plot the derivative, we can see that it is not smooth at <img alt="\beta = \frac{1}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta = \frac{1}{2}"/>:</p>

<a href="https://windowsontheory.org/deriv/"><img alt="" class="attachment-thumbnail size-thumbnail" height="100" src="https://windowsontheory.files.wordpress.com/2018/09/deriv.png?w=150&amp;h=100" width="150"/></a>
<a href="https://windowsontheory.org/deriv-zoom/"><img alt="" class="attachment-thumbnail size-thumbnail" height="100" src="https://windowsontheory.files.wordpress.com/2018/09/deriv-zoom.png?w=150&amp;h=100" width="150"/></a>

<p>And with some calculus it is possible to show that the second derivative is indeed not continuous at <img alt="\beta = \frac{1}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta = \frac{1}{2}"/>.</p>
<p>Qualitatively, it is convincing that this phase transition in the energy density is related to a transition in the magnetization (because the maximizing <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta"/> corresponds to the typical magnetization). One can make this formal by performing a similar calculation to show that the internal energy undergoes a phase transition, which in this case is proportional to the expected squared magnetization, <img alt="\mathbb{E}_{P_\beta}[-n \cdot m(x)^2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7BP_%5Cbeta%7D%5B-n+%5Ccdot+m%28x%29%5E2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}_{P_\beta}[-n \cdot m(x)^2]"/>.</p>
<h3>Beyond the complete graph</h3>
<p>The Ising model on the complete graph <img alt="K_n" class="latex" src="https://s0.wp.com/latex.php?latex=K_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K_n"/> (also called the Curie-Weiss model) is perhaps not a very convincing model for a physical block of iron; we expect that locality should govern the strength of the interactions. But because the energy and the magnetization are related so simply, it is easy to solve.</p>
<p>Solutions are also known for the 1D and 2D grids; solving it on higher-dimensional lattices, as well as in many other interesting settings, remains open. Interestingly, the <a href="https://en.wikipedia.org/wiki/Conformal_bootstrap">conformal bootstrap method</a> that <a href="https://windowsontheory.org/2018/08/06/physics-envy/">Boaz mentioned</a> has been <a href="https://arxiv.org/abs/1203.6064">used</a> towards solving the Ising model on higher-dimensional grids.</p>
<h2>II. Constraint Satisfaction Problems</h2>
<p>For those familiar with constraint satisfaction problems (CSPs), it may have already been clear that the Ising model is a CSP. The spins <img alt="x \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \{\pm 1\}^n"/> are Boolean variables, and the energy function <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E(x)"/> is an objective function corresponding to the EQUALITY CSP on <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> (a pretty boring CSP, when taken without negations). The Boltzmann distribution gives a probability distribution over assignments to the variables <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>, and the temperature <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> determines the objective value of a typical <img alt="x \sim P_{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Csim+P_%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \sim P_{\beta}"/>.</p>
<p>We can similarly define the energy, Boltzmann distribution, and free energy/entropy for any CSP (and even to continuous domains such as <img alt="x \in \mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \mathbb{R}^n"/>). Especially popular with statistical physicists are:</p>
<ul>
<li>CSPs (such as the Ising model) over grids and other lattices.</li>
<li>Gaussian spin glasses: CSPs over <img alt="x \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \{\pm 1\}^n"/> or <img alt="x \in \mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \mathbb{R}^n"/> where the energy function is proportional to <img alt="\langle x^{\otimes k}, J\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x%5E%7B%5Cotimes+k%7D%2C+J%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\langle x^{\otimes k}, J\rangle"/>, where <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J"/> is an order-<img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> symmetric tensor with entries sampled i.i.d. from <img alt="\mathcal{N}(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%280%2C1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(0,1)"/>. The Ising model on a graph with random Gaussian weights is an example for <img alt="k = 2" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3D+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k = 2"/>.</li>
<li>Random instances of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT: Of the <img alt="\binom{n}{k} \cdot 2^k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bn%7D%7Bk%7D+%5Ccdot+2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\binom{n}{k} \cdot 2^k"/> possible clauses (with negations) on <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> variables, <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> clauses <img alt="C_1,\ldots,C_m" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%2C%5Cldots%2CC_m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1,\ldots,C_m"/> are sampled uniformly at random, and the energy function is the number of satisfied clauses, <img alt="E(x) = |\{ i \in [m] : C_i(x) = 1\}|" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29+%3D+%7C%5C%7B+i+%5Cin+%5Bm%5D+%3A+C_i%28x%29+%3D+1%5C%7D%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E(x) = |\{ i \in [m] : C_i(x) = 1\}|"/>.</li>
<li>Random instances of other Boolean and larger-alphabet CSPs.</li>
</ul>
<p>In some cases, these CSPs are reasonable models for physical systems; in<br/>
other cases, they are primarily of theoretical interest.</p>
<h3>Algorithmic questions</h3>
<p>As theoretical computer scientists, we are used to seeing CSPs in the contex of optimization. In statistical physics, the goal is to understand the qualitative behavior of the system as described by the Boltzmann distribution. They ask algorithmic questions such as:</p>
<ul>
<li>Can we estimate the free energy <img alt="F(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=F%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F(\beta)"/>? The partition function <img alt="Z(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(\beta)"/>? And, relatedly,</li>
<li>Can we sample from <img alt="P_{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_{\beta}"/>?</li>
</ul>
<p>But these tasks are not so different from optimization. For example, if our system is an instance of 3SAT, when <img alt="T=0" class="latex" src="https://s0.wp.com/latex.php?latex=T%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T=0"/>, the Boltzmann distribution is the uniform distribution over maximally satisfying assignments, and so estimating <img alt="Z(\infty)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cinfty%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(\infty)"/> is equivalent to deciding the SAT formula, and sampling from <img alt="P_{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=P_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_{\infty}"/> is equivalent to solving the SAT formula. As <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> increases, sampling from <img alt="P_{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_{\beta}"/> corresponds to sampling an approximate solution.</p>
<p>Clearly in the worst case, these tasks are NP-Hard (and even #P-hard). But even for random instances these algorithmic questions are interesting.</p>
<h3>Phase transitions in satisfiability</h3>
<p>In random <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT, the system is controlled not only by the temperature <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> but also by the <em>clause density</em> <img alt="\alpha = \frac{m}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%5Cfrac%7Bm%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha = \frac{m}{n}"/>. For the remainder of the post, we will focus on the zero-temperature regime <img alt="T = 0" class="latex" src="https://s0.wp.com/latex.php?latex=T+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T = 0"/>, and we will see that <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT exhibits phase transitions in <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/> as well.</p>
<p>The most natural “physical” trait to track in a <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT formula is whether or not it is <em>satisfiable</em>. When <img alt="\alpha = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha = 0"/>, <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT instances are clearly satisfiable, because they have no constraints. Similarly when <img alt="\alpha = n^k \cdot 2^k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+n%5Ek+%5Ccdot+2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha = n^k \cdot 2^k"/>, random <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT instances cannot be satisfiable, because for any set of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> variables they will contain all <img alt="2^k" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^k"/> possible <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT constraints (clearly unsatisfiable). It is natural to ask: is there a satisfiability phase transition in <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/>?</p>
<p>For <img alt="k = 2" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3D+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k = 2"/>, one can show that the answer is yes. For <img alt="k \ge 3" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Cge+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k \ge 3"/>, numerical evidence strongly points to this being the case; further, the following theorem of Friedgut gives a partial answer:</p>
<p><strong>Theorem:</strong><br/>
For there exists a function <img alt="\alpha_c(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_c%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_c(n)"/> such that for any <img alt="\epsilon \textgreater 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%5Ctextgreater+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon \textgreater 0"/>, if <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/> is a random <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT formula on <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> variables with <img alt="\alpha n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha n"/> clauses, then</p>
<p><img alt="\lim_{n \to \infty} \Pr[\phi \text{ is satisfiable}] = \begin{cases} 1 &amp; \text{ if } \alpha \textless (1-\epsilon) \alpha_c(n)\\ 0 &amp; \text{ if } \alpha \textgreater (1+\epsilon) \alpha_c(n) \end{cases}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Cto+%5Cinfty%7D+%5CPr%5B%5Cphi+%5Ctext%7B+is+satisfiable%7D%5D+%3D+%5Cbegin%7Bcases%7D+1+%26+%5Ctext%7B+if+%7D+%5Calpha+%5Ctextless+%281-%5Cepsilon%29+%5Calpha_c%28n%29%5C%5C+0+%26+%5Ctext%7B+if+%7D+%5Calpha+%5Ctextgreater+%281%2B%5Cepsilon%29+%5Calpha_c%28n%29+%5Cend%7Bcases%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lim_{n \to \infty} \Pr[\phi \text{ is satisfiable}] = \begin{cases} 1 &amp; \text{ if } \alpha \textless (1-\epsilon) \alpha_c(n)\\ 0 &amp; \text{ if } \alpha \textgreater (1+\epsilon) \alpha_c(n) \end{cases}."/></p>
<p> </p>
<p>However, this theorem allows for the possibility that the threshold depends on <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>. From a statistical physics standpoint, this would be ridiculous, as it suggests that the behavior of the system depends on the number of particles that participate in it. We state the commonly held stronger conjecture:</p>
<p><strong>Conjecture:</strong><br/>
for all <img alt="k \ge 3" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Cge+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k \ge 3"/>, there exists a constant <img alt="\alpha_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_c"/> depending only on <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> such that if <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/> is a random <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT instance in <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> variables and <img alt="\alpha n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha n"/> clauses, then</p>
<p><img alt="\lim_{n \to \infty} \Pr[\phi \text{ is satisfiable}] = \begin{cases} 1 &amp; \text{ if } \alpha \textless \alpha_c \\ 1 &amp; \text{ if } \alpha \textgreater \alpha_c \end{cases}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Cto+%5Cinfty%7D+%5CPr%5B%5Cphi+%5Ctext%7B+is+satisfiable%7D%5D+%3D+%5Cbegin%7Bcases%7D+1+%26+%5Ctext%7B+if+%7D+%5Calpha+%5Ctextless+%5Calpha_c+%5C%5C+1+%26+%5Ctext%7B+if+%7D+%5Calpha+%5Ctextgreater+%5Calpha_c+%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lim_{n \to \infty} \Pr[\phi \text{ is satisfiable}] = \begin{cases} 1 &amp; \text{ if } \alpha \textless \alpha_c \\ 1 &amp; \text{ if } \alpha \textgreater \alpha_c \end{cases}"/></p>
<p> </p>
<p>In 2015, Jian Ding, Allan Sly, and Nike Sun established the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT conjecture all <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> larger than some fixed constant <img alt="k_0" class="latex" src="https://s0.wp.com/latex.php?latex=k_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k_0"/>, and we will be hearing about the proof from Nike later on in the course.</p>
<h3>Bounds on <img alt="\alpha_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_c"/> via the method of moments</h3>
<p>Let us move to a simpler model of random <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT formulas, which is a bit easier to work with and is a close approximation to our original model. Instead of sampling <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT clauses without replacement, we will sample them with replacement and also allow variables to appear multiple times in the same clause (so each literal is chosen uniformly at random). The independence of the clauses makes computations in this model simpler.</p>
<p>We’ll prove the following bounds. The upper bound is a fairly straightforward computation, and the lower bound is given by an elegant argument due to Achlioptas and Peres.</p>
<p><strong>Theorem:</strong><br/>
For every <img alt="k \ge 3" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Cge+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k \ge 3"/>, <img alt="2^k \ln 2 - O(k) \le \alpha_c \le 2^k \ln 2 - O(1)" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ek+%5Cln+2+-+O%28k%29+%5Cle+%5Calpha_c+%5Cle+2%5Ek+%5Cln+2+-+O%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^k \ln 2 - O(k) \le \alpha_c \le 2^k \ln 2 - O(1)"/>.</p>
<p>Let <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/> be a random <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT formula with <img alt="m = \alpha n" class="latex" src="https://s0.wp.com/latex.php?latex=m+%3D+%5Calpha+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m = \alpha n"/> clauses, <img alt="C_1,\ldots,C_m" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%2C%5Cldots%2CC_m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1,\ldots,C_m"/>. For an assignment <img alt="x \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \{\pm 1\}^n"/>, let <img alt="\phi(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(x)"/> be the <img alt="0/1" class="latex" src="https://s0.wp.com/latex.php?latex=0%2F1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0/1"/> indicator that <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> satisfies <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/>. Finally, let <img alt="Z_{\alpha} = \sum_x \phi(x)" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7B%5Calpha%7D+%3D+%5Csum_x+%5Cphi%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_{\alpha} = \sum_x \phi(x)"/> be the number of satisfying assignments of <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/>.</p>
<h4>Upper bound via the first moment method.</h4>
<p>We have by Markov’s inequality that</p>
<p><img alt="\Pr[Z_{\alpha} \ge 1] \le \mathbb{E}[Z_{\alpha}] = \sum_{x} \Pr[\phi(x)]." class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5BZ_%7B%5Calpha%7D+%5Cge+1%5D+%5Cle+%5Cmathbb%7BE%7D%5BZ_%7B%5Calpha%7D%5D+%3D+%5Csum_%7Bx%7D+%5CPr%5B%5Cphi%28x%29%5D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[Z_{\alpha} \ge 1] \le \mathbb{E}[Z_{\alpha}] = \sum_{x} \Pr[\phi(x)]."/></p>
<p>Fix an assignment <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>. Then by the independence of the clauses,</p>
<p><img alt="\Pr[\phi(x)] = \prod_{j \in [m]} \Pr[C_j(x) = 1] = \left(1 - \frac{1}{2^k}\right)^m," class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Cphi%28x%29%5D+%3D+%5Cprod_%7Bj+%5Cin+%5Bm%5D%7D+%5CPr%5BC_j%28x%29+%3D+1%5D+%3D+%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5Ek%7D%5Cright%29%5Em%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[\phi(x)] = \prod_{j \in [m]} \Pr[C_j(x) = 1] = \left(1 - \frac{1}{2^k}\right)^m,"/></p>
<p>since each clause has <img alt="2^k -1" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ek+-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^k -1"/> satisfying assignments. Summing over all<br/>
<img alt="x \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \{\pm 1\}^n"/>,</p>
<p><img alt="\mathbb{E}[Z_{\alpha}] = 2^n \left(1 - \frac{1}{2^k}\right)^{\alpha n}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ_%7B%5Calpha%7D%5D+%3D+2%5En+%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5Ek%7D%5Cright%29%5E%7B%5Calpha+n%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[Z_{\alpha}] = 2^n \left(1 - \frac{1}{2^k}\right)^{\alpha n}."/></p>
<p>We can see that if <img alt="\alpha \textgreater 2^k \ln 2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextgreater+2%5Ek+%5Cln+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \textgreater 2^k \ln 2"/>, this quantity will go to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> with <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>. So we have:</p>
<p><img alt="\alpha_c \le 2^k \ln 2." class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_c+%5Cle+2%5Ek+%5Cln+2.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_c \le 2^k \ln 2."/></p>
<h4>Lower bound via the second moment method.</h4>
<p>To lower bound <img alt="\alpha_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_c"/>, we can use the <em>second moment method</em>. We’ll<br/>
calculate the second moment of <img alt="Z_{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_{\alpha}"/>. An easy use of Cauchy-Schwarz (for non-negative <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/>, <img alt="\mathbb{E}[X] \le \sqrt{\mathbb{E}[X^2]\mathbb{E}[I[X \textgreater 0]]}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BX%5D+%5Cle+%5Csqrt%7B%5Cmathbb%7BE%7D%5BX%5E2%5D%5Cmathbb%7BE%7D%5BI%5BX+%5Ctextgreater+0%5D%5D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[X] \le \sqrt{\mathbb{E}[X^2]\mathbb{E}[I[X \textgreater 0]]}"/>) implies that if there is a constant <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/> such that</p>
<p><img alt="\lim_{n \to \infty}\frac{\left(\mathbb{E} Z_{\alpha}\right)^2}{\mathbb{E}Z_{\alpha}^2} \ge c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Cto+%5Cinfty%7D%5Cfrac%7B%5Cleft%28%5Cmathbb%7BE%7D+Z_%7B%5Calpha%7D%5Cright%29%5E2%7D%7B%5Cmathbb%7BE%7DZ_%7B%5Calpha%7D%5E2%7D+%5Cge+c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lim_{n \to \infty}\frac{\left(\mathbb{E} Z_{\alpha}\right)^2}{\mathbb{E}Z_{\alpha}^2} \ge c"/>,</p>
<p>then with at least constant probability <img alt="Z_{\alpha} \ge 1" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7B%5Calpha%7D+%5Cge+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_{\alpha} \ge 1"/>, and then Friedgut’s theorem implies that <img alt="\alpha \textless \alpha_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextless+%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \textless \alpha_c"/>. From above we have an expression for the numerator, so we now set out to bound the second moment of <img alt="Z_{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_{\alpha}"/>. We have that</p>
<p><img alt="\mathbb{E} Z_{\alpha}^2 = \mathbb{E}\left[\left(\sum_{x}I[\phi(x)]\right)^2\right] = \sum_{x,y} \Pr\left[\phi(x)\wedge \phi(y)\right]," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+Z_%7B%5Calpha%7D%5E2+%3D+%5Cmathbb%7BE%7D%5Cleft%5B%5Cleft%28%5Csum_%7Bx%7DI%5B%5Cphi%28x%29%5D%5Cright%29%5E2%5Cright%5D+%3D+%5Csum_%7Bx%2Cy%7D+%5CPr%5Cleft%5B%5Cphi%28x%29%5Cwedge+%5Cphi%28y%29%5Cright%5D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E} Z_{\alpha}^2 = \mathbb{E}\left[\left(\sum_{x}I[\phi(x)]\right)^2\right] = \sum_{x,y} \Pr\left[\phi(x)\wedge \phi(y)\right],"/></p>
<p>and by the independence of the clauses,</p>
<p><img alt="\Pr\left[\phi(x)\wedge \phi(y)\right] = \prod_{j \in [m]} \Pr[C_j(x) = 1 \wedge C_j(y) = 1] = \left(\Pr[C(x) = 1 \wedge C(y) = 1]\right)^m," class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5Cleft%5B%5Cphi%28x%29%5Cwedge+%5Cphi%28y%29%5Cright%5D+%3D+%5Cprod_%7Bj+%5Cin+%5Bm%5D%7D+%5CPr%5BC_j%28x%29+%3D+1+%5Cwedge+C_j%28y%29+%3D+1%5D+%3D+%5Cleft%28%5CPr%5BC%28x%29+%3D+1+%5Cwedge+C%28y%29+%3D+1%5D%5Cright%29%5Em%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr\left[\phi(x)\wedge \phi(y)\right] = \prod_{j \in [m]} \Pr[C_j(x) = 1 \wedge C_j(y) = 1] = \left(\Pr[C(x) = 1 \wedge C(y) = 1]\right)^m,"/></p>
<p>for a single random <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT clause <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/>. But, for <img alt="x,y \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y \in \{\pm 1\}^n"/>, the events <img alt="C(x) = 1" class="latex" src="https://s0.wp.com/latex.php?latex=C%28x%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C(x) = 1"/> and <img alt="C(y)=1" class="latex" src="https://s0.wp.com/latex.php?latex=C%28y%29%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C(y)=1"/> are not independent. This is easier to see if we apply inclusion-exclusion,</p>
<p><img alt="\Pr[C(x) = 1 \wedge C(y) = 1] = 1 - \Pr[C(x) = 0] - \Pr[C(y) = 0] + \Pr[C(x) = C(y) = 0]," class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5BC%28x%29+%3D+1+%5Cwedge+C%28y%29+%3D+1%5D+%3D+1+-+%5CPr%5BC%28x%29+%3D+0%5D+-+%5CPr%5BC%28y%29+%3D+0%5D+%2B+%5CPr%5BC%28x%29+%3D+C%28y%29+%3D+0%5D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[C(x) = 1 \wedge C(y) = 1] = 1 - \Pr[C(x) = 0] - \Pr[C(y) = 0] + \Pr[C(x) = C(y) = 0],"/></p>
<p>The event that both <img alt="C(x) = C(y) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=C%28x%29+%3D+C%28y%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C(x) = C(y) = 0"/> when <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> is a <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT clause occurs only when <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> agree exactly on the assignments to the variables of <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/>, since otherwise at least one of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> or <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> must be satisfied (because each literal in <img alt="C(x)" class="latex" src="https://s0.wp.com/latex.php?latex=C%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C(x)"/> is negated for <img alt="C(y)" class="latex" src="https://s0.wp.com/latex.php?latex=C%28y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C(y)"/>). Thus, this probability depends on the Hamming distance between <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/>.</p>
<p><img alt="agreement" class="alignnone wp-image-6205" src="https://windowsontheory.files.wordpress.com/2018/09/agreement.png?w=300" width="300"/></p>
<p>For <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> with <img alt="\frac{1}{2}\|x-y\|_1 = (1-\delta)n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D%5C%7Cx-y%5C%7C_1+%3D+%281-%5Cdelta%29n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{2}\|x-y\|_1 = (1-\delta)n"/>, the probability that <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/>‘s variables are all in the subset on which <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> agree is <img alt="\delta^k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta^k"/> (up to lower order terms). So, we have</p>
<p><img alt="\Pr[C(x) = C(y) = 0] = \delta^k \cdot \frac{1}{2^k}," class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5BC%28x%29+%3D+C%28y%29+%3D+0%5D+%3D+%5Cdelta%5Ek+%5Ccdot+%5Cfrac%7B1%7D%7B2%5Ek%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[C(x) = C(y) = 0] = \delta^k \cdot \frac{1}{2^k},"/></p>
<p>and then for <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> with <img alt="\frac{1}{2}\|x-y\|_1 = n - \ell" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D%5C%7Cx-y%5C%7C_1+%3D+n+-+%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{2}\|x-y\|_1 = n - \ell"/>,</p>
<p><img alt="\Pr[C(x) = 1 \wedge C(y) = 1] = 1 - 2\cdot \frac{1}{2^k} + \left(\frac{\ell}{2n}\right)^k." class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5BC%28x%29+%3D+1+%5Cwedge+C%28y%29+%3D+1%5D+%3D+1+-+2%5Ccdot+%5Cfrac%7B1%7D%7B2%5Ek%7D+%2B+%5Cleft%28%5Cfrac%7B%5Cell%7D%7B2n%7D%5Cright%29%5Ek.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[C(x) = 1 \wedge C(y) = 1] = 1 - 2\cdot \frac{1}{2^k} + \left(\frac{\ell}{2n}\right)^k."/></p>
<p>Because there are <img alt="2^n \cdot \binom{n}{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5En+%5Ccdot+%5Cbinom%7Bn%7D%7B%5Cell%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^n \cdot \binom{n}{\ell}"/> pairs <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> at distance <img alt="\frac{1}{2}\|x-y\|_1 = n - \ell" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D%5C%7Cx-y%5C%7C_1+%3D+n+-+%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{2}\|x-y\|_1 = n - \ell"/>,</p>
<p><img alt="\mathbb{E}\left[Z_{\alpha}^2\right] = \sum_{\ell = 0}^{n} 2^n \cdot \binom{n}{\ell} \cdot \left(1 - \frac{1}{2^{k-1}} + \left(\frac{\ell}{2n}\right)^k\right)^m," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%5BZ_%7B%5Calpha%7D%5E2%5Cright%5D+%3D+%5Csum_%7B%5Cell+%3D+0%7D%5E%7Bn%7D+2%5En+%5Ccdot+%5Cbinom%7Bn%7D%7B%5Cell%7D+%5Ccdot+%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5E%7Bk-1%7D%7D+%2B+%5Cleft%28%5Cfrac%7B%5Cell%7D%7B2n%7D%5Cright%29%5Ek%5Cright%29%5Em%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}\left[Z_{\alpha}^2\right] = \sum_{\ell = 0}^{n} 2^n \cdot \binom{n}{\ell} \cdot \left(1 - \frac{1}{2^{k-1}} + \left(\frac{\ell}{2n}\right)^k\right)^m,"/></p>
<p>and using the Stirling bound <img alt="\binom{n}{\ell} \le O(\frac{1}{\sqrt{n}})\exp(n \cdot H(\ell/n))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bn%7D%7B%5Cell%7D+%5Cle+O%28%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7D%29%5Cexp%28n+%5Ccdot+H%28%5Cell%2Fn%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\binom{n}{\ell} \le O(\frac{1}{\sqrt{n}})\exp(n \cdot H(\ell/n))"/>,</p>
<p><img alt="= \sum_{\ell = 0}^{n}O(\frac{1}{\sqrt{n}}) \exp\left( \left(H\left(\frac{\ell}{n}\right) + \ln 2\right)\cdot n + \ln\left(1 - \frac{1}{2^{k-1}} + \left(\frac{\ell}{2n}\right)^k\right) \cdot \alpha n\right)." class="latex" src="https://s0.wp.com/latex.php?latex=%3D+%5Csum_%7B%5Cell+%3D+0%7D%5E%7Bn%7DO%28%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7D%29+%5Cexp%5Cleft%28+%5Cleft%28H%5Cleft%28%5Cfrac%7B%5Cell%7D%7Bn%7D%5Cright%29+%2B+%5Cln+2%5Cright%29%5Ccdot+n+%2B+%5Cln%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5E%7Bk-1%7D%7D+%2B+%5Cleft%28%5Cfrac%7B%5Cell%7D%7B2n%7D%5Cright%29%5Ek%5Cright%29+%5Ccdot+%5Calpha+n%5Cright%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="= \sum_{\ell = 0}^{n}O(\frac{1}{\sqrt{n}}) \exp\left( \left(H\left(\frac{\ell}{n}\right) + \ln 2\right)\cdot n + \ln\left(1 - \frac{1}{2^{k-1}} + \left(\frac{\ell}{2n}\right)^k\right) \cdot \alpha n\right)."/></p>
<p>Using Laplace’s method, we can show that this sum will be dominated by the <img alt="O(\sqrt{n})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(\sqrt{n})"/> terms around the maximizing summand; so defining <img alt="\psi(\delta) = H(\delta) + \ln 2 + \alpha\ln (1 - 2^{-k+1} + \delta^k 2^{-k}))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%28%5Cdelta%29+%3D+H%28%5Cdelta%29+%2B+%5Cln+2+%2B+%5Calpha%5Cln+%281+-+2%5E%7B-k%2B1%7D+%2B+%5Cdelta%5Ek+2%5E%7B-k%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi(\delta) = H(\delta) + \ln 2 + \alpha\ln (1 - 2^{-k+1} + \delta^k 2^{-k}))"/>,</p>
<p><img alt="\mathbb{E}[Z_{\alpha}^2] = O(1) \cdot \exp( n \cdot \max_{\delta \in [0,1]} \psi(\delta))." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ_%7B%5Calpha%7D%5E2%5D+%3D+O%281%29+%5Ccdot+%5Cexp%28+n+%5Ccdot+%5Cmax_%7B%5Cdelta+%5Cin+%5B0%2C1%5D%7D+%5Cpsi%28%5Cdelta%29%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[Z_{\alpha}^2] = O(1) \cdot \exp( n \cdot \max_{\delta \in [0,1]} \psi(\delta))."/></p>
<p>If we want that <img alt="\mathbb{E}[Z^2] \le c \cdot \mathbb{E}[Z]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5E2%5D+%5Cle+c+%5Ccdot+%5Cmathbb%7BE%7D%5BZ%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[Z^2] \le c \cdot \mathbb{E}[Z]^2"/>, then we require that</p>
<p><img alt="\max_{\delta \in [0,1]} \psi(\delta) \le \frac{1}{n}\ln(\mathbb{E}[Z]^2) = 2\ln 2 + 2\alpha \ln (1-2^{-k})." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%7B%5Cdelta+%5Cin+%5B0%2C1%5D%7D+%5Cpsi%28%5Cdelta%29+%5Cle+%5Cfrac%7B1%7D%7Bn%7D%5Cln%28%5Cmathbb%7BE%7D%5BZ%5D%5E2%29+%3D+2%5Cln+2+%2B+2%5Calpha+%5Cln+%281-2%5E%7B-k%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\max_{\delta \in [0,1]} \psi(\delta) \le \frac{1}{n}\ln(\mathbb{E}[Z]^2) = 2\ln 2 + 2\alpha \ln (1-2^{-k})."/></p>
<p>However, we can see (using calculus) that this inequality does not hold whenever <img alt="\alpha \textgreater 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextgreater+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \textgreater 0"/>. In the plot below one can also see this, though the values are very close when <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/> is close to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/>:</p>
<p> </p>

<a href="https://windowsontheory.org/2018/09/15/statistical-physics-an-introduction-in-two-parts/sat-second-moment-3/"><img alt="" class="attachment-thumbnail size-thumbnail" height="100" src="https://windowsontheory.files.wordpress.com/2018/09/sat-second-moment2.png?w=150&amp;h=100" width="150"/></a>
<a href="https://windowsontheory.org/2018/09/15/statistical-physics-an-introduction-in-two-parts/sat-second-moment-zoom-2/"><img alt="" class="attachment-thumbnail size-thumbnail" height="100" src="https://windowsontheory.files.wordpress.com/2018/09/sat-second-moment-zoom1.png?w=150&amp;h=100" width="150"/></a>

<p>So the naive second moment method fails to establish any lower bound on <img alt="\alpha_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_c"/>. The problem here is that the second moment is dominated by the large correlations of close strings; whenever <img alt="\alpha \textgreater 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextgreater+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \textgreater 0"/>, the sum is dominated by pairs of strings <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> which are closer than <img alt="n/2" class="latex" src="https://s0.wp.com/latex.php?latex=n%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n/2"/> in Hamming distance, which is atypical. For strings at Hamming distance <img alt="n/2" class="latex" src="https://s0.wp.com/latex.php?latex=n%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n/2"/>, what is relevant is <img alt="\psi(\frac{1}{2})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%28%5Cfrac%7B1%7D%7B2%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi(\frac{1}{2})"/>, which is always</p>
<p><img alt="\psi(\frac{1}{2}) = H(\frac{1}{2}) + \ln 2 + \alpha \ln \left(1 - \frac{1}{2^{k-1}} + \frac{1}{2^{2k}}\right) = 2\ln 2 + \alpha \ln \left(1 - \frac{1}{2^k}\right)^{2}," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%28%5Cfrac%7B1%7D%7B2%7D%29+%3D+H%28%5Cfrac%7B1%7D%7B2%7D%29+%2B+%5Cln+2+%2B+%5Calpha+%5Cln+%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5E%7Bk-1%7D%7D+%2B+%5Cfrac%7B1%7D%7B2%5E%7B2k%7D%7D%5Cright%29+%3D+2%5Cln+2+%2B+%5Calpha+%5Cln+%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5Ek%7D%5Cright%29%5E%7B2%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi(\frac{1}{2}) = H(\frac{1}{2}) + \ln 2 + \alpha \ln \left(1 - \frac{1}{2^{k-1}} + \frac{1}{2^{2k}}\right) = 2\ln 2 + \alpha \ln \left(1 - \frac{1}{2^k}\right)^{2},"/></p>
<p>which is equal to <img alt="\frac{1}{n}\ln\mathbb{E}[Z_{\alpha}]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bn%7D%5Cln%5Cmathbb%7BE%7D%5BZ_%7B%5Calpha%7D%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{n}\ln\mathbb{E}[Z_{\alpha}]^2"/>. At distance <img alt="n/2" class="latex" src="https://s0.wp.com/latex.php?latex=n%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n/2"/>, the value of <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/> on pairs <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> is essentially uncorrelated (one can think of drawing a uniformly random <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> given <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>), so such pairs are representative of <img alt="\mathbb{E}[Z]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[Z]^2"/>.</p>
<h4>A more delicate approach.</h4>
<p>To get a good bound, we have to perform a fancier second moment method calculation, due to Achlioptas and Peres. We will re-weight the terms so that more typical pairs, at distance <img alt="n/2" class="latex" src="https://s0.wp.com/latex.php?latex=n%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n/2"/>, are dominant. Rather than computing <img alt="Z_{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_{\alpha}"/>, we compute <img alt="X_{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=X_%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_{\alpha}"/>, where</p>
<p><img alt="X_{\alpha} = \sum_{x} \prod_{j \in [m]} w_j(x)," class="latex" src="https://s0.wp.com/latex.php?latex=X_%7B%5Calpha%7D+%3D+%5Csum_%7Bx%7D+%5Cprod_%7Bj+%5Cin+%5Bm%5D%7D+w_j%28x%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_{\alpha} = \sum_{x} \prod_{j \in [m]} w_j(x),"/></p>
<p>where <img alt="w_j(x) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=w_j%28x%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_j(x) = 0"/> if <img alt="C_j(x) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=C_j%28x%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_j(x) = 0"/>, and <img alt="w_j(x) = \eta^t" class="latex" src="https://s0.wp.com/latex.php?latex=w_j%28x%29+%3D+%5Ceta%5Et&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_j(x) = \eta^t"/> if <img alt="C_j(x)" class="latex" src="https://s0.wp.com/latex.php?latex=C_j%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_j(x)"/> is satisfied by <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> variables. Since <img alt="X_{\alpha} \textgreater 0" class="latex" src="https://s0.wp.com/latex.php?latex=X_%7B%5Calpha%7D+%5Ctextgreater+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_{\alpha} \textgreater 0"/> only if <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/> has satisfying assignments, the goal is to still bound <img alt="\mathbb{E}[X_{\alpha}^2] \le c\cdot \mathbb{E}[X_{\alpha}]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BX_%7B%5Calpha%7D%5E2%5D+%5Cle+c%5Ccdot+%5Cmathbb%7BE%7D%5BX_%7B%5Calpha%7D%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[X_{\alpha}^2] \le c\cdot \mathbb{E}[X_{\alpha}]^2"/>. Again using the independence of the clauses, we have that</p>
<p><img alt="\begin{aligned} \mathbb{E}[X_{\alpha}] = 2^n \cdot \mathbb{E}[w_j(x)]^m = 2^n \left(2^{-k}\sum_{t = 1}^k \binom{k}{t} \eta^t \right)^m = 2^n \left(\left(\frac{1+\eta}{2}\right)^k-2^{-k}\right)^m.\label{eq:expect}\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb%7BE%7D%5BX_%7B%5Calpha%7D%5D+%3D+2%5En+%5Ccdot+%5Cmathbb%7BE%7D%5Bw_j%28x%29%5D%5Em+%3D+2%5En+%5Cleft%282%5E%7B-k%7D%5Csum_%7Bt+%3D+1%7D%5Ek+%5Cbinom%7Bk%7D%7Bt%7D+%5Ceta%5Et+%5Cright%29%5Em+%3D+2%5En+%5Cleft%28%5Cleft%28%5Cfrac%7B1%2B%5Ceta%7D%7B2%7D%5Cright%29%5Ek-2%5E%7B-k%7D%5Cright%29%5Em.%5Clabel%7Beq%3Aexpect%7D%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\begin{aligned} \mathbb{E}[X_{\alpha}] = 2^n \cdot \mathbb{E}[w_j(x)]^m = 2^n \left(2^{-k}\sum_{t = 1}^k \binom{k}{t} \eta^t \right)^m = 2^n \left(\left(\frac{1+\eta}{2}\right)^k-2^{-k}\right)^m.\label{eq:expect}\end{aligned}"/></p>
<p>And calculating again the second moment,</p>
<p><img alt="\mathbb{E}[X_{\alpha}^2] = \sum_{x,y} \prod_{j \in [m]} \mathbb{E}[w_j(x)\cdot w_j(y)]." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BX_%7B%5Calpha%7D%5E2%5D+%3D+%5Csum_%7Bx%2Cy%7D+%5Cprod_%7Bj+%5Cin+%5Bm%5D%7D+%5Cmathbb%7BE%7D%5Bw_j%28x%29%5Ccdot+w_j%28y%29%5D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[X_{\alpha}^2] = \sum_{x,y} \prod_{j \in [m]} \mathbb{E}[w_j(x)\cdot w_j(y)]."/></p>
<p>For a fixed clause <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j"/>, we can partition the variables in its scope into 4 sets, according to whether <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> agree on the variable, and whether the variable does or does not satisfy the literals of the clause. Suppose that <img alt="w_j" class="latex" src="https://s0.wp.com/latex.php?latex=w_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_j"/> has <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a"/> variables on which <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> agree, and that of these <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> are satisfying for <img alt="w_j" class="latex" src="https://s0.wp.com/latex.php?latex=w_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_j"/> and <img alt="a - t" class="latex" src="https://s0.wp.com/latex.php?latex=a+-+t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a - t"/> are not.</p>
<p><img alt="agreement-numbers" class="alignnone wp-image-6206" src="https://windowsontheory.files.wordpress.com/2018/09/agreement-numbers.png?w=300" width="300"/></p>
<p>Then, <img alt="w_j" class="latex" src="https://s0.wp.com/latex.php?latex=w_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_j"/> has <img alt="k-a" class="latex" src="https://s0.wp.com/latex.php?latex=k-a&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k-a"/> variables on which <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> disagree, and any variable which does not satisfy <img alt="w_j" class="latex" src="https://s0.wp.com/latex.php?latex=w_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_j"/> for <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> must satisfy it for <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/>. For a <img alt="w_j(x) w_j(y)" class="latex" src="https://s0.wp.com/latex.php?latex=w_j%28x%29+w_j%28y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_j(x) w_j(y)"/> to be nonzero, either there must be at least one literal in the variables on which <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> agree which agrees with <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/>, or otherwise there must be at least one literal in the variables on which <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> disagree which agrees with each string. Therefore, if <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> agree on a <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta"/>-fraction of variables,</p>
<p><img alt="\begin{aligned} \mathbb{E}[w_j(x) \cdot w_j(y)] &amp;= \sum_{a = 1}^k \sum_{t=1}^a \eta^{2t + k - a} \cdot \Pr[w_j \text{ has } a \text{ variables in } x \cap y,\ t \in x \cap y \text{ satisfy }w, \ w_j(x),w_j(y) \textgreater 0]\\ &amp;= \left(\sum_{a = 0}^k\sum_{t=0}^a \eta^{k - a + 2t} \cdot \binom{k}{a}\delta^{a}\left(1 - \delta\right)^{k-a} 2^{-a} \binom{a}{t}\right) - \left(\sum_{a=0}^k \eta^{k-a} \binom{k}{a} \delta^a(1-\delta)^{k-a} 2^{-k+1}\right) + \delta^k2^{-k},\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb%7BE%7D%5Bw_j%28x%29+%5Ccdot+w_j%28y%29%5D+%26%3D+%5Csum_%7Ba+%3D+1%7D%5Ek+%5Csum_%7Bt%3D1%7D%5Ea+%5Ceta%5E%7B2t+%2B+k+-+a%7D+%5Ccdot+%5CPr%5Bw_j+%5Ctext%7B+has+%7D+a+%5Ctext%7B+variables+in+%7D+x+%5Ccap+y%2C%5C+t+%5Cin+x+%5Ccap+y+%5Ctext%7B+satisfy+%7Dw%2C+%5C+w_j%28x%29%2Cw_j%28y%29+%5Ctextgreater+0%5D%5C%5C+%26%3D+%5Cleft%28%5Csum_%7Ba+%3D+0%7D%5Ek%5Csum_%7Bt%3D0%7D%5Ea+%5Ceta%5E%7Bk+-+a+%2B+2t%7D+%5Ccdot+%5Cbinom%7Bk%7D%7Ba%7D%5Cdelta%5E%7Ba%7D%5Cleft%281+-+%5Cdelta%5Cright%29%5E%7Bk-a%7D+2%5E%7B-a%7D+%5Cbinom%7Ba%7D%7Bt%7D%5Cright%29+-+%5Cleft%28%5Csum_%7Ba%3D0%7D%5Ek+%5Ceta%5E%7Bk-a%7D+%5Cbinom%7Bk%7D%7Ba%7D+%5Cdelta%5Ea%281-%5Cdelta%29%5E%7Bk-a%7D+2%5E%7B-k%2B1%7D%5Cright%29+%2B+%5Cdelta%5Ek2%5E%7B-k%7D%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\begin{aligned} \mathbb{E}[w_j(x) \cdot w_j(y)] &amp;= \sum_{a = 1}^k \sum_{t=1}^a \eta^{2t + k - a} \cdot \Pr[w_j \text{ has } a \text{ variables in } x \cap y,\ t \in x \cap y \text{ satisfy }w, \ w_j(x),w_j(y) \textgreater 0]\\ &amp;= \left(\sum_{a = 0}^k\sum_{t=0}^a \eta^{k - a + 2t} \cdot \binom{k}{a}\delta^{a}\left(1 - \delta\right)^{k-a} 2^{-a} \binom{a}{t}\right) - \left(\sum_{a=0}^k \eta^{k-a} \binom{k}{a} \delta^a(1-\delta)^{k-a} 2^{-k+1}\right) + \delta^k2^{-k},\end{aligned}"/></p>
<p>where the first sum ignores the cases when <img alt="w_j(x) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=w_j%28x%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_j(x) = 0"/> or <img alt="w_j(y) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=w_j%28y%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_j(y) = 0"/>, the second sum subtracts for the cases when <img alt="t=0" class="latex" src="https://s0.wp.com/latex.php?latex=t%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t=0"/> the contribution of the terms where all the literals agree with either <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> or <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/>, and the final term accounts for the fact that the term <img alt="a = k" class="latex" src="https://s0.wp.com/latex.php?latex=a+%3D+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a = k"/> is subtracted<br/>
twice. Simplifying,</p>
<p><img alt="\mathbb{E}[w_j(x) \cdot w_j(y)] = \left(\frac{(1+\eta^2)}{2}\delta + \eta(1-\delta)\right)^k - 2^{1-k} \left(\eta(1-\delta) + \delta\right)^k + \left(\frac{\delta}{2}\right)^k." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Bw_j%28x%29+%5Ccdot+w_j%28y%29%5D+%3D+%5Cleft%28%5Cfrac%7B%281%2B%5Ceta%5E2%29%7D%7B2%7D%5Cdelta+%2B+%5Ceta%281-%5Cdelta%29%5Cright%29%5Ek+-+2%5E%7B1-k%7D+%5Cleft%28%5Ceta%281-%5Cdelta%29+%2B+%5Cdelta%5Cright%29%5Ek+%2B+%5Cleft%28%5Cfrac%7B%5Cdelta%7D%7B2%7D%5Cright%29%5Ek.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[w_j(x) \cdot w_j(y)] = \left(\frac{(1+\eta^2)}{2}\delta + \eta(1-\delta)\right)^k - 2^{1-k} \left(\eta(1-\delta) + \delta\right)^k + \left(\frac{\delta}{2}\right)^k."/></p>
<p>Define<br/>
<img alt="q_{\eta}(\delta) =\left(\frac{(1+\eta^2)}{2}\delta + \eta(1-\delta)\right)^k - 2^{1-k} \left(\eta(1-\delta) + \delta\right)^k + \left(\frac{\delta}{2}\right)^k" class="latex" src="https://s0.wp.com/latex.php?latex=q_%7B%5Ceta%7D%28%5Cdelta%29+%3D%5Cleft%28%5Cfrac%7B%281%2B%5Ceta%5E2%29%7D%7B2%7D%5Cdelta+%2B+%5Ceta%281-%5Cdelta%29%5Cright%29%5Ek+-+2%5E%7B1-k%7D+%5Cleft%28%5Ceta%281-%5Cdelta%29+%2B+%5Cdelta%5Cright%29%5Ek+%2B+%5Cleft%28%5Cfrac%7B%5Cdelta%7D%7B2%7D%5Cright%29%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_{\eta}(\delta) =\left(\frac{(1+\eta^2)}{2}\delta + \eta(1-\delta)\right)^k - 2^{1-k} \left(\eta(1-\delta) + \delta\right)^k + \left(\frac{\delta}{2}\right)^k"/>.<br/>
So we have (using Laplace’s method again)</p>
<p><img alt="\mathbb{E}[X^2] = 2^n \sum_{\ell = 0}^n \binom{n}{\ell} q_\eta\left(\frac{\ell}{n}\right)^{\alpha n} \approx O(1) \cdot \exp(\max_{\delta \in [0,1]} \psi_\eta (\delta) \cdot n)," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BX%5E2%5D+%3D+2%5En+%5Csum_%7B%5Cell+%3D+0%7D%5En+%5Cbinom%7Bn%7D%7B%5Cell%7D+q_%5Ceta%5Cleft%28%5Cfrac%7B%5Cell%7D%7Bn%7D%5Cright%29%5E%7B%5Calpha+n%7D+%5Capprox+O%281%29+%5Ccdot+%5Cexp%28%5Cmax_%7B%5Cdelta+%5Cin+%5B0%2C1%5D%7D+%5Cpsi_%5Ceta+%28%5Cdelta%29+%5Ccdot+n%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[X^2] = 2^n \sum_{\ell = 0}^n \binom{n}{\ell} q_\eta\left(\frac{\ell}{n}\right)^{\alpha n} \approx O(1) \cdot \exp(\max_{\delta \in [0,1]} \psi_\eta (\delta) \cdot n),"/></p>
<p>For<br/>
<img alt="\psi_\eta(\delta) = H(\delta) + \ln 2 + \alpha \ln q_{\eta}(\delta)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%5Ceta%28%5Cdelta%29+%3D+H%28%5Cdelta%29+%2B+%5Cln+2+%2B+%5Calpha+%5Cln+q_%7B%5Ceta%7D%28%5Cdelta%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_\eta(\delta) = H(\delta) + \ln 2 + \alpha \ln q_{\eta}(\delta)."/></p>
<p>When <img alt="\delta = \frac{1}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta = \frac{1}{2}"/>,</p>
<p><img alt="q_{\eta}\left(\frac{1}{2}\right) = \left(\left(\frac{1+\eta}{2}\right)^k - \frac{1}{2^k}\right)^2," class="latex" src="https://s0.wp.com/latex.php?latex=q_%7B%5Ceta%7D%5Cleft%28%5Cfrac%7B1%7D%7B2%7D%5Cright%29+%3D+%5Cleft%28%5Cleft%28%5Cfrac%7B1%2B%5Ceta%7D%7B2%7D%5Cright%29%5Ek+-+%5Cfrac%7B1%7D%7B2%5Ek%7D%5Cright%29%5E2%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_{\eta}\left(\frac{1}{2}\right) = \left(\left(\frac{1+\eta}{2}\right)^k - \frac{1}{2^k}\right)^2,"/></p>
<p>which is equal to the log of the square of the expectation, so again for the second moment method to succeed, <img alt="\delta = \frac{1}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta = \frac{1}{2}"/> must be the global maximum.</p>
<p>Guided by this consideration, we can set <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta"/> so that the derivative <img alt="q_{\eta}'(\frac{1}{2}) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=q_%7B%5Ceta%7D%27%28%5Cfrac%7B1%7D%7B2%7D%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_{\eta}'(\frac{1}{2}) = 0"/>, so that <img alt="\psi_\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_\eta"/> achieves a local maximum at <img alt="\delta = \frac{1}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta = \frac{1}{2}"/>. The choice for this (after doing some calculus) turns out to be the positive, real solution to the equation <img alt="(1+\eta)^{k-1}(1-\eta) = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%281%2B%5Ceta%29%5E%7Bk-1%7D%281-%5Ceta%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(1+\eta)^{k-1}(1-\eta) = 1"/>. With this choice, one can show that the global maximum is indeed achieved at <img alt="\delta = \frac{1}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta = \frac{1}{2}"/> as long as <img alt="\alpha \textless 2^k \ln 2 - O(k)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextless+2%5Ek+%5Cln+2+-+O%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \textless 2^k \ln 2 - O(k)"/>. Below, we plot <img alt="\psi_{\eta}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7B%5Ceta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{\eta}"/> with this optimal choice of <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta"/> for several values of <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/> at <img alt="k = 5" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3D+5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k = 5"/>:</p>
<p><img alt="fancy-second-moment" class="alignnone size-full wp-image-6211" src="https://windowsontheory.files.wordpress.com/2018/09/fancy-second-moment.png?w=600"/></p>
<p>So we have bounded <img alt="\alpha_c \ge 2^k \ln 2 - O(k)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_c+%5Cge+2%5Ek+%5Cln+2+-+O%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_c \ge 2^k \ln 2 - O(k)"/>.</p>
<h3>Another phase transition</h3>
<p>What is the correct answer for <img alt="\alpha_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_c"/>? We now have it in a window of size <img alt="O(k)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(k)"/>. Experimental data and heuristic predictions indicate that it is closer to <img alt="2^k \ln 2 - O(1)" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ek+%5Cln+2+-+O%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^k \ln 2 - O(1)"/> (and in fact for large <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>, Ding, Sly and Sun showed that <img alt="\alpha_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_c"/> is a specific constant in the interval <img alt="[2^k \ln 2 -2, 2^k \ln 2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5B2%5Ek+%5Cln+2+-2%2C+2%5Ek+%5Cln+2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[2^k \ln 2 -2, 2^k \ln 2]"/>). So why can’t we push the second moment method further?</p>
<p>It turns out that there is a good reason for this, having to do with another phase transition. In fact, we know that <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT has not only satisfiable and unsatisfiable phases, but also <em>clustering</em> and <em>condensation</em> phases:</p>
<p><img alt="ksat-phases" class="alignnone size-full wp-image-6212" src="https://windowsontheory.files.wordpress.com/2018/09/ksat-phases.png?w=600"/></p>
<p>In the clustering phase, there are exponentially many clusters of solutions, each containing exponentially many solutions, and each at hamming distance <img alt="\Omega(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Omega(n)"/> from each other. In the condensation phase, there are even fewer clusters, but solutions still exist. We can see evidence of this already in the way that the second moment method failed us. When <img alt="\alpha \textgreater 2^k \ln 2 - O(k)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextgreater+2%5Ek+%5Cln+2+-+O%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \textgreater 2^k \ln 2 - O(k)"/>, we see that the global maximum of the exponent is actually attained close to <img alt="\delta = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta = 1"/>. This is because solutions with large overlap come to dominate the set of satisfying assignments.</p>
<p>One can also establish the existence of clusters using the method of moments. The trick is to compute the probability that two solutions, <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> with overlap <img alt="\delta n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta n"/>, are both satisfying for <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/>. In fact, we have already done this. From above,</p>
<p><img alt="\Pr\left[\phi(x) \wedge \phi(y) ~\mid~ \frac{1}{2}\|x - y\|_1 = (1-\delta)n\right] = \left(1 - 2\cdot \frac{1}{2^k} + 2^{-k}\delta^k\right)^{\alpha n}." class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5Cleft%5B%5Cphi%28x%29+%5Cwedge+%5Cphi%28y%29+%7E%5Cmid%7E+%5Cfrac%7B1%7D%7B2%7D%5C%7Cx+-+y%5C%7C_1+%3D+%281-%5Cdelta%29n%5Cright%5D+%3D+%5Cleft%281+-+2%5Ccdot+%5Cfrac%7B1%7D%7B2%5Ek%7D+%2B+2%5E%7B-k%7D%5Cdelta%5Ek%5Cright%29%5E%7B%5Calpha+n%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr\left[\phi(x) \wedge \phi(y) ~\mid~ \frac{1}{2}\|x - y\|_1 = (1-\delta)n\right] = \left(1 - 2\cdot \frac{1}{2^k} + 2^{-k}\delta^k\right)^{\alpha n}."/></p>
<p>Now, by a union bound, an upper bound on the probability that there exists a pair of solutions <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> with overlap <img alt="\delta n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta n"/> for any <img alt="\delta \in [\delta_1,\delta_2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cin+%5B%5Cdelta_1%2C%5Cdelta_2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta \in [\delta_1,\delta_2]"/> is at most</p>
<p><img alt="\Pr[\exists x,y \ s.t. \ \phi(x) \wedge \phi(y), \ \frac{1}{2}\|x-y\|_1 = (1-\delta)n \text{ for } \delta \in [\delta_1,\delta_2]] \le \sum_{\ell = \delta_1 n}^{\delta_2 n} \binom{n}{\ell} \left(1 - 2^{1-k} + \left(\frac{\ell}{2n}\right)^k\right)^{\alpha n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Cexists+x%2Cy+%5C+s.t.+%5C+%5Cphi%28x%29+%5Cwedge+%5Cphi%28y%29%2C+%5C+%5Cfrac%7B1%7D%7B2%7D%5C%7Cx-y%5C%7C_1+%3D+%281-%5Cdelta%29n+%5Ctext%7B+for+%7D+%5Cdelta+%5Cin+%5B%5Cdelta_1%2C%5Cdelta_2%5D%5D+%5Cle+%5Csum_%7B%5Cell+%3D+%5Cdelta_1+n%7D%5E%7B%5Cdelta_2+n%7D+%5Cbinom%7Bn%7D%7B%5Cell%7D+%5Cleft%281+-+2%5E%7B1-k%7D+%2B+%5Cleft%28%5Cfrac%7B%5Cell%7D%7B2n%7D%5Cright%29%5Ek%5Cright%29%5E%7B%5Calpha+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[\exists x,y \ s.t. \ \phi(x) \wedge \phi(y), \ \frac{1}{2}\|x-y\|_1 = (1-\delta)n \text{ for } \delta \in [\delta_1,\delta_2]] \le \sum_{\ell = \delta_1 n}^{\delta_2 n} \binom{n}{\ell} \left(1 - 2^{1-k} + \left(\frac{\ell}{2n}\right)^k\right)^{\alpha n}"/><br/>
<img alt="\le \int_{\delta_1}^{\delta_2} \exp\left( \left(H(\delta) + \ln 2 + \alpha \ln \left(1-2^{1-k} + 2^{-k}\delta^k\right)\right)n\right) d\delta," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cle+%5Cint_%7B%5Cdelta_1%7D%5E%7B%5Cdelta_2%7D+%5Cexp%5Cleft%28+%5Cleft%28H%28%5Cdelta%29+%2B+%5Cln+2+%2B+%5Calpha+%5Cln+%5Cleft%281-2%5E%7B1-k%7D+%2B+2%5E%7B-k%7D%5Cdelta%5Ek%5Cright%29%5Cright%29n%5Cright%29+d%5Cdelta%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\le \int_{\delta_1}^{\delta_2} \exp\left( \left(H(\delta) + \ln 2 + \alpha \ln \left(1-2^{1-k} + 2^{-k}\delta^k\right)\right)n\right) d\delta,"/></p>
<p>and if the function <img alt="\beta(\delta) := H(\delta) + \ln 2 + \alpha\ln (1 + 2^{-k}\delta^k - 2^{1-k})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%28%5Cdelta%29+%3A%3D+H%28%5Cdelta%29+%2B+%5Cln+2+%2B+%5Calpha%5Cln+%281+%2B+2%5E%7B-k%7D%5Cdelta%5Ek+-+2%5E%7B1-k%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta(\delta) := H(\delta) + \ln 2 + \alpha\ln (1 + 2^{-k}\delta^k - 2^{1-k})"/> is such that <img alt="\beta(\delta) \textless 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%28%5Cdelta%29+%5Ctextless+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta(\delta) \textless 0"/> for all <img alt="\delta \in [\delta_1,\delta_2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cin+%5B%5Cdelta_1%2C%5Cdelta_2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\delta \in [\delta_1,\delta_2]"/>, then we conclude that the probability that there is a pair of satisfying assignments at distance between <img alt="(1-\delta_2)n" class="latex" src="https://s0.wp.com/latex.php?latex=%281-%5Cdelta_2%29n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(1-\delta_2)n"/> and <img alt="(1-\delta_1)n" class="latex" src="https://s0.wp.com/latex.php?latex=%281-%5Cdelta_1%29n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(1-\delta_1)n"/> is <img alt="o(1)" class="latex" src="https://s0.wp.com/latex.php?latex=o%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="o(1)"/>.</p>
<p>Achlioptas and Ricci-Tersenghi showed that for <img alt="k \ge 4" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Cge+4&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k \ge 4"/>, <img alt="\alpha = (1-\eta)2^k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%281-%5Ceta%292%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha = (1-\eta)2^k"/>, and <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/> a fixed constant, the above function is less than <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/>. Rather than doing the tedious calculus, we can verify by plotting for <img alt="k = 8, \alpha = 169" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3D+8%2C+%5Calpha+%3D+169&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k = 8, \alpha = 169"/> (with <img alt="169 \textless \alpha_c" class="latex" src="https://s0.wp.com/latex.php?latex=169+%5Ctextless+%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="169 \textless \alpha_c"/>):</p>
<p> </p>
<p><img alt="clusters" class="alignnone size-full wp-image-6207" src="https://windowsontheory.files.wordpress.com/2018/09/clusters.png?w=600"/></p>
<p>They also use the second moment method to show the clusters are non-empty, that there are exponentially many of them, each containing exponentially many solutions. This gives a proof of the existence of a clustering regime.</p>
<h3>Solution geometry and algorithms</h3>
<p>This study of the space of solutions is referred to as <em>solution geometry</em>, and understanding solution geometry turns out to be essential in proving better bounds on the critical density.</p>
<p>Solution geometry is also intimately related to the success of local search algorithms such as belief propagation, and to heuristics for predicting phase transitions such as replica symmetry breaking.</p>
<p>These topics and more to follow, in the coming weeks.</p>
<h2>Resources</h2>
<p>In preparing this blog post/lecture I leaned heavily on Chapters 2 and 10 of Marc Mézard and Andrea Montanari’s <a href="https://web.stanford.edu/~montanar/RESEARCH/book.html">“Information, Physics, and Computation”</a>, on Chapters 12,13,&amp;14 of Cris Moore and Stephan Mertens’ <a href="http://nature-of-computation.org/">“The nature of Computation,”</a> and on Dimitris Achlioptas and Federico Ricci-Tersenghi’s manuscript <a href="https://arxiv.org/abs/cs/0611052">“On the Solution-Space Geometry of Random CSPs”</a>. I also consulted Wikipedia for some physics basics.</p></div>
    </content>
    <updated>2018-09-15T21:10:17Z</updated>
    <published>2018-09-15T21:10:17Z</published>
    <category term="physics"/>
    <author>
      <name>tselilschramm</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-16T16:43:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:typepad.com,2003:post-6a00d83452383469e2017c33bf71a0970b</id>
    <link href="https://3dpancakes.typepad.com/ernie/2012/11/computational-advertising-at-uiuc.html" rel="alternate" type="text/html"/>
    <link href="https://3dpancakes.typepad.com/ernie/2012/11/computational-advertising-at-uiuc.html" rel="replies" type="text/html"/>
    <title>Computational Advertising at UIUC</title>
    <summary>Joint computer science/advertising senior faculty position in computational advertising at UIUC. Application deadline January 15, 2013.  See https://jobs.illinois.edu/search-jobs/job-details?jobID=26890 for details.
      <div class="commentbar">
        <p/>
        <a href="https://3dpancakes.typepad.com/ernie/2012/11/computational-advertising-at-uiuc.html">
          <img class="commenticon" src="/images/post-icon.png"/> Post a comment
        </a>
      </div>
    </summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><div><p>The Departments of Advertising and Computer Science at Illinois are running a search for a joint senior faculty position in computational advertising.  The position is tenured on arrival and will be shared between the two departments.  The application deadline is <b>January 15, 2013</b>.  Please feel free to contact me for more information; I'm co-chairing the search subcommittee.  Here's the official advertisement:</p>

<blockquote><small>
<p>The Departments of <a href="http://cs.illinois.edu">Computer Science</a> (CS) and <a href="http://media.illinois.edu/advertising/">Advertising</a> (ADV) at the University of Illinois at Urbana-Champaign invite applications for a joint faculty position in <i>Computational Advertising.</i> This novel position is part of a new multi-year Strategic Excellence Hiring Program at Illinois that focuses on: (a) Information, Technology and Society, (b) Human Health and Wellness, (c) Energy and Sustainability, and (d) Culture, Communication, and Global Issues.  We seek candidates with CS skills in areas such as “big data”, data-mining, or algorithmic game theory, and with advertising interests or experience in areas such as online /contextual advertising, digital privacy, behavioral targeting, and social media analytics. Applications are encouraged from candidates whose research programs are in traditional as well as in nontraditional areas that would support novel research and teaching across the emerging discipline of Computational Advertising.  Each department is engaged in exciting new and expanding programs for research, education, and professional development, and each has strong ties to industry, across a wide landscape of technology and media partners.</p>

<p>Faculty in the CS department carry out research in a broad spectrum of areas and are supported by world-class facilities, starting with our department’s home in the Siebel Center for Computer Science, and including collaborations with the <a href="http://www.ncsa.illinois.edu/">National Center for Supercomputing Applications</a>, the <a href="http://www.csl.illinois.edu/">Coordinated Science Laboratory</a>, the <a href="http://www.iti.illinois.edu/">Information Trust Institute</a>, and the <a href="http://www.informatics.uiuc.edu/">Illinois Informatics Institute</a>, as well as several industrial centers and programs that foster international collaborations. The CS department has one of the leading programs in the United States, granting approximately 200 B.S. degrees, 70 M.S. degrees, and 60 Ph.D. degrees annually.  The <a href="http://media.illinois.edu/advertising/">Department of Advertising</a>, the first such academic department in the country, was established in 1959 by Charles H. Sandage, considered to be the father of advertising education. Today, the top-ranked department celebrates Sandage's legacy with scholarship and teaching focused on understanding, evaluating and communicating the many facets of advertising. Approximately 150 B.S. degrees and 20 M.S. degrees are granted annually. Faculty can also teach in the <a href="http://media.illinois.edu/icr/">Institute for Communications Research</a> (ICR) doctoral program in the <a href="http://media.illinois.edu/">College of Media</a>.</p>

<p>In order to ensure full consideration by the Search Committee, applications must be received by <b>January 15, 2012</b>. Salary will be commensurate with qualifications. Preferred starting date is August 16, 2013, but is negotiable. Applications can be submitted by going to <a href="https://jobs.illinois.edu/search-jobs/job-details?jobID=26890">http://jobs.illinois.edu</a> (jobid: 26890) and uploading a cover letter, CV, research statement, and teaching statement, along with names of three references. For inquiry, please call 217-244-7949 or email <a href="mailto:HR@cs.illinois.edu">HR@cs.illinois.edu</a>.</p>

<p><i>Illinois is an Affirmative Action /Equal Opportunity Employer and welcomes individuals with diverse backgrounds, experiences, and ideas who embrace and value diversity and inclusivity (<a href="http://www.inclusiveillinois.illinois.edu">www.inclusiveillinois.illinois.edu</a>).</i></p>
</small></blockquote>

The CS department also has <a href="http://cs.illinois.edu/csillinois/employment/CS-tenure-track-faculty-positions#csopen">three open-rank faculty positions</a>, in the broad areas of software engineering, scientific computing, and "big data".  Please (ask your students to) apply!</div></div>
    </content>
    <updated>2012-11-19T16:19:30Z</updated>
    <published>2012-11-19T16:19:30Z</published>
    <category term="Academia"/>
    <category term="Computer science"/>
    <author>
      <name>Jeff Erickson</name>
    </author>
    <source>
      <id>tag:typepad.com,2003:weblog-6686</id>
      <link href="https://3dpancakes.typepad.com/ernie/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="https://3dpancakes.typepad.com/ernie/" rel="alternate" type="text/html"/>
      <subtitle>Let Σ be a combinatorial surface with n vertices, genus g, and b boundaries.  Amen.</subtitle>
      <title>Ernie's 3D Pancakes</title>
      <updated>2018-11-06T17:41:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1423</id>
    <link href="https://theorydish.blog/2018/06/26/will-quantum-communication-make-us-more-cooperative/" rel="alternate" type="text/html"/>
    <title>Will quantum communication make us more cooperative?</title>
    <summary>In a new paper with Mika Goos, we prove an lower bound on the randomized communication complexity of computing an approximate Nash equilibrium in a two-player game. Then Shalev Ben-David observed that our new proof also gives the first non-trivial lower bound on the quantum communication complexity of approximate Nash equilibrium. So quantum computers might not help us cooperate better with each other after all. Or maybe they will? You’ll have to read until the end of this short post. But first let me point out a cool technical challenge: 1. For quantum communication, we lose a quadratic factor (corresponding to Grover’s search), i.e. our lower bound is only . But I don’t know how to use Grover’s search to improve over the naive upper bound. So, is the quantum communication of approximate Nash equilibrium closer to linear or quadratic? Why is this interesting? Before we discuss why quantum communication of approximate Nash is interesting, it is helpful to first recall some game theory, and in particular remind ourselves why the classical (randomized) communication complexity of approximate Nash is important. Briefly, a two-player game is described by two matrices ; if Alice and Bob play actions , their payoffs are and [...]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In a <a href="https://arxiv.org/abs/1805.06387">new paper</a> with <a href="https://www.cs.utoronto.ca/~mgoos/">Mika Goos</a>, we prove an <img alt="N^{2-o(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=N%5E%7B2-o%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="N^{2-o(1)}"/> lower bound on the randomized communication complexity of computing an approximate Nash equilibrium in a two-player <img alt="N\times N" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Ctimes+N&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="N\times N"/> game. Then <a href="http://quics.umd.edu/people/shalev-ben-david">Shalev Ben-David</a> observed that our new proof also gives the first non-trivial lower bound on the <strong>quantum communication complexity of approximate Nash equilibrium</strong>. So quantum computers might not help us cooperate better with each other after all. Or maybe they will? You’ll have to read until the end of this short post. But first let me point out a cool technical challenge:</p>
<p style="padding-left: 30px;">1. For quantum communication, we lose a quadratic factor (corresponding to Grover’s search), i.e. our lower bound is only <img alt="N^{1-o(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=N%5E%7B1-o%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="N^{1-o(1)}"/>. But I don’t know how to use Grover’s search to improve over the naive <img alt="N^{2}" class="latex" src="https://s0.wp.com/latex.php?latex=N%5E%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="N^{2}"/> upper bound. So, is the quantum communication of approximate Nash equilibrium closer to linear or quadratic?</p>
<h3>Why is this interesting?</h3>
<p>Before we discuss why quantum communication of approximate Nash is interesting, it is helpful to first recall some game theory, and in particular remind ourselves why the classical (randomized) communication complexity of approximate Nash is important. Briefly, a two-player game is described by two <img alt="N\times N" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Ctimes+N&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="N\times N"/> matrices <img alt="A,B" class="latex" src="https://s0.wp.com/latex.php?latex=A%2CB&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A,B"/>; if Alice and Bob play actions <img alt="i,j" class="latex" src="https://s0.wp.com/latex.php?latex=i%2Cj&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i,j"/>, their payoffs are <img alt="A_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=A_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A_{i,j}"/> and <img alt="B_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=B_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="B_{i,j}"/>, respectively. Typically, they want to use randomized strategies (called <em>mixed strategies</em>). We say that Alice and Bob are at an (approximate) Nash equilibrium, each player’s strategy is (approximately) best-response to other player’s strategy. I.e. once players are at an equilibrium, they may never want to leave it. The big question is how do they get there in the first place?</p>
<p>A common approach to this question is to look for plausible <em>dynamics</em>, or procedures by which players update their strategies, and which guarantee convergence to Nash equilibrium. Defining “plausible” is a fascinating philosophical discussion far beyond the scope of this post, but two useful desiderata are: (i) <em>uncoupled dynamics</em>, namely each player knows only her own payoff matrix and the history of the game — this rules out the trivial dynamics where players start at a Nash equilibrium; and (ii) <em>efficient dynamics</em>, namely the dynamics must converge faster than it would take Alice to communicate her entire payoff matrix to Bob. Those are certainly not sufficient conditions for plausibility of dynamics, but our communication lower bound rules out <em>any</em> efficient uncoupled dynamics.</p>
<h3>Back to quantum communication</h3>
<p>So, what is a natural model of quantum uncoupled dynamics? I was confused about this for a few weeks: people have studied <a href="https://en.wikipedia.org/wiki/Quantum_game_theory">quantum games</a> where players’ actions are described by qubits, and the payoffs are determined based on their measurements. But this is a generalization of classical games, where we already know that the problem is hard. So I asked Shalev again, and he had another nice observation: the players can still send classical bits (aka play classical actions) — they merely need to share entangled qubits and measure them before deciding what strategies to play. (But admittedly this might not work if the police decide to search the <a href="https://en.wikipedia.org/wiki/Prisoner%27s_dilemma">Dilemma Prisoners</a> for entangled qubits before their interrogation…)</p>
<p>One last motivational comment: my very superficial understanding of the real-world feasibility of all this stuff is that while there is a lot of buzz around the race toward the first quantum <em>computer</em> that may or may not be able to execute a “hello world!” program <a href="https://www.technologyreview.com/s/609451/ibm-raises-the-bar-with-a-50-qubit-quantum-computer/">[1]</a><a href="https://ai.googleblog.com/2018/03/a-preview-of-bristlecone-googles-new.html">[2]</a><a href="https://www.theguardian.com/technology/2018/mar/08/australian-scientists-move-closer-to-world-beating-quantum-computer-michelle-simmons">[3]</a>, quantum <em>communication</em> already allows <a href="https://www.wired.com/story/why-this-intercontinental-quantum-encrypted-video-hangout-is-a-big-deal/">cross-continental video conferences</a>…</p>
<h3>Quantum + game theory</h3>
<p>While writing this post, I realized that there is an entire literature on various ways to entangle quantum with game theory. My favorite is <a href="https://arxiv.org/abs/1101.3380">this paper</a> by Alan Deckelbaum about <strong>quantum correlated equilibrium</strong>. Correlated equilibrium is a generalization of Nash equilibrium where a trusted <em>coordinating device</em> suggests to Alice and Bob pairs of actions <img alt="i,j" class="latex" src="https://s0.wp.com/latex.php?latex=i%2Cj&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i,j"/> drawn from a joint (correlated) distribution. The requirement is that Alice, after seeing the action the coordinating device suggested for her (but not the one for Bob), has no incentive to deviate from the suggestion. It is known that natural no-internal-regret dynamics converge to the <em>set</em> of correlated equilibria. But without the trusted coordinating device the players are still incentivized to keep modifying their strategies. (By the way, the classical communication complexity of approximate correlated equilibrium in two-player games is still open!)</p>
<p>Anyway, Deckelbaum points out that any correlated distribution can be simulated using quantum entanglement. Can quantum entanglement replace the trusted coordinating device? Sometimes, but there is a catch: sampling from the correlated distribution using quantum measurements requires players to cooperate with the sampling protocol. Specifically, Deckelbaum shows that some games have correlated equilibria that cannot be truthfully sampled with quantum entanglement, i.e. one of the players has an incentive to deviate from any sampling protocol. He defines quantum correlated equilibria as those that can be sampled truthfully, and asks what is the computational complexity of finding one. (Note that this is an easier question than the PPAD-complete Nash equilibrium, and harder than the polynomial-time correlated equilibrium.) So here is yet another nice question:</p>
<p style="padding-left: 30px;">2. What is the (randomized/quantum) communication complexity of finding an approximate quantum correlated equilibrium?</p></div>
    </content>
    <updated>2018-06-26T13:13:35Z</updated>
    <published>2018-06-26T13:13:35Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>aviad.rubinstein</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2018-12-17T05:30:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1376</id>
    <link href="https://theorydish.blog/2018/06/04/don-knuth-on-general-principles/" rel="alternate" type="text/html"/>
    <title>Don Knuth on General Principles</title>
    <summary>In the last post (at least for now) of our advice-sharing project, we now share the final installment of Don Knuth’s advice (also see his first and second posts). Interestingly (as I mention in the first comment below), these comments are in disagreement with others we shared. Success in your career will be determined more by your weaknesses than by your strengths. Thus, if you imagine plotting a sequence of scores that rate your ability to carry out different kinds of tasks, it’s far better to have a high minimum than a high maximum. Try to identify your weaknesses and to overcome as many as you can. Every large project has parts that are fun and parts that are dull. Learn to get through the dull parts. Never postpone a distasteful-but-necessary portion of work-to-be-done, unless there’s a very good reason why you’ll be able to do it better later. Niels Hendrik Abel gave wonderful advice: “Read the masters!” Take the time to read lots of papers that were written by top researchers when they were first discovering important ideas. Study the works of great computer scientists, and do your best to understand their mindset. In order to do this well, you’ll have [...]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the last post (at least for now) of our <a href="https://theorydish.blog/tag/tips/">advice-sharing project</a>, we now share the final installment of Don Knuth’s advice (also see his <a href="https://theorydish.blog/2018/02/01/donald-knuth-on-doing-research/">first </a>and <a href="https://theorydish.blog/2018/02/26/donald-knuth-on-writing-up-research/">second</a> posts). Interestingly (as I mention in the first comment below), these comments are in disagreement with others we shared.</p>
<hr/>
<p>Success in your career will be determined more by your weaknesses than<br/>
by your strengths. Thus, if you imagine plotting a sequence of scores<br/>
that rate your ability to carry out different kinds of tasks, it’s<br/>
far better to have a high minimum than a high maximum. Try to identify<br/>
your weaknesses and to overcome as many as you can.</p>
<p>Every large project has parts that are fun and parts that are dull.<br/>
Learn to get through the dull parts. Never postpone a<br/>
distasteful-but-necessary portion of work-to-be-done, unless<br/>
there’s a very good reason why you’ll be able to do it better later.</p>
<p>Niels Hendrik Abel gave wonderful advice: “Read the masters!”<br/>
Take the time to read lots of papers that were written by top researchers<br/>
when they were first discovering important ideas. Study the works of<br/>
great computer scientists, and do your best to understand their mindset.<br/>
In order to do this well, you’ll have to learn how to put yourself into<br/>
their place — remembering what they knew and didn’t know at the time,<br/>
and adjusting to their terminology and notation. The exercise of “getting<br/>
inside another person’s head” is, in itself, extremely valuable for<br/>
building your own mental skills.</p>
<p>Here’s a trick that I often use when reading a technical book or paper:<br/>
After the author has stated a problem to be solved (or a theorem to be<br/>
proved, etc.), I cover up the text and spend some time trying to solve<br/>
that problem by myself. Similarly, before turning the page, I try to<br/>
guess what’s on the next page. Of course I usually fail … but even<br/>
in failure, I’m much more ready to understand the author’s solution,<br/>
than if I hadn’t tried it first. Furthermore, with this modus operandi<br/>
I’m repeatedly learning new ways to get past stumbling blocks.</p>
<p>Instead of promoting yourself aggressively, you should try to write so<br/>
well that others can readily see for themselves the value of what you’ve<br/>
done. Then they’ll spontaneously also tell their friends, and the<br/>
word will spread. On the other hand, if a good writer comes to you and<br/>
wants to publish an account of your work, it never hurts to have a<br/>
good “press agent”.</p>
<hr/>
<p>PS. (from Don) re “reading the masters”</p>
<p>“The purpose of … reading is precisely to suspend one’s mind<br/>
in the workings of another sensibility”.<br/>
— Guy Davenport, quoted in Harvard Magazine Nov-Dec 2017, p54</p>
<div class="yj6qo"/>
<div class="adL"/></div>
    </content>
    <updated>2018-06-04T18:03:41Z</updated>
    <published>2018-06-04T18:03:41Z</published>
    <category term="Research Life"/>
    <category term="tips"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2018-12-17T05:30:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=3955</id>
    <link href="https://www.scottaaronson.com/blog/?p=3955" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=3955#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=3955" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">My Tomassoni-Chisesi Prize talk</title>
    <summary xml:lang="en-US">Update (Sep. 21) Video of Philip Kim’s and my talks is now available! (But not streaming, just a giant mp4 that you can download.) On Thursday, I had the incredible honor of accepting the 2018 Tomassoni-Chisesi Prize in Physics at Università “La Sapienza” in Rome—“incredible” mostly because I’m of course not a physicist.  (I kept worrying […]
      <div class="commentbar">
        <p/>
        <span class="commentbutton" href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=3955"/>
        <a href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=3955">
          <img class="commenticon" src="/images/feed-icon.png"/> Subscribe to comments
        </a>  | 
        <a href="https://www.scottaaronson.com/blog/?p=3955#comments">
          <img class="commenticon" src="/images/post-icon.png"/> Post a comment
        </a>
      </div>
    </summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><b><font color="red">Update (Sep. 21)</font></b> <a href="https://filesender.garr.it/filesender/?vid=735437e2-4a16-8368-cbae-00003117d6b4">Video of Philip Kim’s and my talks is now available!</a>  (But not streaming, just a giant mp4 that you can download.)</p>
<hr/>
<p>On Thursday, I had the incredible honor of accepting the 2018 <a href="http://www.fondazionesapienza.uniroma1.it/?q=node/547">Tomassoni-Chisesi Prize in Physics</a> at <a href="https://en.wikipedia.org/wiki/Sapienza_University_of_Rome">Università “La Sapienza”</a> in Rome—“incredible” mostly because I’m of course not a physicist.  (I kept worrying they’d revoke the award when they realized I could barely solve the wave equation.)  This is not the first time quantum information was recognized; the prize has <a href="https://en.wikipedia.org/wiki/Tomassoni_awards">previously</a> gone to Serge Haroche and Alain Aspect.  This year, for the first time, there was both an under-40 and an over-40 award; the latter went to <a href="https://www.seas.harvard.edu/directory/philipkim">Philip Kim</a>, a quantum materials researcher at Harvard who I had the privilege to meet on this trip (he’s the taller one below).</p>
<p><img src="https://pbs.twimg.com/media/Dm-_GiYX0AIudkD.jpg"/></p>
<p>I’m unbelievably grateful, not only to the committee, and its chair <a href="https://en.wikipedia.org/wiki/Giorgio_Parisi">Giorgio Parisi</a> (whose seminal work on phase transitions and satisfiability I’d long known, but who I met for the first time on this trip), but to Fabio Sciarrino, Paolo Mataloni, Fernanda Lupinacci, and everyone else who graciously hosted me and helped make my hastily-planned visit to Europe a success.</p>
<p>The department I visited has a storied history: here are the notes that Enrico Fermi left, documenting what he covered each day in his physics class in 1938.  The reason the last squares are blank is that, when Fermi and his Jewish wife left for Stockholm on the occasion of Fermi’s Nobel Prize, they continued directly to the US rather than return to an Italy that had just passed the racial laws.</p>
<p><a href="https://www.scottaaronson.com/blog/?attachment_id=3965" rel="attachment wp-att-3965"><img alt="" class="aligncenter size-medium wp-image-3965" height="300" src="https://www.scottaaronson.com/blog/wp-content/uploads/2018/09/fermi-225x300.jpg" width="225"/></a></p>
<p>On my way to Rome, I also gave two talks at a “quantum computing hackathon” in Zurich, called <a href="https://qid.ethz.ch/">QuID</a> (Quantum Information for Developers).  Thanks so much to Lidia del Rio for arranging <em>that</em> visit, which was fantastic as well.</p>
<p>To accept the Tomassoni-Chisesi prize, I had to give a 40-minute talk summarizing all my research from 2000 to the present—the hardest part being that I had to do it while wearing a suit, and sweating at least half my body weight.  (I also had a cold and a hacking cough.)  I think there will eventually be video of my and Prof. Kim’s talks, but it’s not yet available.  In the meantime, for those who are interested, <a href="https://www.scottaaronson.com/talks/3questions.ppt">here</a> are my PowerPoint slides, and here’s the title and abstract:</p>
<p style="padding-left: 30px;"><span class="il">Three</span> <span class="il">Questions</span> About <span class="il">Quantum</span> Computing<br/>
Scott Aaronson (University of Texas at Austin)</p>
<p style="padding-left: 30px;">I’ll discuss some of my work in quantum computing over the past 18 years, organizing it in terms of three questions.  First, how can we demonstrate, using near-future hardware, that quantum computers can get any genuine speedups at all over classical computers (ideally useful speedups)?  Second, what sorts of problems would be hard even for quantum computers, and can we turn the intractability of those problems to our advantage?  Third, are there physically reasonable models of computation even more powerful than quantum computing, or does quantum computing represent an ultimate limit?</p>
<p>If you’re a regular reader here, most of the content will be stuff you’ve seen before, with the exception of a story or two like the following:</p>
<p style="padding-left: 30px;">Last night I was talking to my mom about my grandfather, who as it happens came through Rome 73 years ago, as an engineer with the US Army.  Disabling landmines was, ironically, one of the safer ways to be a Jew in Europe at that time.  If you’d told him then that, three-quarters of a century later, his grandson would be back here in Rome to accept an award for research in quantum computational complexity … well, I’m sure he’d have any number of questions about it.  But one thing I clearly remember is that my grandfather was always full of effusive praise for the warmth of the people he met in Italy—how, for example, Italian farmers would share food with the hungry and inadequately-provisioned Allied soldiers, despite supposedly being on the opposing side.  Today, every time I’m in Italy for a conference or a talk, I get to experience that warmth myself, and certainly the food part.</p>
<p>(<em>Awww!</em>  But I meant it.  Italians <em>are</em> super-warm.)</p>
<p>There’s a view that scientists should just pursue the truth and be serenely unaffected by prizes, recognition, and other baubles.  I think that view has a great deal to be said for it.  But thinking it over recently, I struck the following mental bargain: <em>if</em> I’m going to get depressed on a semi-regular basis by people attacking me online—and experience shows that I will—well then, <em>I also get to enjoy whatever’s the opposite of that with a clear conscience</em>.  It’s not arrogance or self-importance; it’s just trying to balance things out a bit!</p>
<p>So again, thanks so much—to the physics department of La Sapienza, but also to my family, friends, mentors, readers, colleagues at UT Austin and around the world, and everyone else who helps make possible whatever it is that I do.</p></div>
    </content>
    <updated>2018-09-15T05:32:54Z</updated>
    <published>2018-09-15T05:32:54Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Adventures in Meatspace"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2018-12-12T11:39:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=3943</id>
    <link href="https://www.scottaaronson.com/blog/?p=3943" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=3943#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=3943" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Lecture notes!  Intro to Quantum Information Science</title>
    <summary xml:lang="en-US">Someone recently wrote that my blog is “too high on nerd whining content and too low on actual compsci content to be worth checking too regularly.”  While that’s surely one of the mildest criticisms I’ve ever received, I hope that today’s post will help to even things out. In Spring 2017, I taught a new […]
      <div class="commentbar">
        <p/>
        <span class="commentbutton" href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=3943"/>
        <a href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=3943">
          <img class="commenticon" src="/images/feed-icon.png"/> Subscribe to comments
        </a>  | 
        <a href="https://www.scottaaronson.com/blog/?p=3943#comments">
          <img class="commenticon" src="/images/post-icon.png"/> Post a comment
        </a>
      </div>
    </summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Someone recently wrote that my blog is “too high on nerd whining content and too low on actual compsci content to be worth checking too regularly.”  While that’s surely one of the mildest criticisms I’ve ever received, I hope that today’s post will help to even things out.</p>
<p>In Spring 2017, I taught a new undergraduate course at UT Austin, entitled Introduction to Quantum Information Science.  There were about 60 students, mostly CS but also with strong representation from physics, math, and electrical engineering.  One student, Ewin Tang, made a <a href="https://www.scottaaronson.com/blog/?p=3880">previous appearance</a> on this blog.  But today belongs to another student, Paulo Alves, who took it upon himself to make detailed notes of all of my lectures.  Using Paulo’s notes as a starting point, and after a full year of procrastination and delays, I’m now happy to release the full lecture notes for the course.  Among other things, I’ll be using these notes when I teach the course a second time, starting … holy smokes … <em>this Wednesday</em>.</p>
<p>I don’t pretend that these notes break any new ground.  Even if we restrict to undergrad courses only (which rules out, e.g., <a href="http://www.theory.caltech.edu/people/preskill/ph229/">Preskill’s legendary notes</a>), there are already other great quantum information lecture notes available on the web, such as <a href="https://inst.eecs.berkeley.edu/~cs191/fa14/">these from Berkeley</a> (based on a course taught by, among others, my former adviser Umesh Vazirani and committee member Birgitta Whaley), and <a href="https://cs.uwaterloo.ca/~watrous/LectureNotes/CPSC519.Winter2006/all.pdf">these from John Watrous in Waterloo</a>.  There are also dozens of books—including <a href="https://www.amazon.com/Quantum-Computer-Science-David-Mermin/dp/0521876583">Mermin’s</a>, which we used in this course.  The only difference with these notes is that … well, they cover exactly the topics <em>I’d</em> cover, in exactly the order I’d cover them, and with exactly the stupid jokes and stories I’d tell in a given situation.  So if you like my lecturing style, you’ll probably like these, and if not, not (but given that you’re here, there’s hopefully some bias toward the former).</p>
<p>The only prerequisite for these notes is some minimal previous exposure to linear algebra and algorithms.  If you read them all, you might not be ready yet to do research in quantum information—that’s what a grad course is for—but I feel good that you’ll have an honest understanding of what quantum information is all about and where it currently stands.  (In fact, where it <em>already</em> stood by the late 1990s and early 2000s, but with many comments about the theoretical and experimental progress that’s been made since then.)</p>
<p>Also, if you’re one of the people who read <em><a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing Since Democritus</a></em> and who was disappointed by the lack of basic quantum algorithms in that book—a function of the book’s origins, as notes of lectures given to graduate students who already<em> knew</em> basic quantum algorithms—then consider these new notes my restitution.  If nothing else, no one can complain about a dearth of basic quantum algorithms <em>here</em>.</p>
<p>I welcome comments, bugfixes, etc.  Thanks so much, not only to Paulo for transcribing the lectures (and making the figures!), but also to Patrick Rall and Corey Ostrove for TA’ing the course, to Tom Wong and Supartha Podder for giving guest lectures, and of course, to all the students for making the course what it was.</p>
<ul>
<li><a href="https://www.scottaaronson.com/qclec/1.pdf">Lecture 1</a>: Course Intro, Church-Turing Thesis (3 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/2.pdf">Lecture 2</a>: Probability Theory and QM (5 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/3.pdf">Lecture 3</a>: Basic Rules of QM (4 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/4.pdf">Lecture 4</a>: Quantum Gates and Circuits, Zeno Effect, Elitzur-Vaidman Bomb (5 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/5.pdf">Lecture 5</a>: Coin Problem, Inner Products, Multi-Qubit States, Entanglement (5 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/6.pdf">Lecture 6</a>: Mixed States (6 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/7.pdf">Lecture 7</a>: Bloch Sphere, No-Cloning, Wiesner’s Quantum Money (6 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/8.pdf">Lecture 8</a>: More on Quantum Money, BB84 Quantum Key Distribution (5 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/9.pdf">Lecture 9</a>: Superdense Coding (2 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/10.pdf">Lecture 10</a>: Teleportation, Entanglement Swapping, GHZ State, Monogamy (5 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/11.pdf">Lecture 11</a>: Quantifying Entanglement, Mixed State Entanglement (4 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/12.pdf">Lecture 12</a>: Interpretation of QM (Copenhagen, Dynamical Collapse, MWI, Decoherence) (10 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/13.pdf">Lecture 13</a>: Hidden Variables, Bell’s Inequality (5 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/14.pdf">Lecture 14</a>: Nonlocal Games (7 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/15.pdf">Lecture 15</a>: Einstein-Certified Randomness (4 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/16.pdf">Lecture 16</a>: Quantum Computing, Universal Gate Sets (8 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/17.pdf">Lecture 17</a>: Quantum Query Complexity, Deutsch-Jozsa (8 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/18.pdf">Lecture 18</a>: Bernstein-Vazirani, Simon (7 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/19.pdf">Lecture 19</a>: RSA and Shor’s Algorithm (6 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/20.pdf">Lecture 20</a>: Shor, Quantum Fourier Transform (8 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/21.pdf">Lecture 21</a>: Continued Fractions, Shor Wrap-Up (4 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/22.pdf">Lecture 22</a>: Grover (9 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/23.pdf">Lecture 23</a>: BBBV, Applications of Grover (7 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/24.pdf">Lecture 24</a>: Collision and Other Applications of Grover (6 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/25.pdf">Lecture 25</a>: Hamiltonians (10 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/26.pdf">Lecture 26</a>: Adiabatic Algorithm (10 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/27.pdf">Lecture 27</a>: Quantum Error Correction (8 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/28.pdf">Lecture 28</a>: Stabilizer Formalism (9 pages)</li>
<li><a href="https://www.scottaaronson.com/qclec/29.pdf">Lecture 29</a>: Experimental Realizations of QC (9 pages)</li>
</ul>
<p>And by popular request, here are the 2017 problem sets!</p>
<ul>
<li><a href="https://www.scottaaronson.com/qclec/ps1.pdf">Set 1</a></li>
<li><a href="https://www.scottaaronson.com/qclec/ps2.pdf">Set 2</a></li>
<li><a href="https://www.scottaaronson.com/qclec/ps3.pdf">Set 3</a></li>
<li><a href="https://www.scottaaronson.com/qclec/ps4.pdf">Set 4</a></li>
<li><a href="https://www.scottaaronson.com/qclec/ps5.pdf">Set 5</a></li>
<li><a href="https://www.scottaaronson.com/qclec/ps6.pdf">Set 6</a></li>
<li><a href="https://www.scottaaronson.com/qclec/ps7.pdf">Set 7</a></li>
<li><a href="https://www.scottaaronson.com/qclec/ps8.pdf">Set 8</a></li>
<li><a href="https://www.scottaaronson.com/qclec/ps9.pdf">Set 9</a></li>
<li><a href="https://www.scottaaronson.com/qclec/ps10.pdf">Set 10</a></li>
<li><a href="https://www.scottaaronson.com/qclec/ps11.pdf">Set 11</a></li>
</ul>
<p>I <em>might</em> post solutions at a later date.</p>
<p><strong>Note:</strong> If you’re taking the course in 2018 or a later year, these sets should be considered outdated and for study purposes only.</p>
<hr/>
<p><b><span style="color: red;">Notes and Updates (Aug. 27)</span></b></p>
<p><a href="https://www.scottaaronson.com/qclec/combined.pdf">Here’s a 184-page combined file</a>. Thanks so much to Robert Rand, Oscar Cunningham, Petter S, and Noon van der Silk for their help with this.</p>
<p>If it wasn’t explicit: these notes are copyright Scott Aaronson 2018, free for personal or academic use, but not for modification or sale.</p>
<p>I’ve freely moved material between lectures so that it wasn’t arbitrarily cut across lecture boundaries. This is one of the reasons why some lectures are much longer than others.</p>
<p>I apologize that some of the displayed equations are ugly. This is because we never found an elegant way to edit equations in Google Docs.</p>
<p>If you finish these notes and are still hankering for more, try my <a href="http://stellar.mit.edu/S/course/6/fa14/6.845/materials.html">Quantum Complexity Theory</a> or <a href="http://stellar.mit.edu/S/course/6/sp16/6.045/materials.html">Great Ideas in Theoretical Computer Science</a> lecture notes, or my <a href="http://www.scottaaronson.com/barbados-2016.pdf">Barbados lecture notes</a>.  I now have links to all of them on the sidebar on the right.</p></div>
    </content>
    <updated>2018-08-26T23:06:36Z</updated>
    <published>2018-08-26T23:06:36Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2018-12-12T11:39:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=3922</id>
    <link href="https://www.scottaaronson.com/blog/?p=3922" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=3922#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=3922" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Thank you, world!</title>
    <summary xml:lang="en-US">1. This post has no technical content.  As the tag indicates, it’s entirely “Nerd Self-Help”—thoughts I’ve recently found extremely helpful to me, and that I’m hopeful some others might be able to apply to their own life situations.  If that doesn’t interest you, feel free to skip. 2. I’m using the numbered list format simply […]
      <div class="commentbar">
        <p/>
        <span class="commentbutton" href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=3922"/>
        <a href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=3922">
          <img class="commenticon" src="/images/feed-icon.png"/> Subscribe to comments
        </a>  | 
        <a href="https://www.scottaaronson.com/blog/?p=3922#comments">
          <img class="commenticon" src="/images/post-icon.png"/> Post a comment
        </a>
      </div>
    </summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>1. This post has no technical content.  As the tag indicates, it’s entirely “Nerd Self-Help”—thoughts I’ve recently found extremely helpful to <em>me</em>, and that I’m hopeful some others might be able to apply to their own life situations.  If that doesn’t interest you, feel free to skip.</p>
<p>2. I’m using the numbered list format simply because I have a large number of interrelated things to say, and getting each one down precisely seems more important than fashioning them into some coherent narrative.</p>
<p>3. For someone who walks around every day wracked by neurosis, social anxiety, tics, and depression, I’m living an <em>unbelievably</em> happy and fulfilling life.  For this I’m profoundly grateful—to “the universe,” but much more so, to the family and friends and colleagues who’ve made it possible.</p>
<p>4. On bad days, I’ve cursed fate for having placed me in a world to which my social skills were so poorly adapted.  On good days, though, I’ve thanked fate for letting me thrive in such a world, <em>despite</em> my social skills being so maladapted to it.  My ability to thrive in this world owes everything to the gifts of modernity, to <a href="https://www.scottaaronson.com/blog/?p=3654">the stuff Steven Pinker talks about in <em>Enlightenment Now</em></a>: the decline of violence, the rule of law, the freedom from hunger, disease, and war, but most of all the rise of science.  So I have a personal reason to be grateful for modernity and to care deeply about its preservation—and to detest Trump and all the other would-be autocrats who’d gleefully take an ax to it.  Like hothouse plants, nerds can flourish only in artificially safe environments.  I don’t often enough express my gratitude for having been born into a world that contains such environments, so I’m taking the opportunity to do so today.</p>
<p>5. I got back a few days ago from a wonderful visit to Mexico City—thanks so much to Sergio Rajsbaum, Luis González, and all my other new friends there for helping to organize it.  I gave three talks at UNAM, one of the largest universities on earth.  I ate … well, the best Mexican food I ever tasted.  I saw amazing sights, including the <a href="https://en.wikipedia.org/wiki/National_Museum_of_Anthropology_(Mexico)">National Museum of Anthropology</a>, which has hall after hall full of Aztec and Maya artifacts of a grandeur one normally associates with ancient Egypt, Greece, or Rome.  Go there if you want a visceral sense for the scale of the tragedy wrought by the conquistadors.  (On the other hand, having seen the decorated ceremonial knives, the skulls of children whose hearts were ripped out while still beating, I do have to count the end of human sacrifice as a net positive.)</p>
<p>6. The trip was surreal: I discussed quantum computing and philosophy and Mexican history over enchiladas and tequila.  I signed copies of my book, lectured, met fans of this blog.  There was lots of good-natured laughter about the <a href="https://www.scottaaronson.com/blog/?p=3903">tale of my arrest</a>, and stern reminders to be careful when ordering smoothies.  A few people I met shared their own stories of being harassed by US police over trivial mishaps (e.g., “put your hands on the car,” rifle aimed, over a parking violation), exacerbated of course by their being Mexicans.  One colleague opined that he preferred the Mexican system, wherein you and the officer just calmly, politely discussed how many pesos would make the problem go away.  But then, from time to time, I’d check my phone and find fresh comments accusing me of being a thief, a nutcase incapable of functioning in society, a racist who wants to be treated differently from blacks and Latinos (the actual view expressed in my post was <em>precisely the opposite of that</em>), or even a money-grubbing Jew hyperventilating about “anuddah Shoah.”</p>
<p>7. The real world has a lot to be said for it.  Maybe I should spend more time there.</p>
<p>8. Thanks so much to everyone who sent emails or left comments expressing sympathy about my arrest—or even who simply found the story crazy and amusing, like a <em>Seinfeld</em> episode.  Meanwhile, to those who berated me for being unable to function in society: does it <em>bother</em> you, does it present a puzzle for your theory, that rather than starving under a bridge, I’m enjoying a career doing what I love, traveling the world giving lectures, happily married with two kids?  Do I not, if nothing else, illustrate how functional a non-functional person can be?</p>
<p>9. It’s possible that my kids will grow up with none of the anxiety or depression or neuroticism or absentmindedness that I’ve had.  But if they <em>do</em> have those problems … well, I’m thankful that I can provide them at least one example of what it’s possible to do in life in spite of it!</p>
<p>10. On SneerClub, someone opined that not only was I an oblivious idiot at the smoothie counter, I must <em>also</em> be oblivious to how bad the incident makes me look—since otherwise, I would never have blogged about it.  I ask my detractors: can you imagine, for one second, being so drunk on the love of truth that you’d take the experiences that made you look the most pathetic and awkward, and share them with the world in every embarrassing detail—because “that which can be destroyed by the truth should be”?  This drunkenness on truth is scary, it’s destabilizing, it means that every day you run a new risk of looking foolish.  But as far as I can introspect, it’s also barely distinguishable from the impulse that leads to doing good science: asking the questions everyone else knows better than to ask, clarifying the obvious, confessing one’s own doofus mistakes.  So as a scientist, I’m grateful to have this massive advantage, for all its downsides.</p>
<p>11. Of the hundreds of reactions to my arrest, some blamed me, some the police, some both and some neither.  As I mentioned <a href="https://www.scottaaronson.com/blog/?p=3903">before</a>, there was an <em>extremely</em> strong and surprising national split, with Americans siding with the police and non-Americans siding with me.  But there was also an even deeper split: namely, almost everyone who already liked me found the story funny or endearing or whatever, while almost everyone who already hated me found in it new reasons for their hate.  I’ve observed this to be a general phenomenon: <em>within the range of choices I’d realistically consider, none of them seem to do anything to turn enemies into friends or friends into enemies.</em>  If so, then that’s a profoundly liberating realization.  It means that I might as well just continue being myself, saying and doing what seem reasonable to me, without worrying about either winning over the SneerClubbers <em>or</em> losing the people who like this blog.  For neither of those is likely to happen–even if we ignore all the other reasons to eschew overreliance on external validation.</p>
<p>12. Every week or so I get emails from people wanting to share their spiritual theories with me, and to illustrate them with color diagrams.  Most such emails go straight to my trash folder.  This week, however, I received one that contained a little gem of insight:</p>
<p style="padding-left: 30px;">I realize you are professionally reluctant to admit that Spirit actually exists. However, it is obvious to me from your blog that you are personally committed to what I might label “spiritual development.” You are continually pushing yourself and others to be more self-aware, reflect on our actions and assumptions, and choose to become our best selves.</p>
<p style="padding-left: 30px;">I can only imagine how much pain and psychic energy it costs you to do that so publicly and vulnerably. But that is precisely why so many of us love you; and others hate you, because they are understandably terrified of paying that same price.</p>
<p>13. To those who’ve called me a terrible person, based on how they <em>imagine</em> I’d respond in hypothetical scenarios of their own construction, I make one request.  Before passing final judgment, at least exchange emails with me, or meet me, or otherwise give me a chance to differentiate myself from your internal bogeyman.  Ask me for grad school advice, or comments on your CS idea, or whatever—and with nothing in it for me, and swamped with similar requests, see how much time I spend trying to help you.  Or ask me to donate to your favorite charity, and see if I do it.  Or tell me about misconduct by a prominent member of my community, and see how I respond.  See if any of this is noticeably affected by your race, religion, gender, sexual orientation, or anything else besides the honesty of your request.</p>
<p>14. None of the above are hypotheticals for me.  Once I was given firsthand reports, which I judged to be extremely credible, about a serial sexual harasser of women in the math and TCS communities.  The victims had already pursued formal complaints, but with an unsatisfactory resolution.  In response, I immediately offered to publish the perpetrator’s name on this blog along with the evidence and accusations, or help in any other way desired.  My offer was declined, but it still stands if the victims were to change their minds.</p>
<p>15. My mom once told me that, having been hippies concerned about overpopulation, she and my dad weren’t planning to have any kids.  When they finally decided to do so, it was in order to “spite Hitler.”  I felt incredibly proud to have that be the reason for my birth.  Every time I think about it, it fills me with a renewed urge to stand up for whatever seems most human and compassionate, regardless of how unpopular.</p>
<p>16. Going forward, if I ever (hypothetically) experience a relapse of the suicidal thoughts that characterized part of my life, I’m going to say to myself: <strong>no.</strong>  Not only will I remain alive, I’ll continue to enjoy my family and friends and research and teaching, and mentor students, and get involved in issues I care about, and otherwise make the most of life.  And if for no other reason, I’d do this in order that Arthur Chu could remain, as he put it, “unhappy about [my] continued existence”!  Admittedly, spiting Chu and his chorus of SneerClubbers is far from the <em>only</em> reason to continue living, but it’s a perfectly sufficient reason in itself.  And this will be an impenetrable shield against suicidal thoughts.  So thanks, Arthur!</p>
<p>17. Four years ago, I received hundreds of moving responses to comment 171.  But perhaps the most touching were from several female classmates who I’d had crushes on back in the depressed period I wrote about, and who said some variant of: “it’s a shame you never asked me, because I liked you and would’ve gladly said yes.”  One of these classmates, bless her heart, recently asked me to share this information, as an encouragement to young nerdy readers who might find themselves in the same situation I was in.  Four years ago, a few feminists lectured me that the crippling fear I’d suffered was <em>good</em>, a feature rather than a bug: if only every other predatory nerdbro would be paralyzed by the same fear!  (That is, when they weren’t <em>also</em> lecturing me that the fears were ridiculous and existed only in my head.)  But the women who wrote to me are also left-wing feminists.  So if you confess your feelings to someone, know that no one who despises that decision, who considers it ‘problematic’ and ‘entitled’ and ‘privileged’ and all the rest of the modern litany of just-die-already words, can pretend to speak for all feminists.  I love my wife and my children, and wouldn’t go back in time to change my life’s trajectory if I could.  But you, readers, armed with wisdom I lacked, can reach a happy place in your lives a hell of a lot faster than I did.</p>
<p>18. While this has been beneath the surface of a huge number of my posts, it seems worth bringing out explicitly.  On certain blogs and social media sites, I’m regularly described as a “leftist troll,” a “pathetic, mewling feminist,” or a “rabid establishment liberal.”  On others I’m called a “far-right Zionist” or an “anti-feminist men’s rights advocate.”  It’s enough to make even me confused.  But here’s how I choose to define my stance: my party is the Party of Psychological Complexity.  Our party platform consists of Shakespeare’s plays, the movie <em>The Breakfast Club</em>, the novels of Mark Twain and Philip Roth and Rebecca Goldstein, classic <em>Simpsons</em> and <em>Futurama</em>, and anything else that tries to grapple with human nature honestly.  For most of the past few centuries, the Party of Psychological Complexity has been in a coalition with the political left, because both were interested in advancing Enlightenment ideals, ending slavery and female subjugation and other evils, and broadening humankind’s circles of empathy.  But the PoPC and the political left already split once, over the question of Communism, and today they split again over the morality and the wisdom of social justice vigilantism.</p>
<p>19. Here in the PoPC, our emphasis on the staggering complexity of the individual conscience might seem hard to square with utilitarian ethics: with public health campaigns, Effective Altruism, doing the greatest good for the greatest number, etc.  But the two philosophies actually fit beautifully.  In the PoPC, our interest (you might say) is in the <em>psychological prerequisites</em> to utilitarianism: in the “safe spaces” for the weird and nerdy and convention-defying and literal-minded in human nature that need to get established, before discussion about the best ways to fight malaria or global warming or nuclear proliferation or plastic in the oceans can even begin.</p>
<p>20. On leftist forums like SneerClub, whenever I’m brought up, I’m considered a dangerous reactionary—basically Richard Spencer or Alex Jones except with more quantum query complexity.  Yet, while there are differences in emphasis, and while my not being in politics gives me more freedom to venture outside the Overton window, my views on most contemporary American issues are hard to distinguish from those of Barack Obama, who I consider to have been a superb president and a model of thoughtful leadership.  If you want to understand how racist demagogues managed to take over the US—well, there was a perfect storm of horribleness, with no one decisive factor.  But it surely didn’t help that the modern social-justice left so <em>completely</em> disdains coalition-building, so values the purity of the Elect above all else, that it cast even progressive Obama supporters like me into its lowest circle of Hell.</p>
<p>21. Open yourself up to the complicated and the true in human nature.  Don’t be like Donald Trump or Arthur Chu, two men who represent opposite poles of ideology, yet who have in common that they both <a href="https://www.facebook.com/jefftk/posts/649351137992?comment_id=972767&amp;offset=50&amp;total_comments=132">purposefully killed</a> what was complicated in themselves.  For those two, winning is all that matters—they’ve explicitly said so, and have organized their entire lives around that principle.  But winning is not all that matters.  When I stand before the Lord of Song, even though it all went wrong, the only word on my lips will be “hallelujah”–because while I have many faults, I did make some room in life for beauty and truth, even at the expense of winning.  Though everything temporal turns to dust, I experienced some moments of eternity.</p>
<p>22. I can already predict the tweets: “No, Scott Aaronson, your weird numbered ruminations won’t save you from being the privileged douchebag who you fundamentally are.”  How was that?  Let me try another: “Aaronson embarrasses himself yet again, proves he doesn’t get why nerd culture is totally f-cked up.”  Here in the Party of Psychological Complexity, we’re used to this stuff.  We don’t fare well in social media wars, and we’ll gladly lose rather than become what we detest.  And yet, over the long run—which might be the <em>very</em> long run—we do mean to win, much like heliocentrism and quantum mechanics ultimately triumphed over simpler, more soundbite-friendly rivals.  Complex ideas win not through 140-character flinged excrement but through conversations, long-form essays, discourse, verbal technologies able to transfer large interconnected bundles of thoughts and emotions from one mind to another one that’s ready for such things.</p>
<p>23. Try every hour of every day to extend your sympathetic imagination to those who are unlike you (those who are like you don’t need such a strenuous effort).  And carve this message of universal compassion onto your doorposts, and bind it to your wrists, and put it for a sign on your foreheads.  There is no ideology that relieves us of the need to think and to feel: <em>that’s</em> my ideology.</p>
<p>24. When people give feedback about this blog’s topics, they seem roughly evenly split between those who beg for more quantum computing and other technical posts that they can actually learn from, and those who beg for more nontechnical posts that they can actually understand!  The truth is that, from the very beginning, this has <em>never</em> been a quantum computing or theoretical computer science blog—or rather it has been, but only incidentally.  If you had to sum it up in one sentence, I suppose this blog has been about surviving and thriving as a quantum complexity theorist in a world that isn’t designed for quantum complexity theorists?</p>
<p>25. But I’ll tell you what: my next post will be a quantum computing one, and I’ll make it worth the wait.  What else could I do by way of thanks to the world, and (more to the point) my family, friends, and readers?</p></div>
    </content>
    <updated>2018-08-15T20:02:01Z</updated>
    <published>2018-08-15T20:02:01Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Self-Help"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2018-12-12T11:39:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15359</id>
    <link href="https://rjlipton.wordpress.com/2018/10/21/the-inscribed-square-problem/" rel="alternate" type="text/html"/>
    <title>The Inscribed Square Problem</title>
    <summary>A remark on an open problem with an application of the Lebesgue Density Theorem [ Toeplitz ] Otto Toeplitz was a mathematician who made key contributions to functional analysis. He is famous for many things, including a kind of matrix named after him. Today we discuss one of his conjectures that remains open. Over a […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A remark on an open problem with an application of the Lebesgue Density Theorem</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2018/10/21/the-inscribed-square-problem/toeplitz_3/" rel="attachment wp-att-15361"><img alt="" class="alignright size-thumbnail wp-image-15361" height="150" src="https://rjlipton.files.wordpress.com/2018/10/toeplitz_3.jpeg?w=115&amp;h=150" width="115"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Toeplitz ]</font></td>
</tr>
</tbody>
</table>
<p>
Otto Toeplitz was a mathematician who made key contributions to functional analysis. He is famous for many things, including a kind of <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">matrix</a> named after <a href="https://en.wikipedia.org/wiki/Otto_Toeplitz">him</a>.</p>
<p>
Today we discuss one of his conjectures that remains open.<br/>
<span id="more-15359"/></p>
<p>
Over a century ago, in 1911 to be exact, Toeplitz proposed the <a href="https://en.wikipedia.org/wiki/Inscribed_square_problem">inscribed square problem</a> (ISP): </p>
<blockquote><p><b> </b> <em> <em>Does every Jordan curve contain an inscribed square?</em> </em>
</p></blockquote>
<p/><p>
More precisely, if <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> is a Jordan curve, then we want four distinct points on <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> so that they form a square. The dotted curve in Wikipedia’s figure has four squares. <a href="https://rjlipton.wordpress.com/2018/10/21/the-inscribed-square-problem/sq/" rel="attachment wp-att-15364"><img alt="" class="aligncenter size-thumbnail wp-image-15364" height="150" src="https://rjlipton.files.wordpress.com/2018/10/sq.png?w=138&amp;h=150" width="138"/></a></p>
<p>
</p><p/><h2> What Are Jordan Curves? </h2><p/>
<p/><p>
The notion of a curve, now called a <a href="https://en.wikipedia.org/wiki/Jordan_curve_theorem">Jordan curve</a>, is named for Camille Jordan. Sometimes called a plane simple closed curve, a Jordan curve is a non-self-intersecting continuous loop in the plane. More precisely it is the image <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> of an injective continuous map of a circle into the plane. The famous <a href="https://en.wikipedia.org/wiki/Jordan_curve_theorem">Jordan curve theorem</a> says: </p>
<blockquote><p><b> </b> <em> Every Jordan curve divides the plane into two connected regions—one bounded and one unbounded. </em>
</p></blockquote>
<p/><p>
This statement is one of those obvious ones that is actually hard to prove. The reason it is actually hard to prove is critical to our discussion of the ISP. The reason is that Jordan curves can be quite nasty. Here are two key issues with Jordan curves: </p>
<ol>
<li>
There are Jordan curves <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> so that the length of the curve is infinite. <p/>
</li><li>
There are Jordan curves <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> so that the area of the curve is positive.
</li></ol>
<p>
Thus (1) says that a Jordan curve can be infinitely long. This is already a problem with our notion that a curve is just something that you draw with a pen. Or if you draw the curve with a pen, then it takes a very long time to finish drawing the curve. Such curves are strange, but they are perfectly fine examples of Jordan curves. Next (2) says that a curve can have positive area or measure. Curves with positive area are now called Osgood curves after William Osgood who found the first example of such a curve. Intuitively, such a curve hardly seems to be a “curve”, but they are nevertheless. </p>
<p>
Even “nice” Jordan curves with finite length can be nasty.</p>
<p>
<a href="https://rjlipton.wordpress.com/2018/10/21/the-inscribed-square-problem/jcurve/" rel="attachment wp-att-15367"><img alt="" class="aligncenter size-thumbnail wp-image-15367" height="144" src="https://rjlipton.files.wordpress.com/2018/10/jcurve.jpeg?w=150&amp;h=144" width="150"/></a></p>
<p>
Not very intuitive to me. Even the above which is not one of the examples (1,2) is nasty looking. By the way Jordan found a cool proof of the following:</p>
<blockquote><p><b> </b> <em> <em>If <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{J}"/> is a Jordan curve with finite length, then <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{J}"/> has zero area.</em> </em>
</p></blockquote>
<p/><p>
See if you can figure this out—we supply a proof at the end of this post.</p>
<p>
</p><p/><h2> ISP On Nice Jordan Curves </h2><p/>
<p/><p>
So far the ISP is still open for arbitrary Jordan curves. It has been proved for convex curves and sufficiently smooth curves, but the question remains open in general. Even for polygonal curves—curves that come from polygons that are not self intersecting—it is non trivial. See the references below for some of the main results.</p>
<p>
</p><p/><h2> ISP On Nasty Jordan Curves </h2><p/>
<p/><p>
Curiously, if a Jordan curve <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> is nasty, then the ISP is true. That is:</p>
<blockquote><p><b>Theorem:</b> <em> Suppose that a Jordan curve <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{J}"/> has positive measure. Then there is an inscribed square that lies on <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{J}"/>. Thus in this case the ISP is true. </em>
</p></blockquote>
<p/><p>
The punch line here is that “bad” curves are well behaved with respect to having inscribed squares. The proof is based on a standard probabilistic argument. We will need the <a href="https://en.wikipedia.org/wiki/Lebesgue's_density_theorem">Lebesgue’s density theorem</a>. Suppose that <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> is a measurable subset of the plane. Now consider some small enough disk around a random point <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> in the plane. Then either almost all of the disk is in <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> or almost none. This “0-1” type law is a bit surprising, but it is quite useful. In particular, if <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> has positive measure, then almost all points <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> in <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> have the density-1 property. Here is a nice <a href="https://terrytao.wordpress.com/2007/06/18/the-lebesgue-differentiation-theorem-and-the-szemeredi-regularity-lemma/">comment</a> on this from Terry Tao’s blog:</p>
<blockquote><p><b> </b> <em> In other words, almost all the points <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x}"/> of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> are points of density of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/>, which roughly speaking means that as one passes to finer and finer scales, the immediate vicinity of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x}"/> becomes increasingly saturated with <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/>. (Points of density are like robust versions of interior points, thus the Lebesgue density theorem is an assertion that measurable sets are almost like open sets. This is Littlewood’s first principle.) </em>
</p></blockquote>
<p/><p>
Now let’s show that ISP is true for Jordan curves with positive measure.</p>
<p>
<em>Proof:</em>  Suppose that <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> is a Jordan curve that has positive measure. Then by the Lebesgue’s density theorem there is an open disk <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> so that <img alt="{S=D \cap J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%3DD+%5Ccap+J%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S=D \cap J}"/> has measure almost <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. We then argue that if we randomly select a square in the set <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> it has almost probability <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> to be inscribed on <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/>. This follows by a standard union bound. This then shows that there must be a square that is inscribed on <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> and that ISP is true for this curve. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
I am sure that this is well known to experts, but I include it here to highlight the result. I find it interesting that we can turn the “pathological” properties of a curve to our advantage. When given lemons, make lemonade. This is of course a recurrent theme in complexity theory: For example, we know that random boolean functions have high complexity.</p>
<p>
</p><p/><h2> References </h2><p/>
<p/><p>
Here are some references to work on ISP.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “Differentiability of Lipschitz functions, structure of null sets, and other problems” <br/>
<a href="http://pagine.dm.unipi.it/alberti/ricerca/2010-12/acp-icm2010.pdf">by</a> Giovanni Alberti, Marianna Csorynei, David Preiss</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “Splitting Loops And Necklaces: Variants Of The Square Peg Problem” <br/>
<a href="https://arxiv.org/abs/1806.02484">by</a> Jai Aslam, Shujian Chen, Florian Frick, Sam Saloff</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “Squares and other polygons inscribed in curves” <br/>
<a href="http://home.wlu.edu/~dennee/ugradresearch/squarepeg-ugrad-ed.pdf">by</a> Elizabeth Denne</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “Transversality in Configuration Spaces and the `Square Peg’ Theorem” <br/>
<a href="https://arxiv.org/abs/1402.6174">by</a> Jason Cantarella, Elizabeth Denney, and John McCleary</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “Every smooth Jordan curve has an inscribed rectangle with aspect ratio equal to <img alt="{\sqrt{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{3}}"/>” <br/>
<a href="https://arxiv.org/abs/1803.07417">by</a> Cole Hugelmeyer</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “A Combinatorial Approach to the Inscribed Square Problem” <br/>
<a href="http://www-users.math.umn.edu/~kell1642/kelley_thesis.pdf">by</a> Elizabeth Kelley</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “Squares on Curves” <br/>
<a href="https://dongryulkim.wordpress.com/2017/01/18/squares-on-curves/">by</a> Dongryul Kim</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “A survey on the Square Peg Problem” <br/>
<a href="http://people.mpim-bonn.mpg.de/matschke/SurveyOnSquarePeg_extension14.pdf">by</a> Benjamin Matschke</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “Figures Inscribed in Curves A short tour of an old problem” <br/>
<a href="http://www.webpages.uidaho.edu/~markn/squares/">by</a> Mark Nielsen</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “The Discrete Square Peg Problem” <br/>
<a href="https://arxiv.org/pdf/0804.0657.pdf">by</a> Igor Pak</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “On The Unfolding Of Simple Closed Curves” <br/>
<a href="https://www.ams.org/journals/tran/2009-361-04/S0002-9947-08-04781-8/S0002-9947-08-04781-8.pdf">by</a> John Pardon</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/>“On The Number Of Inscribed Squares In A Simple Closed Curve In The Plane” <br/>
<a href="https://arxiv.org/abs/0810.4806">by</a> Strashimir Popvassilev</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “The Jordan curve theorem is non-trivial” <br/>
<a href="https://facultystaff.richmond.edu/~wross/pdf/Jordan.pdf">by</a> Fiona Rossa and William Ross</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “Two discrete versions of the Inscribed Square Conjecture and some related problems” <br/>
<a href="https://www.sciencedirect.com/science/article/pii/S0304397510005487">by</a> Feliu Sagols and Raul Marin</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “A Trichotomy for Rectangles Inscribed in Jordan Loops” <br/>
<a href="https://arxiv.org/abs/1804.00740">by</a> Richard Schwartz</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “An Integration Approach To The Toeplitz Square Peg Problem” <br/>
<a href="https://arxiv.org/abs/1611.07441">by</a> Terence Tao.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Of course the key problem is the full conjecture, which remains open. But a possible approachable problem seems to be this: If a Jordan curve <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> has infinite length, can we show that conjecture is true?</p>
<p>
<em>Jordan’s proof of his claim</em>: Suppose <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> is a curve that has finite length <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/>. Partition the plane into a grid of side length <img alt="{L/n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L/n}"/> for some <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> to be selected in a moment. Also divide the curve <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> into <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> parts of length <img alt="{L/n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L/n}"/>. This is possible since <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> has finite length. The key is: each part of the curve hits at most <img alt="{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{4}"/> squares of the grid. Thus the area of the curve is easily seen to be bounded by </p>
<p align="center"><img alt="\displaystyle  4n\cdot \frac{L}{n} \cdot \frac{L}{n}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++4n%5Ccdot+%5Cfrac%7BL%7D%7Bn%7D+%5Ccdot+%5Cfrac%7BL%7D%7Bn%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  4n\cdot \frac{L}{n} \cdot \frac{L}{n}, "/></p>
<p>but this tends to <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> as <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> goes to infinity. Done.</p>
<p/><p><br/>
[three-&gt;four squares in intro]</p></font></font></div>
    </content>
    <updated>2018-10-22T00:26:52Z</updated>
    <published>2018-10-22T00:26:52Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="computational geometry"/>
    <category term="conjecture"/>
    <category term="geometry"/>
    <category term="square inscribed"/>
    <category term="Toeplitz"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2018-12-17T05:29:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15339</id>
    <link href="https://rjlipton.wordpress.com/2018/10/18/london-calling/" rel="alternate" type="text/html"/>
    <title>London Calling</title>
    <summary>For chess and science: a cautionary tale about decision models Clarke Chronicler blog source Marmaduke Wyvill was a British chess master and Member of Parliament in the 1800s. He was runner-up in what is considered the first major international chess tournament, London 1851, but never played in a comparable tournament again. He promoted chess and […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>For chess and science: a cautionary tale about decision models</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2018/10/18/london-calling/marmadukewyvill/" rel="attachment wp-att-15340"><img alt="" class="alignright wp-image-15340" height="176" src="https://rjlipton.files.wordpress.com/2018/10/marmadukewyvill.jpg?w=135&amp;h=176" width="135"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Clarke Chronicler blog <a href="http://clarkechroniclerspoliticians.blogspot.com/2013/05/150-marmaduke-wyvill.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Marmaduke Wyvill was a British chess master and Member of Parliament in the 1800s. He was runner-up in what is considered the first major international chess tournament, London 1851, but never played in a comparable tournament again. He promoted chess and helped organize and sponsor the great London 1883 chess tournament. Here is a <a href="https://www.thenorthernecho.co.uk/history/8271561.To_the_manor_pawn_for_chess_playing_MP/">fount</a> of information on the name and the man, including that he once proposed marriage to Florence Nightingale, who became a pioneer of statistics.</p>
<p>
Today we use Wyvill’s London 1883 tournament to critique statistical models. Our critique extends to ask, <em>how extensively are models cross-checked?</em></p>
<p>
London is about to take center stage again in chess. The World Championship <a href="https://en.wikipedia.org/wiki/World_Chess_Championship_2018">match</a> between the current world champion, Magnus Carlsen of Norway, and his American challenger Fabiano Caruana will begin there on November 9. This is the first time since 1972 that an American will play for the title. The organizer is <a href="https://worldchess.com/">WorldChess</a> (previously <a href="https://en.wikipedia.org/wiki/Agon_Limited">Agon Ltd.</a>) in partnership with the World Chess Federation (<a href="http://fide.com/">FIDE</a>). <span id="more-15339"/></p>
<p>
The London 1883 tournament had two innovations. It was the first to use chess clocks. The second was that in the event of a game being drawn, the players had to play another game, twice if needed. Only after three draws would the point be considered halved, and this happened only seven times in 182 meetings. Chess clocks have been used in virtually every competition since, but the second experiment has never been repeated—the closest to it will come <a href="https://en.chessbase.com/post/norway-chess-armageddon-gambit">next year</a>. Two scientific imports were that the time to decide on a move was regulated and games without critical action were set aside.</p>
<p>
</p><p/><h2> Chess and Science </h2><p/>
<p/><p>
Chess has long been considered a (or “the”) “game of science.” It has been the focus of numerous scientific studies. Here we emphasize how it is a copious source of scientific <em>data</em>. Millions of games—every top-level game and nowadays many games at lower levels—have been preserved in databases. Except that the time taken by players to choose a move at each turn is recorded only sporadically, we have easy access to full information about each player’s choices of moves. </p>
<p>
What we also have now is authoritative judgment on the <em>true values</em> of those choices via analysis by strong computer chess programs. Those programs, called “engines,” can beat even Carlsen and Caruana with regularity, so we humans have no standing to doubt their judgments. The programs’ values for moves are a robust quality metric, and <a href="https://rjlipton.wordpress.com/2016/11/30/when-data-serves-turkey/">correlate</a> <a href="https://rjlipton.wordpress.com/2016/12/08/magnus-and-the-turkey-grinder/">supremely</a> with the <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo</a> <a href="https://fivethirtyeight.com/features/introducing-nfl-elo-ratings/">Rating</a>, which provides a robust skill metric. </p>
<p>
The move values are the only chess-specific input to my statistical choice model. I have <a href="https://rjlipton.wordpress.com/2018/09/07/sliding-scale-problems/">covered</a> <a href="https://rjlipton.wordpress.com/2017/05/23/stopped-watches-and-data-analytics/">it</a> <a href="https://rjlipton.wordpress.com/2016/11/08/unskewing-the-election/">several</a> <a href="https://rjlipton.wordpress.com/2015/10/06/depth-of-satisficing/">times</a> <a href="https://rjlipton.wordpress.com/2012/03/30/when-is-a-law-natural/">before</a>, but not yet in the sense of going “back to square one” to say how it originated—where it fits among decision models. </p>
<p>
This year I have overhauled the model’s 28,000+ lines of C++ code. More exactly I have “underhauled” it by chopping out stale features, removing assumptions, and simplifying operations. I widened the equations to accommodate multiple alternative models and fitting methods, besides the ones I’ve deployed to judge allegations of cheating on behalf of FIDE and other chess bodies. The main alternative discussed here is one I did already program and reject nine years ago, but having recently tried multiple other possibilities reinforces the points about models that I am making here. So let’s first see one general form of decision model and how the chess application fits the framework.</p>
<p>
</p><p/><h2> The Multinomial Logit Model </h2><p/>
<p/><p>
The general goal is to project the probabilities <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> of certain decision choices or event outcomes <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> in terms of data <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> about the <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> and attributes <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> of the decision makers. An index <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> can refer to multiple actors and/or multiple situations; we will suppress it when intent is clear. The index <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> refers to multiple alternatives <img alt="{m_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_j}"/> (<img alt="{j = 1,\dots,J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj+%3D+1%2C%5Cdots%2CJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j = 1,\dots,J}"/>) in any situation and their probabilities <img alt="{p_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_j}"/>. The goal for any <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> is to infer <img alt="{(p_j) = \{p_{i,j}\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28p_j%29+%3D+%5C%7Bp_%7Bi%2Cj%7D%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(p_j) = \{p_{i,j}\}}"/> as a function <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> of <img alt="{X = \{X_{i,j}\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5C%7BX_%7Bi%2Cj%7D%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X = \{X_{i,j}\}}"/>, <img alt="{S_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_i}"/>, and internal model parameters <img alt="{\vec{\beta}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7B%5Cbeta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{\beta}}"/>. The <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> function is “the” model.</p>
<p>
The models we consider all incorporate into <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> a function that takes <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> and <img alt="{S_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_i}"/> and outputs quantities <img alt="{u_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_{i,j}}"/>—which we will speak of as single numbers but which could be vectors over a separate index <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>. In many settings, <img alt="{u_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_{i,j}}"/> this represents the <em>utility</em> of outcome <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> for the actor or situation <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>, which the actor wants to maximize or at least gain enough of to satisfy needs. Insofar as <img alt="{u_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_{i,j}}"/> depends on <img alt="{S_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_i}"/> it is distinct from a neutral notion of “objective value” <img alt="{v_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_{i,j}}"/>. Such a distinction was already <a href="https://en.wikipedia.org/wiki/Expected_utility_hypothesis#Bernoulli's_formulation">observed</a> in the early 1700s. </p>
<p>
The <a href="https://en.wikipedia.org/wiki/Discrete_choice#Multinomial_choice_without_correlation_among_alternatives">multinomial</a> <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">logit</a> <a href="http://cess.nyu.edu/wp-content/uploads/2018/03/Multinomial-Logit-Processes.pdf">model</a>, and <em>log-linear</em> models in general, represent the logarithms of the probabilities as linear functions of the other elements. Using the utility function this means setting <a name="loglin"/></p><a name="loglin">
<p align="center"><img alt="\displaystyle  \log(p_j) = \alpha + \beta u_j, \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clog%28p_j%29+%3D+%5Calpha+%2B+%5Cbeta+u_j%2C+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \log(p_j) = \alpha + \beta u_j, \ \ \ \ \ (1)"/></p>
</a><p><a name="loglin"/> where we have suppressed <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> and <img alt="{\beta u_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+u_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta u_j}"/> could be multiple linear terms <img alt="{\beta_1 u_{j,1} + \cdots + \beta_k u_{j,k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta_1+u_%7Bj%2C1%7D+%2B+%5Ccdots+%2B+%5Cbeta_k+u_%7Bj%2Ck%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta_1 u_{j,1} + \cdots + \beta_k u_{j,k}}"/>. This makes <a name="exp"/></p><a name="exp">
<p align="center"><img alt="\displaystyle  p_j = e^{\alpha + \beta u_j} \ \ \ \ \ \ \ \ \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_j+%3D+e%5E%7B%5Calpha+%2B+%5Cbeta+u_j%7D+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p_j = e^{\alpha + \beta u_j} \ \ \ \ \ \ \ \ \ \ \ \ \ (2)"/></p>
</a><p><a name="exp"/> for all <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/>. Then <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> becomes a normalization constant to ensure that the probabilities sum to <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, dropping out to give the final equations <a name="softmax"/></p><a name="softmax">
<p align="center"><img alt="\displaystyle  p_j = \frac{e^{\beta u_j}}{\sum_{\ell=1}^J e^{\beta u_\ell}}. \ \ \ \ \ (3)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_j+%3D+%5Cfrac%7Be%5E%7B%5Cbeta+u_j%7D%7D%7B%5Csum_%7B%5Cell%3D1%7D%5EJ+e%5E%7B%5Cbeta+u_%5Cell%7D%7D.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p_j = \frac{e^{\beta u_j}}{\sum_{\ell=1}^J e^{\beta u_\ell}}. \ \ \ \ \ (3)"/></p>
</a><p><a name="softmax"/> Fitting <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> thus yields all the probabilities. Note that putting a difference of probabilities <img alt="{\log(p_1) - \log(p_i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28p_1%29+-+%5Clog%28p_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log(p_1) - \log(p_i)}"/> on the left-hand side of (<a href="https://rjlipton.wordpress.com/feed/#loglin">1</a>), which is the log of the ratio of the probabilities, leads to the same model and normalization (up to the sign of <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/>). The function of normalizing exponentiated quantities is so common it has its own pet name, <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>.</p>
<p>
These last three equations were known already in 1883 via the physicists Josiah Gibbs and Ludwig Boltzmann, with <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> coming out in units of inverse temperature, the denominator of (<a href="https://rjlipton.wordpress.com/feed/#softmax">3</a>) representing the <a href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)">partition function</a> of a physical system, and the numerator the Boltzmann <a href="https://en.wikipedia.org/wiki/Boltzmann_distribution">factor</a>. It seems curious that apart from some contemporary references by Charles Peirce they were not used in wider contexts until the World War II era. Equation (<a href="https://rjlipton.wordpress.com/feed/#softmax">3</a>) essentially appears as equation (1) in the 2000 Economics Nobel <a href="https://pubs.aeaweb.org/doi/pdfplus/10.1257/aer.91.3.351">lecture</a> by Daniel McFadden, who calls it “the” multinomial logit model (see also <a href="https://www.jstor.org/stable/3440992?seq=1#page_scan_tab_contents">this</a>) and traces it to work by Duncan Luce in 1959. Such pan-scientific heft makes its failure in chess all the more surprising.</p>
<p>
</p><p/><h2> The Chess Case </h2><p/>
<p/><p>
In chess tournaments we have multiple players, but only one is involved in deciding each move. So we focus on one player but can treat multiple players as a group. Instead what we represent with the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> index is the player(s) facing multiple positions. To a large extent we can treat those decisions as independent. Even if the player executes a plan over a few moves the covariance is still <em>sparse</em>, and often players realize they have to revise their plans on the next turn. Thus we replace <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> by an index <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> signifying “turn” or “time” for each position faced by the player. Clearly we want to fit by regression over multiple turns <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, so <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> and any other fitted parameters will not depend on <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, which again we sometimes suppress.</p>
<p>
Each possible move <img alt="{m_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_j}"/> at each turn <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is given a value <img alt="{v_{t,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_%7Bt%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_{t,j}}"/> by the chess engine(s) used to analyze the games. We order the moves by those values, so the engine’s first-listed move <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> has the optimal value <img alt="{v_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1}"/>. In just over 90% of positions there is a unique optimal move. There are four salient ways to define the utility <img alt="{u_j = u_{t,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_j+%3D+u_%7Bt%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_j = u_{t,j}}"/> from these values—prefatory to involving model parameters describing the player:</p>
<ul>
<li>
(a) Equate <img alt="{u_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_j}"/> with the move’s value <img alt="{v_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_j}"/>. <p/>
</li><li>
(b) Equate it with the win expectation <img alt="{e_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_j}"/> corresponding to <img alt="{v_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_j}"/>, which I described along with its “sliding-scale” issues in this recent <a href="https://rjlipton.wordpress.com/2018/09/07/sliding-scale-problems/">post</a>. <p/>
</li><li>
(c) Use the loss in value <img alt="{v_1 - v_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1+-+v_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1 - v_j}"/> compared to an optimal move. <p/>
</li><li>
(d) Use the loss in expectation <img alt="{e_1 - e_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_1+-+e_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_1 - e_j}"/> instead.
</li></ul>
<p>
Option (d) automatically scales down differences in positions where one side is significantly ahead. The same small slip that would halve one’s chances in a balanced position might only reduce 90% to 85% in a strong position or be nearly irrelevant in a losing one. I remove the most extremely unbalanced positions from samples anyway. For (c) I use a “non-sliding” scale function <img alt="{\delta(v_1,v_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%28v_1%2Cv_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta(v_1,v_j)}"/> whose efficacy I detailed <a href="https://rjlipton.wordpress.com/2016/11/30/when-data-serves-turkey/">here</a> but I can easily generate results without it. Note that if I were to cut the sample down only to balanced positions—<img alt="{v_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1}"/> near <img alt="{0.00}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.00%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.00}"/>—of which there are a lot, then (a) and (b) become respectively equivalent to (c) and (d) anyway, up to signs which are handled by flipping the sign of <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/>.</p>
<p>
My primary model parameter, called <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> for “sensitivity,” is just a divisor of the values and so gets absorbed by <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/>. I have a second main parameter <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> for “consistency” but more on it later. Having <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> is enough to fill the dictates of the multinomial logit model in the simplest manners. </p>
<p>
Criteria for fitting log-linear models are also a general issue. For linear regressions, least-squares fitting is distinguished by its being equivalent to maximum-likelihood estimation (MLE) under Gaussian error, but <img alt="{L_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_1}"/> or other <img alt="{L_p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_p}"/> distance can be minimized instead. With log-linear regressions the flex is wider and MLE competes with criteria that minimize various discrepancies between quantities projected from the fitted probabilities <img alt="{p_{t,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_%7Bt%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_{t,j}}"/> and their actual values in the sample. Here are four of them—we will see more:</p>
<ol>
<li>
The frequency of playing (i.e., “matching”) the move <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/>. <p/>
</li><li>
The frequency of playing <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> or another move of equal-optimal value (for the 10% of positions that have them). <p/>
</li><li>
The total “loss of Pawns,” whether scaled as <img alt="{\delta(v_1,v_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%28v_1%2Cv_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta(v_1,v_j)}"/> or left unscaled as <img alt="{v_1 - v_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1+-+v_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1 - v_j}"/>. <p/>
</li><li>
The total loss of expectation <img alt="{e_1 - e_\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_1+-+e_%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_1 - e_\ell}"/> between the optimal move and the played move <img alt="{m_\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_\ell}"/>.
</li></ol>
<p>
The reason for the first three in particular is that they create my three main tests for possible cheating, so I want to fit them on my training data (which now encompasses every rating level from Elo 1025 to Elo 2800 in steps of 25) to be <em>unbiased estimators</em>. Besides those and MLE—which here means maximizing the projected likelihood of the moves that were observed to be played (or alternately the likelihood of the observed <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/>-match/non-match sequence and various others)—my code allows composing a <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> from myriad components and weighting them ad-lib. Components unused in the fitting become the cross-checks.</p>
<p>
Ideally, we’d like all the fitting criteria to produce similar fits—that is, close sets of fitted values for <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> and other parameters on the same data. Finally, the code implements other modeling equations besides multinomial logit—and we’d like their results to agree too. But let’s first see how multinomial logit performs.</p>
<p>
</p><p/><h2> One Log Bad </h2><p/>
<p/><p>
I analyzed the 168 official played games of London 1883 (one competitor left just after the halfway point), and separately, the 76 rejected draws, using Stockfish 7 to high depth. The former give 10,289 analyzed game turns after applying the extreme-value cutoff and a few others. Using the simple unscaled version (c) of utility and fitting <img alt="{\beta \equiv 1/s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+%5Cequiv+1%2Fs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta \equiv 1/s}"/> according to <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> matching gives these results for the first three fitting criteria:</p>
<pre>          Test Name       ProjVal   Actual   Proj%  Actual%  z-score
          MoveMatch       4871.02  4871.00  47.34%  47.34%  z = -0.00
          EqValueMatch    5228.95  5201.00  50.82%  50.55%  z = -0.73
          ExpectationLoss  259.20   297.34  0.0252  0.0289  z = -10.58
</pre>
<p>
This is actual output from my code, except that to avoid crowding I have elided some columns including the standard deviations on which the <a href="https://en.wikipedia.org/wiki/Standard_score"><img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-scores</a> are based. The <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-scores give a uniform way to judge goodness of fit. The first one is exactly zero because that was the criterion expressly fitted. The fitted model generates a projection for the second one that is higher than what actually happened at London 1883, but only slightly: the <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-score is within one standard deviation. The third, however, is under-projected by more than 10 standard deviations. In absolute terms it doesn’t look so bad—259 is only 13% smaller than 297—but the large <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-score reflects our having a lot of data. Well, there’s large and there’s huge:</p>
<pre>           Test Name       ProjVal   Actual  Proj%  Actual%  z-score
           AvgDifference   1493.46  2780.07  0.145  0.270  z = -60.9638
</pre>
<p>
The projection is only half what it should be. The <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-score is inconceivable.</p>
<p>
For more cross-checks, there are the projected versus actual frequency of playing the <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>-th best move for <img alt="{k &gt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k &gt; 1}"/>. Here is the table for ranks 1–10:</p>
<pre>           Rk  ProjVal  Actual   Proj%  Actual%  z-score
            1  4871.02  4871.00  47.34%  47.34%  z = -0.00
            2  1416.72  1729.00  13.80%  16.85%  z = +9.44
            3  761.84    951.00   7.47%   9.32%  z = +7.33
            4  523.25    593.00   5.19%   5.88%  z = +3.20
            5  401.63    410.00   4.03%   4.11%  z = +0.43
            6  325.30    295.00   3.29%   2.99%  z = -1.73
            7  272.12    247.00   2.77%   2.51%  z = -1.57
            8  232.05    197.00   2.37%   2.01%  z = -2.36
            9  200.88    169.00   2.06%   1.73%  z = -2.30
           10  175.95    104.00   1.81%   1.07%  z = -5.54
</pre>
<p>
The first row was the one fitted. Then the projections are off by three percentage points for <img alt="{m_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_2}"/> and almost two for <img alt="{m_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_3}"/>. For ranks 5–9 they are tantalizingly close but by <img alt="{m_{10}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_%7B10%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_{10}}"/> they have clearly overshot—as they must for the probabilities to add to 1.</p>
<p>
There are yet more cross-checks of even greater importance. They are the frequency with which players make errors of a given range of magnitude: a small slip, a mistake, a serious misstep, a blunder. Those results are too gruesome to show here. Fitting by MLE helps in some places but throws off the <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> fit entirely. </p>
<p>
The huge gaps in these and especially in the “AvgDifference” test (AD for short) rule out any patch to the log-linear model with one <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/>. I have tried adding other linear <img alt="{\beta_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta_k}"/> terms representing features such as a move <img alt="{m_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_j}"/> turning an advantage into a disadvantage (<img alt="{v_1 &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1 &gt; 0}"/> but <img alt="{v_j &lt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_j+%3C+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_j &lt; 0}"/>). They give haywire results unless the nonlinearity described next is introduced.</p>
<p>
</p><p/><h2> Log-Linear With Revised Utility: Still Bad </h2><p/>
<p/><p>
This is to define the utility function using a new parameter <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> as </p>
<p align="center"><img alt="\displaystyle  u_j = \delta(v_1,v_j)^c. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++u_j+%3D+%5Cdelta%28v_1%2Cv_j%29%5Ec.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  u_j = \delta(v_1,v_j)^c. "/></p>
<p>Without scaling this is just <img alt="{(v_1 - v_j)^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28v_1+-+v_j%29%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(v_1 - v_j)^c}"/>; one can also use <img alt="{(e_1 - e_j)^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28e_1+-+e_j%29%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(e_1 - e_j)^c}"/> or put the power on the values separately. In forming <img alt="{\beta u_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+u_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta u_j}"/> it does not matter whether the fitted value is represented as <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> or as <img alt="{\beta^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta^c}"/>. Using the notation <img alt="{\beta = \frac{1}{s}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+%3D+%5Cfrac%7B1%7D%7Bs%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta = \frac{1}{s}}"/>, so that <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> divides out the “pawn units” of <img alt="{v_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1}"/> and <img alt="{v_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_j}"/>, this means that without loss of generality we can write </p>
<p align="center"><img alt="\displaystyle  u_j = \left(\frac{\delta(v_1,v_j)}{s}\right)^c. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++u_j+%3D+%5Cleft%28%5Cfrac%7B%5Cdelta%28v_1%2Cv_j%29%7D%7Bs%7D%5Cright%29%5Ec.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  u_j = \left(\frac{\delta(v_1,v_j)}{s}\right)^c. "/></p>
<p>This makes clear that the quantity being powered is dimensionless. The motivation for <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> is that in any quantity of the form <img alt="{(\delta/s)^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cdelta%2Fs%29%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\delta/s)^c}"/>, the marginal influence of <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> becomes greater for large <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> than that of <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>. Thus <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> can be said to govern the propensity for making large mistakes while <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> governs the perception of small differences <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> in value. Higher <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> and lower <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> correspond to higher skill. The former connotes the ability to navigate tactical minefields, the latter strategic skill of amassing small advantages. </p>
<p>
Thus I regard <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> as natural in chess. In my results, <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> usually fits with values going up from below <img alt="{0.45}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.45%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.45}"/> to about <img alt="{0.55}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.55%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.55}"/> as the Elo level increases. This is in the rough neighborhood of square-root and definitely apart from <img alt="{c=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c=1}"/>. It also changes the calculus on a property called “independence from irrelevant alternatives,” which McFadden cites from Luce but has issues discussed e.g. <a href="http://www.its.caltech.edu/~mshum/ec106/discrete.pdf">here</a>. </p>
<p>
Since <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> is part of the revised utility function, the model is still log-linear in the utility and the probabilities are still obtained via the procedure (<a href="https://rjlipton.wordpress.com/feed/#loglin">1</a>)–(<a href="https://rjlipton.wordpress.com/feed/#softmax">3</a>). The end-product is that having <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> allows fitting two criteria exactly to yield them as unbiased estimators. Here are the results of fitting <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> and AD in what is now a “log-radical(<img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/>)” model:</p>
<pre>           Test Name       ProjVal   Actual   Proj%  Actual%   z-score
           MoveMatch       4870.99  4871.00  47.34%  47.34%  z = +0.00
           AvgDifference   2780.10  2780.07  0.2702  0.2702  z = +0.00
           EqValueMatch    5261.35  5201.00  51.14%  50.55%  z = -1.44
           ExpectationLoss  413.42   297.35  0.0402  0.0289  z = +15.89
</pre>
<p>
The equal-optimal projection remains OK. The expectation loss, however, flips from an under-projection to a vast over-projection. The cross-checks from the move ranks give further bad news:</p>
<pre>           Rk  ProjVal  Actual   Proj% Actual%  z-score
            1  4870.99  4871.00  47.34% 47.34% z =  +0.00
            2  1123.22  1729.00  10.94% 16.85% z = +19.88
            3  633.30   951.00   6.21%  9.32%  z = +13.27
            4  459.83   593.00   4.56%  5.88%  z =  +6.44
            5  370.58   410.00   3.72%  4.11%  z =  +2.11
            6  311.98   295.00   3.16%  2.99%  z =  -0.99
            7  270.56   247.00   2.75%  2.51%  z =  -1.46
            8  239.36   197.00   2.44%  2.01%  z =  -2.79
            9  214.30   169.00   2.19%  1.73%  z =  -3.15
           10  193.93   104.00   1.99%  1.07%  z =  -6.57
</pre>
<p>
The discrepancy in the second-best move has doubled to <em>six</em> percentage points while the third-best move is off by more than three. </p>
<p>
Maximum-likelihood fitting makes the gaps even worse. No re-jiggering of fitting methods nor the formula for <img alt="{u_{t,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_%7Bt%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_{t,j}}"/> comes anywhere close to coherence. Inconsistency in the second-best move kills everything. The fault must be tied all the way to the log-linear model for the probabilities.</p>
<p>
</p><p/><h2> Two Logs Good </h2><p/>
<p/><p>
As we have noted, taking the difference of logs, and inverting so that signs stay positive like so: </p>
<p align="center"><img alt="\displaystyle  \log(1/p_j) - \log(1/p_1) = \alpha + \beta u_j, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clog%281%2Fp_j%29+-+%5Clog%281%2Fp_1%29+%3D+%5Calpha+%2B+%5Cbeta+u_j%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \log(1/p_j) - \log(1/p_1) = \alpha + \beta u_j, "/></p>
<p>does not change the model. The likelihoods <img alt="{e^{\alpha + \beta u_j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B%5Calpha+%2B+%5Cbeta+u_j%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{\alpha + \beta u_j}}"/> are still normalized arithmetically as in the Gibbs equations. Taking a difference of <em>double</em> logarithms, however, yields something different: </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}  \log\log(1/p_j) - \log\log(1/p_1) &amp;=&amp; \alpha + \beta u_j\\ \implies \frac{\log(1/p_j)}{\log(1/p_1)} &amp;=&amp; \exp(\alpha + \beta u_j)\\ \implies \log(p_j) &amp;=&amp; (\log(p_1))\exp(\alpha + \beta u_j)\\ \implies p_j &amp;=&amp; p_1^{\exp(\alpha + \beta u_j)}. \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Clog%5Clog%281%2Fp_j%29+-+%5Clog%5Clog%281%2Fp_1%29+%26%3D%26+%5Calpha+%2B+%5Cbeta+u_j%5C%5C+%5Cimplies+%5Cfrac%7B%5Clog%281%2Fp_j%29%7D%7B%5Clog%281%2Fp_1%29%7D+%26%3D%26+%5Cexp%28%5Calpha+%2B+%5Cbeta+u_j%29%5C%5C+%5Cimplies+%5Clog%28p_j%29+%26%3D%26+%28%5Clog%28p_1%29%29%5Cexp%28%5Calpha+%2B+%5Cbeta+u_j%29%5C%5C+%5Cimplies+p_j+%26%3D%26+p_1%5E%7B%5Cexp%28%5Calpha+%2B+%5Cbeta+u_j%29%7D.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}  \log\log(1/p_j) - \log\log(1/p_1) &amp;=&amp; \alpha + \beta u_j\\ \implies \frac{\log(1/p_j)}{\log(1/p_1)} &amp;=&amp; \exp(\alpha + \beta u_j)\\ \implies \log(p_j) &amp;=&amp; (\log(p_1))\exp(\alpha + \beta u_j)\\ \implies p_j &amp;=&amp; p_1^{\exp(\alpha + \beta u_j)}. \end{array} "/></p>
<p>With the utility <img alt="{u_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_j}"/> still defined as <img alt="{\delta(v_1,v_j)^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%28v_1%2Cv_j%29%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta(v_1,v_j)^c}"/> this creates a triple stack of exponentials on the right-hand side. This all looks really unnatural, but see the results it gives, now also showing the interval and large-error tests that were “too gruesome” before:</p>
<pre>           Test Name       ProjVal   Actual   Proj%  Actual%  z-score
           MoveMatch       4871.02  4871.00  47.34%  47.34% z = -0.00
           AvgScaledDiff   1142.61  1142.59   0.111   0.111 z = +0.00
           EqValueMatch    5251.90  5201.00  51.04%  50.55% z = -1.10
           ExpectationLoss  333.20   334.46  0.0324  0.0325 z = -0.19

           Rk ProjVal  Sigma    Actual  Proj% Actual%  z-score
            1  4871.02  47.02  4871.00 47.34% 47.34%  z = -0.00
            2  1786.89  37.32  1729.00 17.41% 16.85%  z = -1.55
            3   929.87  28.60   951.00  9.11%  9.32%  z = +0.74
            4   589.93  23.29   593.00  5.85%  5.88%  z = +0.13
            5   419.35  19.84   410.00  4.21%  4.11%  z = -0.47
            6   315.24  17.32   295.00  3.19%  2.99%  z = -1.17
            7   246.68  15.39   247.00  2.51%  2.51%  z = +0.02
            8   198.71  13.85   197.00  2.03%  2.01%  z = -0.12
            9   161.54  12.52   169.00  1.65%  1.73%  z = +0.60
           10   134.18  11.43   104.00  1.38%  1.07%  z = -2.64
           11   111.41  10.43    97.00  1.15%  1.00%  z = -1.38
           12    93.90   9.59    99.00  0.97%  1.02%  z = +0.53
           13    77.94   8.75    76.00  0.81%  0.79%  z = -0.22
           14    65.40   8.02    78.00  0.68%  0.82%  z = +1.57
           15    55.13   7.37    62.00  0.58%  0.65%  z = +0.93

           Selec. Test ProjVal  Actual  Proj%  Actual%  z-score
           Delta01-10   656.08  645.00  6.38%  6.27%  z = -0.56
           Delta11-30   800.75  824.00  7.78%  8.01%  z = +1.02
           Delta31-70   596.51  607.00  5.80%  5.90%  z = +0.50
           Delt71-150   295.70  290.00  2.87%  2.82%  z = -0.38
           Error&gt;=050   709.46  675.00  6.90%  6.56%  z = -1.55
           Error&gt;=100   331.88  300.00  3.23%  2.92%  z = -2.02
           Error&gt;=200   141.56  114.00  1.38%  1.11%  z = -2.62
           Error&gt;=400    68.76   35.00  0.67%  0.34%  z = -4.61
</pre>
<p>
Only the first two lines have been fitted. The other lines follow like obedient ducks—and this persists through all tournaments that I have run. </p>
<p>
There are some wobbles that also persist: The second-best move is somewhat over-projected and the third-best move slightly under—but the remaining indices are off by small amounts whose signs seem random. So are the interval tests at the end, except that large errors are over-projected. The match to moves of equal-optimal worth tends to be over-projected regardless of the patch described <a href="https://rjlipton.wordpress.com/2012/03/30/when-is-a-law-natural/">here</a>. Nevertheless, the overall fidelity under so much cross-validation is an amazing change from the log-linear cases.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>
The most particular issue I see grants that the original log-linear formulation could be <i>fine</i> for a one-shot purpose, say if the match-to-<img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> cheating test were the only thing cared about.  The concern is that in the absence of validation beyond what is needed for that, “mission creep” could extend the usage unknowingly into flawed territory.  It is important to me that a model should score well on a larger slate of pertinent phenomena.  Do other models have as rich a field of data and cross-checks as in chess? </p>
<p>
Is there extensive literature on modeling the <em>double</em> logarithms of probabilities—and on representing probabilities as powers rather than multiples of the “pivot” <img alt="{p_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_1}"/>? We have seen scant references. The term “log-log model” instead refers to having a logarithm on both sides, e.g., <img alt="{\log(p_j) = \alpha + \beta\log(x_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28p_j%29+%3D+%5Calpha+%2B+%5Cbeta%5Clog%28x_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log(p_j) = \alpha + \beta\log(x_j)}"/>. Alternatives to log-linear models need to be more conscious of error terms in the utility functions, so perhaps uncertainty needs a more express representation in my formulas.</p>
<p>
The form where <img alt="{p_j = p_1^{L_j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_j+%3D+p_1%5E%7BL_j%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_j = p_1^{L_j}}"/> does have the general issue that when <img alt="{p_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_1}"/> should be very close to 1—as for a completely obvious move in chess—there is strain on getting the exponents <img alt="{L_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_j}"/> large enough to make <img alt="{p_j = p_1^{L_j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_j+%3D+p_1%5E%7BL_j%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_j = p_1^{L_j}}"/> tiny. The over-projection of large errors (<img alt="{p_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_j}"/> too high) is a symptom of this. Some of <a href="https://rjlipton.wordpress.com/2015/10/06/depth-of-satisficing/">my</a> <a href="https://rjlipton.wordpress.com/2016/11/08/unskewing-the-election/">past</a> <a href="https://rjlipton.wordpress.com/2017/05/23/stopped-watches-and-data-analytics/">posts</a> give my thinking on this, but the implementations have been hard to control, so I would be grateful to hear reader thoughts.</p>
<p>
[some name and word fixes]</p></font></font></div>
    </content>
    <updated>2018-10-18T23:29:55Z</updated>
    <published>2018-10-18T23:29:55Z</published>
    <category term="chess"/>
    <category term="detection"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="cross-validation"/>
    <category term="Daniel McFadden"/>
    <category term="decision theory"/>
    <category term="discrete choice theory"/>
    <category term="Marmaduke Wyvill"/>
    <category term="models"/>
    <category term="multinomial logit model"/>
    <category term="statistics"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2018-12-17T05:29:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://learningwitherrors.org/2017/01/03/discrepancy-constructive-rw</id>
    <link href="http://learningwitherrors.org/2017/01/03/discrepancy-constructive-rw/" rel="alternate" type="text/html"/>
    <title>Discrepancy: a constructive proof via random walks in the hypercube</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div style="display: none;"/>
<p>
In this post, I'll give two constructive/algorithmic discrepancy upper bounds. The first, by Beck and Fiala, applies to sparse set systems. The second, by Lovett and Meka, improves on the Beck-Fiala result and also matches the guarantees of Spencer's theorem.
</p><p>
<!--more-->
</p><h3 class="tex">Discrepancy Minimization</h3> Recall that, given a system of subsets of $[n]$, $\cS = S_1,\ldots,S_m \subseteq [n]$, the discrepancy of a coloring $x \in \{\pm 1\}^n$ on $\cS$ is defined to be \[ \disc(x,\cS) = \max_{S_j \in \cS} \left|\sum_{i \in S_j} x_i\right|. \] In the previous post, we proved Spencer's theorem, which says that for any $\cS$, $\min_{x}\disc(x,\cS) \le O(\sqrt{n\log\frac{m}{n}})$.
<p>
The natural associated algorithmic task is <em>discrepancy minimization</em>---given $\cS$, we want to compute \[ x^* = \argmin_{x \in \{\pm 1\}^n} \disc(x,\cS). \] Spencer's theorem guarantees that some $x$ achieving $\disc(x,\cS) \le O(\sqrt{n\log\frac{m}{n}})$ always exists, but the proof does not provide a natural algorithm for finding the discrepancy minimizer $x^*$. Actually, finding the minimizer $x^*$ is NP-hard.
</p><blockquote><b>Theorem 1 (Charikar-Newman-Nikolov)</b> <em> Given a set system $\cS$ with $O(n)$ sets, it is NP-hard to distinguish whether $\disc(\cS) = 0$ or $\disc(\cS) = \Omega(\sqrt{n})$. </em></blockquote>

<p>


</p><p>
Still, it turns out that Spencer's theorem <em>can</em> be made algorithmic---there are efficient algorithms for computing a coloring with discrepancy $O(\sqrt{n})$. The first such algorithm was given by Bansal in 2010, and it was based on semidefinite programming. Later, in 2012, Lovett and Meka gave a simplified and slightly more general version of Bansal's result. The Lovett-Meka algorithm uses some ideas from Bansal's algorithm, but it does not rely on SDPs, using instead only linear algebra and properties of random vectors.
</p><p>
I think it is more natural to see the Lovett-Meka result after seeing the simpler result of Beck and Fiala for the special case when $\cS$ is sparse, and so I will give a brief account of that algorithm first.
</p><p>
</p><h3 class="tex">Sparse set systems and Beck-Fiala</h3>
<p>
Suppose that we have a set system $\cS$ which is <em>sparse</em>, so that every item is in at most $t$ sets. In this case, we can get the following specialized bound:
</p><blockquote><b>Theorem 2 (Beck-Fiala)</b> <em> If $\cS = S_1,\ldots,S_m$ is a set system with $S_j \subseteq [n] ~ \forall j \in [m]$, and each $i\in[n]$ is only included it at most $t$ sets of $\cS$, then there is an algorithm that computes a coloring $x \in \{\pm 1\}^n$ with \[ 	\disc(x,\cS) \le 2t - 1. \] </em></blockquote>

<p>

 Beck and Fiala also conjectured that one could obtain a bound of $\disc(\cS) \le O(\sqrt{t})$ for this setting---the Beck-Fiala conjecture is a major open problem in discrepancy theory. <em>Proof:</em>  The proof is algorithmic---we'll start with the fractional coloring $x_0 = \vec{0}$, and update $x$ iteratively until we reach an integral point in $\{\pm 1\}^n$, arguing that we cannot do too much damage along the way.
</p><p>
 The algorithm is as follows. At step $k$ of the algorithm, say we have the fractional coloring $x_k \in [-1,1]^n$. We keep track of the “live” items, or items for which $|x_i| &lt; 1$. We also keep track of the “dangerous” sets: a set is called dangerous if it contain more than $t$ live items.
</p><blockquote><b>Claim 1</b> <em> 	At step $k$, if there are $n_k$ live items, then there can be at most $n_k - 1$ dangerous sets. </em></blockquote>

<p>

 This is true because each dangerous set has at least $t+1$ live items, but the maximum degree of each item is $t$, and so if we restrict the incidence matrix $A$ to the rows corresponding to dangerous sets, there are at most $n_k\cdot t$ nonzero entries, and therefore there can be at most $\lfloor\frac{t\cdot n_k}{t+1}\rfloor \le n_k-1$ dangerous sets.
</p><p>
 So, if we let $A_k$ be the restriction of the incidence matrix to live columns and dangerous rows in the $k$th step, $A_k$ is not full rank, so there must always exist some vector $y_k \in \R^{n_k}$ which is orthogonal to all rows of $A_k$, and furthermore we can find $y_k$ efficiently.
</p><p>
 Let $z_k$ be the natural extension of $y_k$ to the space of non-live items (so that $z_k(i) = y_k(i)$ if $i$ is live and $0$ otherwise). We perform the update \[ 	x_{k+1} = x_k + \alpha \cdot z_k, \] where $\alpha \in \R_+$ is chosen to be the largest number so that $x_{k+1} \in [-1,1]^n$. In other words, we start with $\alpha = 0$, and grow $\alpha$ until at least one of the entries of $x_{k+1}$ hits $1$ or $-1$. Thus the number of live items decreases by at least one, and the discrepancy of every dangerous set is $0$.
</p><p>
 Now we only have to argue that once a set $S_j$ is no longer dangerous, its discrepancy can never grow larger than $2t-1$. If $S_j$ stopped being dangerous at step $k'$, $S_j$ had at most $t$ live items in $x_{k'}$ and $\iprod{x_{k'},a_j} = 0$. In the worst case each live $i \in S_j$ can go from $x_{k'}(i) = 1-\epsilon_i$ to $x(i) = -1$, so a bound of $2t$ on the final discrepancy of $S_j$ is easy. To get $2t-1$, we just notice that because the total discrepancy of $S_j$ was $0$ at step $k'$, the sum over the live $i \in S_j$ must be integral, and for live $|x_{k'}(i)| &lt; 1$, so this gives us a lower bound of $\left|\sum \epsilon_i\right| \ge 1$. $$\tag*{$\blacksquare$}$$
</p><p>
</p><h3 class="tex">Constructive Spencer via guided random walks</h3>
<p>
As mentioned above, the first algorithmic proof of Spencer's result was given by Bansal in 2010. The proof was a little bit similar to the Beck-Fiala algorithm, in that it starts with the fractional coloring $x_0 = 0$, and makes updates to $x$ iteratively until hitting some integral coloring, bounding the error incurred along the way. The extreme point of departure is the manner in which the iterative updates to $x$ are chosen. Instead of choosing some arbitrary direction orthogonal to the dangerous sets, Bansal's algorithm uses a semidefinite program to take a random step---the semidefinite program makes sure that this random walk will make progress without violating discrepancy constraints too much. This makes the proof non-constructive, since to argue the feasibility of the SDP, Bansal relied on Spencer's result.
</p><p>
In 2012, Lovett and Meka simplified Bansal's approach. They removed the semidefinite programming step, returning to linear algebraic arguments reminiscent of the Beck-Fiala proof. Instead of using the SDP to guide the random walk, they argue that so long as there are not too many integral vertices, there is a high-dimensional subspace of $\R^n$ in which the random walk can proceed without violating the discrepancy constraints by too much, and then they take a random step in this subspace. This gives a truly constructive proof of Spencer's result.
</p><p>
Ignoring variations in the constants chosen, the main theorem of the paper is the following:
</p><blockquote><b>Theorem 3</b> <em> Suppose that for $\lambda \in \R^m$ with $\lambda \ge 0$, \begin{equation} 	\sum_j \exp\left(-\frac{\lambda_j^2}{32}\right) \le \frac{n}{16}.\label{cond} \end{equation} Then for any starting point $x \in [-1,1]^n$, there exists a partial coloring $x' \in [-1,1]^n$ with at least $n/2$ entries of $x$ having magnitude $1$ and $|\langle x - x', a_j\rangle | \le \lambda_j \sqrt{|S_j|}$ for all $j \in [m]$, and an algorithm that finds such an $x'$ with probability at least $1/10$. </em></blockquote>

<p>


</p><p>
First let's see that this implies Spencer's result. We'll apply the theorem recursively, like Spencer does, $T = O(\log n)$ times. We'll start with the coloring $x_0 = 0$. At the $t$th iteration of the algorithm, say we have $n_t \le n/2^{t-1}$ items uncolored. For all of $j \in [m]$, we will set $\lambda^{(t)}_j = \sqrt{32\log{\frac{m}{n_t}} + \log 16}$ (it's easy to check that this satisfies the condition of the theorem). Then we use the algorithm to find $x_t = x'$. Letting $a_j$ be the 0/1 indicator vector for $S_j$, by the triangle inequality and the guarantees of the theorem, \begin{align*} \disc(x_T, S_j) ~=~ |\langle x_T, a_j\rangle| ~\le~ \sum_{t=0}^T |\langle x_{t} - x_{t+1}, a_j \rangle| ~&amp;\le ~\sum_{t=0} \lambda_j^{(t)}\sqrt{|S_j^{(t)}|}, \end{align*} And since there are at most $n/2^{t-1}$ active items in $S_j$ at timestep $t$, \begin{align*} &amp;\le \sum_{t=0}\sqrt{n_t\log\frac{m}{n_t}} ~\le~ O(\sqrt{n\log m/n}), \end{align*} where the last inequality follows because the sequence $\frac{n_t\log(m/n_t)}{n\log(m/n)}$ decays at least as fast as $2^{-t}\log 2^t$. So, this recovers Spencer's result.
</p><p>

</p><blockquote><b>Remark 1 (Sparse set systems)</b> <em> The algorithms of Bansal and of Lovett and Meka can be generalized to give an upper bound of $O(\sqrt{t}\log n)$ for $t$-sparse set systems. If the $\lambda_j$'s are set to $\lambda_j = c \cdot \sqrt{\frac{t}{|S_j|}}$ for some constant $c$, then by Markov's inequality and by the sparsity of $\cS$ there are at most $2^{-k}n$ sets with $|S_j| \in [2^ktn,2^{k+1}tn]$, and so \[ 	\sum_{j}\exp\left(-\frac{\lambda_j^2}{32}\right) 	\le \sum_{k=0}^{\infty} \frac{n}{2^k}\cdot \exp\left( \frac{-c^2}{2^{k+1}\cdot 32}\right), \] which meets condition (\ref{cond}) of the theorem if $c$ is chosen properly, so the conclusion follows. </em></blockquote>
<p>
Now, we will prove the theorem.
</p><p>
<b>Main idea:</b> Just as Beck and Fiala do, we'll start with some point $x_0$, and update $x$ iteratively, fixing $x(i)$ the moment that $|x(i)| = 1$. We will differ in our updates---we redefine a set to be dangerous when we come close to violating the constraint $|\iprod{x_t, a_j}| \ge \lambda_j\sqrt{|S_j|}$. So unlike Beck-Fiala, by default we start with no dangerous sets, and we add sets to the dangerous list when they become too imbalanced.
</p><p>
Just like Beck-Fiala, we will only make updates orthogonal to the dangerous sets. Our updates will take the form of a random walk in the non-dangerous subspace. The trick will be to argue that by our condition (\ref{cond}) and by properties of Gaussian random walks, with reasonable probability the rank of the dangerous subspace does not become too large as long as there are still many live items to color in.

</p><p>
<em>Proof:</em>  The algorithm is as follows: Set the step size $\gamma = 1/100n^2$, and the safety margin $\delta = \gamma \cdot 10\log n$. Initialize the set of non-live items $D_v = \emptyset $ (notationally this is more convenient than keeping track of live items), and initialize the set of dangerous constraints $D_S$. Initialize the starting coloring $x_0 = x$ and the starting subspace $V_0 = \R^n$.

</p><ol> <li> For $k = 1,\ldots, K= 8/\gamma^2$:

<ol> 	<li> Sample the random vector $g_k$ by sampling $g \sim \cN(0, \Id)$ and projecting $g$ into the subspace $V_k$. 	</li><li> Take a random step by setting $x_k = x_{k-1} + \gamma \cdot g_k$. 	</li><li> For any $i \in [n]$ such that $|x_k(i)| \ge 1 - \delta$, add $i$ to $D_v$. 	</li><li> For any $j \in [m]$ such that $|\langle x_k - x_0, a_j\rangle| \ge \lambda_i\sqrt{|S_j|} - \delta$, add $S_j$ to $D_S$. 	</li><li> Set $V_{k+1}$ to be the subspace orthogonal to all $e_i$ for $i \in D_v$ and orthogonal to all $a_i$ for $S_i \in D_S$.
</li></ol>

 </li><li> If $|x_K(i)| \ge 1-\delta$, set $x'(i) = \sgn(x_K(i))$. Otherwise, set $x'(i) = x_K(i)$.
</li></ol>


<p>
Each of these steps can be done in polynomial time. Now, for proving correctness, there are several concerns: can we always assume $x_k \in [-1,1]^n$ and $|\iprod{x_k,a_j}|\le \lambda_j\sqrt{|S_j|}$, or does step (b) ever make us jump out of the box? Does the rounding in step 5 change the discrepancy of sets by too much? Will the algorithm ever get stuck in a place where we can't make progress (i.e. $V_k = \emptyset$) before coloring at least $n/2$ items?
</p><p>
The first two concerns are easy to take care of, so here are informal arguments. Since the Gaussian steps have small magnitude $\gamma$, and since we have a reasonable safety margin $\delta$ away from violating any constraint, the probability that we ever violate hard constraints in step (b) is polynomially small. The small safety margin also ensures that with high probability, the rounding we perform in step (c) cannot change the discrepancy of any set by more than $n\delta = O(1/\log n)$ over the course of the entire algorithm.
</p><p>
It remains to argue that with probability at least $1/10$, we won't get stuck before we will color at least $n/2$ items. We'll use a couple of (relatively standard) properties of Gaussian projections:
</p><blockquote><b>Claim 2</b> <em><a name="f1"/> 	If $u\in \R^n$ and $g\in \R^n$ is a vector with i.i.d. entries $g_i \sim \cN(0,\sigma^2)$, then $\iprod{g,u}\sim \cN(0,\sigma^2\|u\|_2^2)$. </em></blockquote>

<p>


</p><p>

</p><blockquote><b>Claim 3</b> <em><a name="f2"/> 	If $g\in \R^n$ is a vector with i.i.d. entries $g_i \sim \cN(0,\sigma^2)$, and $g'$ is the orthogonal projection of $g$ into a subspace $S \subseteq \R^n$, then $\E[\|g'\|_2^2] = \sigma^2\cdot \dim(S)$. </em></blockquote>

<p>


</p><p>

</p><blockquote><b>Claim 4</b> <em><a name="f3"/> 	If $u\in \R^n$ and $g\in \R^n$ is a vector with i.i.d. entries $g_i\sim\cN(0,\sigma^2)$, and $g'$ is the orthogonal projection of $g$ into a subspace $S \subseteq \R^n$, then $\iprod{g',u}\sim \cN(0,\alpha\|u\|_2^2)$ where $\alpha \le \sigma^2$. </em></blockquote>

<p>

 The proof of the first claim follows from the additive property of Gaussians. The second and third claims can be proven using the first claim, by considering an orthogonal projection matrix into the subspace $S$.
</p><p>
Now, we are equipped to prove the rest of the theorem. We first relate the progress of the algorithm, as measured by the variance of the Gaussian steps, to the dimension of $V_k$. By the independence of the gaussians $g_k$, have that \begin{align*} \E[\|x_{K} - x_0\|_2^2] ~=~ \E\left[\left\|\gamma \cdot \sum_{k=1}^K g_k\right\|_2^2\right] &amp;= \gamma^2 \cdot \sum_{k=1}^K \E\left[\left\|g_k\right\|_2^2\right]\\ &amp;= \gamma^2 \sum_{k=1}^K \E[\dim(V_k)] \qquad \\ &amp;\ge~ \gamma^2 K \cdot \E[\dim(V_K)], \end{align*} where the second line follows from Claim <a href="http://learningwitherrors.org/f2">2</a> and the last line is because the dimension of $V_k$ decreases with $k$. Since $\dim(V_K) \ge n - |D_v| - |D_S|$, \begin{align*} &amp;\ge 8 \E[n - |D_v| - |D_S|]. \end{align*} On the other hand, $\E[\|x_{K} - x_0\|_2^2] \le 2n$, because we stop moving in the direction of items once the coordinate magnitude is close to $1$. Thus, \begin{align} 2n &amp;\ge 8 (n - \E[|D_S|] - \E[|D_v|],\nonumber\\ \E[|D_v|] &amp;\ge \frac{3}{4}n - \E[|D_S|],\label{dsbd} \end{align}
</p><p>
Now, all that remains for us to do is argue that the expected number of dangerous sets is not too large---if you like, we are arguing that we don't arrive at $V_k = \emptyset$ before coloring in enough vertices. Recall a set $S_j$ is in $D_S$ only if for some $k$, $|\langle x_0 - x_k, a_j\rangle| \ge \lambda_i\sqrt{|S_j|} - \delta$. Since we only move orthogonal to $a_j$ for $S_j \in D_S$, it suffices to count the number of $S_j$ which are dangerous at the final step when $k = K$.
</p><p>
 Let $J \subseteq [m]$ be the set of $j \in [m]$ for which $\lambda_j\sqrt{|S_j|} \ge 2\delta$. By Claim <a href="http://learningwitherrors.org/atom.xml#f2">3</a>, $x_0 - x_K$ is a Gaussian vector supported on $\R^n$ with expected square norm at most $K \cdot \gamma^2 \le 8$, and $a_j$ is a vector of norm $\sqrt{S_j}$. Now, although $x_0-x_K$ is not exactly the orthogonal projection of a Gaussian vector into a subspace of $\R^n$, we can more or less apply Claim <a href="http://learningwitherrors.org/atom.xml#f3">4</a> to<sup><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span class="sidenote" id="footnote1"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a> If we want to be rigorous, we should break up $x_0 - x_K$ into the independent Gaussian increments $x_k - x_{k+1}$ and apply Claim <a href="http://learningwitherrors.org/atom.xml#f3">4</a> to each of them, then look at their sum. </span> conclude that $\langle x_0-x_K, a_j\rangle$ is distributed as a Gaussian with variance at most $8|S_j|$. Therefore for sets $j \in J$, \[ \Pr\left[|\langle x_0 - x_K, a_j\rangle| \ge \lambda_j\sqrt{|S_j|} - \delta\right] \le \Pr\left[|\langle x_0 - x_K, a_j\rangle| \ge \frac{1}{2}\lambda_j\sqrt{|S_j|}\right] \le 2\exp\left(-\frac{\lambda_j^2}{32}\right). \] By condition (\ref{cond}) of the theorem, there are at most $n/16$ sets in $[m]\setminus J$, or sets with $\lambda_j\sqrt{|S_j|} &lt; 2\delta$, since for each such set $\exp(-\lambda_j^2/32) \ge n^{O(1/n^4)} \approx 1$. So the expected number of sets for which $|\langle x_0 - x_K, a_j\rangle| \ge \lambda_j\sqrt{|S_j|} -\delta $ is at most \begin{align*} \E[|D_S|] \le \frac{n}{16} + \sum_{j\in J} \Pr\left[|\langle x_0 - x_K, a_j\rangle| \ge \frac{1}{2}\lambda_j\sqrt{|S_j|} \right] \le \frac{n}{16} + 2\cdot \sum_{j \in J} \exp\left(-\frac{\lambda_i^2}{32}\right) \le \frac{3}{16}n,  \end{align*} where for the last inequality we apply condition (\ref{cond}) of the theorem.
</p><p>
Now, plugging back into (\ref{dsbd}), \begin{align*} \E[|D_v|] \ge \left(\frac{3}{4} - \frac{3}{16}\right)n = \frac{9}{16}n. \end{align*} Let $p$ be the probability that $|D_v|$ has fewer than $n/2$ colored items. Since there can be at most $n$ colored items, \[ \frac{9}{16}n \le \E[|D_v|] &lt; (1-p) \cdot n + p\cdot \frac{n}{2} = \left(1-\frac{p}{2}\right)n. \] From this we have that $p &lt; 7/8$, and so by the union bound the algorithm succeeds with probability at least $1/8 - o(1)$. $$\tag*{$\blacksquare$}$$
</p><p>
</p><h3 class="tex">More sources</h3>
The <a href="https://arxiv.org/abs/1203.5747">paper</a> of Lovett and Meka, as well as the previously mentioned <a href="http://www.win.tue.nl/~nikhil/pubs/author%20-nikhil-2.pdf">chapter</a> of Nikhil Bansal are good resources.
The original algorithmic result can be found in <a href="https://arxiv.org/abs/1002.2259">this</a> paper of Bansal.
<p/></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2017-01-03T00:00:00Z</updated>
    <published>2017-01-03T00:00:00Z</published>
    <author>
      <name>Tselil Schramm</name>
    </author>
    <source>
      <id>http://learningwitherrors.org/atom.xml</id>
      <link href="http://learningwitherrors.org/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="http://learningwitherrors.org" rel="alternate" type="text/html"/>
      <title>Learning With Errors</title>
      <updated>2017-01-04T04:27:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://learningwitherrors.org/2016/12/26/discrepancy-spencer-six</id>
    <link href="http://learningwitherrors.org/2016/12/26/discrepancy-spencer-six/" rel="alternate" type="text/html"/>
    <title>Discrepancy: definitions and Spencer's six standard deviations</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div style="display: none;"/>  This is a first in a series of (probably at least 3) blog posts about discrepancy minimization. There are already many expositions of this topic, and it is not clear that the world needs yet another, but here it is :). In this post, I'll introduce the basic definitions, give one simple upper bound, one simple lower bound, and then prove Spencer's famous (and elegant) “six standard deviations” theorem. I won't focus on mathematical context, but at the end I will give pointers to some resources on the topic.

<!--more-->

<p>
</p><h3 class="tex">Discrepancy</h3>
<p>
Suppose we have $n$ items, and a set of subsets of $[n]$, $\mathcal{S} = S_1,\ldots,S_m$ with $S_j \subseteq [n]$ for all $j \in [m]$. We want to assign each item $i \in [n]$ a sign or “color” $x_i \in \{\pm 1\}$, with the goal that the coloring of each set is as balanced as possible---formally, this is called minimizing the <em>discrepancy</em>. So for a coloring $x \in \{\pm 1\}^n$, the discrepancy or imbalance of $S \subseteq [n]$ is given by \[ \disc(x, S) = \left|\sum_{i \in S} x_i \right|, \] We define the discrepancy of $\cS$ to be the discrepancy of the worst set in $\cS$ under the most balanced coloring, \[ \disc(\cS) = \min_{x \in \{\pm 1\}^n} \max_{S_j \in \cS}~ \disc(x,S_j). \]
</p><p>
It can also be convenient to think of this as a matrix problem. Consider the incidence matrix $A$ of $\cS$, the $m \times n$ matrix with \[ A_{ji} = \begin{cases} 1 &amp; i \in S_j\\ 		 0 &amp; \text{otherwise}. 		 \end{cases} \] Then letting $a_j$ be the 0/1 indicator vector for set $S_j$, or the $j$th row of $A$, the discrepancy of $x \in \{\pm 1\}^n$ on $S_j$ is equal to $|\iprod{a_j,x}|$, and the discrepancy of $\cS$ is equivalent to \[ \disc(\cS) = \min_{x \in \{\pm 1\}^n} \|Ax\|_{\infty}. \]
</p><p>
</p><h3 class="tex">Some cursory upper and lower bounds</h3> A priori, it might not be clear what $\disc(\cS)$ should be. Some set systems have discrepancy zero---consider for example a “planted” set system, in which the rows of the incidence matrix $A$ are chosen to be orthogonal to some coloring $x\in \{\pm 1\}^n$. On the other hand, naively we could have set systems with discrepancy as large as $n$.
<p>
</p><h3 class="tex">Uniformly Random Coloring</h3> It is not very hard to see that $n$ is an egregious upper bound (when $m = |\cS|$ is not too large). Let's consider $x \in \{\pm\}^n$ chosen uniformly at random. For each $S_j \in \cS$, by Hoeffding's inequality, we have that \[ \Pr\left[\left|\sum_{i \in S_j} x_i \right| \ge t \sqrt{n} \right] \le 2\exp\left(-\frac{t^2}{2}\right). \] So, if we set $t = 2\sqrt{\log m})$, we can beat the union bound over the sets: \[ \Pr[\|Ax\|_{\infty} \ge t\sqrt{n}] \le \sum_{j \in [m]} \Pr\left[\left|\sum_{i \in S_j} x_i \right| \ge t\sqrt{n} \right] \le m \cdot 2\exp(-4 \log m) = O\left(\frac{1}{m^3}\right). \] So for any set system $\cS$, the random coloring gives the improved bound, \begin{equation} \disc(\cS) \le O(\sqrt{n\log m}).\label{randomub} \end{equation}
<p>
</p><h3 class="tex">A Lower Bound</h3> We'll see that the upper bound we got from the random coloring is almost tight. Our lower bound will come from the set system defined by the Hadamard matrix $H$---for us, it will be enough to know that the Hadamard matrix is a symmetric matrix with entries in $\{\pm 1\}$, the first column (and therefore row) of $H$ is the all-1's vector $\vec{1}$, and the columns are mutually orthogonal so that $HH^{\top} = n\cdot \Id$.
<p>
The basic idea for the lower bound is that, because $H$ has large eigenvalues, it cannot map any Boolean vector into a ball of small radius, giving us large discrepancy. But $H$ has negative entries, so in the process of translating it into a valid incidence matrix with entries in $\{0,1\}$ we have to make sure we didn't introduce a small eigenvalue in the direction of some $x \in \{\pm 1\}^n$.
</p><p>
The proof is only a couple of lines. Let $J$ be the all-$1$s matrix. We can define the set system $\cS$ that has an incidence matrix $A = \frac{1}{2}(J+H)$---this is valid, because $A$ is a $0/1$ matrix. Now, for any $x \in \{\pm 1\}^n$, we have that \begin{align*} n \cdot \|Ax\|_{\infty}^2 ~\ge~\|Ax\|_2^2 ~=~ x^{\top}A^{\top} A x ~=~\frac{1}{4} x^{\top} \left(H^{\top} H + J^{\top} J + J^{\top} H + H^{\top} J\right)x. \end{align*} To simplify, we can observe that $H^{\top}H = n\cdot \Id$, that $J^{\top} J = n\cdot J$. Also, because the first row/column of $H$ is equal to $\vec{1}$, and because the rows of $H$ are orthogonal, $H\vec{1} = n \cdot e_1$, and so $JH^{\top} = n \cdot \vec{1}e_1^{\top}$. So, \begin{align*} x^{\top} A^{\top}Ax ~=~ \frac{n}{4} \cdot x^{\top}\left( \Id + J + e_1 \vec{1}^{\top} + \vec{1}e_1^{\top}\right) x ~=~ \frac{n}{4}\left(n + \iprod{x,\vec{1}}^2 + 2\cdot x_1 \cdot \iprod{x,\vec{1}}\right) \end{align*} Because $A$'s first row is $\vec{1}$ we have that $|\iprod{x,\vec{1}}| &gt; \sqrt{n} \implies \|Ax\|_{\infty} \ge \sqrt{n}$. And otherwise if $|\iprod{x,\vec{1}}| &lt; \sqrt{n}$, plugging in to the above we have that \begin{align*} n\cdot \|Ax\|_{\infty}^2 ~\ge~ x^{\top} A^{\top} A x ~&gt;~ \frac{n}{4}\left(n - 2\sqrt{n}\right) \quad \implies \quad \|Ax\|_{\infty} \ge O(\sqrt{n}). \end{align*} So for the Hadamard set system $\cS$, \begin{equation} \disc(\cS) \ge O(\sqrt{n}).\label{lb} \end{equation}
</p><p>
</p><h3 class="tex">Spencer's Theorem</h3>
<p>
It turns out that in fact the lower bound from (\ref{lb}) is tight up to constant factors, and we can get rid of the logarithmic factor from the upper bound (\ref{randomub}). In the 1980's, Spencer proved the following result:
</p><blockquote><b>Theorem 1</b> <em> For any set system $\cS$ on $[n]$ with $|\cS| = m$, \[ 	\disc(\cS) \le 6\sqrt{n\log\frac{m}{n}} \] </em></blockquote>

<p>

 Maybe improving on the logarithmic factor in (\ref{randomub}) does not seem like a big deal, but a priori it is not obvious that one should be able to get a bound better than the random assignment. Also, Spencer's proof is extremely elegant (though nonconstructive). We'll prove it below, but we'll assume that $m = n$, and we'll be sloppy with constants (so we'll get an upper bound of $O(\sqrt{n})$ instead of $6\sqrt{n}$).
</p><p>

</p><p>
<b>The gist.</b> The proof is by induction---given $\cS$ and $[n]$, Spencer shows that there exists some <em>partial coloring</em> $y \in \{-1,0,1\}^n$ with at least a constant fraction of the elements colored in and low discrepancy, so that $\|y\|_1 \ge c\cdot n$ and $\|Ay\|_{\infty} \le C\sqrt{n}$ for constants $c,C$. Then this fact is applied inductively for $\log n$ steps, until all of the items are colored, and the total discrepancy is $\sum_{t=0}^{\log n} C\cdot \sqrt{c^t\cdot n} = O(\sqrt{n})$, since $c &lt; 1$.
</p><p>
The reason such a partial coloring $y$ must exist is because the map $Ax$ is not spread out enough---by the pigeonhole principle, one can show that there are at least $2^{\Omega(n)}$ distinct points $x \in \{\pm 1\}^n$ that get mapped to a ball of radius $O(\sqrt{n})$. Since there are so many points, there must exist $x_1,x_2$ in this ball that have large Hamming distance, so that their difference has many nonzero entries, and so the partial coloring is given by $y = \frac{1}{2}(x_1-x_2)$.
</p><p>
Now for the more formal proof. We will prove the following lemma, which we will then apply inductively:
</p><blockquote><b>Lemma 2</b> <em> Let $m,n \in \N$ with $n \le m$, and let $A$ be an $m \times n$ matrix with $0/1$ entries. Then there exist universal constants $c_1,c_2$ such that there always exists a vector $y \in \{-1,0,1\}^n$ so that $\|y\|_1 \ge c_1 \cdot n$ and \[ \|Ay\|_{\infty} \le c_2\cdot \sqrt{n\log\frac{m}{n}}. \] </em></blockquote>

<p>


</p><p>
<em>Proof:</em>  Let $\cB_{\infty}(r,p)$ denote the ball of radius $r$ around $p$, where distance is measured in $\ell_\infty$. We'll show that there must exist some point $q \in \R^m$ such that \[ 	\Pr_{x\sim\{\pm 1\}^n}[ Ax \in \cB_{\infty}(r,q)] \ge 2^{-cn}, \] for some constant $c$. Our strategy will be to use the pigeonhole principle. We'll identify a set $B \subset \R^m$ with $|B| \le 2^{\epsilon n}$, so that for uniformly chosen $x \in \{\pm 1\}^n$, there exists a point in $B$ close to $Ax$ with probability at least $\frac{1}{2}$; because $|B| &lt; 2^{\epsilon n}$, there must be some $q \in B$ which is near at least $2^{(1-\epsilon)n}$. To find such a $B$, which contains few points but is close to $Ax$ with constant probability over uniform $x \in \{\pm 1\}^n$, we'll choose $B$ to be a discretization of the set of points in $\R^m$ that do not have too many entries of large magnitude. The way that we define “large magnitude” will partially depend on the standard deviation of entries of $Ax$, and partly on wanting to keep $B$ relatively small.
</p><p>
 Define the function $f:\{\pm 1\}^n \to \Z^m$ so that \[ 	f(x) = \left\lceil \frac{1}{\sqrt {2n\log \frac{m}{n}}} Ax\right\rceil. \] In words, $f$ maps $x$ to the integral point closest to $Ax/\sqrt{n\log \frac{m}{n}}$.
</p><p>
We next identify some small subset $B \subset \Z^m$ which contains a large fraction of the range of $f$, which will imply that there must exist some $q \in \sqrt{n\log\frac{m}{n}}\cdot B$ which is close to $Ax$ for many $x \in \{\pm 1\}^n$. Define $B \subset \Z^m$ to be the set for which at most a $\kappa_t = 2^{t+2} (m/n)^{-t^2}$-fraction of coordinates are larger than $t$, \[ 	B = \left\{(b_1,\ldots,b_m) \in \Z^m ~|~ |\{b_i ~s.t.~ |b_i| \ge t\}| \le \kappa_t \cdot m\right\}. \] For uniformly chosen $x$, by Hoeffding's inequality, \[ 	\Pr\left[ |\iprod{x,a_j}| \ge t \sqrt{2n\log\frac{m}{n}} \right]\le 2\left(m/n\right)^{-t^2} \] And so the expected number of $j \in [m]$ for which $|\iprod{x,a_j}| \ge t\sqrt{n\log\frac{m}{n}}$ is at most \[ 	\E\left[\sum_{j\in[m]} \Ind\left(\left|\iprod{x,a_j}\right| \ge t\sqrt{2n\log\frac{m}{n}}\right) \right] \le m \cdot 2\left(\frac{m}{n}\right)^{-t^2}, \] And by Markov's inequality \begin{align} 	\Pr\left[\sum_{j\in[m]} \Ind\left(\left|\iprod{x,a_j}\right| \ge t\sqrt{2n\log\frac{m}{n}}\right) \ge 2^{t+1} m \cdot \left(\frac{m}{n}\right)^{-t^2}\right] 	&amp;\le \frac{1}{2^{t+1}}.\label{eq:cond} \end{align} Recall that $\kappa_t = 2^{t+1} \left(\frac{m}{n}\right)^{-t^2}$. Thus, for any $x$, the probability that $f(x) = \lceil (n\log\frac{m}{n})^{-1/2} \cdot Ax\rceil \not\in B$ is the sum over (\ref{eq:cond}) for all $t \le \sqrt{n}$, and so by a union bound, \begin{align*} 	\Pr[f(x) \not \in B] 	~=~ \Pr\left[\exists t ~s.t.~ \sum_{j \in [m]} \Ind\left(|\iprod{x,a_j}| \ge t\sqrt{n\log\frac{m}{n}}\right) \ge \kappa_t \cdot m\right] 	~\le~ \sum_{t=1}^{\sqrt{n}} \frac{1}{2^{t+1}} 	~\le~ \frac{1}{2}. \end{align*}
</p><p>
 At the same time, the size of $B$ can be bounded with some meticulous but uncomplicated counting arguments---we won't reproduce them at full resolution here, but the basic idea is that if we consider a point in $B$, it should have at most $\alpha_t \le \kappa_t$ entries of value $\pm t$. So for any valid sequence $\alpha = \alpha_1,\ldots,\alpha_n \le \kappa_1,\ldots,\kappa_n$, we have at most \[ 	\prod_{t=1}^{n}2^{\alpha_t m} \cdot \binom{\left(1 - \sum_{s &lt; t}\alpha_s\right)\cdot m}{\alpha_t \cdot m} \] points, and then summing over all valid $\alpha$, \begin{align*} 	|B| 	&amp;\le \sum_{\alpha} 	\prod_{t=1}^{n}2^{\alpha_t m} \cdot \binom{\left(1 - \sum_{s &lt; t}\alpha_s\right)\cdot m}{\alpha_t \cdot m}. \end{align*} After applying a number of rearrangements and approximations, by our choice of $\kappa_t$'s one can conclude that \[ 	|B| \le 2^{cn}, \] for some constant $c &lt; 1$.
</p><p>

</p><p>
 Since $|B| \le 2^{cn}$ but $f(x) \in B$ for at least $2^{n-1}$ points, it follows that there must exist some $q \in B$ such that $f$ maps at least $2^{n(1-c) -1}$ of the $x \in \{\pm 1\}^n$ to $q$. If $f(x) = p/\sqrt{2n\log \frac{m}{n}}$, then by definition $x \in \cB(\sqrt{2n\log \frac{m}{n}}, p)$. So we have that \[ 	\Pr\left[Ax \in \cB_{\infty}\left(\sqrt{2n\log\frac{m}{n}}, \sqrt{2n\log\frac{m}{n}}\cdot q\right)\right] \ge 2^{-cn}. \]
</p><p>
The proof is now complete if we observe that any subset $C \subset \{\pm 1\}^n$ with $|C| \ge 2^{cn}$ must have two points at hamming distance at least $\Omega(n)$. This is by a theorem of Kleitman, but it is not hard to see. The idea is that, if we choose a single point $p \in C$, the number of points around it of Hamming distance at most $2\epsilon n$ is \[ \sum_{k=1}^{cn} \binom{n}{k} \le 2^{H(\epsilon)n}, \] where $H(\cdot)$ is the binary entropy function (this is a standard upper bound for a partial sum of binary coefficients). So if $|C| \ge 2^{ H(\epsilon)\cdot n}$, it must contain at least two points of Hamming distance at least $2\epsilon n$.
</p><p>
Since there are at least $2^{(1-c)n}$ points in $\{\pm 1\}^n$ so that $Ax \in \cB_{\infty}(\sqrt{n},q)$, there must be two points $x_1,x_2$ such that $\|x_1-x_2\|_1 \ge 2H^{-1}(1-c)\cdot n$ and $\|Ax_1 - Ax_2\|_{\infty} \le \|Ax_1 - q\|_{\infty} + \|q - Ax_2\|_{\infty} \le 2\sqrt{n}$. Setting $y = \frac{1}{2}(x_1-x_2)$, $c_1 = H^{-1}(1-c)$ and $c_2 = \sqrt{2}$, the conclusion holds. $$\tag*{$\blacksquare$}$$
</p><p>
Now, we are ready to prove Spencer's Theorem.
</p><p>
<em>Proof:</em>  We will apply our lemma recursively, coloring in a $c_1$-fraction of the remaining items at a time. After each partial coloring, we update $A$ by removing columns corresponding to colored items, so that at the $t$th step, there are at most $c_1^t\cdot n$ columns in $A$. There can be at most $\log n/\log c_1$ rounds of partial coloring, and at the $t$th round we can incur a discrepancy of at most $c_2\sqrt{c_1^t n \log\frac{m}{c_1^t n}}$ in each set. Thus the total discrepancy is at most \begin{align*} 	\disc(\cS) 	&amp;\le \sum_{t=0}^{O(\log n)}c_2\sqrt{c_1^t n \log\frac{m}{c_1^t n}}\\ 	&amp;\le c_2\sqrt{n}\cdot \sum_{t=0}^{\infty}c_1^t\left(\log\frac{m}{n} + t\log \frac{1}{c_1} \right)^{1/2} 	\end{align*}
And since $(x+y)^{1/2} \le x^{1/2} + y^{1/2}$ for $x,y \ge 0$,
\begin{align*}
&amp;\le O\left(\sqrt{n\log\frac{m}{n}}\right)\cdot \sum_{t=0}^{\infty}c_1^{t/2} 	+ O\left(\sqrt{n}\right)\sum_{t=0}^{\infty} c_1^{t/2}\cdot t^{1/2}\\ 	&amp;\le O\left(\sqrt{n\log\frac{m}{n}}\right), \end{align*} and the conclusion follows. $$\tag*{$\blacksquare$}$$
</p><p>
</p><h2 class="tex">More sources</h2> There are many very good expositions of discrepancy results.
For this post I heavily relied on:

<ul> <li> Joel Spencer's 1985 <a href="http://www.ams.org/journals/tran/1985-289-02/S0002-9947-1985-0784009-0/">paper</a> with the six standard deviations result.
	</li><li> Nikhil Bansal's <a href="http://link.springer.com/chapter/10.1007/978-3-319-04696-9_6">book chapter</a> about algorithmic discrepancy.
	    Full text available <a href="http://www.win.tue.nl/~nikhil/pubs/author%20-nikhil-2.pdf">here</a> at the time of writing.
</li></ul>

 These references contain pointers to other good resources, especially books, which give a more detailed account of the mathematical/historical context of discrepancy minimization.
<p>
Also, see the proof in Alon and Spencer's “The Probabilistic Method” which is based on entropy and is really clean.
</p><p/></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-12-26T00:00:00Z</updated>
    <published>2016-12-26T00:00:00Z</published>
    <author>
      <name>Tselil Schramm</name>
    </author>
    <source>
      <id>http://learningwitherrors.org/atom.xml</id>
      <link href="http://learningwitherrors.org/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="http://learningwitherrors.org" rel="alternate" type="text/html"/>
      <title>Learning With Errors</title>
      <updated>2017-01-04T04:27:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://learningwitherrors.org/2016/08/24/uniform-direct-product</id>
    <link href="http://learningwitherrors.org/2016/08/24/uniform-direct-product/" rel="alternate" type="text/html"/>
    <title>Constructive Hardness Amplification via Uniform Direct Product</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div style="display: none;"/>
<p>
<em>This post was motivated by trying to understand the recent paper “Learning Algorithms from Natural Proofs”, by Carmosino-Impagliazzo-Kabanets-Kolokolova [<a href="http://learningwitherrors.org/atom.xml#ref-CIKK16">CIKK16</a>]. They crucially use the fact that several results in hardness amplification can be made constructive. In this post, we will look at the Uniform Direct Product Theorem of Impagliazzo-Jaiswal-Kabanets-Wigderson [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>]. We will state the original theorem and algorithm of [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>], then we will present a simpler analysis for a (weaker) non-uniform version of their algorithm, which contains some of the main ideas. </em>
</p><p>
For a given function $f: \{0, 1\}^n \to \{0, 1\}^\ell$, say a circuit $C$ “$\eps$-computes $f$” if $C$ computes $f$ correctly on at least $\eps$-fraction of inputs. That is, $\Pr_x[C(x) = f(x)] \geq \epsilon$. We are interested in the following kind of direct product theorem (informally): “If function $f$ cannot be $\eps$-computed by any small circuit $C$, then the direct-product $f^{\ox k}(x_1, x_2, \dots x_k) := (f(x_1), f(x_2), \dots, f(x_k))$ cannot be computed better than roughly $\eps^k$ by any similarly small circuit.” <sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span class="sidenote" id="footnote1"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a>  If this seems trivial, consider the $k=2$ case. We want to show that if $\Pr_x[C(x) = f(x)] \leq \epsilon$ for all small circuits $C$, then $\Pr_{x, y}[C'(x, y) = (f(x), f(y))] \lesssim \eps^2$ for all similarly small circuits $C'$. This is clearly true if the circuit $C'$ operates independently on its inputs, but not as clear otherwise (eg, the correctness of $C'$-s two outputs could be highly correlated). Indeed, proofs of the direct-product theorem take advantage of this correlation.  </span>
</p><p>
This is usually proved<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote2">2</a></sup><span class="sidenote" id="footnote2"><a href="http://learningwitherrors.org/atom.xml#footnote2" name="footnote2">2.</a> See the last section for good references to prior proofs. </span> in contrapositive, by showing: If there exists a circuit $C'$ that $\eps^k$-computes $f^{\ox k}$, then there exists a similarly-sized circuit $C$ that $\eps$-computes $f$. The very interesting part is, this amplification can be made fully constructive, by a simple algorithm.
<!--more-->
</p><blockquote><b>Theorem 1 ([<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>], and Theorem 4.1 [<a href="http://learningwitherrors.org/atom.xml#ref-CIKK16">CIKK16</a>])</b> <em> <a name="thmuniformDP"/> Let $k \in \N, \eps &gt; 0$. There is a (uniform) PPT algorithm $\A$ with the following guarantees:

<ul> <li> <b>Input:</b> A circuit $C'$ that $\eps$-computes $f^{\ox k}$ for some function $f:\{0,1\}^n \to \{0, 1\}^\ell$. </li><li> <b>Output:</b> With probability $\Omega(\eps)$, output a circuit $C$ that $(1-\delta)$-computes $f$.
</li></ul>

 for $\delta = O(\log(1/\eps)/k)$. In particular, $(1-\delta) = \eps^{O(1/k)}$. The circuit $C$ is of size $|C'|\poly(n, k, \log(1/\delta), 1/\eps)$. </em></blockquote>

<p>


</p><p>
Note that we can only hope to construct the good circuit with probability $\Omega(\eps)$, since unique decoding is impossible: the circuit $C'$ may $\eps$-compute up to $(1/\eps)$ different functions $f$ (agreeing with a different function on each $\eps$-fraction of its inputs).
</p><p>
</p><h2 class="tex">1. Uniform Version </h2>
<p>
The algorithm for Theorem <a href="http://learningwitherrors.org/atom.xml#thmuniformDP">1</a> is: </p><div class="framed"> $\mathcal{A}(C')$:
<p>
Input: A circuit $C'$ that $\eps$-computes the direct-product $f^{\ox k}$.

</p><ol> <li> Pick $k$ iid random inputs $x_i \in \{0, 1\}^n$, let $\vec b = (x_1, \dots, x_k)$, and evaluate $C'(\vec b)$. </li><li> Pick a random subset $A \subset \{x_1, \dots, x_k\}$ of size $k/2$. Record $v := C'(\vec b)|_A$ as the answers of $C'$ on the inputs in $A$. </li><li> Output the circuit $C_{A, v}$ defined below (with the values $v$ on the subset $A$ hardcoded).
</li></ol>

 </div>
 <p/><p>
$C_{A, v}$ is defined as the randomized circuit:
 </p><div class="framed"> $C_{A, v}(x)$:
<p>
On input $x \in \{0, 1\}^n$, check if $x \in A$, in which case output $v|_x$ (the hardcoded value of $x$ according to $v$). Otherwise, repeat the following $T = O(\log(1/\delta)/\epsilon)$ times.

</p><ol> <li> Sample $(k/2 - 1)$ additional iid random strings $\{y_j\}$, each $y_j \in \{0, 1\}^n$, and let $\vec b := (x, A, \{y_j\})$ be the tuple of $k$ strings. </li><li> Evaluate $C'(\pi(\vec b))$ for a random permutation $\pi$ of the $k$ inputs. </li><li> If the answers of $C'$ restricted to $A$ agree with the hardcoded values $v$, then output $C'(\pi(\vec b))|_x$, (the answer $C'$ gave for $x$), and stop.
</li></ol>

 Output an error if no output is produced after $T$ iterations. </div>
<p/><p>
<b>Intuition:</b> Suppose the values $v$ returned when the Algorithm queries $C'(b)$ are actually correct. That is, $v|_x = f(x)$ for all $x \in A$. Then, the circuit $C_{A, v}$ evaluates $C'$ on input $\vec b = (b_1, \dots b_k)$, and it knows the correct value of $f(b_i)$ is on half of these coordinates. So, $C_{A, v}(x)$ tries to estimate whether a random point $C'(\vec b)$ is correct or not, based on if it agrees on the known subset of coordinates. The idea is that a value of $C'(\vec b)$ that is wrong on many coordinates is unlikely to pass this test. (See [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>] for the full proof).
</p><p>
Now, in the remainder of this note, we will develop and prove a simpler (weaker) version.
</p><p>
</p><h2 class="tex">2. Symmetrizing </h2> The direct-product as defined above has a permutation symmetry: \[ f^{\ox k}(\pi(x_1, \dots x_k)) = \pi(f^{\ox k}(x_1, \dots x_k)) \] for any permutation $\pi$.
<p>
The algorithm of Theorem <a href="http://learningwitherrors.org/atom.xml#thmuniformDP">1</a> strongly takes advantage of this symmetry (indeed, the algorithm would not work as promised if we omitted the random permutations).<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote3">3</a></sup><span class="sidenote" id="footnote3"><a href="http://learningwitherrors.org/atom.xml#footnote3" name="footnote3">3.</a> Consider a $C'(x_1, \dots x_k)$ that is correct if $x_1$ lies in some $\eps$-density set, and random otherwise. Without the random permutations, $C_{A, v}(x)$ will always evaluate $C'(x, \dots)$, and produce no output for $(1-\epsilon)$-fraction of inputs $x$. </span> To simplify presentation, it helps to define the direct-product $f^k$ as a function over $k$-<b>multisets</b> of inputs, instead of over $k$-tuples of inputs. Following [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>], for the remainder of this note, we will work in the setting of $k$-multisets, and denote the $k$-multiset direct product as $f^k$. That is, $f^k$ takes as input an (unordered) $k$-multiset $B = \{x_1, x_2, \dots, x_k\}$, and returns the $k$-tuple \[f^k(\{x_1, x_2, \dots, x_k\}) := (f(x_1), f(x_2), \dots, f(x_k))\]
</p><p>
We consider the probability measure induced by the uniform measure over tuples. That is, “pick a random $k$-multiset of $U$” means to generate a multiset by picking $k$ iid random elements from the universe $U$, and forming the (unordered) multiset containing them.<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote4">4</a></sup><span class="sidenote" id="footnote4"><a href="http://learningwitherrors.org/atom.xml#footnote4" name="footnote4">4.</a> So for example, for $k=3$ the multiset $\{a, a, a\}$ has lower probability of being drawn than $\{a, a, b\}$ for $a \neq b$. </span>
</p><p>
The notion of $\eps$-computing remains the same:<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote5">5</a></sup><span class="sidenote" id="footnote5"><a href="http://learningwitherrors.org/atom.xml#footnote5" name="footnote5">5.</a> For our purposes, having a randomized circuit that $\eps$-computes $f^{\ox k}$ is essentially equivalent to having a randomized circuit that $\eps$-computes $f^k$. The proofs will extend to randomized circuits, where we say $C$ $\eps$-computes $f$ if $\Pr_{C, x}[C(x) = f(x)] \geq \eps$, taken over randomness of $C$ as well as $x$. </span> A circuit $C'(B)$ $\epsilon$-computes $f^k$ if \[\Pr_{B \sim \text{random $k$-multiset}}[C'(B) = f^k(B)] \geq \epsilon\] Note that $C'$ is allowed to give different answers for the same element in a multiset, e.g. if $C'(\{a, a, a\}) = (y_1, y_2, y_3)$, the $y_i$s may all be distinct -- we don't take advantage of this symmetry.
</p><p>
</p><h2 class="tex">3. Oracle Version </h2> Here we present and prove a simpler version of the algorithm, in the case when we also have access to an oracle for $f$. (This can be seen as a non-uniform version).
<blockquote><b>Theorem 2</b> <em> <a name="thmoracle"/> Let $k \in \N, \eps &gt; 0$, and $f:\{0,1\}^n \to \{0, 1\}^\ell$. There is a PPT algorithm $\A^f$ with oracle access to $f$, with the following guarantees:

<ul> <li> <b>Input:</b> A circuit $C'$ that $\eps$-computes $f^k$. </li><li> <b>Output:</b> With probability $0.99$, output a circuit $C$ that $(1-\delta)$-computes $f$.
</li></ul>

 for $\delta = O(\log(k)/(\eps k))$. The circuit $C$ is of size $|C'|\poly(n, k, \log(1/\delta), 1/\eps)$. </em></blockquote>

<p>


</p><p>
The idea is, in Step 2 of Algorithm $\A$, we can generate the correct values $v$ for the inputs in set $A$, by querying the oracle. That is, we set $v := f(A)$ directly, instead of using our approximate circuit $C'$. In fact, if we have a perfect oracle for $f$ we can simplify the algorithm even further.
</p><p>
The algorithm is: </p><div class="framed"> $\mathcal{A}^f(C')$:
<p>


</p><ol> <li> Pick $T = O(\log(k)/\epsilon)$ random $(k-1)$-multisets $A_1, \dots A_T$, each $A_i$ containing $(k-1)$ random inputs from $\{0, 1\}^n$. </li><li> Query the $f$-oracle, and record the values of $v_{A_i} := \{f(x): x \in A_i\}$ for all sets $A_i$. </li><li> Output the circuit $C_{A, v}$ defined below (with the values $v_{A_i}$ on the subsets $A_i$ hardcoded).
</li></ol>

 </div>
 <p/><p>
$C_{A, v}$ is defined as the circuit:<br/>
 </p><div class="framed"> $C_{A, v}(x)$:
<p>
 For each $i = 1 \dots T = O(\log(k)/\epsilon)$:

</p><ol> <li> Let $B_i := \{x\} \cup A_i$. </li><li> Evaluate $C'(B_i)$. </li><li> If the answers of $C'(B_i)$ restricted to $A_i$ agree with the hardcoded values $v_{A_i} = f(A_i)$, then output $C'(B_i)|_x$, (the answer $C'$ gave for $x$), and stop.
</li></ol>

 Output an error if no output is produced after $T$ iterations. </div>
<p/><p>
<em>Proof of Theorem <a href="http://learningwitherrors.org/atom.xml#thmoracle">2</a>:</em>
</p><p>
<b>Parameters:</b> We will have $\delta = 10000\log(k)/(\epsilon k)$ and $T = 100 \log(k)/ \epsilon$. (Think of aiming for $\delta \approx 1/k$).
</p><p>
We will argue that \begin{equation} \label{eqn:main} \Pr_{\A, C, x}[C_{A, v}(x) \neq f(x)] \leq \delta / 100 \end{equation} Where the probability is over the randomness of algorithm $\A^f$ (random choice of sets $A_i$), and random input $x \in \{0, 1\}^n$. Then, by Markov \[\Pr_{\A}\left[ \Pr_{C, x}[C_{A, v}(x) \neq f(x)] &gt; \delta \right] \leq 1/100\] so the algorithm $\A^f$ will produce a good circuit $C_{A, v}$ except with probability $1/100$.
</p><p>
In the execution of circuit $C_{A, v}(x)$, let us say “iteration $i$ fails” if Step 3 of the circuit at iteration $i$ outputs a wrong answer. That is, iteration $i$ fails if $C'(B_i)$ is correct on the $(k-1)$ values in $A_i = B_i \setminus \{x\}$, but wrong on $x$.
</p><p>
Consider the probability that iteration 1 fails. Notice that the distribution of $(x, A_1, B_1)$ is equivalently generated as:
</p><p>
</p><table align="center"><tbody><tr><td align="center"> $\{(x, A_1, B_1)\}$ </td><td align="center"> $\equiv$ </td><td align="center"> $\{(x, A_1, B_1)\}$ </td></tr><tr><td align="center"> $A_1 \sim$ random $(k-1)$-multiset </td><td align="center"> </td><td align="center"> $B_1 \sim$ random $k$-multiset </td></tr><tr><td align="center"> $x \in \{0, 1\}^n$ </td><td align="center"> </td><td align="center"> $x \in B_1$</td></tr><tr><td align="center"> $B_1 := \{x\} \cup A_1$ </td><td align="center"> </td><td align="center"> $A_1 := B_1 \setminus \{x\}$ </td></tr></tbody></table>
<p>
That is, we can think of first sampling a random $k$-multiset $B_1$, then sampling a random $x \in B_1$. Iteration 1 only returns an output when $C'(B_1)$ has at most $1$ wrong answer (since it checks correctness on the $(k-1)$ values of $A_1$). Thus iteration 1 only fails if the random $x \in B_1$ falls on this $1$ (of $k$) answers. So \begin{equation} \Pr_{x, A_1, B_1}[~\text{Iteration 1 fails}~] \leq \frac{1}{k} \end{equation}
</p><p>
 Now, we just union bound: \begin{align*} \Pr[\text{error}] &amp;= \Pr_{\A, C, x}[C_{A, v}(x) \neq f(x)]\\ &amp;\leq \Pr[\text{no output produced after $T$ iterations, or some iteration fails}]\\ &amp;\leq \Pr[\text{no output produced}] + T \cdot \Pr[\text{Iteration 1 fails}]\\ &amp;\leq \Pr[\text{no output produced}] + \frac{T}{k} \end{align*}
</p><p>
For our choice of $T, \delta$, the second term is $\frac{T}{k} \leq \delta / 200$. We will show the first term is $\leq \delta / 200$ as well, completing the proof.
</p><p>
<b>Produces output w.h.p.</b>
</p><p>
It remains to show that the circuit $C_{A, v}$ produces an output with high probability. In Step 3 of the circuit $C_{A, v}$, notice that if $C'$ is queried on a correct input $B_i$, it will pass the test and output a value.
</p><p>
The idea is: since $C'$ is correct on $\epsilon$-fraction of inputs, if we try $T = \Omega(\log(1/\delta)/\epsilon)$ iid random inputs, we will be sure to hit a correct input, except with probability $O(\delta)$. This doesn't quite work, since the inputs $B_i$ are not iid random (they all contain the input $x$) -- but this dependence is minimal, so it still works out.
</p><p>
Following [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>], it helps to think in term of this bipartite graph. Define $G$ as a biregular bipartite graph between inputs $x \in \{0, 1\}^n$, and $k$-<b>tuples</b><sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote6">6</a></sup><span class="sidenote" id="footnote6"><a href="http://learningwitherrors.org/atom.xml#footnote6" name="footnote6">6.</a> Going back to tuples just to simplify the notation, so we can deal with the uniform measure. </span> $B \in (\{0, 1\}^n)^k$, with an edge $(x, B)$ if $x \in B$. We can think of the circuit $C_{A, v}(x)$ as picking up to $T$ random neighbors of $x$ in the graph $G$, until hitting an input $B$ where $C'(B)$ is correct on all $B \setminus \{x\}$. We know that $\epsilon$-fraction of $k$-tuples $B$ are correct, and in fact we will show that almost all inputs $x$ have close to $\eps$-fraction of their neighbors as correct.
</p><p>
</p><p align="center"><img src="http://learningwitherrors.org/sources/uniform-DP/graph_color.png" width="350"/></p>
<p>

</p><blockquote><b>Lemma 3</b> <em> <a name="lemnotbad"/> There are at most $O(\delta)$-fraction of “$\bad$” inputs $x \in \{0, 1\}^n$ for which \[\Pr_{B \in N(x)}[C'(B) \text{ is correct}] \leq \epsilon/10\] </em></blockquote>

<p>

 This is sufficient to show that $\Pr[\text{no output produced}] \leq O(\delta)$, since for inputs $x$ that are not $\bad$, sampling $T = \Omega(\log(k)/\eps)$ iid neighbors of $x$ will hit a correct neighbor, except with probability $O(1/k) \leq O(\delta)$. <sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote7">7</a></sup><span class="sidenote" id="footnote7"><a href="http://learningwitherrors.org/atom.xml#footnote7" name="footnote7">7.</a>  $(1-\eps/10)^{T} \leq e^{-T\eps / 10} \leq 1/k \leq \delta$.  </span>
</p><p>
It is easier to show the related property:
</p><blockquote><b>Lemma 4 (Mixing Lemma)</b> <em> <a name="lemmixing"/> Let $H \subseteq \{0, 1\}^n$ be a set of inputs on the left of $G$, with the density of $H$ at least $\mu$. Then, except for some $2e^{-\Omega(\mu k)}$-fraction of tuples $B$, all tuples $B$ on the right of $G$ have \[\Pr_{x \in N(B)}[x \in H] = \mu \pm \mu/2\] </em></blockquote>

<p>

 <em>Proof of Lemma <a href="http://learningwitherrors.org/atom.xml#lemmixing">4</a>:</em>  Drawing a uniformly random tuple $B$ on the right is exactly drawing $k$ iid samples of inputs $B := (x_1, x_2, \dots, x_k)$. Then, by definition of $G$, picking a random neighbor $x \in N(B)$ is just picking a random $x \in B$. Thus, it is sufficient to show that if we draw $k$ iid inputs $x_1, x_2, \dots, x_k$, the fraction of inputs that fall in $H$ is within a multiplicative factor $(1 \pm 1/2)$ of its expectation $\mu$ (with high probability). This follows immediately from Chernoff bounds. $$\tag*{$\blacksquare$}$$
</p><p>
From this, the above Lemma <a href="http://learningwitherrors.org/atom.xml#lemnotbad">3</a> follows easily:
</p><p>
<em>Proof of Lemma <a href="http://learningwitherrors.org/atom.xml#lemnotbad">3</a>:</em>  Let $\bad$ be the set of “bad” inputs $x$, where $\Pr_{B \in N(x)}[C'(B) \text{ is correct}] \leq \epsilon/10$. Suppose the density of $\bad$ is $\mu$. Let us count fraction of total edges in $G$ that go between $\bad$, and the set of correct tuples (which we call $\good$). By the mixing lemma, there are at least $(\epsilon - 2e^{\Omega(\mu k)})$ fraction of tuples $B^*$ with $\Pr_{x \in N(B^*)}[\text{$x$ is bad}] \geq \mu / 2$. So there are at least $(\epsilon - 2e^{\Omega(\mu k)}) (\mu/2)$ fraction of edges between the $\bad$ and $\good$ sets.
</p><p>
But, each bad input $x$ has at most $\eps/10$ fraction of edges into $\good$ by definition, so the fraction of $\bad \leftrightarrow \good$ edges is at most $\mu (\eps/10)$.
</p><p>
Thus we must have \begin{align*} (\epsilon - 2e^{-\Omega(\mu k)}) (\mu/2) &amp;\leq \mu (\eps/10)\\ \implies \mu &amp;\leq O(\log(1/\eps)/k) \end{align*} This gives $\mu \leq \delta/200$ for our choice of $\delta$. $$\tag*{$\blacksquare$}$$
</p><p>
This concludes the proof of correctness of the oracle version (Theorem <a href="http://learningwitherrors.org/atom.xml#thmoracle">2</a>). $$\tag*{$\blacksquare$}$$
</p><p>
</p><h2 class="tex">4. Closing Remarks </h2>
<p>


</p><ul> <li> Note that in the oracle version, we were able to output a good circuit with probability $0.99$, instead of w.p. $\Theta(\eps)$ as in the fully uniform version. This makes sense because if we have an $f$-oracle, we can “check” if our circuit is actually computing the desired $f$, so we don't run into the unique decoding problem. (Indeed, we can construct an optimal version of algorithm $\A^f$ of Theorem <a href="http://learningwitherrors.org/atom.xml#thmoracle">2</a> from the algorithm $\A$ of Theorem <a href="http://learningwitherrors.org/atom.xml#thmuniformDP">1</a> in a black-box way, by checking if the output circuit of $\A$ mostly agrees with $f$ on enough random inputs).
<p>
 </p></li><li> There were several simplifications we made from $\A$ to $\A^f$.<br/>
 (1) We queried the oracle for the hardcoded values $v$, instead of the circuit.<br/>
 (2) We hardcoded $(k-1)$-multisets instead of $(k/2)$-multisets.<br/>
 (3) We hardcoded $T$ iid multisets $\{A_i\}$, instead of just one multiset $A$.<br/>
 Note that we could not have done (2) without also doing (3) -- otherwise there would not have been enough mixing (the circuit would fail with probability close to $\eps$). Also, (3) would not have worked in the fully uniform case ($\A$, without the oracle) -- because then all the hardcoded sets will be correct with only very small probability.
<p>
 </p></li><li> The reason Theorem <a href="http://learningwitherrors.org/atom.xml#thmoracle">2</a> has suboptimal parameters (eg, compare the setting of $\delta$ to Theorem <a href="http://learningwitherrors.org/atom.xml#thmuniformDP">1</a>) is because our analysis used the loose union bound, instead of using the fact that circuit $C_{A, v}$, by only outputting values that pass a test, is doing rejection-sampling on a certain conditional probability space. The tight analysis in [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>] takes advantage of this fact.
<p>
 </p></li><li> In the proof of Thereom <a href="http://learningwitherrors.org/atom.xml#thmoracle">2</a>, we used a property of the graph $G$ that was essentially like an “Expander Mixing Lemma”. We may hope that if we replace $G$ with something sufficiently expander-like, we could get a derandomized direct-product theorem. Indeed, something like this is done in [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>] (“Uniform direct product theorems: simplified, optimized, and <i>derandomized</i>”).
<p>
 </p></li><li> I think the oracle version is sufficient for the applications in [<a href="http://learningwitherrors.org/atom.xml#ref-CIKK16">CIKK16</a>], since there we have query access to the function $f$ we are trying to learn/compress.
<p>
 </p></li><li> For a good survey on direct-product for non-uniform hardness amplification, and the related “Yao's XOR Lemma”, see [<a href="http://learningwitherrors.org/atom.xml#ref-GNW11">GNW11</a>] (which includes at least 3 different proofs of the non-uniform XOR lemma). For a clean proof of Impagliazzo's Hardore Set theorem, which is used in some proofs of the XOR lemma, see for example Arora-Barak.
<p>

</p></li></ul>


<p>
<br/></p><hr/><h3>References</h3>
<p>
<a name="ref-CIKK16">[CIKK16]</a> Marco~L Carmosino, Russell Impagliazzo, Valentine Kabanets, and Antonina
  Kolokolova.
 Learning algorithms from natural proofs.
 In <em>LIPIcs-Leibniz International Proceedings in Informatics</em>,
  volume~50. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.
 URL:
  <a href="http://drops.dagstuhl.de/opus/volltexte/2016/5855/pdf/34.pdf">http://drops.dagstuhl.de/opus/volltexte/2016/5855/pdf/34.pdf</a>.
</p><p>

</p><p>
<a name="ref-GNW11">[GNW11]</a> Oded Goldreich, Noam Nisan, and Avi Wigderson.
 On yao's xor-lemma.
 In <em>Studies in Complexity and Cryptography. Miscellanea on the
  Interplay between Randomness and Computation</em>, pages 273--301. Springer,
  2011.
 URL: <a href="http://www.wisdom.weizmann.ac.il/~oded/COL/yao.pdf">http://www.wisdom.weizmann.ac.il/~oded/COL/yao.pdf</a>.
</p><p>

</p><p>
<a name="ref-IJKW10">[IJKW10]</a> Russell Impagliazzo, Ragesh Jaiswal, Valentine Kabanets, and Avi Wigderson.
 Uniform direct product theorems: simplified, optimized, and
  derandomized.
 <em>SIAM Journal on Computing</em>, 39(4):1637--1665, 2010.
 URL: <a href="http://www.cs.columbia.edu/~rjaiswal/IJKW-Full.pdf">http://www.cs.columbia.edu/~rjaiswal/IJKW-Full.pdf</a>.
</p><p/></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-08-25T01:00:00Z</updated>
    <published>2016-08-25T01:00:00Z</published>
    <author>
      <name>Preetum Nakkiran</name>
    </author>
    <source>
      <id>http://learningwitherrors.org/atom.xml</id>
      <link href="http://learningwitherrors.org/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="http://learningwitherrors.org" rel="alternate" type="text/html"/>
      <title>Learning With Errors</title>
      <updated>2017-01-04T04:27:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://learningwitherrors.org/2016/08/13/first-post</id>
    <link href="http://learningwitherrors.org/2016/08/13/first-post/" rel="alternate" type="text/html"/>
    <title>New Theory Blog</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We’re starting a theory student blog!  The idea is, this is a collaborative blog
about theoretical computer science, where people can post about interesting
things they’re learning / have learnt. The goal is to help everyone learn from
each other, and also have a forum for student discussion.</p>

<p>Hopefully this will help make TCS concepts more accessible: Sometimes reading
the original paper is not the best / most efficient way to learn something, and
there are several perspectives or proofs of the same thing that are not
explicitly written down in the literature, but are known in the community. We
hope this blog will be a way to share this knowledge among theory students.</p>

<p>One important aspect is, we want this to feel like an informal place to learn
and discuss – think more like chalk talks than STOC talks. It’s fine to have
rough calculations, sketched figures, etc – the emphasis is on explaining
things nicely. And we want to encourage asking clarifying questions and
discussing in the comments.</p>

<h2 id="on-posts">On posts:</h2>
<ul>
  <li>
    <p>Posts can be about anything technical that other students may find
interesting or learn from. Anything from current research to classical
results.</p>
  </li>
  <li>
    <p>The length and thoroughness can vary, anything from “survey of this field” to
“summary of cool paper” to “interesting technical lemma” to “something cool I
learnt this week”, etc.</p>
  </li>
  <li>
    <p>You don’t need to be an expert on the topic to write about it (as the name
suggests, there may be some errors, but hopefully also some learning).</p>
  </li>
  <li>
    <p>The aim is to convey interesting or useful techniques and intuition (e.g. try
not to just announce a new result without explaining the ideas behind it)</p>
  </li>
</ul>

<h2 id="contributing">Contributing:</h2>
<p>Everyone is welcome (and encouraged!) to contribute – including
non-students, and generally anyone interested in TCS.</p>

<p>The easiest way is to
simply write a LaTeX or Markup document, and email it to me (preetum [at]
berkeley).
There is also a <a href="http://learningwitherrors.org/contributing/">harder way</a>.</p>

<p>Ideally, both readers and writers would get something out of this blog.
(Personally, I like to present topics to make sure I understand them
fully. And of course, we can have an interesting discussion about it.)</p>

<!--(In theory, the entire source code of this blog is public on Github, so you
can author a new post by compiling and pushing the appropriate files in the
appropriate places. In practice it's rather messy, but details are here).-->

<h2 id="comments-and-subreddit">Comments and Subreddit:</h2>
<p>We have comments below each post, which we encourage
people to use to discuss the post.</p>

<p>We also have the subreddit <a href="https://www.reddit.com/r/LWE">r/LWE</a>,
which we hope can be used as a more general
forum among theory students. Feel free to use this for both blog-related things
and general theory questions. Let’s see how this works.</p>

<h2 id="initial-posts">Initial Posts:</h2>
<p>We’re launching with posts on:</p>

<ul>
  <li>
    <p><a href="http://learningwitherrors.org/2016/06/23/intro-sos/">Intro to the Sum-of-Squares Hierarchy</a>  <br/>
by Tselil Schramm.</p>
  </li>
  <li>
    <p><a href="http://learningwitherrors.org/2016/08/12/pseudocalibration-for-planted-clique-sos/">Pseudo-calibration for Planted Clique Sum-of-Squares Lower Bounds</a>  <br/>
by Pasin Manurangsi.</p>
  </li>
  <li>
    <p><a href="http://learningwitherrors.org/2016/07/06/deterministic-sparsification/">Deterministic Sparsification</a>  <br/>
by Chenyang Yuan.</p>
  </li>
  <li>
    <p><a href="http://learningwitherrors.org/2016/06/03/small-bias/">Simple Lower Bounds for Small-bias Spaces</a> and
<a href="http://learningwitherrors.org/2016/05/27/fast-johnson-lindenstrauss/">Fast Johnson-Lindenstrauss</a>  <br/>
by Preetum Nakkiran.</p>
  </li>
</ul>

<p>Thanks especially to the above people (and all future authors) for contributing.</p>

<h2 id="conclusion-and-open-questions">Conclusion and Open Questions</h2>
<p>When conceiving this blog, we had some other
ideas for things that should exist, such as a set of collaboratively-edited
pages on “How to best learn topic X”. Are people interested in contributing to
something like this?  In general, any suggestions for things you would like to
see (regarding this blog, or otherwise)?</p>

<p>Feel free to use the comments section below.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-08-13T00:00:00Z</updated>
    <published>2016-08-13T00:00:00Z</published>
    <author>
      <name>Preetum Nakkiran</name>
    </author>
    <source>
      <id>http://learningwitherrors.org/atom.xml</id>
      <link href="http://learningwitherrors.org/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="http://learningwitherrors.org" rel="alternate" type="text/html"/>
      <title>Learning With Errors</title>
      <updated>2017-01-04T04:27:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://learningwitherrors.org/2016/08/12/pseudocalibration-for-planted-clique-sos</id>
    <link href="http://learningwitherrors.org/2016/08/12/pseudocalibration-for-planted-clique-sos/" rel="alternate" type="text/html"/>
    <title>Pseudo-calibration for Planted Clique Sum-of-Squares Lower Bound</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div style="display: none;"/>
<p>

</p><p>
 Recently, Barak, Hopkins, Kelner, Kothari, Moitra and Potechin [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>] proved an essentially tight Sum-of-Squares lower bound for the <em>planted clique</em> problem. Their result can be divided into two main parts: coming up with the <em>pseudo-distribution</em> and proving positivity of such pseudo-distribution. In this short blog, we summarize the first part of the paper, which provides a general systematic way to come up with pseudo-distributions for problems other than the planted clique problem, without going into details of the proof. We do not touch on the second part, which is more technically involved, here but we will hopefully do so in future posts.
<!--more-->
</p><p>
</p><h2 class="tex">1. SoS Lower Bounds and the Planted Clique Problem </h2>
<p>
In this section, we provide some background for readers unfamiliar with proving Sum-of-Squares lower bounds and the planted clique problem. Those who are accustomed to the topic can skip this section. For SoS, we use notations from <a href="http://learningwitherrors.org/2016/06/23/intro-sos/">Tselil's blog</a> on Sum-of-Squares Hierarchy, which is also a good place to start for those unfamiliar with SoS Hierarchy.
</p><p>
In this blog, we do not need the optimization version of SoS Hierarchy but we will only use a feasibility one. Recall that, given a polynomial feasibility problem of the form \[Q = \left\{x \in \mathbb{R}^n : \forall i \in [m], g_i(x) = 0\right\},\] the degree-$2d$ Sum-of-Squares relaxation of the problem, which can be solved in $n^{O(d)}$ time, is<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span class="sidenote" id="footnote1"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a> Note that, in Tselil's blog, the positivity condition is written as the pseudo-moment matrix being positive semidefinite but it is not hard to see that this is the same as requiring that $\tE[q^2] \geq 0$ for every $q$ with $\deg(q) \leq d$. </span> \begin{align} \label{eq:sos} \sos_d(Q) = \left\{\tE : \begin{array}{lr} {\tE}: \{q: \deg(q) \leq 2d\} \rightarrow \mathbb{R} \text{ is a linear operator with } \tE[1] = 1, \\ \forall q \text{ with } \deg(q) \leq d, \tE[q^2] \geq 0, \\ \forall i \in [m] \forall q \text{ with } \deg(q) \leq 2d - \deg(g_i), \tE[g_i q] = 0. \end{array} \right\}. \end{align}
</p><p>
Roughly speaking, if we want to show that degree-$2d$ SoS fails to certify that a polynomial feasibility problem $Q$ is infeasible, we need to come up with a degree-$2d$ pseudo-distribution $\tE$ that satisfies the conditions in (\ref{eq:sos}). For concreteness, let us consider the <em>planted clique</em> problem defined as follows.
</p><p>

</p><blockquote><b>Definition 1 (Planted Clique$(n, k)$)</b> <em> Given as an input a graph $G = (V, E)$ drawn from one of the two following distributions (each with probability $1/2$):

<ol> <li> $\cG(n, 1/2)$: the Erdos-Renyi random graph of $n$ vertices where each edge is included with probability 1/2, </li><li> $\cG(n, 1/2, k)$: the planted distribution, in which a graph $G$ is first drawn from $\cG(n, 1/2)$. Then, $k$ vertices of $G$ are chosen uniformly at random and an edge between each pair of chosen vertices are added to $G$.
</li></ol>

 The goal is to determine, with correctness probability $1/2 + \varepsilon$ for some constant $\varepsilon &gt; 0$, which distribution $G$ is drawn from. </em></blockquote>

<p>


</p><p>
In this blog, we always restrict ourselves to the case where $k \gg \log n$ so that the maximum clique sizes of the two cases are different. Since the largest clique in $\cG(n, 1/2)$ is of size $O(\log n)$ with high probability, brute-force search solves the planted clique problem with high probability in $n^{O(\log n)}$ time. On the other hand, the best known polynomial-time algorithm works only when $k = \Omega(\sqrt{n})$ [<a href="http://learningwitherrors.org/atom.xml#ref-AKS98">AKS98</a>]. A natural question is of course whether the SoS Hierarchy can do any better than this.
</p><p>
The most widely-used formulation of planted clique in terms of polynomial feasibility, and the one used in [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>], is to formulate it as “does $G$ have a clique of size $k$?”. For convenience, let $V = [n] = \{1, \dots, n\}$. This formulation can be written as follows.
</p><p>
\begin{align*} \cli_k(G) = \left\{x \in \mathbb{R}^n : \begin{array}{lr} \forall i \in [n], x_i^2 = x_i, \\ \forall (i, j) \notin E, x_ix_j = 0, \\ \sum_{i \in [n]} x_i = k \end{array} \right\} \end{align*}
</p><p>
When the constraints are satisfied, $x_i$ is simply a boolean indicator variable whether $i$ is included in the clique. If we can solve $\cli_k(G)$ in polynomial time, then we are done because $G \sim \cG(n, 1/2, k)$ always has clique of size $k$ whereas the maximum clique of $G \sim \cG(n, 1/2)$ is of size $O(\log n)$ w.h.p. Thus, there is always a solution in $\cli_k(G)$ for $G \sim \cG(n, 1/2, k)$ but, w.h.p., there is no feasible solution for $G \sim \cG(n, 1/2)$. But of course solving $\cli_k(G)$ is NP-hard so we will try to relax it using degree-$2d$ SoS which we can solve in $n^{2d}$ time.
</p><p>
Again, when $G \sim \cG(n, 1/2, k)$, $\sos_d(\cli_k(G))$ remains feasible. If we want to tell which distribution $G$ is drawn from by looking only at whether $\sos_d(\cli_k(G))$ is feasible, we need that, when $G \sim \cG(n, 1/2)$, $\sos_d(\cli_k(G))$ is infeasible with probability at least $\varepsilon$. The main result of [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>] is that this is impossible. In particular, they show the following:
</p><p>

</p><blockquote><b>Theorem 2</b> <em> <a name="thmmain-clique"/> For every $d \ll \log n$, when $k \leq n^{1/2 - O(\sqrt{d/\log n})}$ and $G$ is drawn from $\cG(n, 1/2)$, $\sos_d(\cli_k(G))$ is feasible with high probability. </em></blockquote>

<p>


</p><p>
In other words, Barak et al.'s result says that the SoS approach to planted clique is no better (up to the $O(\sqrt{d/\log n})$ factor in the exponent) than the known algorithm from [<a href="http://learningwitherrors.org/atom.xml#ref-AKS98">AKS98</a>].
</p><p>
From how $\sos_d(\cli_k(G))$ is defined, proving Theorem <a href="http://learningwitherrors.org/atom.xml#thmmain-clique">2</a> boils down to find a linear operator ${\tE}_G: \{q: \deg(q) \leq 2d\} \rightarrow \mathbb{R}$ for each graph $G$ such that, if $G = ([n], E)$ is drawn from $\cG(n, 1/2)$, the following conditions are satisfied with high probability:

</p><ol> <li> $\tE_G[1] = 1$, </li><li> $\forall i \in [n] \forall q$ with $ \deg(q) \leq 2d - 2, \tE_G[x_i^2q] = \tE_G[x_iq]$, </li><li> $\forall (i, j) \notin E \forall q$ with $\deg(q) \leq 2d - 2,\tE_G[x_ix_jq] = 0$, </li><li> $\tE_G[\sum_{i \in [n]} x_i] = k$, </li><li> $\forall q$ with $\deg(q) \leq d, \tE_G[q^2] \geq 0$.
</li></ol>


<p>
</p><h2 class="tex">2. Pseudo-calibration for Planted Clique </h2>
<p>
Coming up with degree-$2d$ pseudo-distribution $\tE_G$ with desired properties stated in the previous section is particularly hard for planted clique and past attempts often involve some ad-hoc fixes that prevent them from getting tight bound for large $d$. This is where Barak et al.'s so-called <em>pseudo-calibration</em> method, which is a systematic way to derive $\tE_G$, comes in. Since the method is more of an intuitive heuristic rather than a provable approach, we will be informal here. We also note that the explanation given here is somewhat different than that in [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>] and the readers should consult the full paper for a more thorough view of pseudo-calibration.
</p><p>
Let us take a step back and think about our algorithm for planted clique for a moment. Given $G$, we try to solve $\sos_d(\cli_k(G))$. If it is infeasible, then we know for certain that $G$ is drawn from $\cG(n, 1/2)$. Otherwise, we do not seem to gain anything. However, this may not be entirely true; we actually get back $\tE_G$. One thing we can do here is to pick $f_G$ (which can depend on $G$) of degree (with respect to $x$) at most $2d$ as a test function and ask for $\tE_G[f_G]$. If the distributions of $\tE_G[f_G]$ under $G \sim \cG(n, 1/2)$ and $G \sim \cG(n, 1/2, k)$ are “very different”<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote2">2</a></sup><span class="sidenote" id="footnote2"><a href="http://learningwitherrors.org/atom.xml#footnote2" name="footnote2">2.</a> In other words, they are distinguishable in polynomial time. </span>, then we should be able to tell $G$'s from the two distributions apart by just looking at $\tE_G[f_G]$. Hence, not only that $\sos_d(\cli_k(G))$ must be feasible with high probability when $G \sim \cG(n, 1/2)$ but the distributions of $\tE_G[f_G]$ when $G \sim \cG(n, 1/2)$ and when $G \sim \cG(n, 1/2, k)$ must also be indistinguishable in polynomial time for every test function $f_G$. An implication of this is that the expectation of $\tE_G[f_G]$ over the two distributions are roughly equal, i.e., \begin{align*} \E_{G \sim \cG(n, 1/2)} \tE_G[f_G] \approx \E_{G \sim \cG(n, 1/2, k)} \tE_G[f_G]. \end{align*}
</p><p>
We of course do not know what $\tE_G$ is even when $G$ is drawn from $\cG(n, 1/2, k)$ so the above equality does not tell us much yet. But recall that $\tE_G$ is our fake solution and we want it to resemble the actual solution as much as possible. Hence, a reasonable heuristic here is to try to make $\E_{G \sim \cG(n, 1/2, k)} \tE_G[f_G]$ roughly equal to $\E_{G \sim \cG(n, 1/2, k)} f_G(x_G)$ where $x_G$ denote the actual solution, i.e., the indicator vector for the maximum clique in $G$.
</p><p>
For convenience, let us write $(G, x) \sim \cG(n, 1/2, k)$ to denote $G$ drawn from $\cG(n, 1/2, k)$ and $x$ being the indicator vector whether each vertex is included as part of the planted $k$-clique. Under this notation, the aforementioned condition can be written as \begin{align*} \E_{G \sim \cG(n, 1/2, k)} \tE_G[f_G] \approx \E_{(G, x) \sim \cG(n, 1/2, k)} f_G(x). \end{align*}
</p><p>
Combining the above two equations, we get \begin{align} \label{eq:calib} \E_{G \sim \cG(n, 1/2)} \tE_G[f_G] \approx \E_{(G, x) \sim \cG(n, 1/2, k)} f_G(x). \end{align}
</p><p>
Condition (\ref{eq:calib}) is what Barak et al. called <em>pseudo-calibration</em><sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote3">3</a></sup><span class="sidenote" id="footnote3"><a href="http://learningwitherrors.org/atom.xml#footnote3" name="footnote3">3.</a> In [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>], the pseudo-calibration condition is in fact slightly stronger that stated here; equality is required instead of approximate equality. However, it does not matter anyway since there will be approximations in subsequent calculations. </span>. As noted in the paper, this condition is quite strong. For example, for fixed $i, j \in [n]$ and $q$ with $\deg(q) \leq 2d - 2$, if we define $f$ as \begin{align*} f_G(x) = \begin{cases} 0 &amp; \text{ if } (i, j) \in E, \\ x_ix_jq(x) &amp; \text{ otherwise}, \end{cases} \end{align*} then $f_G(x)$ is always zero on the right hand side. Hence, $\E_{G \sim \cG(n, 1/2)} \tE_G[f_G] \approx 0$. If we assume that $\tE_G[f_G]$ is non-negative, then Condition 3 at the end of the previous section is almost immediately satisfied. In fact, as we will see next, Condition (\ref{eq:calib}) almost fully determines $\tE_G$ for every $G$.
</p><p>
</p><h3 class="tex">2.1. From Pseudo-Calibration to Pseudo-Distribution</h3>
<p>
We will now see how to arrive at $\tE_G$ from the pseudo-calibration condition. As stated earlier, the condition is quite strong; in fact, it is too that it cannot hold for every $f_G$. For instance, we can pick $f_G$ to simply be the indicator function of whether $G$ has a clique of size $k$. By doing so, the left hand side of (\ref{eq:calib}) is approximately zero whereas the right hand side is one. However, we are “cheating” by picking such $f_G$ because we do not even know how to compute this test function in polynomial time! Hence, roughly speaking, we need to restrict $f_G$ to only those that are not more “powerful” that the SoS relaxation itself.
</p><p>
To state the exact condition we enforce on $f_G$, let us think of $f_G(x)$ as a function $f(G, x)$ of both $G$ and $x$ where the graph $G$ is encoded naturally as a string in $\{\pm 1\}^{[n] \choose 2}$, i.e., the $(i, j)$-index of the input is $+1$ if there is an edge between $i$ and $j$ and $-1$ otherwise. Now, we can write $f$ as a polynomial on both $G$ and $x$: \begin{align*} f(G, x) = \sum_{T \subseteq {[n] \choose 2}, S \subseteq [n]} a_{(T, S)} \chi_T(G) x_S \end{align*} where $\chi_T(G)$ and $x_S$ denote $\prod_{e \in T} G_e$ and $\prod_{i \in S} x_i$ respectively, and, $a_{(T, S)}$'s are the coefficients of the polynomial. We will require the pseudo-calibration condition to hold only for $f_G$ such that each monomial depends on at most $\tau$ vertices where $\tau = O(d)$ is a truncation threshold. In other words, we only restrict ourselves to $f$ that can be written as \begin{align*} f(G, x) = \sum_{T \subseteq {[n] \choose 2}, S \subseteq [n] \atop |\cV(T) \cup S| \leq \tau} a_{(T, S)} \chi_T(G) x_S \end{align*} where $\cV(T)$ is the set of all vertices which are endpoints of edges in $T$. The intuition behind this heuristic is that, in the conditions on $\tE_G$ imposed by the SoS relaxation, each monomial involves at most $2d$ vertices because $\tE_G$ is defined only on polynomials on $x$ of degree at most $2d$. As a result, each monomial appearing in $f(G, x)$ should involve no more than $O(d)$ vertices in order to limit its “power” to be not much more than the SoS relaxation.
</p><p>
Now, let us use the pseudo-calibration condition to determine $\tE_G$. Fixed a subset $S \subseteq [n]$ of size at most $2d$, we will compute $\tE_G[x_S]$ for the monomial $x_S$. Note that, since $\tE_G$ is linear and $\tE_G[x_i^2] = x_i$ for all $i \in [n]$, these $\tE_G[x_S]$'s uniquely determine $\tE_G$. By viewing $\tE_G[x_S]$ as a function of $G$, $\tE_G[x_S]$ can be written as fourier expansion \begin{align*} \tE_G[x_S] = \sum_{T \subseteq {[n] \choose 2}} \widehat{\tE_G[x_S]}(T) \chi_T(G). \end{align*} The final heuristic employed by Barak et al. is to enforce $\tE_G[x_S]$ to be low degree by letting $\widehat{\tE_G[x_S]}(T) = 0$ for every $T$ with $|\cV(T) \cup S| &gt; \tau$. This heuristic makes sense since $\tE_G$ must be output by the SoS relaxation solver, which runs in $n^{O(d)}$ time; hence, $\tE_G$ cannot be too hard to compute. More importantly, as we will see shortly, this condition allows us to almost uniquely determine $\tE_G$ from the pseudo-calibration condition.
</p><p>
Recall that each fourier coefficient $\widehat{\tE_G[x_S]}(T)$ is simply equal to $\E_{G \sim \cG(n, 1/2)} \tE_G[x_S \chi_T(G)].$ Plugging in the pseudo-calibration condition with $f = x_S\chi_T(G)$, this is approximately $\E_{(G, x) \sim \cG(n, 1/2, k)} [x_S\chi_T(G)]$. It is not hard to see that this expression is equal to the probability that every vertex in $\cV(T) \cup S$ is in the planted clique, which is roughly $(k/n)^{|\cV(T) \cup S|}$ when $|\cV(T) \cup S|$ is small. Indeed, we will set $\widehat{\tE_G[x_S]}(T)$ to be exactly this. In other words, the final pseudo-distribution is \begin{align*} \tE_G[x_S] = \sum_{T \subseteq {[n] \choose 2} \atop |\cV(T) \cup S| \leq \tau} \left(\frac{k}{n}\right)^{|\cV(T) \cup S|} \chi_T(G). \end{align*}
</p><p>
It is not hard to see that $\tE_G$ indeed satisfies the pseudo-calibration condition for $f$'s of our interest. As explained right before the beginning of this subsection, this almost immediately implies that the third condition required for $\tE_G$ is satisfied; it is also pretty easy to check that the condition is indeed true (see Lemma 5.5 in the paper). Using concentration inequalities, Barak et al. also show that $\tE_G[1] = 1 \pm o(1)$ and $\tE_G[\sum_{i \in [n]} x_i] = k \pm o(1)$ (see full proof in Appendix A.2 of the paper). Note that while these two conditions are only approximately satisfied, $\tE_G$ can be scaled so that they are exactly satisfied as well. As mentioned briefly earlier, the proof of the positivity condition $\tE_G[q^2] \geq 0$ is much harder and is the paper's main technical contribution. We do not attempt to discuss it here but we will try to blog about it in the future.
</p><p>
</p><h2 class="tex">3. Further Reading </h2>
<p>
The authors of [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>] have given talks on the paper and some of them are available online, such as <a href="https://www.youtube.com/watch?v=ZmFOsAB7Y1k">Moitra's</a> and <a href="https://www.youtube.com/watch?v=H2C2ZdgynX4">Kothari's</a>. Barak also wrote <a href="https://windowsontheory.org/2016/04/13/bayesianism-frequentism-and-the-planted-clique-or-do-algorithms-believe-in-unicorns/">a blog</a> regarding pseudo-calibration. All the materials mentioned discuss the pseudo-calibration in much more detail than in this post. Moitra's talk also contains the proof sketch of positivity of the pseudo-distribution, which is not covered in this blog post.
</p><p>
Apart from the paper, I am not aware of the pseudo-calibration technique being used to prove new lower bounds for other problems yet. I will update this section when I come across new results based on pseudo-calibration.
</p><p>
<br/></p><hr/><h3>References</h3>
<p>
<a name="ref-AKS98">[AKS98]</a> Noga Alon, Michael Krivelevich, and Benny Sudakov.
 Finding a large hidden clique in a random graph.
 <em>Random Struct. Algorithms</em>, 13(3-4):457--466, 1998.
</p><p>

</p><p>
<a name="ref-BHKKMP16">[BHKKMP16]</a> Boaz Barak, Samuel~B. Hopkins, Jonathan~A. Kelner, Pravesh Kothari, Ankur
  Moitra, and Aaron Potechin.
 A nearly tight sum-of-squares lower bound for the planted clique
  problem.
 <em>CoRR</em>, abs/1604.03084, 2016.
</p><p/></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-08-12T00:00:00Z</updated>
    <published>2016-08-12T00:00:00Z</published>
    <author>
      <name>Pasin Manurangsi</name>
    </author>
    <source>
      <id>http://learningwitherrors.org/atom.xml</id>
      <link href="http://learningwitherrors.org/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="http://learningwitherrors.org" rel="alternate" type="text/html"/>
      <title>Learning With Errors</title>
      <updated>2017-01-04T04:27:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://learningwitherrors.org/2016/07/06/deterministic-sparsification</id>
    <link href="http://learningwitherrors.org/2016/07/06/deterministic-sparsification/" rel="alternate" type="text/html"/>
    <title>Deterministic Sparsification</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div style="display: none;"/>

<p>Let $G$ be a dense graph. A sparse graph $H$ is a sparsifier of $G$
approximation of $G$ that preserves certain properties such as quadratic forms
of its Laplacian. This post will formally define spectral sparsification, then
present the intuition behind the deterministic construction of spectral
sparsifiers by Baston, Spielman and Srivastava [<a href="http://learningwitherrors.org/atom.xml#BSS08">BSS08</a>].</p>

<!--more-->

<p>Benczúr and Karger [<a href="http://learningwitherrors.org/atom.xml#BK96">BK96</a>] introduced the cut sparsifier, which
ensures that the value of all cuts in $H$ approximates that of all cuts in $G$:</p>

<blockquote>
  <p><a name="def:cut-sp"/><strong>Definition 1 (Cut Sparsification):</strong> A weighted
undirected graph $H = (V, E_H)$ is an $\epsilon$-cut sparsifier of a weighted
undirected graph $G = (V, E_G)$ if for all $S \subset V$,</p>

  

  <p>Where $E_G$ and $E_H$ are the sums of edge weights crossing the cuts in $G$ and
$H$ respectively.</p>
</blockquote>

<p>Spielman and Teng [<a href="http://learningwitherrors.org/atom.xml#ST08">ST08</a>] introduced another notion of graph sparsification
with the quadratic form of the Laplacian:</p>

<blockquote>
  <p><a name="def:spectral-sp"/><strong>Definition 2 (Spectral Sparsification):</strong> A
weighted undirected graph $H = (V, E_H)$ is an $\epsilon$-spectral sparsifier
of a weighted undirected graph $G = (V, E_G)$ if for all $x \in
\mathbb{R}^{|V|}$,</p>

  

  <p>Where $L_G$ and $L_H$ are the graph Laplacians of $G$ and $H$ respectively.</p>
</blockquote>

<p>Cut sparsifiers can be used in approximating max-flow (via the max-flow min-cut
theorem) and spectral sparsifiers are a key ingredient in solving Laplacian
inear systems in near linear time.</p>

<!-- Add why it's important for H to be weighted? -->

<p>Note that this stronger than cut sparsification, as we can fix $x$ to be the
indicator vectors of cuts to obtain <a href="http://learningwitherrors.org/atom.xml#def:cut-sp">definition 1</a>. Also note that
this notion of sparsifiers also provides bounds on Laplacian eigenvalues, and
thus spectral sparsifiers of complete graphs are also expanders. These
sparsifiers can be constructed in a randomized manner by sampling edges
proportional to their effective resistance [<a href="http://learningwitherrors.org/atom.xml#SS08">SS08</a>], but in this post we
will focus on a deterministic construction presented in [<a href="http://learningwitherrors.org/atom.xml#BSS08">BSS08</a>], as
stated more precisely in the following theorem:</p>

<blockquote>
  <p><a name="thm:sparsifier"/><strong>Theorem 1:</strong> For every $d &gt; 1$, every undirected
graph $G = (V, E)$ on $n$ vertices contains a weighted subgraph $H = (V, F,
\tilde{w})$ with $\lceil d(n-1) \rceil$ edges that satisfies:</p>

  
</blockquote>

<h2 id="preliminaries">Preliminaries</h2>
<p>The Laplacian $L$ of a graph can be seen as a linear transformation relating the
flow and demand in an electrical flow on the graph where each edge has unit
resistance. Let $B \in \mathbb{R}^{m \times n}$ be the vertex-edge incidence
matrix and $W$ is a diagonal matrix of edge weights, then $L = B^TWB$. $L$ also
has a pseudoinverse which acts like an actual inverse for all vectors
$x \bot \mathbb 1$, resulting from solving an electrical flow on $G$.</p>

<p>Let $\kappa = \frac{d+1+2\sqrt d}{d+1-2\sqrt d}$, we assume that the graph is
connected thus $x \bot \mathbf{1}$, and perform a transformation on the
condition in <a href="http://learningwitherrors.org/atom.xml#thm:sparsifier">Theorem 1</a>:</p>



<p>Let $b_{e = (u, v)} = \mathbf{1}_u - \mathbf{1}_v$ be a row of incidence matrix
$B$ , $s_e$ be the weight of edge $e$ in $E_H$ and $A \succeq B$ when $A - B$ is
a psd matrix. Then the above condition can be rewritten as:</p>

<p><a name="eq:sparse-approx"/>
</p>

<p>We then define a vector $v_e = L_G^{-1/2}b_e^T$ for each $e \in E_G$. Notice
that over all edges of $G$, the rank 1 matrices formed by $v_eV_e^T$ sum to the
identity matrix:</p>



<p>Then <a href="http://learningwitherrors.org/atom.xml#eq:sparse-approx">equation 1</a> can be interpreted as choosing a sparse
subset of the edges in $G$, as well as weights $s_e$, so that the matrix
obtained by summing over the edges of $H$, $\sum_{e \in E_H} s_e v_ev_e^T$, has
a low condition number (ratio between the largest and smallest eigenvalues):</p>



<p>If we can find such a sparse set of edges and weights, then we have proved
<a href="http://learningwitherrors.org/atom.xml#thm:sparsifier">Theorem 1</a>. In [<a href="http://learningwitherrors.org/atom.xml#SS08">SS08</a>] this was done by randomly
sampling these rank-1 matrices based on their effective resistances of their
corresponding edges, using a distribution that has the identity matrix as the
expectation. Convergence is shown using a matrix concentration inequality. The
construction in [<a href="http://learningwitherrors.org/atom.xml#BSS08">BSS08</a>] deterministically chooses each $v_e$ and
$s_e$, bounding the increase in $\kappa$ in each step using barrier
functions. One useful lemma for this procedure is:</p>

<blockquote>
  <p><a name="lem:matrix-det"/><strong>Lemma 1 (Matrix Determinant Lemma):</strong> If $A$ is
nonsingular and $v$ is a vector, then:</p>

  
</blockquote>

<h2 id="main-proof">Main Proof</h2>

<p>Recall from the previous section the main theorem that need to be proved is:</p>

<blockquote>
  <p><a name="thm:rank1approx"/><strong>Theorem 2:</strong>
Suppose $d &gt; 1$ and $v_1, \cdots, v_m$ are vectors in $\mathbb{R}^n$ with

Then there exist scalars $s_i &gt; 0$ with $|{i: s_i \ne 0 }| \le dn$ so that
</p>
</blockquote>

<p>This is equivalent to bounding the ratio of $\lambda_{\min}$ and
$\lambda_{\max}$ of the matrix $\sum_{i \le m} s_i v_iv_i^T$.</p>

<p>We start with a matrix $A = 0$, and build it by adding rank-1 updates
$s_ev_ev_e^T$. One interesting fact is that for any vector $v$, the eigenvalues
of $A$ and $A + vv^T$ interlace. Consider the characteristic polynomial of $A +
vv^T$:</p>



<p>Which can be written in terms of the characteristic polynomial of $A$ using
<a href="http://learningwitherrors.org/atom.xml#lem:matrix-det">Lemma 1</a>. $u_j$ are the eigenvectors of $A$.  Let $\lambda$ be
a zero of $p_{A + vv^T}(x)$. It can either:</p>

<ol>
  <li>Be a zero of $p_A(x)$, so $\lambda$ is equal to an eigenvalue $\lambda_i$
  of $A$, and the corresponding eigenvector $u_i$ is orthogonal to $v$. In this
  case, this eigenvalue doesn’t move.</li>
  <li>Strictly interlace with the old eigenvalues. This happens when
  $p_A(\lambda) \ne 0$ and
  
  This can be interpreted with a physical model. Consider $n$ positive charges
  arranged vertically with the $j$-th charge’s position corresponding to the
  $j$-th eigenvalue of $A$, and its charge is $\dotp{v, u_j}^2$. The points
  where the electric potential is 1 are the new eigenvalues. Since between any
  two charges the potential changes direction from $+ \infty$ to $- \infty$,
  there has to be a point between every two charges where the potential is 1,
  thus the new eigenvalues strictly interlace the old ones.</li>
</ol>

<p>To get some intuition, we see what happens when we sample $v_i$ uniformly
randomly. Since $\sum_j v_jv_j^T = I$, $\mathbb{E}_v[\dotp{v, u}^2]$ is constant
for any normalized vector $u$. Therefore adding the average $v$ increases the
charges by the same amount in the physical model, causing the new eigenvalues to
all increase by the same amount. Informally, we expect all the eigenvalues to
“march forward” at similar rates with each $vv^T$ added, so $\lambda_{\max} /
\lambda_{\min}$ is bounded.</p>

<p>We construct a sequence of matrices $A^{(0)}, \cdots, A^{(q)}$ by adding rank-1
updates $t vv^T$. To bound the condition number after each update, we create two
barriers $l &lt; \lambda_{\min}(A) &lt; \lambda_{\max}(A) &lt; u$ so that the eigenvalues
of $A$ lie between them. $\Phi_l(A)$ and $\Phi^u(A)$ are defined as the
potentials at the barriers respectively:</p>



<p>The crucial step is to show that there exists a $v_i$ and $t$ so that we can
add $t v_i v_i^T$ to $A$, so that each barrier is shifted by a constant, and the
potentials at each barrier doesn’t change. We will sketch out the proof briefly,
readers can pursue the details in [<a href="http://learningwitherrors.org/atom.xml#BSS08">BSS08</a>].</p>

<p>Let constants $\delta_U$ and $\delta_L$ be the maximum amount each barrier can
increase each round, and constants $\epsilon_U = \Phi^{u_0}(A^{(0)})$ and
$\epsilon_L = \Phi_{l_0}(A^{(0)})$ be the initial potentials at each
barrier. The first lemma shows that if $t$ is not too large, adding $t vv^T$ to
$A$ and shifting the upper barrier by $\delta_U$ will not increase the upper
potential $\Phi^u$.</p>

<blockquote>
  <p><strong>Lemma 2 (Upper Barrier Shift):</strong> Suppose $\lambda_{\max}(A) &lt; u$, and $v$ is
any vector. If

Then:
</p>
</blockquote>

<p>The second lemma shows that if $t$ is not too small, adding $t vv^T$ to $A$ and
shifting the lower barrier by $\delta_L$ will not increase the lower potential
$\Phi^u$.</p>

<blockquote>
  <p><strong>Lemma 3 (Lower Barrier Shift):</strong> Suppose $\lambda_{\min}(A) &gt; l$, $\Phi_l(A)
\le 1/\delta_L$ and $v$ is any vector. If

Then:
</p>
</blockquote>

<p>Finally, it can be shown that there exists a $t$ and $v_i$ that satisfy the
conditions of the above lemmas.</p>

<blockquote>
  <p><strong>Lemma 3 (Both Barriers):</strong> If $\lambda_{\max}(A) &lt; u$, $\lambda_{\min}(A) &gt;
l$, $\Phi^u(A) \le \epsilon_U$, $\Phi_l(A) \le \epsilon_L$, and $\epsilon_U$ ,
$\epsilon_L$, $\delta_U$, $\delta_L$ satisfy:

then there exists a $v_i$ and positive $t$ for which
</p>
</blockquote>

<p>This is proved by an averaging argument relating the behavior of vector $v$ to
the behavior of the expected vector, showing that

therefore there exists a $i$ for which there is a gap between
$L_A(v_i) - U_A(v_i)$. Choosing the constants carefully, we can get the required
bound on the condition number.</p>

<h2 id="extension">Extension</h2>

<p>There is a similarity between <a href="http://learningwitherrors.org/atom.xml#thm:rank1approx">Theorem 2</a> and the
Kadison-Singer conjecture. One formulation of it is stated below:</p>

<blockquote>
  <p><a name="prop:KSC"/><strong>Proposition 1:</strong> There are universal constants
$\epsilon, \delta &gt; 0$ and $r \in \mathbb N$ for which the following statement
holds. If $v_1, \cdots, v_m \in \mathbb{R}^n$ satisfy $||v_i|| \le \delta$ for
all $i$ and

then there is a partition $X_1, \cdots X_r$ of ${1, \cdots, m }$ for which

for every $j = 1, \cdots, r$.</p>
</blockquote>

<p>This conjecture was positively resolved in [<a href="http://learningwitherrors.org/atom.xml#MSS13">MSS13</a>], using techniques
arising from generalizing the barrier function argument used to prove
<a href="http://learningwitherrors.org/atom.xml#thm:rank1approx">Theorem 2</a> to a multivariate version.</p>

<h2 id="references">References</h2>

<p><a name="BK96">[BK96]</a>
A. A. Benczúr and D. R. Karger. Approximating s-t minimum cuts in
$\tilde{O}(n^2)$ time. In <em>STOC ‘96</em>, pages 47-55, 1996.</p>

<p><a name="BSS08">[BSS08]</a>
J. Baston, D. A. Spielman and N. Srivastava. Twice-Ramanujan
Sparsifiers. Available at <a href="http://arxiv.org/abs/0808.0163">http://arxiv.org/abs/0808.0163</a>, 2008.</p>

<p><a name="MSS13">[MSS13]</a>
A. W. Marcus, D. A. Spielman and N. Srivastava. Interlacing Families
II: Mixed Characteristic Polynomials and The Kadison-Singer Problem. Available
at <a href="http://arxiv.org/abs/1306.3969">http://arxiv.org/abs/1306.3969</a>, 2013.</p>

<p><a name="SS08">[SS08]</a>
D. A. Spielman and N. Srivastava. Graph Sparsification by Effective
Resistances. In <em>STOC ‘08</em>, pages 563-568, 2008.</p>

<p><a name="ST08">[ST08]</a>
D. A. Spielman and S.-H. Teng. Spectral Sparsification of Graphs. Available at
<a href="http://arxiv.org/abs/0808.4134">http://arxiv.org/abs/0808.4134</a>, 2008.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-07-06T00:00:00Z</updated>
    <published>2016-07-06T00:00:00Z</published>
    <author>
      <name>Chenyang Yuan</name>
    </author>
    <source>
      <id>http://learningwitherrors.org/atom.xml</id>
      <link href="http://learningwitherrors.org/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="http://learningwitherrors.org" rel="alternate" type="text/html"/>
      <title>Learning With Errors</title>
      <updated>2017-01-04T04:27:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://learningwitherrors.org/2016/06/23/intro-sos</id>
    <link href="http://learningwitherrors.org/2016/06/23/intro-sos/" rel="alternate" type="text/html"/>
    <title>Intro to the Sum-of-Squares Hierarchy</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div style="display: none;"/>
<p>
This note is intended to introduce the Sum-of-Squares Hierarchy. We start by SDP relaxation, using the Goemans-Williamson Max-Cut SDP as a jumping off point. We then discuss duality and sum-of-squares proofs. Finally, we give an example non-trivial application of the sum-of-squares hierarchy: an algorithm for finding planted sparse vectors inside random subspaces. We will give no historical context, but in the final section there will be pointers to other resources which give a better sense of the history and alternative expositions of the same content.
<!--more-->
</p><p>
</p><h2 class="tex">1. A Relaxation for Polynomial Optimization </h2>
<p>
Suppose we are interested in some polynomial optimization problem: \[ Q = \left\{\max_{x \in \R^n}\ p(x),\qquad s.t.\quad g_i(x) = 0 \quad \forall\ i \in [m]\right\}. \] That is, we want to maximize our objective function, the polynomial $p:\R^n\to \R$, subject to the polynomial constraints $g_1(x) = 0, g_2(x) = 0,\ldots, g_m(x) = 0$.
</p><p>
The problem $Q$ may be non-convex, and solving such programs is NP-complete (i.e. this captures integer programming). A standard approach for a situation like this is to relax our problem $Q$ to a semidefinite program (SDP). Perhaps the most famous example is the Goemans-Williamson relaxation for Max-Cut:
</p><p>

</p><blockquote><b>Example 1 (Max-Cut)</b> <em> We can formulate the max cut problem on an $n$-vertex graph $G$ as a polynomial optimization problem with objective function $p(x) = \sum_{(i,j) \in E(G)} \frac{1- x_i x_j}{2}$ and the constraint polynomials $g_i(x) = x_i^2 - 1 = 0\ \forall i\in[n]$, which ensure that each $x_i = \pm 1$.
</em><p><em>
The Goemans-Williamson SDP relaxation assigns program variables $X_{ij}$ for each $i,j \in [n]$, where $X_{ij}$ is a stand-in for the monomial $x_i x_j$. The SDP then becomes \[ \left\{ \max \sum_{(i,j) \in E(G)}\tfrac{1}{2}(1 - X_{ij}), \qquad s.t. \quad X_{ii} -1= 0\quad \forall i \in [n],\quad X \succeq 0 \right\} \] where $X$ is the $n \times n$ matrix with variable $X_{ij}$ in the $(i,j)$th entry. </em></p></blockquote>

<p>


</p><p>

</p><p>
<b>The Sum-of-Squares SDP: Extending Goemans-Williamson.</b> After seeing the Goemans-Williamson Max-Cut SDP, it seems natural to apply a similar relaxation to other polynomial optimization problems. Suppose that the maximum degree of any term in $p, g_1,\ldots,g_m$ is at most $2d$. The strategy is to relax the polynomial optimization problem by replacing each monomial $\prod_{i\in S \subset [n]} x_i$ which appears in the program $Q$ with an SDP variable $X_S$. So for each $S \subset [n]$, $|S| \le 2d$, we have an SDP variable. Then we arrange the variables into an $(n+1)^d \times (n+1)^d$ matrix $X$ in the natural way, with rows and columns indexed by every ordered subset of at most $d$ variables: </p><p align="center"><img height="300" src="http://learningwitherrors.org/sources/june2016-sos/sos-X.png"/></p> Now we enforce some natural constraints:

<ul> <li> “Commutativity” or “symmetry”: If the ordered multisets $S,T,U,V \subset [n]$ are such that $S \cup T = U \cup V$ as unordered multisets, then $X_{S\cup T} = X_{U \cup V}$. This is meant to reflect the commutative property of monomials. That is, for any $x\in \R^n$ \[ \prod_{i \in S}x_i \cdot \prod_{j\in T}x_j = \prod_{k \in U}x_k \cdot \prod_{\ell \in V} x_{\ell}. \] </li><li> “Normalization”: we set $X_{\emptyset} = 1$. This is the “scale” of the coefficients. One way to see that this is the correct scale is to think of $X_{\emptyset}$ as the monomial multiplier of a polynomial's constant term. </li><li> “PSDness”: we require that $X \succeq 0$, or that $X$ is positive-semidefinite. This constraint is natural because for any point $y \in \R^n$, if we take the matrix $X=X_y$ given by setting $X_{S} = \prod_{i\in S} y_i$, the resulting matrix $X$ is PSD. The proof is that if we take the vector $\tilde{y}$ so that $\tilde{y}^{\top} = [1 \ y^\top]$, then $X_y = \tilde{y}^{\otimes d}(\tilde{y}^{\otimes d})^\top$ (where $y^{\otimes d}$ is the $d$th Kronecker power<sup><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span class="sidenote" id="footnote1"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a> The Kronecker product of an $n \times m$ matrix $A$ and a $\ell \times k$ matrix $B$ is a $n\ell \times mk$ matrix $A \otimes B$, which we can naturally index by pairs so that the $(a,b),(c,d)$th entry is the product of $A_{ac}B_{bd}$. So, the Kronecker product of an $n \times 1$ vector $x$ with itself is a $n^2 \times 1$ vector whose $(i,j)$th entry is simply the product $x_ix_j$. </span> of $y$ ), and thus for any vector $v$, $v^\top X_yv = \langle v, \tilde{y}^{\otimes d}\rangle^2 \ge 0$.
<p>

</p></li></ul>


<p>

</p><blockquote><b>Remark 1</b> <em> Notice that any feasible solution $y \in \R^n$ for the program $Q$ yields a feasible solution to $\sos_d(Q)$: we just assign $X_S := \prod_{i\in S}y_i$, and the above arguments show that this is feasible. </em></blockquote>

<p>


</p><p>
One nice consequence of these constraints is that, if we evaluate the square of some degree-$d$ polynomial $q$ in the SDP monomials, the polynomial value will be non-negative! This is because, if $\hat q$ is the vector of coefficients of the polynomial $q$, then $q^2(X) = \hat{q}^\top X \hat{q} \ge 0$.
</p><p>

</p><blockquote><b>Example 2</b> <em> Consider the square polynomial $(x_1 + c \cdot x_2)^2$. We would evaluate this square in $X$ by taking the quadratic form \[ \begin{bmatrix} 0 &amp; 1 &amp; c \end{bmatrix} \begin{bmatrix} X_{\emptyset} &amp; X_{\{1\}} &amp; X_{\{2\}} \\ X_{1} &amp; X_{\{1,1\}} &amp; X_{\{1,2\}} \\ X_{2} &amp; X_{\{2,1\}} &amp; X_{\{2,2\}}\\ \end{bmatrix} \begin{bmatrix} 0\\ 1 \\ c \end{bmatrix} = X_{1,1} + c\cdot X_{1,2} + c\cdot X_{2,1} + c^2 \cdot X_{2,2}. \] </em></blockquote>

<p>


</p><p>
<br/>

We now formalize the above definition.
</p><blockquote><b>Definition 1 (Sum-of-Squares Relaxation for $Q$ at degree $2d$)</b> <em> Given a polynomial optimization problem $Q$ with $\deg(p)\le 2d$ and $\deg(g_i) \le 2d\ \forall i \in [m]$, we define the <em>degree-$2d$ sum-of-squares relaxation</em> for $Q$, $\sos_{d}(Q)$.
</em><p><em>
We define a variable $X_{S}$ for each unordered multiset $S \subset [n]$ of size $|S| \le 2d$, and define the $(n+1)^d \times (n+1)^d$ matrix $X$, with rows and columns indexed by ordered multisets $U,V \subset [n]$, so that the $U,V$th entry of $X$ contains the variable $X_{U\cup V}$. Define the linear operator $\tilde{E}:\text{polynomials}_{\le 2d}\to \R$ such that $\tilde{E}[\prod_{i\in S} x_i] = X_S$ for $|S| \le 2d$. Then, \[ \sos_d(Q) = \left\{ \max \ \tilde{E}[p(x)] \quad s.t.\quad \begin{aligned} &amp;X \succeq 0,\\ &amp;X_{\emptyset} = 1,\\ &amp;\tilde{E}[ g_i(X)\cdot\prod_{i\in U} x_i] = 0\quad \forall i \in [m], U \subset [n], \deg(g_i) + |U| \le 2d \end{aligned} \right\} \] </em></p></blockquote>

<p>


</p><p>

</p><p>
<b>Sum-of-Squares Hierarchy.</b> Earlier, we only mentioned that we must have $2d \ge \deg(p), \deg(g_i) \forall i \in [m]$. In fact, we can choose $d$ to be as large as we wish--as long as we are willing to solve an SDP with $n^{O(d)}$ variables and $n^{O(d)}$ constraints. Taking successively larger values for $d$ gives us a systematic way of adding constraints to our program, giving us a family of larger but more powerful programs as we increase the value of $d$; this is why we call the family of relaxations $\{\sos_d\}_{d = 1}^{\infty}$ the <em>sum-of-squares hierarchy</em>.
</p><p>

</p><p>
<b>How to make sense of the Sum-of-Squares SDP relaxation?</b> In the Goemans-Williamson SDP relaxation, there is a natural interpretation of the SDP as a vector program: if we view the positive semidefinite matrix solution to the SDP, $X$, according to its Cholesky decomposition $X = VV^{\top}$, then we can identify each node in the underlying graph $G$ with a unit vector corresponding to a row of the matrix $V$, and we can see that the objective function tries to push vectors corresponding to adjacent nodes apart on the unit sphere.
</p><p>
This geometric intuition is extremely crisp, but unfortunately it is hard to come up with an analogue of this in programs where we care about more than $2$ variables interacting at a time (i.e. when we have variables $X_S$ with $|S| \ge 3$). <em>As of now, we do not have a similar geometric understanding of general sum-of-squares SDP relaxations.</em> We can instead develop alternative ways to (partially) understand these SDP relaxations.
</p><p>

</p><p>
<b>Pseudomoments.</b> As a start, one perspective is to think of the variables $X_S$ as the “moments” of a fake distribution over solutions to the program $Q$.
</p><p>
If we were to actually solve the (non-convex) problem $Q$ (using some inefficient algorithm), what we would have is either a single solution $y^*\in \R^n$, or a distribution over some set of solutions $Y\subset \R^n$, which maximize $p$, so that \[ OPT(Q) = \E_{y \in Y} [p(y)]. \] We cannot expect that the solution to the relaxation $\sos_d(Q)$ comes from an <em>actual</em> distribution over feasible solutions, but our constraints ensure that it still satisfies some of the properties of actual distributions.<sup><a href="http://learningwitherrors.org/atom.xml#footnote2">2</a></sup><span class="sidenote" id="footnote2"><a href="http://learningwitherrors.org/atom.xml#footnote2" name="footnote2">2.</a>  Because of our constraints on $\sos_d(Q)$, the pseudoexpectation $\tilde{E}$ satisfies linearity of expectation \[ \tilde{E}[\alpha\cdot q_1(x) + \beta \cdot q_2(x)] = \alpha \cdot \tilde{E}[q_1(x)] + \beta \cdot \tilde{E}[q_2(x)]\quad \text{if} \quad \deg(q_1),\deg(q_2) \le 2d,\]  and also the non-negativity of low-degree squares, \[ \tilde{E}[q(x)^2] \ge 0 \quad \text{if} \quad \deg(q)\le d. \]
</span></p><p>
 
 For this reason we can also call the solution to $\sos_d(Q)$ a <em>pseudodistribution</em>, and that is why we use the notation \[ \tilde{E}\left[\prod_{i\in S} x_i \right] = X_S. \] In other words, we interpret the variable $X_S$ as being the <em>pseudomoment</em> of the monomial $\prod_{i\in S} x_i$ under a <em>pseudodistribution</em> over solutions to $Q$.
</p><p>
Thinking about the SDP solution in this way can be helpful in designing algorithms (and in proving lower bounds), but I will not discuss this perspective further here (maybe in a future post).
</p><p>
</p><h2 class="tex">2. Sum-of-Squares Proofs </h2>
<p>
One immediate question is, why should the sum-of-squares relaxation be a good relaxation? When we design SDP algorithms for maximization problems, we want to bound \[ OPT(Q) \le OPT(\sos_d(Q)) \le \alpha \cdot OPT(Q), \] for $\alpha$ as close to $1$ as possible. Why should we expect $\alpha$ to be small?
</p><p>
We can give a concrete but somewhat technical answer to this question by considering the dual program: the dual program will give us a “sum-of-squares” proof of an upper bound on the primal program. In my opinion this is most easily explained via demonstration, so let's write down the dual program.
</p><p>
For convenience, we'll start by re-writing the primal program $\sos_d(Q)$ in a matrix-based notation. For two matrices $A,B$ of equal dimension, define the inner product $\langle A,B \rangle = \sum_{(i,j)} A_{ij} B_{ij}$. Now, define $(n+1)^d \times (n+1)^d$ matrices $P,G_1,\ldots,G_{m}$ so that $\langle P, X \rangle = \tilde{E}[p(x)]$ and $\langle G_i,X\rangle = \tilde{E}[g_i(x)]$ (where have redefined the polynomial constraints $g_1,\ldots,g_m$ to include most of our SDP constraints: symmetry/commutativity, and $g_i(x)\cdot X_U = 0$).
</p><blockquote><b>Example 3</b> <em> If we have $p(x) = \sum_{i}x_i^2$, then one could choose the matrix $P$ to contain the identity in the submatrix indexed by sets of cardinality $1$, and $0$ elsewhere. </em></blockquote>

<p>


</p><p>
Our program can now be written as the minimization problem, \[ \sos_d'(Q) = \left\{ \min_{X\succeq 0} - \langle P,X\rangle \quad s.t.\quad \langle G_i,X\rangle = 0 \quad \forall i \in [m], \langle J_{\emptyset},X\rangle = 1 \right\}, \] where $J_{\emptyset}$ is the matrix with a single $1$ in the entry $\emptyset,\emptyset$ and zeros elsewhere, and the constraint $\langle J_\emptyset, X\rangle = 1$ enforces normalization. The optimal value of $\sos_d(Q)'$ is the negation of the optimal value of $\sos_d(Q)$.
</p><p>
The dual is the SDP problem \[ \sos_d^+(Q) = \left\{ \max_{y\in \R^{m+1}} y_{\emptyset} \qquad s.t.\quad \left(-P - y_{\emptyset}\cdot J_{\emptyset} - \sum_{j\in[m]}y_j \cdot G_j\right) = S \succeq 0 \right\}. \] Fixing $y^*$ to be the optimal dual point, from the dual constraints we have that \[ P = -y^*_{\emptyset} \cdot J_{\emptyset} - S + \sum_{j} y^*_j\cdot G_j. \] By duality, we have that in the optimal solution of $\sos_d^+(Q)$, \[ y_{\emptyset}^*= c + OPT(\sos'_d(Q) = c - OPT(\sos_d(Q)) \] for some $c \ge 0$, and therefore taking $S' = S + c\cdot J_{\emptyset} \succeq 0$, \[ P = OPT \cdot J_{\emptyset} - S' + \sum_{j} y^*_j\cdot G_j. \]
</p><p>
We will turn this matrix equation into a polynomial equation. Let $x \in \R^n$, and let $\tilde{x} = [1 \ x^{\top}]^\top$. Now, let $S'$ have the Cholesky decomposition $S' =\sum ss^{\top}$. We take the quadratic form of the Kronecker power of $\tilde{x}$ with the left- and right-hand sides, \begin{align*} (\tilde{x}^{\otimes d})^{\top} P (\tilde{x}^{\otimes d}) &amp;= OPT - \sum \langle s, \tilde{x}^{\otimes d}\rangle^2 + \sum_{j\in[m]} y_j^* \cdot (\tilde{x}^{\otimes d})^{\top} G_j (\tilde{x}^{\otimes d}) \end{align*} and re-writing each of the above vector products as polynomials, where $q_s$ is the polynomial encoded by the vector of coefficients $s$ \begin{align*} p(x) &amp;= OPT -\sum q_s(x)^2 - \sum_{j\in[m]} y_j^* \cdot g_j(x). \end{align*} This final line is a <em>sum-of-squares proof</em> that the value of $p(x)$ cannot exceed $OPT(\sos_d(Q))$ on the feasible region: any feasible point $x \in \R^n$ evaluates to $0$ for each $g_i$, and the square polynomials $q_s(x)^2$ can never contribute positively to the right-hand side. We have thus proven the following theorem:
</p><blockquote><b>Theorem 2</b> <em> The dual of the SDP $\sos_d(Q)$ provides a degree-$d$ sum-of-squares proof that $p(x) \le OPT(\sos_d(Q))$ for all $x$ in the feasible region of $Q$. </em></blockquote>

<p>


</p><p>
At the start of this section, our goal was to understand how to bound \[ OPT(Q) \le OPT(\sos_d(Q)) \le \alpha \cdot OPT(Q). \] This theorem gives us a primal-dual tool for bounding the value of $\sos_d(Q)$--{if we can provide a sum-of-squares proof of degree at most $d$ that $p(x) \le \alpha\cdot OPT(Q)$, then that sum-of-squares proof is a valid dual certificate!}
</p><p>

</p><p>
<b>Degree of the proof.</b> Notice that the dual can only use polynomials of degree at most $d$ in the sum-of-squares proof. So, suppose now that we write down two SDP relaxations for $Q$: $\sos_d(Q)$ and $\sos_{d'}(Q)$ for some $d' &gt; d$. Then clearly, \[ OPT(\sos_{d}(Q)) \ge OPT(\sos_{d'}(Q)) \ge OPT(Q), \] since the degree-$d'$ sum-of-squares program contains more constraints than the degree-$d$ program.
</p><p>
In the primal, it is difficult to understand exactly what these additional constraints buy you. From the perspective of the dual, the power of these additional constraints becomes clearer: the dual now has access to sum-of-squares proofs that use polynomials of <em>higher degree</em>, and this additional power may allow the dual to prove a potentially tighter upper bound. This is still a relatively mysterious condition, but in a later post I will give some concrete examples of situations in which it helps.
</p><p>
</p><h2 class="tex">3. Planted Sparse Vector </h2>
<p>
In this section, we give one algorithmic application: the planted sparse vector problem.
</p><p>
 Given an $n \times d$ matrix $A$, distinguish between the following two cases:

</p><ul> <li> If the columns of $A$ are uniformly sampled from a $d$-dimensional subspace of $\R^n$ which contains a vector with at most $k &lt; n/100$ nonzero entries, return YES, </li><li> If the columns of $A$ are sampled from a uniformly random $d$-dimensional subspace of $\R^n$, return NO with high probability.
</li></ul>

  This is a somewhat simple variant of the problem--other variants ask you to find the sparse vector as well. The exposition for this pared-down “distinguishing” version is simpler, and gets the main ideas across.
<p>
Without loss of generality, we may apply a random rotation $R \in \R^{d\times d}$ to the columns of $A$, then normalize by the maximum column norm, so that we work with $A \leftarrow \max_{i\in[d]} \frac{1}{\|ARe_i\|}AR$. This is to ensure that the columns of $A$ have roughly the same norm, are roughly orthogonal, and all have norm roughly $1$--for the remainder of the post we will assume that these conditions all hold.
</p><p>
We introduce the following polynomial optimization problem $Q_{sparse}$ for the planted sparse vector problem: \[ Q_{sparse}(A) = \left\{ \max_{x \in R^d} \| Ax \|^4_4 \qquad s.t. \qquad \|x\|^2_2 = 1 \right\} \] In other words, we want to find the linear combination of the columns of $A$ that will maximize the $4$-norm of $A$, while having $2$-norm roughly $1$. This program picks out sparse vectors over balanced vectors: a unit vector $e_i$ with only one nonzero entry has $\|e_i\|^2_2 = \|e_i\|^4_4 = 1$, while a unit vector $v$ with all $n$ entries of the same magnitude has $\|v\|_4^4 = n \cdot (1/\sqrt{n})^4 = 1/n \ll \|v\|_2^2$.
</p><p>
We prove the following theorem:
</p><blockquote><b>Theorem 3</b> <em>(Barak-Brandao-Harrow-Kelner-Steurer-Zhao '12) <a name="thmplsp"/> If $1/k \ge \tilde{O}(\sqrt{d^3/n^3} + 1/n)$, then $\sos_4(Q_{sparse}(A))$ solves the planted $k$-sparse vector in a random subspace problem. </em></blockquote>

<p>


</p><p>
We will prove this by showing that the value of the program is large in the planted case, and small in the random case. It is actually possible to prove a better tradeoff between $k,d$ and $n$, but to simplify the arguments, we prove a weaker theorem. For the full details, see [Barak-Brandao-Harrow-Kelner-Steurer-Zhao '12].
</p><p>
<em>Proof:</em>  If the span of the columns of $A$ actually contains a $k$-sparse vector $v^*$, then $\|v^*\|_4^4$ is minimized when all entries of $v^*$ have equal magnitude. So, if we normalize $v^*$ so that $\|v^*\| = 1$, \[ \|v^*\|^4_4 \le k\cdot \left(\frac{1}{\sqrt{k}}\right)^{4} = \frac{1}{k}. \]
</p><p>
We will show that in the random case, the value is bounded by a function of $n$ and $d$:
</p><blockquote><b>Lemma 4</b> <em><a name="lemrandomcase"/> If $A$ has iid Gaussian columns with $\E[A_{ij}^2] = \frac{1}{n}$, then with high probability the program $\sos_4(Q_{sparse}(A))$ has optimal value $\tilde{O}(\sqrt{d^3/n^3} + 1/n)$. </em></blockquote>

<p>

 Given this lemma, the proof of Theorem <a href="http://learningwitherrors.org/atom.xml#thmplsp">3</a> is essentially trivial--we know that the objective value at most $\tilde{O}(\sqrt{d^3/n^3} + 1/n)$ with high probability in the random case, and at least $1/k$ in the planted case, and so the objective value of $\sos_4(Q)$ distinguishes so long as $1/k \ge\tilde{O}(\sqrt{d^3/n^3} + 1/n)$. $$\tag*{$\blacksquare$}$$
</p><p>
Now, we prove the lemma, using sum-of-squares proofs to bound the objective value of the SDP in the random case. <em>Proof of Lemma <a href="http://learningwitherrors.org/atom.xml#lemrandomcase">4</a>:</em>  For any $d^2 \times d^2$ matrix $M$, there is a sum-of-squares proof of the following fact: \begin{align*} \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ M\right\rangle &amp;\le \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ \|M\|\cdot \Id \right\rangle. \end{align*} The proof simply follows because $\|M\|\cdot \Id \succeq M$, and therefore $M = \|M\|\cdot \Id - S$ for some $S \succeq 0$; by taking the Cholesky decomposition of $S$ and using the vectors as polynomial coefficients, this gives us a sum-of-squares proof of the inequality.
</p><p>
We will use this sum-of-squares fact to bound the SDP value of our objective function. First, we re-interpret our objective function as an inner product of two matrices. Let $a_1,\ldots,a_n$ be the rows of $A$. We will re-write our objective function as a matrix inner-product: \begin{align*} \|Ax\|^4_4 &amp;= \sum_{i}\langle a_i, x \rangle^4 = \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ \sum_i (a_i \otimes a_i)(a_i \otimes a_i)^{\top}\right\rangle. \end{align*} At this point, we could apply the above trick, but unfortunately, the maximum eigenvalue of $\sum_i (a_i\otimes a_i)(a_i \otimes a_i)^{\top}$ is $\approx d/n$--much larger than our goal of $\sqrt{d^3/n^3}$. This is because at indices $(\alpha\beta,\gamma\delta)$ where $\alpha = \beta$ and $\gamma = \delta$, our matrix has positive entries, whereas most entries have a random sign. These positive entries create a large eigenvalue in the matrix.
</p><p>
So, we will decompose this further--we will separate the portion of the matrix with entries corresponding to even-multiplicity indices. Define $B_{\neq}$ to be the matrix $\sum_{i} (a_i \otimes a_i)(a_i \otimes a_i)^{\top}$ in which all even-multiplicity entries are zeroed out. \begin{align*} \|Ax\|^4_4 &amp;= \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ B_{\neq} \right\rangle + \sum_{\alpha,\beta \in [n]} x_{\alpha}^2x_{\beta}^2\cdot \sum_{i} a_{i}(\alpha)^2 a_{i}(\beta)^2. \end{align*} By the above arguments, there is a sum-of-squares proof that \begin{align*} \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ B_{\neq}\right\rangle &amp;\le \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ \|B_{\neq}\|\cdot \Id \right\rangle\\ &amp;= \|B_{\neq}\|\cdot \sum_{\alpha,\beta\in[d]} x_\alpha^2 x_{\beta}^2 = \|B_{\neq}\|\cdot \left(\sum_{\alpha\in[d]} x_{\alpha}^2 \right)^2. \end{align*}
</p><p>
For the other term, we will use an even simpler bound. Let $c_{\alpha,\beta} = \sum_{i} a_i(\alpha)^2 a_j(\beta)^2$, for convenience. Also, let $c^* = \max_{\alpha,\beta} c_{\alpha,\beta}$. The following equality, \[ \sum_{\alpha,\beta\in[d]}c_{\alpha,\beta}\cdot x_{\alpha}^2 x_{\beta}^2 = c^*\cdot \sum_{\alpha, \beta} x_{\alpha}^2 x_{\beta}^2 - \left(\sum_{\alpha,\beta} (c^* - c_{\alpha,\beta}) \cdot x_{\alpha}^2 x_{\beta}^2\right), \] is a sum-of-squares proof that \[ \sum_{\alpha,\beta\in[d]}c_{\alpha,\beta}\cdot x_{\alpha}^2 x_{\beta}^2 \le c^*\cdot \sum_{\alpha, \beta} x_{\alpha}^2 x_{\beta}^2, \] because $c^* - c_{\alpha,\beta} \ge 0$ for all $\alpha,\beta$ by definition, and thus the parenthesized term is a sum-of-squares.
</p><p>
Putting the two arguments together, we have a sum-of-squares proof that \[ \|Ax\|_4^4 \le \left(\|B_{\neq}\| + c^*\right)\cdot \left(\sum_{\alpha} x_{\alpha}^2\right) \] Because we have the SDP constraint that $\|x\|_2^2 = 1$, the objective value is thus bounded by \[ \tilde{E}[\|Ax\|_4^4] = (\|B_{\neq} \|+ c^*) \cdot \tilde{E}\left[\left(\sum_{\alpha\in [d]}x_{\alpha}^2\right)^2\right] = \|B_{\neq}\| + c^*. \] The final step in the proof consists of showing that with high probability over the choice of $A$, \[ \|B_{\neq}\| \le \tilde{O}(\sqrt{d^3/n^3})\quad \text{and}\quad c^* \le \tilde{O}(1/n). \] The first fact we can prove using a matrix Chernoff bound, and the second fact we can prove using a Chernoff bound and a union bound. This concludes the proof! $$\tag*{$\blacksquare$}$$
</p><p>

To get the theorem with the better parameters mentioned above, Barak et al.
remove the even-multiplicity indeces more carefully: they project away from the subspace containing the vectors which correlate too much with the even-multiplicity entries (whereas we just zeroed them out).
This more careful treatment lets them prove a better matrix concentration result.
</p><p>

</p><h2 class="tex">4. Other Resources </h2> Check out the following other resources for historical details and more sum-of-squares algorithms/lower bounds:

<ul> <li> For notes about SDPs and duality, I like these notes by Lap Chi Lau:
<p>
<a href="https://cs.uwaterloo.ca/ lapchi/cs270/notes.html">https://cs.uwaterloo.ca/ lapchi/cs270/notes.html</a>
</p><p>
I also like these notes by Anupam Gupta and Ryan O'Donnell:
</p><p>
<a href="https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15859-f11/www/">https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15859-f11/www/</a> </p></li><li> Lecture notes from Boaz Barak on sum-of-squares:
<p>
<a href="http://www.boazbarak.org/sos/">http://www.boazbarak.org/sos/</a>
</p><p>
</p></li><li> Lecture notes from Massimo Lauria on sum-of-squares and other relaxations for polynomial optimization <a href="http://www.csc.kth.se/ lauria/sos14/">http://www.csc.kth.se/ lauria/sos14/</a> </li><li> The introduction of this paper by Barak, Kelner and Steurer: <a href="https://arxiv.org/pdf/1312.6652v1.pdf">https://arxiv.org/pdf/1312.6652v1.pdf</a>
<p>
The appendix of the paper also contains many sum-of-squares proofs of basic inequalities (e.g. Cauchy-Schwarz) that can be of use for providing good dual certificates.
</p></li></ul></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-06-23T00:00:00Z</updated>
    <published>2016-06-23T00:00:00Z</published>
    <author>
      <name>Tselil Schramm</name>
    </author>
    <source>
      <id>http://learningwitherrors.org/atom.xml</id>
      <link href="http://learningwitherrors.org/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="http://learningwitherrors.org" rel="alternate" type="text/html"/>
      <title>Learning With Errors</title>
      <updated>2017-01-04T04:27:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://learningwitherrors.org/2016/06/03/small-bias</id>
    <link href="http://learningwitherrors.org/2016/06/03/small-bias/" rel="alternate" type="text/html"/>
    <title>Simple Lower Bounds for Small-bias Spaces</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div style="display: none;"/>
<p>
I was reading about PRGs recently, and I think a lemma mentioned last time (used for Johnson-Lindenstrauss lower-bounds) can give simple lower-bounds for $\epsilon$-biased spaces.
</p><p>
Notice:

</p><ul> <li> $2^n$ mutually orthogonal vectors requires dimension at least $2^n$, but $2^n$ “almost orthogonal” vectors with pairwise inner-products $|\innp{v_i, v_j}| \leq \epsilon$ exists in dimension $O(n/\epsilon^2)$, by Johnson-Lindenstrauss. </li><li> Sampling $n$ iid uniform bits requires a sample space of size $2^n$, but $n$ $\epsilon$-biased bits can be sampled from a space of size $O(n/\epsilon^2)$.
</li></ul>


<p>
First, let's look at $k$-wise independent sample spaces, and see how the lower-bounds might be extended to the almost $k$-wise independent case.
</p><p>
<i>Note: To skip the background, just see Lemma <a href="http://learningwitherrors.org/atom.xml#lemrank">1</a>, and its application in Claim <a href="http://learningwitherrors.org/atom.xml#claimkeps">3</a>. </i>
<!--more-->
</p><p>
</p><h2 class="tex">1. Preliminaries </h2> What “size of the sample space” means is: For some sample space $S$, and $\pm 1$ random variables $X_i$, we will generate bits $x_1, \dots x_n$ as an instance of the r.vs $X_i$. That is, by drawing a sample $s \in S$, and setting $x_i = X_i(s)$. We would like to have $|S| \ll 2^n$, so we can sample from it using less than $n$ bits.
<p>
Also, any random variable $X$ over $S$ can be considered as a vector $\t X \in \R^{|S|}$, with coordinates $\t X[s] := \sqrt{\Pr[s]} X(s)$. This is convenient because $\innp{\t X, \t Y} = \E[XY]$.
</p><p>
</p><h2 class="tex">2. Exact $k$-wise independence </h2> <a name="seckwise"/> A distribution $D$ on $n$ bits is <em>$k$-wise independent</em> if any subset of $k$ bits are iid uniformly distributed. Equivalently, the distribution $D : \{\pm 1\}^n \to \R_{\geq 0}$ is $k$-wise independent iff the Fourier coefficients $\hat D(S) = 0$ for all $S \neq 0, |S| \leq k$.
<p>
$n$ such $k$-wise independent bits can be generated from a seed of length $O(k \log n)$ bits, using say Reed-Solomon codes. That is, the size of the sample space is $n^{O(k)}$. This size is optimal, as the below claim shows (adapted from Umesh Vazirani's lecture notes [<a href="http://learningwitherrors.org/atom.xml#ref-Vaz99">Vaz99</a>]).
</p><blockquote><b>Claim 1</b> <em> <a name="claimkwise"/> Let $D$ be a $k$-wise independent distribution on $\{\pm 1\}$ random variables $x_1, \dots, x_n$, over a sample space $S$. Then, $|S| = \Omega_k(n^{k / 2})$. </em></blockquote>

<p>


</p><p>
<em>Proof:</em>  For subset $T \subseteq [n]$, let $\chi_T(x) = \prod_{i \in T} x_i$ be the corresponding Fourier character. Consider these characters as vectors in $\R^{|S|}$ as described above, with \[\innp{\chi_A, \chi_B} = \E_{x \sim D}[\chi_A(x)\chi_B(x)] \]
</p><p>
 Let $J$ be the family of all subsets of size $\leq k/2$. Note that, for $A, B \in J$, the characters $\chi_A, \chi_B$ are orthogonal: \begin{align*} \innp{\chi_A, \chi_B} &amp;= \E_{x \sim D}[\chi_A(x)\chi_B(x)]\\ &amp;= \E_{x \sim D}[(\prod_{i \in A \cap B} x_i^2)(\prod_{i \in A \Delta B} x_i)]\\ &amp;= \E_{x \sim D}[\chi_{A \Delta B}(x)] \note{since $x_i^2 = 1$}\\ &amp;= 0 \note{since $|A \Delta B| \leq k$, and $D$ is $k$-wise independent} \end{align*} Here $A \Delta B$ denotes symmetric difference, and the last equality is because $\chi_{A \Delta B}$ depends on $\leq k$ variables, so the expectation over $D$ is the same as over iid uniform bits.
</p><p>
 Thus, the characters $\{\chi_A\}_{A \in J}$ form a set of $|J|$ mutually-orthogonal vectors in $\R^{|S|}$. So we must have $|S| \geq |J| = \Omega_k(n^{k/2})$. $$\tag*{$\blacksquare$}$$
</p><p>
The key observation was relating independence of random variables to linear independence (orthogonality). Similarly, we could try to relate $\epsilon$-almost $k$-wise independent random variables to almost-orthogonal vectors.
</p><p>
</p><h2 class="tex">3. Main Lemma </h2> This result is Theorem 9.3 from Alon's paper [<a href="http://learningwitherrors.org/atom.xml#ref-Alo03">Alo03</a>]. The proof is very clean, and Section 9 can be read independently. <sup><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span class="sidenote" id="footnote1"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a>  Theorem 9.3 is stated in terms of lower bounding the rank of a matrix $B \in \R^{N \x N}$ where $B_{i,i} = 1$ and $|B_{i, j}| \leq \epsilon$. The form stated here follows by defining $B_{i, j} := \innp{v_i, v_j}$.  </span>
<p>

</p><blockquote><b>Lemma 1</b> <em> <a name="lemrank"/> Let $\{v_i\}_{i \in [N]}$ be a collection of $N$ unit vectors in $\R^d$, such that $|\innp{v_i, v_j}| \leq \epsilon$ for all $i \neq j$. Then, for $\frac{1}{\sqrt{N}} \leq \epsilon \leq 1/2$, \[d \geq \Omega\left(\frac{\log N}{\epsilon^2 \log(1/\epsilon)}\right)\] </em></blockquote>

<p>


</p><p>
This lower-bound on the dimension of “almost-orthogonal” vectors translates to a nearly-tight lower-bound on Johnson-Lindenstrauss embedding dimension, and will also help us below.
</p><p>
</p><h2 class="tex">4. Small bias spaces </h2> A distribution $D$ on $n$ bits is <em>$\epsilon$-biased w.r.t linear tests</em> (or just “$\epsilon$-biased”) if all $\F_2$-linear tests are at most $\epsilon$-biased. That is, for $x \in \{\pm 1\}^n$, the following holds for all subsets $S \subseteq [n]$: \[\left|\E_{x \sim D}[\chi_S(x)]\right| = \left|\Pr_{x \sim D}[\chi_S(x) = 1] - \Pr_{x \sim D}[\chi_S(x) = -1]\right| \leq \epsilon\] Similarly, a distribution is <em>$\epsilon$-biased w.r.t. linear tests of size $k$</em> (or “$k$-wise $\epsilon$-biased) if the above holds for all subsets $S$ of size $\leq k$.
<p>
There exists an $\epsilon$-biased space on $n$ bits of size $O(n / \epsilon^2)$: a set of $O(n / \epsilon^2)$ random $n$-bit strings will be $\epsilon$-biased w.h.p. Further, explicit constructions exist that are nearly optimal: the such first construction was in [<a href="http://learningwitherrors.org/atom.xml#ref-NN93">NN93</a>], and was nicely simplified by [<a href="http://learningwitherrors.org/atom.xml#ref-AGHP92">AGHP92</a>] (both papers are very readable).
</p><p>
These can be used to sample $n$ bits that are $k$-wise $\epsilon$-biased, from a space of size almost $O(k \log(n)/\epsilon^2)$; much better than the size $\Omega(n^k)$ required for perfect $k$-wise independence. For example<sup><a href="http://learningwitherrors.org/atom.xml#footnote2">2</a></sup><span class="sidenote" id="footnote2"><a href="http://learningwitherrors.org/atom.xml#footnote2" name="footnote2">2.</a>  This can be done by composing an $(n, k')$ ECC with dual-distance $k$ and an $\epsilon$-biased distribution on $k' = k\log n$ bits. Basically, use a linear construction for generating $n$ exactly $k$-wise independent bits from $k'$ iid uniform bits, but use an $\epsilon$-biased distribution on $k'$ bits as the seed instead.  </span>, see [<a href="http://learningwitherrors.org/atom.xml#ref-AGHP92">AGHP92</a>] or the lecture notes [<a href="http://learningwitherrors.org/atom.xml#ref-Vaz99">Vaz99</a>].
</p><p>
</p><h3 class="tex">4.1. Lower Bounds</h3> The best lower bound on size of an $\epsilon$-biased space on $n$ bits seems to be $\Omega(\frac{n}{\epsilon^2 \log(1/\epsilon)})$, which is almost tight. The proofs of this in the literature (to my knowledge) work by exploiting a nice connection to error-correcting codes: Say we have a sample space $S$ under the uniform measure. Consider the characters $\chi_T(x)$ as vectors $\t \chi_T \in \{\pm 1\}^{|S|}$ defined by $\t \chi_T[s] = \chi_T(x(s))$, similar to what we did in Section <a href="http://learningwitherrors.org/atom.xml#seckwise">2</a>. The set of $2^n$ vectors $\{\t \chi_T\}_{T \subseteq [n]}$ defines the codewords of a linear code of length $|S|$ and dimension $n$. Further, the hamming-weight of each codeword (number of $-1$s in each codeword, in our context), is within $n(\frac{1}{2} \pm \epsilon)$, since each parity $\chi_T$ is at most $\epsilon$-biased. Thus this code has relative distance at least $\frac{1}{2} - \epsilon$, and we can use sphere-packing-type bounds from coding-theory to lower-bound the codeword length $|S|$ required to achieve such a distance. Apparently the “McEliece-Rodemich-Rumsey-Welch bound” works in this case; a more detailed discussion is in [<a href="http://learningwitherrors.org/atom.xml#ref-AGHP92">AGHP92</a>, Section 7].
<p>
We can also recover this same lower bound using Lemma <a href="http://learningwitherrors.org/atom.xml#lemrank">1</a> in a straightforward way.
</p><p>

</p><blockquote><b>Claim 2</b> <em> <a name="claimepsbias"/> Let $D$ be an $\epsilon$-biased distribution on $n$ bits $x_1, \dots, x_n$, over a sample space $S$. Then, \[|S| = \Omega\left(\frac{n}{\epsilon^2 \log(1/\epsilon)}\right)\] </em></blockquote>

<p>

 <em>Proof:</em>  Following the proof of Claim <a href="http://learningwitherrors.org/atom.xml#claimkwise">1</a>, consider the Fourier characters $\chi_T(x)$ as vectors $\t \chi_T \in \R^{|S|}$, with $\t \chi_T[s] = \sqrt{\Pr[s]} \chi_T(x(s))$. Then, for all distinct subsets $A, B \subseteq [n]$, we have \[\innp{\t \chi_A, \t \chi_B} = \E_{x \sim D}[\chi_A(x)\chi_B(x)] = \E_{x \sim D}[\chi_{A \Delta B}(x)]\] Since $D$ is $\epsilon$-biased, $\left|\E_{x \sim D}[\chi_{A \Delta B}(x)]\right| \leq \epsilon$ for all $A \neq B$. Thus, applying Lemma <a href="http://learningwitherrors.org/atom.xml#lemrank">1</a> to the collection of $N = 2^n$ unit vectors $\{\t \chi_T\}_{T \subseteq [n]}$ gives the lower bound $|S| = \Omega\left(\frac{n}{\epsilon^2 \log(1/\epsilon)}\right)$. $$\tag*{$\blacksquare$}$$
</p><p>
This also nicely generalizes the proof of Claim <a href="http://learningwitherrors.org/atom.xml#claimkwise">1</a>, to give an almost-tight lower bound on spaces that are $\epsilon$-biased w.r.t linear tests of size $k$.
</p><p>

</p><blockquote><b>Claim 3</b> <em> <a name="claimkeps"/> Let $D$ be a distribution on $n$ bits that is $\epsilon$-biased w.r.t. linear tests of size $k$. Then, the size of the sample space is \[|S| = \Omega\left(\frac{k \log (n/k)}{\epsilon^2 \log(1/\epsilon)}\right)\] </em></blockquote>

<p>

 <em>Proof:</em>  As before, consider the Fourier characters $\chi_T(x)$ as vectors $\t \chi_T \in \R^{|S|}$, with $\t \chi_T[s] = \sqrt{\Pr[s]} \chi_T(x(s))$. Let $J$ be the family of all subsets $T \subseteq [n]$ of size $\leq k/2$. Then, for all distinct subsets $A, B \in J$, we have \[\left|\innp{\t \chi_A, \t \chi_B}\right| = \left|\E_{x \sim D}[\chi_{A \Delta B}(x)]\right| \leq \epsilon\] since $|A \Delta B| \leq k$, and $D$ is $\epsilon$-biased w.r.t such linear tests. Applying Lemma <a href="http://learningwitherrors.org/atom.xml#lemrank">1</a> to the collection of $|J|$ unit vectors $\{\t \chi_T\}_{T \in J}$ gives $|S| = \Omega(\frac{k \log (n/k)}{\epsilon^2 \log(1/\epsilon)})$. $$\tag*{$\blacksquare$}$$
</p><p>
<i>Note: I couldn't find the lower bound given by Claim <a href="http://learningwitherrors.org/atom.xml#claimkeps">3</a> in the literature, so please let me know if you find a bug or reference.
</i></p><p><i>
Also, these bounds do not directly imply nearly tight lower bounds for <em>$\epsilon$-almost $k$-wise independent</em> distributions (that is, distributions s.t. their marginals on all sets of $k$ variables are $\epsilon$-close to the uniform distribution, in $\ell_{\infty}$ or $\ell_{1}$ norm). Essentially because of the loss in moving between closeness in Fourier domain and closeness in distributions. <sup><a href="http://learningwitherrors.org/atom.xml#footnote3">3</a></sup><span class="sidenote" id="footnote3"><a href="http://learningwitherrors.org/atom.xml#footnote3" name="footnote3">3.</a>  Eg, $\epsilon$-biased $\implies$ $\epsilon$-close in $\ell_{\infty}$, but $\epsilon$-close in $\ell_{\infty}$ can be up to $2^{k-1}\epsilon$-biased. And $2^{-k/2}\epsilon$-biased $\implies$ $\epsilon$-close in $\ell_{1}$, but not the other direction.  </span> </i>
</p><p>
<br/></p><hr/><h3>References</h3>
<p>
<a name="ref-AGHP92">[AGHP92]</a> Noga Alon, Oded Goldreich, Johan Håstad, and Ren{é} Peralta.
 Simple constructions of almost k-wise independent random variables.
 <em>Random Structures \&amp; Algorithms</em>, 3(3):289--304, 1992.
 URL: <a href="http://www.tau.ac.il/~nogaa/PDFS/aghp4.pdf">http://www.tau.ac.il/~nogaa/PDFS/aghp4.pdf</a>.
</p><p>

</p><p>
<a name="ref-Alo03">[Alo03]</a> Noga Alon.
 Problems and results in extremal combinatorics, part i.
 <em>Discrete Math</em>, 273:31--53, 2003.
 URL: <a href="http://www.tau.ac.il/~nogaa/PDFS/extremal1.pdf">http://www.tau.ac.il/~nogaa/PDFS/extremal1.pdf</a>.
</p><p>

</p><p>
<a name="ref-NN93">[NN93]</a> Joseph Naor and Moni Naor.
 Small-bias probability spaces: Efficient constructions and
  applications.
 <em>SIAM journal on computing</em>, 22(4):838--856, 1993.
 URL: <a href="http://www.wisdom.weizmann.ac.il/~naor/PAPERS/bias.pdf">http://www.wisdom.weizmann.ac.il/~naor/PAPERS/bias.pdf</a>.
</p><p>

</p><p>
<a name="ref-Vaz99">[Vaz99]</a> Umesh Vazirani.
 k-wise independence and epsilon-biased k-wise indepedence.
 1999.
 URL:
  <a href="https://people.eecs.berkeley.edu/~vazirani/s99cs294/notes/lec4.pdf">https://people.eecs.berkeley.edu/~vazirani/s99cs294/notes/lec4.pdf</a>.
</p><p/></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-06-03T00:00:00Z</updated>
    <published>2016-06-03T00:00:00Z</published>
    <author>
      <name>Preetum Nakkiran</name>
    </author>
    <source>
      <id>http://learningwitherrors.org/atom.xml</id>
      <link href="http://learningwitherrors.org/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="http://learningwitherrors.org" rel="alternate" type="text/html"/>
      <title>Learning With Errors</title>
      <updated>2017-01-04T04:27:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://learningwitherrors.org/2016/05/27/fast-johnson-lindenstrauss</id>
    <link href="http://learningwitherrors.org/2016/05/27/fast-johnson-lindenstrauss/" rel="alternate" type="text/html"/>
    <title>Fast Johnson-Lindenstrauss</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div style="display: none;"/>
<p>
The Johnson-Lindenstrauss (JL) Transform says that, informally, we can embed high-dimensional points into a much lower dimension, while still preserving their pairwise distances. In this post we'll start with the classical JL transform, then focus on the Fast JL Transform (FJLT) by Ailon and Chazelle [<a href="http://learningwitherrors.org/atom.xml#ref-AC09">AC09</a>], which achieves the JL embedding more efficiently (w.r.t. runtime and randomness). We'll look at the FJLT from the perspective of “preconditioning” a sparse estimator, which comes with nice intuition from Fourier duality. We conclude by mentioning more recent developments in this area (faster/sparser/derandomizeder).
<!--more-->
</p><p>
<b>Motivation.</b> It's an interesting structural result, and the JL transform also has algorithmic and machine-learning applications. Eg, any application that only depends on approximate pairwise distances (such as nearest-neighbors), or even on pairwise inner-products<sup><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span class="sidenote" id="footnote1"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a> Because, if the norms $||x_i-x_j||$, $||x_i||$, and $||x_j||$ are approximately preserved, then so is the inner-product $\innp{x_i,x_j}$.  </span> (such as kernel methods) may equivalently work with the dimensionality-reduced version of their input. This also has applications in sketching and scarification.
</p><p>
</p><h2 class="tex">1. Classical JL </h2> The JL embedding theorem is:
<p>

</p><blockquote><b>Theorem 1</b> <em> Given points $x_1, \dots, x_n \in \R^d$, and $\epsilon &gt; 0$, there exists an embedding $f: \R^d \to \R^k$ such that \[\forall i, j: \quad (1-\epsilon) ||x_i - x_j||_2 \leq ||f(x_i) - f(x_j)||_2 \leq (1+\epsilon) ||x_i - x_j||_2 \] and $k = O(\epsilon^{-2}\log n)$. </em></blockquote>

<p>

 That is, the embedding roughly preserves pairwise distances in $\ell_2$. Think of the regime $n \gg d \gg k$. Note that the target dimension $k = O(\epsilon^{-2}\log n)$ is (perhaps surprisingly) independent of the source dimension $d$, and only logarithmic in the number of points $n$.
</p><p>
In fact, a random linear map works as an embedding (w.h.p.). This is established by the following lemma.
</p><p>

</p><blockquote><b>Lemma 2</b> <em> <a name="lemjl"/> For any $\delta &gt; 0$, set $k = O(\epsilon^{-2}\log(1/\delta))$, and let $A \in \R^{k \x d}$ be a random matrix with iid normal $\N(0, 1/k)$ entries. Then \[\forall x \in \R^d: \quad \Pr_{A}\left[~ ||Ax||_2^2 \in (1 \pm \epsilon)||x||_2^2 ~\right] \geq 1- \delta\] </em></blockquote>

<p>

 That is, a random matrix preserves the norm of vectors with good probability. To see that this implies the JL Theorem, consider applying the matrix $A$ on the $O(n^2)$ vectors of pairwise differences $(x_i - x_j)$. For fixed $i,j$, the lemma implies that $||A(x_i - x_j)|| \approx ||x_i - x_j||$ except w.p. $\delta$. Thus, setting $\delta = 1/n^3$ and union bounding, we have that $A$ preserves the norm of <em>all</em> differences $(x_i - x_j)$ with high probability. Letting the embedding map $f = A$, we have \[\forall i,j: \quad ||f(x_i) - f(x_j)|| = ||Ax_i - Ax_j|| = ||A(x_i - x_j)|| \approx ||x_i - x_j||\] as desired. <sup><a href="http://learningwitherrors.org/atom.xml#footnote2">2</a></sup><span class="sidenote" id="footnote2"><a href="http://learningwitherrors.org/atom.xml#footnote2" name="footnote2">2.</a> Note that it was important that $f$ be linear for us to reduce preserving pairwise-distances to preserving norms. </span>
</p><p>
<b>Runtime.</b> The runtime of embedding a single vector with this construction (setting $\delta=1/n^3$ and $\epsilon=O(1)$) is $O(dk) = O(d \log n)$. In the Fast JL below, we will show how to do this in time almost $O(d \log d)$.
</p><p>
We will now prove Lemma <a href="http://learningwitherrors.org/atom.xml#lemjl">2</a>. First, note that our setting is scale-invariant, so it suffices to prove the lemma for all unit vectors $x$.
</p><p>
As a warm-up, let $g \in \R^d$ be a random vector with entires iid $\N(0, 1)$, and consider the inner-product \[Y := \innp{g, x}\] (for a fixed unit vector $x \in \R^d$). Notice the random variable $Y$ has expectation $\E[Y] = 0$, and variance \[\E[Y^2] = \E[\innp{g,x}^2] = ||x||^2\] <b>Thus, $Y^2$ is an unbiased estimator for $||x||^2$.</b> Further, it concentrates well: assuming (wlog) that $||x||=1$, we have $Y \sim \N(0, 1)$, so $Y^2$ has constant variance (and more generally, is subguassian<sup><a href="http://learningwitherrors.org/atom.xml#footnote3">3</a></sup><span class="sidenote" id="footnote3"><a href="http://learningwitherrors.org/atom.xml#footnote3" name="footnote3">3.</a>  Subguassian with parameter $\sigma$ basically means tail probabilities behave as a guassian with variance $\sigma^2$ would behave. Formally, a zero-mean random variable $X$ is “subgaussian with parameter $\sigma$” if: $\E[e^{\lambda X}] \leq e^{\sigma^2\lambda^2/2}$ for all $\lambda \in \R$.  </span> with a constant parameter).
</p><p>
This would correspond to a random linear projection into 1 dimension, where the matrix $A$ in Lemma <a href="http://learningwitherrors.org/atom.xml#lemjl">2</a> is just $A = g^T$. Then $||Ax||^2 = \innp{g, x}^2 = Y^2$. However, this estimator does not concentrate well enough (we want tail probability $\delta$ to eventually be inverse-poly, not a constant).
</p><p>
We can get a better estimator for $||x||^2$ by averaging many iid copies. In particular, for any iid subgaussian random variables $Z_i$, with expectation $1$ and subguassian parameter $\sigma$, the Hoeffding bound gives \[\Pr\left[ \left|\left(\frac{1}{k}\sum_{i=1}^k Z_i \right) - 1 \right| &gt; \epsilon \right] \leq e^{-\Omega(k\epsilon^2/\sigma^2)}\] Applying this for $Z_i = Y_i^2$ and $\sigma = O(1)$, we can set $k = O(\epsilon^{-2}\log(1/\delta))$ so the above tail probability is bounded by $\delta$.
</p><p>
This is exactly the construction of Lemma <a href="http://learningwitherrors.org/atom.xml#lemjl">2</a>. Each row of $A$ is $\frac{1}{\sqrt{k}} g_i^T$, where $g_i \in \R^d$ is iid $\N(0, 1)$. Then \[||A x||^2 = \sum_{i=1}^k \innp{\frac{1}{\sqrt{k}} g_i, x}^2 = \frac{1}{k} \sum_{i=1}^k \innp{g_i, x}^2 = \frac{1}{k} \sum_{i=1}^k Y_i^2\]
</p><p>
And thus, for $||x||=1$, \[\Pr\left[ \left| ||A x||^2 - 1 \right| &gt; \epsilon\right] \leq \delta\] as desired in Lemma <a href="http://learningwitherrors.org/atom.xml#lemjl">2</a>. $$\tag*{$\blacksquare$}$$
</p><p>
To recap, the key observation was that if we draw $g \sim N(0, I_d)$, then $\innp{g, x}^2$ is a “good” estimator of $||x||^2$, so we can average $O(\epsilon^{-2}\log(1/\delta))$ iid copies and get an estimator within a multiplicative factor $(1 \pm \epsilon)$ with probability $\geq 1-\delta$. Filling the transform matrix $A$ with guassians is clearly not necessary; any distribution with iid entries that are subguassian would work, with the same proof as above. For example, picking each entry in $A$ as iid $\pm \frac{1}{\sqrt k}$ would work.
</p><p>
We can now think of different JL transforms as constructing different estimators of $||x||^2$. For example, can we draw from a distribution such that $g$ is sparse? (Not quite, but with some “preconditioning” this will work, as we see below).
</p><p>
</p><h2 class="tex">2. Fast JL </h2>
<p>
</p><h3 class="tex">2.1. First Try: Coordinate Sampling</h3> As a (bad) first try, consider the estimator that randomly samples a coordinate of the given vector $x$, scaled appropriately. That is, \[Y := \sqrt{d} ~x_j \quad\text{for uniformly random coordinate $j \in [d]$}\] Equivalently, draw a random standard basis vector $e_j$, and let $Y := \sqrt{d} \innp{e_j, x}$.
<p>
Notice that $Y^2$ has the right expectation: \[\E[Y^2] = \E_j[(\sqrt{d} x_j)^2] = d \E_j[x_j^2] = ||x||^2\] However, it does not concentrate well. The variance is $Var[Y^2] = Var[d x_j^2] = d^2 Var[x_j]$. If $x$ is a standard basis vector (say $x = e_1$), then this could be as bad as $Var[Y^2] \approx d$. This is bad, because it means we would need to average $\Omega(d)$ iid samples to get a sufficiently good estimator, which does not help us in reducing the dimension of $x \in \R^d$.
</p><p>
The bad case in the above analysis is when $x$ is very concentrated/sparse, so sampling a random coordinate of $x$ is a poor estimator of its magnitude. However, if $x$ is very “spread out”, then sampling a random coordinate would work well. For example, if all entries of $x$ are bounded by $\pm O(\sqrt{\frac{1}{d}})$, then $Var[Y^2] = O(1)$, and taking iid copies of this estimator would work. This would be nice for runtime, since randomly sampling a coordinate can be done quickly (it is not a dense inner-product).
</p><p>
Thus, if we can (quickly) “precondition” our vector $x$ to have $||x||_{\infty} \leq O(\sqrt{\frac{1}{d}})$, we could then use coordinate sampling to achieve a fast JL embedding. We won't quite achieve this, but we will be able to precondition such that $||x||_\infty \leq O(\sqrt{\frac{\log(d/\delta)}{d}})$, as described in the next section. With this in mind, we will need the following easy claim (that with the weaker bound on $\ell_\infty$, coordinate sampling works to reduce the dimension to almost our target dimension).
</p><p>

</p><blockquote><b>Lemma 3</b> <em> <a name="lemS"/> Let $t = \Theta(\epsilon^{-2}\log(1/\delta)\log(d/\delta))$. Let $S \in \R^{t \x d}$ be a matrix with rows $s_i := \sqrt{\frac{d}{t}} e^{(i)}_{j_i}$, where each $j_i \in [d]$ is an iid uniform index. (That is, each row of $S$ randomly samples a coordinate, scaled appropriately). Then, for all $x$ s.t $||x||_2=1$ and $||x||_\infty \leq O(\sqrt{\frac{\log(d/\delta)}{d}})$, we have
</em><p><em>
 \[\Pr_{S}[ ||Sx||^2 \in 1 \pm \epsilon] \geq 1-\delta\] </em></p></blockquote>

<p>

 <em>Proof:</em>  \[||Sx||^2 = \sum_{i=1}^t \innp{\sqrt{\frac{d}{t}} e_{j_i}, x}^2 = \frac{1}{t}\sum_{i=1}^t d (x_{j_i})^2\] Then, the r.vs $\{d (x_{j_i})\}$ are iid and absolutely bounded by $O(\sqrt{\log(d/\delta)})$, so by Chernoff-Hoeffding and our choice of $t$, \[\Pr[ |\frac{1}{t}\sum_{i=1}^t d (x_{j_i})^2 - 1| &gt; \epsilon] \leq e^{-\Omega(\epsilon^2 t / \log(d/\delta))} \leq \delta\] $$\tag*{$\blacksquare$}$$
</p><p>
</p><h3 class="tex">2.2. FJLT: Preconditioning with random Hadamard</h3> The main idea of FJLT is that we can quickly precondition vectors to be “smooth”, by using the Fast Hadamard Transform.
<p>
Recall the $d \x d$ Hadamard transform $H_d$ (for $d$ a power of 2) is defined recursively as \[H_1 := 1 ,\quad H_{2d} := \frac{1}{\sqrt{2}} \bmqty{H_d &amp; H_d\\H_d &amp; -H_d}\] More explicitly, $H_d[i,j] = \frac{1}{\sqrt{d}} (-1)^{\innp{i, j}}$ where indices $i,j \in \{0, 1\}^{\log d}$, and the inner-product is mod 2. The Hadamard transform is just like the discrete Fourier transform<sup><a href="http://learningwitherrors.org/atom.xml#footnote4">4</a></sup><span class="sidenote" id="footnote4"><a href="http://learningwitherrors.org/atom.xml#footnote4" name="footnote4">4.</a> Indeed, it is exactly the Fourier transform over the group $(\Z_2)^n$. For more on Fourier transforms over abelian groups, see for example <a href="https://lucatrevisan.wordpress.com/2016/03/16/cs294-lecture-15-abelian-cayley-graphs/">Luca's notes</a>. </span> : it can be computed in time $O(d\log d)$ by recursion, and it is an orthonormal transform.
</p><p>
Intuitively, the Hadamard transform may be useful to “spread out” vectors, since Fourier transforms take things that are sparse/concentrated in time-domain to things that are spread out in frequency domain (by time-frequency duality/Uncertainty principle). Unfortunately this won't quite work, since duality goes both ways: It will also take vectors that are already spread out and make them sparse.
</p><p>
To fix this, it turns out we can first randomize the signs, then apply the Hadamard transform.
</p><p>

</p><blockquote><b>Lemma 4</b> <em> <a name="lemHadamard"/> Let $H_d$ be the $d \x d$ Hadamard transform, and let $D$ be a random diagonal matrix with iid $\pm 1$ entries on the diagonal. Then, \[\forall x \in \R^d, ||x||=1: \quad \Pr_{D}[ ||H_d D x||_\infty &gt; \Omega(\sqrt{\frac{\log(d/\delta)}{d}}) ] \leq \delta\] </em></blockquote>

<p>


</p><p>
We may expect something like this to hold: randomizing the signs of $x$ corresponds to pointwise-multiplying by random white noise. White noise is spectrally flat, and multiplying by it in time-domain corresponds to convolving by its (flat) spectrum in frequency domain. Thus, multiplying by $D$ should “spread out” the spectrum of $x$. Applying $H_d$ computes this spectrum, so should yield a spread-out vector.
</p><p>
The above intuition seems messy to formalize but the proof is surprisingly simple. <sup><a href="http://learningwitherrors.org/atom.xml#footnote5">5</a></sup><span class="sidenote" id="footnote5"><a href="http://learningwitherrors.org/atom.xml#footnote5" name="footnote5">5.</a> This proof presented slightly differently from the one in Alon-Chazelle, but the idea is the same. </span>
</p><p>
<em>Proof:</em> } Consider the first entry of $H_d D x$. Let $D = diag(a_1, a_2, \dots, a_d)$ where $a_i$ are iid $\pm 1$. The first row of $H_d$ is $\frac{1}{\sqrt{d}} \bmqty{1 &amp; 1 &amp; \dots &amp; 1}$, so \[(H_d D x)[1] = \frac{1}{\sqrt{d}} \sum_i a_ix_i\] Here, the $x_i$ are fixed s.t. $||x||_2=1$, and the $a_i = \pm 1$ iid. Thus we can again bound this by Hoeffding <sup><a href="http://learningwitherrors.org/atom.xml#footnote6">6</a></sup><span class="sidenote" id="footnote6"><a href="http://learningwitherrors.org/atom.xml#footnote6" name="footnote6">6.</a>  The following form of Hoeffding bound is useful (it follows directly from Hoeffding for subgaussian variables, but is also a corollary of Azuma-Hoeffding): For iid zero-mean random variables $Z_i$, absolutely bounded by $1$, $\Pr[|\sum c_i Z_i| &gt; \epsilon] \leq 2exp(-\frac{\epsilon^2}{2 \sum_i c_i^2})$.  </span> (surprise), \[ \Pr[ | \sum_{i=1}^d a_i \frac{x_i}{\sqrt{d}}| &gt; \eta] \leq e^{-\Omega(\eta^2 d / ||x||_2^2)} \] For $\eta = \Omega(\sqrt{\frac{\log(d/\delta)}{d}})$, this probability is bounded by $(\frac{\delta}{d})$. Moreover, the same bound applies for all coordinates of $H_d Dx$, since all rows of $H_d$ have the form $\frac{1}{\sqrt{d}} \bmqty{\pm 1 &amp; \pm1 &amp; \dots &amp; \pm1}$. Thus, union bound over $d$ coordinates establishes the lemma. $$\tag*{$\blacksquare$}$$
</p><p>
</p><h3 class="tex">2.3. The Full Fast JL Transform</h3> <i>This presentation of FJLT is due to Jelani Nelson; see the notes [<a href="http://learningwitherrors.org/atom.xml#ref-Nel10">Nel10</a>].</i>
<p>
Putting all the pieces together, the FJLT is defined as: \[A = J S H_d D\] or, \[ A: \quad \R^d \overset{D}{\longrightarrow} \R^d \overset{H_d}{\longrightarrow} \R^d \overset{S}{\longrightarrow} \R^t \overset{J}{\longrightarrow} \R^k \] where

</p><ul> <li> $S$: the sparse coordinate-sampling matrix of Lemma <a href="http://learningwitherrors.org/atom.xml#lemS">3</a> </li><li> $H_d$: the $d \x d$ Hadamard transform. </li><li> $D$: diagonal iid $\pm 1$. </li><li> $J$: a dense “normal” JL matrix (iid Gaussian entries).
</li></ul>

 For parameters

<ul> <li> $t = \Theta(\epsilon^{-2}\log(1/\delta)\log(d / \delta))$ </li><li> $k = \Theta(\epsilon^{-2}\log(1/\delta))$
</li></ul>


<p>
That is, we first precondition with the randomized Hadamard transform, then sample random coordinates (which does most of the dimensionality reduction), then finally apply a normal JL transform to get rid of the last $\log(d/\delta)$ factor in the dimension.
</p><p>
<b>Correctness.</b> Since the matrix $D$ and the Hadamard transform are isometric, they do not affect the norms of vectors. Then, after the preconditioning, Lemma <a href="http://learningwitherrors.org/atom.xml#lemS">3</a> guarantees that $S$ only affects norms by $(1 \pm \epsilon)$, and Lemma <a href="http://learningwitherrors.org/atom.xml#lemjl">2</a> guarantees that the final step is also roughly isometric. These steps fail w.p. $\delta$, so the final transform affects norms by at most say $(1\pm 3\epsilon)$ except w.p. $3\delta$. This is sufficient to establish the JL embedding.
</p><p>
<b>Runtime.</b> For computing a JL embedding (ie, setting $\delta = 1/n^3, \epsilon=O(1)$), the time to embed a single vector is $O(d \log d + \log^3 n)$.
</p><p>
</p><h2 class="tex">3. Closing Remarks </h2>
<p>
<b>Optimality.</b> The target dimension given by the JL construction is known to be optimal. That is, one cannot embed $n$ points into dimension less than $k=\Omega(\epsilon^{-2}\log n)$ with distortion $\epsilon$. The first near-optimal lower-bound, in [<a href="http://learningwitherrors.org/atom.xml#ref-Alo03">Alo03</a>, Section 9] works by showing upper-bounds on the number of nearly-orthogonal vectors in a given dimension (so a too-good embedding of orthogonal vectors would violate this bound). A more recent, optimal bound, is in [<a href="http://learningwitherrors.org/atom.xml#ref-KMN11">KMN11</a>, Section 6]. They actually show optimality of the JL Lemma (that is, restricting to linear embeddings), which works (roughly) by arguing that if the target dimension is too small, then the kernel is too big, so a random vector is likely to be very distorted.
</p><p>
<b>Recent Advances.</b> Note that the FJLT is fast, but is not <em>sparse</em>. We may hope that embedding a sparse vector $x$ will take time proportional to the sparsity of $x$. A major result in this area was the sparse JL construction of [<a href="http://learningwitherrors.org/atom.xml#ref-KN14">KN14</a>]; see also the notes [<a href="http://learningwitherrors.org/atom.xml#ref-Nel10">Nel10</a>]. There is also work in derandomized JL, see for example [<a href="http://learningwitherrors.org/atom.xml#ref-KMN11">KMN11</a>].
</p><p>
<i>I'll stop here, since I haven't read these works yet, but perhaps we will revisit this another time.<br/>
 This post was derived from my talk at Berkeley theory retreat, on the theme of “theoretical guarantees for machine learning.” </i>
</p><p>
<br/></p><hr/><h3>References</h3>
<p>
<a name="ref-AC09">[AC09]</a> Nir Ailon and Bernard Chazelle.
 The fast johnson-lindenstrauss transform and approximate nearest
  neighbors.
 <em>SIAM Journal on Computing</em>, 39(1):302--322, 2009.
 URL:
  <a href="https://www.cs.princeton.edu/~chazelle/pubs/FJLT-sicomp09.pdf">https://www.cs.princeton.edu/~chazelle/pubs/FJLT-sicomp09.pdf</a>.
</p><p>

</p><p>
<a name="ref-Alo03">[Alo03]</a> Noga Alon.
 Problems and results in extremal combinatorics, part i.
 <em>Discrete Math</em>, 273:31--53, 2003.
 URL: <a href="http://www.tau.ac.il/~nogaa/PDFS/extremal1.pdf">http://www.tau.ac.il/~nogaa/PDFS/extremal1.pdf</a>.
</p><p>

</p><p>
<a name="ref-KMN11">[KMN11]</a> Daniel Kane, Raghu Meka, and Jelani Nelson.
 Almost optimal explicit johnson-lindenstrauss families.
 In <em>Approximation, Randomization, and Combinatorial Optimization.
  Algorithms and Techniques</em>, pages 628--639. Springer, 2011.
 URL:
  <a href="http://people.seas.harvard.edu/~minilek/papers/derand_jl.pdf">http://people.seas.harvard.edu/~minilek/papers/derand_jl.pdf</a>.
</p><p>

</p><p>
<a name="ref-KN14">[KN14]</a> Daniel~M Kane and Jelani Nelson.
 Sparser johnson-lindenstrauss transforms.
 <em>Journal of the ACM (JACM)</em>, 61(1):4, 2014.
 URL: <a href="https://arxiv.org/pdf/1012.1577v6.pdf">https://arxiv.org/pdf/1012.1577v6.pdf</a>.
</p><p>

</p><p>
<a name="ref-Nel10">[Nel10]</a> Jelani Nelson.
 Johnson-lindenstrauss notes.
 Technical report, Technical report, MIT-CSAIL, 2010.
 URL: <a href="http://web.mit.edu/minilek/www/jl_notes.pdf">http://web.mit.edu/minilek/www/jl_notes.pdf</a>.
</p><p/></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-05-27T00:00:00Z</updated>
    <published>2016-05-27T00:00:00Z</published>
    <author>
      <name>Preetum Nakkiran</name>
    </author>
    <source>
      <id>http://learningwitherrors.org/atom.xml</id>
      <link href="http://learningwitherrors.org/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="http://learningwitherrors.org" rel="alternate" type="text/html"/>
      <title>Learning With Errors</title>
      <updated>2017-01-04T04:27:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/workshops-2018</id>
    <link href="http://grigory.github.io/blog/workshops-2018/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Workshops in 2018</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p>I got invited to talk at quite a few workshops this year and am close to reaching the limit of travel I can handle. Just want to help the organizers advertise these events (most of them on sublinear algorithms and complexity). Please, consider attending and help spread the word!</p>

<ul>
  <li><a href="https://warwick.ac.uk/fac/sci/dcs/research/focs/conf2017">Workshop on Algorithms for Data Summarization</a> at the University of Warwick, UK (March 19-22). Organized by Graham Cormode and Artur Czumaj.</li>
  <li>68th Midwest Theory Day(s) at TTI-Chicago (April 12-13). Organized by Madhur Tulsiani, Aravindan Vijayaraghavan and Anindya De among others.</li>
  <li>Workshop on Sublinear Algorithms (June 11-13) and 2nd Workshop on Local Algorithms (June 14-15) at MIT. WoLA is organized by Mohsen Ghaffari, Reut Levi, Moti Medina, Andrea Montanari, Elchanan
Mossel and Ronitt Rubinfeld.</li>
  <li><a href="https://simons.berkeley.edu/complexity2018-2">Workshop on Interactive Complexity</a> at the Simons Institute, Berkeley (October 15-19). Organized by Kasper Green Larsen, Mark Braverman and Michael Saks.</li>
</ul>

<p>Hope to see some of you there!</p>


  <p><a href="http://grigory.github.io/blog/workshops-2018/">Workshops in 2018</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on February 23, 2018.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2018-02-23T00:00:00Z</updated>
    <published>2018-02-23T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/whats-new-in-big-data-theory-2017</id>
    <link href="http://grigory.github.io/blog/whats-new-in-big-data-theory-2017/" rel="alternate" type="text/html"/>
    <title xml:lang="en">What's New in the Big Data Theory 2017</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><div align="center"><img alt="Happy 2018!" src="http://grigory.github.io/blog/pics/o2017.png"/> </div>

<p><br/></p>

<p>This year I will continue the <a href="http://grigory.us/blog/whats-new-in-big-data-theory-2016/">tradition started last year</a> and summarize a few papers on efficient algorithms for big data that caught my attention last year.
Same disclaimers as last year apply and this is by no means supposed to be the list of “best” papers in the field which is quite loosely defined anyway (e.g. I will intentionally avoid deep learning and gradient descent methods here as I am not actively working in these areas myself and there are a lot of resources on these topics already).
In particular, this year it was even harder to pick clear favorites so it is even more likely that I have missed some excellent work. 
Below I will assume familiary with the basics of streaming algorithms and the massively parallel computation model (MPC) discussed in an <a href="http://grigory.us/blog/mapreduce-model/">earlier post</a>.</p>

<p>Before we begin let me quickly plug some of my own work from last year.
With my student Adithya Vadapalli we have a new paper ``<a href="https://arxiv.org/pdf/1710.01431.pdf">Massively Parallel Algorithms and Hardness of Single-Linkage Clustering under -distances</a>’’. As it turns out, while single-linkage clustering and minimum spanning tree problems are the same for exact computation, for vector data round complexity of approximating these two problems in the MPC model is quite different.
In <a href="http://grigory.us/files/approx-linsketch.pdf">another paper</a> I introduce a study of approximate binary linear sketching of valuation functions.
This is an extension of our <a href="https://eccc.weizmann.ac.il/report/2016/174/">recent study</a> of binary linear sketching to the case when the function of interest should only be computed approximately.</p>

<!--
Lack of clear favorites probably means that we might not see all of these results taught in advanced algorithms classes but rather used for reading groups, etc.
Many of the papers listed below have been discussed in detail in our ``algorithms for big data'' reading group here at IU.
-->
<h2>New Massively Parallel Algorithms for Matchings</h2>

<p>Search for new algorithms for matchings has lead to development of new algorithmic ideas for many decades (motivating the study of the class P of polynomial-time algorithms) and this year is no exception.
Two related papers on matchings caught my attention this year:</p>
<ul>
  <li>“<a href="https://arxiv.org/abs/1707.03478">Round Compression for Parallel Matching Algorithms</a>” by Czumaj, Lacki, Madry, Mitrovic, Onak and Sankowski.</li>
  <li>“<a href="https://arxiv.org/abs/1711.03076">Coresets Meet EDCS: Algorithms for Matching and Vertex Cover on Massive Graphs</a>” by Assadi, Bateni, Bernstein, Mirrokni and Stein.</li>
</ul>

<p>Both papers are highly technical but achieve similar results.
The first paper gives an -round MPC algorithm for the maximum matching problem that uses  memory per machine. The second paper improves the number of rounds down to  using slightly larger memory  per machine.
Using a standard reduction mentioned in the latter paper both papers can achieve multiplicative -approximation for any constant .
These results should be contrasted with the <a href="http://theory.stanford.edu/~sergei/papers/spaa11-matchings.pdf">previous work</a> by Lattanzi, Moseley, Suri and Vassilvitskii who give -round algorithms at the expense of using  memory per machine for any constant .
Overall, this is remarkable progress but likely not the end of the story.</p>

<h2>Massively Parallel Methods for Dynamic Programming</h2>
<p>Dynamic programming, <a href="https://www.rand.org/content/dam/rand/pubs/papers/2008/P550.pdf">pioneered by Bellman at RAND</a>, is one of the key techniques in algorithm design. Some would even go as far as saying that there are only two algorithmic tecniques and dynamic programming is one of them.
However, dynamic programming programming is notoriously sequential and difficult to use for sublinear time/space computation.
Most successful stories of speeding up dynamic programming so far have been problem-specific and often highly non-trivial.</p>

<p>In their paper “<a href="http://www.andrew.cmu.edu/user/moseleyb/papers/stoc17-main279.pdf">Efficient Massively Parallel Methods for Dynamic Programming</a>” (STOC’17) Im, Moseley and Sun suggest a fairly generic approach for designing massively parallel dynamic programming algorithms.
Three textbook dynamic programming problems can be handled within their framework:</p>

<ul>
  <li>Longest Increasing Subsequence: multiplicative -approximation in  rounds of MPC.</li>
  <li>Optimal Binary Search Tree: multiplicative -approximation in  rounds of MPC.</li>
  <li>Weighted Interval Scheduling: multiplicative -approxiamtion in  rounds of MPC.</li>
</ul>

<p>On a technical level this paper identifies two key properties that these problems have in common: monotonicity and decmoposability. Montonicity just requires that the answer to a subproblem should always be at most (for maximization)/at least(for minimization) the answer to the problem itself.
Decomposability is more subtle and requires that the problem can be decomposed into a two-level recursive family of subproblems where entries of the top level are called groups and entries of the bottom level are called blocks. 
It should then be possible to 1) construct a nearly optimal solution for the entire problem by concatenating solutions for subproblems, 2) construct a nearly optimal solution for each group from only a constant number of blocks.
While monotonicity holds for many standard dynamic problems, decomposability seems much more restrictive so it is interesting to see whether this technique can be extended to some other problems.</p>

<p>See Ben Moseley’s <a href="http://caml.indiana.edu/slides/ben.pdf">presentation</a> at the Midwest Theory Day for more details.</p>

<h2>Randomized Composable Coresets for Matching and Vertex Cover</h2>
<p>The simplest massively parallel algorithm one can think of can be described as follows: partition the data between  machines, let each machine select a small subset of the data points, collect these locally selected data points on one central machine and compute the solution there. 
The hardest part here is the design of the local subset selection procedures.
Such subsets are called coresets and have received a lot of attention the study of algorihtms for high-dimensional vectors, see e.g. <a href="http://sarielhp.org/p/04/survey/survey.pdf">this survey</a> by Agarwal, Har-Peled and Varadarajan.</p>

<p>Note that in the distributed setting construction of coresets can be affected by the initial distribution of data.
In fact, for the maximum mathcings problem non-trivially small coresets can’t be used to approximate the maximum matching up to a reasonable error (see <a href="http://grigory.us/files/soda16.pdf">our paper</a> with Assadi, Khanna and Li for the exact statement which in fact rules out not just coresets but any small-space representations) if no assumptions about the distribution of the data is made.</p>

<p>However, if the initial distribution of data is uniformly random then the situation changes quite dramatically.
As shown in “<a href="http://www.seas.upenn.edu/~sassadi/stuff/papers/randomized-coreset_matching-vc.pdf">Randomized Composable Coresets for Matching and Vertex Cover</a>” by Assadi and Khanna (best paper at SPAA’17) for uniformly distributed data coresets of size  can be computed locally and then combined to obtain -approximation.</p>

<h2>Optimal Lower Bounds for L<sub>p</sub>-Samplers, etc </h2>
<p>Consider the following problem: a vector  (initially consisting of all zeros) is changed by a very long sequence of updates that can flip an arbitrary coordinate of this vector. After seeing this sequence of updates can we retrieve some non-zero entry of this vector without storing all  bits used to represent ?
Surprisingly, the answer is “yes” and this can be done with only  space.
If we are required to generate a uniformly random non-zero entry of  then the corresponding problem is called -sampling.</p>

<p>-sampling turns out to be a remarkably useful primitive in the design of small-space algorithms. Almost all known streaming algorithms for dynamically changing graphs are based on -sampling or its relaxation where the uniformity requirement is removed.</p>

<p>While almost optimal upper and lower bounds on the amount of space necessary for -sampling have been known since <a href="https://arxiv.org/pdf/1012.4889.pdf">the work</a> of Jowhari, Saglam and Tardos, there were still gaps in terms of the dependence on success probability.
If our recovery of a non-zero entry of  has to be successful with probability  then the tight bound on space turns out to be .
This is one of the results of the <a href="http://people.seas.harvard.edu/~minilek/publications/papers/sampler_lb_merged.pdf">recent FOCS’17 paper</a> by Kapralov, Nelson, Pahocki, Wang, Woodruff and Yahyazadeh.</p>

<h2>Looking forward to more results in 2018!</h2>
<p>Please, let me know if there are any other interesting papers that I missed.
Also here is a quick shout out goes to some other papers that were close to making the above list:</p>

<ul>
  <li>“<a href="https://nips.cc/Conferences/2017/Schedule?showEvent=9453">Affinity Clustering: Hierarchical Clustering at Scale</a>” by Bateni, Behnezhad, Derakhshan, Hajiaghayi, Kiveris, Lattanzi and Mirrokni (NIPS’17).</li>
  <li>“<a href="https://www.ilyaraz.org/static/papers/lshforest.pdf">LSH Forest: Practical Algorithms Made Theoretical</a>” by Andoni, Razenshteyn and Shekel Nosatzki (SODA’17).</li>
  <li>“<a href="https://arxiv.org/abs/1610.08096">Almost Optimal Streaming Algorithms for Coverage Problems</a>” by Bateni, Esfandiari and Mirrokni (SPAA’17).</li>
  <li>“<a href="http://www.cs.utexas.edu/~ecprice/papers/compressed-generative.pdf">Compressed Sensing using Generative Models</a> by Bora, Jalal, Price and Dimakis (ICML’17).</li>
  <li>“<a href="https://arxiv.org/pdf/1707.08484.pdf">MST in O(1) Rounds of Congested Clique</a>” by Jurdzinski and Nowicki (SODA’18).</li>
</ul>

  <p><a href="http://grigory.github.io/blog/whats-new-in-big-data-theory-2017/">What's New in the Big Data Theory 2017</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on January 27, 2018.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2018-01-27T00:00:00Z</updated>
    <published>2018-01-27T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/whats-new-in-big-data-theory-2016</id>
    <link href="http://grigory.github.io/blog/whats-new-in-big-data-theory-2016/" rel="alternate" type="text/html"/>
    <title xml:lang="en">What's New in the Big Data Theory 2016</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><div align="center"><img alt="Happy 2017!" src="http://grigory.github.io/blog/pics/o2016.png"/> </div>

<p><br/></p>

<p>This post will give an overview of papers on theory of algorithms for big data that caught my attention in 2016.
The basic rule that I used when making the list was whether I can see these results being included into some of the advanced graduate classes on algorithms in the future.
Also, while I obviously can’t include my own results here, among my own 2016 papers my two personal favorites are <a href="http://grigory.us/files/soda16.pdf">tight bounds on space complexity of computing approximate matchings in dynamic streams</a> (with S. Assadi, S. Khanna and Y. Li) and the <a href="http://eccc.hpi-web.de/report/2016/174/">-sketching paper</a> (with S. Kannan and E. Mossel and some special credit to Swagato Sanyal who subsequently improved the dependence on error in one of our main theorems).</p>

<p>It’s been a great year with several open problems resolved, old algorithms improved and new lines of research started.
All papers discussed below are presented in no particular order and their selection is clearly somewhat biased towards my own research interests.</p>

<h2>Maximum Weighted Matching in Semi-Streaming</h2>
<p>Sweeping both the best paper and the best student paper awards at the upcoming 28th ACM Symposium on Discrete Algorithms is a paper on semi-streaming algorithms for maximum weighted matching by graduate students Ami Paz and Gregory Schwartzman.
In semi-streaming we are given one pass over edges of an -vertex and only  bits of space.
It is easy to get a 2-approximation to the maximum matching by just maintaining the maxim<strong>al</strong> matching of the graph.
However, for weighted graphs maximal matching no longer guarantees a 2-approximation.</p>

<p>A long line of work has previously given constant factor approximations for this problem and finally we have a -approixmation. 
It is achieved via a careful implementation of the primal-dual algorithm for matchings in the semi-streaming setting.
It may seem somewhat surprising that primal-dual hasn’t been applied to this problem before since in the area of approximation algorithms it is a pretty standard way of reducing weighted problems to their unweighted versions, but the exact details of how to implement primal-dual in the streaming setting are quite delicate. I couldn’t find a version of this paper online so the best bet might be to wait for the SODA proceedings.</p>

<p>Now the big open question is whether one can beat the 2-approximation which is open even in the unweighted case.</p>

<h2>Shuffles and Circuits</h2>
<p>Best paper award at the 28th ACM Symposium on Parallelism in Algorithms and Architectures went to ‘‘<a href="http://theory.stanford.edu/~sergei/papers/spaa16-mrshuffle.pdf">Shuffles and Circuits</a>’’, a paper by Roughgarden, Vassilvitskii and Wang.
This paper emphasizes the difference between rounds of MapReduce and depth of a circuit.
Because some of the machines can choose to stay silent between the rounds a round of MapReduce can be more complex than a layer of a circuit as the machines sending input to the next round might depend on the original input data. 
The paper shows that nevertheless the standard circuit complexity ‘‘degree bound’’ can be applied to MapReduce computation.
I.e. any Boolean function whose polynomial representation has degree  requires  rounds of MapReduce using machines with space .
This implies an  lower bound on the number of rounds for computing connectivity of a graph.
The authors also make explicit a connection between the MapReduce model and  (see definition <a href="https://en.wikipedia.org/wiki/NC_(complexity)">here</a>) which implies that improving lower bounds beyond   for polynomially many machines would imply separating  from .</p>

<h2>Beating Counting Sketches for Insertion-Only Streams</h2>
<p>Both <a href="http://www.cs.princeton.edu/courses/archive/spring04/cos598B/bib/CharikarCF.pdf">CountSketch</a> and <a href="https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch">Count-Min Sketch</a>, which are textbook approximate data structures for storing very large dynamically changing numerical tables in small space, have been improved this year under the assumption that data in the table is only incremented.
These improvements are for the most common application of such sketches to ``heavy hitters’’– the task of recovering largest entries from the table approximately. 
For CountSketch see <a href="http://researcher.watson.ibm.com/researcher/files/us-dpwoodru/bciw16.pdf">the paper</a> by Braverman, Chestnut, Ivkin, Woodruff from STOC’16 and for CountMin Sketch <a href="https://arxiv.org/abs/1603.00213">the paper</a> by Bhattacharyya, Dey and Woodruff from PODS’16.</p>

<h2>Optimality of the Johnson-Lindenstrauss Transform</h2>
<p>Two papers by <a href="https://arxiv.org/pdf/1609.02094v1.pdf">Green Larsen and Nelson</a> and by <a href="http://www.cs.tau.ac.il/~nogaa/PDFS/compression3.pdf">Alon and Klartag</a> have resolved the question of proving optimality of the Johnson-Lindenstrauss transform.
Based on doing a projection on random low-dimensional subspace JL-transform is the main theoretical tool for dimensionality reduction of high-dimensional vectors.
As these papers show no low-dimensional embedding and furthermore no data structure can achieve better bit complexity than  for -approximating all pairwise distances between  vectors in Euclidean space (for a certain regime of parameters).
This matches the Johnson and Lindenstrauss upper bound and improves an old lower bound of  due to Alon.
Even though Alon’s argument is significantly simpler getting an optimal lower bound is a very nice achievement.</p>

<h2>Fast Algorithm for Edit Distance if It's Small</h2>

<p><a href="https://en.wikipedia.org/wiki/Edit_distance">Edit distance</a> is one of the cornerstone metrics of text similairity in computer science. It can be computed in quadratic time using standard dynamic programming which is optimal assuming SETH due to the <a href="https://arxiv.org/abs/1412.0348">result of Backurs and Indyk</a>.
Edit distance also has a number of applications including comparing DNAs in computational biology.
In these applications it is usually reasonable to assume that edit distance is only interesting if it is not too large.
Unfortunately, this doesn’t help speed up the standard dynamic program.
A series of papers, including two papers from this year by <a href="http://iuuk.mff.cuni.cz/~koucky/papers/editDistance.pdf">Chakraborty, Goldenberg and Koucky</a> (STOC’16) and 
<a href="http://homes.soic.indiana.edu/qzhangcs/papers/focs16-ED.pdf">Belazzogui and Zhang</a> lead to the following result: sketches of size  bits suffice for computing edit distance . Such sketches can be applied not just in centralized but also in distributed and streaming settings making it possible to compress input strings down to size that (up to logarithmic factors) only depends on .</p>

<h2>Tight Bounds for Set Cover in Streaming</h2>
<p>Set Cover is a surprisingly powerful abstraction for a lot of applications that involve providing coverage for some set of terminals. 
Given a collection of sets  the goal is to find the smallest cardinality subcollection of these sets such that their union is , i.e. all of the underlying elements are covered.
In approximation algorithms a celebrated greedy algorithm gives an -approximation for this problem. 
In streaming there has been a lot of interest lately in approximating classic combinatorial optimization problems in small space with Set Cover being one of the main examples.
For an overview from last year check Piotr Indyk’s <a href="https://www.youtube.com/embed/_4mM1UGI9Dg?list=PLqxsGMRlY6u659-OgCvs3xTLYZztJpEcW">talk</a> from the <a href="http://grigory.us/mpc-workshop-dimacs.html">DIMACS Workshop on Big Data and Sublinear Algorithms</a>.</p>

<p>As <a href="http://www.seas.upenn.edu/~sassadi/stuff/papers/tbfsscotscp-conf.pdf">this STOC’16 paper</a> by Assadi, Khanna and Li shows savings in space for streaming Set Cover can only be proportional to the loss in approximation.  In particular, if we are interested in computing Set Cover which is within a multiplicative factor  of the optimum then:
1) for computing the cover itself space  is necessary and sufficient,
2) for just esimating the size space  is necessary and sufficient.</p>

<h2>Polynomial Lower Bound for Monotonicity Testing</h2>
<p>Finally a polynomial lower bound has been shown for adaptive algorithms for testing monotonicity of Boolean functions .
The lower bound implies that any algorithm that can tell whether  is monotone or differs from monotone on a constant fraction of inputs has to query at least  values of . 
This result is due to <a href="https://arxiv.org/abs/1511.05053">Belovs and Blais</a> (STOC’16) and is in contrast with the upper bound of  by Khot, Minzer and Safra from last year’s FOCS.
Probably the biggest result in property testing this year.</p>

<h2>Linear Hashing is Awesome</h2>
<p>While ‘‘<a href="http://ieee-focs.org/FOCS-2016-Papers/3933a345.pdf">Linear Hashing is Awesome</a>’’ by Mathias Bæk Tejs Knudsen doesn’t fall into the traditional ‘‘sublinear algorithms for big data’’ category this paper still has some sublinear flavor because of its focus on very fast query times.
Linear hashing is a classic hashing scheme 
 
where  are random. It is very often used in practice and discussed extensively in CLRS.
This paper proves that linear hashing <strike>is awesome</strike> results in expected length of the longest chain of only  compared to the previous simple bound of .</p>

<p>Finally, this paper also decisively wins my ‘‘Best Paper Title 2016’’ award.</p>

<h2>Looking forward to more cool results in 2017!</h2>
<p>There has been a lot of great results in 2016 and it’s hard to mention all of them in one post and I certainly might have missed some exciting papers. Here is a quick shout out to some other papers that were close to making the above list:</p>
<ul>
<li><a href="https://arxiv.org/abs/1507.04299">Tight Bounds for Data-Dependent LSH</a> by Andoni and Razenshteyn from SoCG'16.</li>
<li><a href="http://arxiv.org/abs/1603.05346">Optimal Quantile Estimation in Streams</a> by Karnin, Lang and Liberty from FOCS'16.
</li>
</ul>

<p>Happy 2017!</p>


  <p><a href="http://grigory.github.io/blog/whats-new-in-big-data-theory-2016/">What's New in the Big Data Theory 2016</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on December 30, 2016.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-12-30T00:00:00Z</updated>
    <published>2016-12-30T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/video-recording-class-screencast</id>
    <link href="http://grigory.github.io/blog/video-recording-class-screencast/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Video Recording Screencasts of Talks</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p>Here is the setup I came up with for video recording live lectures and talks.</p>

<p>Hardware:</p>
<ul>
<li> Laptop with a camera: MacBook</li>
<li> Tablet with a pen/stylus: iPad (Pro), MSRP $1K</li>
<li> Microphone: Sennheiser ME 3-EW, MSRP $200</li>
<li> Projector (connected to the laptop)</li>
</ul>

<p>Software:</p>
<ul>
<li> Open Broadcasting Software (laptop), free</li>
<li> Microsoft Powerpoint and Office 365 (tablet), free from IU</li>
<li> AirServer (laptop) and AirServer Connect (tablet), $10</li>
</ul>

<p>Full description and demo on Youtube:
<br/></p>



  <p><a href="http://grigory.github.io/blog/video-recording-class-screencast/">Video Recording Screencasts of Talks</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on August 01, 2017.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2017-08-01T00:00:00Z</updated>
    <published>2017-08-01T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/video-recording-class-question</id>
    <link href="http://grigory.github.io/blog/video-recording-class-question/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Advice on video recording lectures?</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p>Looking for advice: I am thinking of making videos of my class this Fall, the setup I am thinking about is a tablet (just ordered a large iPad Pro) + mic and camera + some software (maybe Google Hangouts live?) to record a screencast of the tablet with my video on the side. Did anyone have experience doing something like this? What hardware and software did you use? If this sounds too clunky to set up on your own, is it worth creating a MOOC and/or having professionals do the recording for you?</p>


  <p><a href="http://grigory.github.io/blog/video-recording-class-question/">Advice on video recording lectures?</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on July 31, 2017.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2017-07-31T00:00:00Z</updated>
    <published>2017-07-31T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/theory-jobs-2018</id>
    <link href="http://grigory.github.io/blog/theory-jobs-2018/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Theory Jobs 2018</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p><img src="http://grigory.github.io/blog/pics/theory-jobs-2018.png"/>
<a href="https://docs.google.com/spreadsheets/d/1P5okKjeNlvkEEFMzX3l8VcL4SHpWBKNFET-5mPTWfDQ/edit#gid=0">Here is a link</a> to a crowdsourced spreadsheet created to collect information about theory jobs this year. Previously my academic uncle Lance Fortnow set it up (check <a href="https://blog.computationalcomplexity.org/2017/06/theory-jobs-2016.html">this link</a> to his post from last year which also has links to all the previous years), but this year he has kindly agreed to try and pass the baton. Rules about the spreadsheet have been copied from last years and all edits to the document are anonymized.</p>

<ul>
 <li>Separate sheets for faculty, industry and postdocs/visitors.  <b>New:</b> As suggested by <a href="http://onak.pl">Krzysztof Onak</a> a new tab for sabbaticals was added.</li>
 <li>People should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are absolutely sure have been offered and accepted. This is not the place for speculation and rumors. </li>
 <li>You are welcome to add yourself, or people your department has hired. </li>
</ul>

<p>This document will continue to grow as more jobs settle.</p>



  <p><a href="http://grigory.github.io/blog/theory-jobs-2018/">Theory Jobs 2018</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on May 25, 2018.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2018-05-25T00:00:00Z</updated>
    <published>2018-05-25T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/theory-jobs-2017</id>
    <link href="http://grigory.github.io/blog/theory-jobs-2017/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Theory Jobs 2017</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>UPD</b>: Lance created his Theory Jobs spreadsheet, I’ve moved all the information there and changed the link below to Lance’s spreadsheet.</p>

<p><a href="https://docs.google.com/spreadsheets/d/1xBpgBZXcSxjEAbU7SYCeXOJOJtPeXYCOFArqL28Uho8/edit?usp=sharing">Here is a link</a> to a crowdsourced spreadsheet created by Lance Fortnow that collects information about theory jobs this year. <strike>Previously Lance set it up, but this year it is getting late in the year so I decided to go ahead and create one myself. In previous years the jobs post was up a few weeks back so I hope I am not jumping the gun here. Rules about the spreadsheet have been copied from <a href="http://blog.computationalcomplexity.org/2016/05/theory-jobs-2016.html">Lance's last year post</a> and all edits to the document are anonymized.  
</strike></p>

<ul>
 <li>Separate sheets for faculty, industry and postdoc/visitors. </li>
 <li>People should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are absolutely sure have been offered and accepted. This is not the place for speculation and rumors. </li>
 <li>You are welcome to add yourself, or people your department has hired. </li>
 </ul>

<p>This document will continue to grow as more jobs settle.</p>




  <p><a href="http://grigory.github.io/blog/theory-jobs-2017/">Theory Jobs 2017</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on June 08, 2017.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2017-06-08T00:00:00Z</updated>
    <published>2017-06-08T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/the-simple-economics-of-algorithms-for-big-data</id>
    <link href="http://grigory.github.io/blog/the-simple-economics-of-algorithms-for-big-data/" rel="alternate" type="text/html"/>
    <title xml:lang="en">The Simple Economics of Algorithms for Big Data</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p>
In this blog post I want to suggest a simple reason why you should study your algorithms <b>really</b> well if you want to design algorithms that deal with big data.
This reason comes from <b>the way billings offered by cloud services work</b>.
</p>
<p>
Maybe you remember yourself taking that algorithms class and thinking: “Who really cares if that algorithm uses a bit more time? Can't we just wait a little longer?”.
Or “Ok, we can save some space here, but if it all fits into my RAM anyway then why bother?”.
These are both great reasons not to care too much about efficiency of your algorithms if your data is small, fits into RAM and the running times aren't significant enough to matter anyway.
So you would go on to program your favorite video game and not care about that professor talking about all that big-Oh nonsense.
And in the short run you would be right. While you are developing a prototype of your favorite video game you shouldn't care.
When I was working at a startup I remember myself learning the hard way that <a href="http://c2.com/cgi/wiki?PrematureOptimization">premature optimization is the root of all evil</a>.

</p>
<div align="center"><img alt="abstruse-goose-video-games" src="http://grigory.github.io/blog/pics/abstruse-goose-video-games.png"/> </div>

<p><br/></p>
<p>
However, once your video game becomes successful and you get to deal with big data that has to be stored and processed in the cloud this reasoning starts to fall short.
Let's say you developed <a href="https://en.wikipedia.org/wiki/Candy_Crush_Saga">Candy Crush Saga</a> (<a href="http://www.standard.co.uk/business/business-news/candy-crush-saga-owner-king-digital-entertainment-valued-at-7bn-9216058.html">valued at $7bn in 2014</a>) and now you are interested in doing some data analytics about your &gt;10 million active users.
You are now considering outsourcing your data storage and computation to the cloud.
Here is where you might want to learn why the design of space and time-efficient algorithms matters for the bottom line of your future business. 

</p><h1>100x more efficient algorithms = 100x less money in billings</h1>

So that time and space your professor was talking about – what does it have to do with your spending on the cloud services?
The answer is surprisingly simple – <b>if you need 100x more time and space then your billing increases 100 times</b>.
Below I used the pricing calculator that comes with Google Compute Engine to see how the cost scales if I want to use 100/1000/10000 identical machines for a year.
<div align="center"><img alt="abstruse-goose-video-games" src="http://grigory.github.io/blog/pics/cloud-pricings.png"/> </div>
<br/>
<p>
I was myself surprised to find this out since I expected some economy of scale to kick in. In fact, sometimes it does but usually is quite negligible. Say, you can get an X% discount but that doesn't help much against linear scaling.
</p>





<p/>

  <p><a href="http://grigory.github.io/blog/the-simple-economics-of-algorithms-for-big-data/">The Simple Economics of Algorithms for Big Data</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on January 20, 2016.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-01-20T00:00:00Z</updated>
    <published>2016-01-20T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/the-binary-sketchman</id>
    <link href="http://grigory.github.io/blog/the-binary-sketchman/" rel="alternate" type="text/html"/>
    <title xml:lang="en">The Binary Sketchman</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p>In this post I will talk about some of my recent work with <a href="http://www.cis.upenn.edu/~kannan/">Sampath Kannan</a> and <a href="https://stat.mit.edu/people/elchanan-mossel/">Elchanan Mossel</a> on linear methods for binary data compression. The paper is <a href="http://eccc.hpi-web.de/report/2016/174/">available here</a>, slides from my talk at Penn are <a href="http://grigory.us/files/talks/penn16.pdf">here</a> and another talk at Columbia is <a href="http://www.cs.columbia.edu/theory/f16-theoryread.html#Grigory">coming up on Nov 21</a>.</p>

<p>Given very large data represented in binary format as a string of length , i.e.  
we are interested in a compression algorithm that can transform  into a much shorter binary string .
Here  so that we can achieve some non-trivial savings in space.
Moreover, if  changes in the future we would like to be able to update our compressed version of it (without having to store the original ).</p>

<p>Clearly compression introduces some loss making it impossible to recover certain properties of the original data from the compressed string.
However, if we know in advance which property of  we are interested in then efficient compression often becomes possible.
We will model the property of interest as a binary function  which labels all possible ’s with two labels.
So our goal will be to be able to: 1) perform this binary classification, i.e. compute  using compressed data  only, 2) do this even if  changes over time – updates for us will be bit flips in the coordinates of  specified by the index of the bit that is getting flipped.</p>

<p>Finally, if  is so big that it can’t be stored locally and has to be divided into chunks stored across multiple machines then we will be able to compress the chunks locally and then combine them on a central server into a compressed version of the entire data – one simple round of MapReduce or whatever your favorite distributed framework is.</p>

<p>To make the above discussion less abstract let’s consider a machine learning application – evaluating a linear classifier over binary data.
Let’s say we have trained a linear classifier of the form  where sign is the sign function. 
Is it possible to compress  in such a way that we can still evaluate our classifier in the scenarios described above?
Turns out we can compress the input down to  bits where  is a parameter of the linear classifier known as its margin. Moreover, no compression scheme can do better.</p>

<h1 id="introducing-the-binary-sketchman">Introducing the Binary Sketchman</h1>

<div align="center"><img alt="The Binary Sketchman" src="http://grigory.github.io/blog/pics/binary-sketchman-final.png"/> </div>

<p><br/></p>

<p>While the setting described above may seem quite challenging it can be handled through a framework of linear sketching.
In the binary case the interpretation of linear sketching is particularly simple as our binary sketchman is just going to compute  parities of the bits of , say for :</p>



<p>In a matrix form this corresponds to computing  where  is a  binary matrix and the operations are performed over .
Note that now our sketch easily satisfies all the requirement above since as  changes we can just update the corresponding parities. In the distributed case we can compute them locally and then add up on a central server.</p>

<p>Unfortunately the power of a deterministic sketchman who just uses a fixed set of parities is quite limited and no such sketchman can compress even a simple linear classifier down to less than  bits.
In fact, even for the OR function  no deterministic sketch can have less than  bits.
So our binary sketchman will “<a href="http://www.cs.cmu.edu/~haeupler/15859F14/">unleash the power of randomization</a>” in his quest for a perfect sketch.
According to <a href="http://www.cs.cmu.edu/~haeupler/">Bernhard Haeupler</a> this can be quite dramatic and looks kind of like this:</p>
<div align="center"><img alt="The power of randomness unleashed" src="http://www.cs.cmu.edu/~haeupler/15859F14/images/posternoinf.jpg" width="300px"/> </div>

<p><br/>
So our sketchman will instead pick the matrix  randomly while the rest is the same as before.
Now the OR function is easy to handle: pick a parity over a random subset of  where each coordinate is included with probability .
If  then this parity catches a non-zero coordinate of  with probability  and thus evaluates to  with probability at least .
If  then the parity never evaluates to  so we can distinguish the two cases with probability  using  such parities.
This illustrates a more general idea – if  is a constant function on all but  different inputs then a sketch of size  suffices.</p>

<p>Now for linear thresholds the high-level ideas behind this sketching process are as follows:
1) observe that any linear threshold function takes the same value on all but  inputs,
2) apply the same argument as above to obtain a sketch of size .
The only thing missing in the above argument is that we still have dependence on .
This can be avoided if we first hash the domain reducing its size down to  which replaces  in the above calculations giving us .
While this compression method is quite simple the remarkable fact is that it can’t be improved.
Even for the simplest threshold function that corresponds to a threshold for the Hamming weight of , i.e. , any compression mechanism would require  bits as follows from <a href="http://link.springer.com/chapter/10.1007/978-3-642-32512-0_44">this work</a> by Dasgupta, Kumar and Sivakumar.
Note that it isn’t assumed that the protocol is based on linear sketching – it can be an arbitrary scheme.</p>

<h1 id="the-power-of-randomized-binary-sketchman">The Power of Randomized Binary Sketchman</h1>

<p>Linear sketching by itself is not a new idea and has been studied extensively in the last two decades.
See surveys by <a href="http://researcher.watson.ibm.com/researcher/view.php?person=us-dpwoodru">Woodruff</a> and <a href="http://people.cs.umass.edu/~mcgregor/">McGregor</a> on how it can be applied to problems in <a href="http://researcher.ibm.com/files/us-dpwoodru/wNow3.pdf">numerical linear algebra</a> and <a href="http://link.springer.com/referenceworkentry/10.1007/978-3-642-27848-8_796-1">graph compression</a>.
However, this work focuses on linear sketching over large finite fields (used to represent real values with bounded precision).
Nevertheless some striking results are known about linear sketching that are applicable in our context as well.
In particular, if  is updated through a very long (triply exponential in ) stream of adversarial updates then linear sketches over finite fields are optimal for any function  as shown by Li, Nguyen and Woodruff <a href="https://pdfs.semanticscholar.org/bf89/98d76741f3ee7b4ba1f82524353e7083c3b5.pdf">here</a> in STOC’14.</p>

<p>As our paper shows the same result holds for much shorter random streams of length  in a simple model where each update flips uniformly at random chosen coordinate of .
In other words binary sketching is optimal if in the end of the stream the input  is uniformly distributed.
The proof of this fact is quite technical and relies on a notion of <i>approximate Fourier dimension</i> for Boolean functions that we use to characterize binary sketching under the uniform distribution – check the paper for details if you are interested.
Whether the same result holds for short (length , say) adversarial streams is the main open question left open.</p>


  <p><a href="http://grigory.github.io/blog/the-binary-sketchman/">The Binary Sketchman</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on October 07, 2016.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-10-07T00:00:00Z</updated>
    <published>2016-10-07T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/teaching-algorithms-for-big-data</id>
    <link href="http://grigory.github.io/blog/teaching-algorithms-for-big-data/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Teaching algorithms for Big Data</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><!--<h1>Teaching &ldquo;algorithms for Big Data&rdquo;</h1>
-->

<p>“algorithms for Big Data” (sometimes the name can slightly vary) is a new graduate class that has been introduced by many top computer science programs in the recent years.
In this post I would like to share my experience teaching this class at the University of Pennsylvania this semester. Here is the <a href="http://grigory.us/big-data-class.html">homepage</a>.</p>

<div align="center"><img alt="Keep calm and crunch data on o(N)" src="http://grigory.github.io/blog/pics/class-logo-large.png"/> </div>

<p><br/></p>

<p>First off, let me get the most frequently asked question out of the way and say that by “big data” in this class I mean data that doesn’t fit into a local RAM
since if the data fits into RAM then algorithms from the standard algorithms curricula will do the job. 
At the moment a terabyte of data is already tricky to fit into RAM so this is where we will draw the line. 
In particular, this is so that the <a href="http://www.frankmcsherry.org/graph/scalability/cost/2015/02/04/COST2.html">arguments about beating algorithms for big data using your laptop</a> don’t apply.</p>

<p>Second, I tried to focus as much as possible on algorithms that are known to work in practice and have implementations.
Because this is a theory class we didn’t do programming but I made sure to give links to publicly available implementations whenever possible.
As it is always the case, the best algorithms to teach are never exactly the same as the best implementations.
Even the most vanilla problem of sorting an array in RAM is handled in C++ STL via a combination of QuickSort, InsertionSort and HeapSort.
Picking the right level of abstraction is always a delicate decision to make when teaching algorithms and I am pretty happy with the set of choices made in this offering.</p>

<p>Finally, “algorithms for Big Data” isn’t an entirely new phenomenon as a class since it builds on its predecessors
typically called “Sublinear Algorithms”, “Streaming Algorithms”, etc.
Here is a <a href="http://grigory.us/big-data-class.html#sketch">list of closely related classes offered at some other schools</a>.
In fact, my version of this class consisted of <a href="http://grigory.us/big-data-class.html#lectures">four modules</a>:</p>

<ul>
<li><b>Part 1: Streaming Algorithms.</b> It is very convenient to start with this topic since techniques developed in streaming turn out to be useful later. In fact, I could as well call this part “linear sketching” since every streaming algorithm that I taught in this part was a linear sketch. I find single-pass streaming algorithms to be the most motivated and for so-called dynamic streams that can contain both insertions and deletions linear sketches are known to be almost optimal under fairly mild conditions.
Moreover, linear sketches are the baseline solution in the more advanced massively parallel computational models studied later.
</li>
<li><b>Part 2: Selected Topics.</b> This part became very eclectic, containing selected topics in numerical linear algebra, convex optimization and compressed sensing.
In fact, some of the algorithms in this part aren't even “algorithms for Big Data” according to the RAM size based definition.
However, I considered these topics to be too important to skip in a “big data” class. 
For example, right after we covered gradient descent methods for convex optimization Google released <a href="https://www.tensorflow.org/">TensorFlow</a>.
This state of the art machine learning library allows one to choose any of its <a href="https://www.tensorflow.org/versions/master/api_docs/python/train.html#optimizers">5 available versions</a> of gradient descent for optimizing learned models. These days when you can run into some <a href="https://aws.amazon.com/machine-learning/pricing/">pretty steep pricing</a> for outsourcing your machine learning to the cloud knowing what is under the hood of free publicly available frameworks I think is increasingly important.  
</li>
<li><b>Part 3: Massively Parallel Computation.</b> I am clearly biased here, but this is my favorite. Unlike, say, streaming where many results are already tight, we are still quite far from understanding full computational power of MapReduce-like systems. Potential impact of such algorithms I think is also likely to be the highest. In this class because of the time constraints I only touched the tip of the iceberg. This part will be expanded in the future.</li>
<li><b>Part 4: Sublinear Time Algorithms.</b> I always liked clever sublinear time algorithms, but for many years believed that they are not quite “big data“ since they operate under the assumption of random access to the data. Well, this year I had to change my mind after Google launched its <a href="https://code.google.com/codejam/distributed_index.html">Distributed Code Jam</a>.
I have to admit that I have no idea how this works on the systems level but apparently it is possible to implement reasonably fast random access to large data.
The problems that I have seen being used for Distributed Code Jam allow one to use 100 nodes each having small RAM. The goal is to process a large dataset available via random access.
</li>
</ul>

<p>Overall parts 1 and 4 are by now fairly standard. Part 2 has some new content from <a href="http://researcher.watson.ibm.com/researcher/files/us-dpwoodru/journal.pdf">David Woodruff’s great new survey</a>. Some algorithms from it are also available in IBM’s <a href="https://github.com/xdata-skylark/libskylark">Skylark library for fast computational linear algebra and machine learning</a>.
Part 3 is what makes this class most different from most other similar classes.</p>

<h1>Mental Notes</h1>
<p>Here is a quick summary of things I was happy with in this offering + potential changes in the future.</p>
<ul>
<li><b>Research insights.</b> One of the main reasons why I love teaching is that it often leads to research insights, especially when it comes to simple connections I have been missing. For example, I didn't previously realize that one can use <a href="http://grigory.us/files/publications/BRY14-Lp-Testing.pdf">L<sub>p</sub>-testing</a> as a tool for testing assumptions about convexity and Lipschitzness used in the analysis of the convergence rate of gradient descent methods. </li>
<li><b>Project.</b> Overall I am very happy with the students' projects. 
Some students implemented algorithms, some wrote surveys and some started new research projects.
Most unexpected to me were the projects done by non-theory students connecting their areas of expertise with the topics discussed in the class. E.g. surveys of streaming techniques used in natural language processing and bionformatics were really fun to read.</li> 
<li><b>Cross-list the class for other departments.</b> It was a serious blunder on my behalf to not cross-list this class for other departments, especially Statistics and Applied Math.
Given how much interest there is from other fields this is probably the easiest to fix and the most impactful mistake.
Somehow some students from other departments learned about the class anyway and expressed their interest, often too late.</li>
<li><b>New content.</b> Because of time constraints I couldn't fit in some of the topics I really wanted to cover.
These include coresets (there has been a resurgence of interest in coresets for massively parallel computing, but I didn't have time to cover it), nearest neighbor search (somehow I couldn't find a good source to teach from, suggestions are very welcome), Hyperloglog algorithm (same reason), more algorithms for massively parallel computing (no time), more sublinear time algorithms (no time).    
In the next version of this class I will make sure to cover at least some of these. 
</li>
<li><b>Better structure.</b> Overall I am pretty happy with the structure of the class but there is definitely room for improvement. A priority will be to better incorporate selected topics discussed in Part 2 into the overall structure of the class. In particular, convex optimization came a little out of the blue even though I am really glad I included it.</li>
<li><b>Slides and equipment.</b> I really like teaching with slides that contain only some of the material and use the blackboard to fill in the missing details and pictures.
On one hand, slides are a backbone that the students can later use to catch up on the parts they missed.  On the other hand, the risk of rushing through the slides too fast is minimized since the details are discussed on the board. Also a lot of time is saved on drawing pictures. I initially used Microsoft Surface Pro 2 to fill in the gaps on the tablet instead of the board but later gave up on this idea because of technical difficulties. Having a larger tablet would help too. I still think that the tablet can work but requires a better setup. Next time I will try to use the tablet again and post the final slides online. 
</li>
<li><b>Assign homework and get a TA.</b> Michael Kearns and I managed to teach “Computational Learning Theory” without a TA last semester so I decided against getting one for my class as well. This was fine except that having a TA for grading homework would have helped a lot.</li>
<li><b>Make lecture notes and maybe videos.</b> With fairly detailed slides I didn't consider lecture notes necessary. Next time it would be nice to have some since some of my fellow facutly friends asked for them. I think I will stick with the tested “a single scribe per lecture“ approach although I heard in France students sometimes collaboratively work on the same file during the lecture and the result comes out nice. When I had to scribe lectures I just LaTeXed them on the fly so I don't see why you can't do this collaboratively. 
As for videos, Jelani had <a href="http://people.seas.harvard.edu/~minilek/cs229r/fall15/lec.html">videos</a> from his class this time and they look pretty good. </li> 
<li><b>Consider MOOCing.</b> Given that the area is in high demand doing a MOOC in the future is definitely an option. It would be nice to stabilize the content first so that the startup cost of setting up a MOOC could be amortized by running it multiple times.</li>
</ul>

<h1>Thanks</h1>
<p>I am very grateful to my friends and colleagues discussions with whom helped me a lot while developing this class.
Thanks to Alex Andoni, Ken Clarkson, Sampath Kannan, Andew McGregor, Jelani Nelson, Eric Price, Sofya Raskhodnikova, Ronitt Rubinfeld and David Woodruff (this is an incomplete list, sorry if I forgot to mention you). Special thanks to all the students who took the class and <a href="http://www.seas.upenn.edu/~sassadi/">Sepehr Assadi</a> who gave a guest lecture on our <a href="http://arxiv.org/pdf/1505.01467.pdf">joint paper about linear sketches of approximate matchings</a>.</p>

  <p><a href="http://grigory.github.io/blog/teaching-algorithms-for-big-data/">Teaching algorithms for Big Data</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on December 24, 2015.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2015-12-24T00:00:00Z</updated>
    <published>2015-12-24T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/stoc-focs-proposal-colocate</id>
    <link href="http://grigory.github.io/blog/stoc-focs-proposal-colocate/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Colocate, Colocate, Colocate</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p>Adding my two cents to the discussion of the new format for STOC/FOCS conferences I would like to propose only one change which I think is also fairly modest: colocate, colocate, colocate. Well, I agree that it sounds like three changes — the point is that the more colocation the better :) In fact, I realized that I once again agree with Matt Welsh (who recently proposed a similar change for conferences in his community <a href="http://matt-welsh.blogspot.com/2015/05/a-modest-proposal-sosigcommobixdi.html">here</a>) which often happens when he takes a break from bashing academia.
Here are a few fairly straightforward reasons why I think colocation of multiple conferences at the same location and similar time is good:</p>
<ul>
<li> It has been tested and already works pretty well at FCRC. There are things that can happen at a scale of multiple communities that don't happen at the scale of just theory conferences. This year FCRC is hosting SPAA/EC/CCC as well as a few other conferences which might be of interest to theorists.
Possible synergies between different communities can be in the form of joint workshops, tutorials, keynotes, award lectures, etc. E.g., I hope that the <a href="http://grigory.us/mpc-workshop-fcrc.html">workshop on massively parallel algorithms</a> that I am co-organizing will benefit a lot from colocation with other conferences at FCRC. Overall, I am pretty sure these advantages are already fairly well understood. 
</li>
<li>Increased number of options among possible talks to attend. I am sure almost everyone has been in a situation when there is nothing interesting happening at their favorite conferences. I would personally much rather attend a great talk on a new topic I don't know much about (even if it is applied) than sit through a mediocre STOC/FOCS talk.
</li>
<li>
Less travel. Well, at a certain stage of their career I believe many of us would like to have to travel less. 
Now a more subtle aspect here is that there are conferences that I would really like to attend but I don't submit my papers there (e.g. EC, ICML, COLT), so I would really love to see them colocated with other conferences that I usually attend. 
</li>
<li>
No structural changes to the format of existing conferences. This eliminates all concerns associated with allocation of credit for publications, presentations, etc. thus ensuring backwards compatibility.
</li>
</ul>

<h1>What to colocate?</h1>
<p>A possible idea for colocation might be to change the set of colocated conferences in different years which creates a lot of opportunities.
Here are some concrete proposals and I am pretty sure you can come up with more:</p>
<ul>
<li><b>STOC+FOCS+...</b> Possible proposals for ... are: CCC, SPAA, PODC, EC, SOCG, ICALP, COLT, ICML, SIGMOD, PODS  since they happen around the same time. </li> 
&lt;/li&gt;
<li><b>SODA+ITCS</b>. I really try to attend both conferences whenever I can given that SODA and ITCS always happen back to back. Without colocation this always creates a seemingly unnecessary logistical overhead. In fact, I first heard this proposal from researchers at Google NYC who strongly supported it.
</li>
<li>
<b>Other conferences</b>. Some of the conferences that don't quite fit in given the time of the year when they usually happen but I would personally love to see colocated in some way: NIPS, VLDB, KDD, CIKM, WSDM, ICDM. I am pretty sure some people have their own list too (e.g. crypto conferences).
</li>
</ul>


  <p><a href="http://grigory.github.io/blog/stoc-focs-proposal-colocate/">Colocate, Colocate, Colocate</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on June 02, 2015.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2015-06-02T00:00:00Z</updated>
    <published>2015-06-02T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/mtd</id>
    <link href="http://grigory.github.io/blog/mtd/" rel="alternate" type="text/html"/>
    <title xml:lang="en">67th Midwest Theory Day</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><div align="center"><img alt="IU :)" src="http://grigory.github.io/blog/pics/iu-sample-gates.jpg"/> </div>

<p><br/>
<a href="http://caml.indiana.edu/mtd.html">67th Midwest Theory</a> <strike>Day</strike> Weekend took place two weeks ago here at Indiana University, Bloomington organized by <a href="http://homes.soic.indiana.edu/qzhangcs/">Qin Zhang</a>, <a href="http://homes.soic.indiana.edu/yzhoucs/">Yuan Zhou</a> and myself. We were lucky to have a flurry of fantastic speakers from the Midwest and three external headliners: <a href="http://www.mit.edu/~andoni/">Alex Andoni</a>, <a href="https://people.csail.mit.edu/mirrokni/Welcome.html">Vahab Mirrokni</a> and <a href="https://www.cs.cmu.edu/~odonnell/">Ryan O’Donnell</a>. Slides are now posted online.</p>

<p>As organizers we’ve decided to experiment with the format of this MTD making it a 2-day event. Based on feedback we received we believe that this format has worked out perfectly. For a geographically spread out Midwest area travel logistics makes overheads of attending a one-day event often too much for many of those interested to come.</p>


  <p><a href="http://grigory.github.io/blog/mtd/">67th Midwest Theory Day</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on April 30, 2017.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2017-04-30T00:00:00Z</updated>
    <published>2017-04-30T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/foundations-of-data-science-class</id>
    <link href="http://grigory.github.io/blog/foundations-of-data-science-class/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Teaching Foundations of Data Science</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p>This week I started teaching a graduate class called “<a href="http://grigory.us/data-science-class.html">Foundations of Data Science</a>” that will be mostly based on an eponymous book by <a href="https://en.wikipedia.org/wiki/Avrim_Blum">Avrim Blum</a>, <a href="https://en.wikipedia.org/wiki/John_Hopcroft">John Hopcroft</a> and <a href="https://en.wikipedia.org/wiki/Ravindran_Kannan">Ravi Kannan</a>.
The book is still a draft and I am using <a href="http://grigory.us/files/bhk-book.pdf">this version</a>.
Target audience includes advanced undergraduate and graduate level students.
We had some success using this book as a core material for an undergraduate class at Penn this Spring (<a href="http://www.thedp.com/article/2016/02/cis-399-students">link to the news article</a>).
The draft has been around for a while and in fact I ran a reading group that used it four years back when I was in grad school and the book was called 
“Computer Science Theory for the Information Age”.</p>

<div align="center"><img alt="Keep calm and dig foundations of Data Science" src="http://grigory.github.io/blog/pics/b609-poster-homepage.png" width="200px"/> </div>

<p>“Data Science” is one of those buzzwords that can mean very different things to different people.
In particular, a new graduate <a href="http://www.soic.indiana.edu/graduate/degrees/data-science/index.html">Masters program in Data Science here at IU</a> attracts hundreds of students from diverse backgrounds. 
What I personally really like about the Blum-Hopcroft-Kannan book is that it doesn’t go into any philosophy about the meaning of data science but rather offers a collection of mathematical tools and topics that can be considered as foundational for data science as seen from computer science perspective.
It should be noted that just as any “Foundations of Computing” class has little to do with finding bugs in your code so do this class and book have little to do with data cleaning and other data analysis routine.</p>

<h1>Topics</h1>

<p>While the jury is still out on what topics should be considered as fundamental for data science I think that the Blum-Hopcroft-Kannan book makes a good first step in this direction.</p>

<p>Let’s look at the table of contents:</p>
<ul>
<li>Chapter 2 introduces basic properties of the high-dimensional space, focusing on concentration of measure, properties of high-dimensional Gaussians and basic dimension reduction. </li>
<li>Chapter 3 covers the Singular Value Decomposition (SVD) and its applications (principal component analysis, clustering mixture of Gaussians, etc.).</li>
<li>Chapter 4 focuses on random graphs (primarily in the Erdos-Renyi model).</li>
<li>Chapter 5 introduces random walks and Markov chains, including Markov Chain Monte Carlo methods, random works on graphs and applications such as Page Rank.</li>
<li>Chapter 6 covers the very basics of machine learning theory, including learning basic function classes, perceptron algorithm, regularization, kernelization, support vector machines, VC-dimension bounds, boosting, stochastic gradient descent and a bunch of other topics. </li>
<li>Chapter 7 describes a couple of streaming and sampling methods for big data: frequency moments in streaming and matrix sampling.</li>
<li>Chapter 8 is about clustering methods: k-means, k-center, spectral clustring, cut-based clustering, etc.</li>
<li>Chapters 9 through 11 cover a very diverse set of topics that includes hidden Markov processes, graphical models, belief propagation, topic models, voting systems, compressed sensing, optimization methods and wavelets among others.</li>
</ul>

<h1>Discussion</h1>
<p>Overall this looks like a good stab at the subject and a big advantage of this book is that unlike some of its competitors it treats its topics with mathematical rigor.
The only chapter that I personally don’t really see fit into a “data science” class is Chapter 4. Because of its focus on the Erdos-Renyi model that I haven’t seen being used realistically for graph modeling applications this chapter seems to be mostly of purely mathematical interest.</p>

<p>Selection of some of the smaller topics is a matter of personal taste, especially when it comes to those that are missing.
A couple of quick suggestions is to cover new sketching algorithms for <a href="http://researcher.watson.ibm.com/researcher/files/us-dpwoodru/wNow.pdf">high-dimensional linear regression</a>, <a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">locality-sensitive hashing</a> and possibly <a href="http://groups.csail.mit.edu/netmit/sFFT/index.html">Sparse FFT</a>.</p>

<p>Slides will be posted <a href="http://grigory.us/data-science-class.html#lectures">here</a> and I will write a report on the final selection of topics and my experience in the end of semester. Stay tuned :)</p>

  <p><a href="http://grigory.github.io/blog/foundations-of-data-science-class/">Teaching Foundations of Data Science</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on August 27, 2016.</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </content>
    <updated>2016-08-27T00:00:00Z</updated>
    <published>2016-08-27T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2018-05-27T09:46:42Z</updated>
    </source>
  </entry>
</feed>
