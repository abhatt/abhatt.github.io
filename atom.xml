<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-11-30T09:38:38Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/29/assistant-professor-at-university-of-toronto-apply-by-january-10-2022/</id>
    <link href="https://cstheory-jobs.org/2021/11/29/assistant-professor-at-university-of-toronto-apply-by-january-10-2022/" rel="alternate" type="text/html"/>
    <title>Assistant Professor at University of Toronto (apply by January 10, 2022)</title>
    <summary>The Department of Computer Science at the University of Toronto is conducting three open-area searches for full-time tenure stream positions. The appointment will be at the rank of Assistant Professor and will commence on July 1, 2022, or shortly thereafter. We start reviewing applications on December 6, 2021. Website: https://academicjobsonline.org/ajo/jobs/19687 Email: recruit@cs.toronto.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Toronto is conducting three open-area searches for full-time tenure stream positions. The appointment will be at the rank of Assistant Professor and will commence on July 1, 2022, or shortly thereafter. We start reviewing applications on December 6, 2021.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/19687">https://academicjobsonline.org/ajo/jobs/19687</a><br/>
Email: recruit@cs.toronto.edu</p></div>
    </content>
    <updated>2021-11-29T16:38:44Z</updated>
    <published>2021-11-29T16:38:44Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-30T09:37:33Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2021/11/29/cluster-power/</id>
    <link href="http://benjamin-recht.github.io/2021/11/29/cluster-power/" rel="alternate" type="text/html"/>
    <title>The cult of statistical significance and the Bangladesh Mask RCT.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the last post, <a href="https://www.argmin.net/2021/11/23/mask-rct-revisited/">I argued that the effect size in the Bangladesh Mask RCT was too small to inform policy making</a>. I deliberately avoided diving into statistical significance as arguments about p-values quickly devolve into scientific gish gallop. Statistical validity is the most overrated form of experimental validity, and it crowds out more important questions of effect size, bias, design, and applicability.</p>

<p>But shoot, sometimes Byzantine academic arguments are fun. And, though they are always wrong, sometimes they are useful. In this blog I want to discuss how we analyze statistical validity in cluster randomized controlled trials. It is quite subtle and sensitive, and should give us pause about these experimental designs. The sample sizes needed for validating large effects in cluster randomized trials can be absurdly high, and running a clean trial with millions of participants is likely impossibly difficult and almost never worth doing.</p>

<p>To review in the <a href="https://www.poverty-action.org/sites/default/files/publications/Mask_Second_Stage_Paper_20211108.pdf.pdf">Bangladesh Mask RCT</a> there were $n_C=$163,861 individuals from $k_C=$300 villages in the control group. There were $n_T=$178,322 individuals from $k_T=$300 villages in the intervention group. The main end point of the study was whether their intervention reduced the number of individuals who reported covid-like symptoms and tested seropositive at some point during the trial. There were $i_C=$1,106 symptomatic individuals confirmed seropositive in the control group and $i_T=$1,086 such individuals in the treatment group.</p>

<p>What can we say about the statistical significance of this 20 case difference? Most would guess this difference is not significant. Indeed, in a balanced design with $n_T=n_C$ and 180,000 individuals in each arm, this study would not be statistically significant. Let’s imagine we ran an experiment where we could treat the outcomes of each individual as independent, identically distributed random variables. What is the p-value associated with the null hypothesis that the prevalence of infections in the control group is less than or equal to the prevalence in the treatment group? A simple statistical test of this hypothesis is the z-test for proportions. For the z-test, the p-value when the groups are balanced is 0.3.</p>

<p>The authors claim a balanced design, but, though the number of treatment and control villages are indeed equal, the number of <em>individuals</em> in the treatment group is 1.1x bigger than the control group. <a href="https://www.argmin.net/2021/11/23/mask-rct-revisited/">As I mentioned in the previous post</a>, this discrepancy can likely be explained by the large differential in response rates between the groups: 1.05x fewer households were approached for surveys in control and the control group responded at 1.07x lower rate than treatment. The most significant difference between the treatment and control group may very well be the consent rate of the household survey. For the medical statistics experts, <a href="https://en.wikipedia.org/wiki/Intention-to-treat_analysis">the intention to treat principle</a> says that the  individuals who are unreachable or who refuse to be surveyed must be counted in the study. Omitting them invalidates the study.</p>

<p>But the issues of significance remain even if we forgive this large imbalance in the study. If we re-run the z-test with the $n_C$ and $n_T$ in the study data, the p-value is now 0.009, which would be quite significant at the standard p &lt; 0.05 threshold. However, the individual outcomes are <em>not</em> independent. The trial was cluster-randomized, so everyone in the same village received the same intervention. This means that the outcomes inside a village are correlated, and they are likely more correlated inside a village than outside.</p>

<p>To capture the correlation among intra-cluster participants, statisticians use the notion of the <a href="https://www.povertyactionlab.org/resource/power-calculations"><em>intra-cluster correlation coefficient</em></a> $\rho$. $\rho$ is a scalar between 0 and 1 that measures the relative variance within clusters and between clusters. When $\rho=1$, all of the responses in each cluster are identical. When $\rho=0$, the clustering has no effect, and we can treat our assignment as purely randomized. Once we know $\rho$ we can compute an <em>effective sample size</em>: if the villages are completely correlated, the number of samples in the study would be 600. If they were independent, the number of samples would be over 340,000. The number of effective samples is equal to the total number of samples divided by the <em>design effect</em>:</p>

\[{\small
    DE = 1+\left(\frac{n_T+n_C}{k_T+k_C}-1\right)\rho \,.
}\]

<p>What is the design effect of the Bangladesh RCT? Measuring the intra-cluster correlation $\rho$ is nontrivial: the true value of $\rho$ depends on both potential outcomes in an experiment and needs to be estimated using some side experiment or previous trials at baseline. $\rho$ is often inferred from secondary covariates of earlier experiments on a similar population. We don’t have a pre-specified estimate, but can cheat a bit here and estimate $\rho$ from the provided data in the control villages. A standard ANOVA calculation says that the observed symptomatic seropositivity in the control villages has an intra-village correlation of $\rho=$0.007. This value isn’t particularly unreasonable. Some practitioners suggest that because of behavioral contagion alone, $\rho$ should be <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1466680">between 0.01 and 0.02 for human studies.</a> <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0013998#pone.0013998-Carrat2">This cluster RCT on mask use to prevent influenza in households</a> uses $\rho=$0.24.  As a sanity check, the intra-village correlation of reported systems is 0.03. So let’s stick with $\rho=$0.007 and see where it takes us.</p>

<p>For the Bangladesh RCT, assuming $\rho=$0.007, the design effect is about 5. This reduces the effective sample size from over 340,000 to just under 70,000. What happens with our z-test? We simply take the z-score and divide by the square root of the design effect, yielding a p-value of 0.14. The result is not statistically significant once we take into account the intra-cluster correlation. In order to “achieve” statistical significance, $\rho$ would need to be less than 0.001 and the design effect would have to be less than 2.2.</p>

<p>We can also do similar design-effect adjustments for relative risk reduction. Recall the relative risk reduction is the ratio of the rate of infection in the treatment group to the rate of infection in the control group</p>

\[{\small
    RR = \frac{i_T/n_T}{i_C/n_C}\,.
}\]

<p>A small $RR$ corresponds to a large reduction in risk. For the mask study, the estimated risk reduction is $RR=$0.9.  If the assignments of every individual to treatment and control were random, we could compute error bars on the log of the risk ratio. The log risk ratio is</p>

\[{\small
    \ell RR = \log \frac{i_T/n_T}{i_C/n_C}\,,
}\]

<p>and <a href="https://en.wikipedia.org/wiki/Relative_risk#Inference">a standard estimate of the standard error $SE$ of $\ell RR$</a> is</p>

\[{\small
    SE = \sqrt{ \frac{1}{i_T} + \frac{1}{i_C} - \frac{1}{n_T}- \frac{1}{n_C}}\,.
}\]

<p>In the mask study, $SE=$0.043. Using a Gaussian approximation, our confidence interval would then be</p>

\[{\scriptsize
    [\exp(\ell RR - 1.96 SE), \exp(\ell RR + 1.96 SE)] = [0.83, 0.98]\,.
}\]

<p>The way we interpret the confidence interval (and I’ll likely screw this up) is that if the Gaussian approximation were true, and all of the individual assignments to treatment and control were independent, and we repeated the experiment many times, the true risk ratio would fall inside the confidence interval 95% of the time. This calculation suggests that the confidence interval (barely) excludes a risk ratio of 1. However, this calculation does not take into account the cluster effects.  Assuming again that $\rho=$0.007, when we adjust our confidence intervals for cluster effects, we get the larger interval</p>

\[{\scriptsize
    \left[\exp(\ell RR - 1.96 SE \sqrt{DE}), \exp(\ell RR + 1.96 SE \sqrt{DE} )\right] = [0.75, 1.09]\,.
}\]

<p>Again, a standard cluster RCT analysis would not be able to reject a null effect for the complex masking intervention. In terms of my most-loathed statistic of efficacy, the confidence interval ranges from -9% to 25% after adjustment.</p>

<p>Note that even the strong claims made in the paper about subgroups are not significant once intra-cluster correlation is accounted for. A commonly quoted result is that surgical masks dramatically reduced infections for individuals over 60 years old. In this case, $n_C =$14,826, $n_T$=16,088, $i_C=$157 and $i_T=$124. The estimated effectiveness is 27%. However, with a design effect of 5, the p-value for the z-test here is 0.13 and the confidence intervals for the efficacy are -23% to 57%. So again, one can’t rest on statistical significance to argue this effect is real.</p>

<p>As a last statistical grumble, all of these corrections don’t even account for the multiple hypothesis testing in the manuscript where nearly 200 hypotheses were evaluated. <strong>After a <a href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni correction</a> and accounting for design effect, none of the p-values would be less than 0.5.</strong></p>

<p>How large would the trial have to be in order to have statistical significance? We can focus on the z-test, and ask how many samples would be needed to reject the null hypothesis 95% of the time when the relative risk is 0.9, the prevalence in the control group is 0.076, and the intra-cluster correlation is 0.007. The answer is <strong><em>1.1 million people</em></strong>, over 3 times larger than the actual study size.</p>

<p>When a power calculation reveals a trial needs more than a million subjects, researchers need to pause to think if they are asking the right question. It is likely impossible to conduct a precise experiment that rules out all confounding at such a scale. The number of people needed to run such a trial is huge, and maintaining data quality would be both prohibitively difficult and expensive. Any trial has potential harms to its subjects, and the larger the sample size, the more likely harm may occur. Ensuring beneficence and informed consent at this scale is likely impossible. And if one really expects the clinical significance to be this small, why invest all of these resources into running an RCT instead of looking for more powerful interventions?</p>

<p><em>For those interested in seeing how I computed all of the numbers in this post and the <a href="https://www.argmin.net/2021/11/23/mask-rct-revisited/">last post</a>, <a href="https://nbviewer.jupyter.org/url/argmin.net/code/revisiting-bd-mask-rct.ipynb">here is a Jupyter notebook.</a></em></p></div>
    </summary>
    <updated>2021-11-29T00:00:00Z</updated>
    <published>2021-11-29T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2021-11-30T05:21:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13621</id>
    <link href="http://arxiv.org/abs/2111.13621" rel="alternate" type="text/html"/>
    <title>An Optimal Algorithm for Finding Champions in Tournament Graphs</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Lorenzo Beretta, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nardini:Franco_Maria.html">Franco Maria Nardini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trani:Roberto.html">Roberto Trani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Venturini:Rossano.html">Rossano Venturini</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13621">PDF</a><br/><b>Abstract: </b>A tournament graph $T = \left(V, E \right)$ is an oriented complete graph,
which can be used to model a round-robin tournament between $n$ players. In
this paper, we address the problem of finding a champion of the tournament,
also known as Copeland winner, which is a player that wins the highest number
of matches. Solving this problem has important implications on several
Information Retrieval applications, including Web search, conversational IR,
machine translation, question answering, recommender systems, etc. Our goal is
to solve the problem by minimizing the number of times we probe the adjacency
matrix, i.e., the number of matches played. We prove that any
deterministic/randomized algorithm finding a champion with constant success
probability requires $\Omega(\ell n)$ comparisons, where $\ell$ is the number
of matches lost by the champion. We then present an optimal deterministic
algorithm matching this lower bound without knowing $\ell$ and we extend our
analysis to three strictly related problems. Lastly, we conduct a comprehensive
experimental assessment of the proposed algorithms to speed up a
state-of-the-art solution for ranking on public data. Results show that our
proposals speed up the retrieval of the champion up to $13\times$ in this
scenario.
</p></div>
    </summary>
    <updated>2021-11-29T22:45:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13552</id>
    <link href="http://arxiv.org/abs/2111.13552" rel="alternate" type="text/html"/>
    <title>Edge-Vertex Dominating Set in Unit Disk Graphs</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singireddy:Vishwanath_R=.html">Vishwanath R. Singireddy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Basappa:Manjanna.html">Manjanna Basappa</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13552">PDF</a><br/><b>Abstract: </b>Given an undirected graph $G=(V,E)$, a vertex $v\in V$ is edge-vertex (ev)
dominated by an edge $e\in E$ if $v$ is either incident to $e$ or incident to
an adjacent edge of $e$. A set $S^{ev}\subseteq E$ is an edge-vertex dominating
set (referred to as ev-dominating set) of $G$ if every vertex of $G$ is
ev-dominated by at least one edge of $S^{ev}$. The minimum cardinality of an
ev-dominating set is the ev-domination number. The edge-vertex dominating set
problem is to find a minimum ev-domination number. In this paper we prove that
the ev-dominating set problem is {\tt NP-hard} on unit disk graphs. We also
prove that this problem admits a polynomial-time approximation scheme on unit
disk graphs. Finally, we give a simple 5-factor linear-time approximation
algorithm.
</p></div>
    </summary>
    <updated>2021-11-29T22:50:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13476</id>
    <link href="http://arxiv.org/abs/2111.13476" rel="alternate" type="text/html"/>
    <title>Reconfiguration of Regular Induced Subgraph</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eto:Hiroshi.html">Hiroshi Eto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ito:Takehiro.html">Takehiro Ito</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Yasuaki.html">Yasuaki Kobayashi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Otachi:Yota.html">Yota Otachi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wasa:Kunihiro.html">Kunihiro Wasa</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13476">PDF</a><br/><b>Abstract: </b>We study the problem of checking the existence of a step-by-step
transformation of $d$-regular induced subgraphs in a graph, where $d \ge 0$ and
each step in the transformation must follow a fixed reconfiguration rule. Our
problem for $d=0$ is equivalent to \textsc{Independent Set Reconfiguration},
which is one of the most well-studied reconfiguration problems. In this paper,
we systematically investigate the complexity of the problem, in particular, on
chordal graphs and bipartite graphs. Our results give interesting contrasts to
known ones for \textsc{Independent Set Reconfiguration}.
</p></div>
    </summary>
    <updated>2021-11-29T22:46:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13464</id>
    <link href="http://arxiv.org/abs/2111.13464" rel="alternate" type="text/html"/>
    <title>How heavy independent sets help to find arborescences with many leaves in DAGs</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fernandes:Cristina_G=.html">Cristina G. Fernandes</a>, Carla N. Lintzmayer <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13464">PDF</a><br/><b>Abstract: </b>Trees with many leaves have applications on broadcasting, which is a method
in networks for transferring a message to all recipients simultaneously.
Internal nodes of a broadcasting tree require more expensive technology,
because they have to forward the messages received. We address a problem that
captures the main goal, which is to find spanning trees with few internal nodes
in a given network. The Maximum Leaf Spanning Arborescence problem consists of,
given a directed graph D, finding a spanning arborescence of D, if one exists,
with the maximum number of leaves. This problem is known to be NP-hard in
general and MaxSNP-hard on the class of rooted directed acyclic graphs. In this
paper, we explore a relation between Maximum Leaf Spanning Arborescence in
rooted directed acyclic graphs and maximum weight set packing. The latter
problem is related to independent sets on particular classes of intersection
graphs. Exploiting this relation, we derive a 7/5-approximation for Maximum
Leaf Spanning Arborescence on rooted directed acyclic graphs, improving on the
previous 3/2-approximation. The approach used might lead to improvements on the
best approximation ratios for the weighted k-set packing problem.
</p></div>
    </summary>
    <updated>2021-11-29T22:42:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13269</id>
    <link href="http://arxiv.org/abs/2111.13269" rel="alternate" type="text/html"/>
    <title>On Queries Determined by a Constant Number of Homomorphism Counts</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Yijia.html">Yijia Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Flum:J=ouml=rg.html">Jörg Flum</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Mingjun.html">Mingjun Liu</a>, Zhiyang Xun <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13269">PDF</a><br/><b>Abstract: </b>It is well known [Lov\'{a}sz, 1967] that up to isomorphism a graph $G$ is
determined by the homomorphism counts $\hom(F, G)$, i.e., the number of
homomorphisms from $F$ to $G$, where $F$ ranges over all graphs. Moreover, it
suffices that $F$ ranges over the graphs with at most as many vertices as $G$.
Thus in principle we can answer any query concerning $G$ with only accessing
the $\hom(\cdot,G)$'s instead of $G$ itself. In this paper, we zoom in on those
queries that can be answered using a constant number of $\hom(\cdot,G)$ for
every graph $G$. We observe that if a query $\varphi$ is expressible as a
Boolean combination of universal sentences in first-order logic, then whether a
graph $G$ satisfies $\varphi$ can be determined by the vector
\[\overrightarrow{\mathrm{hom}}_{F_1, \ldots, F_k}(G):= \big(\mathrm{hom}(F_1,
G), \ldots, \mathrm{hom}(F_k, G)\big),\] where the graphs $F_1,\ldots,F_k$ only
depend on $\varphi$. This leads to a query algorithm for $\varphi$ that is
non-adaptive in the sense that those $F_i$ are independent of the input $G$. On
the other hand, we prove that the existence of an isolated vertex, which is not
definable by such a $\varphi$ but in first-order logic, cannot be determined by
any $\overrightarrow{\mathrm{hom}}_{F_1, \ldots, F_k}(\cdot)$. These results
provide a clear delineation of the power of non-adaptive query algorithms with
access to a constant number of $\hom(\cdot, G)$'s.
</p>
<p>For adaptive query algorithms, i.e., algorithms that might access some
$\hom(F_{i+1}, G)$ with $F_{i+1}$ depending on $\hom(F_1, G), \ldots, \hom(F_i,
G)$, we show that three homomorphism counts $\hom(\cdot,G)$ are both sufficient
and in general necessary to determine the graph $G$. In particular, by three
adaptive queries we can answer any question on $G$. Moreover, adaptively
accessing two $\hom(\cdot, G)$'s is already enough to detect an isolated
vertex.
</p></div>
    </summary>
    <updated>2021-11-29T22:40:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13263</id>
    <link href="http://arxiv.org/abs/2111.13263" rel="alternate" type="text/html"/>
    <title>Negative curvature obstructs acceleration for geodesically convex optimization, even with exact first-order oracles</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Christopher Criscitiello, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boumal:Nicolas.html">Nicolas Boumal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13263">PDF</a><br/><b>Abstract: </b>Hamilton and Moitra (2021) showed that it is not possible to accelerate
Riemannian gradient descent in the hyperbolic plane if we restrict ourselves to
algorithms which make queries in a (large) bounded domain and which receive
gradients and function values corrupted by a (small) amount of noise. We show
that acceleration remains unachievable for any deterministic algorithm which
receives exact gradient and function-value information (unbounded queries, no
noise). Our results hold for the classes of strongly and nonstrongly
geodesically convex functions, and for a large class of Hadamard manifolds
including hyperbolic spaces and the symmetric space $\mathrm{SL}(n) /
\mathrm{SO}(n)$ of positive definite $n \times n$ matrices of determinant one.
This cements a surprising gap between the complexity of convex optimization and
geodesically convex optimization: for hyperbolic spaces, Riemannian gradient
descent is optimal on the class of smooth and geodesically convex functions.
The key idea for proving the lower bound consists of perturbing the hard
functions of Hamilton and Moitra (2021) with sums of bump functions chosen by a
resisting oracle.
</p></div>
    </summary>
    <updated>2021-11-29T22:40:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13253</id>
    <link href="http://arxiv.org/abs/2111.13253" rel="alternate" type="text/html"/>
    <title>A Simple and Tight Greedy OCRS</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Livanos:Vasilis.html">Vasilis Livanos</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13253">PDF</a><br/><b>Abstract: </b>In recent years, Contention Resolution Schemes (CRSs), introduced by Chekuri,
Vondr\'{a}k, and Zenklusen, have emerged as a general framework for obtaining
feasible solutions to combinatorial optimization problems with constraints. The
idea is to first solve a continuous relaxation and then round the fractional
solution. When one does not have any control on the order of rounding, Online
Contention Resolution Schemes (OCRSs) can be used instead, and have been
successfully applied in settings such as prophet inequalities and stochastic
probing. Intuitively, a greedy OCRS has to decide which elements to include in
the integral solution before the online process starts.
</p>
<p>In this work, we give a simple $1/e$ - selectable greedy single item OCRS,
and then proceed to show that it is optimal.
</p></div>
    </summary>
    <updated>2021-11-29T22:46:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13240</id>
    <link href="http://arxiv.org/abs/2111.13240" rel="alternate" type="text/html"/>
    <title>New Diameter-Reducing Shortcuts and Directed Hopsets: Breaking the $\sqrt{n}$ Barrier</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kogan:Shimon.html">Shimon Kogan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parter:Merav.html">Merav Parter</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13240">PDF</a><br/><b>Abstract: </b>For an $n$-vertex digraph $G=(V,E)$, a \emph{shortcut set} is a (small)
subset of edges $H$ taken from the transitive closure of $G$ that, when added
to $G$ guarantees that the diameter of $G \cup H$ is small. Shortcut sets,
introduced by Thorup in 1993, have a wide range of applications in algorithm
design, especially in the context of parallel, distributed and dynamic
computation on directed graphs. A folklore result in this context shows that
every $n$-vertex digraph admits a shortcut set of linear size (i.e., of $O(n)$
edges) that reduces the diameter to $\widetilde{O}(\sqrt{n})$. Despite
extensive research over the years, the question of whether one can reduce the
diameter to $o(\sqrt{n})$ with $\widetilde{O}(n)$ shortcut edges has been left
open.
</p>
<p>We provide the first improved diameter-sparsity tradeoff for this problem,
breaking the $\sqrt{n}$ diameter barrier. Specifically, we show an
$O(n^{\omega})$-time randomized algorithm for computing a linear shortcut set
that reduces the diameter of the digraph to $\widetilde{O}(n^{1/3})$. This
narrows the gap w.r.t the current diameter lower bound of $\Omega(n^{1/6})$ by
[Huang and Pettie, SWAT'18]. Moreover, we show that a diameter of
$\widetilde{O}(n^{1/2})$ can in fact be achieved with a \emph{sublinear} number
of $O(n^{3/4})$ shortcut edges. Formally, letting $S(n,D)$ be the bound on the
size of the shortcut set required in order to reduce the diameter of any
$n$-vertex digraph to at most $D$, our algorithms yield: \[
S(n,D)=\begin{cases} \widetilde{O}(n^2/D^3), &amp; \text{for~} D\leq n^{1/3},\\
\widetilde{O}((n/D)^{3/2}), &amp; \text{for~} D&gt; n^{1/3}~. \end{cases} \] We also
extend our algorithms to provide improved $(\beta,\epsilon)$ hopsets for
$n$-vertex weighted directed graphs.
</p></div>
    </summary>
    <updated>2021-11-29T22:41:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13238</id>
    <link href="http://arxiv.org/abs/2111.13238" rel="alternate" type="text/html"/>
    <title>Quasi-Isometric Graph-Simplifications</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khoussainov:Bakhadyr.html">Bakhadyr Khoussainov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soo:Kh=iacute==U=iacute=.html">Khí-Uí Soo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13238">PDF</a><br/><b>Abstract: </b>We propose a general framework based on quasi-isometries to study graph
simplifications. Quasi-isometries are mappings on metric spaces that preserve
the distance functions within an additive and a multiplicative constant. We use
them to measure the distance distortion between the original graph and the
simplified graph. We also introduce a novel concept called the centre-shift,
which quantifies how much a graph simplification affects the location of the
graph centre. Given a quasi-isometry, we establish a weak upper bound on the
centre-shift. We present methods to construct so-called partition-graphs, which
are quasi-isometric graph simplifications. Furthermore, in terms of the
centre-shift, we show that partition-graphs constructed in a certain way
preserve the centres of trees. Finally, we also show that by storing extra
numerical information, partition-graphs preserve the median of trees.
</p></div>
    </summary>
    <updated>2021-11-29T22:50:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13222</id>
    <link href="http://arxiv.org/abs/2111.13222" rel="alternate" type="text/html"/>
    <title>Quantum Motif Clustering</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cade:Chris.html">Chris Cade</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Labib:Farrokh.html">Farrokh Labib</a>, Ido Niesen <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13222">PDF</a><br/><b>Abstract: </b>We present three quantum algorithms for clustering graphs based on
higher-order patterns, known as motif clustering. One uses a straightforward
application of Grover search, the other two make use of quantum approximate
counting, and all of them obtain square-root like speedups over the fastest
classical algorithms in various settings. In order to use approximate counting
in the context of clustering, we show that for general weighted graphs the
performance of spectral clustering is mostly left unchanged by the presence of
constant (relative) errors on the edge weights. Finally, we extend the original
analysis of motif clustering in order to better understand the role of multiple
`anchor nodes' in motifs and the types of relationships that this method of
clustering can and cannot capture.
</p></div>
    </summary>
    <updated>2021-11-29T22:41:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13203</id>
    <link href="http://arxiv.org/abs/2111.13203" rel="alternate" type="text/html"/>
    <title>Entropy-Based Approximation of the Secretary Problem</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kowalski:Dariusz_R=.html">Dariusz R. Kowalski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krysta:Piotr.html">Piotr Krysta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Olkowski:Jan.html">Jan Olkowski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13203">PDF</a><br/><b>Abstract: </b>In this work we consider the well-known Secretary Problem -- a number $n$ of
elements, each having an adversarial value, are arriving one-by-one according
to some random order, and the goal is to choose the highest value element. The
decisions are made online and are irrevocable -- if the algorithm decides to
choose or not to choose the currently seen element, based on the previously
observed values, it cannot change its decision later regarding this element.
The measure of success is the probability of selecting the highest value
element, minimized over all adversarial assignments of values. We show
existential and constructive upper bounds on approximation of the success
probability in this problem, depending on the entropy of the randomly chosen
arrival order, including the lowest possible entropy $O(\log\log (n))$ for
which the probability of success could be constant. We show that below entropy
level $\mathcal{H}&lt;0.5\log\log n$, all algorithms succeed with probability $0$
if random order is selected uniformly at random from some subset of
permutations, while we are able to construct in polynomial time a non-uniform
distribution with entropy $\mathcal{H}$ resulting in success probability of at
least $\Omega\left(\frac{1}{(\log\log n +3\log\log\log n
-\mathcal{H})^{2+\epsilon}}\right)$, for any constant $\epsilon&gt;0$. We also
prove that no algorithm using entropy $\mathcal{H}=O((\log\log n)^a)$ can
improve our result by more than polynomially, for any constant $0&lt;a&lt;1$. For
entropy $\log\log (n)$ and larger, our analysis precisely quantifies both
multiplicative and additive approximation of the success probability. In
particular, we improve more than doubly exponentially on the best previously
known additive approximation guarantee for the secretary problem.
</p></div>
    </summary>
    <updated>2021-11-29T22:48:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13198</id>
    <link href="http://arxiv.org/abs/2111.13198" rel="alternate" type="text/html"/>
    <title>The Implicit Graph Conjecture is False</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hatami:Hamed.html">Hamed Hatami</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hatami:Pooya.html">Pooya Hatami</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13198">PDF</a><br/><b>Abstract: </b>An efficient implicit representation of an $n$-vertex graph $G$ in a family
$\mathcal{F}$ of graphs assigns to each vertex of $G$ a binary code of length
$O(\log n)$ so that the adjacency between every pair of vertices can be
determined only as a function of their codes. This function can depend on the
family but not on the individual graph. Every family of graphs admitting such a
representation contains at most $2^{O(n\log(n))}$ graphs on $n$ vertices, and
thus has at most factorial speed of growth.
</p>
<p>The Implicit Graph Conjecture states that, conversely, every hereditary graph
family with at most factorial speed of growth admits an efficient implicit
representation. We refute this conjecture by establishing the existence of
hereditary graph families with factorial speed of growth that require codes of
length $n^{\Omega(1)}$.
</p></div>
    </summary>
    <updated>2021-11-29T22:44:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.13054</id>
    <link href="http://arxiv.org/abs/2111.13054" rel="alternate" type="text/html"/>
    <title>On the Strong Metric Dimension of directed co-graphs</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmitz:Yannick.html">Yannick Schmitz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wanke:Egon.html">Egon Wanke</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.13054">PDF</a><br/><b>Abstract: </b>Let $G$ be a strongly connected directed graph and $u,v,w\in V(G)$ be three
vertices. Then $w$ strongly resolves $u$ to $v$ if there is a shortest
$u$-$w$-path containing $v$ or a shortest $w$-$v$-path containing $u$. A set
$R\subseteq V(G)$ of vertices is a strong resolving set for a directed graph
$G$ if for every pair of vertices $u,v\in V(G)$ there is at least one vertex in
$R$ that strongly resolves $u$ to $v$ and at least one vertex in $R$ that
strongly resolves $v$ to $u$. The distances of the vertices of $G$ to and from
the vertices of a strong resolving set $R$ uniquely define the connectivity
structure of the graph. The Strong Metric Dimension of a directed graph $G$ is
the size of a smallest strong resolving set for $G$. The decision problem
Strong Metric Dimension is the question whether $G$ has a strong resolving set
of size at most $r$, for a given directed graph $G$ and a given number $r$. In
this paper we study undirected and directed co-graphs and introduce linear time
algorithms for Strong Metric Dimension. These algorithms can also compute
strong resolving sets for co-graphs in linear time.
</p></div>
    </summary>
    <updated>2021-11-29T22:41:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.12981</id>
    <link href="http://arxiv.org/abs/2111.12981" rel="alternate" type="text/html"/>
    <title>Efficient Mean Estimation with Pure Differential Privacy via a Sum-of-Squares Exponential Mechanism</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hopkins:Samuel_B=.html">Samuel B. Hopkins</a>, Gautam Kamath, Mahbod Majid <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.12981">PDF</a><br/><b>Abstract: </b>We give the first polynomial-time algorithm to estimate the mean of a
$d$-variate probability distribution with bounded covariance from
$\tilde{O}(d)$ independent samples subject to pure differential privacy. Prior
algorithms for this problem either incur exponential running time, require
$\Omega(d^{1.5})$ samples, or satisfy only the weaker concentrated or
approximate differential privacy conditions. In particular, all prior
polynomial-time algorithms require $d^{1+\Omega(1)}$ samples to guarantee small
privacy loss with "cryptographically" high probability, $1-2^{-d^{\Omega(1)}}$,
while our algorithm retains $\tilde{O}(d)$ sample complexity even in this
stringent setting.
</p>
<p>Our main technique is a new approach to use the powerful Sum of Squares
method (SoS) to design differentially private algorithms. SoS proofs to
algorithms is a key theme in numerous recent works in high-dimensional
algorithmic statistics -- estimators which apparently require exponential
running time but whose analysis can be captured by low-degree Sum of Squares
proofs can be automatically turned into polynomial-time algorithms with the
same provable guarantees. We demonstrate a similar proofs to private algorithms
phenomenon: instances of the workhorse exponential mechanism which apparently
require exponential time but which can be analyzed with low-degree SoS proofs
can be automatically turned into polynomial-time differentially private
algorithms. We prove a meta-theorem capturing this phenomenon, which we expect
to be of broad use in private algorithm design.
</p>
<p>Our techniques also draw new connections between differentially private and
robust statistics in high dimensions. In particular, viewed through our
proofs-to-private-algorithms lens, several well-studied SoS proofs from recent
works in algorithmic robust statistics directly yield key components of our
differentially private mean estimation algorithm.
</p></div>
    </summary>
    <updated>2021-11-29T22:44:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.12800</id>
    <link href="http://arxiv.org/abs/2111.12800" rel="alternate" type="text/html"/>
    <title>Tiny Pointers</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bender:Michael_A=.html">Michael A. Bender</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Conway:Alex.html">Alex Conway</a>, Martín Farach-Colton, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuszmaul:William.html">William Kuszmaul</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tagliavini:Guido.html">Guido Tagliavini</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.12800">PDF</a><br/><b>Abstract: </b>This paper introduces a new data-structural object that we call the tiny
pointer. In many applications, traditional $\log n $-bit pointers can be
replaced with $o (\log n )$-bit tiny pointers at the cost of only a
constant-factor time overhead. We develop a comprehensive theory of tiny
pointers, and give optimal constructions for both fixed-size tiny pointers
(i.e., settings in which all of the tiny pointers must be the same size) and
variable-size tiny pointers (i.e., settings in which the average tiny-pointer
size must be small, but some tiny pointers can be larger). If a tiny pointer
references an element in an array filled to load factor $1 - 1 / k$, then the
optimal tiny-pointer size is $\Theta(\log \log \log n + \log k) $ bits in the
fixed-size case, and $ \Theta (\log k) $ expected bits in the variable-size
case. Our tiny-pointer constructions also require us to revisit several classic
problems having to do with balls and bins; these results may be of independent
interest.
</p>
<p>Using tiny pointers, we revisit five classic data-structure problems: the
data-retrieval problem, succinct dynamic binary search trees, space-efficient
stable dictionaries, space-efficient dictionaries with variable-size keys, and
the internal-memory stash problem. These are all well-studied problems, and in
each case tiny pointers allow for us to take a natural space-inefficient
solution that uses pointers and make it space-efficient for free.
</p></div>
    </summary>
    <updated>2021-11-29T22:42:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.09444</id>
    <link href="http://arxiv.org/abs/2111.09444" rel="alternate" type="text/html"/>
    <title>Hypercontractivity on High Dimensional Expanders: a Local-to-Global Approach for Higher Moments</title>
    <feedworld_mtime>1638144000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bafna:Mitali.html">Mitali Bafna</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hopkins:Max.html">Max Hopkins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaufman:Tali.html">Tali Kaufman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lovett:Shachar.html">Shachar Lovett</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.09444">PDF</a><br/><b>Abstract: </b>Hypercontractivity is one of the most powerful tools in Boolean function
analysis. Originally studied over the discrete hypercube, recent years have
seen increasing interest in extensions to settings like the $p$-biased cube,
slice, or Grassmannian, where variants of hypercontractivity have found a
number of breakthrough applications including the resolution of Khot's 2-2
Games Conjecture (Khot, Minzer, Safra FOCS 2018). In this work, we develop a
new theory of hypercontractivity on high dimensional expanders (HDX), an
important class of expanding complexes that has recently seen similarly
impressive applications in both coding theory and approximate sampling. Our
results lead to a new understanding of the structure of Boolean functions on
HDX, including a tight analog of the KKL Theorem and a new characterization of
non-expanding sets.
</p>
<p>Unlike previous settings satisfying hypercontractivity, HDX can be
asymmetric, sparse, and very far from products, which makes the application of
traditional proof techniques challenging. We handle these barriers with the
introduction of two new tools of independent interest: a new explicit
combinatorial Fourier basis for HDX that behaves well under restriction, and a
new local-to-global method for analyzing higher moments. Interestingly, unlike
analogous second moment methods that apply equally across all types of
expanding complexes, our tools rely inherently on simplicial structure. This
suggests a new distinction among high dimensional expanders based upon their
behavior beyond the second moment.
</p></div>
    </summary>
    <updated>2021-11-29T22:37:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6105624713426867173</id>
    <link href="http://blog.computationalcomplexity.org/feeds/6105624713426867173/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/open-4-colorability-for-graphs-of.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6105624713426867173" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6105624713426867173" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/open-4-colorability-for-graphs-of.html" rel="alternate" type="text/html"/>
    <title>Open: 4  colorability for graphs of bounded genus or bounded crossing number (has this been asked before?)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I have  co-authored (with Nathan Hayes, Anthony Ostuni, Davin Park) an open problems column  on the topic of this post. It is <a href="https://www.cs.umd.edu/users/gasarch/open/cross.pdf">here</a>.</p><p>Let g(G) be the genus of a graph and cr(G) be the crossing number of a graph.</p><p>As usual chi(G) is the chromatic number of a graph. </p><p>KNOWN to most readers of this blog:</p><p>{G: \chi(G) \le 2} is in P</p><p>{G: \chi(G) \le 3 and g(G)\le 0 } is NPC (planar graph 3-col)</p><p>{G : \chi(G) \le 4 and g(G) \le 0} is in P (it's trivial since all planar graphs are 4-col)</p><p>{G: \chi(G) \le 3 and cr(G) \le 0} is NPC (planar graph 3-col)</p><p>{G: \chi(G) \le 4 and cr(G) \le 0} is in P (trivial since all planar graphs are 4-col)</p><p>LESS WELL KNOWN BUT TRUE (and brought to my attention by my co-authors and also Jacob Fox and Marcus Schaefer) </p><p>For all g\ge 0 and r\ge 5, {G : \chi(G) \le r and g(G) \le g} is in P</p><p>For all c\ge 0 and r\ge 5, {G : \chi(G) \le r and cr(G) \le c} is in P </p><p>SO I asked the question: for various r,g,c what is the complexity of the following sets:</p><p>{G: \chi(G) \le r AND g(G) \le g} </p><p>{G: \chi(G) \le r AND cr(G) \le c}</p><p>SO I believe the status of the following sets is open</p><p>{G : \chi(G) \le 4 and g(G)\le 1} (replace 1 with 2,3,4,...)</p><p>{G : \chi(G) \le 4 and cr(G)\le 1} (replace 1 with 2,3,4...) </p><p><br/></p><p>QUESTIONS</p><p>1) If anyone knows the answer to these open questions, please leave comments. </p><p>2) The paper pointed to above mentions all of the times I read of someone asking questions like this. There are not many, and the problem does not seem to be out there. Why is that?</p><p>a) It's hard to find out who-asked-what-when. Results are published, open problems often are not. My SIGACT News open problems column gives me (and others) a chance to write down open problems; however, such venues are rare. So it's possible that someone without a blog or an open problems column raised these questions before. (I checked cs stack exchange- not there- and I posted there but didn't get much of a response.) </p><p>b) Proving NPC seems hard since devising gadgets with only one crossing is NOT good enough since you use the gadget many times. This may have discouraged people from thinking about it. </p><p>c) Proving that the problems are in P (for the r\ge 6 case) was the result of using a hard theorem in graph theory from 2007. The authors themselves did not notice the algorithmic result. The first published account of the algorithmic result might be my open problems column.  This may be a case of the graph theorists and complexity theorists not talking to each other, though that is surprising since there is so much overlap that I thought there was no longer a distinction. </p><p>d) While I think this is a natural question to ask, I may be wrong. See <a href="https://blog.computationalcomplexity.org/2021/05/what-is-natural-question-who-should.html">here</a> for a blog post about when I had a natural question and found out why I may be wrong about the problems naturalness. </p><p><br/></p></div>
    </content>
    <updated>2021-11-28T21:47:00Z</updated>
    <published>2021-11-28T21:47:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-11-29T04:08:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/170</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/170" rel="alternate" type="text/html"/>
    <title>TR21-170 |  Pseudorandom Self-Reductions for NP-Complete Problems | 

	Reyad Abed Elrazik, 

	Robert Robere, 

	Assaf Schuster, 

	Gal Yehuda</title>
    <summary>A language $L$ is random-self-reducible if deciding membership in $L$ can be reduced (in polynomial time) to deciding membership in $L$ for uniformly random instances. It is known that several "number theoretic" languages (such as computing the permanent of a matrix) admit random self-reductions. Feigenbaum and Fortnow showed that NP-complete languages are not non-adaptively random-self-reducible unless the polynomial-time hierarchy collapses, giving suggestive evidence that NP may not admit random self-reductions. Hirahara and Santhanam introduced a weakening of random self-reductions that they called pseudorandom self-reductions, in which a language $L$ is reduced to a distribution that is computationally indistinguishable from the uniform distribution. They then showed that the Minimum Circuit Size Problem (MCSP) admits a non-adaptive pseudorandom self-reduction, and suggested that this gave further evidence that MCSP is "distinguished" from standard NP-Complete problems.

We show that, in fact, the NP-Complete Clique problem admits a non-adaptive pseudorandom self-reduction, assuming the planted clique conjecture. More generally we show the following. Call a property of graphs $\pi$ hereditary if $G \in \pi$ implies $H \in \pi$ for every induced subgraph of $G$. We show that for any infinite hereditary property $\pi$, the problem of finding a maximum induced subgraph $H \in \pi$ of a given graph $G$ admits a non-adaptive pseudorandom self-reduction.</summary>
    <updated>2021-11-28T10:18:31Z</updated>
    <published>2021-11-28T10:18:31Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-30T09:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=585</id>
    <link href="https://tcsplus.wordpress.com/2021/11/25/tcs-talk-wednesday-december-1-william-kuszmaul-mit/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, December 1 — William Kuszmaul, MIT</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, December 1th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). William Kuszmaul from MIT will speak about “Linear Probing Revisited: Tombstones Mark the Demise of Primary Clustering” (abstract below). You can reserve a spot as an individual or […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, December 1th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <a href="https://sites.google.com/site/williamkuszmaul"><strong>William Kuszmaul</strong></a> from MIT will speak about “<em>Linear Probing Revisited: Tombstones Mark the Demise of Primary Clustering</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: The linear-probing hash table is one of the oldest and most widely used data structures in computer science. However, linear probing also famously comes with a major drawback: as soon as the hash table reaches a high memory utilization, elements within the hash table begin to cluster together, causing insertions to become slow. This phenomenon, now known as “primary clustering”, was first captured by Donald Knuth in 1963; at a load factor of <img alt="1 - 1/x" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+1%2Fx&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>, the expected time per insertion becomes <img alt="\Theta(x^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CTheta%28x%5E2%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>, rather than the more desirable <img alt="\Theta(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CTheta%28x%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>.</p>
<p>We show that there is more to the story than the classic analysis would seem to suggest. It turns out that small design decisions in how deletions are implemented have dramatic effects on the asymptotic performance of insertions. If these design decisions are made correctly, then even a hash table that is continuously at a load factor <img alt="1 - \Theta(1/x)" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+%5CTheta%281%2Fx%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> can achieve average insertion time <img alt="\tilde{O}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BO%7D%28x%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>. A key insight is that the tombstones left behind by deletions cause a surprisingly strong “anti-clustering” effect, and that when insertions and deletions are one-for-one, the anti-clustering effects of deletions actually overpower the clustering effects of insertions.</p>
<p>We also present a new variant of linear probing, which we call “graveyard hashing”, that completely eliminates primary clustering on any sequence of operations. If, when an operation is performed, the current load factor is <img alt="1 - 1/x" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+1%2Fx&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> for some <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>, then the expected cost of the operation is <img alt="O(x)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28x%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>. One corollary is that, in the external-memory model with a data block size of <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>, graveyard hashing offers the following remarkable guarantee: at any load factor <img alt="1-1/x" class="latex" src="https://s0.wp.com/latex.php?latex=1-1%2Fx&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> satisfying <img alt="x = o(B)" class="latex" src="https://s0.wp.com/latex.php?latex=x+%3D+o%28B%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>, graveyard hashing achieves <img alt="1 + o(1)" class="latex" src="https://s0.wp.com/latex.php?latex=1+%2B+o%281%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> expected block transfers per operation. Past external-memory hash tables have only been able to offer a <img alt="1+o(1)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2Bo%281%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> guarantee when the block size <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> is at least <img alt="\Omega(x^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28x%5E2%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>.</p>
<p>Based on joint work with Michael A. Bender and Bradley C. Kuszmaul (<a href="https://tcsplus.wordpress.com/feed/_wp_link_placeholder">arXiv:2107.01250</a>). To appear in FOCS 2021.</p></blockquote></div>
    </content>
    <updated>2021-11-25T22:02:37Z</updated>
    <published>2021-11-25T22:02:37Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-11-30T09:38:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19356</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/11/24/best-to-dean-mynatt/" rel="alternate" type="text/html"/>
    <title>Best To Dean Mynatt</title>
    <summary>Plus updates on gender disparity, equity, and POPL 2022 IPaT page Beth Mynatt is heading north to become the new Dean of Computer Science at Northeastern University. Georgia Tech will miss her; she has been a key part of Tech for over twenty years. Northeastern is getting a great leader, a valued colleague, and an […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Plus updates on gender disparity, equity, and POPL 2022</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/24/best-to-dean-mynatt/bm/" rel="attachment wp-att-19358"><img alt="" class="alignright wp-image-19358" height="150" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/bm.png?resize=150%2C150&amp;ssl=1" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">IPaT <a href="https://news.gatech.edu/expert/elizabeth-mynatt">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Beth Mynatt is heading north to become the new Dean of Computer Science at Northeastern University. Georgia Tech will miss her; she has been a key part of Tech for over twenty years. Northeastern is getting a great leader, a valued colleague, and an excellent PhD graduate of Georgia Tech.</p>
<p>
Today we hail her work on solving problems of <em>aging</em> and compare to what we do in theory. These musings wend toward solving problems of the kind raised in our previous <a href="https://rjlipton.wpcomstaging.com/2021/11/13/popl-2022-et-tu-brute/">post</a>, on which we have an update from POPL General Chair Rajeev Alur that comes full circle to a Dean at Northeastern.</p>
<p>
One aspect of aging is having a long memory of a field. When Ken and I were young, it was all about what humans can do with computers. Now it is much more about what computers can do with humans. <em>With</em> humans, not just <em>for</em> humans—the aspect of collaboration is key. </p>
<p>
</p><p/><h2> Aging… </h2><p/>
<p/><p>
Beth worked on various projects over her time at Tech. In 1999–2000, she was part of a team that launched the Aware Home Research Initiative (<a href="https://www.cc.gatech.edu/fce/house/house.html">AHRI</a>), “whose goal is to develop the requisite technologies to create a home environment that can both perceive and assist its occupants.” She has led the Institute of People and Technology (IPaT) since 2011, its inaugural year, where her team has helped to support numerous, impactful research programs for faculty across Georgia Tech. </p>
<p>
To show how leadership and persistent work pay dividends, she is a co-PI on a new multi-institution grant led by Tech’s Sonia Chernova to build intelligent systems that support aging. The five-year, $20 million grant from the National Science Foundation, sponsored also by Amazon and Google, will go to create the NSF AI Institute for Collaborative Assistance and Responsive Interaction for Networked Groups (<a href="http://ai-caring.org/">AI-CARING</a>). The <a href="https://www.cc.gatech.edu/news/649114/new-ai-institute-builds-tech-support-aging">announcement</a> says:</p>
<blockquote><p><b> </b> <em> The institute aims to develop new longitudinal, collaborative AI systems that work with aging adults including those diagnosed with mild cognitive impairment, and their caregivers. </em>
</p></blockquote>
<p/><p>
The AI-CARING front page says that they </p>
<blockquote><p><b> </b> <em> “will develop a discipline focused on personalized, longitudinal, collaborative AI, enabling the development of AI systems that learn personalized models of user behavior, understand how people’s behavior changes over time, and integrate that knowledge to support people and AIs working together. These networked Human-AI teams will work with elderly adults and their caregivers in order to provide sustainable long-term care solutions.” </em>
</p></blockquote>
<p>
</p><p>
As someone who is not young, I can definitely see why this is important. </p>
<p>
</p><p/><h2> Missed the … </h2><p/>
<p/><p>
I sometimes feel that we in theory have missed the boat. The issue is that our problems are weighty—P=NP anyone?—but they do not directly impact practical computing. The areas that Beth is interested in, such as <a href="https://en.wikipedia.org/wiki/Ubiquitous_computing">ubiquitous computing</a>, have by definition had a large impact on real computing. The snippet from MIT’s <a href="http://oxygen.csail.mit.edu/Overview.html">Project Oxygen</a> quoted by Wikipedia expresses the direction of impact:</p>
<blockquote><p><b> </b> <em> In the future, computation will be human centered. It will be freely available everywhere, like batteries and power sockets, or oxygen in the air we breathe. … [C]onfigurable generic devices, either handheld or embedded in the environment, will bring computation to us, whenever we need it and wherever we might be. … We won’t have to type, click, or learn new computer jargon. Instead, we’ll communicate naturally, using speech and gestures that describe our intent … and leave it to the computer to carry out our will. </em>
</p></blockquote>
<p/><p>
This may conjure a “Star Trek” vision, but some people already live with substantial parts of this reality. Is there a market for P=NP? On the “equals” side, we can think of a <a href="https://en.wikipedia.org/wiki/Sneakers_(1992_film)">couple</a> of <a href="https://en.wikipedia.org/wiki/Travelling_Salesman_(2012_film)">movies</a> featuring interested parties. For the “not equal” side, not so much?</p>
<p>
If pressed to think of a theory result that “launched a thousand ships” of practical effort, Peter Shor’s theorem about factoring belonging to quantum polynomial time springs to mind. But the ship of universal quantum computing needed to get it under steam won’t come in for decades.  It is <em>quantum devices with rudimentary computational features</em> that we see ruling in the meantime, while quantum communication protocols backed by quantum information theory are <a href="https://thequantuminsider.com/2021/06/23/11-global-banks-probing-the-wonderful-world-of-quantum-technologies/">banked on</a> now. </p>
<p>
</p><p/><h2> The Solutions Business </h2><p/>
<p/><p>
I, Ken writing from here, have often winced at the way the word “solutions” is used in business advertising. To a theorist, <em>solutions</em> are what make a conjecture become a theorem, what we grade in theory courses, what enjoyers of puzzles pursue for recreation. </p>
<p>
I described a <em>solution</em> of the more practical kind in my “Pandemic Lag” <a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/">post</a> last July. It solves the problem of estimating the true current strength of rapidly developing young players (such as I was once) whose official ratings have been largely frozen for over a year and a half by the lack of in-person chess during the pandemic. Online play is not officially rated. </p>
<p>
For example, if a preteen’s frozen rating is <b>1575</b>, my formula will currently add 25 Elo points times 19 months of the pandemic to make <b>2050</b>. Players with higher ratings and longer track records are adjusted less. In some recent instances, when such kids have defeated players with more-established ratings near 2200, people seeing the 1500s rating in the official tournament table have raised questions. My answer is based on the average performance of <em>many</em> children keen enough to compete in similar-level championship events. Thus my computer-intensive studies are safeguarding the welfare of minors.</p>
<p>
</p><p/><h2> Gender Gap and Pipeline in Chess </h2><p/>
<p/><p>
My rating solution raises another problem of the kind addressed in our last <a href="https://rjlipton.wpcomstaging.com/2021/11/13/popl-2022-et-tu-brute/">post</a>. For a fresh instance, on Monday I submitted my final report on the European Team Championship. This ten-day tournament finished Sunday with the <a href="https://www.chess.com/news/view/alireza-firouzja-youngest-chess-player-ever-to-break-2800">sensation</a> of Alireza Firouzja becoming the youngest player ever rated over 2800, six months younger than world champion Magnus Carlsen was in 2009. (Carlsen will <a href="https://www.fide.com/news/1445">defend</a> his title starting Friday in Dubai against the Russian Ian Nepomniachtchi.)</p>
<p>
My intrinsic rating performance projections for the men’s/open section were all accurate to within 10 Elo points, within two-sigma error bars about <img alt="{\pm 25}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. When restricted to the 32 juniors (out of 191 total players) whose ratings I adjusted, all projections were within 23 Elo points of the results and their average was within <b>2</b> Elo points—on adjustments averaging 80 Elo points per the 32 players. </p>
<p>
In the women’s section, my gender-neutral formula adjusted up 39 of the 153 female players by an average of 169 Elo. The prescribed amount was larger because their ratings (average 2168) were lower than the 32 junior males’ ratings (average 2419) to begin with. The formula overshot their performance by 113 Elo, and accounted for the bulk of a 36 Elo average shortfall of my projections for the women’s section overall. This continues a pattern of observing not only a lower starting point, but also a lower first derivative when compared to males of the <em>same</em> rating, in tournaments with high junior participation where my adjustments are involved.</p>
<p>
The <em>Queen’s Gambit</em> miniseries magnified awareness of the disparity—witness the <a href="https://womensagenda.com.au/latest/whats-behind-the-gender-imbalance-in-top-level-chess/">article</a> a year ago by the Australian economist and grandmaster David Smerdon, “What’s behind the gender imbalance in top-level chess?” My work offers a new way to pinpoint when and where the male and female pipelines diverge. I do not see how it could <em>solve</em> the disparity, however. Even converting my big spreadsheets of test results into publications is a tall ask—for one, they are considered sensitive data. </p>
<p>
I have discoursed about the theory of my predictive model on this blog, and could say more about the thrill of empirical success in ways much of theory doesn’t reach. But this is still short of solving human problems in the manner of Dick’s intro, let alone solving the gender gap. On that, we’re grateful to have a communication from Rajeev Alur, General Chair of POPL, on how the statistics we noted came about and what the conference is doing.</p>
<p>
</p><p/><h2> More About POPL 2022 </h2><p/>
<p/><p>
Rajeev began by observing that POPL PCs in previous years have been closer to the percentages we quoted for STOC and FOCS: </p>
<ul>
<li>
2021: 15.4% (8/52) <p/>
</li><li>
2020: 14.8% (8/54) <p/>
</li><li>
2019: 11.5% (6/52) <p/>
</li><li>
2018: 23.1% (12/52) <p/>
</li><li>
2017: 17.2% (5/29) <p/>
</li><li>
2016: 21.4% (6/28) <p/>
</li><li>
2015: 20.0% (6/30)
</li></ul>
<p>
A more private figure that he gave us permission to divulge is that the proportion of PC <em>invitations</em> this year was just under 20% for women. Had they accepted at the same rate as the men, there would have been 12 women on the committee this year, as there was in the first year the PC size was expanded in 2018. </p>
<p>
He went on to note an issue also raised in comments to our last post, namely that among the smaller population of women the same people are asked multiple times, leading to more declines. We can put the scaling problem another way: nearly doubling the committee size in 2018 also doubled the ratio of women being invited to their total number. Not all effects of scaling-up keep equal proportion. The root cause of course is not only the smaller population but its smaller first derivative: as Rajeev noted, the latest figures from the Computing Research Association in 2019 show that out of 417 PhDs in POPL-core areas, only 42 (10%) were female.</p>
<p>
The main thing that they are doing is to take the kind of steps pointed up in the quotation from Valerie King in our post. As shown in the POPL 2022 Overview schedule on their <a href="https://popl22.sigplan.org/">front page</a>, they have special lunch and breakfast events for women and LGTBQ attendees (the latter held since <a href="https://popl20.sigplan.org/track/POPL-2020-lgbtq-lunch">2020</a>) and mentoring workshops for graduate and undergraduate level students. POPL 2022 is the first to appoint a special Chair for organizing these events, Jennifer Paykin, who we note gave a special quantum-for-POPL <a href="https://www.youtube.com/watch?v=nVMm0PrF-j8">presentation</a> in 2020. </p>
<p>
Next year’s POPL PC will be chaired by <a href="https://www.khoury.northeastern.edu/people/amal-ahmed/">Amal Ahmed</a>. Coincidentally, she is Associate Dean for Graduate Programs at Northeastern’s Khoury College where Beth Mynatt will be Dean. <a href="https://alexandrasilva.org/#/main.html">Alexandra Silva</a>, who was on the POPL 2020 PC and on the POPL 2021 Organizing Committee as Accessibility Chair, is one of three Keynote Speakers this year. Hope for the pipeline picking up was raised by the 2021 SIGPLAN Robin Milner Award for Junior Researchers going to <a href="https://homes.cs.washington.edu/~emina/">Emina Torlak</a> of UW. </p>
<p>
Of course, it will take a long time to translate the promotion of opportunity into closer parity. The low dip this year is what we noticed, but the long time effect is what our post highlighted at the end. Again we thank Rajeev for bringing both the time range and effort by SIGPLAN and POPL into greater context.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We’ve tried to “zoom out” where theory people are used to zooming in. How can that change perspectives on our field?</p>
<p>
We are also thankful for how our field has branched out and wish everyone a happy Thanksgiving.</p>
<p/><p><br/>
[fixed name in POPL section]</p></font></font></div>
    </content>
    <updated>2021-11-25T04:48:54Z</updated>
    <published>2021-11-25T04:48:54Z</published>
    <category term="All Posts"/>
    <category term="chess"/>
    <category term="detection"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="aging"/>
    <category term="assistive computing"/>
    <category term="Beth Mynatt"/>
    <category term="Gender equity"/>
    <category term="gender gap"/>
    <category term="POPL 2022"/>
    <category term="Problems"/>
    <category term="projections"/>
    <category term="Rajeev Alur"/>
    <category term="solutions"/>
    <category term="Thanksgiving"/>
    <category term="Theory"/>
    <category term="women in computing"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-11-30T09:37:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-6494812234459505156</id>
    <link href="http://processalgebra.blogspot.com/feeds/6494812234459505156/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=6494812234459505156" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6494812234459505156" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6494812234459505156" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/11/inria-innovation-prize-2021-to-mateescu.html" rel="alternate" type="text/html"/>
    <title>Inria Innovation Prize 2021 to Mateescu, Garavel, Lang and Serwe</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I just learnt that <a href="https://convecs.inria.fr/people/Radu.Mateescu/">Radu Mateescu</a>, <a href="http://convecs.inria.fr/people/Hubert.Garavel/">Hubert Garavel</a>, <a href="http://convecs.inria.fr/people/Frederic.Lang/">Frédéric Lang</a> and <a href="http://convecs.inria.fr/people/Wendelin.Serwe/">Wendelin Serwe</a> from the <a href="http://convecs.inria.fr/">Construction of Verified Concurrent Systems</a> (Convecs) project team at INRIA Grenoble – Rhône-Alpes Centre have been recently awarded the <a href="https://www.inria.fr/en/convecs-team-safety-modeling-distributed-parallel-systems" target="_blank">Inria Innovation Prize – Académie des sciences – Dassault Systèmes</a>. Their work contributes to the development of the <a href="https://cadp.inria.fr/" target="_blank">CADP toolbox</a> for modelling and verifying parallel and distributed systems. The aim of that project is to automatically detect design flaws in highly complex systems. </p><p>Readers of this blog will be as delighted as I am by this news. <a href="https://en.wikipedia.org/wiki/Construction_and_Analysis_of_Distributed_Processes" target="_blank">CADP</a> is one of the tools from the concurrency community that has the longest history, dating back to its early releases in 1989. It has been used to good effect in a variety of applications, is still under continuous development and makes excellent use  in practice of classic tools from concurrency theory. By way of example, let me mention the recent successes by the Convecs team in dealing with difficult challenges posed by <a href="http://ls5-www.cs.tu-dortmund.de/cms/de/mitarbeiter/prof/Bernhard_Steffen.html" target="_blank">Bernhard Steffen</a> and his team on the evaluation of CTL and LTL formulae on large products of automata. (See, for instance, the news items <a href="http://cadp.inria.fr/news12.html" target="_blank">here</a> and <a href="http://cadp.inria.fr/news13.html#section-3" target="_blank">here</a>.) Traditional model checkers fail on those challenges because the state space of the product automata is too large for them. However, a wise use of bisimulations and congruence results allows CADP to solve many of those challenges. Interested readers might also wish to peruse the slides at </p><ul style="text-align: left;"><li><a href="http://cadp.inria.fr/ftp/presentations/Mazzanti-RERS-18.pdf">http://cadp.inria.fr/ftp/presentations/Mazzanti-RERS-18.pdf</a> </li><li><a href="http://cadp.inria.fr/ftp/presentations/Lang-RERS-19.pdf">http://cadp.inria.fr/ftp/presentations/Lang-RERS-19.pdf</a> and</li><li><a href="http://cadp.inria.fr/ftp/presentations/Lang-RERS-20.pdf">http://cadp.inria.fr/ftp/presentations/Lang-RERS-20.pdf</a>. </li></ul>Congratulations to the whole CADP team and to the concurrency community for this award!</div>
    </content>
    <updated>2021-11-24T22:50:00Z</updated>
    <published>2021-11-24T22:50:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-11-25T09:25:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/169</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/169" rel="alternate" type="text/html"/>
    <title>TR21-169 |  Hypercontractivity on High Dimensional Expanders: a Local-to-Global Approach for Higher Moments | 

	Max Hopkins, 

	Mitali Bafna, 

	Tali Kaufman, 

	Shachar Lovett</title>
    <summary>Hypercontractivity is one of the most powerful tools in Boolean function analysis. Originally studied over the discrete hypercube, recent years have seen increasing interest in extensions to settings like the $p$-biased cube, slice, or Grassmannian, where variants of hypercontractivity have found a number of breakthrough applications including the resolution of Khot’s 2-2 Games Conjecture (Khot, Minzer, Safra FOCS 2018). In this work, we develop a new theory of hypercontractivity on high dimensional expanders (HDX), an important class of expanding complexes that has recently seen similarly impressive applications in both coding theory and approximate sampling. Our results lead to a new understanding of the structure of Boolean functions on HDX, including a tight analog of the KKL Theorem and a new characterization of non-expanding sets. 

Unlike previous settings satisfying hypercontractivity, HDX can be asymmetric, sparse, and very far from products, which makes the application of traditional proof techniques challenging. We handle these barriers with the introduction of two new tools of independent interest: a new explicit combinatorial Fourier basis for HDX that behaves well under restriction, and a new local-to-global method for analyzing higher moments. Interestingly, unlike analogous second moment methods that apply equally across all types of expanding complexes, our tools rely inherently on simplicial structure. This suggests a new distinction among high dimensional expanders based upon their behavior beyond the second moment.</summary>
    <updated>2021-11-24T08:19:32Z</updated>
    <published>2021-11-24T08:19:32Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-30T09:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/168</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/168" rel="alternate" type="text/html"/>
    <title>TR21-168 |  Hypercontractivity on High Dimensional Expanders: Approximate Efron-Stein Decompositions for $\epsilon$-Product Spaces | 

	Tom Gur, 

	Noam Lifshitz, 

	Siqi Liu</title>
    <summary>We prove hypercontractive inequalities on high dimensional expanders. As in the settings of the p-biased hypercube, the symmetric group, and the Grassmann scheme, our inequalities are effective for global functions, which are functions that are not significantly affected by a restriction of a small set of coordinates. As applications, we obtain Fourier concentration, small-set expansion, and Kruskal-Katona theorems for high dimensional expanders. Our techniques rely on a new approximate Efron-Stein decomposition for high dimensional link expanders.</summary>
    <updated>2021-11-24T08:10:52Z</updated>
    <published>2021-11-24T08:10:52Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-30T09:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/167</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/167" rel="alternate" type="text/html"/>
    <title>TR21-167 |  Post-Quantum Zero Knowledge, Revisited (or: How to Do Quantum Rewinding Undetectably) | 

	Alex Lombardi, 

	Fermi Ma, 

	Nicholas Spooner</title>
    <summary>A major difficulty in quantum rewinding is the fact that measurement is destructive: extracting information from a quantum state irreversibly changes it. This is especially problematic in the context of zero-knowledge simulation, where preserving the adversary's state is essential.
    
    In this work, we develop new techniques for quantum rewinding in the context of extraction and zero-knowledge simulation:
    
1. We show how to extract information from a quantum adversary by rewinding it without disturbing its internal state. We use this technique to prove that important interactive protocols, such as the Goldreich-Micali-Wigderson protocol for graph non-isomorphism and the Feige-Shamir protocol for NP, are zero-knowledge against quantum adversaries. 

2. We prove that the Goldreich-Kahan protocol for NP is post-quantum zero knowledge using a simulator that can be seen as a natural quantum extension of the classical simulator. 

Our results achieve (constant-round) black-box zero-knowledge with negligible simulation error, appearing to contradict a recent impossibility result due to Chia-Chung-Liu-Yamakawa (FOCS 2021). This brings us to our final contribution:

3. We introduce coherent-runtime expected quantum polynomial time, a computational model that (a) captures all of our zero-knowledge simulators, (b) cannot break any polynomial hardness assumptions, and (c) is not subject to the CCLY impossibility. In light of our positive results and the CCLY negative results, we propose coherent-runtime simulation to be the right quantum analogue of classical expected polynomial-time simulation.</summary>
    <updated>2021-11-23T11:09:07Z</updated>
    <published>2021-11-23T11:09:07Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-30T09:37:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2021/11/23/mask-rct-revisited/</id>
    <link href="http://benjamin-recht.github.io/2021/11/23/mask-rct-revisited/" rel="alternate" type="text/html"/>
    <title>Revisiting the Bangladesh Mask RCT.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In an earlier post, <a href="https://www.argmin.net/2021/09/13/effect-size/">I raised a few issues</a> with a <a href="https://www.poverty-action.org/sites/default/files/publications/Mask_Second_Stage_Paper_20211108.pdf.pdf">large-scale RCT run in Bangladesh aimed at estimating the effectiveness of masks on reducing the spread of the coronavirus</a>. In particular, I was a bit dismayed that the authors did not post the raw number of seropositive cases in their study, preventing me from computing standard statistical analyses of their results. I also objected to the number of statistical regressions run to pull signals out of a very complex intervention.</p>

<p>Recently, the authors were kind enough to release their <a href="https://gitlab.com/emily-crawford/bd-mask-rct">code and data</a>. I send nothing but kudos to them in this regard. Releasing code and data can help disambiguate questions that are not always answerable from papers alone. In fact, I was immediately able to answer my question by querying their data. In this post, I will walk through a simple analysis to estimate the efficacy of their proposed intervention.</p>

<p>In the Bangladesh Mask RCT, there were $n_C=$163,861 individuals from 300 villages in the control group. There were $n_T=$178,322 individuals from 300 villages in the intervention group. The main end point of the study was whether their intervention reduced the number of individuals who both reported covid-like symptoms and tested seropositive at some point during the trial. The number of such individuals appears nowhere in their paper, and one has to compute this from the data they kindly provided: There were $i_C=$1,106 symptomatic individuals confirmed seropositive in the control group and $i_T=$1,086 such individuals in the treatment group. The difference between the two groups was small: only <em>20 cases</em> out of over 340,000 individuals over a span of 8 weeks.</p>

<p>I have a hard time going from these numbers to the assured conclusions that “masks work” that was <a href="https://www.theatlantic.com/ideas/archive/2021/09/masks-were-working-all-along/619989/">promulgated</a> <a href="https://www.nature.com/articles/d41586-021-02457-y">by</a> <a href="https://www.nbcnews.com/science/science-news/largest-study-masks-yet-details-importance-fighting-covid-19-rcna1858">the</a> <a href="https://www.washingtonpost.com/world/2021/09/01/masks-study-covid-bangladesh/">media</a> or <a href="https://www.nytimes.com/2021/09/26/opinion/do-masks-work-for-covid-prevention.html">the authors</a> after this preprint appeared. This study was not blinded, as it’s impossible to blind a study on masks. The intervention was highly complex and included a mask promotion campaign and education about other mitigation measures including social distancing. Moreover, individuals were only added to the study if they consented to allow the researchers to visit and survey their household. There was a large differential between the control and treatment groups here, with 95% consenting in the treatment group but only 92% consenting in control. <em>This differential alone could wash away the difference in observed cases.</em> Finally, symptomatic seropositivity is a crude measure of covid as the individuals could have been infected before the trial began.</p>

<p>Given the numerous caveats and confounders, the study still only found a tiny effect size. My takeaway is that a complex intervention including an educational program, free masks, encouraged mask wearing, and surveillance in a poor country with low population immunity and no vaccination showed at best modest reduction in infection. I think this summary is fair to the study authors. And this is valuable information to have! It reaffirms my priors that non-pharmaceutical interventions are challenging to implement and have only modest benefits in the presence of a highly contagious respiratory infection. But your mileage may vary.</p>

<p>As I mentioned, of course, this was not the message that the majority of the media took away from this study. Instead we were told that this trial finally confirmed that masks worked. I think one of the key confusing points was <a href="http://www.argmin.net/2021/08/13/relative-risk/">using “efficacy” instead of relative risk</a> as a measure of intervention power.</p>

<p>One of the dark tricks of biostatistics is moving away from absolute case counts to  measures of risk such as relative risk reduction, efficacy, or the odds ratio. All of these measures are relative, and they tend to exaggerate effects. The relative risk reduction is the ratio of the rate of infection in the treatment group to the rate of infection in the control group</p>

\[{\small
    RR = \frac{i_T/n_T}{i_C/n_C}\,.
}\]

<p>A small $RR$ corresponds to a large reduction in risk. For the mask study, $RR=$0.9. That’s not a lot of risk reduction: in this study, community masking improved an individual’s risk of infection by a factor of only 1.1x. As a convenient comparator, the $RR$ in the MRNA vaccine trials was 0.05. In this case, vaccines reduce the risk of symptomatic infection by a factor of 20x.</p>

<p>The academic vaccine community unfortunately uses “efficacy” or “effectiveness” to describe relative risk reduction. <a href="http://www.argmin.net/xxx">Efficacy is a confusing, commonly misinterpreted metric</a>. Efficacy in a trial is one minus the relative risk reduction:</p>

\[{\small
EFF = 1-RR\,,
}\]

<p>reported as a percentage. So if the $RR=$0.9, then $EFF=$10%.</p>

<p>The important thing to realize about efficacy is that the range from 0% to 20% is barely better than nothing. Here, even a 20% efficacy corresponds to a reduction of risk by a factor of 1.25x. 1.25x is not literally nothing, but it’s also not enough to halt a highly contagious respiratory infection. For what it’s worth, a vaccine with 20% efficacy would not be approved. Another major flaw of using efficacy as a metric is that it is highly nonlinear. The difference between 10% and 20% efficacy is very small whereas the difference between 85% and 95% is huge, corresponding to a 7-fold and 20-fold risk reduction respectively. Efficacy is a nonlinear metric, but these percentages are bandied around as if they are linear effects, and this adds confusion to the public dialogue.</p>

<p class="center"><img alt="The relationship between effectiveness and risk reduction is highly nonlinear" src="http://www.argmin.net/assets/eff_v_rr.png" width="65%"/></p>

<p>To further dive into the absurdity of efficacy, let’s examine the claim that “cloth masks” worked less well than “surgical masks.” This is too strong an observation to be gleaned from the data. The preprint provides two stratified calculations to estimate the efficacy of types of masks. In the first case, the authors analyzed villages randomized to only be given surgical masks and their matched control villages. In this case there were 190 pairs of villages consisting of $n_C=$103,247 individuals in the control group and $n_T=$113,082 individuals in the treatment group. They observed $i_C=$774 symptomatic and seropositive individuals in the control group and $i_T=$756 symptomatic and seropositive individuals in the treatment group. <em>This is a difference of 18 individuals.</em> The corresponding efficacy is 11%, still woefully low.</p>

<p>We can do a similar analysis for the villages only given cloth masks. There were 96 pairs of villages consisting of $n_C=$53,691 individuals in the control group and $n_T=$57,415 individuals in the treatment group. They observed $i_C=$332 symptomatic and seropositive individuals in the control group and $i_T=$330 symptomatic and seropositive individuals in the treatment group. <em>This is a difference of only 2 individuals.</em> Certainly, no one would put much faith in an intervention where we see a difference of 2 cases in a study with over one hundred thousand people. However, to further demonstrate the absurdity of the notion of efficacy, the observed efficacy for cloth masks in this study is 7%. I think in many people’s minds, the difference between 7% and 11% is small. And 7% should be considered “no effect” as should 11%. <del>As a final absurd comparison, the study data shows cloth masks are more efficacious than purple surgical masks where the estimated efficacy is 0% ($n_C=$27,918, $n_T=$29,541, $i_C=$177, $i_T=$187)!</del> (<em>Ed note: turns out the purple masks were cloth. So the cloth purple masks did nothing, but the red masks “work.” Indeed, red masks were more effective than surgical masks!)</em> Certainly, comparing a bunch of such small effects is not telling us much.</p>

<p>Anyone who spends too much time around statisticians will note that I never once tried to compute a p-value for any of these results. As I’ve belabored, obsession with statistical significance distracts us from discussing effect sizes. We should be able to just look at the effect size and conclude the study did not find a significant impact of masks on coronavirus spread. We don’t need a p-value to tell us 10% efficacy is not helpful in this context. But it’s also important to note that you can’t just run a standard binomial test on this data because it is cluster-randomized and the subjects are anything but independent. <a href="http://www.argmin.net/2021/11/29/cluster-power/">In the next blog</a>, just for the sake of academic navel gazing, I’ll discuss the lack of statistical significance of this study and show why cluster randomized trials are inherently more challenging to interpret than standard RCTs.</p></div>
    </summary>
    <updated>2021-11-23T00:00:00Z</updated>
    <published>2021-11-23T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2021-11-30T05:21:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4583902398250806221</id>
    <link href="http://blog.computationalcomplexity.org/feeds/4583902398250806221/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/finding-element-with-nonadaptive.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4583902398250806221" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4583902398250806221" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/finding-element-with-nonadaptive.html" rel="alternate" type="text/html"/>
    <title>Finding an element with nonadaptive questions</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Suppose you have a non-empty subset S of {1,...N} and want to find an element of S. You can ask arbitrary questions of the form "Does S contain an element in A?" for some A a subset of {1,...N}. How many questions do you need?</p><p>Of course you can use binary search, using questions of the form "is there number greater than <i>m</i> in S?". This takes log N questions and it's easy to show that's tight.</p><p>What if you have to ask all the questions ahead of time before you get any of the answers? Now binary search won't work. If |S|=1 you can ask "is there a number in S whose <i>i</i>th bit is one?" That also takes log N questions.</p><p>For arbitrary S the situation is trickier. With randomness you still don't need too many questions. <a href="https://doi.org/10.1007/BF02579206">Mulmuley, Vazirani and Vazirani</a>'s isolating lemma works as follows: For each i &lt;= log N, pick a random weight w<sub>i</sub> between 1 and 2 log N. For each element m in S, let the weight of m be the sum of the weights of the bits of m that are 1. With probability at least 1/2 there will be an m with an unique minimum weight. There's a <a href="https://blog.computationalcomplexity.org/2015/07/new-proof-of-isolation-lemma.html">cool proof</a> of an isolating lemma by Noam Ta-Shma.</p><p>Once you have this lemma, you can ask questions of the form "Given a list of w<sub>i</sub>'s and a value v, is there an m in S of weight v whose jth bit is 1?" Choosing w<sub>i</sub> and v at random you have a 1/O(log N) chance of a single m whose weight is v, and trying all j will give you a witness. </p><p>Randomness is required. The X-search problem described by <a href="https://doi.org/10.1016/0022-0000(88)90027-X">Karp, Upfal and Wigderson</a> shows that any deterministic procedure requires essentially N queries. </p><p>This all came up because Bill had some colleagues looking a similar problems testing machines for errors. </p><p>I've been interested in the related question of finding satisfying assignments using non-adaptive NP queries. The results are similar to the above. In particular, you can randomly find a satisfying assignment with high probability using a polynomial number of non-adaptive NP queries. It follows from the techniques above, and even earlier papers, but I haven't been able to track down a reference for the first paper to do so.</p></div>
    </content>
    <updated>2021-11-22T20:39:00Z</updated>
    <published>2021-11-22T20:39:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-11-29T04:08:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=964</id>
    <link href="https://emanueleviola.wordpress.com/2021/11/22/phd-in-complexity-theory-with-me/" rel="alternate" type="text/html"/>
    <title>PhD in complexity theory with me</title>
    <summary>This is around the time when people start applying for PhD programs, at least judging from my inbox. If you are applying, consider that we have an amazing theory group , are highly ranked, have tons of resources, and I am looking for students.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is around the time when people start applying for PhD programs, at least judging from my inbox.  If you are applying, consider that we have an <a href="https://www2.ccs.neu.edu/theory/">amazing theory group </a>, are <a href="http://csrankings.org/#/index?all&amp;us">highly ranked</a>, have <a href="https://philanthropynewsdigest.org/news/northeastern-receives-50-million-for-college-of-computer-sciences">tons </a>of <a href="https://mainestartupsinsider.com/roux-institute-receives-another-100m-gift-to-support-its-high-tech-education-initiative-in-portland/">resources</a>, and I am looking for students.</p></div>
    </content>
    <updated>2021-11-22T19:31:33Z</updated>
    <published>2021-11-22T19:31:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2021-11-30T09:37:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/22/assistant-professors-theory-positions-at-uestc-chengdu-china-apply-by-february-28-2022/</id>
    <link href="https://cstheory-jobs.org/2021/11/22/assistant-professors-theory-positions-at-uestc-chengdu-china-apply-by-february-28-2022/" rel="alternate" type="text/html"/>
    <title>Assistant Professors, theory positions at UESTC, Chengdu, China (apply by February 28, 2022)</title>
    <summary>The theory group at the cs school invites applications for Assistant Professor positions. The school is a top school in China and competitive at the world stage. We thrive to become among the best theoretical computer science groups in China. By joining us, you work with experienced, young, exciting, curiosity driven researchers and talented students. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The theory group at the cs school invites applications for Assistant Professor positions. The school is a top school in China and competitive at the world stage. We thrive to become among the best theoretical computer science groups in China. By joining us, you work with experienced, young, exciting, curiosity driven researchers and talented students. Life is exciting &amp; remuneration is generous.</p>
<p>Website: <a href="https://tcs.uestc.edu.cn/">https://tcs.uestc.edu.cn/</a><br/>
Email: bmk@uestc.edu.cn</p></div>
    </content>
    <updated>2021-11-22T12:07:31Z</updated>
    <published>2021-11-22T12:07:31Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-30T09:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/21/research-fellow-at-university-of-oxford-apply-by-november-29-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/21/research-fellow-at-university-of-oxford-apply-by-november-29-2021/" rel="alternate" type="text/html"/>
    <title>Research Fellow at University of Oxford (apply by November 29, 2021)</title>
    <summary>The Department of Computer Science is pleased to invite applications for the Glasstone Fellowship in Computer Science—a three-year postdoctoral fellowship supported by the Glasstone Bequest. Candidates should be completing or have recently (i.e. normally within the past 3 years) completed a doctorate in Computer Science or a closely related discipline. Website: https://www.cs.ox.ac.uk/news/1984-full.html Email: james.worrell@cs.ox.ac.uk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science is pleased to invite applications for the Glasstone Fellowship in Computer Science—a three-year<br/>
postdoctoral fellowship supported by the Glasstone Bequest. Candidates should be completing or have recently (i.e. normally within the past 3 years) completed a doctorate in<br/>
Computer Science or a closely related discipline.</p>
<p>Website: <a href="https://www.cs.ox.ac.uk/news/1984-full.html">https://www.cs.ox.ac.uk/news/1984-full.html</a><br/>
Email: james.worrell@cs.ox.ac.uk</p></div>
    </content>
    <updated>2021-11-21T18:32:47Z</updated>
    <published>2021-11-21T18:32:47Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-30T09:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/166</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/166" rel="alternate" type="text/html"/>
    <title>TR21-166 |  Average-case Hardness of NP and PH from Worst-case Fine-grained Assumptions | 

	Lijie Chen, 

	Shuichi Hirahara, 

	Neekon Vafa</title>
    <summary>What is a minimal worst-case complexity assumption that implies non-trivial average-case hardness of NP or PH? This question is well motivated by the theory of fine-grained average-case complexity and fine-grained cryptography. In this paper, we show that several standard worst-case complexity assumptions are sufficient to imply non-trivial average-case hardness of NP or PH:
    
    1. NTIME[$n$] cannot be solved in quasi-linear time on average if UP is not in DTIME[$2^{\widetilde{O}\left(\sqrt{n}\right)}$].
    
    2. $\Sigma_2$TIME[$n$] cannot be solved in quasi-linear time on average if $\Sigma_k$SAT cannot be solved in time $2^{\widetilde{O}\left(\sqrt{n}\right)}$ for some constant $k$. Previously, it was not known if even average-case hardness of $\Sigma_3$SAT implies the average-case hardness of $\Sigma_2$TIME[$n$].
    
    3. Under the Exponential-Time Hypothesis (ETH), there is no average-case $n^{1+\varepsilon}$-time algorithm for NTIME[$n$] whose running time can be estimated in time $n^{1+\varepsilon}$ for some constant $\varepsilon &gt; 0$.
    
    Our results are given by generalizing the non-black-box worst-case-to-average-case connections presented by Hirahara (STOC 2021) to the settings of fine-grained complexity. To do so, we construct quite efficient complexity-theoretic pseudorandom generators under the assumption that the nondeterministic linear time is easy on average, which may be of independent interest.</summary>
    <updated>2021-11-21T01:35:02Z</updated>
    <published>2021-11-21T01:35:02Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-30T09:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/165</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/165" rel="alternate" type="text/html"/>
    <title>TR21-165 |  Improved Merlin-Arthur Protocols for Central Problems in Fine-Grained Complexity | 

	Shyan Akmal, 

	Lijie Chen, 

	Ce Jin, 

	Malvika Raj, 

	Ryan Williams</title>
    <summary>In a Merlin-Arthur proof system, the proof verifier (Arthur) accepts valid proofs (from Merlin) with probability $1$, and rejects invalid proofs with probability arbitrarily close to $1$. The running time of such a system is defined to be the length of Merlin's proof plus the running time of Arthur.  We provide new Merlin-Arthur proof systems for some key problems in fine-grained complexity. In several cases our proof systems have optimal running time. Our main results include:

$\bullet$ Certifying that a list of $n$ integers has no 3-SUM solution can be done in Merlin-Arthur time $\tilde{O}(n)$. Previously, Carmosino et al. [ITCS 2016] showed that the problem has a nondeterministic algorithm running in $\tilde{O}(n^{1.5})$  time (that is, there is a proof system with proofs of length $\tilde{O}(n^{1.5})$ and a deterministic verifier running in $\tilde{O}(n^{1.5})$ time).

$\bullet$ Counting the number of $k$-cliques with total edge weight equal to zero in an $n$-node graph can be done in Merlin-Arthur time $\tilde O(n^{\lceil k/2\rceil })$ (where $k\ge 3$). For odd $k$, this bound can be further improved for sparse graphs: for example, counting the number of zero-weight triangles in an $m$-edge graph can be done in Merlin-Arthur time $\tilde O(m)$. Previous Merlin-Arthur protocols by Williams [CCC'16] and Bj\"orklund and Kaski [PODC'16] could only count $k$-cliques in unweighted graphs, and had worse running times for small $k$.

$\bullet$ Computing the All-Pairs Shortest Distances matrix for an $n$-node graph can be done in Merlin-Arthur time $\tilde{O}(n^2)$. Note this is optimal, as the matrix can have $\Omega(n^2)$ nonzero entries in general. Previously, Carmosino et al. [ITCS 2016] showed that this problem has an $\tilde{O}(n^{2.94})$ nondeterministic time algorithm.

$\bullet$ Certifying that an $n$-variable $k$-CNF is unsatisfiable can be done in Merlin-Arthur time $2^{n/2 - n/O(k)}$. We also observe an algebrization barrier for the previous $2^{n/2}\cdot \mathrm{poly}(n)$-time Merlin-Arthur protocol of R. Williams [CCC'16] for $\#$SAT: in particular, his protocol algebrizes, and we observe there is no algebrizing protocol for $k$-UNSAT running in $2^{n/2}/n^{\omega(1)}$ time. Therefore we have to exploit non-algebrizing properties to obtain our new protocol.


$\bullet$ Certifying a Quantified Boolean Formula is true can be done in Merlin-Arthur time $2^{4n/5}\cdot \mathrm{poly}(n)$. Previously, the only nontrivial result known along these lines was an Arthur-Merlin-Arthur protocol (where Merlin's proof depends on some of Arthur's coins) running in $2^{2n/3}\cdot\mathrm{poly}(n)$ time. 

Due to the centrality of these problems in fine-grained complexity, our results have consequences for many other problems of interest. For example, our work implies that certifying there is no Subset Sum solution to $n$ integers can be done in Merlin-Arthur time $2^{n/3}\cdot\mathrm{poly}(n)$, improving on the previous best protocol by Nederlof [IPL 2017] which took $2^{0.49991n}\cdot\mathrm{poly}(n)$ time.</summary>
    <updated>2021-11-21T00:21:01Z</updated>
    <published>2021-11-21T00:21:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-30T09:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/20/assistant-associate-professors-at-aalto-university-apply-by-january-12-2022/</id>
    <link href="https://cstheory-jobs.org/2021/11/20/assistant-associate-professors-at-aalto-university-apply-by-january-12-2022/" rel="alternate" type="text/html"/>
    <title>Assistant &amp; Associate Professors at Aalto University (apply by January 12, 2022)</title>
    <summary>We invite applications for tenure-track positions at the Assistant Professor level, and tenured positions at the Associate Professor level. We are a diverse community welcoming applications in ALL AREAS of Computer Science. Our CS Theory group (https://research.cs.aalto.fi/theory/) has e.g. received the best paper awards in FOCS 2019 and ICALP 2017, as well as ERC starting […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We invite applications for tenure-track positions at the Assistant Professor level, and tenured positions at the Associate Professor level. We are a diverse community welcoming applications in ALL AREAS of Computer Science. Our CS Theory group (<a href="https://research.cs.aalto.fi/theory/">https://research.cs.aalto.fi/theory/</a>) has e.g. received the best paper awards in FOCS 2019 and ICALP 2017, as well as ERC starting grants in 2014 and 2017.</p>
<p>Website: <a href="https://bit.ly/aalto-csprof">https://bit.ly/aalto-csprof</a><br/>
Email: laura.kuusisto-noponen@aalto.fi</p></div>
    </content>
    <updated>2021-11-20T19:20:40Z</updated>
    <published>2021-11-20T19:20:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-30T09:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/19/simons-berkeley-fellowships-for-fall-2022-and-spring-2023-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/19/simons-berkeley-fellowships-for-fall-2022-and-spring-2023-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2021/" rel="alternate" type="text/html"/>
    <title>Simons-Berkeley Fellowships for Fall 2022 and Spring 2023 at Simons Institute for the Theory of Computing (apply by December 15, 2021)</title>
    <summary>The Simons Institute for the Theory of Computing invites applications for Simons-Berkeley Research Fellowships for the Fall 2022 and Spring 2023 semesters. The Institute will host programs on “Data-Driven Decision Processes” and “Graph Limits and Processes on Networks: From Epidemics to Misinformation” in Fall 2022 and “Meta-Complexity in Spring 2023. Website: https://simons.berkeley.edu/simons-berkeley-research-fellowship-call-applications Email: simonsvisitorservices@berkeley.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Simons Institute for the Theory of Computing invites applications for Simons-Berkeley Research Fellowships for the Fall 2022 and Spring 2023 semesters. The Institute will host programs on “Data-Driven Decision Processes” and “Graph Limits and Processes on Networks: From Epidemics to Misinformation” in Fall 2022 and “Meta-Complexity in Spring 2023.</p>
<p>Website: <a href="https://simons.berkeley.edu/simons-berkeley-research-fellowship-call-applications">https://simons.berkeley.edu/simons-berkeley-research-fellowship-call-applications</a><br/>
Email: simonsvisitorservices@berkeley.edu</p></div>
    </content>
    <updated>2021-11-19T22:07:32Z</updated>
    <published>2021-11-19T22:07:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-30T09:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6129</id>
    <link href="https://scottaaronson.blog/?p=6129" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6129#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6129" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Acrobatics of BQP</title>
    <summary xml:lang="en-US">Just in case anyone is depressed this afternoon and needs something to cheer them up, students William Kretschmer, DeVon Ingram, and I have finally put out a new paper: The Acrobatics of BQP Abstract: We show that, in the black-box setting, the behavior of quantum polynomial-time (BQP) can be remarkably decoupled from that of classical […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Just in case anyone is depressed this afternoon and needs something to cheer them up, students <a href="https://www.cs.utexas.edu/~kretsch/">William Kretschmer</a>, <a href="https://www.quantitativebiology.northwestern.edu/2021/02/23/three-students-awarded-prizes-in-the-great-math-challenge-in-biology-contest/">DeVon Ingram</a>, and I have finally put out a new paper:</p>



<blockquote class="wp-block-quote"><p><strong><a href="https://eccc.weizmann.ac.il/report/2021/164/">The Acrobatics of BQP</a></strong></p><p><strong>Abstract:</strong> We show that, in the black-box setting, the behavior of quantum polynomial-time (BQP) can be remarkably decoupled from that of classical complexity classes like NP.  Specifically:</p><p>– There exists an oracle relative to which NP<sup>BQP</sup>⊄BQP<sup>PH</sup>, resolving a 2005 problem of Fortnow. Interpreted another way, we show that AC<sup>0</sup> circuits cannot perform useful homomorphic encryption on instances of the Forrelation problem. As a corollary, there exists an oracle relative to which P=NP but BQP≠QCMA.</p><p>– Conversely, there exists an oracle relative to which BQP<sup>NP</sup>⊄PH<sup>BQP</sup>.</p><p>– Relative to a random oracle, PP=PostBQP is not contained in the “QMA hierarchy” QMA<sup>QMA^QMA^…</sup>, and more generally PP⊄(MIP*)<sup>(MIP*)^(MIP*)^…</sup> (!), despite the fact that MIP*=RE in the unrelativized world. This result shows that there is no black-box quantum analogue of Stockmeyer’s approximate counting algorithm.</p><p>– Relative to a random oracle, Σ<sub>k+1</sub>⊄BQP<sup>Σ_k</sup> for every k.</p><p>– There exists an oracle relative to which BQP=P<sup>#P</sup> and yet PH is infinite. (By contrast, if NP⊆BPP, then PH collapses relative to all oracles.)</p><p>– There exists an oracle relative to which P=NP≠BQP=P<sup>#P</sup>.</p><p>To achieve these results, we build on the 2018 achievement by Raz and Tal of an oracle relative to which BQP⊄PH, and associated results about the Forrelation problem. We also introduce new tools that might be of independent interest. These include a “quantum-aware” version of the random restriction method, a concentration theorem for the block sensitivity of AC<sup>0</sup> circuits, and a (provable) analogue of the Aaronson-Ambainis Conjecture for sparse oracles.</p></blockquote>



<p>Incidentally, particularly when I’ve worked on a project with students, I’m often tremendously excited and want to shout about it from the rooftops for the students’ sake … but then I also don’t want to use this blog to privilege my own papers “unfairly.”  Can anyone suggest a principle that I should follow going forward?</p></div>
    </content>
    <updated>2021-11-19T21:48:06Z</updated>
    <published>2021-11-19T21:48:06Z</published>
    <category scheme="https://scottaaronson.blog" term="Complexity"/>
    <category scheme="https://scottaaronson.blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-11-19T21:48:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/19/postdoc-at-georgia-institute-of-technology-apply-by-december-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/19/postdoc-at-georgia-institute-of-technology-apply-by-december-15-2021/" rel="alternate" type="text/html"/>
    <title>PostDoc at Georgia Institute of Technology (apply by December 15, 2021)</title>
    <summary>Algorithms and Randomness Center (ARC) at Georgia Tech is seeking postdoctoral fellows starting Fall 2022. ARC has faculty associated with many departments including CS, Math, ISyE. The selected candidate may work on any aspect of algorithms, optimization, broadly interpreted. Qualified applicants must possess a PhD in CS, Math, OR or a related field. Apply by […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Algorithms and Randomness Center (ARC) at Georgia Tech is seeking postdoctoral fellows starting Fall 2022. ARC has faculty associated with many departments including CS, Math, ISyE. The selected candidate may work on any aspect of algorithms, optimization, broadly interpreted. Qualified applicants must possess a PhD in CS, Math, OR or a related field. Apply by December 15, 2021.</p>
<p>Website: <a href="http://arc.gatech.edu/node/384">http://arc.gatech.edu/node/384</a><br/>
Email: ftonge3@cc.gatech.edu</p></div>
    </content>
    <updated>2021-11-19T20:01:19Z</updated>
    <published>2021-11-19T20:01:19Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-30T09:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=1968</id>
    <link href="https://toc4fairness.org/our-2022-postdoc-program-is-up/" rel="alternate" type="text/html"/>
    <title>Our 2022 Postdoc Program is up</title>
    <summary>We are excited to announce our new postdoc program. We are seeking strong candidates from a diverse set of academic backgrounds and personal experiences who want to work with one ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are excited to announce <a href="https://toc4fairness.org/postdoc-opportunities/">our new postdoc program</a>. We are seeking strong candidates from a diverse set of academic backgrounds and personal experiences who want to work with one or more of the PIs on algorithmic fairness and responsible computing more broadly. We expect to be extending multiple offers.  </p></div>
    </content>
    <updated>2021-11-19T13:45:11Z</updated>
    <published>2021-11-19T13:45:11Z</published>
    <category term="Blog"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i1.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2021-11-30T09:38:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/164</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/164" rel="alternate" type="text/html"/>
    <title>TR21-164 |  The Acrobatics of BQP | 

	Scott Aaronson, 

	DeVon Ingram, 

	William Kretschmer</title>
    <summary>We show that, in the black-box setting, the behavior of quantum polynomial-time (${BQP}$) can be remarkably decoupled from that of classical complexity classes like ${NP}$. Specifically:

-There exists an oracle relative to which ${NP}^{{BQP}}\not \subset {BQP}^{{PH}}$, resolving a 2005 problem of Fortnow. Interpreted another way, we show that ${AC^0}$ circuits cannot perform useful homomorphic encryption on instances of the Forrelation problem. As a corollary, there exists an oracle relative to which ${P} = {NP}$ but ${BQP} \neq {QCMA}$.
-Conversely, there exists an oracle relative to which ${BQP}^{{NP}}\not \subset {PH}^{{BQP}}$.
-Relative to a random oracle, ${PP} = {PostBQP}$ is not contained in the "${QMA}$ hierarchy" ${QMA}^{{QMA}^{{QMA}^{\cdots}}}$, and more generally ${PP} \not\subset ({MIP}^*)^{({MIP}^*)^{({MIP}^*)^{\cdots}}}$ (!), despite the fact that ${MIP}^{\ast}={RE}$ in the unrelativized world. This result shows that there is no black-box quantum analogue of Stockmeyer's approximate counting algorithm.
-Relative to a random oracle, ${\Sigma}_{k+1}^{P} \not\subset {BQP}^{{\Sigma}_{k}^{P}}$ for every $k$.
-There exists an oracle relative to which ${BQP} = {P^{\# P}}$ and yet ${PH}$ is infinite. (By contrast, if ${NP}\subseteq{BPP}$, then ${PH}$ collapses relative to all oracles.)
-There exists an oracle relative to which ${P}={NP} \neq {BQP}={P}^{{\#P}}$.

To achieve these results, we build on the 2018 achievement by Raz and Tal of an oracle relative to which ${BQP}\not \subset {PH}$, and associated results about the Forrelation problem. We also introduce new tools that might be of independent interest. These include a "quantum-aware" version of the random restriction method, a concentration theorem for the block sensitivity of ${AC^0}$ circuits, and a (provable) analogue of the Aaronson-Ambainis Conjecture for sparse oracles.</summary>
    <updated>2021-11-19T13:02:40Z</updated>
    <published>2021-11-19T13:02:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-30T09:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/163</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/163" rel="alternate" type="text/html"/>
    <title>TR21-163 |  Algorithmizing the Multiplicity Schwartz-Zippel Lemma | 

	Siddharth Bhandari, 

	Prahladh Harsha, 

	Mrinal Kumar, 

	A. Shankar</title>
    <summary>The multiplicity Schwartz-Zippel lemma asserts that over a field, a low-degree polynomial cannot vanish with high multiplicity very often on a sufficiently large product set. Since its discovery in a work of Dvir, Kopparty, Saraf and Sudan [DKSS13], the lemma has found nu- merous applications in both math and computer science; in particular, in the definition and properties of multiplicity codes by Kopparty, Saraf and Yekhanin [KSY14].

In this work, we show how to algorithmize the multiplicity Schwartz-Zippel lemma for ar- bitrary product sets over any field. In other words, we give an efficient algorithm for unique decoding of multivariate multiplicity codes from half their minimum distance on arbitrary product sets over all fields. Previously, such an algorithm was known either when the un- derlying product set had a nice algebraic structure (for instance, was a subfield) [Kop15] or when the underlying field had large (or zero) characteristic, the multiplicity parameter was sufficiently large and the multiplicity code had distance bounded away from 1 [BHKS21]. In particular, even unique decoding of bivariate multiplicity codes with multiplicity two from half their minimum distance was not known over arbitrary product sets over any field.

Our algorithm builds upon a result of Kim &amp; Kopparty [KK17] who gave an algorithmic version of the Schwartz-Zippel lemma (without multiplicities) or equivalently, an efficient al- gorithm for unique decoding of Reed-Muller codes over arbitrary product sets. We introduce a refined notion of distance based on the multiplicity Schwartz-Zippel lemma and design a unique decoding algorithm for this distance measure. On the way, we give an alternate proof of Forney’s classical generalized minimum distance decoder that might be of independent interest.</summary>
    <updated>2021-11-18T23:12:39Z</updated>
    <published>2021-11-18T23:12:39Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-30T09:37:24Z</updated>
    </source>
  </entry>
</feed>
