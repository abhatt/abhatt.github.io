<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-10-03T20:22:04Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3615744836127152440</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3615744836127152440/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/10/quantum-supremacy-guest-post-by-abhinav.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3615744836127152440" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3615744836127152440" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/10/quantum-supremacy-guest-post-by-abhinav.html" rel="alternate" type="text/html"/>
    <title>Quantum Supremacy: A Guest Post by Abhinav Deshpande</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I am delighted to introduce you to Abhinav Deshpande, who is a graduate student at the University of Maryland, studying Quantum Computing. This will be a guest post on the rumors of the recent Google breakthrough on Quantum Supremacy. For other blog posts on this exciting rumor, see <a href="https://www.scottaaronson.com/blog/?p=4317">Scott Aaronson's post</a>, <a href="https://www.scottaaronson.com/blog/?p=4342">Scott Aaronson's second post on it</a>, <a href="https://www.quantamagazine.org/john-preskill-explains-quantum-supremacy-20191002/">John Preskill's quanta article</a>, <a href="https://blog.computationalcomplexity.org/2019/09/quantum-supremacy.html">Fortnow's post</a>,<br/>
and there may be others.<br/>
<br/>
Guest post by Abhinav:<br/>
<br/>
I (Abhinav) thank Bill Fefferman for help with this post, and Bill Gasarch for inviting me to do a guest post.<br/>
<br/>
<br/>
<b>The quest towards quantum computational supremacy</b><br/>
<br/>
September saw some huge news in the area of quantum computing, with rumours that the Google AI Lab has achieved a milestone known as 'quantum computational supremacy', also termed 'quantum supremacy' or 'quantum advantage' by some authors. Today, we examine what this term means, the most promising approach towards achieving this milestone, and the best complexity-theoretic evidence we have so far against classical simulability of quantum mechanics. We will not be commenting on details of the purported paper since there is no official announcement or claim from the authors so far.<br/>
<br/>
<b>What it means</b><br/>
<br/>
First off, the field of quantum computational supremacy arose from trying to formally understand the differences in the power of classical and quantum computers. A complexity theorist would view this goal as trying to give evidence to separate the complexity classes BPP and BQP. However, it turns out that one can gain more traction from considering the sampling analogues of these classes, SampBPP and SampBQP.  These are classes of distributions that can be efficiently sampled on classical and quantum computers, respectively. Given a quantum circuit U on n qubits, one may define an associated probability distribution over 2^n outcomes as follows: apply U to the fiducial initial state |000...0&gt; and measure the resulting state in the computational basis. This produces a distribution D_U.<br/>
<br/>
A suitable way to define the task of simulating the quantum circuit is as follows<b style="font-style: italic;">:</b><br/>
<br/>
Input: Description of a quantum circuit U acting on n qubits.<br/>
<br/>
Output: A sample from the probability distribution D_U obtained by measuring U|000...0&gt; in the computational basis.<br/>
<br/>
One of the early works in this field was that of <a href="https://arxiv.org/abs/quant-ph/0205133">Terhal and DiVincenzo</a>, which first considered the complexity of sampling from a distribution (weak simulation) as opposed to that of calculating the exact probability of a certain outcome (strong simulation). Weak simulation is arguably the more natural notion of simulating a quantum system, since in general, we cannot feasibly compute the probability of a certain outcome even if we can simulate the quantum circuit. Subsequent works by <a href="https://arxiv.org/abs/1011.3245">Aaronson and Arkhipov</a>, and by <a href="https://arxiv.org/abs/1005.1407">Bremner, Jozsa, and Shepherd</a> established that if there is a classically efficient weak simulator for different classes of quantum circuits, the polynomial hierarchy collapses to the third level.<br/>
<br/>
<br/>
So far, we have only considered the question of exactly sampling from the distribution D_U. However, any realistic experiment is necessarily noisy, and a more natural problem is to sample from a distribution that is not exactly D_U but from any distribution D_O that is ε-close in a suitable distance measure, say the variation distance.<br/>
<br/>
The aforementioned work by Aaronson and Arkhipov was the first to consider this problem, and they made progress towards showing that a special class of quantum circuits (linear optical circuits) is classically hard to approximately simulate in the sense above. The task of sampling from the output of linear optical circuits is known as boson sampling. At the   time, it was the best available way to show that quantum computers  may solve some problems that are far beyond the reach of classical computers.<br/>
<br/>
Even granting that the PH doesn't collapse, one still needs to make an additional conjecture to establish that boson sampling is not classically simulable.  The conjecture is that additively approximating the output probabilities of a random linear optical quantum circuit is #P-hard.  The reason this may be true is that output probabilities of random linear optical quantum circuits are Permanents of a Gaussian random matrix, and the Permanent is as hard to compute on a random matrix as it is on a worst-case matrix. Therefore, the only missing link is to go from average-case hardness of exact computation to average-case hardness of an additive estimation. In addition, if we make a second conjecture known as the "anti-concentration" conjecture, we can show that this additive estimation is non-trivial: it suffices to give us a good multiplicative estimation with high probability.<br/>
<br/>
So that's what quantum computational supremacy is about: we have a computational task that is efficiently solvable with quantum computers, but which would collapse the polynomial hierarchy if done by a classical computer (assuming certain other conjectures are true). One may substitute "collapse of the polynomial hierarchy" with stronger conjectures and incur a corresponding tradeoff in the likelihood of the conjecture being true.<br/>
<br/>
<b>Random circuit sampling</b><br/>
<br/>
In 2016,<a href="https://arxiv.org/abs/1608.00263"> Boixo et al</a>. proposed to replace the class of quantum circuits for which some hardness results were known (commuting circuits and boson sampling) by random circuits of sufficient depth on a 2D grid of qubits having nearest-neighbour interactions. Concretely, the proposed experiment would be to apply random unitaries from a specified set on n qubits arranged on a 2D grid for sufficient depth, and then sample from the resulting distribution. The two-qubit unitaries in the set are restricted to act between nearest neighbours, respecting the geometric This task is called random circuit sampling (RCS).<br/>
<br/>
At the time, the level of evidence for the hardness of this scheme was not yet the same as the linear optical scheme. However, given the theoretical and experimental interest in the idea of demonstrating a quantum speedup over classical computers, subsequent works by<a href="https://arxiv.org/abs/1803.04402"> Bouland, Fefferman, Nirkhe and Vazirani</a>, and <a href="https://arxiv.org/abs/1809.06957">Harrow and Mehraban</a> bridged this gap (the relevant work by <a href="https://arxiv.org/abs/1612.05903">Aaronson and Chen</a> will be discussed in the following section). Harrow and Mehraban proved anticoncentration for random circuits. In particular, they showed that a 2-dimensional grid of n qubits achieve anticoncentration in depth O(\sqrt{n}), improving upon earlier results with higher depth due to <a href="https://arxiv.org/abs/1208.0692">Brandao, Harrow and Horodeck</a>i. Bouland et al. proved the same supporting evidence for RCS as that for boson sampling, namely a worst-to-average-case reduction for exactly computing most output probabilities, even without the permanent structure possessed by linear optical quantum circuits.<br/>
<br/>
<b>Verification</b><br/>
<br/>
So far, we have not discussed the elephant in the room: of verifying that the output distribution supported on 2^n outcomes. It turns out that there are concrete lower bounds such as those due to Valiant and Valiant, showing that verifying whether an empirical distribution is close to a target distribution is impossible if one has few samples.<br/>
<br/>
Boixo et al. proposed a way of certifying the fidelity of the purported simulation. Their key observation was to note that if their experimental system is well modelled by a noise model called global depolarising noise, estimating the output fidelity is possible with relatively few outcomes. Under global depolarising noise with fidelity f, the noisy distribution takes the form D_N = f D_U + (1-f) I, where I is the uniform distribution over the 2^n outcomes. Together with another empirical observation about the statistics of output probabilities of the ideal distribution D_U, they argued that computing the following cross-entropy score would serve as a good estimator of the fidelity:<br/>
<br/>
f ~ H(I, D_U) - H(D_exp, D_U), where H(D_A,D_B) is the cross-entropy between the two distributions: H(D_A, D_B) = -\sum_i p_A log (p_B).<br/>
<br/>
The proposal here was to experimentally collect several samples from D_exp, classically compute using brute-force the probabilities of these outcomes in the distribution D_U, and estimate the cross-entropy using this information. If the test outputs a high score for a computation on sufficiently many qubits and depth, the claim is that quantum supremacy has been achieved.<br/>
<br/>
Aaronson and Chen gave alternative form of evidence for the hardness of scoring well on a test that aims to certify quantum supremacy similar to the manner above. This sidesteps the issue of whether a test similar to the one above does indeed certify the fidelity. The specific problem considered was "Heavy Output Generation" (HOG), the problem of outputting strings that have higher than median probability in the output distribution. Aaronson and Chen linked the hardness of HOG to a closely related problem called "QUATH", and conjectured that QUATH is hard for classical computers.<br/>
<br/>
<b>Open questions</b><br/>
<br/>
Assuming the Google team has performed the impressive feat of both running the experiment outlined before and classically computing the probabilities of the relevant outcomes to see a high score on their cross-entropy test, I discuss the remaining positions a skeptic might take regarding the claim about quantum supremacy.<br/>
<br/>
"The current evidence of classical hardness of random circuit sampling is not sufficient to conclude that the task is hard". Assuming that the skeptic believes that the polynomial hierarchy does not collapse, a remaining possibility is that there is no worst-to-average-case reduction for the problem of *approximating* most output probabilities, which kills the proof technique of Aaronson and Arkhipov to show hardness of approximate sampling.<br/>
<br/>
"The cross-entropy proposal does not certify the fidelity." Boixo et al. gave numerical evidence and other arguments for this statement, based on the observation that the noise is of the global depolarising form. A skeptic may argue that the assumption of global depolarising noise is a strong one.<br/>
<br/>
"The QUATH problem is not classically hard." In order to give evidence for the hardness of QUATH, Aaronson and Chen examined the best existing algorithms for this problem and also gave a new algorithm that nevertheless do not solve QUATH with the required parameters.<br/>
<br/>
It would be great if the community could work towards strengthening the evidence we already have for this task to be hard, either phrased as a sampling experiment or together with the verification test.<br/>
<br/>
Finally, I think this is an exciting time for quantum computing and to witness this landmark event. It may not be the first probe of an experiment that is "hard" to classically simulate, since there are many quantum experiments that are beyond the reach of current classical simulations, but the inherent programmability and control present in the experimental system is what enables the tools of complexity theory to be applied to the problem. A thought that fascinates me is the idea that we may be exploring quantum mechanics in a regime never probed this carefully before, the "high complexity regime" of quantum mechanics. One imagines there are important lessons in physics here.<br/>
<br/></div>
    </content>
    <updated>2019-10-03T19:41:00Z</updated>
    <published>2019-10-03T19:41:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-10-03T20:08:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=18199</id>
    <link href="https://gilkalai.wordpress.com/2019/10/03/noisy-quantum-circuits-how-do-we-know-that-we-have-robust-experimental-outcomes-at-all-and-do-we-care/" rel="alternate" type="text/html"/>
    <title>Noisy quantum circuits: how do we know that we have robust experimental outcomes at all? (And do we care?)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In a recent post we discussed Google’s claim of achieving “quantum supremacy” and my reasons to think that these claims will not stand. (See also this comment for necessary requirements from a quantum supremacy experiment.) This debate gives a good … <a href="https://gilkalai.wordpress.com/2019/10/03/noisy-quantum-circuits-how-do-we-know-that-we-have-robust-experimental-outcomes-at-all-and-do-we-care/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">a recent post we discussed Google’s claim of achieving “quantum supremacy” and my reasons to think that these claims will not stand.</a> (See also <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/#comment-61043">this comment</a> for necessary requirements from a quantum supremacy experiment.) This debate gives a good opportunity to discuss some conceptual issues regarding sampling, probability distributions, statistics, and computational complexity. This time we will discuss <span style="color: #ff0000;">chaotic behavior vs. robust experimental outcomes.</span></p>
<p>On unrelated matter, I just heard Shachar Lovett’s very beautiful TCS+ lecture on the sunflower conjecture (<a href="https://gilkalai.wordpress.com/2019/08/23/amazing-ryan-alweiss-shachar-lovett-kewen-wu-jiapeng-zhang-made-dramatic-progress-on-the-sunflower-conjecture/">see this post</a> on the Alweiss, Lovett, Wu, and Zhang’s breakthrough). You can see the lecture and many others on the <a href="https://www.youtube.com/user/TCSplusSeminars/videos">TCS+ you tube channel</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/10/cern-slide-30c.png"><img alt="" class="alignnone size-full wp-image-18240" height="449" src="https://gilkalai.files.wordpress.com/2019/10/cern-slide-30c.png?w=640&amp;h=449" width="640"/></a></p>
<p style="text-align: center;"><span style="color: #ff0000;"><span style="color: #993366;">Slide 30 from my August, ’19 CERN lecture: predictions of near-term experiments. (Here is the</span> <a href="https://gilkalai.files.wordpress.com/2019/09/cern.pptx">full powerpoint presentation</a><span style="color: #993366;">.) In this post we mainly</span> <strong>discuss</strong> <strong>point b) about chaotic behavior. </strong><span style="color: #800080;">See also <a href="https://arxiv.org/abs/1908.02499">my paper: The argument against quantum computers</a>.</span></span></p>
<p>Consider an experiment aimed for establishing quantum supremacy: your quantum computer produced a sample <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> which is a 0-1 string of length <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> from a certain distribution <img alt="D_i" class="latex" src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_i"/>. The research assumption is that <img alt="D_i" class="latex" src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_i"/>  is close enough to a fixed distribution <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/> (<img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/> accounts for the computing process and the noise) which is very hard to be demonstrated on a classical computer. By looking at a large number of samples you can perform a statistical test on the samples to verify that they were (approximately) sampled from <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/>, or at least that they were sampled from a probability distribution that is very hard to be computed on a classical computer!</p>
<p>But, is it possible that all the distributions <img alt="D_i" class="latex" src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_i"/>‘s are very different? Namely that each sample is taken from a completely different distribution? More formally, is it possible  that under a correct modeling of the device for two different samples <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> and <img alt="x_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_j"/>, <img alt="D_i" class="latex" src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_i"/> has a very small correlation with <img alt="D_j" class="latex" src="https://s0.wp.com/latex.php?latex=D_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_j"/>? In this case we say that the experiment outcomes are <strong>not robust</strong> and that the situation is <strong>chaotic</strong>.</p>
<p>Here are a couple of questions that I propose to think about:</p>
<ul>
<li>How do we test robustness?</li>
<li>Do the supremacy experiments require that the experiment is robust?</li>
<li>If, after many samples, you reach a probability distribution that require exponential time on a classical computer should you worry about the question whether the experiment is robust?</li>
<li><span style="color: #0000ff;">Do the 10,000,000 samples for the Google 53-qubit experiment represent a robust sampling experiment?</span></li>
</ul>
<p> </p></div>
    </content>
    <updated>2019-10-03T19:23:25Z</updated>
    <published>2019-10-03T19:23:25Z</published>
    <category term="Computer Science and Optimization"/>
    <category term="Quantum"/>
    <category term="chaos"/>
    <category term="chaos and computation"/>
    <category term="quantum supremacy"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-10-03T20:20:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/03/faculty-position-at-university-of-minnesota-twin-cities-apply-by-november-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/03/faculty-position-at-university-of-minnesota-twin-cities-apply-by-november-1-2019/" rel="alternate" type="text/html"/>
    <title>Faculty position at University of Minnesota-Twin Cities (apply by November 1, 2019)</title>
    <summary>The Department of Computer Science &amp; Engineering at the University of Minnesota-Twin Cities is hiring to fill multiple tenure-track positions at the assistant professor level, although higher levels of appointments may be considered when commensurate with experience and accomplishments. One of the areas of interest is theoretical computer science. Website: https://www.cs.umn.edu/news/cse-now-hiring-new-faculty Email: csciadmin@umn.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science &amp; Engineering at the University of Minnesota-Twin Cities is hiring to fill multiple tenure-track positions at the assistant professor level, although higher levels of appointments may be considered when commensurate with experience and accomplishments. One of the areas of interest is theoretical computer science.</p>
<p>Website: <a href="https://www.cs.umn.edu/news/cse-now-hiring-new-faculty">https://www.cs.umn.edu/news/cse-now-hiring-new-faculty</a><br/>
Email: csciadmin@umn.edu</p></div>
    </content>
    <updated>2019-10-03T18:02:59Z</updated>
    <published>2019-10-03T18:02:59Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-03T20:21:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/03/faculty-at-virginia-tech-apply-by-december-31-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/03/faculty-at-virginia-tech-apply-by-december-31-2019/" rel="alternate" type="text/html"/>
    <title>Faculty at Virginia Tech (apply by December 31, 2019)</title>
    <summary>http://careers.pageuppeople.com/968/cw/en-us/job/510994 Website: http://www.cs.vt.edu/ Email: facdev@cs.vt.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="http://careers.pageuppeople.com/968/cw/en-us/job/510994">http://careers.pageuppeople.com/968/cw/en-us/job/510994</a></p>
<p>Website: <a href="http://www.cs.vt.edu/">http://www.cs.vt.edu/</a><br/>
Email: facdev@cs.vt.edu</p></div>
    </content>
    <updated>2019-10-03T14:24:41Z</updated>
    <published>2019-10-03T14:24:41Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-03T20:21:01Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-4476874159248982333</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/4476874159248982333/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=4476874159248982333" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/4476874159248982333" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/4476874159248982333" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2019/10/harvard-admissions-lawsuit-decision-out.html" rel="alternate" type="text/html"/>
    <title>Harvard Admissions Lawsuit Decision Out</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">As someone who reads a significant number of court documents and decisions (I still do expert witness work), I can recommend for your reading pleasure the <a href="https://admissionscase.harvard.edu/files/adm-case/files/2019-10-30_dkt_672_findings_of_fact_and_conclusions_of_law.pdf">very recent decision on the Harvard admissions case</a>.  For those who want a sense of how Harvard admissions works, you will get a good summary of the information that came out during the trial.  For those who want to see a well-written court decision, in my opinion, this is a good example.  (Whether you agree with the decision or not, you should find the decision well written;  it lays out the issues and challenges in determining the decision clearly, and similarly explains the reasons for the ultimate conclusion clearly.)  And for those who care about the actual underlying issues of discrimination and affirmative action, I think the document provides a lot of food for thought, with a depth beyond what you'll see in the  news coverage. </div>
    </content>
    <updated>2019-10-03T05:06:00Z</updated>
    <published>2019-10-03T05:06:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2019-10-03T05:06:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4342</id>
    <link href="https://www.scottaaronson.com/blog/?p=4342" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4342#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4342" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">From quantum supremacy to classical fallacy</title>
    <summary xml:lang="en-US">Maybe I should hope that people never learn to distinguish for themselves which claimed breakthroughs in building new forms of computation are obviously serious, and which ones are obviously silly. For as long as they don’t, this blog will always serve at least one purpose. People will cite it, tweet it, invoke its “authority,” even […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Maybe I should hope that people <em>never</em> learn to distinguish for themselves which claimed breakthroughs in building new forms of computation are obviously serious, and which ones are obviously silly.  For as long as they don’t, this blog will always serve at least one purpose.  People will cite it, tweet it, invoke its “authority,” even while from my point of view, I’m offering nothing more intellectually special than my toddler does when he calls out “moo-moo cow! baa-baa sheep!” as we pass them on the road.</p>



<p>But that’s too pessimistic.  Sure, most readers <em>must</em> more-or-less already know what I’ll say about each thing: that <a href="https://www.scottaaronson.com/blog/?p=4317">Google’s quantum supremacy claim</a> is serious, that <a href="https://www.scottaaronson.com/blog/?p=2212">memcomputing to solve NP-complete problems</a> is not, etc.  Even so, I’ve heard from many readers that this blog was at least helpful for double-checking their initial impressions, and for making <a href="https://www.scottaaronson.com/blog/?p=2410">common knowledge</a> what before had merely been known to many.  I’m fine for it to continue serving those roles.</p>



<p>Last week, even as I dealt with fallout from Google’s quantum supremacy leak, I also got several people asking me to comment on a <em>Nature</em> paper entitled <a href="https://www-nature-com.ezproxy.lib.utexas.edu/articles/s41586-019-1557-9">Integer factorization using stochastic magnetic tunnel junctions</a> (warning: paywalled).  See also <a href="https://www.purdue.edu/newsroom/releases/2019/Q3/poor-mans-qubit-can-solve-quantum-problems-without-going-quantum.html">here</a> for a university press release.</p>



<p>The authors report building a new kind of computer based on asynchronously updated “p-bits” (probabilistic bits).  A p-bit is “a robust, classical entity fluctuating in time between 0 and 1, which interacts with other p-bits … using principles inspired by neural networks.”  They build a device with 8 p-bits, and use it to factor integers up to 945.  They present this as another “unconventional computation scheme” alongside quantum computing, and as a “potentially scalable hardware approach to the difficult problems of optimization and sampling.”</p>



<p>A <a href="https://www.nature.com/articles/d41586-019-02742-x">commentary accompanying the </a><em><a href="https://www.nature.com/articles/d41586-019-02742-x">Nature</a></em><a href="https://www.nature.com/articles/d41586-019-02742-x"> paper</a> goes much further still—claiming that the new factoring approach, “if improved, could threaten data encryption,” and that resources should now be diverted from quantum computing to this promising new idea, one with the advantages of requiring no refrigeration or maintenance of delicate entangled states.  (It should’ve added: and how big a number has Shor’s algorithm factored anyway, 21?  Compared to 945, that’s peanuts!)</p>



<p>Since I couldn’t figure out a gentler way to say this, here goes: it’s <strong>astounding</strong> that this paper and commentary made it into <em>Nature</em> in the form that they did.  Juxtaposing Google’s sampling achievement with p-bits, as several of my Facebook friends did last week, is juxtaposing the Wright brothers with some guy bouncing around on a pogo stick.</p>



<p>If you were looking forward to watching me dismantle the p-bit claims, I’m afraid you might be disappointed: the task is over almost the moment it begins.  <strong>“p-bit” devices can’t scalably outperform classical computers, for the simple reason that they <font color="red">are</font> classical computers.</strong>  A little unusual in their architecture, but still well-covered by the classical <a href="https://www.scottaaronson.com/talks/bernays2.ppt">Extended Church-Turing Thesis</a>.  Just like with the <a href="https://en.wikipedia.org/wiki/Adiabatic_quantum_computation">quantum adiabatic algorithm</a>, an energy penalty is applied to coax the p-bits into running a local optimization algorithm: that is, making random local moves that preferentially decrease the number of violated constraints.  Except here, because the whole evolution is classical, there doesn’t seem to be even the <em>pretense</em> that anything is happening that a laptop with a random-number generator couldn’t straightforwardly simulate.  In terms of <a href="https://www.nytimes.com/2019/10/02/opinion/impeachment-trump-nixon.html">this editorial</a>, if adiabatic quantum computing is Richard Nixon—hiding its lack of observed speedups behind subtle arguments about tunneling and spectral gaps—then p-bit computing is Trump.</p>



<p>Even so, I wouldn’t be writing this post if you opened the paper and it immediately said, in effect, “look, <em>we know</em>.  You’re thinking that this is just yet another stochastic local optimization method, which could clearly be simulated efficiently on a conventional computer, thereby putting it into a different conceptual universe from quantum computing.  You’re thinking that factoring an n-bit integer will self-evidently take exp(n) time by this method, as compared to exp(n<sup>1/3</sup>) for the <a href="https://en.wikipedia.org/wiki/General_number_field_sieve">Number Field Sieve</a>, and that no crypto is in even remote danger from this.  But here’s why you should still be interested in our p-bit model: because of other advantages X, Y, and Z.”  Alas, in vain one searches the whole paper, <em>and</em> the lengthy supplementary material, <em>and</em> the commentary, for any acknowledgment of the pachyderm in the pagoda.  Not an asymptotic runtime scaling in sight.  Quantum computing is there, but stripped of the theoretical framework that gives it its purpose.</p>



<p>That silence, in the pages of <em>Nature</em>—<em>that’s</em> the part that convinced me that, while on the negative side this blog seems to have accomplished nothing for the world in 14 years of existence, on the positive side it will likely have a role for decades to come.</p>



<p><strong>(Partly) Unrelated Announcement #1:</strong> My new postdoc, <a href="https://andrearocchetto.github.io/">Andrea Rocchetto</a>, had the neat idea of compiling a <a href="https://quantumfactsheet.github.io/">Quantum Computing Fact Sheet</a>: a quick “Cliffs Notes” for journalists, policymakers, and others looking to get the basics right.  The fact sheet might grow in the future, but in the meantime, check it out!</p>



<p><strong>Unrelated Announcement #2:</strong> Daniel Wichs asked me to give a shout-out to a new <a href="https://itcrypto.github.io/">Conference on Information-Theoretic Cryptography</a>, to be held June 17-19 in Boston.</p>



<p><strong>Third Announcement:</strong> Several friends asked me to share that <a href="https://peterwittek.com/">Prof. Peter Wittek</a>, quantum computing researcher at the University of Toronto, has <a href="https://www.theglobeandmail.com/canada/article-renowned-ai-expert-university-of-toronto-prof-missing-after-avalanche/?fbclid=IwAR0FTnzQxRL79-oo43xjKaNEA7Oe1rA8A2yVjvhrgodxG1wJzhfhJZt9oJw">gone missing</a> in the Himalayas.  Needless to say we hope for his safe return.</p></div>
    </content>
    <updated>2019-10-03T03:59:37Z</updated>
    <published>2019-10-03T03:59:37Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Speaking Truth to Parallelism"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-10-03T15:54:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.01099</id>
    <link href="http://arxiv.org/abs/1910.01099" rel="alternate" type="text/html"/>
    <title>Parameterized complexity of edge-coloured and signed graph homomorphism problems</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Foucaud:Florent.html">Florent Foucaud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hocquard:Herv=eacute=.html">Hervé Hocquard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lajou:Dimitri.html">Dimitri Lajou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitsou:Valia.html">Valia Mitsou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pierron:Th=eacute=o.html">Théo Pierron</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01099">PDF</a><br/><b>Abstract: </b>We study the complexity of graph modification problems for homomorphism-based
properties of edge-coloured graphs. A homomorphism from an edge-coloured graph
$G$ to an edge-coloured graph $H$ is a vertex-mapping from $G$ to $H$ that
preserves adjacencies and edge-colours. We consider the property of having a
homomorphism to a fixed edge-coloured graph $H$. Given an edge-coloured graph
$G$, can we perform $k$ graph operations so that the resulting graph has a
homomorphism to $H$? The operations we consider are vertex-deletion,
edge-deletion and switching (an operation that permutes the colours of the
edges incident to a given vertex). Switching plays an important role in the
theory of signed graphs, that are $2$-edge-coloured graphs whose colours are
$+$ and $-$. We denote the corresponding problems (parameterized by $k$) by
VERTEX DELETION $H$-COLOURING, EDGE DELETION $H$-COLOURING and SWITCHING
$H$-COLOURING. These generalise $H$-COLOURING (where one has to decide if an
input graph admits a homomorphism to $H$). Our main focus is when $H$ has order
at most $2$, a case that includes standard problems such as VERTEX COVER, ODD
CYCLE TRANSVERSAL and EDGE BIPARTIZATION. For such a graph $H$, we give a
P/NP-complete complexity dichotomy for all three studied problems. Then, we
address their parameterized complexity. We show that all VERTEX DELETION
$H$-COLOURING and EDGE DELETION $H$-COLOURING problems for such $H$ are FPT.
This is in contrast with the fact that already for some $H$ of order~$3$,
unless P=NP, none of the three considered problems is in XP. We show that the
situation is different for SWITCHING $H$-COLOURING: there are three
$2$-edge-coloured graphs $H$ of order $2$ for which this is W-hard, and
assuming the ETH, admits no algorithm in time $f(k)n^{o(k)}$ for inputs of size
$n$. For the other cases, SWITCHING $H$-COLOURING is FPT.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.01095</id>
    <link href="http://arxiv.org/abs/1910.01095" rel="alternate" type="text/html"/>
    <title>The Multivariate Schwartz-Zippel Lemma</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>M. Levent Doğan, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Erg=uuml=r:Alperen_A=.html">Alperen A. Ergür</a>, Jake D. Mundo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsigaridas:Elias.html">Elias Tsigaridas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01095">PDF</a><br/><b>Abstract: </b>We show that, except for a special family of polynomials -that we call
{\lambda}-reducible-, a natural generalization of
Schwartz-Zippel-DeMillo-Lipton lemma holds. Moreover, we develop a symbolic
algorithm to detect {\lambda}-reducibility. Our work is motivated by and has
applications in combinatorial geometry. Along the way we also present a
multivariate generalization of Combinatorial Nullstellensatz, which might be of
independent interest.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.01082</id>
    <link href="http://arxiv.org/abs/1910.01082" rel="alternate" type="text/html"/>
    <title>Subexponential-time algorithms for finding large induced sparse subgraphs</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Novotn=aacute=:Jana.html">Jana Novotná</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Okrasa:Karolina.html">Karolina Okrasa</a>, Michał Pilipczuk, Paweł Rzążewski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leeuwen:Erik_Jan_van.html">Erik Jan van Leeuwen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Walczak:Bartosz.html">Bartosz Walczak</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01082">PDF</a><br/><b>Abstract: </b>Let $\mathcal{C}$ and $\mathcal{D}$ be hereditary graph classes. Consider the
following problem: given a graph $G\in\mathcal{D}$, find a largest, in terms of
the number of vertices, induced subgraph of $G$ that belongs to $\mathcal{C}$.
We prove that it can be solved in $2^{o(n)}$ time, where $n$ is the number of
vertices of $G$, if the following conditions are satisfied:
</p>
<p>* the graphs in $\mathcal{C}$ are sparse, i.e., they have linearly many edges
in terms of the number of vertices;
</p>
<p>* the graphs in $\mathcal{D}$ admit balanced separators of size governed by
their density, e.g., $\mathcal{O}(\Delta)$ or $\mathcal{O}(\sqrt{m})$, where
$\Delta$ and $m$ denote the maximum degree and the number of edges,
respectively; and
</p>
<p>* the considered problem admits a single-exponential fixed-parameter
algorithm when parameterized by the treewidth of the input graph.
</p>
<p>This leads, for example, to the following corollaries for specific classes
$\mathcal{C}$ and $\mathcal{D}$:
</p>
<p>* a largest induced forest in a $P_t$-free graph can be found in
$2^{\tilde{\mathcal{O}}(n^{2/3})}$ time, for every fixed $t$; and
</p>
<p>* a largest induced planar graph in a string graph can be found in
$2^{\tilde{\mathcal{O}}(n^{3/4})}$ time.
</p></div>
    </summary>
    <updated>2019-10-03T01:20:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.01073</id>
    <link href="http://arxiv.org/abs/1910.01073" rel="alternate" type="text/html"/>
    <title>Online Geometric Discrepancy for Stochastic Arrivals with Applications to Envy Minimization</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Haotian.html">Haotian Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kulkarni:Janardhan.html">Janardhan Kulkarni</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singla:Sahil.html">Sahil Singla</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01073">PDF</a><br/><b>Abstract: </b>Consider a unit interval $[0,1]$ in which $n$ points arrive one-by-one
independently and uniformly at random. On arrival of a point, the problem is to
immediately and irrevocably color it in $\{+1,-1\}$ while ensuring that every
interval $[a,b] \subseteq [0,1]$ is nearly-balanced. We define
\emph{discrepancy} as the largest imbalance of any interval during the entire
process. If all the arriving points were known upfront then we can color them
alternately to achieve a discrepancy of $1$. What is the minimum possible
expected discrepancy when we color the points online?
</p>
<p>We show that the discrepancy of the above problem is sub-polynomial in $n$
and that no algorithm can achieve a constant discrepancy. This is a substantial
improvement over the trivial random coloring that only gets an
$\widetilde{O}(\sqrt n)$ discrepancy. We then obtain similar results for a
natural generalization of this problem to $2$-dimensions where the points
arrive uniformly at random in a unit square. This generalization allows us to
improve recent results of Benade et al.\cite{BenadeKPP-EC18} for the online
envy minimization problem when the arrivals are stochastic.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.01071</id>
    <link href="http://arxiv.org/abs/1910.01071" rel="alternate" type="text/html"/>
    <title>Computing the largest bond of a graph</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Gabriel L. Duarte, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lokshtanov:Daniel.html">Daniel Lokshtanov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pedrosa:Lehilton_L=_C=.html">Lehilton L. C. Pedrosa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schouery:Rafael_C=_S=.html">Rafael C. S. Schouery</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Souza:U=eacute=verton_S=.html">Uéverton S. Souza</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01071">PDF</a><br/><b>Abstract: </b>A bond of a graph $G$ is an inclusion-wise minimal disconnecting set of $G$,
i.e., bonds are cut-sets that determine cuts $[S,V\setminus S]$ of $G$ such
that $G[S]$ and $G[V\setminus S]$ are both connected. Given $s,t\in V(G)$, an
$st$-bond of $G$ is a bond whose removal disconnects $s$ and $t$. Contrasting
with the large number of studies related to maximum cuts, there are very few
results regarding the largest bond of general graphs. In this paper, we aim to
reduce this gap on the complexity of computing the largest bond and the largest
$st$-bond of a graph. Although cuts and bonds are similar, we remark that
computing the largest bond of a graph tends to be harder than computing its
maximum cut. We show that {\sc Largest Bond} remains NP-hard even for planar
bipartite graphs, and it does not admit a constant-factor approximation
algorithm, unless $P = NP$. We also show that {\sc Largest Bond} and {\sc
Largest $st$-Bond} on graphs of clique-width $w$ cannot be solved in time
$f(w)\times n^{o(w)}$ unless the Exponential Time Hypothesis fails, but they
can be solved in time $f(w)\times n^{O(w)}$. In addition, we show that both
problems are fixed-parameter tractable when parameterized by the size of the
solution, but they do not admit polynomial kernels unless NP $\subseteq$
coNP/poly.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.01047</id>
    <link href="http://arxiv.org/abs/1910.01047" rel="alternate" type="text/html"/>
    <title>TE-ETH: Lower Bounds for QBFs of Bounded Treewidth</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fichte:Johannes_Klaus.html">Johannes Klaus Fichte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hecher:Markus.html">Markus Hecher</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pfandler:Andreas.html">Andreas Pfandler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01047">PDF</a><br/><b>Abstract: </b>The problem of deciding the validity (QSAT) of quantified Boolean formulas
(QBF) is a vivid research area in both theory and practice. In the field of
parameterized algorithmics, the well-studied graph measure treewidth turned out
to be a successful parameter. A well-known result by Chen in parameterized
complexity is that QSAT when parameterized by the treewidth of the primal graph
of the input formula together with the quantifier depth of the formula is
fixed-parameter tractable. More precisely, the runtime of such an algorithm is
polynomial in the formula size and exponential in the treewidth, where the
exponential function in the treewidth is a tower, whose height is the
quantifier depth. A natural question is whether one can significantly improve
these results and decrease the tower while assuming the Exponential Time
Hypothesis (ETH). In the last years, there has been a growing interest in the
quest of establishing lower bounds under ETH, showing mostly problem-specific
lower bounds up to the third level of the polynomial hierarchy. Still, an
important question is to settle this as general as possible and to cover the
whole polynomial hierarchy. In this work, we show lower bounds based on the ETH
for arbitrary QBFs parameterized by treewidth (and quantifier depth). More
formally, we establish lower bounds for QSAT and treewidth, namely, that under
ETH there cannot be an algorithm that solves QSAT of quantifier depth i in
runtime significantly better than i-fold exponential in the treewidth and
polynomial in the input size. In doing so, we provide a versatile reduction
technique to compress treewidth that encodes the essence of dynamic programming
on arbitrary tree decompositions. Further, we describe a general methodology
for a more fine-grained analysis of problems parameterized by treewidth that
are at higher levels of the polynomial hierarchy.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00994</id>
    <link href="http://arxiv.org/abs/1910.00994" rel="alternate" type="text/html"/>
    <title>Doubly-Efficient Pseudo-Deterministic Proofs</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Michel Goemans, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldwasser:Shafi.html">Shafi Goldwasser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Holden:Dhiraj.html">Dhiraj Holden</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00994">PDF</a><br/><b>Abstract: </b>In [20] Goldwasser, Grossman and Holden introduced pseudo-deterministic
interactive proofs for search problems where a powerful prover can convince a
probabilistic polynomial time verifier that a solution to a search problem is
canonical. They studied search problems for which polynomial time algorithms
are not known and for which many solutions are possible. They showed that
whereas there exists a constant round pseudo deterministic proof for graph
isomorphism where the canonical solution is the lexicographically smallest
isomorphism, the existence of pseudo-deterministic interactive proofs for
NP-hard problems would imply the collapse of the polynomial time hierarchy.
</p>
<p>In this paper, we turn our attention to studying doubly-efficient
pseudo-deterministic proofs for polynomial time search problems:
pseudo-deterministic proofs with the extra requirement that the prover runtime
is polynomial and the verifier runtime to verify that a solution is canonical
is significantly lower than the complexity of finding any solution, canonical
or otherwise. Naturally this question is particularly interesting for search
problems for which a lower bound on its worst case complexity is known or has
been widely conjectured.
</p>
<p>We show doubly-efficient pseudo-deterministic algorithms for a host of
natural problems whose complexity has long been conjectured. In particular:
</p>
<p>We show a doubly efficient pseudo-deterministic proof for linear programming
where the canonical solution which the prover will provide is the
lexicographically greatest optimal solution for the LP.
</p>
<p>We show a doubly efficient pseudo-deterministic proof for 3-SUM and problems
reducible to 3-SUM.
</p>
<p>We show a doubly-efficient pseudo-deterministic proof for the hitting set
problem.
</p>
<p>We show a doubly-efficient pseudo-deterministic proof for the Zero Weight
Triangle problem.
</p></div>
    </summary>
    <updated>2019-10-03T01:21:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00960</id>
    <link href="http://arxiv.org/abs/1910.00960" rel="alternate" type="text/html"/>
    <title>A Framework for Differential Calculus on Persistence Barcodes</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leygonie:Jacob.html">Jacob Leygonie</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oudot:Steve.html">Steve Oudot</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tillmann:Ulrike.html">Ulrike Tillmann</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00960">PDF</a><br/><b>Abstract: </b>We define notions of differentiability for maps from and to the space of
persistence barcodes. Inspired by the theory of diffeological spaces, the
proposed framework uses lifts to the space of ordered barcodes, from which
derivatives can be computed. The two derived notions of differentiability
(respectively from and to the space of barcodes) combine together naturally to
produce a chain rule that enables the use of gradient descent for objective
functions factoring through the space of barcodes. We illustrate the
versatility of this framework by showing how it can be used to analyze the
smoothness of various parametrized families of filtrations arising in
topological data analysis.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00941</id>
    <link href="http://arxiv.org/abs/1910.00941" rel="alternate" type="text/html"/>
    <title>A Self-contained Analysis of the Lempel-Ziv Compression Algorithm</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sudan:Madhu.html">Madhu Sudan</a>, David Xiang <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00941">PDF</a><br/><b>Abstract: </b>This article gives a self-contained analysis of the performance of the
Lempel-Ziv compression algorithm on (hidden) Markovian sources. Specifically we
include a full proof of the assertion that the compression rate approaches the
entropy rate of the chain being compressed.
</p></div>
    </summary>
    <updated>2019-10-03T01:23:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00901</id>
    <link href="http://arxiv.org/abs/1910.00901" rel="alternate" type="text/html"/>
    <title>Sublinear Algorithms for Gap Edit Distance</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldenberg:Elazar.html">Elazar Goldenberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krauthgamer:Robert.html">Robert Krauthgamer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saha:Barna.html">Barna Saha</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00901">PDF</a><br/><b>Abstract: </b>The edit distance is a way of quantifying how similar two strings are to one
another by counting the minimum number of character insertions, deletions, and
substitutions required to transform one string into the other. A simple dynamic
programming computes the edit distance between two strings of length $n$ in
$O(n^2)$ time, and a more sophisticated algorithm runs in time $O(n+t^2)$ when
the edit distance is $t$ [Landau, Myers and Schmidt, SICOMP 1998]. In pursuit
of obtaining faster running time, the last couple of decades have seen a flurry
of research on approximating edit distance, including polylogarithmic
approximation in near-linear time [Andoni, Krauthgamer and Onak, FOCS 2010],
and a constant-factor approximation in subquadratic time [Chakrabarty, Das,
Goldenberg, Kouck\'y and Saks, FOCS 2018].
</p>
<p>We study sublinear-time algorithms for small edit distance, which was
investigated extensively because of its numerous applications. Our main result
is an algorithm for distinguishing whether the edit distance is at most $t$ or
at least $t^2$ (the quadratic gap problem) in time
$\tilde{O}(\frac{n}{t}+t^3)$. This time bound is sublinear roughly for all $t$
in $[\omega(1), o(n^{1/3})]$, which was not known before. The best previous
algorithms solve this problem in sublinear time only for $t=\omega(n^{1/3})$
[Andoni and Onak, STOC 2009].
</p>
<p>Our algorithm is based on a new approach that adaptively switches between
uniform sampling and reading contiguous blocks of the input strings. In
contrast, all previous algorithms choose which coordinates to query
non-adaptively. Moreover, it can be extended to solve the $t$ vs
$t^{2-\epsilon}$ gap problem in time $\tilde{O}(\frac{n}{t^{1-\epsilon}}+t^3)$.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00887</id>
    <link href="http://arxiv.org/abs/1910.00887" rel="alternate" type="text/html"/>
    <title>Node Multiway Cut and Subset Feedback Vertex Set on Graphs of Bounded Mim-width</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Bergougnoux Benjamin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Papadopoulos:Charis.html">Charis Papadopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Telle:Jan_Arne.html">Jan Arne Telle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00887">PDF</a><br/><b>Abstract: </b>The two weighted graph problems Node Multiway Cut (NMC) and Subset Feedback
Vertex Set (SFVS) both ask for a vertex set of minimum total weight, that for
NMC disconnects a given set of terminals, and for SFVS intersects all cycles
containing a vertex of a given set. We design a meta-algorithm that will allow
to solve both problems in time $2^{O(rw^3)}\cdot n^{4}$,
$2^{O(q^2\log(q))}\cdot n^{4}$, and $n^{O(k^2)}$ where $rw$ is the rank-width,
$q$ the $\mathbb{Q}$-rank-width, and $k$ the mim-width of a given
decomposition. This answers in the affirmative an open question raised by
Jaffke et al. (Algorithmica, 2019) concerning an XP algorithm for SFVS
parameterized by mim-width.
</p>
<p>By a unified algorithm, this solves both problems in polynomial-time on the
following graph classes: Interval, Permutation, and Bi-Interval graphs,
Circular Arc and Circular Permutation graphs, Convex graphs, $k$-Trapezoid,
Circular $k$-Trapezoid, $k$-Polygon, Dilworth-$k$ and Co-$k$-Degenerate graphs
for fixed $k$, and on arbitrary powers of graphs in any of these classes. Prior
to our results, only SFVS was known to be tractable restricted only on Interval
and Permutation graphs, whereas all other results are new.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00868</id>
    <link href="http://arxiv.org/abs/1910.00868" rel="alternate" type="text/html"/>
    <title>Advice Complexity of Adaptive Priority Algorithms</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boyar:Joan.html">Joan Boyar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Larsen:Kim_S=.html">Kim S. Larsen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pankratov:Denis.html">Denis Pankratov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00868">PDF</a><br/><b>Abstract: </b>The priority model was introduced by Borodin, Rackoff and Nielsen to capture
"greedy-like" algorithms. Motivated, in part, by the success of advice
complexity in the area of online algorithm, recently Borodin et al. have
extended the fixed priority model to include an advice tape oracle. They also
developed a reduction-based framework for proving lower bounds against this
rather powerful model. In this paper, we extend the advice tape model further
to the arguably more useful adaptive priority algorithms. We show how to modify
the reduction-based framework in order for it to apply against the more
powerful adaptive priority algorithms. In the process, we manage to simplify
the proof that the framework works, and we strengthen all the lower bounds by a
factor of 2. As a motivation of an adaptive priority model with advice, we
present a purely combinatorial adaptive priority algorithm with advice for the
minimum vertex cover problem on graphs of maximum degree 3. Our algorithm
achieves optimality and uses at most 15n/46 bits of advice. This advice is
provably shorter than what can be achieved by online algorithms with advice.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00831</id>
    <link href="http://arxiv.org/abs/1910.00831" rel="alternate" type="text/html"/>
    <title>On the Hardness of Set Disjointness and Set Intersection with Bounded Universe</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldstein:Isaac.html">Isaac Goldstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lewenstein:Moshe.html">Moshe Lewenstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Porat:Ely.html">Ely Porat</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00831">PDF</a><br/><b>Abstract: </b>In the SetDisjointness problem, a collection of $m$ sets $S_1,S_2,...,S_m$
from some universe $U$ is preprocessed in order to answer queries on the
emptiness of the intersection of some two query sets from the collection. In
the SetIntersection variant, all the elements in the intersection of the query
sets are required to be reported. These are two fundamental problems that were
considered in several papers from both the upper bound and lower bound
perspective.
</p>
<p>Several conditional lower bounds for these problems were proven for the
tradeoff between preprocessing and query time or the tradeoff between space and
query time. Moreover, there are several unconditional hardness results for
these problems in some specific computational models. The fundamental nature of
the SetDisjointness and SetIntersection problems makes them useful for proving
the conditional hardness of other problems from various areas. However, the
universe of the elements in the sets may be very large, which may cause the
reduction to some other problems to be inefficient and therefore it is not
useful for proving their conditional hardness.
</p>
<p>In this paper, we prove the conditional hardness of SetDisjointness and
SetIntersection with bounded universe. This conditional hardness is shown for
both the interplay between preprocessing and query time and the interplay
between space and query time. Moreover, we present several applications of
these new conditional lower bounds. These applications demonstrates the
strength of our new conditional lower bounds as they exploit the limited
universe size. We believe that this new framework of conditional lower bounds
with bounded universe can be useful for further significant applications.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00788</id>
    <link href="http://arxiv.org/abs/1910.00788" rel="alternate" type="text/html"/>
    <title>Streaming Balanced Clustering</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esfandiari:Hossein.html">Hossein Esfandiari</a>, Vahab Mirrokni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhong:Peilin.html">Peilin Zhong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00788">PDF</a><br/><b>Abstract: </b>Clustering of data points in metric space is among the most fundamental
problems in computer science with plenty of applications in data mining,
information retrieval and machine learning. Due to the necessity of clustering
of large datasets, several streaming algorithms have been developed for
different variants of clustering problems such as $k$-median and $k$-means
problems. However, despite the importance of the context, the current
understanding of balanced clustering (or more generally capacitated clustering)
in the streaming setting is very limited. The only previously known streaming
approximation algorithm for capacitated clustering requires three passes and
only handles insertions.
</p>
<p>In this work, we develop \emph{the first single pass streaming algorithm} for
a general class of clustering problems that includes capacitated $k$-median and
capacitated $k$-means in Euclidean space, using only poly$( k d \log \Delta)$
space, where $k$ is the number of clusters, $d$ is the dimension and $\Delta$
is the maximum relative range of a coordinate. (Note that $d\log \Delta$ is the
space required to represent one point.) This algorithm only violates the
capacity constraint by a $1+\epsilon$ factor. Interestingly, unlike the
previous algorithm, our algorithm handles both insertions and deletions of
points. To provide this result we define a decomposition of the space via some
curved half-spaces. We used this decomposition to design a strong coreset of
size poly$( k d \log \Delta)$ for balanced clustering. Then, we show that this
coreset is implementable in the streaming and distributed settings.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00777</id>
    <link href="http://arxiv.org/abs/1910.00777" rel="alternate" type="text/html"/>
    <title>Prime Clocks</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fiske:Michael_Stephen.html">Michael Stephen Fiske</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00777">PDF</a><br/><b>Abstract: </b>Physical implementations of digital computers began in the latter half of the
1930's and were first constructed from various forms of logic gates. Based on
the prime numbers, we introduce prime clocks and prime clock sums, where the
clocks utilize time and act as computational primitives instead of gates. The
prime clocks generate an infinite abelian group, where for each n, there is a
finite subgroup S such that for each Boolean function f : {0, 1}^n --&gt; {0, 1},
there exists a finite prime clock sum in S that can represent and compute f. A
parallelizable algorithm, implemented with a finite prime clock sum, is
provided that computes f. In contrast, the negation, conjunction, and
disjunction operations generate a Boolean algebra. In terms of computation,
Boolean circuits computed with logic gates NOT, AND, OR have a depth. This
means that a completely parallel computation of Boolean functions is not
possible with these gates. Overall, some new connections between number theory,
Boolean functions and computation are established.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00773</id>
    <link href="http://arxiv.org/abs/1910.00773" rel="alternate" type="text/html"/>
    <title>Approximating the Geometric Edit Distance</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fox:Kyle.html">Kyle Fox</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Xinyi.html">Xinyi Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00773">PDF</a><br/><b>Abstract: </b>Edit distance is a measurement of similarity between two sequences such as
strings, point sequences, or polygonal curves. Many matching problems from a
variety of areas, such as signal analysis, bioinformatics, etc., need to be
solved in a geometric space. Therefore, the geometric edit distance (GED) has
been studied. In this paper, we describe the first strictly sublinear
approximate near-linear time algorithm for computing the GED of two point
sequences in constant dimensional Euclidean space. Specifically, we present a
randomized (O(n\log^2n)) time (O(\sqrt n))-approximation algorithm. Then, we
generalize our result to give a randomized $\alpha$-approximation algorithm for
any $\alpha\in [1, \sqrt{n}]$, running in time $\tilde{O}(n^2/\alpha^2)$. Both
algorithms are Monte Carlo and return approximately optimal solutions with high
probability.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00724</id>
    <link href="http://arxiv.org/abs/1910.00724" rel="alternate" type="text/html"/>
    <title>A Pre-defined Sparse Kernel Based Convolutionfor Deep CNNs</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kundu:Souvik.html">Souvik Kundu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prakash:Saurav.html">Saurav Prakash</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akrami:Haleh.html">Haleh Akrami</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beerel:Peter_A=.html">Peter A. Beerel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chugg:Keith_M=.html">Keith M. Chugg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00724">PDF</a><br/><b>Abstract: </b>The high demand for computational and storage resources severely impede the
deployment of deep convolutional neural networks (CNNs) in limited-resource
devices. Recent CNN architectures have proposed reduced complexity versions
(e.g. SuffleNet and MobileNet) but at the cost of modest decreases inaccuracy.
This paper proposes pSConv, a pre-defined sparse 2D kernel-based convolution,
which promises significant improvements in the trade-off between complexity and
accuracy for both CNN training and inference. To explore the potential of this
approach, we have experimented with two widely accepted datasets, CIFAR-10 and
Tiny ImageNet, in sparse variants of both the ResNet18 and VGG16 architectures.
Our approach shows a parameter count reduction of up to 4.24x with modest
degradation in classification accuracy relative to that of standard CNNs. Our
approach outperforms a popular variant of ShuffleNet using a variant of
ResNet18 with pSConv having 3x3 kernels with only four of nine elements not
fixed at zero. In particular, the parameter count is reduced by 1.7x for
CIFAR-10 and 2.29x for Tiny ImageNet with an increased accuracy of ~4%.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00704</id>
    <link href="http://arxiv.org/abs/1910.00704" rel="alternate" type="text/html"/>
    <title>Spherical k-Nearest Neighbors Interpolation</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Philippe Trempe <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00704">PDF</a><br/><b>Abstract: </b>Geospatial interpolation is a challenging task due to real world data often
being sparse, heterogeneous and inconsistent. For that matter, this work
presents SkNNI, a spherical interpolation algorithm capable of working with
such challenging geospatial data. This work also presents NDDNISD an accurate
and efficient interpolation function for SkNNI which shines due to its spatial
awareness in terms of proximity and distribution of observation neighbors.
SkNNI's open source implementation is also discussed and illustrated with a
simple usage example.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00692</id>
    <link href="http://arxiv.org/abs/1910.00692" rel="alternate" type="text/html"/>
    <title>Retrieving Top Weighted Triangles in Graphs</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Raunak Kumar, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Paul.html">Paul Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Charikar:Moses.html">Moses Charikar</a>, Austin R. Benson Cornell University, Stanford University) <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00692">PDF</a><br/><b>Abstract: </b>Pattern counting in graphs is a fundamental primitive for many network
analysis tasks, and a number of methods have been developed for scaling
subgraph counting to large graphs. Many real-world networks carry a natural
notion of strength of connection between nodes, which are often modeled by a
weighted graph, but existing scalable graph algorithms for pattern mining are
designed for unweighted graphs. Here, we develop a suite of deterministic and
random sampling algorithms that enable the fast discovery of the 3-cliques
(triangles) with the largest weight in a graph, where weight is measured by a
generalized mean of a triangle's edges. For example, one of our proposed
algorithms can find the top-1000 weighted triangles of a weighted graph with
billions of edges in thirty seconds on a commodity server, which is orders of
magnitude faster than existing "fast" enumeration schemes. Our methods thus
open the door towards scalable pattern mining in weighted graphs.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00581</id>
    <link href="http://arxiv.org/abs/1910.00581" rel="alternate" type="text/html"/>
    <title>On the Parameterized Complexity of Reconfiguration of Connected Dominating Sets</title>
    <feedworld_mtime>1570060800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lokshtanov:Daniel.html">Daniel Lokshtanov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mouawad:Amer_E=.html">Amer E. Mouawad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panolan:Fahad.html">Fahad Panolan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Siebertz:Sebastian.html">Sebastian Siebertz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00581">PDF</a><br/><b>Abstract: </b>In a reconfiguration version of an optimization problem $\mathcal{Q}$ the
input is an instance of $\mathcal{Q}$ and two feasible solutions $S$ and $T$.
The objective is to determine whether there exists a step-by-step
transformation between $S$ and $T$ such that all intermediate steps also
constitute feasible solutions. In this work, we study the parameterized
complexity of the \textsc{Connected Dominating Set Reconfiguration} problem
(\textsc{CDS-R)}. It was shown in previous work that the \textsc{Dominating Set
Reconfiguration} problem (\textsc{DS-R}) parameterized by $k$, the maximum
allowed size of a dominating set in a reconfiguration sequence, is
fixed-parameter tractable on all graphs that exclude a biclique $K_{d,d}$ as a
subgraph, for some constant $d \geq 1$. We show that the additional
connectivity constraint makes the problem much harder, namely, that
\textsc{CDS-R} is \textsf{W}$[1]$-hard parameterized by $k+\ell$, the maximum
allowed size of a dominating set plus the length of the reconfiguration
sequence, already on $5$-degenerate graphs. On the positive side, we show that
\textsc{CDS-R} parameterized by $k$ is fixed-parameter tractable, and in fact
admits a polynomial kernel on planar graphs.
</p></div>
    </summary>
    <updated>2019-10-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1288</id>
    <link href="https://thmatters.wordpress.com/2019/10/02/a-solicitation-for-tcs-job-market-profiles/" rel="alternate" type="text/html"/>
    <title>A solicitation for TCS job market profiles</title>
    <summary>CATCS is piloting an effort this year to collect and disseminate profiles of junior theory researchers who are going on the job market during the 2019-20 academic year, complementing the job postings collected under the Jobs tab. The SIGecom community has run a similar effort very successfully for a number of years and we are […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>CATCS is piloting an effort this year to collect and disseminate profiles of junior theory researchers who are going on the job market during the 2019-20 academic year, complementing the job postings collected under the <a href="https://cstheory-jobs.org/">Jobs tab</a>. The SIGecom community has run a similar effort very successfully for a number of years and we are following their lead. The goals are two-fold:</p>
<div/>
<ul>
<li>Provide a platform to job-seekers to advertise their credentials.</li>
<li>Provide an interface for institutions/individuals with open positions to find prospective candidates.</li>
</ul>
<p>Candidates looking for theory jobs can fill out <a href="https://forms.gle/gMDaChCqQocKSXT79" rel="noopener" target="_blank">this form</a>. The form asks for basic personal information, thesis title, graduation date (past or future), research/teaching interests, bibliographic information for three publications, and allows you to add links to publications and a brief CV.</p>
<div/>
<p>The responses will be reviewed and, if approved, edited and posted on Theory Matters starting in Nov’19. There is no deadline, but for responses received after Nov 1 please allow two weeks for review before your profile appears on the website. Responses received by Dec 15 will have summaries published in the following issue of SIGACT News.</p></div>
    </content>
    <updated>2019-10-02T21:09:02Z</updated>
    <published>2019-10-02T21:09:02Z</published>
    <category term="for PhD students"/>
    <category term="postdocs"/>
    <category term="Uncategorized"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2019-10-03T20:21:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/133</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/133" rel="alternate" type="text/html"/>
    <title>TR19-133 |  More on $AC^0[\oplus]$ and Variants of the Majority Function | 

	Utkarsh Tripathi, 

	Nutan Limaye, 

	Srikanth Srinivasan</title>
    <summary>In this paper we prove two results about $AC^0[\oplus]$ circuits. 

We show that for $d(N) = o(\sqrt{\log N/\log \log N})$ and $N \leq s(N) \leq 2^{dN^{1/d^2}}$ there is an explicit family of functions $\{f_N:\{0,1\}^N\rightarrow \{0,1\}\}$ such that 
$f_N$ has uniform $AC^0$ formulas of depth $d$ and size at most $s$; 
$f_N$ does not have $AC^0[\oplus]$ formulas of depth $d$ and size $s^{\varepsilon}$, where $\varepsilon$ is a fixed absolute constant. 

This gives a quantitative improvement on the recent result of Limaye, Srinivasan, Sreenivasaiah, Tripathi, and Venkitesh, (STOC, 2019), which proved a similar Fixed-Depth Size-Hierarchy theorem but for $d \ll \log \log N$ and $s \ll \exp(N^{1/2^{\Omega(d)}})$. 

As in the previous result, we use the Coin Problem to prove our hierarchy theorem. Our main technical result is the construction of uniform size-optimal formulas for solving the coin problem with improved sample complexity $(1/\delta)^{d+4}$ (down from $(1/\delta)^{2^{O(d)}}$ in the previous result).

In our second result, we show that randomness buys depth in the $AC^0[\oplus]$ setting. Formally, we show that for any fixed constant $d\geq 2$, there is a family of Boolean functions that has polynomial-sized randomized uniform $AC^0$ circuits of depth $d$ but no polynomial-sized (deterministic) $AC^0[\oplus]$ circuits of depth $d$.

Previously Viola (Computational Complexity, 2014) showed that an increase in depth (by at least $2$) is essential to avoid superpolynomial blow-up while derandomizing randomized $AC^0$ circuits. We show that an increase in depth (by at least $1$) is essential even for $AC^0[\oplus]$. 

As in Viola's result, the separating examples are promise variants of the Majority function on $N$ inputs that accept inputs of weight at least $N/2 + N/(\log N)^{d-1}$ and reject inputs of weight at most $N/2 - N/(\log N)^{d-1}$.</summary>
    <updated>2019-10-02T11:00:43Z</updated>
    <published>2019-10-02T11:00:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-03T20:20:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/02/quics-fellows-at-joint-center-for-quantum-information-and-computer-science-apply-by-october-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/02/quics-fellows-at-joint-center-for-quantum-information-and-computer-science-apply-by-october-15-2019/" rel="alternate" type="text/html"/>
    <title>QuICS Fellows at Joint Center for Quantum Information and Computer Science (apply by October 15, 2019)</title>
    <summary>The Joint Center for Quantum Information and Computer Science (QuICS) is currently seeking outstanding quantum information researchers to join the Center faculty as QuICS Fellows. QuICS is a research partnership between the University of Maryland and the National Institute of Standards and Technology, with faculty from both institutions. Website: http://quics.umd.edu/join-quics/new-faculty Email: quics-coordinator@umiacs.umd.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Joint Center for Quantum Information and Computer Science (QuICS) is currently seeking outstanding quantum information researchers to join the Center faculty as QuICS Fellows. QuICS is a research partnership between the University of Maryland and the National Institute of Standards and Technology, with faculty from both institutions.</p>
<p>Website: <a href="http://quics.umd.edu/join-quics/new-faculty">http://quics.umd.edu/join-quics/new-faculty</a><br/>
Email: quics-coordinator@umiacs.umd.edu</p></div>
    </content>
    <updated>2019-10-02T03:13:13Z</updated>
    <published>2019-10-02T03:13:13Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-03T20:21:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3422</id>
    <link href="https://agtb.wordpress.com/2019/10/02/matching-markets-simons-driven-by-theory-driving-the-economy/" rel="alternate" type="text/html"/>
    <title>Matching Markets @ Simons:  Driven by Theory, Driving the Economy</title>
    <summary>[Guest post by Sid Banerjee.] Divergent Evolution: The formation of new species when populations experience different selective pressures. While the canonical example is Darwin’s finches, it could apply as well to matching theorists! A notable feature of the first and second workshops at the Simons Institute program on Matching Markets was how researchers in Economics, […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Sid Banerjee.]</em></p>



<blockquote class="wp-block-quote"><p><em>Divergent Evolution: The formation of new species when populations experience different selective pressures.</em></p></blockquote>



<p>While the canonical example is Darwin’s finches, it could apply as well to matching theorists! A notable feature of the <a href="https://simons.berkeley.edu/workshops/market2019-1" rel="noreferrer noopener" target="_blank">first</a> and <a href="https://simons.berkeley.edu/workshops/market2019-2" rel="noreferrer noopener" target="_blank">second</a> workshops at the <a href="https://simons.berkeley.edu/programs/market2019" rel="noreferrer noopener" target="_blank">Simons Institute program on Matching Markets</a> was how researchers in Economics, Operations Research and TCS all share common antecedents (Fulkerson, Gale, Scarf, Shapley, Walras — to name but a few giants invoked regularly), and yet have taken the theory in  diverse directions. The workshops helped create a healthy dialogue between the communities, as everyone tries to understand each other’s objectives and techniques. </p>



<p>A more notable aspect of matching theory in recent years has been its  impact on the design of real-world marketplaces. Over the two workshops,  a mix of speakers from academia and industry covered a host of markets,  including <a href="https://simons.berkeley.edu/talks/tba-121" rel="noreferrer noopener" target="_blank">payment routing</a>, <a href="https://simons.berkeley.edu/talks/tba-127" rel="noreferrer noopener" target="_blank">online advertising</a>, <a href="https://simons.berkeley.edu/talks/tba-124" rel="noreferrer noopener" target="_blank">kidney exchange</a>, <a href="https://simons.berkeley.edu/talks/tba-128" rel="noreferrer noopener" target="_blank">real-estate</a>, <a href="https://simons.berkeley.edu/talks/tba-131" rel="noreferrer noopener" target="_blank">public housing</a>, <a href="https://simons.berkeley.edu/talks/ridesharing-panel" rel="noreferrer noopener" target="_blank">ride-sharing</a>, <a href="https://simons.berkeley.edu/talks/driving-efficiencies-freight-industry" rel="noreferrer noopener" target="_blank">long-haul trucking</a>, <a href="https://simons.berkeley.edu/talks/ratings-design-and-barriers-entry" rel="noreferrer noopener" target="_blank">restaurant reviews</a>, <a href="https://simons.berkeley.edu/talks/tba-129" rel="noreferrer noopener" target="_blank">school choice</a>, <a href="https://simons.berkeley.edu/talks/unreasonable-effectiveness-artificial-currencies" rel="noreferrer noopener" target="_blank">food-banks</a> and many many others. A common theme that emerged was that online marketplaces, with the support of good algorithm and mechanism designers, are slowly taking over the economy.</p>



<p>And talking of giants of matching theory, another event held in parallel with the program was a <a href="https://simons.berkeley.edu/events/richard-m-karp-distinguished-lecture-inaugural-lecture" rel="noreferrer noopener" target="_blank">celebration</a> of the achievements and contributions of Dick Karp, with Vijay Vazirani giving the <a href="https://simons.berkeley.edu/rmklectures2019-fall-1" rel="noreferrer noopener" target="_blank">inaugural lecture</a> of the Simons Institute Richard M. Karp Distinguished Lecture Series. Vijay’s talk touched on both the above themes, with a sweeping overview of three great threads in matching theory (stable matching, market equilibria, and online matching). He highlighted the critical role of algorithmic thinking in their development, and concluded with a tantalizing 40-year-old open problem connected to finding a polynomial-time algorithm for the Hylland-Zeckhauser market equilibrium. It is an excellent starting point for those interested in the program, or matching markets in general!</p></div>
    </content>
    <updated>2019-10-02T00:45:32Z</updated>
    <published>2019-10-02T00:45:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>robertkleinberg</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-10-03T20:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00551</id>
    <link href="http://arxiv.org/abs/1910.00551" rel="alternate" type="text/html"/>
    <title>An Efficient Sampling Algorithm for Non-smooth Composite Potentials</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mou:Wenlong.html">Wenlong Mou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Flammarion:Nicolas.html">Nicolas Flammarion</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wainwright:Martin_J=.html">Martin J. Wainwright</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bartlett:Peter_L=.html">Peter L. Bartlett</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00551">PDF</a><br/><b>Abstract: </b>We consider the problem of sampling from a density of the form $p(x) \propto
\exp(-f(x)- g(x))$, where $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a smooth
and strongly convex function and $g: \mathbb{R}^d \rightarrow \mathbb{R}$ is a
convex and Lipschitz function. We propose a new algorithm based on the
Metropolis-Hastings framework, and prove that it mixes to within TV distance
$\varepsilon$ of the target density in at most $O(d \log (d/\varepsilon))$
iterations. This guarantee extends previous results on sampling from
distributions with smooth log densities ($g = 0$) to the more general composite
non-smooth case, with the same mixing time up to a multiple of the condition
number. Our method is based on a novel proximal-based proposal distribution
that can be efficiently computed for a large class of non-smooth functions $g$.
</p></div>
    </summary>
    <updated>2019-10-02T23:27:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00517</id>
    <link href="http://arxiv.org/abs/1910.00517" rel="alternate" type="text/html"/>
    <title>Learning Multi-Stage Sparsification for Maximum Clique Enumeration</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Marco Grassia, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lauri:Juho.html">Juho Lauri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dutta:Sourav.html">Sourav Dutta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ajwani:Deepak.html">Deepak Ajwani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00517">PDF</a><br/><b>Abstract: </b>We propose a multi-stage learning approach for pruning the search space of
maximum clique enumeration, a fundamental computationally difficult problem
arising in various network analysis tasks. In each stage, our approach learns
the characteristics of vertices in terms of various neighborhood features and
leverage them to prune the set of vertices that are likely not contained in any
maximum clique. Furthermore, we demonstrate that our approach is domain
independent -- the same small set of features works well on graph instances
from different domain. Compared to the state-of-the-art heuristics and
preprocessing strategies, the advantages of our approach are that (i) it does
not require any estimate on the maximum clique size at runtime and (ii) we
demonstrate it to be effective also for dense graphs. In particular, for dense
graphs, we typically prune around 30 \% of the vertices resulting in speedups
of up to 53 times for state-of-the-art solvers while generally preserving the
size of the maximum clique (though some maximum cliques may be lost). For large
real-world sparse graphs, we routinely prune over 99 \% of the vertices
resulting in several tenfold speedups at best, typically with no impact on
solution quality.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00510</id>
    <link href="http://arxiv.org/abs/1910.00510" rel="alternate" type="text/html"/>
    <title>Joint Subcarrier and Power Allocation in NOMA: Optimal and Approximate Algorithms</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sala=uuml=n:Lou.html">Lou Salaün</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coupechoux:Marceau.html">Marceau Coupechoux</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Chung_Shue.html">Chung Shue Chen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00510">PDF</a><br/><b>Abstract: </b>Non-orthogonal multiple access (NOMA) is a promising technology to increase
the spectral efficiency and enable massive connectivity in 5G and future
wireless networks. In contrast to orthogonal schemes, such as OFDMA, NOMA
multiplexes several users on the same frequency and time resource. Joint
subcarrier and power allocation problems (JSPA) in NOMA are NP-hard to solve in
general. In this family of problems, we consider the weighted sum-rate (WSR)
objective function as it can achieve various tradeoffs between sum-rate
performance and user fairness. Because of JSPA's intractability, a common
approach in the literature is to solve separately the power control and
subcarrier allocation (also known as user selection) problems, therefore
achieving sub-optimal result. In this work, we first improve the computational
complexity of existing single-carrier power control and user selection schemes.
These improved procedures are then used as basic building blocks to design new
algorithms, namely Opt-JSPA, $\varepsilon$-JSPA and Grad-JSPA. Opt-JSPA
computes an optimal solution with lower complexity than current optimal schemes
in the literature. It can be used as a benchmark for optimal WSR performance in
simulations. However, its pseudo-polynomial time complexity remains impractical
for real-world systems with low latency requirements. To further reduce the
complexity, we propose a fully polynomial-time approximation scheme called
$\varepsilon$-JSPA. Since, no approximation has been studied in the literature,
$\varepsilon$-JSPA stands out by allowing to control a tight trade-off between
performance guarantee and complexity. Finally, Grad-JSPA is a heuristic based
on gradient descent. Numerical results show that it achieves near-optimal WSR
with much lower complexity than existing optimal methods.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00494</id>
    <link href="http://arxiv.org/abs/1910.00494" rel="alternate" type="text/html"/>
    <title>Approximating the Percolation Centrality through Sampling and Pseudo-dimension</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Alane M. de Lima, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Murilo_V=_G=_da.html">Murilo V. G. da Silva</a>, André L. Vignatti <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00494">PDF</a><br/><b>Abstract: </b>In this work we investigate the problem of percolation centrality, a
generalization of betweenness centrality, in a weighted graph $G$ under the
light of sample complexity theory. For both betweenness and percolation
centrality the computation of the {\it exact} value for a given vertex $v$ is
not known to be easier than the computation the same value, all at once, for
all $n$ vertices of $G$. In any one of these cases it is an open problem
whether these measures can be computed in $\mathcal{O}(n^{3-c})$ time, for any
constant $c&gt;0$. In this paper we first present a $\mathcal{O}(m \log^2 n)$
randomized approximation algorithm for the percolation centrality for every
vertex of $G$, generalizing techniques developed by \cite{RiondatoUpfal} (this
complexity is reduced to $\mathcal{O}((m+n) \log n)$ for unweighted graphs).
The estimative obtained by the algorithm is within $\epsilon$ of the exact
value with probability $1- \delta$, for {\it fixed} constants $0 &lt;
\epsilon,\delta \leq 1$. Additionally, we show that sample complexity theory
can be used to distinguish the problem of estimating the percolation centrality
of a single vertex, refered as computing $\tilde{p}(v)$, from the problem of
estimating the percolation centrality of every vertex of $G$, refered as
computing $\tilde{p}(G)$. More precisely, we show that $\tilde{p}(v)$ and
$\tilde{p}(G)$ can be estimated respectively in time $\mathcal{O}(m \log n)$
and $\mathcal{O}(m \log^2 n)$. Our results also imply a similar "separation"
for percolation estimation in unweighted dense graphs as well as separations
for the estimation of betweenness centrality that holds in any combination of
the following scenarios: weighted or unweighted for either sparse or dense
graphs.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00477</id>
    <link href="http://arxiv.org/abs/1910.00477" rel="alternate" type="text/html"/>
    <title>Parameterized complexity of quantum invariants</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maria:Cl=eacute=ment.html">Clément Maria</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00477">PDF</a><br/><b>Abstract: </b>We give a general fixed parameter tractable algorithm to compute quantum
invariants of links presented by diagrams, whose complexity is singly
exponential in the carving-width (or the tree-width) of the diagram. In
particular, we get a $O(N^{\frac{3}{2} \mathrm{cw}} \mathrm{poly}(n))$ time
algorithm to compute any Reshetikhin-Turaev invariant---derived from a simple
Lie algebra $\mathfrak{g}$---of a link presented by a planar diagram with $n$
crossings and carving-width $\mathrm{cw}$, and whose components are coloured
with $\mathfrak{g}$-modules of dimension at most $N$. For example, this
includes the $N^{th}$ coloured Jones polynomials and the $N^{th}$ coloured
HOMFLYPT polynomials.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00440</id>
    <link href="http://arxiv.org/abs/1910.00440" rel="alternate" type="text/html"/>
    <title>The Complexity of Packing Edge-Disjoint Paths</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dreier:Jan.html">Jan Dreier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fuchs:Janosch.html">Janosch Fuchs</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hartmann:Tim_A=.html">Tim A. Hartmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuinke:Philipp.html">Philipp Kuinke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rossmanith:Peter.html">Peter Rossmanith</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tauer:Bjoern.html">Bjoern Tauer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Hung=Lung.html">Hung-Lung Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00440">PDF</a><br/><b>Abstract: </b>We introduce and study the complexity of Path Packing. Given a graph $G$ and
a list of paths, the task is to embed the paths edge-disjoint in $G$. This
generalizes the well known Hamiltonian-Path problem.
</p>
<p>Since Hamiltonian Path is efficiently solvable for graphs of small treewidth,
we study how this result translates to the much more general Path Packing. On
the positive side, we give an FPT-algorithm on trees for the number of paths as
parameter. Further, we give an XP-algorithm with the combined parameters
maximal degree, number of connected components and number of nodes of degree at
least three. Surprisingly the latter is an almost tight result by runtime and
parameterization. We show an ETH lower bound almost matching our runtime.
Moreover, if two of the three values are constant and one is unbounded the
problem becomes NP-hard.
</p>
<p>Further, we study restrictions to the given list of paths. On the positive
side, we present an FPT-algorithm parameterized by the sum of the lengths of
the paths. Packing paths of length two is polynomial time solvable, while
packing paths of length three is NP-hard. Finally, even the spacial case EPC
where the paths have to cover every edge in $G$ exactly once is already NP-hard
for two paths on 4-regular graphs.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00400</id>
    <link href="http://arxiv.org/abs/1910.00400" rel="alternate" type="text/html"/>
    <title>Incorporating Trip Chaining within Online Demand Estimation</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Guido Cantelmoa, Moeid Qurashia, A. Arun Prakashc, Constantinos Antonioua, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Viti:Francesco.html">Francesco Viti</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00400">PDF</a><br/><b>Abstract: </b>Time-dependent Origin-Destination (OD) demand flows are fundamental inputs
for Dynamic Traffic Assignment (DTA) systems and real-time traffic management.
This work introduces a novel state-space framework to estimate these demand
flows in an online context. Specifically, we propose to explicitly include
trip-chaining behavior within the state-space formulation, which is solved
using the well-established Kalman Filtering technique. While existing works
already consider structural information and recursive behavior within the
online demand estimation problem, this information has been always considered
at the OD level. In this study, we introduce this structural information by
explicitly representing trip-chaining within the estimation framework. The
advantage is twofold. First, all trips belonging to the same tour can be
jointly calibrated. Second, given the estimation during a certain time
interval, a prediction of the structural deviation over the whole day can be
obtained without the need to run additional simulations. The effectiveness of
the proposed methodology is demonstrated first on a toy network and then on a
large real-world network. Results show that the model improves the prediction
performance with respect to a conventional Kalman Filtering approach. We also
show that, on the basis of the estimation of the morning commute, the model can
be used to predict the evening commute without need of running additional
simulations.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00305</id>
    <link href="http://arxiv.org/abs/1910.00305" rel="alternate" type="text/html"/>
    <title>Complexity of Stability</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Frei:Fabian.html">Fabian Frei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hemaspaandra:Edith.html">Edith Hemaspaandra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rothe:J=ouml=rg.html">Jörg Rothe</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00305">PDF</a><br/><b>Abstract: </b>Graph parameters such as the clique number, the chromatic number, and the
independence number are central in many areas, ranging from computer networks
to linguistics to computational neuroscience to social networks. In particular,
the chromatic number of a graph (i.e., the smallest number of colors needed to
color all vertices such that no two adjacent vertices are of the same color)
can be applied in solving practical tasks as diverse as pattern matching,
scheduling jobs to machines, allocating registers in compiler optimization, and
even solving Sudoku puzzles. Typically, however, the underlying graphs are
subject to (often minor) changes. To make these applications of graph
parameters robust, it is important to know which graphs are stable for them in
the sense that adding or deleting single edges or vertices does not change
them. We initiate the study of stability of graphs for such parameters in terms
of their computational complexity. We show that, for various central graph
parameters, the problem of determining whether or not a given graph is stable
is complete for \Theta_2^p, a well-known complexity class in the second level
of the polynomial hierarchy, which is also known as "parallel access to NP."
</p></div>
    </summary>
    <updated>2019-10-02T23:20:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00223</id>
    <link href="http://arxiv.org/abs/1910.00223" rel="alternate" type="text/html"/>
    <title>An improved analysis and unified perspective on deterministic and randomized low rank matrix approximations</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demmel:James.html">James Demmel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grigori:Laura.html">Laura Grigori</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rusciano:Alexander.html">Alexander Rusciano</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00223">PDF</a><br/><b>Abstract: </b>We introduce a Generalized LU-Factorization (\textbf{GLU}) for low-rank
matrix approximation. We relate this to past approaches and extensively analyze
its approximation properties. The established deterministic guarantees are
combined with sketching ensembles satisfying Johnson-Lindenstrauss properties
to present complete bounds. Particularly good performance is shown for the
sub-sampled randomized Hadamard transform (SRHT) ensemble. Moreover, the
factorization is shown to unify and generalize many past algorithms. It also
helps to explain the effect of sketching on the growth factor during Gaussian
Elimination.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00152</id>
    <link href="http://arxiv.org/abs/1910.00152" rel="alternate" type="text/html"/>
    <title>On the Complexity of Approximating Multimarginal Optimal Transport</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Tianyi.html">Tianyi Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Ho:Nhat.html">Nhat Ho</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cuturi:Marco.html">Marco Cuturi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jordan:Michael_I=.html">Michael I. Jordan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00152">PDF</a><br/><b>Abstract: </b>We study the complexity of approximating the multimarginal optimal transport
(OT) problem, a generalization of the classical optimal transport distance,
considered here between $m$ discrete probability distributions supported each
on $n$ support points. First, we show that the multimarginal OT problem is not
a minimum-cost flow problem when $m \geq 3$. This implies that many of the
combinatorial algorithms developed for classical OT are not applicable to
multimarginal OT, and therefore the standard interior-point algorithm bounds
result in an intractable complexity bound of $\widetilde{\mathcal{O}}(n^{3m})$.
Second, we propose and analyze two simple algorithms for approximating the
multimarginal OT problem. The first algorithm, which we refer to as
multimarginal Sinkhorn, improves upon previous multimarginal generalizations of
the celebrated Sinkhorn algorithm. We show that it achieves a near-linear time
complexity bound of $\widetilde{\mathcal{O}}(m^3 n^m / \varepsilon^2)$ for a
tolerance $\varepsilon \in (0, 1)$. This matches the best known complexity
bound for the Sinkhorn algorithm when $m = 2$ for approximating the classical
OT distance. The second algorithm, which we refer to as multimarginal
Randkhorn, accelerates the first algorithm by incorporating a randomized
estimate sequence and achieves a complexity bound of
$\widetilde{\mathcal{O}}(m^{8/3} n^{m+1/3}/\varepsilon)$. This improves on the
complexity bound of the first algorithm by $1/\varepsilon$ and matches the best
known complexity bound for the Randkhorn algorithm when $m=2$ for approximating
the classical OT distance.
</p></div>
    </summary>
    <updated>2019-10-02T23:38:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>
</feed>
