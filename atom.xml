<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2022-05-03T23:38:22Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/065</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/065" rel="alternate" type="text/html"/>
    <title>TR22-065 |  Streaming and Sketching Complexity of CSPs: A survey | 

	Madhu Sudan</title>
    <summary>In this survey we describe progress over the last decade or so in understanding the complexity of solving constraint satisfaction problems (CSPs) approximately in the streaming and sketching models of computation. After surveying some of the results we give some sketches of the proofs and in particular try to explain why there is a tight dichotomy result for sketching algorithms working in subpolynomial space regime.</summary>
    <updated>2022-05-03T19:56:03Z</updated>
    <published>2022-05-03T19:56:03Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-05-03T23:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/064</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/064" rel="alternate" type="text/html"/>
    <title>TR22-064 |  Improved Low-Depth Set-Multilinear Circuit Lower Bounds | 

	Deepanshu Kush, 

	Shubhangi Saraf</title>
    <summary>In this paper, we prove strengthened lower bounds for constant-depth set-multilinear formulas. More precisely, we show that over any field, there is an explicit polynomial $f$ in VNP defined over $n^2$ variables, and of degree $n$, such that any product-depth $\Delta$ set-multilinear formula computing $f$ has size at least $n^{\Omega \left( n^{1/\Delta}/\Delta\right)}$. The hard polynomial $f$ comes from the class of Nisan-Wigderson (NW) design-based polynomials. 

Our lower bounds improve upon the recent work of Limaye, Srinivasan and Tavenas (STOC 2022), where a lower bound of the form $(\log n)^{\Omega (\Delta n^{1/\Delta})}$ was shown for the size of product-depth $\Delta$ set-multilinear formulas computing the iterated matrix multiplication (IMM) polynomial of the same degree and over the same number of variables as $f$. Moreover, our lower bounds are novel for any $\Delta\geq 2$.

The precise quantitative expression in our lower bound is interesting also because the lower bounds we obtain are "sharp" in the sense that any asymptotic improvement would imply general set-multilinear circuit lower bounds via depth reduction results. 

In the setting of general set-multilinear formulas, a lower bound of the form $n^{\Omega(\log n)}$ was already obtained by Raz (J. ACM 2009) for the more general model of multilinear formulas. The techniques of LST (which extend the techniques of the same authors in (FOCS 2021)) give a different route to set-multilinear formula lower bounds, and allow them to obtain a lower bound of the form $(\log n)^{\Omega(\log n)}$ for the size of general set-multilinear formulas computing the IMM polynomial. Our proof techniques are another variation on those of LST, and enable us to show an improved lower bound (matching that of Raz) of the form $n^{\Omega(\log n)}$, albeit for the same polynomial $f$ in VNP (the NW polynomial). As observed by LST, if the same $n^{\Omega(\log n)}$ size lower bounds for unbounded-depth set-multilinear formulas could be obtained for the IMM polynomial, then using the self-reducibility of IMM and using hardness escalation results, this would imply super-polynomial lower bounds for general algebraic formulas.</summary>
    <updated>2022-05-03T19:37:30Z</updated>
    <published>2022-05-03T19:37:30Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-05-03T23:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/063</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/063" rel="alternate" type="text/html"/>
    <title>TR22-063 |  Fast Multivariate Multipoint Evaluation Over All Finite Fields | 

	Mrinal Kumar, 

	Sumanta Ghosh, 

	Zeyu Guo, 

	Chris Umans, 

	Vishwas Bhargava</title>
    <summary>Multivariate multipoint evaluation is the problem of evaluating a multivariate polynomial, given as a coefficient vector, simultaneously at multiple evaluation points. In this work, we show that there exists a deterministic algorithm for multivariate multipoint evaluation over any finite field $\mathbb{F}$ that outputs the evaluations of an $m$-variate polynomial of degree less than $d$ in each variable at $N$ points in time 
\[
(d^m+N)^{1+o(1)}\cdot poly(m,d,\log|\F|)
\]
for all $m\in\mathbb{N}$ and all sufficiently large $d\in \mathbb{N}$.

A previous work of Kedlaya and Umans (FOCS 2008, SICOMP 2011) achieved the same time complexity when the number of variables $m$ is at most $d^{o(1)}$ and had left the problem of removing this condition as an open problem. A recent work of Bhargava, Ghosh, Kumar and Mohapatra (STOC 2022) answered this question when the underlying field is not too large  and has characteristic less than $d^{o(1)}$. In this work, we remove this constraint on the number of variables over all finite fields, thereby answering the question of Kedlaya and Umans over all finite fields. 

Our algorithm relies on a non-trivial combination of ideas from three seemingly different previously known algorithms for multivariate multipoint evaluation, namely the algorithms of Kedlaya and Umans, that of  Bj\"orklund, Kaski and Williams (IPEC 2017, Algorithmica 2019), and that of Bhargava, Ghosh, Kumar and Mohapatra, together with a result of Bombieri and Vinogradov from analytic number theory about the distribution of primes in an arithmetic progression. 

We also present a second algorithm for multivariate multipoint evaluation that is completely elementary and in particular, avoids the use of the Bombieri--Vinogradov Theorem. However, it requires a mild assumption that the field size is bounded by an exponential-tower in $d$ of bounded height. More specifically, our second algorithm solves the multivariate multipoint evaluation problem over a finite field $\mathbb{F}$ in time 
\[
(d^m+N)^{1+o(1)}\cdot poly(m,d,\log |\mathbb{F}|)\]
for all $m\in \mathbb{N}$ and all sufficiently large $d\in \mathbb{N}$, provided that the size of the finite field $\mathbb{F}$ is at most $(\exp(\exp(\exp(\cdots (\exp(d)))))$, where the height of this tower of exponentials is fixed.</summary>
    <updated>2022-05-03T19:35:28Z</updated>
    <published>2022-05-03T19:35:28Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-05-03T23:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8321</id>
    <link href="https://windowsontheory.org/2022/05/03/philosophy-of-science-and-the-blockchain-a-book-review/" rel="alternate" type="text/html"/>
    <title>Philosophy of science and the blockchain: A book review</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This blog post is a book review of sorts for the following two books: To Explain the World: The Discovery of Modern Science by Steven Weinberg (2016) The Knowledge Machine: How Irrationality Created Modern Science by Michael Strevens (2020). Both books cover (in different proportions) the history and philosophy of science. By the end of … <a class="more-link" href="https://windowsontheory.org/2022/05/03/philosophy-of-science-and-the-blockchain-a-book-review/">Continue reading <span class="screen-reader-text">Philosophy of science and the blockchain: A book review</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This blog post is a book review of sorts for the following two books:</p>



<p><a href="https://www.amazon.com/Explain-World-Discovery-Modern-Science/dp/0062346660">To Explain the World: The Discovery of Modern Science</a> by Steven Weinberg (2016)</p>



<p><a href="https://www.amazon.com/Knowledge-Machine-Irrationality-Created-Science/dp/1631491377/">The Knowledge Machine: How Irrationality Created Modern Science</a> by Michael Strevens (2020).</p>



<p>Both books cover (in different proportions) the history and philosophy of science. By the end of this post, I will also discuss my thoughts. <em>Trigger warning:</em> I will end up comparing science to the blockchain and finding positive aspects of the dreaded “reviewer 2”.</p>



<p>Both books attempt to answer the question of <strong>“why and how did the scientific revolution happen?”</strong>. Strevens describes this conundrum as follows:</p>



<p><em>“[If you were a citizen of ancient Greece], you could enjoy just about every cultural invention that makes life worth living today. You could delight in the poetry of Homer and Sappho, visit the theater to relish Oedipus Rex and other masterpieces of ancient drama… You could live in cities regulated by law and a system of courts shaped by the architects and sculptors who built some of the seven wonders of the world and governed in accordance with political models that have lasted to this day: monarchy, oligarchy, sweet democracy.”</em></p>



<p>The point Strevens makes is that while obviously, a lot has changed in culture and society since 300 BC, the change has not been so drastic as to make life unrecognizably different. In contrast, science and technology have undergone drastic changes in the same period, with most changes occurring in the last few centuries. Weinberg quotes the historian Herbert Butterfield, who claimed that the scientific revolution <em>“outshines everything since the rise of Christianity and reduces the Renaissance and Reformation to the rank of mere episodes.”</em></p>



<p>If you are the type of person who prefers data to quotes from historians, there is no shortage of graphs demonstrating this point, including the following:</p>



<figure class="wp-block-image"><img alt="" src="https://lh3.googleusercontent.com/DMx0_KjPGixzzYdBleMYRX2Xx_sf0ubgbU04kyluA_vMugVbJhnRnI_I8lY7u8h2qYfHVxjnKltPKG8V94LrNYMGAeUPvV8I-h8JPFL00rC8FCReH0E-aavJsV7gnxpwXSRv43RH0tRF_ES8uA"/></figure>



<p>(The right graph is adapted from Terry Tao’s excellent <a href="https://terrytao.wordpress.com/2010/10/10/the-cosmic-distance-ladder-ver-4-1/">cosmic distance ladder</a> presentation; I was happy to hear Tao is planning to turn it into a <a href="https://terrytao.wordpress.com/2020/10/10/climbing-the-cosmic-distance-ladder-book-announcement/">popular science book</a>.)</p>



<p>One distinguishing property of science (compared to activities such as religion, philosophy, art, political thought, sports etc.) is that it simultaneously satisfies the following three properties:</p>



<ul><li><strong>Consensus:</strong> While scientists don’t agree about everything, they generally agree on much, and the scientific literature does seem to converge toward consensus in the long run. Perhaps most importantly, even when scientists disagree on interpreting facts, they agree on what these facts are and agree on the forum where these should be discussed. In contrast, there is much less consensus in other spheres of human activities. Often, there isn’t even a consensus on what forum to hold the debate or even if to hold it at all. As far as I know, there isn’t a <em>Science</em> or <em>Nature</em> journal of theology where clergypersons of all faiths debate with one another. Indeed, with the polarized media these days, there are fewer and fewer forums where Democrats and Republicans debate with one another.<br/></li><li><strong>Change:</strong> The consensus is not just frozen in time but instead evolves. Scientists today generally agree with one another, but their views have radically changed over time. While, as Strevens notes, in art and religion, millennia-old texts are still very much relevant, in Machine Learning, when I recommend my students “read the classics,” I mean papers written before 2018 <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/><br/></li><li><strong>Correlation with reality:</strong> Science does not merely change but also <em>progresses</em>, in the sense that with time we gain a better understanding of nature. While some radical subjectivists might dispute that external reality exists or that science gets to know more of it, most people who have ever flown in a plane, were administered a vaccine, or read a blog post over the Internet, will agree that science did advance. Science’s path may be a random walk, but that walk is biased towards progress.</li></ul>



<p>Science did not always possess the above properties, and the distinction between science and philosophy was much murkier in the past. Both Strevens and Weinberg trace the birth of modern science to the Scientific Revolution of the 16th and 17th centuries. Both books describe the history of science and attempt to answer what changed since ancient times and why.</p>



<p><strong>Strevens: Modern Science as a result of Newton and the “Iron Rule”:</strong></p>



<p>Strevens begins by explaining the philosophies of Karl Popper and Thomas Kuhn. I found this part very informative since I have never read a proper philosophy of science book.  (He also describes their fascinating personal histories.)</p>



<p>For Popper, progress in science happens via <em>refutations</em>. At a given point in time, there is a collection of hypotheses H₁,…,Hₖ that are currently consistent with known experiments. Then an experiment happens which refutes some of these, narrowing down the set of potentially feasible theories.</p>



<p>According to Strevens, Popper is an absolutist. Once a theory has been falsified, it’s dead, and all theories that have not been yet falsified will <em>“forever remain … conjectures”</em>. That is, Popper was not a Bayesian and did not assign any likelihood to theories based on how many true predictions they made. The only question was whether they still survive or have been refuted.</p>



<p>Popper is said to have been inspired by Einstein, who was willing to put his theory to the test and said, <em>“if the redshift of spectral lines due to the gravitational potential should not exist, then [my] general theory of relativity will be untenable.” </em> Indeed, a classical story of testing a theory via refutation is <a href="https://en.wikipedia.org/wiki/Eddington_experiment">Eddington’s expedition</a> to check Einstein’s predictions during the 1919 total solar eclipse. Specifically, due to the curvature of space, the effect of the sun’s gravity on the light shining from stars would be double the effect predicted by Newton’s theory. </p>



<p>A priori, the Eddington expedition seems like a textbook Popperian refutation: an experiment is set out to test conflicting predictions of two theories, after which only one of them will survive. However, as Strevens points out, the truth is murkier. The expedition took three measurements: one on the island of Principe, off the coast of West Africa, and two in Brazil. The weather in Principe was cloudy, and the resulting photos were blurry. In Brazil weather was good, but the measurements were taken using two different telescopes that gave conflicting results. Concretely, one of the Brazil telescopes’ measurements conformed with Einstein’s predictions, while the other conformed with the Newtonian theory. Eddington argued that the second Brazil telescope had a systematic error, and hence Einstein’s theory was confirmed. This was accepted by the Royal Society and made big headlines at the time. While Eddington probably <a href="https://www.nature.com/articles/d41586-019-01172-z">made the best conclusion given the data</a>, Strevens claims that he also had extra-scientific motivations to endorse Einstein’s theory: a desire to unite British and German science after WW-I. Einstein’s response to the experiment was also famously non-Popperian: When asked what he would have done if the investigation had the opposing results, he said, <em>“Then I would feel sorry for the dear Lord. The theory is correct anyway.”</em></p>



<p>Thomas Kuhn famously saw scientific progress as not so much a process of incremental refutation as periodic revolutions and paradigm shifts. A paradigm consists not just of mere factual predictions but also ways of thinking and determining truth. As such, paradigms are internally self-consistent, and one cannot “refute” them as much as step away from them, when they outlive their usefulness and cannot handle “anomalies”. Geo-centrism is a good example. While Strevens doesn’t say this, it is impossible to refute a geocentric point of the world (we can always say that the sun rotates around the earth but just does it in a very complex path). And of course, as Earth-based scientists, making it the center of the universe is a very natural prior assumption. Therefore, Helio-centrism did not arise from refuting geo-centrism but through switching to a different paradigm. </p>



<p>For Popper, scientific theories die in a duel with one another. For Kuhn, they die of old age. As scientists struggle ever more to get useful results out of their current paradigm (maybe fine-tuning it more and more to fit the data), they get the sinking feeling that it’s a dead end, and are ready to accept a new paradigm.  While some of Kuhn’s radical followers believed there is no such thing as scientific progress and all paradigms are equally valid, Strevens says that (at least later in life)  Kuhn did not agree with this view. Thus, for Kuhn, while switching paradigms is not as much a rational deliberation as a “leap of faith,” it typically results in improved understanding and predictive power. </p>



<p>If the Popperian scientist is a refutation machine, according to Kuhn, the vast majority of scientists are confirmation machines. They cannot see beyond the current paradigm, but rather because of their belief in it, they continue to push it forward, accumulating more and more data until its natural demise. Popper thought of scientists as always keeping an open mind and never believing in anything not proven. Kuhn thought that blind faith (and the pleasure of “puzzle-solving”) keeps scientists going in what 99% of the time is a slog to do more experiments and produce more data.</p>



<p>Strevens has his own view on the philosophy of science. In particular, he pinpoints the beginning of the Scientific Revolution to a single person – Isaac Newton. To Strevens, Newton’s theory was so different from what came before because it gives precise predictions without claiming to explain the deeper reasons. Newton gave the equation for gravity without explaining why objects can act on each other at a distance. In his postscript to the Principia, Newton said,</p>



<p> <em>“I have not as yet been able to deduce from phenomena the reasons for these properties of gravity, and I do not feign hypotheses. For whatever is not deduced from the phenomena … [has] no place in experimental philosophy. … It is enough that gravity really exists and acts according to the laws that we have set forth and is sufficient to explain all the motions of the heavenly bodies and of our sea.” </em></p>



<p>In other words, Strevens sees Newton as saying, “shut up and calculate,” and indeed believes that Newton would have had no trouble with quantum mechanics’ famous “measurement problem.”  </p>



<p>Strevens sees this as key to modern science: giving up on deeper explanation and focusing only on empirical predictions. He admits scientists take other considerations, including the beauty of theories, into account in their motivations. But he claims that the “rules of engagement” are that all scientific disputes should be settled based on empirical evidence alone and within the domain of the formal scientific literature. He calls this the “Iron Rule of Explanations.” Strevens also sees Newton as science personified, in the sense in which Newton was an archetype of the modern scientific professionalism, formalism, and compartmentalization of thought. While Newton was very interested in both alchemy and biblical studies, unlike prior philosophers, he completely separated those interests from his work on physics. In that, Strevens says, Newton anticipated the modern universities, with its focus on specialization. </p>



<p>“Breaking down silos” is a common slogan these days, but Strevens argues that specialization and compartmentalization were crucial to science’s success. He says that  <em>“whatever is lost through detachment and disregard for the grand view of life is more than recompensed by the narrow, tightly focused beam that searches out the diminutive but telling fact.”</em></p>



<p><strong>Weinberg: Science history through modern eyes</strong></p>



<p>Weinberg’s is a gem of a book. Weinberg does not merely describe the works of ancient scientists, but he redoes their calculations, explaining what they got right, what they got wrong, and whether or not they could have done better with the data available. The book contains 35 technical notes, including everything from reworking Aristarchus’ derivations of the sizes and distances of the sun and the moon, through Descartes’ derivation of the law of refraction, to Newton’s calculations showing that the moon’s motion can be explained by the same gravitational force we see here on earth.</p>



<p>Weinberg places the beginning of the scientific revolution with Copernicus. While the choice of starting point might seem immaterial, this betrays a nearly opposite philosophical stance to Strevens’. While Strevens sees the reduction of science’s goal to empirical predictions as key, Copernicus’ heliocentric theory was actually a <em>step back</em> in terms of predictive power. The theory fitted the data worse than the prior geocentric theory of Ptolemy. Ptolemy introduced <em>epicycles</em> to adjust Aristotle’s clean but wrong geocentric theory to fit the data better. Today we know that epicycles correspond to the Fourier decomposition, and hence with enough of them, one can fit any periodic function. So, Ptolemy’s theory was the quintessential “shut up and calculate” theory: a good fit with observed data, but ugly and with several “fine-tuned” aspects that didn’t have any explanations. This has troubled astronomers for ages. The following words of the 12th-century Muslim scholar Ibn Rushd about Ptolemy’s theory could have been written about quantum mechanics today: <em>“The astronomical science of our days surely offers nothing from which one can derive an existing reality. The model that has been developed in the time in which we live accords with the computations, not with existence.” </em>[A side note is that quantum mechanics is very different from Ptolemy’s theory in the sense that while it does not offer a satisfactory story about reality, it is not “fine-tuned” and has its own sense of beauty, even if seeing this beauty is an “acquired taste”.]</p>



<p>Thus Copernicus’ contribution was not to find a more predictive theory, but rather a more beautiful one. As Weinberg says, this theory <em>“provides a classic example of how a theory can be selected on aesthetic criteria, with no experimental evidence that factors it over other theories… [this is] a recurrent theme in the history of physical science: a simple and beautiful theory that agrees pretty well with observation is often closer to the truth than a complicated ugly theory that agrees better with observation.”</em> [This might be a point to make a side note: while Popper, Kuhn, and (to some extent) Strevens often consider experiments as having discrete or logical outcomes – either true or false – for Weinberg quantities are always continuous or approximate, and every experiment always has a measure of uncertainty.]</p>



<p>Not surprisingly, while Strevens calls Francis Bacon’s extreme-empiricist book “one of the most significant books ever written on scientific inquiry”, Weinberg says that Bacon is one of the individuals “whose importance in the scientific revolution is most overrated” and that “it is not clear to me that anyone’s scientific work was actually changed for the better by Bacon’s writing.”</p>



<p>That is, as far as Weinberg is concerned, the novel parts of Bacon’s writing– the extreme empiricism– are false, and the true parts— plain-old experimentalism— are not novel and were known to scientists before Bacon’s time.</p>



<p>However, both Strevens and Weinberg agree on the significance of Newton. Newton’s innovation was not showing that gravity induces a uniform acceleration on objects. As Weinberg describes, already in 1603 Galileo did experiments showing that an object in free fall undergoes uniform acceleration, and showed that the distance it undergoes under gravity is proportional to the square of the time. Newton’s crucial insight was that the same relation is enough to explain the motion of the moon around the earth. He then saw that the ratio between the gravitational acceleration on earth and the one applied on the moon corresponds to the square of the distance between the two objects, and so (through Kepler’s law and his own invention of calculus) used this relation to explain the motion of all planets around the sun. </p>



<p>This was an astounding achievement. Not because Newton dared use a calculation without understanding the underlying mechanism: that was already done thousands of years before by Ptolemy and many others. Rather the impressive feat was that Newton formulated a simple law that works equally well on falling apples here on earth as well as moving stars in space. By doing this, Newton gave us the reason to hope that there are simple mathematical laws that govern all objects in the universe, and truly initiated modern science.</p>



<p><strong>Concluding thoughts</strong></p>



<p>I enjoyed reading both books and highly recommend them.  Personally, I am more inclined toward Weinberg’s view than Strevens’. I think the history of science is fascinating and can teach us a lot about science even today. The philosophy of science, whether Bacon’s, Popper’s, Kuhn’s, or Strevens’, is nice to know but is of second-order importance. That said, I think Strevens does make an essential point that the scientific “rules of engagement” may well have been key to maintaining the balance between progress and consensus that made science so successful. In particular, he believes that much of science’s success can be explained by its emphasis on formal publications, with their insistence on a “sterile language” and spelling out all details of experiments or calculations, maintained through the peer review process and generations of “reviewers 2”. This might be something to keep in mind in our era, where social media, including blogs, Twitter, and company websites, sometimes replace formal venues as the main medium for conveying scientific results. </p>



<p>My own view is that truth in science is not settled as much by debates as by momentum.  Whether the scientific consensus is X is not determined by some committee of experts, but rather by scientists, and especially students, “voting with their feet”. It is not so much a result of a debate between two distinguished scientists, but a result of a new generation of students abandoning what they think of as dead ends. (This is also known as <a href="https://en.wikipedia.org/wiki/Planck%27s_principle">Planck’s Principle</a>, sometimes grimly expressed as “science progresses one funeral at a time”.) For example,  in Computer Science, scores of papers and Ph.D. dissertations would be rendered meaningless if P=NP. This is stronger proof that the consensus is P≠NP than any poll of Turing laureates. </p>



<p>In that sense, science reminds me of the <em>blockchain</em> of bitcoin and other cryptocurrencies. The blockchain is a history of transactions of the form “User A paid X amount to user B”. User B will only be able to use this amount if there is <em>consensus</em> on this history. The blockchain is decentralized, and so several “forks” of it can exist simultaneously, but if users work on a “dead-end fork” that will not end up as part of the consensus history then their work will be for nothing. This motivates users to ensure that they are on the “right side of history” and only work on extensions of the chains that they believe will be part of the future consensus. This also means that we have more assurance of the veracity of an entry in the blockchain the further back it is, since there has been more work done that is built on this entry.</p>



<p>Similarly, scientists can be said to mine natural and artificial reality for scientific credit (not credit in the crass CV-padding or citation-counting way, but in terms of impact on the long-term direction of the field). But they will not get any such credit if their assumptions are eventually invalidated. Hence scientists are incentivized to ensure that they are always on the “right side of history”. They don’t want to work on a “dead-end fork” that will never get “merged” into the future main scientific discourse. <strong>A corollary is that if the truth of a particular proposition P is irrelevant to the way people do science then science will never need to form a consensus on whether P  is true or false. </strong>For example, as long as the choice of interpretation of quantum mechanics remains irrelevant to actual papers, debates between the Many-Worlds, Bohm and other interpretations will continue to take place in “cheap talk” outside the formal literature (whether over beer, in blog comments, or the popular press) and science will not need to form a consensus on which interpretation is correct. In particular, I don’t think science will ever need to form a consensus on whether any view of the philosophy of science, including my own, is the true one.</p>



<p><strong>Acknowledgments:</strong> Thanks to Scott Aaronson for recommending Weinberg’s book to me and Ludwig Schmidt for recommending Strevens’ book. Thanks also to Scott, Ludwig, and Preetum Nakkiran for useful comments on an earlier draft of this post.</p></div>
    </content>
    <updated>2022-05-03T13:20:22Z</updated>
    <published>2022-05-03T13:20:22Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2022-05-03T23:37:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00658</id>
    <link href="http://arxiv.org/abs/2205.00658" rel="alternate" type="text/html"/>
    <title>Sparse Fourier Transform over Lattices: A Unified Approach to Signal Reconstruction</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Zhao Song, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Baocheng.html">Baocheng Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weinstein:Omri.html">Omri Weinstein</a>, Ruizhe Zhang <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00658">PDF</a><br/><b>Abstract: </b>We revisit the classical problem of band-limited signal reconstruction -- a
variant of the *Set Query* problem -- which asks to efficiently reconstruct (a
subset of) a $d$-dimensional Fourier-sparse signal ($\|\hat{x}(t)\|_0 \leq k$),
from minimum noisy samples of $x(t)$ in the time domain. We present a unified
framework for this problem, by developing a theory of sparse Fourier transforms
over *lattices*, which can be viewed as a "semi-continuous" version of SFT,
in-between discrete and continuous domains. Using this framework, we obtain the
following results:
</p>
<p>$\bullet$ *High-dimensional Fourier sparse recovery* We present a
sample-optimal discrete Fourier Set-Query algorithm with $O(k^{\omega+1})$
reconstruction time in one dimension, independent of the signal's length ($n$)
and $\ell_\infty$-norm ($R^* \approx \|\hat{x}\|_\infty$). This complements the
state-of-art algorithm of [Kap17], whose reconstruction time is $\tilde{O}(k
\log^2 n \log R^*)$, and is limited to low-dimensions. By contrast, our
algorithm works for arbitrary $d$ dimensions, mitigating the $\exp(d)$ blowup
in decoding time to merely linear in $d$.
</p>
<p>$\bullet$ *High-accuracy Fourier interpolation* We design a polynomial-time
$(1+ \sqrt{2} +\epsilon)$-approximation algorithm for continuous Fourier
interpolation. This bypasses a barrier of all previous algorithms [PS15,
CKPS16] which only achieve $&gt;100$ approximation for this problem. Our algorithm
relies on several new ideas of independent interest in signal estimation,
including high-sensitivity frequency estimation and new error analysis with
sharper noise control.
</p>
<p>$\bullet$ *Fourier-sparse interpolation with optimal output sparsity* We give
a $k$-Fourier-sparse interpolation algorithm with optimal output signal
sparsity, improving on the approximation ratio, sample complexity and runtime
of prior works [CKPS16, CP19].
</p></div>
    </summary>
    <updated>2022-05-03T22:48:05Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00644</id>
    <link href="http://arxiv.org/abs/2205.00644" rel="alternate" type="text/html"/>
    <title>Eigenstripping, Spectral Decay, and Edge-Expansion on Posets</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gaitonde:Jason.html">Jason Gaitonde</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hopkins:Max.html">Max Hopkins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaufman:Tali.html">Tali Kaufman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lovett:Shachar.html">Shachar Lovett</a>, Ruizhe Zhang <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00644">PDF</a><br/><b>Abstract: </b>Fast mixing of random walks on hypergraphs has led to myriad breakthroughs in
theoretical computer science in the last five years. On the other hand, many
important applications (e.g. to locally testable codes, 2-2 games) rely on a
more general class of underlying structures called posets, and crucially take
advantage of non-simplicial structure. These works make it clear the global
expansion properties of posets depend strongly on their underlying architecture
(e.g. simplicial, cubical, linear algebraic), but the overall phenomenon
remains poorly understood. In this work, we quantify the advantage of different
poset architectures in both a spectral and combinatorial sense, highlighting
how regularity controls the spectral decay and edge-expansion of corresponding
random walks.
</p>
<p>We show that the spectra of walks on expanding posets (Dikstein, Dinur,
Filmus, Harsha APPROX-RANDOM 2018) concentrate in strips around a small number
of approximate eigenvalues controlled by the regularity of the underlying
poset. This gives a simple condition to identify poset architectures (e.g. the
Grassmann) that exhibit exponential decay of eigenvalues, versus architectures
like hypergraphs whose eigenvalues decay linearly -- a crucial distinction in
applications to hardness of approximation such as the recent proof of the 2-2
Games Conjecture (Khot, Minzer, Safra FOCS 2018). We show these results lead to
a tight characterization of edge-expansion on posets in the $\ell_2$-regime
(generalizing recent work of Bafna, Hopkins, Kaufman, and Lovett (SODA 2022)),
and pay special attention to the case of the Grassmann where we show our
results are tight for a natural set of sparsifications of the Grassmann graphs.
We note for clarity that our results do not recover the characterization of
expansion used in the proof of the 2-2 Games Conjecture which relies on
$\ell_\infty$ rather than $\ell_2$-structure.
</p></div>
    </summary>
    <updated>2022-05-03T22:43:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00611</id>
    <link href="http://arxiv.org/abs/2205.00611" rel="alternate" type="text/html"/>
    <title>Improved Low-Depth Set-Multilinear Circuit Lower Bounds</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kush:Deepanshu.html">Deepanshu Kush</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saraf:Shubhangi.html">Shubhangi Saraf</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00611">PDF</a><br/><b>Abstract: </b>We prove strengthened lower bounds for constant-depth set-multilinear
formulas. More precisely, we show that over any field, there is an explicit
polynomial $f$ in VNP defined over $n^2$ variables, and of degree $n$, such
that any product-depth $\Delta$ set-multilinear formula computing $f$ has size
at least $n^{\Omega \left( n^{1/\Delta}/\Delta\right)}$. The hard polynomial
$f$ comes from the class of Nisan-Wigderson (NW) design-based polynomials.
</p>
<p>Our lower bounds improve upon the recent work of Limaye, Srinivasan and
Tavenas (STOC 2022), where a lower bound of the form $(\log n)^{\Omega (\Delta
n^{1/\Delta})}$ was shown for the size of product-depth $\Delta$
set-multilinear formulas computing the iterated matrix multiplication (IMM)
polynomial of the same degree and over the same number of variables as $f$.
Moreover, our lower bounds are novel for any $\Delta\geq 2$.
</p>
<p>For general set-multilinear formulas, a lower bound of the form $
n^{\Omega(\log n)}$ was already obtained by Raz (J. ACM 2009) for the more
general model of multilinear formulas. The techniques of LST give a different
route to set-multilinear formula lower bounds, and allow them to obtain a lower
bound of the form $(\log n)^{\Omega(\log n)}$ for the size of general
set-multilinear formulas computing the IMM polynomial. Our proof techniques are
another variation on those of LST, and enable us to show an improved lower
bound (matching that of Raz) of the form $n^{\Omega(\log n)}$, albeit for the
same polynomial $f$ in VNP (the NW polynomial). As observed by LST, if the same
$n^{\Omega(\log n)}$ size lower bounds for unbounded-depth set-multilinear
formulas could be obtained for the IMM polynomial, then using the
self-reducibility of IMM and using hardness escalation results, this would
imply super-polynomial lower bounds for general algebraic formulas.
</p></div>
    </summary>
    <updated>2022-05-03T22:38:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00539</id>
    <link href="http://arxiv.org/abs/2205.00539" rel="alternate" type="text/html"/>
    <title>Enumeration Classes Defined by Circuits</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Creignou:Nadia.html">Nadia Creignou</a>, Arnaud Durand, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vollmer:Heribert.html">Heribert Vollmer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00539">PDF</a><br/><b>Abstract: </b>We refine the complexity landscape for enumeration problems by introducing
very low classes defined by using Boolean circuits as enumerators. We locate
well-known enumeration problems, e.g., from graph theory, Gray code
enumeration, and propositional satisfiability in our classes. In this way we
obtain a framework to distinguish between the complexity of different problems
known to be in $\mathbf{DelayP}$, for which a formal way of comparison was not
possible to this day.
</p></div>
    </summary>
    <updated>2022-05-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00518</id>
    <link href="http://arxiv.org/abs/2205.00518" rel="alternate" type="text/html"/>
    <title>Scheduling for Multi-Phase Parallelizable Jobs</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vaze:Rahul.html">Rahul Vaze</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00518">PDF</a><br/><b>Abstract: </b>With multiple identical unit speed servers, the online problem of scheduling
jobs that migrate between two phases, limitedly parallelizable or completely
sequential, and choosing their respective speeds to minimize the total flow
time is considered. In the limited parallelizable regime, allocating $k$
servers to a job, the speed extracted is $k^{1/\alpha}, \alpha&gt;1$, a
sub-linear, concave speedup function, while in the sequential phase, a job can
be processed by at most one server with a maximum speed of unity. A LCFS based
algorithm is proposed for scheduling jobs which always assigns equal speed to
the jobs that are in the same phase (limitedly parallelizable/sequential), and
is shown to have a constant (dependent only on $\alpha &gt; 1$) competitive ratio.
For the special case when all jobs are available beforehand, improved
competitive ratio is obtained.
</p></div>
    </summary>
    <updated>2022-05-03T22:49:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00474</id>
    <link href="http://arxiv.org/abs/2205.00474" rel="alternate" type="text/html"/>
    <title>Voting in Two-Crossing Elections</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Constantinescu:Andrei.html">Andrei Constantinescu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wattenhofer:Roger.html">Roger Wattenhofer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00474">PDF</a><br/><b>Abstract: </b>We introduce two-crossing elections as a generalization of single-crossing
elections, showing a number of new results. First, we show that two-crossing
elections can be recognized in polynomial time, by reduction to the
well-studied consecutive ones problem. We also conjecture that recognizing
$k$-crossing elections is NP-complete in general, providing evidence by
relating to a problem similar to consecutive ones proven to be hard in the
literature. Single-crossing elections exhibit a transitive majority relation,
from which many important results follow. On the other hand, we show that the
classical Debord-McGarvey theorem can still be proven two-crossing, implying
that any weighted majority tournament is inducible by a two-crossing election.
This shows that many voting rules are NP-hard under two-crossing elections,
including Kemeny and Slater. This is in contrast to the single-crossing case
and outlines an important complexity boundary between single- and two-crossing.
Subsequently, we show that for two-crossing elections the Young scores of all
candidates can be computed in polynomial time, by formulating a totally
unimodular linear program. Finally, we consider the Chamberlin-Courant rule
with arbitrary disutilities and show that a winning committee can be computed
in polynomial time, using an approach based on dynamic programming.
</p></div>
    </summary>
    <updated>2022-05-03T22:49:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00442</id>
    <link href="http://arxiv.org/abs/2205.00442" rel="alternate" type="text/html"/>
    <title>On Binary Networked Public Goods Game with Altruism</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maiti:Arnab.html">Arnab Maiti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dey:Palash.html">Palash Dey</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00442">PDF</a><br/><b>Abstract: </b>In the classical Binary Networked Public Goods (BNPG) game, a player can
either invest in a public project or decide not to invest. Based on the
decisions of all the players, each player receives a reward as per his/her
utility function. However, classical models of BNPG game do not consider
altruism, which players often exhibit and can significantly affect equilibrium
behavior. Yu et al. [24] extended the classical BNPG game to capture the
altruistic aspect of the players. We, in this paper, first study the problem of
deciding the existence of a Pure Strategy Nash Equilibrium (PSNE) in a BNPG
game with altruism. This problem is already known to be NP-complete. We
complement this hardness result by showing that the problem admits efficient
algorithms when the input network is either a tree or a complete graph. We
further study the Altruistic Network Modification problem, where the task is to
compute if a target strategy profile can be made a PSNE by adding or deleting a
few edges. This problem is also known to be NP-complete. We strengthen this
hardness result by exhibiting intractability results even for trees. A perhaps
surprising finding of our work is that the above problem remains NP-hard even
for bounded degree graphs when the altruism network is undirected, but becomes
polynomial-time solvable when the altruism network is directed. We also show
some results on computing an MSNE and some parameterized complexity results. In
summary, our results show that it is much easier to predict how the players in
a BNPG game will behave compared to how the players in a BNPG game can be made
to behave in a desirable way.
</p></div>
    </summary>
    <updated>2022-05-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00441</id>
    <link href="http://arxiv.org/abs/2205.00441" rel="alternate" type="text/html"/>
    <title>Dynamic data structures for parameterized string problems</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Jędrzej Olkowski, Michał Pilipczuk, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rychlicki:Mateusz.html">Mateusz Rychlicki</a>, Karol Węgrzycki, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zych=Pawlewicz:Anna.html">Anna Zych-Pawlewicz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00441">PDF</a><br/><b>Abstract: </b>We revisit classic string problems considered in the area of parameterized
complexity, and study them through the lens of dynamic data structures. That
is, instead of asking for a static algorithm that solves the given instance
efficiently, our goal is to design a data structure that efficiently maintains
a solution, or reports a lack thereof, upon updates in the instance.
</p>
<p>We first consider the Closest String problem, for which we design randomized
dynamic data structures with amortized update times $d^{\mathcal{O}(d)}$ and
$|\Sigma|^{\mathcal{O}(d)}$, respectively, where $\Sigma$ is the alphabet and
$d$ is the assumed bound on the maximum distance. These are obtained by
combining known static approaches to Closest String with color-coding.
</p>
<p>Next, we note that from a result of Frandsen et al.~[J. ACM'97] one can
easily infer a meta-theorem that provides dynamic data structures for
parameterized string problems with worst-case update time of the form
$\mathcal{O}(\log \log n)$, where $k$ is the parameter in question and $n$ is
the length of the string. We showcase the utility of this meta-theorem by
giving such data structures for problems Disjoint Factors and Edit Distance. We
also give explicit data structures for these problems, with worst-case update
times $\mathcal{O}(k2^{k}\log \log n)$ and $\mathcal{O}(k^2\log \log n)$,
respectively. Finally, we discuss how a lower bound methodology introduced by
Amarilli et al.~[ICALP'21] can be used to show that obtaining update time
$\mathcal{O}(f(k))$ for Disjoint Factors and Edit Distance is unlikely already
for a constant value of the parameter $k$.
</p></div>
    </summary>
    <updated>2022-05-03T23:02:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00371</id>
    <link href="http://arxiv.org/abs/2205.00371" rel="alternate" type="text/html"/>
    <title>The Johnson-Lindenstrauss Lemma for Clustering and Subspace Approximation: From Coresets to Dimension Reduction</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Charikar:Moses.html">Moses Charikar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Waingarten:Erik.html">Erik Waingarten</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00371">PDF</a><br/><b>Abstract: </b>We study the effect of Johnson-Lindenstrauss transforms in various Euclidean
optimization problems. We ask, for a particular problem and an accuracy
parameter $\epsilon \in (0, 1)$, what is the smallest target dimension $t \in
\mathbb{N}$ such that a Johnson-Lindenstrauss transform $\Pi \colon
\mathbb{R}^d \to \mathbb{R}^t$ preserves the cost of the optimal solution up to
a $(1+\epsilon)$-factor.
</p>
<p>$\bullet$ For center-based $(k,z)$-clustering, we show $t = O( (\log k + z
\log(1/\epsilon)) / \epsilon^2)$ suffices, improving on $O(z^4
\log(k/\epsilon)/\epsilon^2)$ [MMR19].
</p>
<p>$\bullet$ For $(k,z)$-subspace approximation, we show $t = \tilde{O}(zk^2 /
\epsilon^3)$ suffices. The prior best bound, of $O(k/\epsilon^2)$, only applied
to the case $z = 2$ [CEMMP15].
</p>
<p>$\bullet$ For $(k,z)$-flat approximation, we show $t =
\tilde{O}(zk^2/\epsilon^3)$ suffices, improving on a bound of $\tilde{O}(zk^2
\log n/\epsilon^3)$ [KR15].
</p>
<p>$\bullet$ For $(k,z)$-line approximation, we show $t = O((k \log \log n + z +
\log(1/\epsilon)) / \epsilon^3)$ suffices. No prior results were known.
</p>
<p>All the above results follow from one general technique: we use algorithms
for constructing coresets as an analytical tool in randomized dimensionality
reduction.
</p></div>
    </summary>
    <updated>2022-05-03T23:00:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00342</id>
    <link href="http://arxiv.org/abs/2205.00342" rel="alternate" type="text/html"/>
    <title>Fast Multivariate Multipoint Evaluation Over All Finite Fields</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhargava:Vishwas.html">Vishwas Bhargava</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Sumanta.html">Sumanta Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guo:Zeyu.html">Zeyu Guo</a>, Mrinal Kumar, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Umans:Chris.html">Chris Umans</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00342">PDF</a><br/><b>Abstract: </b>Multivariate multipoint evaluation is the problem of evaluating a
multivariate polynomial, given as a coefficient vector, simultaneously at
multiple evaluation points. In this work, we show that there exists a
deterministic algorithm for multivariate multipoint evaluation over any finite
field $\mathbb{F}$ that outputs the evaluations of an $m$-variate polynomial of
degree less than $d$ in each variable at $N$ points in time \[
(d^m+N)^{1+o(1)}\cdot\poly(m,d,\log|\mathbb{F}|) \] for all $m\in\N$ and all
sufficiently large $d\in\mathbb{N}$.
</p>
<p>A previous work of Kedlaya and Umans (FOCS 2008, SICOMP 2011) achieved the
same time complexity when the number of variables $m$ is at most $d^{o(1)}$ and
had left the problem of removing this condition as an open problem. A recent
work of Bhargava, Ghosh, Kumar and Mohapatra (STOC 2022) answered this question
when the underlying field is not \emph{too} large and has characteristic less
than $d^{o(1)}$. In this work, we remove this constraint on the number of
variables over all finite fields, thereby answering the question of Kedlaya and
Umans over all finite fields.
</p>
<p>Our algorithm relies on a non-trivial combination of ideas from three
seemingly different previously known algorithms for multivariate multipoint
evaluation, namely the algorithms of Kedlaya and Umans, that of Bj\"orklund,
Kaski and Williams (IPEC 2017, Algorithmica 2019), and that of Bhargava, Ghosh,
Kumar and Mohapatra, together with a result of Bombieri and Vinogradov from
analytic number theory about the distribution of primes in an arithmetic
progression.
</p>
<p>We also present a second algorithm for multivariate multipoint evaluation
that is completely elementary and in particular, avoids the use of the
Bombieri--Vinogradov Theorem. However, it requires a mild assumption that the
field size is bounded by an exponential-tower in $d$ of bounded
\textit{height}.
</p></div>
    </summary>
    <updated>2022-05-03T23:07:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00277</id>
    <link href="http://arxiv.org/abs/2205.00277" rel="alternate" type="text/html"/>
    <title>Chromatic $k$-Nearest Neighbor Queries</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Horst:Thijs_van_der.html">Thijs van der Horst</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/L=ouml=ffler:Maarten.html">Maarten Löffler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Staals:Frank.html">Frank Staals</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00277">PDF</a><br/><b>Abstract: </b>Let $P$ be a set of $n$ colored points. We develop efficient data structures
that store $P$ and can answer chromatic $k$-nearest neighbor ($k$-NN) queries.
Such a query consists of a query point $q$ and a number $k$, and asks for the
color that appears most frequently among the $k$ points in $P$ closest to $q$.
Answering such queries efficiently is the key to obtain fast $k$-NN
classifiers. Our main aim is to obtain query times that are independent of $k$
while using near-linear space.
</p>
<p>We show that this is possible using a combination of two data structures. The
first data structure allow us to compute a region containing exactly the
$k$-nearest neighbors of a query point $q$, and the second data structure can
then report the most frequent color in such a region. This leads to linear
space data structures with query times of $O(n^{1 / 2} \log n)$ for points in
$\mathbb{R}^1$, and with query times varying between $O(n^{2/3}\log^{2/3} n)$
and $O(n^{5/6} {\rm polylog} n)$, depending on the distance measure used, for
points in $\mathbb{R}^2$. Since these query times are still fairly large we
also consider approximations. If we are allowed to report a color that appears
at least $(1-\varepsilon)f^*$ times, where $f^*$ is the frequency of the most
frequent color, we obtain a query time of $O(\log n +
\log\log_{\frac{1}{1-\varepsilon}} n)$ in $\mathbb{R}^1$ and expected query
times ranging between $\tilde{O}(n^{1/2}\varepsilon^{-3/2})$ and
$\tilde{O}(n^{1/2}\varepsilon^{-5/2})$ in $\mathbb{R}^2$ using near-linear
space (ignoring polylogarithmic factors).
</p></div>
    </summary>
    <updated>2022-05-03T23:08:12Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00162</id>
    <link href="http://arxiv.org/abs/2205.00162" rel="alternate" type="text/html"/>
    <title>A Faster Algorithm for Betweenness Centrality Based on Adjacency Matrices</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Yelai.html">Yelai Feng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Huaixi.html">Huaixi Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lu:Hongyi.html">Hongyi Lu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00162">PDF</a><br/><b>Abstract: </b>Betweenness centrality is essential in complex network analysis; it
characterizes the importance of nodes and edges in networks. It is a crucial
problem that exactly computes the betweenness centrality in large networks
faster, which urgently needs to be solved. We propose a novel algorithm for
betweenness centrality based on the parallel computing of adjacency matrices,
which is faster than the existing algorithms for large networks. The time
complexity of the algorithm is only related to the number of nodes in the
network, not the number of edges. Experimental evidence shows that the
algorithm is effective and efficient. This novel algorithm is faster than
Brandes' algorithm on small and dense networks and offers excellent solutions
for betweenness centrality index computing on large-scale complex networks.
</p></div>
    </summary>
    <updated>2022-05-03T22:37:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00140</id>
    <link href="http://arxiv.org/abs/2205.00140" rel="alternate" type="text/html"/>
    <title>Improved Approximation to First-Best Gains-from-Trade</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fei:Yumou.html">Yumou Fei</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00140">PDF</a><br/><b>Abstract: </b>We study the two-agent single-item bilateral trade. Ideally, the trade should
happen whenever the buyer's value for the item exceeds the seller's cost.
However, the classical result of Myerson and Satterthwaite showed that no
mechanism can achieve this without violating one of the Bayesian incentive
compatibility, individual rationality and weakly balanced budget conditions.
This motivates the study of approximating the
trade-whenever-socially-beneficial mechanism, in terms of the expected
gains-from-trade. Recently, Deng, Mao, Sivan, and Wang showed that the
random-offerer mechanism achieves at least a 1/8.23 approximation. We improve
this lower bound to 1/3.15 in this paper. We also determine the exact
worst-case approximation ratio of the seller-pricing mechanism assuming the
distribution of the buyer's value satisfies the monotone hazard rate property.
</p></div>
    </summary>
    <updated>2022-05-03T23:02:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00127</id>
    <link href="http://arxiv.org/abs/2205.00127" rel="alternate" type="text/html"/>
    <title>Baba is You is Undecidable</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Geller:Jonathan.html">Jonathan Geller</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00127">PDF</a><br/><b>Abstract: </b>We establish the undecidability of 2019 puzzle game Baba is You through a
reduction from the Post correspondence problem. In particular, we consider a
restricted form of the Post correspondence problem introduced by Neary
(<a href="http://export.arxiv.org/abs/1312.6700">arXiv:1312.6700</a>) that is limited to five pairs of words. Baba is You is an
award winning tile-based game in which the player can reprogram the game's
mechanisms by pushing blocks that spell out the rules. We achieve
undecidability through a generalization of the size of the playfield in the
horizontal direction, adding a "hallway" to one side of the level. The
undecidability of Baba is You has been claimed several times online using
different source problems, including the simulation of Turing machines and
Conway's Game of Life, however, this contribution appears to be the first
formal proof of the result.
</p></div>
    </summary>
    <updated>2022-05-03T22:43:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00117</id>
    <link href="http://arxiv.org/abs/2205.00117" rel="alternate" type="text/html"/>
    <title>A Scalable 5,6-Qubit Grover's Quantum Search Algorithm</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vemula:Dinesh_Reddy.html">Dinesh Reddy Vemula</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Konar:Debanjan.html">Debanjan Konar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Satheesan:Sudeep.html">Sudeep Satheesan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kalidasu:Sri_Mounica.html">Sri Mounica Kalidasu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cangi:Attila.html">Attila Cangi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00117">PDF</a><br/><b>Abstract: </b>Recent studies have been spurred on by the promise of advanced quantum
computing technology, which has led to the development of quantum computer
simulations on classical hardware. Grover's quantum search algorithm is one of
the well-known applications of quantum computing, enabling quantum computers to
perform a database search (unsorted array) and quadratically outperform their
classical counterparts in terms of time. Given the restricted access to
database search for an oracle model (black-box), researchers have demonstrated
various implementations of Grover's circuit for two to four qubits on various
platforms. However, larger search spaces have not yet been explored. In this
paper, a scalable Quantum Grover Search algorithm is introduced and implemented
using 5-qubit and 6-qubit quantum circuits, along with a design pattern for
ease of building an Oracle for a higher order of qubits. For our
implementation, the probability of finding the correct entity is in the high
nineties. The accuracy of the proposed 5-qubit and 6-qubit circuits is
benchmarked against the state-of-the-art implementations for 3-qubit and
4-qubit. Furthermore, the reusability of the proposed quantum circuits using
subroutines is also illustrated by the opportunity for large-scale
implementation of quantum algorithms in the future.
</p></div>
    </summary>
    <updated>2022-05-03T22:48:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2205.00086</id>
    <link href="http://arxiv.org/abs/2205.00086" rel="alternate" type="text/html"/>
    <title>Enumerating Connected Dominating Sets</title>
    <feedworld_mtime>1651536000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Faisal Abu-Khzam, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fernau:Henning.html">Henning Fernau</a>, Benjamin Gras, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liedloff:Mathieu.html">Mathieu Liedloff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mann:Kevin.html">Kevin Mann</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2205.00086">PDF</a><br/><b>Abstract: </b>The question to enumerate all inclusion-minimal connected dominating sets in
a graph of order $n$ in time significantly less than $2^n$ is an open question
that was asked in many places. We answer this question affirmatively, by
providing an enumeration algorithm that runs in time $\mathcal{O}(1.9896^n)$,
using polynomial space only. The key to this result is the consideration of
this enumeration problem on 2-degenerate graphs, which is proven to be possible
in time $\mathcal{O}(1.9767^n)$. We also show new lower bound results by
constructing a family of graphs of order $n$ with $\Omega(1.4890^n)$ minimal
connected dominating sets, while previous examples achieved $\Omega(1.4422^n)$.
Our construction results in lower bounds for a few special graph classes.
</p>
<p>We also address essential questions concerning output-sensitive enumeration.
Namely, we give reasons why our algorithm cannot be turned into an enumeration
algorithm that guarantees polynomial delay without much efforts. More
precisely, we prove that it is NP-complete to decide, given a graph $G$ and a
vertex set $U$, if there exists a minimal connected dominating set $D$ with
$U\subseteq D$, even if $G$ is known to be 2-degenerate. Our reduction also
shows that even any subexponential delay is not easy to achieve for enumerating
minimal connected dominating sets. Another reduction shows that no
FPT-algorithms can be expected for this extension problem concerning minimal
connected dominating sets, parameterized by $|U|$. We also relate our
enumeration problem to the famous open Hitting Set Transversal problem, which
can be phrased in our context as the question to enumerate all minimal
dominating sets of a graph with polynomial delay by showing that a
polynomial-delay enumeration algorithm for minimal connected dominating sets
implies an affirmative algorithmic solution to the Hitting Set Transversal
problem.
</p></div>
    </summary>
    <updated>2022-05-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8314</id>
    <link href="https://windowsontheory.org/2022/05/02/brian-conrad-takes-down-the-cmf/" rel="alternate" type="text/html"/>
    <title>Brian Conrad takes down the CMF</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I’ve written before on the California Math Framework, but must admit that I have only read parts of this 900+ page document. Brian Conrad (Professor of Mathematics and Director of Undergraduate Studies at Stanford) has read the entire thing, and chased down many citations, reading the original papers. The results are not pretty. In a … <a class="more-link" href="https://windowsontheory.org/2022/05/02/brian-conrad-takes-down-the-cmf/">Continue reading <span class="screen-reader-text">Brian Conrad takes down the CMF</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I’ve <a href="https://windowsontheory.org/category/k12-math/">written before</a> on the California Math Framework, but must admit that I have only read parts of this 900+ page document. <a href="http://math.stanford.edu/~conrad/">Brian Conrad</a> (Professor of Mathematics and Director of Undergraduate Studies at Stanford) has read the entire thing, and chased down many citations, reading the original papers. The results are not pretty. In a great many cases, the cited papers do not come close to justifying the claims that the CMF makes about them; sometimes they even come to the opposite conclusions. See the full analysis on this website: </p>



<p><strong><a href="https://sites.google.com/view/publiccommentsonthecmf/" rel="nofollow">https://sites.google.com/view/publiccommentsonthecmf/</a></strong></p>



<p><strong>Unrelated announcement:</strong> The Simons Institute for Theoretical Computer Science will have a <a href="https://simons.berkeley.edu/workshops/simons-institute-10th-anniversary-symposium">10th-anniversary symposium on May 25-27</a>. The program looks awesome! I plan to be there in person and hope to catch up with many friends and colleagues I haven’t seen for a long time.</p></div>
    </content>
    <updated>2022-05-02T19:37:50Z</updated>
    <published>2022-05-02T19:37:50Z</published>
    <category term="K12-math"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2022-05-03T23:37:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/062</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/062" rel="alternate" type="text/html"/>
    <title>TR22-062 |  Superredundancy: A tool for Boolean formula minimization complexity analysis | 

	Paolo Liberatore</title>
    <summary>A superredundant clause is a clause that is redundant in the resolution closure of a formula. The converse concept of superirredundancy ensures membership of the clause in all minimal CNF formulae that are equivalent to the given one. This allows for building formulae where some clauses are fixed when minimizing size. An example are proofs of complexity hardness of the problems of minimal formula size. Others are proofs of size when forgetting variables or revising a formula. Most clauses can be made superirredundant by splitting them over a new variable.</summary>
    <updated>2022-05-02T10:53:10Z</updated>
    <published>2022-05-02T10:53:10Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-05-03T23:37:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2247671755820931798</id>
    <link href="http://blog.computationalcomplexity.org/feeds/2247671755820931798/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/05/elon-musk-to-buy-complexityblog.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/2247671755820931798" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/2247671755820931798" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/05/elon-musk-to-buy-complexityblog.html" rel="alternate" type="text/html"/>
    <title>Elon Musk To Buy Complexityblog</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Elon Musk has offered to buy out Complexityblog. The money is too good to turn down. As part of the contract we can't say how much or in what cryptocurrency, but suffice to say it will be more than the royalties from our books. Or not.</p><p>Readers be aware of the following changes to the blog:</p><p>1) We can no longer send Donald's Trump comments into our spam folder.</p><p>2) All of our posts will be written by Tesla autopilot.</p><p>3) Complete free speech in the comments. Or not. Depends on Elon's mood that day</p><p>4) Purchases made through this blog can be in bitcoin or other cryptocurrencies. Or not. Depends on Elon's mood that day.</p><p>5) Switch the business model (Lance- we have a business model?) from advertisements to a subscription service. How much will people pay per year to access complexity blog? </p><p>6) Lance and I have been invited to fly to orbit on a SpaceX rocket so we can post from space. Or was it posting from the backseat of a Tesla? Depends on how you read the contract.</p><p>7)  April Fools day posts will henceforth be on May 1. </p></div>
    </content>
    <updated>2022-05-02T00:06:00Z</updated>
    <published>2022-05-02T00:06:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-05-03T21:59:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19958</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/04/30/math-monthly/" rel="alternate" type="text/html"/>
    <title>Math Monthly</title>
    <summary>Just because we can’t find a solution, it doesn’t mean there isn’t one—Andrew Wiles. Della Dumbaugh is a professor of mathematics at the University of Richmond. Her research is in the history of algebra and number theory. She says: When Mark Warner was governor of Virginia he asked, “what do historians of mathematics study?” That […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Just because we can’t find a solution, it doesn’t mean there isn’t one—Andrew Wiles.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p>
Della Dumbaugh is a professor of mathematics at the University of Richmond. Her research is in the history of algebra and number theory. She says:</p>
<blockquote><p><b> </b> <em> When Mark Warner was governor of Virginia he asked, “what do historians of mathematics study?” That is a great question. We study mathematics, the people who create mathematics, the institutions that support mathematics, the false starts in particular areas of mathematics, the politics that influence the development of mathematics, and how the larger cultural context impinges on the development of mathematics. This discipline combines technical questions in mathematics with appropriate historical questions to yield a rich perspective on both mathematics and history. </em>
</p></blockquote>
<p/><p>
I must say that I wish I knew more about the history of math. When trying to add to the collection of theorems that are known to be true, it would definitely have helped to be more aware of their history. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/30/math-monthly/dd/" rel="attachment wp-att-19960"><img alt="" class="aligncenter size-full wp-image-19960" height="375" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/dd.jpg?resize=250%2C375&amp;ssl=1" width="250"/></a>
</p></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><i>Hidden Figures</i> article <a href="https://theconversation.com/7-lessons-from-hidden-figures-nasa-mathematician-katherine-johnsons-life-and-career-132481">source</a></font>
</td>
</tr>
</tbody></table>
<p>
Dumbaugh was <a href="https://news.richmond.edu/releases/article/-/19943/ur-mathematics-professor-della-dumbaugh-named-editor-in-chief-of-american-mathematical-monthly.html">named</a> the editor-in-chief of <em>American Mathematical Monthly</em>, starting in January 2022. The <em>Monthly</em> has been published since 1894, and its editors mostly have served five-year terms. She is only the second woman to serve as editor, a figure that seems to be low, way low. Yet she is also the second consecutive female editor. Maybe that is the beginning of a streak.</p>
<p>
</p><p/><h2> Her View of the Monthly </h2><p/>
<p/><p>
Dumbaugh found the <em>Monthly</em> in a novel way: Here is a neat <a href="http://digitaleditions.walsworthprintgroup.com/publication/?i=704334&amp;article_id=4005794&amp;view=articleBrowser&amp;ver=html5">interview</a> with her that explains it. Check out the part on her relationship to the <em>Monthly</em>:</p>
<blockquote><p><b> </b> <em> I may be one of the few members of the mathematical community who did not meet the Monthly through the content on its pages. Instead, I met the journal through my PhD research on Leonard Dickson and his work in the theory of algebras. In particular, Dickson aimed to elevate the standard of American mathematics through quality journal publications and he served as an editor of the <em>Monthly</em> from 1903 to 1906. After I finished my dissertation, I turned my attention to a question about Dickson’s celebrated <b>History of the Theory of Numbers</b> and published that article in the <em>Monthly</em>. </em>
</p></blockquote>
<p>
</p><p/><h2> My View of the Monthly </h2><p/>
<p/><p>
I found the <em>Monthly</em> in a more common way: as a high school student. I loved it from the beginning, have read it for decades, yet have never submitted to it. I still recall using my hard earned money, money from odd jobs, to pay for my subscription to the <em>Monthly</em>.</p>
<p>
I enjoyed the articles, but especially liked two features. The problem <a href="https://www.mat.uniroma2.it/~tauraso/AMM/amm.html">session</a> each issue and the reviews of other articles and books. Since this was decades before the Internet seeing these articles helped me learn about directions and areas of mathematics. </p>
<p>
I still recall seeing the 1962 <a href="https://maa.tandfonline.com/toc/uamm20/70/7?nav=tocList">Putnam</a> exam in the <em>Monthly</em>. It was the first that I ever saw. I later took the exam as an undergraduate and did poorly. Oh well. </p>
<p>
More recently, I was struck by the <a href="https://scholarship.claremont.edu/cgi/viewcontent.cgi?article=1497&amp;context=jhm">article</a>, “Calculating Intersections: The Crossroads of Mathematics and Literature in the Lives of Mother and Daughter” by a mother and daughter: Della Dumbaugh and Hannah Fenster. Read it for interesting comments about the mom and daughter relationship:</p>
<blockquote><p><b> </b> <em> This article tells the stories of how we, Della (mom, mathematician) and Hannah (daughter, writing instructor and bookseller), arrived in our professional settings through the lens of mother and daughter. For us, the intersections of literature and mathematics inspired many of our pivotal moments. </em>
</p></blockquote>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Ken recently <a href="https://rjlipton.wpcomstaging.com/2018/06/08/hilberts-irreducibility-theorem/">had</a> an <a href="https://www.tandfonline.com/doi/abs/10.1080/00029890.2018.1448181">article</a> in the <em>Monthly</em> with Bill Gasarch and also Mark Villarino, who had <a href="https://www.tandfonline.com/doi/full/10.1080/00029890.2017.1416875">another</a> that same year. What I would think of as a good paper for the <em>Monthly</em> has a mixture of history, survey elements, and originality of approach to an entertaining problem, framed to be accessible to a high-school student. What topics might our readers suggest?</p>
<p/></font></font></div>
    </content>
    <updated>2022-05-01T01:29:49Z</updated>
    <published>2022-05-01T01:29:49Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Teaching"/>
    <category term="Della Dumbaugh"/>
    <category term="editor"/>
    <category term="history of mathematics"/>
    <category term="Math Monthly"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-05-03T23:37:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/04/30/linkage</id>
    <link href="https://11011110.github.io/blog/2022/04/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Talk videos from the Discrete &amp; Computational Geometry Day in Memory of Eli Goodman and Ricky Pollack (\(\mathbb{M}\)).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://youtube.com/playlist?list=PLl18CMjwy1yZmhwbMW6THPbu8vluKDrEz">Talk videos</a> from the <a href="https://math.nyu.edu/faculty/pollack/seminar/spring22/DCGDay22.html">Discrete &amp; Computational Geometry Day in Memory of Eli Goodman and Ricky Pollack</a> <span style="white-space: nowrap;">(<a href="https://mastodon.social/@sarielhp/108141757413063563">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2022/04/17/i-would-love-to-see-examples-of-what-florida-found-unacceptable-in-those-math-textbooks.html">Commenters baffled as Florida schools ban 54 mathematics textbooks for “prohibited topics”</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108148774015958780">\(\mathbb{M}\)</a>).</span> In Florida these days this seems likely to mean any hint of the existence of non-white non-cisgender people, but the quoted press release also mentions <a href="https://en.wikipedia.org/wiki/Social%E2%80%93emotional_learning">social-emotional learning</a>, helping students develop a positive self-image during their studies and making classrooms a safe space for learning, which for some reason the US right wing wants to prevent.</p>
  </li>
  <li>
    <p><a href="https://www.caltech.edu/about/news/american-institute-of-mathematics-moves-to-caltech">American Institute of Mathematics Moves to Caltech</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108154533641166847">\(\mathbb{M}\)</a>,</span> <a href="https://www.math.columbia.edu/~woit/wordpress/?p=12775">via</a>). It was formerly on the site of a Fry’s Electronics store in San Jose, California (its funding came from John Fry) but Fry’s is now defunct, hence the move much farther south. The via link also discusses MathSciNet’s choice of a supporter of Mochizuki to <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=4225476">review Mochizuki’s abc conjecture papers</a> and failure to mention that there is any controversy with the papers. <a href="https://zbmath.org/1465.14002">zbMATH did much better</a>.</p>
  </li>
  <li>
    <p>Who was the last holdout to publish a mathematics paper in Latin <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108160261381422211">\(\mathbb{M}\)</a>)?</span> It appears to be Vadim Schechtman, whose “<a href="https://doi.org/10.1007/0-8176-4478-4_18">Definitio nova algebroidis verticiani</a>” is from 2006.</p>
  </li>
  <li>
    <p>Two more Wikipedia good articles <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108168292013780341">\(\mathbb{M}\)</a>):</span></p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/M%C3%B6bius_strip">Möbius strip</a>. This was difficult because the readership spans such a wide range: there’s a lot of popular-culture material but also some nontrivial mathematics, on topics like smooth embeddability of surfaces, uniform Riemannian geometries, and group models of Lie algebras.</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Heilbronn_triangle_problem">Heilbronn triangle problem</a>: if \(n\) points are placed in a square, how small is the smallest triangle we can be guaranteed to find?</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://www.timeshighereducation.com/opinion/open-access-closed-middle-income-countries">Gold-model open access shuts out researchers from middle-income countries like Brazil who can’t afford publication fees and can’t obtain fee waivers</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108174468963868095">\(\mathbb{M}\)</a>, </span> <a href="https://retractionwatch.com/2022/04/16/weekend-reads-white-academics-book-about-black-feminism-pulled-retraction-notices-as-a-genre-forget-the-scientific-paper/">via</a>). See also <a href="https://www.nature.com/articles/d41586-022-00864-3">original <em>Nature</em> article</a>.
Of course, diamond open access has no such issues…</p>
  </li>
  <li>
    <p>Lots of great geometric visualizations on <a href="https://twitter.com/theAlbertChern">Albert Chern’s twitter feed</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108180116083608611">\(\mathbb{M}\)</a>).</span> Here’s one: Most images of the <a href="https://en.wikipedia.org/wiki/Penrose_triangle">Penrose triangle</a>, an impossible object appearing to be made from three mutually perpendicular square beams, are drawn in an isometric perspective. But did you know that <a href="https://twitter.com/theAlbertChern/status/1392028293596028930">it can be drawn in three-point perspective</a>? (It’s also that way in Penrose’s original “<a href="https://www.iri.upc.edu/people/ros/StructuralTopology/ST17/st17-05-a2-ocr.pdf">cohomology of impossible figures</a>”.)</p>
  </li>
  <li>
    <p>This is your semi-regular reminder that to format LaTeX with automatic internally linked theorems and lemmas, sharing numbers, there is a required order <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108185458546061752">\(\mathbb{M}\)</a>):</span></p>

    <ol>
      <li>
        <p>load amsthm and hyperref</p>
      </li>
      <li>
        <p>load cleveref</p>
      </li>
      <li>
        <p>define theorems and lemmas</p>
      </li>
    </ol>

    <p>If you write LaTeX document classes that try to make things convenient by doing one of these steps out of order, you will frustrate your authors because there is no easy later fixup, and lead them to customize your class file. I’m looking at you, <a href="https://www.ryerson.ca/canadian-conference-computational-geometry-2022/submission/">cccg22.cls</a>.</p>
  </li>
  <li>
    <p><a href="https://minimalsurfaces.blog/2018/12/23/make-your-own-gyroid/">Make your own gyroid</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108191418595804372">\(\mathbb{M}\)</a>).</span> You just need to connect together lots of hexagrams and squares in the right pattern…unfortunately I don’t think any of the snap-together polygon shapes I have are hexagrams.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/elegant-six-page-proof-reveals-the-emergence-of-random-structure-20220425/"><em>Quanta</em> article on the Kahn–Kalai conjecture</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@btcprox/108196065209167912">\(\mathbb{M}\)</a>).</span> This is the principle that, for monotonic properties of random structures (like, say, Hamiltonicity of random graphs), the threshold density for the property to hold with probability \(\tfrac12\) and the threshold density for the expected number of witnesses to be \(\tfrac12\) are within a logarithmic factor of each other. See also <a href="https://arxiv.org/abs/2203.17207">preprint by Jinyoung Park and Huy Tuan Pham</a> with a proof of the conjecture.</p>
  </li>
  <li>
    <p><a href="https://sites.google.com/view/mathindatamatters/home">Data Science and the High School Math Curriculum</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108200062099969634">\(\mathbb{M}\)</a>),</span> an open letter by California academics (including me) asserting that high school algebra is essential for STEM, and data science (albeit valuable) cannot replace it. The context is a push to water down high school math, eliminate advanced tracks, and replace math by math appreciation, in the misguided hope that this will improve diversity and remove inequities. See also reactions at <a href="https://scottaaronson.blog/?p=6389">Scott Aaronson’s blog</a> and <a href="https://news.ycombinator.com/item?id=31170431">Hacker News</a>, and a <a href="https://windowsontheory.org/2022/04/27/a-personal-faq-on-the-math-education-controversies/">more detailed explainer of the context by Boaz Barak</a>.</p>
  </li>
  <li>
    <p>The news of Elon Musk’s impending buyout of Twitter and of his plans to release the trolls and propagandabots from their moderated chains led to a flood of new users on Mastodon. <a href="https://mathstodon.xyz/@11011110/108206072769622175">Here’s my welcome message and self-introduction</a>.</p>
  </li>
  <li>
    <p><a href="https://www.unesco.org/en/articles/girls-performance-mathematics-now-equal-boys-unesco-report">UNESCO reports that a mathematical advantage for boys over girls at very young ages disappears as they get older, and is even reversed in some countries</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@btcprox/108212750902862362">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="http://trademarkblog.kluweriplaw.com/2021/12/06/gomboc-3-the-final-decision/">Mathematics not trademarkable</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108219360525065087">\(\mathbb{M}\)</a>).</span> Somehow I missed this, but late last year the Supreme Court of Hungary ruled on a lawsuit asking to trademark the <a href="https://en.wikipedia.org/wiki/G%C3%B6mb%C3%B6c">Gömböc</a>, a 3D convex shape that, with only a single stable position, rights itself when placed on a flat surface. (Like a Weeble, but unweighted.) The answer was no: as “an answer given to a question of mathematics” with a shape “necessary for the Gömböc to self-right”, it could not be trademarked.</p>
  </li>
  <li>
    <p><a href="https://a3nm.net/blog/adversarial_tetris.html">Computer search proves that you can always complete at least one line in standard-size Tetris</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108222482587123946">\(\mathbb{M}\)</a>,</span> <a href="https://www.metafilter.com/195140/Beastly-AIs-known-to-let-the-piece-mmmdrop">via</a>). This is true even if the piece order is specifically chosen to make it difficult for you rather than randomly, and even if you are only allowed to rotate and drop pieces without slides or spins after they drop.</p>
  </li>
</ul></div>
    </content>
    <updated>2022-04-30T12:07:00Z</updated>
    <published>2022-04-30T12:07:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-04-30T20:00:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/061</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/061" rel="alternate" type="text/html"/>
    <title>TR22-061 |  On Approximability of Satisfiable $k$-CSPs: I | 

	Amey Bhangale, 

	Subhash Khot, 

	Dor Minzer</title>
    <summary>We consider the $P$-CSP problem for $3$-ary predicates $P$ on satisfiable instances. We show that under certain conditions on $P$ and a $(1,s)$ integrality gap instance of the $P$-CSP problem, it can be translated into a dictatorship vs. quasirandomness test with perfect completeness and soundness $s+\varepsilon$, for every constant $\varepsilon&gt;0$. Compared to Ragahvendra's result [STOC, 2008], we do not lose perfect completeness. This is particularly interesting as this test implies new hardness results on satisfiable constraint satisfaction problems, assuming the Rich 2-to-1 Games Conjecture by Braverman, Khot, and Minzer [ITCS, 2021]. Our result can be seen as the first step of a potentially long-term challenging program of characterizing optimal inapproximability of every satisfiable $k$-ary  CSP.
	
	At the heart of the reduction is our main analytical lemma for a class of $3$-ary predicates, which is a generalization of a lemma by  Mossel [Geometric and Functional Analysis, 2010]. The lemma and a further generalization of it that we propose as a hypothesis may be of independent interest.</summary>
    <updated>2022-04-30T10:35:38Z</updated>
    <published>2022-04-30T10:35:38Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-05-03T23:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=22570</id>
    <link href="https://gilkalai.wordpress.com/2022/04/29/joshua-hinman-proved-baranys-conjecture-on-face-numbers-of-polytopes-and-lei-xue-proved-a-lower-bound-conjecture-by-grunbaum/" rel="alternate" type="text/html"/>
    <title>Joshua Hinman proved Bárány’s conjecture on face numbers of polytopes, and Lei Xue proved a lower bound conjecture by Grünbaum.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Joshua Hinman proved Bárány’s conjecture. One of my first posts on this blog was a 2008 post Five Open Problems Regarding Convex Polytopes, now 14 years later, I can tell you about the first problem on the list to get solved. … <a href="https://gilkalai.wordpress.com/2022/04/29/joshua-hinman-proved-baranys-conjecture-on-face-numbers-of-polytopes-and-lei-xue-proved-a-lower-bound-conjecture-by-grunbaum/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h3>Joshua Hinman proved Bárány’s conjecture.</h3>
<p>One of my first posts on this blog was a 2008 post <a href="https://gilkalai.wordpress.com/2008/05/07/five-open-problems-regarding-convex-polytopes/" rel="bookmark">Five Open Problems Regarding Convex Polytopes</a>, now 14 years later, I can tell you about the first problem on the list to get solved.</p>
<p>Imre Bárány posed in the late 1990s the following question:</p>
<p>For a <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/>-dimensional polytope <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/> and every <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/>, <img alt="0 \le k \le d-1" class="latex" src="https://s0.wp.com/latex.php?latex=0+%5Cle+k+%5Cle+d-1&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/>,  is it true that <img alt="f_k(P) \ge \min (f_0(P),f_{d-1}(P))" class="latex" src="https://s0.wp.com/latex.php?latex=f_k%28P%29+%5Cge+%5Cmin+%28f_0%28P%29%2Cf_%7Bd-1%7D%28P%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/>?</p>
<p>Now, Joshua Hinman settled the problem! In his paper <a href="https://arxiv.org/abs/2204.02568">A Positive Answer to Bárány’s Question on Face Numbers of Polytopes</a> he actually proved even stronger linear relations. The abstract of Joshua’s paper starts with the very true assertion: “Despite a full characterization of the face vectors of simple and simplicial polytopes, the face numbers of general polytopes are poorly understood.” He moved on to describe his new inequalities:</p>
<pre><img alt="\frac{f_k(P)}{f_0(P)} \geq \frac{1}{2}\biggl[{\lceil \frac{d}{2} \rceil \choose k} + {\lfloor \frac{d}{2} \rfloor \choose k}\biggr], \qquad \frac{f_k(P)}{f_{d-1}(P)} \geq \frac{1}{2}\biggl[{\lceil \frac{d}{2} \rceil \choose d-k-1} + {\lfloor \frac{d}{2} \rfloor \choose d-k-1}\biggr]." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bf_k%28P%29%7D%7Bf_0%28P%29%7D+%5Cgeq+%5Cfrac%7B1%7D%7B2%7D%5Cbiggl%5B%7B%5Clceil+%5Cfrac%7Bd%7D%7B2%7D+%5Crceil+%5Cchoose+k%7D+%2B+%7B%5Clfloor+%5Cfrac%7Bd%7D%7B2%7D+%5Crfloor+%5Cchoose+k%7D%5Cbiggr%5D%2C+%5Cqquad+%5Cfrac%7Bf_k%28P%29%7D%7Bf_%7Bd-1%7D%28P%29%7D+%5Cgeq+%5Cfrac%7B1%7D%7B2%7D%5Cbiggl%5B%7B%5Clceil+%5Cfrac%7Bd%7D%7B2%7D+%5Crceil+%5Cchoose+d-k-1%7D+%2B+%7B%5Clfloor+%5Cfrac%7Bd%7D%7B2%7D+%5Crfloor+%5Cchoose+d-k-1%7D%5Cbiggr%5D.&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/></pre>
<h3>Lei Xue proved Grünbaum’s conjecture</h3>
<p>In her 2020 paper: <a href="https://arxiv.org/abs/2004.08429">A Proof of Grünbaum’s Lower Bound Conjecture for general polytopes</a>, <a href="http://sites.math.washington.edu/~lxue/">Lei Xue</a> proved a lower bound conjecture of Grünbaum: In 1967, Grünbaum conjectured that any <em><span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3">d</span></span></span></span></em>-dimensional polytope with <em><span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-4"><span class="mrow" id="MathJax-Span-5"><span class="mi" id="MathJax-Span-6">d</span><span class="mo" id="MathJax-Span-7">+</span><span class="mi" id="MathJax-Span-8">s</span><span class="mo" id="MathJax-Span-9">≤</span><span class="mn" id="MathJax-Span-10">2</span><span class="mi" id="MathJax-Span-11">d</span></span></span></span></em> vertices has at least</p>
<pre><img alt="\phi_k(d+s,d) = {d+1 \choose k+1 }+{d \choose k+1 }-{d+1-s \choose k+1 }" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi_k%28d%2Bs%2Cd%29+%3D+%7Bd%2B1+%5Cchoose+k%2B1+%7D%2B%7Bd+%5Cchoose+k%2B1+%7D-%7Bd%2B1-s+%5Cchoose+k%2B1+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/></pre>
<p><em><span class="MathJax" id="MathJax-Element-4-Frame"><span class="math" id="MathJax-Span-86"><span class="mrow" id="MathJax-Span-87"><span class="mi" id="MathJax-Span-88">k</span></span></span></span></em>-faces. Lei Xue proved this conjecture and also characterized the cases in which equality holds.</p>
<p><strong>Congratulations to Lei Xue and to Joshua Hinman.<br/></strong><strong><br/></strong></p>
<p/>
<p> </p>


<p/></div>
    </content>
    <updated>2022-04-29T07:11:24Z</updated>
    <published>2022-04-29T07:11:24Z</published>
    <category term="Combinatorics"/>
    <category term="Convex polytopes"/>
    <category term="Branko Grunbaum"/>
    <category term="Imre B&#xE1;r&#xE1;ny"/>
    <category term="Joshua Hinman"/>
    <category term="Lei Xue"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2022-05-03T23:37:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=619</id>
    <link href="https://tcsplus.wordpress.com/2022/04/28/tcs-talk-wednesday-may-4-vera-traub-eth-zurich/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 4 — Vera Traub, ETH Zürich</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 4th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Vera Traub from ETH Zürich will speak about “Approximation Algorithms for Connectivity Augmentation Problems” (abstract below). You can reserve a spot as an individual or a group to […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 4th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://people.math.ethz.ch/~vtraub/"><strong>Vera Traub</strong></a> from ETH Zürich will speak about “<em>Approximation Algorithms for Connectivity Augmentation Problems</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Augmentation problems are a fundamental class of network design problems. They ask about the cheapest way to increase the (edge-)connectivity of a graph by adding edges among a given set of options. One of the most elementary and intensely studied augmentation problems is the (Weighted) Tree Augmentation Problem. Here, a spanning tree has to be augmented into a 2-edge-connected graph.</p>
<p>Classic techniques for network design yield 2-approximation algorithms for a wide class of augmentation problems. For the Unweighted Tree Augmentation Problem, better-than-2 approximations are known for more than 20 years. However, only recently the first better-than-2 approximations have been found for the more general Unweighted Connectivity Augmentation Problem and Weighted Tree Augmentation Problem. In this talk we will discuss these recent advances.</p></blockquote></div>
    </content>
    <updated>2022-04-28T10:39:45Z</updated>
    <published>2022-04-28T10:39:45Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2022-05-03T23:38:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/060</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/060" rel="alternate" type="text/html"/>
    <title>TR22-060 |  How much randomness is needed to convert MA protocols to AM protocols? | 

	Nikolay Vereshchagin</title>
    <summary>The Merlin-Arthur  class of languages MA is included into Arthur-Merlin  class AM, and into PP. For a  standard transformation of a given MA protocol with  Arthur's message (= random string) of length $a$ and Merlin's message of length $m$ to a PP machine, the latter needs  $O(ma)$ random bits. The same holds for simulating MA protocols by AM protocols: in the resulting AM protocol the length of Arthur's message (= random string) is $O(ma)$. And the same holds for simulating heuristic MA protocols by  heuristic  AM protocols as well. In the paper  [A. Knop, Circuit Lower Bounds for Average-Case MA, CSR 2015] it was conjectured that, in the transformation of heuristic MA protocols to heuristic AM protocols, $O(ma)$ can be replaced by a polynomial of $a$ only. A similar question can be asked for normal MA and AM protocols, and for the simulation of MA protocols by PP machines. In the present paper we show that, relative to an oracle,  both latter questions answer in the negative and Knop's conjecture is false. Moreover,  the same is true for simulation of MA protocols by AM protocols in which the  error probability is not bounded away from 1/2, the so called PP$\cdot$NP protocols. The latter protocols generalize both AM protocols and  PP machines.</summary>
    <updated>2022-04-27T16:52:27Z</updated>
    <published>2022-04-27T16:52:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-05-03T23:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8309</id>
    <link href="https://windowsontheory.org/2022/04/27/a-personal-faq-on-the-math-education-controversies/" rel="alternate" type="text/html"/>
    <title>A personal FAQ on the math education controversies</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I was a contact person for the open letter on K-12 math education, and am in strong support of the recent letter on the role of data science in math education (and would encourage readers that are faculty members in California to sign it). Since I tend to see the same questions and objections arise … <a class="more-link" href="https://windowsontheory.org/2022/04/27/a-personal-faq-on-the-math-education-controversies/">Continue reading <span class="screen-reader-text">A personal FAQ on the math education controversies</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I was a contact person for the <a href="https://windowsontheory.org/2021/12/03/an-alarming-trend-in-k-12-math-education/">open letter on K-12 math education</a>, and am in strong support of the <a href="https://scottaaronson.blog/?p=6389">recent letter on the role of data science in math education</a> (and would encourage readers that are faculty members in California to sign it). Since I tend to see the same questions and objections arise time and again, I thought it would be useful to write my responses to these. The following is my own opinions only, and does not reflect the views of any other contact person or signer of these letters. I also apologize in advance for the lack of links and references below. However, if you ask questions in the comments, I’d be happy to give more sources.</p>



<p><strong>Q: The status quo in US math education is terrible and doesn’t work for many children, especially students of color and low income students. Aren’t these letter writers just trying to protect a flawed system that worked well for them?</strong></p>



<p><strong>A: </strong>The US math education system is very complex, but I agree that it is largely failing a large chunk of our students. But that doesn’t mean that any change will necessarily be for the better. Just because a system is bad, doesn’t mean you can’t make it worse. We have a responsibility to “first do no harm” when we experiment on millions of children of our largest state, and the proposed changes are not based on any solid evidence, and in my opinion (and the opinion of many other experts) will make things worse. Indeed, there is no shortage of experiments in mathematics education that yielded no or even negative progress in terms of equity.</p>



<p>Also, not all change needs to be curricular change. The inequalities in the system were not created because of the math curriculum, and will not be fixed by it. Many top-performing countries use a fairly traditional math curriculum, some strongly influenced by the Soviet system.  The inequalities in the US arise from huge disparities in the resources at school, and a highly unequal society at large. I personally think that improving education is much more about support for students, resources, tutoring, teacher training, etc, than whether we teach logarithms using method X or method Y.</p>



<p><strong>Q:</strong> <strong>Shouldn’t CS and STEM faculty stay out of this debate, and leave it to the math education faculty that are the true subject matter experts?</strong></p>



<p><strong>A:</strong> The education system has many stakeholders, including students, parents, teachers, citizens, employers, and post-secondary educators, and they all should be heard. STEM careers and other quantitative fields are the fastest growing in opportunities and student interest in college. As such, one of the goals (though not the only one) of K-12 math education is to prepare students to have the option to major in STEM in college, if that is what they want to do. STEM faculty are the best equipped to <a href="https://mathematics.stanford.edu/news/submission-ca-math-framework-revision">say what is needed</a> for success in their field, and what they see is the impact of K-12 preparation. Math Ed and STEM faculty can and should be working together on the  K-12 curriculum. </p>



<p><strong>Q: But not all students will go to STEM in college. Shouldn’t the K-12 math education system also offer something for the students that are not interested in STEM?</strong></p>



<p><strong>A: </strong>I agree that not all students will go to STEM and that (for example) not all high-school students should be forced to take calculus. However, if we are offering an option that is designed for students that are not interested in STEM then we should be honest about it. At the moment high-school data science courses are marketed as a way to “have your cake and eat it too” – courses that are on one hand easier than algebra and calculus and on the other hand give just as good or maybe even better preparation for a career in tech or data science. This is misinformation, and the students most likely to fall for it are the ones with the least resources. In addition, the type of thinking developed by rigorous math courses can benefit students throughout their lives and careers, regardless of the path they take.</p>



<p><strong>Q: Isn’t equity more important than giving students opportunities to advance in math? Shouldn’t mathematical education policy be focused on the kids that are struggling the most and facing most challenges, in particular students of color?</strong></p>



<p><strong>A: </strong>I personally think equity and expanding access to mathematical education is absolutely crucial. This is why I was and am involved in initiatives including <a href="https://www.addiscoder.com/">AddisCoder</a>, <a href="https://jamcoders.org.jm/">JamCoders</a>, <a href="https://womenintheory.wordpress.com/">Women In Theory</a>, and <a href="https://boazbk.github.io/tcs-summerschool/">New Horizons in TCS</a>. But true equity is about actually educating students more, not about moving the goalposts and claiming success. This is doubly true in the context of the US education system. There will be many routes open to well-resourced students to bypass any limitations of the public system. These include private tutoring, courses such as Russian School of Math, Art of Problem Solving, or simply opting out of the public system altogether. Also, due to local control, wealthier districts are likely to opt out of any reforms that they perceive (correctly) as giving worse preparation for post-secondary success. </p>



<p>Hence changes such as the CMF will disproportionately harm low-income students and students of color, and make it less likely for them to succeed in STEM. Not coincidentally, many educators, researchers, and practitioners of color have signed both letters, while the CMF itself has no Black authors. There are efforts that actually do work to decrease educational gaps: these include Bob Moses’ <a href="https://www.nytimes.com/2001/01/07/education/algebra-project-bob-moses-empowers-students.html">Algebra Project</a>, Adrian Mims’ (contact person for one of the letters) <a href="https://thecalculusproject.org/">Calculus Project</a>,  <a href="https://www.theescalanteprogram.org/">Jaime Escalante</a>  (from “stand and deliver”) math program, and the <a href="https://www.aeaweb.org/articles?id=10.1257/app.3.3.158">Harlem Children’s Zone</a>. Notably, none of these projects involve lowering the bar. </p>



<p><strong>Q: Maybe the problem is with the STEM college curriculum as well? For example, why do students need calculus for a computer science degree when hardly any software engineer uses it? If colleges would make their curriculum more practically oriented then we wouldn’t need to teach high-school kids this hard math.</strong></p>



<p><strong>A:</strong> There is a narrow answer and a deeper answer to this question. The narrow answer is that the goal of K-12 education is to prepare students for success in the world as it exists now. If you want to fight these battles at the higher education level, then you should fight them and win them, and only later change the K-12 education to fit the new college curricula.</p>



<p>However, there is a deeper answer why university education has always been about more than just giving students the minimal vocational skills. We believe that our mission is not just to give students some tools that they’ll use in the first job out of school, but broader ways of thinking that will help them keep up with new developments throughout their careers. Computer Science is a great example of this. Fifteen years ago, most computer scientists didn’t need to know much about algebra, probability or calculus, but these days deep learning is fast expanding to every area of CS. The time between an academic paper to a real-world product is getting shorter and shorter, and one skill a computer scientist needs these days is the ability to read a complex technical text and not be afraid of learning new math. Foundational courses such as college calculus and linear algebra (which themselves build on high-school Algebra II and  pre-calculus) are crucial for this skill.</p>



<p><strong>Q: Aren’t you devaluing data science and saying it’s less important than Algebra or Calculus?</strong></p>



<p><strong>A: </strong>Absolutely not. I think literacy with data is an essential skill for any citizen in our modern society, and strongly support it being taught for every K-12 student. This does not mean that it can or should replace the basic math foundational skills. Data science can and be included in variety of courses, ranging from the computational and natural sciences to the social sciences and even humanities. (For example see the courses satisfying Harvard’s <a href="https://oue.fas.harvard.edu/quantitative-reasoning-data">quantitative reasoning with data requirement</a>.)</p>



<p>“Data science” is an evolving field and at the moment not very well defined, and as such there are data science courses of vastly different types. A high-school course or module in data proficiency can be extremely beneficial for students, but without prerequisites such as algebra, probability, and programming, there is a severe limit to the depth that it can go to. For students who will not go into STEM or data science, such a data literacy course would be highly recommended. Students who will take a deeper course later on are better served by foundational courses such as Algebra, Pre-calculus, and Calculus. </p>



<p>By the way, there is nothing about data science that makes it inherently easier than algebra or calculus. While (univariate) calculus is ultimately about functions you can draw on a paper and reason intuitively about their graphs, issues of correlations vs causation are highly subtle, and even experts could get it wrong. The skills and rigorous modes of thinking developed in courses such as Algebra II and beyond are required to develop a true understanding of data science. </p>



<p>There is another reason why a prerequisite-free data literacy course should not be considered as part of the math curriculum, which was eloquently put by <a href="https://blog.mathed.page/2019/10/14/freakonomics-radio-on-math-curriculum/">Henri Picciotto</a> (see also <a href="https://blog.mathed.page/2021/12/14/yet-more-on-the-california-framework-part-1/">this</a>): <em>“in math we should not teach black-box formulas and software packages that students cannot possibly understand thoroughly. We have been moving towards teaching math for understanding at all levels. There is no reason to use data analysis as an excuse to backtrack. Let science and social studies teachers use standard deviation, correlation coefficient, regression, confidence interval, and the like without understanding the underlying assumptions and the reasoning and calculations that lead to those. Math teachers should not. “</em></p>



<p><strong>Q: Isn’t the CMF an evidence-based proposal that is backed by a huge number of citations?</strong></p>



<p><strong>A:</strong> The short answer to this question is “No”. The medium answer is that the CMF contains many citations but often the research cited is <a href="https://notepad.michaelpershan.com/more-youcubed-research-that-is-difficult-to-explain/">sloppy</a>, or is cited incorrectly. (For example, they make plenty of <a href="https://docs.google.com/document/d/1WO4gCR0fCHtlN8uN5lRztAsdqEGuRgKE/edit#bookmark=id.d8y5jr2fqobx" rel="noreferrer noopener" target="_blank">unsupported</a> <a href="https://docs.google.com/document/d/1WO4gCR0fCHtlN8uN5lRztAsdqEGuRgKE/edit#bookmark=id.biqb9dlec448" rel="noreferrer noopener" target="_blank">claims</a> on neuroscience ,whereas essentially all neuroscientists agree that our understanding of the brain is nowhere near the level that it could be used to guide curriculum development.) The long answer is out of scope for this blog post, but here are some links to analyses done by other people. I will update this blog as more are put out (this is a 900 page document after all), but some people that wrote about this include <a href="https://notepad.michaelpershan.com/what-is-happening-in-california-with-math-right-now/">Michael Pershan</a> and <a href="https://twitter.com/BethKellySF/status/1516875836573454338?s=20&amp;t=ewWPTA0FYdJBpJNDzeTjDw">Beth Kelly</a> (see also <a href="https://twitter.com/BethKellySF/status/1518991575526699008?s=20&amp;t=ewWPTA0FYdJBpJNDzeTjDw">this</a>). Brian Conrad has been working on fuller analysis of the CMF, and I will update this post (as well as <a href="https://twitter.com/boazbaraktcs">tweet</a> about it) as parts of it become available.</p></div>
    </content>
    <updated>2022-04-27T15:37:13Z</updated>
    <published>2022-04-27T15:37:13Z</published>
    <category term="K12-math"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2022-05-03T23:37:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/059</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/059" rel="alternate" type="text/html"/>
    <title>TR22-059 |  Diameter versus Certificate Complexity of Boolean Functions | 

	Siddhesh Chaubal, 

	Anna Gal</title>
    <summary>In this paper, we introduce a measure of Boolean functions we call diameter, that captures the relationship between certificate complexity and several other measures of Boolean functions. Our measure can be viewed as a variation on alternating number, but while alternating number can be exponentially larger than certificate complexity, we show that diameter is always upper bounded by certificate complexity. We argue that estimating diameter may help to get improved bounds on certificate complexity in terms of sensitivity, and other measures.

Previous results due to Lin and Zhang  imply that $s(f) \ge \Omega(n^{1/3})$ for transitive functions with constant alternating number. We improve and extend this bound and prove that $s(f) \ge \sqrt{n}$ for transitive functions with constant alternating number, as well as for transitive functions with constant diameter. We also show that $bs(f) \ge \Omega(n^{3/7})$ for transitive functions under the weaker condition that the ``minimum'' diameter is constant.

Furthermore, we prove that the log-rank conjecture holds for functions of the form $f(x \oplus y)$ for functions $f$ with diameter bounded above by a polynomial of the logarithm of the Fourier sparsity of the function $f$.</summary>
    <updated>2022-04-27T03:46:54Z</updated>
    <published>2022-04-27T03:46:54Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-05-03T23:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/058</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/058" rel="alternate" type="text/html"/>
    <title>TR22-058 |  Separations in Proof Complexity and TFNP | 

	Mika Göös, 

	Alexandros Hollender, 

	Siddhartha Jain, 

	Gilbert Maystre, 

	William Pires, 

	Robert Robere, 

	Ran Tao</title>
    <summary>It is well-known that Resolution proofs can be efficiently simulated by Sherali-Adams (SA) proofs. We show, however, that any such simulation needs to exploit huge coefficients: Resolution cannot be efficiently simulated by SA when the coefficients are written in unary. We also show that Reversible Resolution (a variant of MaxSAT Resolution) cannot be efficiently simulated by Nullstellensatz (NS).

These results can be interpreted in the language of total NP search problems. We show that PPADS, PPAD, SOPL are captured by unary-SA, unary-NS, and Reversible Resolution, respectively. Consequently, relative to an oracle, PLS $\not\subseteq$ PPADS and SOPL $\not\subseteq$ PPA.</summary>
    <updated>2022-04-26T15:47:44Z</updated>
    <published>2022-04-26T15:47:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-05-03T23:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/057</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/057" rel="alternate" type="text/html"/>
    <title>TR22-057 |  When Arthur has Neither Random Coins nor Time to Spare: Superfast Derandomization of Proof Systems | 

	Lijie Chen, 

	Roei Tell</title>
    <summary>What is the actual cost of derandomization? And can we get it for free? These questions were recently raised by Doron et. al (FOCS 2020) and have been attracting considerable interest. In this work we extend the study of these questions to the setting of *derandomizing interactive proofs systems*. 

First, we show conditional derandomization of $\mathcal{MA}$ and of $\mathcal{AM}$ with *optimal runtime overhead*, where optimality is under the  $\#NSETH$ assumption. Specifically, denote by $\mathcal{AM}\mathcal{TIME}^{[\rightleftharpoons c]}[T]$ a protocol with $c$ turns of interaction in which the verifier runs in polynomial time $T$. We prove that for every $\epsilon&gt;0$ there exists $\delta&gt;0$ such that:

1. $\mathcal{MATIME}[T]\subseteq \mathcal{NTIME}[T^{2+\epsilon}]$, and

2. $\mathcal{AM}\mathcal{TIME}^{[\rightleftharpoons c]}[T] \subseteq \mathcal{NTIME}[n\cdot T^{\lceil c/2 \rceil+\epsilon}]$,

where $(1)$ follows if there is a property $\Pi$ of Boolean functions that can be recognized from a $2^n$-length truth-table in $\mathcal{NTIME}[2^{(2+\epsilon/3)\cdot n}]$ such that functions with $\Pi$ are hard for $(\mathcal{N}\cap co\mathcal{N})\mathcal{TIME}[2^{(2-\delta)\cdot n}]/2^{(1-\delta)\cdot n}$; and $(2)$ follows if for every $k\ge1$ there is a $\Pi$ that can be recognized from a $2^n$-length truth-table in $\mathcal{NTIME}[2^{(k+\epsilon/3)\cdot n}]$ such that functions with $\Pi$ are hard for $\mathcal{MAMTIME}[2^{(1-\delta)\cdot k n}]/2^{(1-\delta)\cdot n}$.

To obtain faster derandomization, we introduce the notion of a *deterministic effective argument system*: This is a deterministic verifier $V$ such that correct claims $x\in L$ can be proved to $V$ (i.e., there is a proof $\pi$ such that $V(x,\pi)=1$), and for every probabilistic polynomial-time adversary $\tilde{P}$, the probability that $\tilde{P}$ finds an incorrect claim $x\notin L$ and a misleading proof $\pi$ such that $V(x,\pi)=1$ is negligible.

Under strong hardness assumptions, we prove that *any constant-round proof system can be compiled into a deterministic effective argument system, with essentially no time overhead*. As one corollary, under the foregoing hardness assumptions, for every $\epsilon&gt;0$ there is a deterministic verifier $V$ that gets as input an $n$-bit formula of size $2^{o(n)}$, runs in time $2^{\epsilon \cdot n}$, and satisfies the following: For every formula $\Phi$ there is a proof $\pi$ such that $V(\Phi,\pi)$ prints the number of satisfying assignments for $\Phi$; and for every adversary $\tilde{P}$ running in time $2^{O(n)}$, the probability that $\tilde{P}$ finds $\Phi$ and $\pi$ such that $V(\Phi,\pi)$ prints an incorrect count is $2^{-\omega(n)}$.</summary>
    <updated>2022-04-25T07:50:36Z</updated>
    <published>2022-04-25T07:50:36Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-05-03T23:37:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-889555349371097512</id>
    <link href="http://blog.computationalcomplexity.org/feeds/889555349371097512/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/04/the-roeder-problem-was-solved-before-i.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/889555349371097512" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/889555349371097512" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/04/the-roeder-problem-was-solved-before-i.html" rel="alternate" type="text/html"/>
    <title>The Roeder Problem was Solved Before I Posed it (how we missed it)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>(This is a joint post with David and Tomas Harris.)</p><p><br/></p><p>In my an earlier post (see <a href="https://blog.computationalcomplexity.org/2022/04/the-roeder-seq-problems-was-solved.html">here</a>) I discussed the MATH behind a problem that I worked on, with David and Tomas Harris, that we later found out had already been solved. In this post we discuss HOW this happened. </p><p>Recall that Bill Gasarch read a column of Oliver Roeder (see <a href="https://fivethirtyeight.com/features/pick-a-number-any-number/">here</a>) on Nate Silvers' blog where he challenged his readers to the following:</p><p>Find the longest sequence using numbers from {1,...,100} such that every number is either a factor or multiple of the previous number. (A later column (see <a href="https://fivethirtyeight.com/features/is-this-bathroom-occupied/">here</a> ) revealed the answer to be 77 via a computer search, which we note is not a human-readable proof.)</p><p>Bill wrote a blog post (see <a href="https://blog.computationalcomplexity.org/2017/09/a-problem-i-thought-was-interesting-now.html">here</a>) and an open problems column (see <a href="https://www.cs.umd.edu/~gasarch/open/nate.pdf">here</a> ) asking about the general case of {1,...,n}. Before doing this Bill DID try to check the literature to see what was known, but he didn't check very hard since this was not going to be a published paper. Also, he vaguely thought that if it was a known problem then one of his readers would tell him.</p><p>QUESTION: Is it appropriate to blog on things that you have not done a search of the literature on?</p><p>ANSWER: Yes, but you should SAY SO in the blog post.</p><p>As measured by comments, the post did not generate much interest- 10 comments. 2 were me (Gasarch) responding to comments.</p><p>David (who has a PhD from UMCP under Aravind Srinivasan) asked Bill to find a HS project for his son Tomas. Bill gave Tomas the sequence problem (as he called it) to look at- perhaps write a program to find what happens for {1,...,n} for small n, perhaps find human-readable proofs of weaker bound, for small n or for n=100.</p><p>David got interested in the MATH behind the problem so the project became three projects: Tomas would look at the programing aspects and the human-readable aspects, David would look at the Math, and Bill would...  hmmm, not clear what Bill would do, but he did write up a great deal of it and cleaned up some of the proofs.</p><p>David showed</p><p>Omega( n/( (log n)^{1.68} )  LE  L(n)  LE  O( n/( (log n)^{0.79} ). </p><p>Tomas and Bill obtained a human-readable proof that L(100) LE 83. (Comments on my blog sketched a proof that L(100) LE 83, and someone else that L(100) LE 80). See my previous post (<a href="https://blog.computationalcomplexity.org/2022/04/the-roeder-seq-problems-was-solved.html">here</a>) for more on the known numbers for L. </p><p>At that point David did a brief literature search; however, he didn't know what to look for.</p><p>BILL still thought of this as a HS project so he didn't think much about a paper coming out of it, or if it was original. So he didn't do the due diligence of seeing what was already known.</p><p>David and Tomas were busy working on it, so they only did a few cursory checks of the literature.</p><div><div><br/></div><div>With the two results above,  we had a paper! David then looked much more carefully at the literature. He DID find some earlier papers -- he did a Google search for Roeder's puzzle, which mentioned another mathematician, who was quoted in a blog by another mathematician, who eventually mentioned Pomerance's old paper on the topic. Once he found a reference to an actual math paper it was easy to use Google Scholar to find forward/backward citations and find the current state of the art.</div><div><br/></div><div>His email had subject title</div><div><br/></div><div>                        SHUT IT ALL DOWN!!!</div><div><br/></div><div>Which made Bill think it involved a nuclear reactor undergoing The China Syndrome rather than just telling us that other people did had better and earlier results. </div><div><br/></div><div><p class="MsoNormal">In 1995 Gerald Tenenbaum showed, in a paper written in French,  that there exists a,b such that </p><p class="MsoNormal">                               n/(log n)^a LE L(n) LE n/(log n)^b (see <a href="http://www.numdam.org/item/ASENS_1995_4_28_2_115_0/">here</a>). </p><p class="MsoNormal">More recently, in 2021, Saias showed, in a paper written in French, that </p><p class="MsoNormal">                                      L(n) GE (0.3 - o(1)) n/log n (see <a href="https://arxiv.org/abs/2107.03855">here</a>). </p></div><div><br/></div><div>SO, why didn't Bill, David, Tomas find that it was already known until late in the process:</div><div><br/></div><div>1) They didn't know the right search term: <i>Divisor Graph</i></div><div><br/></div><div>2) The literature was in French so the right search term is <i>graphe divisoriel</i></div><div><br/></div><div>3) The transition from FUN HS PROJECT to SERIOUS MATH PAPER was somewhat abrupt and caught Bill by surprise.</div><div><br/></div><div>Was this a disappointment?</div><div><br/></div><div>1) We all learned some math from it, so that was nice.</div><div><br/></div><div>2) We were in a position to read and understand the paper since we knew all of the difficulties --- however, it was in French which I do not read. David reads some, Tomas does not read French.  I prefer to be scooped in English, but even then  I might not be able to read up on the problem since  math is... hard. When did math get so hard? see my blog on that <a href="https://blog.computationalcomplexity.org/2019/07/when-did-math-get-so-hard.html">here</a>. When did CS theory get so hard? See my blog on that <a href="https://blog.computationalcomplexity.org/2021/11/when-did-computer-science-theory-get-so.html">here</a>.)</div><div><br/></div><div>Could this happen again?</div><div><br/></div><div>1) Yes. Language barriers are hard to overcome. Though this is rare nowadays--- not much serious mathematics seems to be done outside English. French mathematicians seem to like to keep their language alive, although they probably know English as well. There may be a few other countries (China, perhaps), where English language skills are not advanced and researchers are cut off from the English literature.</div><div><br/></div><div>2) Yes. I've heard of cases where many people discovered the same theorem but were unaware of each others results since they were in different fields.</div><div><br/></div><div>3) Is it easier or harder to reprove a theorem now then it was X years ago?</div><div><br/></div></div><div>We have better search tools, but we also have more to search. </div><div><br/></div></div>
    </content>
    <updated>2022-04-25T01:19:00Z</updated>
    <published>2022-04-25T01:19:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-05-03T21:59:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/056</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/056" rel="alternate" type="text/html"/>
    <title>TR22-056 |  Optimal Coding Theorems in Time-Bounded Kolmogorov Complexity | 

	Zhenjian Lu, 

	Igor Carboni Oliveira, 

	Marius Zimand</title>
    <summary>The classical coding theorem in Kolmogorov complexity states that if an $n$-bit string $x$ is sampled with probability $\delta$ by an algorithm with prefix-free domain then K$(x) \leq \log(1/\delta) + O(1)$. In a recent work, Lu and Oliveira [LO21] established an unconditional time-bounded version of this result, by showing that if $x$ can be efficiently sampled with probability $\delta$ then rKt$(x) = O(\log(1/\delta)) + O(\log n)$, where rKt denotes the randomized analogue of Levin's Kt complexity. Unfortunately, this result is often insufficient when transferring applications of the classical coding theorem to the time-bounded setting, as it  achieves a  $O(\log(1/\delta))$ bound instead of the information-theoretic optimal $\log(1/\delta)$.

Motivated by this discrepancy, we investigate optimal coding theorems in the time-bounded setting. Our main contributions can be summarised as follows.

Efficient coding theorem for rKt with a factor of $2$. Addressing a question from [LO21], we show that if $x$ can be efficiently sampled with probability at least $\delta$ then rKt$(x) \le (2 + o(1)) \cdot \log(1/\delta) + O\!\left(\log n\right)$. As in previous work, our coding theorem is efficient in the sense that it provides a polynomial-time probabilistic algorithm that, when given $x$, the code of the sampler, and $\delta$,  it outputs, with  probability $\ge 0.99$, a probabilistic representation of $x$ that certifies this rKt complexity bound.

Optimality under a cryptographic assumption. Under a hypothesis about the security of cryptographic pseudorandom generators, we show that no efficient coding theorem can achieve a bound of the form rKt$(x) \leq (2 - o(1)) \cdot \log(1/\delta) +$ poly$(\log n)$. Under a weaker assumption, we exhibit a gap between efficient coding theorems and existential coding theorems with near-optimal parameters.

Optimal coding theorem for pK$^t$ and unconditional Antunes-Fortnow. We consider pK$^t$ complexity [GKLO22], a variant of rKt where the randomness is public and the time bound is fixed. We observe the existence of an optimal coding theorem for pK$^t$, and employ this result to establish an unconditional version of a theorem of Antunes and Fortnow [AF09] which characterizes the worst-case running times of languages that are in average polynomial-time over all P-samplable distributions.</summary>
    <updated>2022-04-24T20:25:42Z</updated>
    <published>2022-04-24T20:25:42Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-05-03T23:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19926</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/04/22/a-paradox/" rel="alternate" type="text/html"/>
    <title>A Paradox</title>
    <summary>If you can’t solve a problem, it’s because you’re playing by the rules—Paul Arden Dietrich Braess is a professor of mathematics at Ruhr University in Bochum, Germany. He is famous for the discovery of a paradox. When he was doing research on traffic modeling in 1968, he discovered that the flow in a road network […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>If you can’t solve a problem, it’s because you’re playing by the rules—Paul Arden</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2022/04/22/a-paradox/db/" rel="attachment wp-att-19928"><img alt="" class="alignright wp-image-19928" height="165" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/db.jpg?resize=220%2C165&amp;ssl=1" width="220"/></a></p>
<p>
Dietrich Braess is a professor of mathematics at Ruhr University in Bochum, Germany.  He is famous for the discovery of a paradox. When he was doing research on traffic modeling in 1968, he discovered that the flow in a road network could be <i>hindered</i> by <i>adding</i> a new road.</p>
<p>
Today we note a connection between him and the new ACM Athena Award winner, Éva Tardos, whom we congratulate.<br/>
<span id="more-19926"/></p>
<p>
The paradox has a paradox riding on it.  The original is about the game theory of elective actions by drivers individually trying to optimize their routes. Yet it shows up in purely physical dynamics.  Consider the following <a href="http://www.davros.org/science/roadparadox.html">network</a> of springs and strings:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/22/a-paradox/par/" rel="attachment wp-att-19929"><img alt="" class="aligncenter wp-image-19929" height="192" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/par.png?resize=100%2C192&amp;ssl=1" width="100"/></a></p>
<p>
Quoting Clive Feathers, who <a href="http://www.davros.org/">studies</a> Britain’s railways and the London Underground:</p>
<blockquote><p><b> </b> <em> If all the strings are taut, then each of them will be carrying one-third of the weight of the block. So if the purple string is cut, the remaining strings will carry more of the weight and the block will drop down a bit as the springs take up the load, right?</em></p><em>
<p>
Wrong!</p>
</em><p><em>
As shown, each string has a tension of 4N, so the springs will each be under a tension of 8N and will stretch accordingly. Now, when the purple string is cut the remaining strings have a tension of 6N, and so does each spring. So there is less tension in the springs, and they will contract accordingly, making the block rise. </em>
</p></blockquote>
<p>
</p><p/><h2> The Braess Paradox </h2><p/>
<p/><p>
Who cares about springs and strings? Indeed. </p>
<p>
The point is that this paradox arises in problems we study in CS. It is named now as <a href="https://en.wikipedia.org/wiki/Braess's_paradox">Braess’s Paradox</a>. Springs and strings can be changed to edges. Edges in turn relate to networks of many kinds; the following figure comes from a <a href="https://iopscience.iop.org/article/10.1088/1367-2630/aad490">paper</a> on power grids:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/22/a-paradox/smile/" rel="attachment wp-att-19931"><img alt="" class="aligncenter wp-image-19931" height="282" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/smile.jpg?resize=500%2C282&amp;ssl=1" width="500"/></a></p>
<p>
The original paradox is that adding one or more roads to a road network can slow down overall traffic flow through it. The new road initially makes its use locally optimal for everyone until a saturation point <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is reached. The limit on choices before and after using the new road, however, also makes the <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> usage increase the cost of the previously optimal strategies. This evolves a worse configuration for everyone that is not locally improvable. </p>
<p>
The flip side is that a clog-prone network can sometimes be improved by removing certain parts of it. This paradox has been used to explain instances of improved traffic flow when existing major roads are closed. Here are two examples:</p>
<ul>
<li>
<a href="https://resources.mpi-inf.mpg.de/departments/d1/teaching/ws12/ct/Braess-paradox.pdf">In Stuttgart, Germany</a> after investments into the road network in 1969, the traffic situation did not improve until a section of newly-built road was closed for traffic again.<p/>
<p/></li><li>
<a href="https://www.nytimes.com/1990/12/25/health/what-if-they-closed-42d-street-and-nobody-noticed.html">ON Earth Day</a> this year (1990), New York City’s Transportation Commissioner decided to close 42d Street, which as every New Yorker knows is always congested. “Many predicted it would be doomsday,” said the Commissioner, Lucius J. Riccio. “You didn’t need to be a rocket scientist or have a sophisticated computer queuing model to see that this could have been a major problem.”<p/>
<p>
But to everyone’s surprise, Earth Day generated no historic traffic jam. Traffic flow actually improved when 42d Street was closed.</p>
</li></ul>
<p>
</p><p/><h2> Today’s News and the Paradox </h2><p/>
<p/><p>
This started as a post on Éva Tardos, who is a professor at Cornell University. She just was named the 2022-2023 ACM <a href="https://awards.acm.org/athena">Athena Lecturer</a>:</p>
<blockquote><p><b> </b> <em> This award celebrates women researchers who have made fundamental contributions to Computer Science. Each year ACM honors a preeminent woman computer scientist as the Athena Lecturer. The recipient gives an invited talk at a major ACM conference of her choice. </em>
</p></blockquote>
<p/><p>
What is the connection between Tardos as Athena Lecturer and the Braess Paradox? Indeed, the citation notes her contributions to algorithmic game theory. These are game-theory situations in her purview.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/22/a-paradox/et-3/" rel="attachment wp-att-19933"><img alt="" class="aligncenter wp-image-19933" height="200" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/et.jpg?resize=300%2C200&amp;ssl=1" width="300"/></a></p>
<p>
She has many papers that bound approximations to optimal solutions. Often the answers are likely to be hard to solve exactly—they are NP-complete for example. But she has some wonderful papers that study questions where paradoxes apply regardless of whether <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is true.</p>
<p>
One is the <a href="http://www.timroughgarden.org/papers/mcbp.pdf">paper</a> by Henry Lin, Tim Roughgarden, Eva Tardos, and Asher Walkover, and is titled, “Stronger Bounds On Braess’s Paradox And The Maximum Latency Of Selfish Routing.” It addresses the flip-side paradox: how much can the performance of a network be improved by removing one edge? One might expect that removing an edge might be less likely to improve the smallest networks, as well as less impactful on larger networks, so that the highest improvement would come in some mid-size example. In fact, they show that Braess’s original example of four nodes and five edges has the highest improvement. </p>
<p>
They also show that the problem of finding the maximum improvement possible within a sub-network of a large network is <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-hard to approximate, even sub-exponentially. We wonder how this connects to an earlier <a href="https://mathweb.ucsd.edu/~fan/wp/braess_rsa.pdf">paper</a> by Fan Chung, Stephen Young, and Wenbo Zhao showing that expander graphs tend to have many subgraphs exhibiting the Braess paradox. This is surprising because expanders by-and-large promote free network flows.</p>
<p/><p>
We add that Tardos is just the third Athena Lecturer from the theory area since the first award in 2006: Shafi Goldwasser in 2008 and Nancy Lynch in 2012. Congrats, Éva. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I did not know about the Braess Paradox before reading about her award; Ken thinks he saw it a decade or so ago. Previously my favorite result of Tardos was in the <a href="https://www.cs.cornell.edu/~eva/Gap.Between.Monotone.NonMonotone.Circuit.Complexity.is.Exponential.pdf">paper</a>, “The Gap Between Monotone And Non-Monotone Circuit Complexity Is Exponential.” Now not so clear.</p>
<p>
What is your favorite of her results?</p>
<p/></font></font></div>
    </content>
    <updated>2022-04-22T04:48:21Z</updated>
    <published>2022-04-22T04:48:21Z</published>
    <category term="algorithms"/>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="People"/>
    <category term="ACM"/>
    <category term="approximation"/>
    <category term="Athena Award"/>
    <category term="Dietrich Braess"/>
    <category term="Eva Tardos"/>
    <category term="graphs"/>
    <category term="networks"/>
    <category term="paradox"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-05-03T23:37:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/missingness/</id>
    <link href="https://gradientscience.org/missingness/" rel="alternate" type="text/html"/>
    <title>Missingness Bias in Model Debugging</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><!-- <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> -->
<!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"> -->









<!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css"> -->
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script> -->
<!-- <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script> -->


<!-- <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css"> -->
<!-- chart.js -->


<p><a class="bbutton" href="https://arxiv.org/abs/2204.08945" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/missingness" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<br/></p>

<p><i>
“Missingness”, or the absence of features from an input, is a concept that is fundamental to many model debugging tools. In our <a href="https://arxiv.org/abs/2204.08945">latest paper</a>, we examine the challenges of implementing missingness in computer vision. In particular, we demonstrate how current approximations of missingness introduce biases into the debugging process of computer vision models. We then show how a natural implementation of missingness based on VITs can mitigate these biases and lead to more reliable model debugging.
</i></p>

<p>Deep learning models can learn powerful features. However, these learned features can be unintuitive or spurious. Indeed, recent studies have pointed out that ML models often leverage unexpected—and, in fact, undesirable—associations. These include image pathology detection models that rely on <a href="https://cerre.eu/wp-content/uploads/2020/07/ai_explainability_whitepaper_google.pdf">pen marks made by radiologist</a> and image classifiers that focus too much on <a href="https://arxiv.org/abs/2006.09994">backgrounds</a> or on <a href="https://arxiv.org/abs/1811.12231">texture</a>. Such prediction mechanisms can cause models to fail in downstream tasks or new environments.</p>

<p>How can we detect model failures caused by relying on such brittle or undesirable associations? Answering this question is a major goal of model debugging. Work in this context brought forth techniques that allow, for example, surfacing <a href="https://arxiv.org/abs/1704.05796">human aligned</a> <a href="https://gradientscience.org/glm_saga/">concepts</a>, <a href="https://gradientscience.org/3db/">probing specific types of bias a model uses</a>, or highlighting features that were <a href="https://arxiv.org/abs/1904.07451">important for a</a> <a href="https://arxiv.org/abs/1602.04938">specific prediction</a>.</p>

<p>A common theme in many of these debugging methods is to study model behavior on so-called counterfactual inputs, i.e., inputs with and without specific features. For example, consider the image of a dog being held by its owner below. By removing the owner from the image, we can study how much our model’s prediction depends on the presence of a human. In a similar vein, we can remove parts of the dog (head, body, paws) to identify which ones among them are most critical for correctly classifying the image. This concept of “the absence of a feature” from the input is sometimes referred to as <a href="https://distill.pub/2020/attribution-baselines/">missingness</a>.</p>

<p><img alt="Woman-Dog Example" src="https://gradientscience.org/assets/missingness/woman_dog_example.png" style="width: 100%;"/></p>
<div class="footnote">
        By removing parts of the image, we can study how much our model depends on a given input feature (such as the human, or the dog's snout).  
</div>

<p>Indeed, this primitive of missingness is used quite a lot in model debugging techniques. For example, widely-used methods such as <a href="https://arxiv.org/abs/1602.04938">LIME</a> and <a href="https://arxiv.org/abs/1703.01365">integrated gradients</a> leverage it. It also has been applied to <a href="https://arxiv.org/abs/1802.07623">radiology images</a> to understand the regions of the scan that are important for diagnoses, or to <a href="https://arxiv.org/abs/2110.04301">flag spurious correlations</a>. Finally, in natural language processing, model designers often remove words <a href="https://arxiv.org/abs/2010.12487">to understand their impact on the output</a>.</p>

<p><img alt="Example of LIME" src="https://gradientscience.org/assets/missingness/main_lime.png" style="width: 100%;"/></p>
<div class="footnote">
LIME, a popular model debugging method, relies on the  missingness primitive to generate model explanations. The method first generates a set of ablations by randomly removing parts of the image. These ablations are then used to generate a model explanation, typically obtained by fitting a sparse linear model to them. 
</div>

<h2 id="challenges-of-implementing-missingness-in-computer-vision">Challenges of implementing missingness in computer vision</h2>

<p>Missingness is a rather intuitive notion: we simply would like the model to predict as if the corresponding part of the input didn’t exist. Also, in a domain such as NLP implementing this primitive is fairly straightforward: we simply drop the corresponding words from the sentence. However, in the context of computer vision, its proper implementation turns out to be much more challenging. This is because images are spatially contiguous objects: it is unclear how to leave a “hole” in the image.</p>

<p>Researchers thus have come up with all kinds of ways to “fill” such hole, i.e., approximating missingness by replacing the region with other pixels. This involves <a href="https://arxiv.org/pdf/1703.01365.pdf">blacking out the pixels</a>, replacing them with <a href="https://arxiv.org/pdf/1706.03825.pdf">random noise</a>, or <a href="https://arxiv.org/pdf/1704.03296.pdf">blurring the corresponding image region</a>. However, these approaches, even though very natural, turn out to have unintended effects. For example, researchers found that <a href="https://distill.pub/2020/attribution-baselines/">saliency maps generated with integrated gradients are quite sensitive to the chosen baseline color of the filling</a>, and thus can change significantly based on the (arbitrary) choice of that missingness approximation.</p>

<p><img alt="Woman-Dog Example" src="https://gradientscience.org/assets/missingness/various_approx_original.png" style="width: 100%;"/></p>
<div class="footnote">
    Practitioners often approximate missingness in computer vision by replacing the removed region with other types of pixels. Examples include blacking out, using the mean pixel color, filling with random noise, or blurring the image.
</div>

<!-- What impact do missingness approximations have—especially with respect to model debugging? In our [recent work][arxiv], we systematically investigate how approximations like blacking out pixels can create bias, where the model uses masked out regions to make predictions rather than ignoring them. Moreover, we find that this _missingness bias_ can hinder our ability to debug models. We then show how transformer-based architectures can enable a more natural implementation of missingness, allowing us to mitigate this bias and thus improve model debugging.  -->

<h2 id="missingness-approximations-can-create-bias">Missingness approximations can create bias</h2>

<p>So, what impact do such missingness approximations actually have on the resulting model predictions? In our <a href="https://arxiv.org/abs/2204.08945">recent work</a>, we systematically investigate the bias that these approximations can imbue. Specifically, <i>we find that models end up using masked out regions to make predictions rather than simply ignoring them.</i></p>

<p>For example, take a look below at the image of a spider from ImageNet, where we have removed various regions from the image by blacking out the corresponding pixels.</p>

<div id="bias_examples_widget" style="overflow: auto; text-align: center;"/>
<div class="footnote">
    Various regions of an image are removed by blacking out pixels. Regardless of what regions from the image are masked, a ResNet-50 outputs a wrong class. 
</div>

<p>It turns out that irrespective of what subregions of the image are removed, a standard CNN (e.g., ResNet-50) outputs incorrect classes, even when most of the foreground object is not obscured. In fact, taking a closer look at the randomly masked images, we find that the model seems to be relying on the masking pattern itself to make the prediction (e.g., predicting class “crossword”).</p>

<p>To analyze this bias more quantitatively, we measure how missingness approximations impact the output class distribution of a Resnet-50 classifier (over the whole dataset).  Specifically, we iteratively black out parts of ImageNet images, and keep track of how the probability of predicting any one class changes. Before blacking out any pixels, the model predicts a roughly equal number of each class on the ImageNet test set (which is what one would expect given that this test set is class balanced). However, when we apply missingness approximations, this distribution skews heavily toward classes such as maze, crossword puzzle, and carton.</p>

<div id="anno"> 
    <canvas height="15%" id="resnet_bar_chart" width="30%"/>
</div>
<div class="footnote">
    We measure the shift in output class distribution after applying a (blacking out) missingness approximations. As patches are increasingly blacked out, the ResNet-50 classifier's predictions skew from a uniform distribution toward a few specific classes such as maze, crossword puzzle, and carton.
</div>

<p>Our <a href="https://arxiv.org/abs/2204.08945">paper</a> investigates this missingness bias in more depth by exploring different approximations, datasets, mask sizes, etc.</p>

<h2 id="missingness-bias-in-model-debugging-a-case-study-on-lime">Missingness bias in model debugging: A case study on LIME</h2>
<p>As we mentioned earlier, missingness primitive is often used by model debugging techniques—how does the above-mentioned bias impact them? To answer this question, we focus on a popular feature attribution method: local interpretable model-agnostic explanations, or <a href="https://arxiv.org/abs/1602.04938">LIME</a>.</p>

<div id="lime_examples_widget" style="overflow: auto; text-align: center;"/>
<div class="footnote">
    Examples of generated LIME explanations represented as a heat map. High-intensity regions contribute more to the prediction of the model than low-intensity ones. On the right, the top 20 image regions identified by the LIME explanation are masked in black.
</div>

<p>The image above depicts an example LIME explanation generated for a ResNet-50 classifier. Specifically, for a given image, LIME generates a heatmap highlighting the regions of the image based on how much they impact prediction (according to LIME). While sometimes LIME highlights the foreground object (which is what one would expect), in most cases it also highlights rather irrelevant regions scattered all over the image. Why is this the case?</p>

<p>Although we can’t say for sure, we suspect that this behavior is caused in large part by the fact that the underlying ResNet-50 classifier relies on masked regions in the image to make predictions, and this tricks it into believing that these regions are important. In <a href="https://arxiv.org/abs/2204.08945">our paper</a>, we perform a more detailed quantitative analysis of the missingness bias in LIME explanations, and find that it indeed makes them more inconsistent and, during evaluation, indistinguishable from random explanations.</p>

<h2 id="a-more-natural-implementation-of-missingness">A more natural implementation of missingness</h2>
<p>What is the right way to represent missing pixels then? Ideally, since replacing pixels with other pixels can lead to missingness bias, we would like to be able to remove these regions altogether. But what about the need of having the images be represented by contiguous inputs?</p>

<p>Certainly, convolutional neural networks (CNNs) require such spatial contiguity because convolutions slide filters across the image. But do we even need to use CNNs? Not really!</p>

<p>How about we turn to a different architecture:  <a href="https://arxiv.org/abs/2010.11929">vision transformers</a> (ViTs)? Unlike CNNs, ViTs do not use convolutions and operate instead on sets of patches that correspond to positionally encoded regions of the image. Specifically, a ViT has two stages when processing an input image:</p>

<ul>
  <li><strong>Tokenization</strong>: split the image into square patches, and positionally encode these patches into tokens.</li>
  <li><strong>Self-attention</strong>: pass the set of tokens through several self-attention layers.</li>
</ul>

<p>After tokenization, the self-attention layers operate on the set of tokens rather than the entire image. Thus, ViTs enables a far more natural implementation of missingness: <i>we can simply drop the tokens that encode the region we want to remove!</i></p>

<p><img alt="Woman-Dog Example" src="https://gradientscience.org/assets/missingness/vit_dropping.png" style="width: 100%;"/></p>
<div class="footnote">
    ViTs split the image into a set of tokens, where each token represents a patch in the image. In order to implement missingness, we can simply drop the tokens corresponding to the regions we would like to remove.
</div>

<h2 id="mitigating-missingness-bias-through-vits">Mitigating missingness bias through ViTs</h2>

<p>As we now demonstrate,  using this more natural implementation of missingness turns out to substantially mitigate the missingness bias that we saw earlier. Indeed, recall that, with a ResNet-50, blacking out pixels biased the model toward specific classes (such as crossword). But, a similarly sized ViT-small (ViT-S) architecture is able to side-step this bias (when we implement token dropping), maintaining a correct (or at least related) prediction.</p>

<div id="bias_examples_widget2" style="overflow: auto; text-align: center;"/>
<div class="footnote">
   When we remove image regions by dropping tokens with a ViT-S classifier, the model maintains its original prediction or, at least, predicts a related class.
</div>

<p>This difference is even more striking when we examine the output class distribution corresponding to removal of image regions: while for our ResNet-50 classifier we have seen this distribution being skewed toward certain classes, for the ViT classifier, this distribution remains close to uniform.</p>

<div id="anno"> 
    <canvas height="15%" id="vit_bar_chart" width="30%"/>
</div>
<div class="footnote">
The shift in output class distribution after dropping tokens using a ViT architecture. In contrast to the ResNet-50 classifier, ViT largely maintains its original uniform class distribution.
</div>

<h2 id="improving-model-debugging-with-vits">Improving model debugging with ViTs</h2>

<p>So, we have seen that ViTs allow us to mitigate missingness bias. Can we thus use ViTs to improve model debugging too? We return to our case study of LIME. As we demonstrated earlier, LIME relies on a notion of missingness, and is thus vulnerable to missingness bias when using approximations like blacking out pixels. What happens when we use ViTs and token dropping instead?</p>

<div id="lime_examples_widget2" style="overflow: auto; text-align: center;"/>
<div class="footnote">
Examples of generated LIME explanations for a ResNet (blacking out pixels) and a ViT (dropping tokens). 
</div>

<p>The figure above displays examples of the LIME explanations generated for a ResNet classifier and for a ViT classifier (dropping tokens). Qualitatively, one can see that the explanations for the ViT seem more aligned with human intuition, highlighting the main object instead of regions in the background. We confirm these observations with a more quantitative analysis in <a href="https://arxiv.org/abs/2204.08945">our paper</a>.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this post, we studied how missingness approximations can lead to biases and, in turn, impact the model debugging techniques that leverage them. We also demonstrated how transformer-based architectures can enable a seamless implementation of missingness that allows us to side-step missingness bias and lead to more reliable model debugging.</p>

<!-- <br><br><br><br> -->
<!-- <div id="anno"> 
  <canvas id="resnet_bar_chart" width="100%" height="35%"></canvas>
  </div>
  <div class="footnote"> Some Caption
</div>

<div id="anno"> 
  <canvas id="vit_bar_chart" width="100%" height="35%"></canvas>
  </div>
  <div class="footnote"> Some Caption
</div> -->

<!-- <div id="anno"> 
  <canvas id="entropy_plot" width="100%" height="35%"></canvas>
  </div>
  <div class="footnote"> Some Caption
</div> -->

<!-- <div id="anno"> 
  <canvas id="resnet_50_lime" width="100%" height="35%"></canvas>
  </div>
  <div class="footnote"> Some Caption
</div>


<div id="anno"> 
  <canvas id="vit_lime" width="100%" height="35%"></canvas>
  </div>
  <div class="footnote"> Some Caption
</div> --></div>
    </summary>
    <updated>2022-04-20T00:00:00Z</updated>
    <published>2022-04-20T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2022-05-03T23:11:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2022/04/19/ideal-workshop-on-clustering/</id>
    <link href="https://cstheory-events.org/2022/04/19/ideal-workshop-on-clustering/" rel="alternate" type="text/html"/>
    <title>IDEAL Workshop on Clustering</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">April 22-23, 2022 Northwestern University &amp; Online https://www.ideal.northwestern.edu/events/clustering/ We are inviting you to attend the IDEAL Workshop on Clustering. The workshop will take place at Northwestern University on Friday, April 22, and Saturday, April 23. It will be in a hybrid format. If you are interested in participating in the workshop (in-person or remotely), please … <a class="more-link" href="https://cstheory-events.org/2022/04/19/ideal-workshop-on-clustering/">Continue reading <span class="screen-reader-text">IDEAL Workshop on Clustering</span></a></div>
    </summary>
    <updated>2022-04-19T15:37:34Z</updated>
    <published>2022-04-19T15:37:34Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2022-05-03T23:38:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/04/18/postdoc-at-university-of-texas-at-san-antonio-apply-by-april-30-2022/</id>
    <link href="https://cstheory-jobs.org/2022/04/18/postdoc-at-university-of-texas-at-san-antonio-apply-by-april-30-2022/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Texas at San Antonio (apply by April 30, 2022)</title>
    <summary>Applicants with background in one of the following fields are invited to apply: discrete and convex geometry, algebraic algorithms, convex optimization, randomized numerical analysis, average case complexity theory. One year research only position. Please send your CV, a letter on your technical background and research interests, and two references for recommendation letters to Alperen Ergur. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applicants with background in one of the following fields are invited to apply: discrete and convex geometry, algebraic algorithms, convex optimization, randomized numerical analysis, average case complexity theory. One year research only position. Please send your CV, a letter on your technical background and research interests, and two references for recommendation letters to Alperen Ergur.</p>
<p>Website: <a href="http://alpergur.xyz">http://alpergur.xyz</a><br/>
Email: alperen.ergur@utsa.edu</p></div>
    </content>
    <updated>2022-04-18T20:10:11Z</updated>
    <published>2022-04-18T20:10:11Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-05-03T23:37:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8738724093968037845</id>
    <link href="http://blog.computationalcomplexity.org/feeds/8738724093968037845/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/04/1-week-long-summer-school-for-ugrads.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/8738724093968037845" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/8738724093968037845" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/04/1-week-long-summer-school-for-ugrads.html" rel="alternate" type="text/html"/>
    <title>1-week long Summer School for Ugrads Interested in Theory, and my comments on it</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Recently a grad student in CS at UMCP emailed me the following email he got,  thinking (correctly) that I should forward it to interested ugrads. </p><p>-------------------------------------------------------</p><p>Are you interested in theoretical computer science including topics like algorithms, cryptography, machine learning, and others? If so, please consider applying to the New Horizons in Theoretical Computer Science week-long online summer school! The school will contain several mini-courses from top researchers in the field. The course is free of charge,and we welcome applications from undergraduates majoring in computer science or related fields. We particularly encourage applications from students that are members of groups that are currently under-represented in theoretical computer science.</p><p>Students from previous years have shared with us that the mini-lectures, online group activities, and interactions with other students and the friendly TAs were extraordinarily engaging and fun.</p><p>For full consideration, please complete the application (it’s short and easy!) by April 25, 2022. The summer school will take place online from June 6 to June 10.</p><p>Please see our website for details: see <a href="https://tcs-summerschool.ttic.edu/">here</a> </p><p>Any questions can be directed to summer-school-admin-2022@ttic.edu.</p><p>--------------------------------------------------------------------------</p><p>A few points about this</p><p>1) I emailed them asking `why do people need to apply if its online and free?'</p><p>I had one answer in mind, but they gave me another one</p><p><i>Their Answer: </i>They want to have SMALL online activities in groups. If they had X students and want groups of size g then if X is large, X/g may be too large. </p><p><i>My Answer</i>: If people REGISTER for something they are more likely to actually show up. (I know of a conference that got MORE people going once they had registation, and even MORE when they began charging for it.) </p><p>2) I emailed them asking if the talks will, at some later point, be on line. They will be. I then realized that there are already LOTS of theory talks online that I have not gotten around to watching, and perhaps never will. Even so, the talks on line may well benefit people who goto the summer school if they want to look back and something. </p><p>3) Online conferences PROS and CONS:</p><p>PROS: Free (or very low cost), no hassle getting airfare and hotel, and if talks are recorded then you can see them later (that applies to in-person as well). </p><p>CONS: Less committed to going to it. Can go in a half-ass way. For example, you can go and then in the middle of a talk go do your laundry. Being FORCED to be in a ROOM with the SPEAKER may be good. Also, of course, no informal conversations in the hallways.  Also, less serendipity. </p><p>I want to say <i>It would to be good to see talks outside of my area </i>however, this may only be true for easy talks, perhaps talks in a new field, OR talks that are just barely outside my area so I have some context. </p><p>4) I was surprised I didn't get the email directly since I have more contact with ugrads (and I have this blog) then the grad student who alerted me to it. However, I have learned that information gets to people in random ways so perhaps not to surprising. </p></div>
    </content>
    <updated>2022-04-18T13:54:00Z</updated>
    <published>2022-04-18T13:54:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-05-03T21:59:22Z</updated>
    </source>
  </entry>
</feed>
