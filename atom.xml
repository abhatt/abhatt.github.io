<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-01-07T22:44:49Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42171</id>
    <link href="https://cstheory.stackexchange.com/questions/42171/minimum-relevant-variables-in-linear-system-additive-approximation" rel="alternate" type="text/html"/>
    <title>Minimum relevant variables in linear system - additive approximation</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the problem <a href="https://en.wikipedia.org/wiki/Minimum_relevant_variables_in_linear_system" rel="nofollow noreferrer">Minimum Relevant Variables in Linear System</a> (Min-RVLS), the input is a linear system, e.g.:</p>

<p><span class="math-container">$$ A x = b $$</span></p>

<p>and the goal is to find a solution <span class="math-container">$x$</span> with as few nonzero variables as possible. </p>

<p>The problem is known to be NP-hard and hard to approximate to within a constant multiplicative factor (see the wikipedia page for details). </p>

<p>My question is: is anything known about <em>additive</em> approximations? In particular: what is the complexity of finding a solution that has at most <span class="math-container">$\text{OPT}+d$</span> nonzero variables, where <span class="math-container">$\text{OPT}$</span> is the smallest number of nonzero variables in a solution, and <span class="math-container">$d$</span> is some constant?</p>

<p>A related problem is: there always exists a solution with at most <span class="math-container">$m$</span> nonzero variables, where <span class="math-container">$m$</span> is the number of constraints (number of rows in <span class="math-container">$A$</span>). What is the complexity of finding a solution that has at most <span class="math-container">$m-d$</span> nonzero variables, for some constant <span class="math-container">$d$</span>?</p></div>
    </summary>
    <updated>2019-01-07T16:08:53Z</updated>
    <published>2019-01-07T16:08:53Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="approximation-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="optimization"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="linear-programming"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="approximation-hardness"/>
    <author>
      <name>Erel Segal-Halevi</name>
      <uri>https://cstheory.stackexchange.com/users/9453</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42170</id>
    <link href="https://cstheory.stackexchange.com/questions/42170/is-there-exists-a-polynomial-time-algorithm-to-find-a-order-k-subgroup" rel="alternate" type="text/html"/>
    <title>Is there exists a polynomial time algorithm to find a order $k$ subgroup?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>How to find subgroups ( unique up to isomorphism) of order <span class="math-container">$k$</span> of a group <span class="math-container">$G$</span>, when the input group is given in the <a href="http://mathworld.wolfram.com/MultiplicationTable.html" rel="nofollow noreferrer">explicit</a> form. The idea coming to my mind follows, try all possible subsets of size <span class="math-container">$k$</span> and then check whether they form a subgroup of <span class="math-container">$G$</span> or not which can be checked in polynomial time but overall runtime may not be polynomial in the order of the group <span class="math-container">$G$</span> depending upon the value of <span class="math-container">$k$</span>. What is the fastest known algorithm for the task described above? What if the input group is a <span class="math-container">$p$</span>-group? Is it possible to find an order <span class="math-container">$k$</span> subgroup in polynomial time, when <span class="math-container">$k$</span> is very large? </p></div>
    </summary>
    <updated>2019-01-07T15:37:57Z</updated>
    <published>2019-01-07T15:37:57Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="gr.group-theory"/>
    <author>
      <name>aaaa</name>
      <uri>https://cstheory.stackexchange.com/users/43707</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42169</id>
    <link href="https://cstheory.stackexchange.com/questions/42169/optimal-algorithm-to-compare-lines-of-different-files-without-repetition" rel="alternate" type="text/html"/>
    <title>Optimal algorithm to compare lines of different files without repetition</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have 1600 ASCII files with 1000 lines in each file. Each line has only one entry and is a floating point number e.g. 1.67923.
Let's denote the line1 of file1 with <code>L(1,1)</code>, line2 of file1 with <code>L(1,2)</code> and so forth to ...<code>L(1,1000)</code>. Similarly, line1 of file2 will be <code>L(2,1)</code> and the last line of file1600 will thus be <code>L(1600,1000)</code>.
My task is to come up with a memory efficient algorithm to compare all lines between each file and the lines within each file. Since, I have 1600 files and 1000 lines in each file, it will take approx. <code>10^12</code> calculations. These first comparisons will look like this:</p>

<pre><code>1. {L(1,1)-L(1,2)}, {L(1,1)-L(1,3)},....,{L(1,1)-L(1,1000)}
2. {L(1,1)-L(2,1)}, {L(1,1)-L(2,2)},....,{L(1,1)-L(2,1000)}
3. {L(1,1)-L(3,1)}, {L(1,1)-L(3,2)},....,{L(1,1)-L(3,1000)}
.
.
. 
</code></pre>

<p>Please note that I don't want repetitions i.e <code>{L(1,1)-L(2,1)} = {L(2,1)-L(1,1)}</code>.
I need to code this problem in Fortran but any help on a general scheme as to how the problem needs to be approached will be useful.
Thank you in advance!  </p></div>
    </summary>
    <updated>2019-01-07T15:12:49Z</updated>
    <published>2019-01-07T15:12:49Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="ds.algorithms"/>
    <author>
      <name>Abedin Y. Abedin</name>
      <uri>https://cstheory.stackexchange.com/users/51670</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42167</id>
    <link href="https://cstheory.stackexchange.com/questions/42167/decomposition-for-a-certain-class-of-graphs" rel="alternate" type="text/html"/>
    <title>Decomposition for a certain class of graphs</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Suppose a graph, <span class="math-container">$G = (V,E)$</span> is characterized as a lattice/network of cliques as in the picture below. Does there exist some decomposition principle (i.e. on the right) for <span class="math-container">$G$</span>, that yields some special structure that may be used to explain efficiencies experienced with what are supposed to be combinatorial hard problems?</p>

<p><a href="https://i.stack.imgur.com/FTbx8.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/FTbx8.png"/></a></p></div>
    </summary>
    <updated>2019-01-07T06:19:24Z</updated>
    <published>2019-01-07T06:19:24Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="graph-theory"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="graph-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="co.combinatorics"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="treewidth"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="integer-lattice"/>
    <author>
      <name>Student</name>
      <uri>https://cstheory.stackexchange.com/users/51578</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42166</id>
    <link href="https://cstheory.stackexchange.com/questions/42166/algorithm-for-k-best-non-perfect-bipartite-matchings" rel="alternate" type="text/html"/>
    <title>Algorithm for K-best NON perfect bipartite matchings</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I was reading this great article: <a href="https://core.ac.uk/download/pdf/82129717.pdf" rel="nofollow noreferrer">https://core.ac.uk/download/pdf/82129717.pdf</a></p>

<p>It solves a generalization of the maximum sum assignment problem by finding the k best assignments and not only the best.
However, it only looks at perfect matchings. I'm am especially interested in bipartite matchings.</p>

<p>In particular, for the bipartite graphs, the Theorem 1 p. 161 uses the fact that the matchings are considered perfect.</p>

<p>How can I solve the k-best assignment problem for general bipartite graphs?</p></div>
    </summary>
    <updated>2019-01-06T23:47:08Z</updated>
    <published>2019-01-06T23:47:08Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="graph-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="matching"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="bipartite-graphs"/>
    <author>
      <name>Labo</name>
      <uri>https://cstheory.stackexchange.com/users/43172</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42163</id>
    <link href="https://cstheory.stackexchange.com/questions/42163/immutable-space-model" rel="alternate" type="text/html"/>
    <title>Immutable Space Model</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have heard it said that time is more precious than space because we can reuse space but not time.  What if we treat space with this much reverence?</p>

<h3>What is generally known about models of computation in which space is immutable?</h3>

<p>I would expect such models to initialize each memory cell to some "blank" state and then only allow the writing of some "non-blank" value to each cell at most once.</p>

<p>The study of <a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="noreferrer">persistent data structures</a> seems to me like a possible way to answer this question.</p>

<p>I thought of this question while studying functional programming, which highly values immutability.</p></div>
    </summary>
    <updated>2019-01-06T15:37:04Z</updated>
    <published>2019-01-06T15:37:04Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="reference-request"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="ds.data-structures"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="functional-programming"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="space-complexity"/>
    <author>
      <name>Tyson Williams</name>
      <uri>https://cstheory.stackexchange.com/users/3964</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42161</id>
    <link href="https://cstheory.stackexchange.com/questions/42161/is-this-partition-problem-strongly-np-complete" rel="alternate" type="text/html"/>
    <title>Is this partition problem strongly NP-complete?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Some computational problems have variants that appear to be harder. For instance, Graph Automorphism (GA) problem has quasi-polynomial time algorithm ( by Babai's Graph Isomorphism result) while the fixed-point free GA problem is NP-complete. </p>

<p><a href="https://en.wikipedia.org/wiki/Partition_problem" rel="nofollow noreferrer">Partition problem</a> is weakly NP-complete problem since it has pseudo-polynomial time algorithm. I am interested in variants that are strongly NP-complete.</p>

<p>Here is a variant of partition problem:</p>

<p>Restricted partition problem</p>

<p><strong>Input</strong>: Set <span class="math-container">$S$</span> of <span class="math-container">$2N$</span> integers, and a collection <span class="math-container">$P$</span> of pairs from <span class="math-container">$S$</span>, <span class="math-container">$0 \lt |P| \lt N$</span> </p>

<p><strong>Query</strong>: Is there a partition of <span class="math-container">$S$</span> into two equal cardinality parts <span class="math-container">$A$</span> and <span class="math-container">$S-A$</span> such that both parts have the same sum and no pair in <span class="math-container">$P$</span> has both elements in one side of the partition?</p>

<blockquote>
  <p>Is this variant of partition problem NP-complete in the strong sense? </p>
</blockquote>

<p>This was posted first on <a href="https://mathoverflow.net/questions/306039/is-this-partition-problem-strongly-np-complete">Math overflow</a> (I believe the posted answer is incorrect since the proposed dynamic programming algorithm does not take into consideration the cardinality of <span class="math-container">$P$</span>).</p></div>
    </summary>
    <updated>2019-01-06T12:45:42Z</updated>
    <published>2019-01-06T12:45:42Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="cc.complexity-theory"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="partition-problem"/>
    <author>
      <name>Mohammad Al-Turkistany</name>
      <uri>https://cstheory.stackexchange.com/users/495</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42160</id>
    <link href="https://cstheory.stackexchange.com/questions/42160/maximize-edges-minus-vertices-in-a-weighted-graph" rel="alternate" type="text/html"/>
    <title>maximize edges minus vertices in a weighted graph</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>for a given weighted vertices and edges graph, we want to find the maximum subgraph. the maximum subgraph is made of some vertices and some edges of the given graph which sum of the edges minus sum of the vertices is maximum. what is the algorithm for this problem? or any help with the code please.</p></div>
    </summary>
    <updated>2019-01-06T11:00:18Z</updated>
    <published>2019-01-06T11:00:18Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="graph-theory"/>
    <author>
      <name>andrew</name>
      <uri>https://cstheory.stackexchange.com/users/51663</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42159</id>
    <link href="https://cstheory.stackexchange.com/questions/42159/nusmv-how-to-indicate-the-execution-should-visit-some-states-infinitely-often" rel="alternate" type="text/html"/>
    <title>NuSMV - How to indicate the execution should visit some states infinitely often?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have the following kripke structure:</p>

<p><a href="https://i.stack.imgur.com/3xDPG.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/3xDPG.png"/></a></p>

<p>I need my model to follow the LTL constraint that state d will be visited infinitely often:</p>

<pre><code>LTLSPEC  G F (modelState=d)
</code></pre>

<p>This constraint fails due to existence of the loop .... b-&gt;c-&gt;b-&gt;c ......  </p>

<p>Question: What would be a solution to this problem? This may be related to fair traces, but I am not very familiar with that, or how to indicate d as a fair state in NuSMV. </p>

<p>I am learning model checking on my own and I appreciate your help very much.</p></div>
    </summary>
    <updated>2019-01-06T05:47:32Z</updated>
    <published>2019-01-06T05:47:32Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="model-checking"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="formal-methods"/>
    <author>
      <name>Fabiana</name>
      <uri>https://cstheory.stackexchange.com/users/51646</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42158</id>
    <link href="https://cstheory.stackexchange.com/questions/42158/best-polynomial-time-approximation-factor-for-np-optimization-problems" rel="alternate" type="text/html"/>
    <title>Best polynomial-time approximation factor for NP-optimization problems</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Let us say that a function <span class="math-container">$f(n)$</span> is the <strong>best approximation factor</strong> for an NP-optimization problem, if both of the following hold:</p>

<ol>
<li><p>There exist a polynomial-time algorithm <span class="math-container">$A,$</span> and an integer <span class="math-container">$n_0$</span>, such that <span class="math-container">$A$</span> provides an <span class="math-container">$f(n)$</span>-approximation for the NP-optimization problem for every instance with size <span class="math-container">$n\geq n_0$</span>. (Note: the role of <span class="math-container">$n_0$</span> is merely to treat potentially deviant small instances, which might make the function "ugly.")</p></li>
<li><p>There is no polynomial-time <span class="math-container">$(1-o(1))f(n)$</span> approximation, unless <span class="math-container">$P=NP$</span>.</p></li>
</ol>

<p>A classic example where such a best approximation is known is the SET COVER problem (for a summary and references see its Wikipedia page): the Greedy Algorithm provides an <span class="math-container">$\ln n$</span> approximation, but there is no  <span class="math-container">$(1-o(1))\ln n$</span> approximation, unless <span class="math-container">$P=NP$</span>.</p>

<p><strong>Questions:</strong></p>

<ol>
<li><p>Which are some other interesting NP-optimization problems for which a best approximation factor, along with its realizing algorithm, are known?  </p></li>
<li><p>Are there any counterexamples, i.e., NP-optimization problems, for which such a best approximation cannot exist, unless <span class="math-container">$P=NP$</span>?</p></li>
</ol></div>
    </summary>
    <updated>2019-01-05T16:54:03Z</updated>
    <published>2019-01-05T16:54:03Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="cc.complexity-theory"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="np-hardness"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="approximation-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="approximation-hardness"/>
    <author>
      <name>Andras Farago</name>
      <uri>https://cstheory.stackexchange.com/users/12710</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42155</id>
    <link href="https://cstheory.stackexchange.com/questions/42155/why-cant-a-left-recursive-non-deterministic-or-ambiguous-grammar-be-ll1" rel="alternate" type="text/html"/>
    <title>Why can't a left-recursive, non-deterministic, or ambiguous grammar be LL(1)?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I've learned from several sources that an LL(1) grammar is:</p>

<ol>
<li>unambiguous,</li>
<li>not left-recursive,</li>
<li>and, deterministic (left-factorized).</li>
</ol>

<p>What I can't fully understand is why the above is true for any LL(1) grammar. I know the LL(1) parsing table will have multiple entries at some cells, but what I really want to get is a formal and general (not with an example) proof to the following proposition(s):</p>

<p>A left-recursive (1), non-deterministic (2), or ambiguous (3) grammar is not LL(1).</p></div>
    </summary>
    <updated>2019-01-05T13:29:02Z</updated>
    <published>2019-01-05T13:29:02Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="fl.formal-languages"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="grammars"/>
    <author>
      <name>Mr Geek</name>
      <uri>https://cstheory.stackexchange.com/users/39204</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42150</id>
    <link href="https://cstheory.stackexchange.com/questions/42150/prove-that-if-a-is-np-complete-and-b-is-conp-complete-than-axb-is-np-conp-com" rel="alternate" type="text/html"/>
    <title>Prove that if A is NP-complete and B is coNP-complete, than AxB is NP-, coNP-complete</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>AxB means cartesian product of A and B.</p>

<p>May someone help me with this? I even have no idea how to prove that AxB belongs to NP or coNP</p></div>
    </summary>
    <updated>2019-01-04T22:45:04Z</updated>
    <published>2019-01-04T22:45:04Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="cc.complexity-theory"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="np-hardness"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="complexity-classes"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="np-complete"/>
    <author>
      <name>guest</name>
      <uri>https://cstheory.stackexchange.com/users/51651</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42148</id>
    <link href="https://cstheory.stackexchange.com/questions/42148/feel-dissatisfied-after-each-submission" rel="alternate" type="text/html"/>
    <title>Feel dissatisfied after each submission</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I am a third year graduate student at a "top-20" university who works on fine-grained complexity (lots of playing with 3-SUM, OV and the usual popular hardness conjectures). I have been fairly productive over the last year or so and have 3 accepted papers and two submitted papers. All of this to say that I am a fairly experienced graduate student and what I am about to describe is not anecdotal.</p>

<p>Every submission brings me more dissatisfaction than satisfaction. Just before I start working on a problem, me and my advisor identify a list of concrete questions that need to be answered. After lots of thinking, we have some very nice non-trivial results which gives me a lot of happiness and satisfaction. As we start to write down all of the results, inevitably, there are some more interesting variants that pop up but are much harder to make progress on. After the initial euphoria point, I feel everything seems to go downhill. There are so many variants that also need to be answered, are clearly in the purview of the problem at hand but I am not able to. By the time we submit the paper, I am so dismayed that results in the paper seem almost trivial. Perhaps this is simply tunnel vision, but I can't overcome the sadness about not being able to answer peripheral questions (although these make for a terrific conclusion section).</p>

<p>This has happened every single time and I am wondering if this is a common feeling. Do other people in theory community feel the same way? I am not sure if this is an academia wide feeling. My fellow graduate students from other areas are over the moon after every submission (but this is just anecdotal).</p>

<p>Edit - I see that there is another soft-question on the front page. I apologize for adding another one. Its holiday season and (only?) after a few drinks, one starts to ponder over these things!</p></div>
    </summary>
    <updated>2019-01-04T17:14:28Z</updated>
    <published>2019-01-04T17:14:28Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="soft-question"/>
    <author>
      <name>karmanaut</name>
      <uri>https://cstheory.stackexchange.com/users/35523</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1474</id>
    <link href="https://theorydish.blog/2019/01/04/on-pac-analysis-and-deep-neural-networks/" rel="alternate" type="text/html"/>
    <title>On PAC Analysis and Deep Neural Networks</title>
    <summary>Guest post by Amit Daniely and Roy Frostig. For years now—especially since the landmark work of Krishevsky et. al.—learning deep neural networks has been a method of choice in prediction and regression tasks, especially in perceptual domains found in computer vision and natural language processing. How effective might it be for solving theoretical tasks? Specifically, focusing on supervised learning: Can a deep neural network, paired with a stochastic gradient method, be shown to PAC learn any interesting concept class in polynomial time? Depending on assumptions, and on one’s definition of “interesting,” present-day learning theory gives answers ranging from “no, that would solve hard problems,” to, more recently: Theorem: Networks with depth between 2 and ,1 having standard activation functions,2 with weights initialized at random and trained with stochastic gradient descent, learn, in polynomial time, constant degree large margin polynomial thresholds. Learning constant-degree polynomials can also be done simply with a linear predictor over a polynomial embedding, or, in other words, by learning a halfspace. That said, what a linear predictor can do is also essentially the state of the art in PAC learning, so this result pushes neural net learning at least as far as one might hope at first. [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Guest post by <a href="http://amitdaniely.com/">Amit Daniely</a> and <a href="https://cs.stanford.edu/~rfrostig/">Roy Frostig</a>.</em></p>
<p>For years now—especially since the landmark work of <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">Krishevsky et. al.</a>—learning deep neural networks has been a method of choice in prediction and regression tasks, especially in perceptual domains found in computer vision and natural language processing. How effective might it be for solving <em>theoretical</em> tasks?</p>
<p>Specifically, focusing on supervised learning:</p>
<blockquote><p>Can a deep neural network, paired with a stochastic gradient method, be shown to <a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">PAC learn</a> any interesting concept class in polynomial time?</p></blockquote>
<p>Depending on assumptions, and on one’s definition of “interesting,” present-day learning theory gives answers ranging from “no, that would solve hard problems,” to, more recently:</p>
<blockquote><p><strong>Theorem:</strong> Networks with depth between 2 and <img alt="\log(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\log(n)"/>,<a class="footnoteRef" href="https://theorydish.blog/feed/#fn1" id="fnref1"><sup>1</sup></a> having standard activation functions,<a class="footnoteRef" href="https://theorydish.blog/feed/#fn2" id="fnref2"><sup>2</sup></a> with weights initialized at random and trained with stochastic gradient descent, learn, in polynomial time, constant degree large margin polynomial thresholds.</p></blockquote>
<p>Learning constant-degree polynomials can also be done simply <em>with a linear predictor</em> over a polynomial embedding, or, in other words, by learning a halfspace. That said, what a linear predictor can do is also <em>essentially the state of the art</em> in PAC learning, so this result pushes neural net learning at least as far as one might hope at first. We will return to this point later, and discuss some limitations of PAC analysis once they are more apparent. In this sense, this post will turn out to be as much an overview of some PAC learning theory as it is about neural networks.</p>
<p>Naturally, there is a wide variety of theoretical perspectives on neural network analysis, especially in the past couple of years. Our goal in this post is not to survey or cover any extensive body of work, but simply to summarize our own recent line (from two papers: <a href="https://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity">DFS’16</a> and <a href="https://papers.nips.cc/paper/6836-sgd-learns-the-conjugate-kernel-class-of-the-network">D’17</a>), and to highlight the interaction with PAC learning.</p>
<h2 id="neural-network-learning">Neural network learning</h2>
<p>First, let’s define a learning task. To keep things simple, we’ll focus on binary classification over the boolean cube, without noise. Formally:</p>
<blockquote><p><strong>(Binary classification.)</strong> Given examples of the form <img alt="(x,h^*(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Ch%5E%2A%28x%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="(x,h^*(x))"/>, where <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x"/> is sampled from some unknown distribution <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal D"/> on <img alt="\{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\{\pm 1\}^n"/>, and <img alt="h^*:\{\pm 1\}^n\to\{\pm 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=h%5E%2A%3A%5C%7B%5Cpm+1%5C%7D%5En%5Cto%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h^*:\{\pm 1\}^n\to\{\pm 1\}"/> is some unknown function (the one that we wish to learn), find a function <img alt="h:\{\pm 1\}^n\to\{\pm 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=h%3A%5C%7B%5Cpm+1%5C%7D%5En%5Cto%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h:\{\pm 1\}^n\to\{\pm 1\}"/> whose error, <img alt="\mathrm{Err}(h) = \mathrm{Pr}_{x\sim\mathcal{D}} \left(h(x) \ne h^*(x)\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BErr%7D%28h%29+%3D+%5Cmathrm%7BPr%7D_%7Bx%5Csim%5Cmathcal%7BD%7D%7D+%5Cleft%28h%28x%29+%5Cne+h%5E%2A%28x%29%5Cright%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathrm{Err}(h) = \mathrm{Pr}_{x\sim\mathcal{D}} \left(h(x) \ne h^*(x)\right)"/>, is small.</p></blockquote>
<p>Second, define a neural network <img alt="\mathcal N" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+N&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal N"/> formally as a directed acyclic graph <img alt="(V, E)" class="latex" src="https://s0.wp.com/latex.php?latex=%28V%2C+E%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="(V, E)"/> whose vertices <img alt="V" class="latex" src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="V"/> are called neurons. Of them, <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n"/> are input neurons, one is an output neuron, and the rest are called hidden neurons.<a class="footnoteRef" href="https://theorydish.blog/feed/#fn3" id="fnref3"><sup>3</sup></a> A network together with a weight vector <img alt="w = \{w_{uv} : uv \in E\} \cup \{b_v : v \in V \}" class="latex" src="https://s0.wp.com/latex.php?latex=w+%3D+%5C%7Bw_%7Buv%7D+%3A+uv+%5Cin+E%5C%7D+%5Ccup+%5C%7Bb_v+%3A+v+%5Cin+V+%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w = \{w_{uv} : uv \in E\} \cup \{b_v : v \in V \}"/> defines a predictor <img alt="h_{\mathcal N, w} : \{\pm 1\}^n \to \{\pm 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7B%5Cmathcal+N%2C+w%7D+%3A+%5C%7B%5Cpm+1%5C%7D%5En+%5Cto+%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h_{\mathcal N, w} : \{\pm 1\}^n \to \{\pm 1\}"/> whose prediction is computed by propagating <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x"/> forward through the network. Concretely:</p>
<ul>
<li>For an input neuron <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="v"/>, <img alt="h_{v,w}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bv%2Cw%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h_{v,w}(x)"/> is the corresponding coordinate in <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x"/>.</li>
<li>For a hidden neuron <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="v"/>, define<img alt="h_{v,w}(x) = \sigma\left( \sum_{u \in \mathrm{IN}(v)} w_{uv} h_{u,w}(x) + b_v \right)." class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bv%2Cw%7D%28x%29+%3D+%5Csigma%5Cleft%28+%5Csum_%7Bu+%5Cin+%5Cmathrm%7BIN%7D%28v%29%7D+w_%7Buv%7D+h_%7Bu%2Cw%7D%28x%29+%2B+b_v+%5Cright%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h_{v,w}(x) = \sigma\left( \sum_{u \in \mathrm{IN}(v)} w_{uv} h_{u,w}(x) + b_v \right)."/>The scalar weight <img alt="b_v" class="latex" src="https://s0.wp.com/latex.php?latex=b_v&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="b_v"/> is called a “bias.” In this post, the function <img alt="\sigma : \mathbb{R} \to \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma+%3A+%5Cmathbb%7BR%7D+%5Cto+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\sigma : \mathbb{R} \to \mathbb{R}"/> is the ReLU activation <img alt="\sigma(t) = \max\{t, 0\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%28t%29+%3D+%5Cmax%5C%7Bt%2C+0%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\sigma(t) = \max\{t, 0\}"/>, though others are possible as well.</li>
<li>For the output neuron <img alt="o" class="latex" src="https://s0.wp.com/latex.php?latex=o&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="o"/>, we drop the activation: <img alt="h_{o,w}(x) = \sum_{u \in \mathrm{IN}(o)} w_{uo} h_{u,w}(x) + b_o" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bo%2Cw%7D%28x%29+%3D+%5Csum_%7Bu+%5Cin+%5Cmathrm%7BIN%7D%28o%29%7D+w_%7Buo%7D+h_%7Bu%2Cw%7D%28x%29+%2B+b_o&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h_{o,w}(x) = \sum_{u \in \mathrm{IN}(o)} w_{uo} h_{u,w}(x) + b_o"/>.</li>
</ul>
<p>Finally, let <img alt="h_{\mathcal N, w}(x) = h_{o, w}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7B%5Cmathcal+N%2C+w%7D%28x%29+%3D+h_%7Bo%2C+w%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h_{\mathcal N, w}(x) = h_{o, w}(x)"/>. This computes a real-valued function, so where we’d like to use it for classification, we do so by thresholding, and abuse the notation <img alt="\mathrm{Err}(h_w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BErr%7D%28h_w%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathrm{Err}(h_w)"/> to mean <img alt="\mathrm{Err}(\mathrm{sign} \circ h_w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BErr%7D%28%5Cmathrm%7Bsign%7D+%5Ccirc+h_w%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathrm{Err}(\mathrm{sign} \circ h_w)"/>.</p>
<p>Some intuition for this definition would come from verifying that:</p>
<ul>
<li>Any function <img alt="h : \{\pm 1\}^n \to \{\pm 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=h+%3A+%5C%7B%5Cpm+1%5C%7D%5En+%5Cto+%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h : \{\pm 1\}^n \to \{\pm 1\}"/> can be computed by a network of depth two and <img alt="2^n" class="latex" src="https://s0.wp.com/latex.php?latex=2%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="2^n"/> hidden neurons.</li>
<li>The parity function <img alt="h(x) = \prod_{i=1}^n x_i" class="latex" src="https://s0.wp.com/latex.php?latex=h%28x%29+%3D+%5Cprod_%7Bi%3D1%7D%5En+x_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h(x) = \prod_{i=1}^n x_i"/> can be computed by a network of depth two and <img alt="4n" class="latex" src="https://s0.wp.com/latex.php?latex=4n&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="4n"/> hidden neurons. (NB: this one is a bit more challenging.)</li>
</ul>
<p>In practice, the network architecture (this DAG) is designed based on some domain knowledge, and its design can impact the predictor that’s later selected by SGD. One default architecture, useful in the absence of domain knowledge, is the multi-layer perceptron, comprised of layers of complete bipartite graphs:</p>
<figure><img alt="full_con_net" class="  wp-image-1479 aligncenter" height="426" src="https://theorydish.files.wordpress.com/2019/01/full_con_net.png?w=431&amp;h=426" width="431"/>A toy “fully-connected neural network”, a.k.a. a multi-layer perceptronAnother paradigmatic architecture is a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional network</a>:<p/>
</figure>
<figure><img alt="conv_net" class="  wp-image-1478 aligncenter" height="463" src="https://theorydish.files.wordpress.com/2019/01/conv_net.png?w=490&amp;h=463" width="490"/>A toy convolutional neural network</figure>
<p>Convolutional nets capture the notion of spatial input locality in signals such as images and audio.<a class="footnoteRef" href="https://theorydish.blog/feed/#fn4" id="fnref4"><sup>4</sup></a> In the toy example drawn, each clustered triple of neurons is a so-called convolution filter applied to two components below it. In image domains, convolutions filters are two-dimensional and capture responses to spatial 2-D patches of the image or of an intermediate layer.</p>
<p>Training a neural net comprises (i) initialization, and (ii) iterative optimization run until <img alt="\mathrm{sign}(h_w(x)) = h^*(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bsign%7D%28h_w%28x%29%29+%3D+h%5E%2A%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathrm{sign}(h_w(x)) = h^*(x)"/> for sufficiently many examples <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x"/>. The initialization step sets the starting values of the weights <img alt="w^0" class="latex" src="https://s0.wp.com/latex.php?latex=w%5E0&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w^0"/> at random:</p>
<blockquote><p><strong>(Glorot initialization.)</strong> Draw weights <img alt="\{w^0_{uv}\}_{uv\in E}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bw%5E0_%7Buv%7D%5C%7D_%7Buv%5Cin+E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\{w^0_{uv}\}_{uv\in E}"/> from centered Gaussians with variance <img alt="|\mathrm{IN}(v)|^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cmathrm%7BIN%7D%28v%29%7C%5E%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="|\mathrm{IN}(v)|^{-1}"/> and biases <img alt="\{b^0_{v}\}_{v\in V}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%5E0_%7Bv%7D%5C%7D_%7Bv%5Cin+V%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\{b^0_{v}\}_{v\in V}"/> from independent standard Gaussians.<a class="footnoteRef" href="https://theorydish.blog/feed/#fn5" id="fnref5"><sup>5</sup></a></p></blockquote>
<p>While other initialization schemes exists, this one is canonical, simple, and, as the reader can verify, satisfies <img alt="\mathbb{E}_{w^0}\left[(h_{v,w^0}(x))^2\right] = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bw%5E0%7D%5Cleft%5B%28h_%7Bv%2Cw%5E0%7D%28x%29%29%5E2%5Cright%5D+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathbb{E}_{w^0}\left[(h_{v,w^0}(x))^2\right] = 1"/> for every neuron <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="v"/> and input <img alt="x \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x \in \{\pm 1\}^n"/>.</p>
<p>The optimization step is essentially a local search method from the initial point, using <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a> (SGD) or a variant thereof.<a class="footnoteRef" href="https://theorydish.blog/feed/#fn6" id="fnref6"><sup>6</sup></a> To apply SGD, we need a function suitable for descent, and we’ll use the commonplace logistic loss <img alt="\ell(z) = \log_2(1+e^{-z})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell%28z%29+%3D+%5Clog_2%281%2Be%5E%7B-z%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\ell(z) = \log_2(1+e^{-z})"/>, which bounds the zero-one loss <img alt="\ell^{0-1}(z) = \mathbf{1}[z \le 0]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell%5E%7B0-1%7D%28z%29+%3D+%5Cmathbf%7B1%7D%5Bz+%5Cle+0%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\ell^{0-1}(z) = \mathbf{1}[z \le 0]"/> from above:</p>
<figure><img alt="losses" class="  wp-image-1480 aligncenter" height="246" src="https://theorydish.files.wordpress.com/2019/01/losses.png?w=329&amp;h=246" width="329"/>The logistic and zero-one losses</figure>
<p> </p>
<p>Define <img alt="L_{\mathcal D}(w) = \mathbb{E}_{x\sim\mathcal D}\left[ \ell(h_w(x)h^*(x)) \right]" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7B%5Cmathcal+D%7D%28w%29+%3D+%5Cmathbb%7BE%7D_%7Bx%5Csim%5Cmathcal+D%7D%5Cleft%5B+%5Cell%28h_w%28x%29h%5E%2A%28x%29%29+%5Cright%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{\mathcal D}(w) = \mathbb{E}_{x\sim\mathcal D}\left[ \ell(h_w(x)h^*(x)) \right]"/>. Note that <img alt="\mathrm{Err}(h_w) \le L_{\mathcal D}(w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BErr%7D%28h_w%29+%5Cle+L_%7B%5Cmathcal+D%7D%28w%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathrm{Err}(h_w) \le L_{\mathcal D}(w)"/>, so finding weights <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w"/> for which the upper bound <img alt="L_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{\mathcal D}"/> is small enough implies low error in turn. Meanwhile, <img alt="L_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{\mathcal D}"/> is amenable to iterative gradient-based minimization.</p>
<p>Given samples from <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal D"/>, stochastic gradient descent creates an unbiased estimate of the gradient at each step <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="t"/> by drawing a batch of i.i.d. samples <img alt="S_t" class="latex" src="https://s0.wp.com/latex.php?latex=S_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="S_t"/> from <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal D"/>. The gradient <img alt="\nabla L_{S_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+L_%7BS_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\nabla L_{S_t}"/> at a point <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w"/> can be computed efficiently by the <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> algorithm.</p>
<p>In more complete detail, our prototypical neural network training algorithm is as follows. On input a network <img alt="\mathcal N" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+N&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal N"/>, an iteration count <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="T"/>, a batch size <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="b"/>, and a step size <img alt="\eta &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta+%3E+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\eta &gt; 0"/>:</p>
<p><strong>Algorithm: <em>SGDNN</em></strong></p>
<ol type="1">
<li>Let <img alt="w^0" class="latex" src="https://s0.wp.com/latex.php?latex=w%5E0&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w^0"/> be random weights sampled per Glorot initialization</li>
<li>For <img alt="t = 1, \ldots, T" class="latex" src="https://s0.wp.com/latex.php?latex=t+%3D+1%2C+%5Cldots%2C+T&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="t = 1, \ldots, T"/>:
<ol type="1">
<li>Sample a batch <img alt="S_{t} = \{(x^t_1, h^*(x^t_1)), \ldots, (x^t_b, h^*(x^t_b))\}" class="latex" src="https://s0.wp.com/latex.php?latex=S_%7Bt%7D+%3D+%5C%7B%28x%5Et_1%2C+h%5E%2A%28x%5Et_1%29%29%2C+%5Cldots%2C+%28x%5Et_b%2C+h%5E%2A%28x%5Et_b%29%29%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="S_{t} = \{(x^t_1, h^*(x^t_1)), \ldots, (x^t_b, h^*(x^t_b))\}"/>, where <img alt="x^t_1, \ldots, x^t_b" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Et_1%2C+%5Cldots%2C+x%5Et_b&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x^t_1, \ldots, x^t_b"/> are i.i.d. samples from <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal D"/>.</li>
<li>Update <img alt="w^t \gets w^{t-1} - \eta \nabla L_{S_t}(w^{t-1})" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Et+%5Cgets+w%5E%7Bt-1%7D+-+%5Ceta+%5Cnabla+L_%7BS_t%7D%28w%5E%7Bt-1%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w^t \gets w^{t-1} - \eta \nabla L_{S_t}(w^{t-1})"/>, where<img alt="L_{S_t}(w^{t-1}) = b^{-1} \sum_{i=1}^b \ell(h_{w}(x^t_i) h^*(x^t_i))" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7BS_t%7D%28w%5E%7Bt-1%7D%29+%3D+b%5E%7B-1%7D+%5Csum_%7Bi%3D1%7D%5Eb+%5Cell%28h_%7Bw%7D%28x%5Et_i%29+h%5E%2A%28x%5Et_i%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{S_t}(w^{t-1}) = b^{-1} \sum_{i=1}^b \ell(h_{w}(x^t_i) h^*(x^t_i))"/>.</li>
</ol>
</li>
<li>Output <img alt="w^T" class="latex" src="https://s0.wp.com/latex.php?latex=w%5ET&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w^T"/></li>
</ol>
<h2 id="pac-learning">PAC learning</h2>
<p>Learning a predictor from example data is a general task, and a hard one in the worst case. We cannot efficiently (i.e. in <img alt="\mathrm{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathrm{poly}(n)"/> time) compute, let alone learn, general functions from <img alt="\{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\{\pm 1\}^n"/> to <img alt="\{\pm 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\{\pm 1\}"/>. In fact, any learning algorithm that is guaranteed to succeed in general (i.e. with any target predictor <img alt="h^*" class="latex" src="https://s0.wp.com/latex.php?latex=h%5E%2A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h^*"/> over any data distribution <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal D"/>) runs, in the worst case, in time exponential in <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n"/>. This is true even for rather weak definitions of “success,” such as finding a predictor with error less than <img alt="1/2 - 2^{-n/2}" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2+-+2%5E%7B-n%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="1/2 - 2^{-n/2}"/>, i.e. one that slightly outperforms a random guess.</p>
<p>While it is impossible to efficiently learn general functions under general distributions, it might still be possible to learn efficiently under some assumptions on the target <img alt="h^*" class="latex" src="https://s0.wp.com/latex.php?latex=h%5E%2A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h^*"/> or the distribution <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal D"/>. Charting out such assumptions is the realm of learning theorists: by now, they’ve built up a broad catalog of function classes, and have studied the complexity of learning when the target function is in each such class. Although their primary aim has been to develop theory, the potential guidance for practice is easy to imagine: if one’s application domain happens to be modeled well by one of these easily-learnable function classes, there’s a corresponding learning algorithm to consider as well.</p>
<p>The vanilla PAC model makes no assumptions on the data distribution <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal D"/>, but it does assume the target <img alt="h^*" class="latex" src="https://s0.wp.com/latex.php?latex=h%5E%2A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h^*"/> belongs to some simple, predefined class <img alt="\mathcal H" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+H&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal H"/>. Formally, a <em>PAC learning problem</em> is defined by a function class<a class="footnoteRef" href="https://theorydish.blog/feed/#fn7" id="fnref7"><sup>7</sup></a> <img alt="\mathcal H \subset \{\pm 1\}^{\{\pm 1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+H+%5Csubset+%5C%7B%5Cpm+1%5C%7D%5E%7B%5C%7B%5Cpm+1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal H \subset \{\pm 1\}^{\{\pm 1\}^n}"/>. A learning algorithm <img alt="\mathcal A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal A"/> <em>learns</em> the class <img alt="\mathcal H" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+H&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal H"/> if, whenever <img alt="h^* \in \mathcal H" class="latex" src="https://s0.wp.com/latex.php?latex=h%5E%2A+%5Cin+%5Cmathcal+H&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="h^* \in \mathcal H"/>, and provided <img alt="\epsilon &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\epsilon &gt; 0"/>, it runs in time <img alt="\mathrm{poly}(1/\epsilon, n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bpoly%7D%281%2F%5Cepsilon%2C+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathrm{poly}(1/\epsilon, n)"/>, and returns a function of error at most <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\epsilon"/>, with probability at least 0.9. Note that:</p>
<ol type="1">
<li>The learning algorithm need not return a function from the learnt class.</li>
<li>The polynomial-time requirement means in particular that the learning algorithm cannot output a complete truth table, as its size would be exponential. Instead, it must output a short description of a hypothesis that can be evaluated in polynomial time.</li>
</ol>
<p>For a taste of the computational learning theory literature, here are some of the function classes studied by theorists over the years:</p>
<ol type="1">
<li><em>Linear thresholds (halfspaces):</em> functions that map a halfspace to 1 and its complement to -1. Formally, functions of the form <img alt="x \mapsto \theta(\langle w, x \rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+%5Ctheta%28%5Clangle+w%2C+x+%5Crangle%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x \mapsto \theta(\langle w, x \rangle)"/> for some <img alt="w \in \mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=w+%5Cin+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w \in \mathbb{R}^n"/>, where <img alt="\theta(z) = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta%28z%29+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\theta(z) = 1"/> when <img alt="z &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=z+%3E+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z &gt; 0"/> and <img alt="\theta(z) = -1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta%28z%29+%3D+-1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\theta(z) = -1"/> when <img alt="z \le 0" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Cle+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z \le 0"/>.</li>
<li><em>Large-margin linear thresholds:</em> for<img alt="\rho(z) = \begin{cases} 1 &amp; z \ge 1 \\ * &amp; -1 \le z \le 1 \\ -1 &amp; z \le -1 \end{cases}," class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%28z%29+%3D+%5Cbegin%7Bcases%7D+1+%26+z+%5Cge+1+%5C%5C+%2A+%26+-1+%5Cle+z+%5Cle+1+%5C%5C+-1+%26+z+%5Cle+-1+%5Cend%7Bcases%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\rho(z) = \begin{cases} 1 &amp; z \ge 1 \\ * &amp; -1 \le z \le 1 \\ -1 &amp; z \le -1 \end{cases},"/>the class<img alt="\left\{ h : \{\pm 1\}^n \to \{\pm 1\} \mid h(x) = \rho(\langle w,x \rangle) \text{ with } \|w\|_2^2 \le \mathrm{poly}(n) \right\}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7B+h+%3A+%5C%7B%5Cpm+1%5C%7D%5En+%5Cto+%5C%7B%5Cpm+1%5C%7D+%5Cmid+h%28x%29+%3D+%5Crho%28%5Clangle+w%2Cx+%5Crangle%29+%5Ctext%7B+with+%7D+%5C%7Cw%5C%7C_2%5E2+%5Cle+%5Cmathrm%7Bpoly%7D%28n%29+%5Cright%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\left\{ h : \{\pm 1\}^n \to \{\pm 1\} \mid h(x) = \rho(\langle w,x \rangle) \text{ with } \|w\|_2^2 \le \mathrm{poly}(n) \right\}."/></li>
<li><em>Intersections of halfspaces:</em> functions that map an intersection of polynomially many halfspaces to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="1"/> and its complement to <img alt="-1" class="latex" src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="-1"/>.</li>
<li><em>Polynomial threshold functions:</em> thresholds of constant-degree polynomials.</li>
<li><em>Large-margin polynomial threshold functions:</em> the class</li>
</ol>
<p><img alt="\left\{ h : \{\pm 1\}^n \to \{\pm 1\} \mid h(x) = \rho\left( \sum_{A \subset [n], |A| \le O(1)} \alpha_A \prod_{i \in A} x_i \right) \;\text{ with }\; \sum_{A} \alpha^2_A \le \mathrm{poly}(n) \right\}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7B+h+%3A+%5C%7B%5Cpm+1%5C%7D%5En+%5Cto+%5C%7B%5Cpm+1%5C%7D+%5Cmid+h%28x%29+%3D+%5Crho%5Cleft%28+%5Csum_%7BA+%5Csubset+%5Bn%5D%2C+%7CA%7C+%5Cle+O%281%29%7D+%5Calpha_A+%5Cprod_%7Bi+%5Cin+A%7D+x_i+%5Cright%29+%5C%3B%5Ctext%7B+with+%7D%5C%3B+%5Csum_%7BA%7D+%5Calpha%5E2_A+%5Cle+%5Cmathrm%7Bpoly%7D%28n%29+%5Cright%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\left\{ h : \{\pm 1\}^n \to \{\pm 1\} \mid h(x) = \rho\left( \sum_{A \subset [n], |A| \le O(1)} \alpha_A \prod_{i \in A} x_i \right) \;\text{ with }\; \sum_{A} \alpha^2_A \le \mathrm{poly}(n) \right\}."/></p>
<ol type="1">
<li><em>Decision trees</em>, <em>deterministic automata</em>, and <em><a href="https://en.wikipedia.org/wiki/Disjunctive_normal_form">DNF</a> formulas</em> of polynomial size.</li>
<li><em>Monotone conjunctions:</em> functions that, for some <img alt="A \subset [n]" class="latex" src="https://s0.wp.com/latex.php?latex=A+%5Csubset+%5Bn%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A \subset [n]"/> map <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x"/> to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="1"/> if <img alt="x_i = 1" class="latex" src="https://s0.wp.com/latex.php?latex=x_i+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x_i = 1"/> for all <img alt="i \in A" class="latex" src="https://s0.wp.com/latex.php?latex=i+%5Cin+A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i \in A"/>, and to <img alt="-1" class="latex" src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="-1"/> otherwise.</li>
<li><em>Parities:</em> functions of the form <img alt="x \mapsto \prod_{i \in A} x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+%5Cprod_%7Bi+%5Cin+A%7D+x_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x \mapsto \prod_{i \in A} x_i"/> for some <img alt="A \subset [n]" class="latex" src="https://s0.wp.com/latex.php?latex=A+%5Csubset+%5Bn%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A \subset [n]"/>.</li>
<li><em>Juntas:</em> functions that depend on at most <img alt="\log(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\log(n)"/> variables.</li>
</ol>
<p>Learning theorists look at these function classes and work to distinguish those that are efficiently learnable from those that are <em>hard</em> to learn. They establish hardness results by reduction from other computational problems that are conjectured to be hard, such as random XOR-SAT (though none today are conditioned outright on NP hardness); see for example <a href="https://arxiv.org/abs/1404.3378">these</a> <a href="https://arxiv.org/abs/1505.05800">two</a> results. Meanwhile, halfspaces are learnable by linear programming. Parities, or more generally, <img alt="\mathbb{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathbb{F}"/>-linear functions for a field <img alt="\mathbb{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathbb{F}"/>, are learnable by Gaussian elimination. In turn, via reductions, many other classes are efficiently learnable. This includes polynomial thresholds, decision lists, and more. To give an idea of what’s known in the literature, here is an artist’s depiction of some of what’s currently known:</p>
<figure><img alt="classes" class=" size-full wp-image-1477 aligncenter" src="https://theorydish.files.wordpress.com/2019/01/classes.png?w=620"/>Learnable and conjectured hard-to-learn function classes</figure>
<p> </p>
<p>At a high-level, the upshot from all of this—and if you take away just one thing from this quick tour of PAC—is that:</p>
<blockquote><p>Barring a small handful of exceptions, all known efficiently learnable classes can be reduced to halfspaces or <img alt="\mathbb{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathbb{F}"/>-linear functions.</p></blockquote>
<p>Or, to put it more bluntly, <strong>the state of the art in PAC-learnability is essentially linear prediction</strong>.</p>
<h2 id="pac-analyzing-neural-nets">PAC analyzing neural nets</h2>
<p>Research in algorithms and complexity often follows these steps:</p>
<ol type="1">
<li>define a computational problem,</li>
<li>design an algorithm that solves it, and then</li>
<li>establish bounds on the resource requirements of that algorithm.</li>
</ol>
<p>A bound on the algorithm’s performance forms, in turn, a bound on the <em>computational problem’s</em> inherent complexity.</p>
<p>By contrast, we have already decided on our SGDNN algorithm, and we’d like to attain some grasp on its capabilities. So we’d like to do things in a different order:</p>
<ol type="1">
<li>define an <em>algorithm</em> (done),</li>
<li>design a computational problem to which the algorithm can be applied, and then</li>
<li>establish bounds on the resource requirements of the algorithm in solving the problem.</li>
</ol>
<p>Our computational problem will be a PAC learning problem, corresponding to a function class. For SGDNN, an ambitious function class we might consider is the class of all functions realizable by the network. But if we were to follow this approach, we would run up against the same hardness results mentioned before.</p>
<p>So instead, we’ve established the theorem stated at the top of this post. That is, that SGDNN, over a range of network configurations, learns a class that we <em>already know</em> to be learnable: large margin polynomial thresholds. Restated:</p>
<blockquote><p><strong>Theorem, again:</strong> There is a choice of SGDNN step size <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\eta"/> and number of steps <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="T"/>, as well as a with parameter <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="r"/>, where <img alt="T, r \le \mathrm{poly}(n/\epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=T%2C+r+%5Cle+%5Cmathrm%7Bpoly%7D%28n%2F%5Cepsilon%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="T, r \le \mathrm{poly}(n/\epsilon)"/>, such that SGDNN on a multi-layer perceptron of depth between 2 and <img alt="\log(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\log(n)"/>, and of width<a class="footnoteRef" href="https://theorydish.blog/feed/#fn8" id="fnref8"><sup>8</sup></a> <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="r"/>, learns large magin polynomials.</p></blockquote>
<p>How rich are large margin polynomials? They contain disjunctions, conjunctions, DNF and <a href="https://en.wikipedia.org/wiki/Conjunctive_normal_form">CNF</a> formulas with a constant many terms, DNF and CNF formulas with a constant many literals in each term. By corollary, SGDNN can PAC learn these classes as well. And at this point, we’ve covered a considerable fraction of the function classes known to be poly-time PAC learnable by <em>any</em> method.</p>
<p>Exceptions include constant-degree polynomial thresholds with no restriction on the coefficients, decision lists, and parities. It is well known that SGDNN cannot learn parities, and in ongoing work with Vitaly Feldman, we show that SGDNN cannot learn decision lists nor constant-degree polynomial thresholds with unrestricted coefficients. So the picture becomes more clear:</p>
<figure><img alt="classes_nn" class=" size-full wp-image-1476 aligncenter" src="https://theorydish.files.wordpress.com/2019/01/classes_nn.png?w=620"/>Conjectured hard-to-learn classes, known learnable classes, and those known to be learnable by SGDNN.</figure>
<p> </p>
<p>The theorem above runs SGDNN with a multi-layer perceptron. What happens if we change the network architecture? It can be shown then that SGDNN learns a qualitatively different function class. For instance, with convolutional networks, the learnable functions include certain polynomials of <em>super-constant</em> degree.</p>
<h3 id="a-word-on-the-proof">A word on the proof</h3>
<p>The path to the theorem traverses two papers. There’s a corresponding outline for the proof.</p>
<p>The first step is to show that, with high probability, the Glorot random initialization renders the network in a state where the final hidden layer (just before the output node) is rich enough to approximate all large-margin polynomial threshold functions (LMPTs). Namely, every LMPT can be approximated by the network up to some setting of the weights that enter the output neuron (all remaining weights random). The tools for this part of the proof include (i) the connection between kernels and random features, (ii) a characterization of symmetric kernels of the sphere, and (iii) a variety of properties of Hermite polynomials. It’s described in our <a href="https://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity">2016 paper</a>.</p>
<p>An upshot of this correspondence is that if we run SGD <em>only on the top layer</em> of a network, leaving the remaining weights as they were randomly initialized, we learn LMPTs. (Remember when we said that we won’t beat what a linear predictor can do? There it is again.) The second step of the proof, then, is to show that the correspondence continues to hold even if we train all the weights. In the assumed setting (e.g. provided at most logarithmic depth, sufficient width, and so forth), what’s represented in the final hidden layer changes sufficiently slowly that, over the course of SGDNN’s iterations, it <em>remains</em> rich enough to approximate all LMPTs. The final layer does the remaining work of picking out the right LMPT. The argument is in Amit’s <a href="https://papers.nips.cc/paper/6836-sgd-learns-the-conjugate-kernel-class-of-the-network">2017 paper</a>.</p>
<h2 id="pacing-up">PACing up</h2>
<p>To what extent should we be satisfied, knowing that our algorithm of interest (SGDNN) can solve a (computationally) easy problem?</p>
<p>On the positive side, we’ve managed to say something at all about neural network training in the PAC framework. Roughly speaking, some class of non-trivially layered neural networks, trained as they typically are, learns any known learnable function class that isn’t “too sensitive.” It’s also appealing that the function classes vary across different architectures.</p>
<p>On the pessimistic side, we’re confronted to a major limitation on the “function class” perspective, prevalent in PAC analysis and elsewhere in learning theory. All of the classes that SGDNN learns, <em>under the assumptions</em> touched on in this post, are so-called large-margin classes. Large-margin classes are essentially linear predictors over a <em>fixed and data-independent</em> embedding of input examples, as alluded to before. These are inherently “shallow models.”</p>
<p>That seems rather problematic in pursuing any kind of theory for learning layered networks, where the entire working premise is that a deep network uses its hidden layers to learn a representation adapted to the example domain. Our analysis—both its goal and its proof—clash with this intuition: it works out that a “shallow model” can be learned when assumptions imply that “not too much” change takes place in hidden layers. It seems that the representation learning phenomenon is what’s interesting, yet the typical PAC approach, as well as the analysis touched on in this post, all avoid capturing it.</p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Here <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n"/> is the dimension of the instance space.<a href="https://theorydish.blog/feed/#fnref1"><img alt="&#x21A9;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" style="height: 1em;"/></a></li>
<li id="fn2">For instance, ReLU activations, of the form <img alt="x \mapsto \max\{x,0\}" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+%5Cmax%5C%7Bx%2C0%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x \mapsto \max\{x,0\}"/>.<a href="https://theorydish.blog/feed/#fnref2"><img alt="&#x21A9;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" style="height: 1em;"/></a></li>
<li id="fn3">Recurrent networks allow for cycles, but in this post we stick to DAGs.<a href="https://theorydish.blog/feed/#fnref3"><img alt="&#x21A9;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" style="height: 1em;"/></a></li>
<li id="fn4">Convolutional networks often also constrain subsets of their weights to be equal; that turns out not to bear much on this post.<a href="https://theorydish.blog/feed/#fnref4"><img alt="&#x21A9;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" style="height: 1em;"/></a></li>
<li id="fn5">Although not essential to the results described, it also simplifies this post to zero the weights on edges incident to the output node as part of the initialization.<a href="https://theorydish.blog/feed/#fnref5"><img alt="&#x21A9;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" style="height: 1em;"/></a></li>
<li id="fn6"><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants">Variants of SGD</a> are used in practice, including algorithms used elsewhere in optimization (e.g. <a href="https://distill.pub/2017/momentum/">SGD with momentum</a>, <a href="http://www.jmlr.org/papers/v12/duchi11a.html">AdaGrad</a>) or techniques developed more specifically for neural nets (e.g. RMSprop, <a href="https://arxiv.org/abs/1412.6980">Adam</a>, <a href="https://arxiv.org/abs/1502.03167">batch norm</a>). We’ll stick to plain SGD.<a href="https://theorydish.blog/feed/#fnref6"><img alt="&#x21A9;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" style="height: 1em;"/></a></li>
<li id="fn7">More accurately, a sequence of function classes <img alt="\mathcal H_n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+H_n&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal H_n"/> for <img alt="n = 1, 2, \ldots" class="latex" src="https://s0.wp.com/latex.php?latex=n+%3D+1%2C+2%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n = 1, 2, \ldots"/>.<a href="https://theorydish.blog/feed/#fnref7"><img alt="&#x21A9;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" style="height: 1em;"/></a></li>
<li id="fn8">The width of a multi-layer perceptron is the number of neurons in each hidden layer.<a href="https://theorydish.blog/feed/#fnref8"><img alt="&#x21A9;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" style="height: 1em;"/></a></li>
</ol>
</section></div>
    </content>
    <updated>2019-01-04T15:14:02Z</updated>
    <published>2019-01-04T15:14:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>amitdanielymailhujiacil</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-01-07T22:43:20Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42145</id>
    <link href="https://cstheory.stackexchange.com/questions/42145/grid-minor-theorem-of-robertson-and-seymour-and-its-algorithmic-applications" rel="alternate" type="text/html"/>
    <title>Grid-Minor Theorem of Robertson and Seymour and its Algorithmic Applications</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Graph-Minor Theorem of Robertson and Seymour [<a href="https://www.sciencedirect.com/science/article/pii/S0095895684710732" rel="nofollow noreferrer">1</a>] states that if graph G has large treewidth, then it contains a large grid as minor. Most approximation results on general classes of graphs with excluded minors make heavy use of Robertson and Seymour’s structure theory for graphs with excluded minors, especially when the treewidth is large (small treewidth usually makes problem to be easily solved by dynamic programming) [<a href="http://chekuri.cs.illinois.edu/talks/NIPS-Tutorial.pdf" rel="nofollow noreferrer">2</a>]. </p>

<p>However, there are some results are trying to avoid using the grid minor theorem. For example, Chekuri and Chuzhoy [<a href="https://arxiv.org/abs/1304.1577" rel="nofollow noreferrer">3</a>] show a framework for using theorems to bypass the well-known Grid-Minor Theorem of Robertson and Seymour in some applications. In particular, this leads to substantially improved parameters in some Erdos-Posa-type results, and faster running times for algorithms for some fi�xed parameter tractable problems.</p>

<p>Do you know any other examples of problems with large treewidth avoid using the grid minor theorem? </p></div>
    </summary>
    <updated>2019-01-04T09:44:58Z</updated>
    <published>2019-01-04T09:44:58Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="graph-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="approximation-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="treewidth"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="graph-minor"/>
    <author>
      <name>Rupei Xu</name>
      <uri>https://cstheory.stackexchange.com/users/17918</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42144</id>
    <link href="https://cstheory.stackexchange.com/questions/42144/maximum-weight-independent-set-on-a-changing-graph" rel="alternate" type="text/html"/>
    <title>Maximum Weight Independent Set on a Changing Graph?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Suppose I have an optimal solution to the maximum weight independent/stable set problem on an arbitrary graph. If I were to induce a clique among a subset of its vertices (and perhaps add in some additional nodes that are only adjacent to the nodes of the induced clique), does there exist an efficient way in which I use the original optimal solution (i.e. its structure as a starting solution) to find the new optimal maximum weight independent set in the modified graph??</p></div>
    </summary>
    <updated>2019-01-04T07:48:21Z</updated>
    <published>2019-01-04T07:48:21Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="graph-theory"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="graph-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="co.combinatorics"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="optimization"/>
    <author>
      <name>Student</name>
      <uri>https://cstheory.stackexchange.com/users/51578</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42140</id>
    <link href="https://cstheory.stackexchange.com/questions/42140/explanation-of-monadic-second-order-logic" rel="alternate" type="text/html"/>
    <title>Explanation of Monadic Second Order Logic [on hold]</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I am reading Wolfgang's Book <a href="https://drona.csa.iisc.ac.in/~deepakd/atc-common/wolfgang-aat.pdf" rel="nofollow noreferrer">Applied Automata Theory </a>, wherein, I came across what Monadic Second Order Logic means. </p>

<blockquote>
  <p>MSO stands for “monadic second-order”:
  Second-order because it allows quantification not only over (first-order) position
  variables but also over (second-order) set variables.
  Monadic because quantification is allowed at most over unary (monadic) relations,
  namely sets.</p>
</blockquote>

<p>I have a fundamental question , how do position variables become first order variables, and how are set variables second order? I am not able to go further, since I cannot wrap my head around this. </p></div>
    </summary>
    <updated>2019-01-03T13:49:09Z</updated>
    <published>2019-01-03T13:49:09Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="lo.logic"/>
    <author>
      <name>GermanShepherd</name>
      <uri>https://cstheory.stackexchange.com/users/30360</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42135</id>
    <link href="https://cstheory.stackexchange.com/questions/42135/strong-seeded-randomness-extractors-with-low-entropy-loss" rel="alternate" type="text/html"/>
    <title>Strong seeded randomness extractors with low entropy loss</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I would like to implement a strong seeded randomness extractor for flat sources as a part of my project. </p>

<p>Most of the literature on seeded extractors is concentrated on minimizing seed length. However, low entropy loss is crucial for my construction. What are the known extractors with minimal entropy loss? How efficient is the extractor in practice? </p>

<p>Is there a lower bound on the entropy loss for strong seeded extractors? </p>

<p>Are there any implementations of extractors that I can use off the shelf?</p></div>
    </summary>
    <updated>2019-01-02T16:48:50Z</updated>
    <published>2019-01-02T16:48:50Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="lower-bounds"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="randomness"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="pseudorandomness"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="extractors"/>
    <author>
      <name>satya</name>
      <uri>https://cstheory.stackexchange.com/users/51635</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42122</id>
    <link href="https://cstheory.stackexchange.com/questions/42122/what-to-do-as-a-theoretical-computer-science-phd-student-in-a-free-time" rel="alternate" type="text/html"/>
    <title>What to do as a Theoretical computer science PhD student in a free time?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I am a mid-stage theoretical computer science student. Although I have a busy schedule, I still have a one or one a half hour in a day which I devote to reading and solving the question given Jeff Erickson's lecture note etc. I am doing this thing from many months and wondering. Is this a right thing for me to do in free time as now I am a Ph.D. student not an undergraduate student. Now why I do this to become more strong in an algorithm, discrete maths etc part. Another thing which seems more valuable to me is to read more and more research paper of my research domain as my goal after my Ph.D. is to publish more quality research papers in the field related to my current field. I am wondering which one is better or suggest anything else which may be more valuable to me keeping my future perspective in mind.</p>

<p><strong>Question:</strong> What to do as a Theoretical computer science PhD student in free time? I am wondering what star experienced researchers do in their time ( assuming they have a free time ).</p>

<p>Some of my free time I also spent on watching video lecture of workshops related to my field.</p>

<p>After looking at all the comments and answers, I have to edit my question. I think, I have not been able to convey what I was trying to ask. My question was how to sharp my technical skills in the free time for a better future. It has nothing to do with my personal life or some one's personal space. I was here for the various possibilities and opinions of users, who have experience in theoretical computer science. Let me clarify my question further, I was to trying to ask in the free time what is more significant to do " continue to think about the research problem at hand or do the problems related to maths or algorithms " and so on. Looking at the comments have made me realise that definitely I need to improve my writing skills also. </p></div>
    </summary>
    <updated>2018-12-31T12:15:55Z</updated>
    <published>2018-12-31T12:15:55Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="soft-question"/>
    <author>
      <name>A_Theory</name>
      <uri>https://cstheory.stackexchange.com/users/49003</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42120</id>
    <link href="https://cstheory.stackexchange.com/questions/42120/is-a-binary-sequence-computable-iff-the-kolmogorov-complexity-of-its-initial-seg" rel="alternate" type="text/html"/>
    <title>Is a binary sequence computable iff the Kolmogorov complexity of its initial segments is bounded?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Disclaimer:</strong> I am mostly unfamiliar with theoretical computer science, making it hard for me to navigate literature in the field. I ask the following out of curiosity.</p>

<p><strong>Background/Motivation:</strong> Coming from information theory, I recently learned about a connection of entropy and Kolmogorov complexity: Loosely speaking, entropy of a random variable is the expected rate at which the Kolmogorov complexity of a long sample sequence increases per sample. <a href="http://www.cs-114.org/wp-content/uploads/2015/01/Elements_of_Information_Theory_Elements.pdf" rel="nofollow noreferrer">[Elements of Information Theory, p. 154]</a> Kolmogorov complexity can therefore capture the notion of entropy, but it is more general than that. Hereby, and in the following, whenever I write complexity, I implicitly refer to the complexity given the length of the output.</p>

<p>For non-zero entropy, the Kolmogorov complexity of initial segments of an infinite sequence of samples from a random variable is therefore unbounded. I was wondering whether this is equivalent to the fact that an infinite sequence of samples is uncomputable. This led me to the hypothesis in the title: Is a binary sequence computable if and only if the Kolmogorov complexity of its initial segments is bounded?</p>

<p>If the hypothesis was true, then computability could be understood as an indicator that the "amount of information" in a sequence is finite. In some sense, the initial segment complexities would allow a more finely graded characterization of infinite sequences than just "computable" and "uncomputable". We could get a notion of "information content" and "information rate" of infinite sequences by analyzing the size of the bound or, in the unbounded case, the rate/type of growth, as in the entropy case above. My question boils down to whether "computable" and "uncomputable" are regions on this scale.</p>

<p>If the hypothesis is true, I'd be interested in whether this perspective is useful for TCS research. If yes, are there references elaborating this idea? If not, why not?</p>

<p><strong>What I found in literature:</strong> It is shown that a sequence is Martin-Löf random iff there is a constant <span class="math-container">$c$</span> so that there are infinitely many initial segments with complexity greater than <span class="math-container">$n - c$</span> where <span class="math-container">$n$</span> is the segment length. <a href="https://arxiv.org/pdf/math/0110086.pdf" rel="nofollow noreferrer">[Randomness, p. 18]</a></p>

<p>This means that random sequences have unbounded initial segment complexity. Since they are not computable, the hypothesis is true at least for this case. If I am not mistaken, a similar argument could even be made for a weaker form of randomness, since Mises-Wald-Church random sequences cannot have initial segment complexity of O(log n). <a href="https://www.math.uni-heidelberg.de/logic/merkle/ps/JCSS-stoch.pdf" rel="nofollow noreferrer">[The complexity of stochastic sequences]</a></p>

<p><strong>What's missing for a proof:</strong></p>

<p><span class="math-container">$\Leftarrow$</span>:
Assume a sequence is computable. We know that a program <code>generate_bit(n)</code> exists that generates any bit of the sequence. Now, we can build a program <code>generate_initial_segment(n) = concat(map(1..n, generate_bit))</code> that, given the segment length <span class="math-container">$n$</span>, generates the initial segment up to position n by invoking <code>generate_bit</code> <span class="math-container">$n$</span> times and concatenating the results. The Kolmogorov complexity of this task is therefore bounded by the length of this program. ☐</p>

<p><span class="math-container">$\Rightarrow$</span>: I struggle to prove/disprove this direction, namely: If initial segment complexity is bounded, is a sequence always computable?</p>

<p>Update: The last two pages of <a href="https://www.sciencedirect.com/science/article/pii/S0019995869905385/pdf?md5=5ff60459e171a92caef1156280e1ce2c&amp;pid=1-s2.0-S0019995869905385-main.pdf" rel="nofollow noreferrer">A variant of the Kolmogorov concept of complexity</a> prove this direction.</p></div>
    </summary>
    <updated>2018-12-31T09:22:03Z</updated>
    <published>2018-12-31T09:22:03Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="computability"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="it.information-theory"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="kolmogorov-complexity"/>
    <author>
      <name>Julius Kunze</name>
      <uri>https://cstheory.stackexchange.com/users/51614</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42104</id>
    <link href="https://cstheory.stackexchange.com/questions/42104/does-a-given-regular-language-contain-an-infinite-prefix-free-subset" rel="alternate" type="text/html"/>
    <title>Does a given regular language contain an infinite prefix-free subset?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A set of words over a finite alphabet is <em>prefix-free</em> if there are no two distinct words where one is a prefix of the other.</p>

<p>The question is: </p>

<p><strong>What is the complexity of checking whether a regular language given as an NFA contains an infinite prefix-free subset?</strong></p>

<p><strong>Answer (due to Mikhail Rudoy, here below)</strong>: It can be done in polynomial time, and I think in even in NL. </p>

<p>Paraphrasing Mikhail's answer, let <span class="math-container">$(\Sigma,q_0,F,\delta)$</span> be the input NFA in the normal form (no epsilon transitions, trim), and let <span class="math-container">$L[p,r]$</span> (resp. <span class="math-container">$L[p,R]$</span>) be the language obtained by having state <span class="math-container">$r$</span> as initial state and <span class="math-container">$\{s\}$</span> as final state (resp. state <span class="math-container">$r$</span> as inital and the set <span class="math-container">$S$</span> as final). For a word <span class="math-container">$u$</span> let <span class="math-container">$u^\omega$</span> be the infinite word obtained by iterating <span class="math-container">$u$</span>.</p>

<p>The following are equivalent:</p>

<ol>
<li>The language <span class="math-container">$L[q_0,F]$</span> contains an infinite prefix-free subset.</li>
<li><span class="math-container">$\exists q \in Q$</span>, <span class="math-container">$\exists u \in L[q,q]$</span> <span class="math-container">$\exists v \in L[q,F]$</span> so that <span class="math-container">$v$</span> is not a prefix of <span class="math-container">$u^\omega$</span>.</li>
<li><span class="math-container">$\exists q \in Q$</span> <span class="math-container">$L[q,q] \neq \emptyset$</span> <span class="math-container">$\forall u \in L[q,q]$</span> <span class="math-container">$\exists v \in L[q,F]$</span> so that <span class="math-container">$v$</span> is not a prefix of <span class="math-container">$u^\omega$</span>.</li>
</ol>

<p>Proof:</p>

<p>3<span class="math-container">$\Rightarrow$</span>2 trivial.</p>

<p>For 2<span class="math-container">$\Rightarrow$</span>1, it suffices to see that for any <span class="math-container">$w \in L[q_0,q]$</span> we have that <span class="math-container">$w (u^{|v|})^* v$</span> is an infinite prefix-free subset of <span class="math-container">$L[q_0,F]$</span>.</p>

<p>Finally, 1<span class="math-container">$\Rightarrow$</span>3 is the "correctness" proof in Mikhail's answer.</p></div>
    </summary>
    <updated>2018-12-27T15:03:12Z</updated>
    <published>2018-12-27T15:03:12Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="fl.formal-languages"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="automata-theory"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="prefix-free-code"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="nfa"/>
    <author>
      <name>Googlo</name>
      <uri>https://cstheory.stackexchange.com/users/49964</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5365</id>
    <link href="https://adamsheffer.wordpress.com/2018/12/20/incidences-in-a-recent-work-of-walsh/" rel="alternate" type="text/html"/>
    <title>Incidences in a Recent Work of Walsh</title>
    <summary>Recently, Miguel Walsh posted a very interesting paper on arXiv. The main purpose of the paper is to study various properties of polynomials and varieties. These properties are related to incidence problems – some originally arose from studying incidences. Walsh also presents new incidence bounds as applications of his results. In this post I’ll briefly […]</summary>
    <updated>2018-12-20T17:26:10Z</updated>
    <published>2018-12-20T17:26:10Z</published>
    <category term="Incidences"/>
    <category term="Recent Results"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2019-01-07T22:39:16Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42139</id>
    <link href="https://cstheory.stackexchange.com/questions/42139/why-is-the-general-notion-of-a-reduction-inherent-to-the-notion-of-self-r" rel="alternate" type="text/html"/>
    <title>Why is the "general notion of a reduction [...] inherent to the notion of self-reducibility"?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>While reading "Computational Complexity: A Conceptual Perspective" by Oded Goldreich, I have come across the following passage, which I simply cannot get my head around:</p>

<blockquote>
  <p>Note that the general notion of a reduction (i.e., Cook-reduction) seems inherent to the notion of self-reducibility. This is the case not only due to syntactic considerations, but also due to the following inherent reason. An oracle to any decision problem returns a single bit per invocation, while the intractability of a search problem in <span class="math-container">$\mathcal{PC}$</span> must be due to lacking more than a "single bit of information" [...].</p>
</blockquote>

<p>For those unfamiliar with Goldreich's text, <em>self-reducibility</em> is used in the sense that a search problem <span class="math-container">$R$</span> is Cook-reducible to deciding membership in the respective solution set (i.e., the problem "given <span class="math-container">$x$</span>, is <span class="math-container">$x$</span> a solution to <span class="math-container">$R$</span>?"). <span class="math-container">$\mathcal{PC}$</span> is the class of polynomially verifiable search problems. The text in this chapter appears to be an expanded version of an article intitled "On Teaching the Basics of Complexity Theory" by the same author (<a href="https://link.springer.com/chapter/10.1007/11685654_15" rel="noreferrer">Springer</a>, <a href="http://www.wisdom.weizmann.ac.il/~oded/PSX/cc-teach-r2.pdf" rel="noreferrer">non-paywall link</a>).</p>

<p>Regarding the last quoted sentence, the text refers to an exercise to prove that, for any search problem which is in <span class="math-container">$\mathcal{PC}$</span> but not in <span class="math-container">$\mathcal{PF}$</span> (i.e., the class of search problems for which the solution can be found in poly-time) and which is self-reducible, the respective (Cook-)reduction performs at least (asymptotically) log queries to its oracle. (The proof is also not hard; if using only log queries, then the oracle may be replaced by a brute force subroutine without affecting the reduction's poly-time complexity.)</p>

<p>My question is two-fold:</p>

<ol>
<li>What does Goldreich mean by "syntactic considerations"? Does he simply mean it is cumbersome to define self-reducibility by other means than using reductions?</li>
<li>What relation does the exercise referred to in the text have to the claim regarding "lacking more than a 'single bit of information'"? (After all, the exercise is about log many queries, not a single one.) And what does this have to do with the inherentness of reductions to self-reducibility?</li>
</ol>

<p>I assume Goldreich is alluding to widely-known facts here. Apologies if this can only be answered by the man himself.</p></div>
    </summary>
    <updated>2018-12-20T08:56:05Z</updated>
    <published>2018-12-20T08:56:05Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="reductions"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="oracles"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="polynomial-time"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="search-problem"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="complexity-theory"/>
    <author>
      <name>dkaeae</name>
      <uri>https://cstheory.stackexchange.com/users/45242</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1470</id>
    <link href="https://theorydish.blog/2018/12/18/2019-godel-prize/" rel="alternate" type="text/html"/>
    <title>2019 Gödel Prize</title>
    <summary>If I write a post and the blog aggregator is down, does it still make a sound?   The call for nomination for the 2019 Gödel Prize is out and the deadline is February 15th. For all awards, we sometimes have the tendency to think that worthy candidates have surely been nominated by others. Often it is not the case (and thus worthy candidates are often left behind). So if there is a paper or papers deserving nomination, please nominate! The call for nomination is below.   The Gödel Prize 2019 – Call for Nominations Deadline: February 15, 2019 The Gödel Prize for outstanding papers in the area of theoretical computer science is sponsored jointly by the European Association for Theoretical Computer Science (EATCS) and the Association for Computing Machinery, Special Interest Group on Algorithms and Computation Theory (ACM SIGACT). The award is presented annually, with the presentation taking place alternately at the International Colloquium on Automata, Languages, and Programming (ICALP) and the ACM Symposium on Theory of Computing (STOC). The 27th Gödel Prize will be awarded at 51st Annual ACM Symposium on the Theory of Computing to be held during June 23-26, 2019 in Phoenix, AZ. The Prize is [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>If I write a post and the blog aggregator is down, does it still make a sound?</p>
<hr/>
<p> </p>
<p>The call for nomination for the 2019 Gödel Prize is out and the deadline is February 15th. For all awards, we sometimes have the tendency to think that worthy candidates have surely been nominated by others. Often it is not the case (and thus worthy candidates are often left behind). So if there is a paper or papers deserving nomination, please nominate! The call for nomination is below.</p>
<hr/>
<p> </p>
<h1>The Gödel Prize 2019 – Call for Nominations</h1>
<p>Deadline: February 15, 2019</p>
<p>The Gödel Prize for outstanding papers in the area of theoretical computer science is sponsored jointly by the European Association for Theoretical Computer Science (EATCS) and the Association for Computing Machinery, Special Interest Group on Algorithms and Computation Theory (ACM SIGACT). The award is presented annually, with the presentation taking place alternately at the International Colloquium on Automata, Languages, and Programming (ICALP) and the ACM Symposium on Theory of Computing (STOC). The 27th Gödel Prize will be awarded at 51st Annual ACM Symposium on the Theory of Computing to be held during June 23-26, 2019 in Phoenix, AZ. The Prize is named in honor of Kurt Gödel in recognition of his major contributions to mathematical logic and of his interest, discovered in a letter he wrote to John von Neumann shortly before von Neumann’s death, in what has become the famous “P versus NP” question. The Prize includes an award of USD 5,000.</p>
<p><strong>Award Committee: </strong>The 2019 Award Committee consists of Anuj Dawar (Cambridge University), Robert Krauthgamer (Weizmann Institute), Joan Feigenbaum (Yale University), Giuseppe Persiano (Università di Salerno), Omer Reingold (Chair, Stanford University) and Daniel Spielman (Yale University).</p>
<p><strong>Eligibility:</strong> The 2019 Prize rules are given below and they supersede any different interpretation of the generic rule to be found on websites of both SIGACT and EATCS. Any research paper or series of papers by a single author or by a team of authors is deemed eligible if: – The main results were not published (in either preliminary or final form) in a journal or conference proceedings before January 1st, 2006. – The paper was published in a recognized refereed journal no later than December 31, 2018. The research work nominated for the award should be in the area of theoretical computer science. Nominations are encouraged from the broadest spectrum of the theoretical computer science community so as to ensure that potential award winning papers are not overlooked. The Award Committee shall have the ultimate authority to decide whether a particular paper is eligible for the Prize.</p>
<p><strong>Nominations:</strong></p>
<p>Nominations for the award should be submitted by email to the Award Committee Chair: <a href="mailto:reingold@stanford.edu">reingold@stanford.edu</a>. Please make sure that the Subject line of all nominations and related messages begin with “Goedel Prize 2019.” To be considered, nominations for the 2019 Prize must be received by February 15, 2019.</p>
<p>A nomination package should include:</p>
<p>1. A printable copy (or copies) of the journal paper(s) being nominated, together with a complete citation (or citations) thereof.</p>
<p>2. A statement of the date(s) and venue(s) of the first conference or workshop publication(s) of the nominated work(s) or a statement that no such publication has occurred.</p>
<p>3. A brief summary of the technical content of the paper(s) and a brief explanation of its significance.</p>
<p>4. A support letter or letters signed by at least two members of the scientific community.</p>
<p>Additional support letters may also be received and are generally useful. The nominated paper(s) may be in any language. However, if a nominated publication is not in English, the nomination package must include an extended summary written in English.</p>
<p>Those intending to submit a nomination should contact the Award Committee Chair by email well in advance. The Chair will answer questions about eligibility, encourage coordination among different nominators for the same paper(s), and also accept informal proposals of potential nominees or tentative offers to prepare formal nominations. The committee maintains a database of past nominations for eligible papers, but fresh nominations for the same papers (especially if they highlight new evidence of impact) are always welcome.</p>
<p><strong>Selection Process:</strong></p>
<p>The Award Committee is free to use any other sources of information in addition to the ones mentioned above. It may split the award among multiple papers, or declare no winner at all. All matters relating to the selection process left unspecified in this document are left to the discretion of the Award Committee.</p>
<p><strong>Recent Winners</strong></p>
<p>(all winners since 1993 are listed at <a href="http://www.sigact.org/Prizes/Godel/">http://www.sigact.org/Prizes/Godel/</a> and <a href="http://eatcs.org/index.php/goedel-prize">http://eatcs.org/index.php/goedel-prize</a>):</p>
<p><strong>2018:</strong> Oded Regev, On lattices, learning with errors, random linear codes, and cryptography, Journal of the ACM (JACM), Volume 56 Issue 6, 2009 (preliminary version in Symposium on Theory of Computing, STOC 2005).</p>
<p><strong>2017:</strong> Cynthia Dwork, Frank McSherry, Kobbi Nissim and Adam Smith, Calibrating Noise to Sensitivity in Private Data Analysis, Journal of Privacy and Confidentiality, Volume 7, Issue 3, 2016 (preliminary version in Theory of Cryptography, TCC 2006).</p>
<p><strong>2016:</strong> Stephen Brookes, A Semantics for Concurrent Separation Logic. Theoretical Computer Science 375(1-3): 227-270 (2007). Peter W. O’Hearn, Resources, Concurrency, and Local Reasoning. Theoretical Computer Science 375(1-3): 271-307 (2007).</p>
<p><strong>2015:</strong> Dan Spielman and Shang-Hua Teng, Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems, Proc. 36th ACM Symposium on Theory of Computing, pp. 81-90, 2004; Spectral sparsification of graphs, SIAM J. Computing 40:981-1025, 2011; A local clustering algorithm for massive graphs and its application to nearly linear time graph partitioning, SIAM J. Computing 42:1-26, 2013; Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems, SIAM J. Matrix Anal. Appl. 35:835-885, 2014.</p>
<p><strong>2014: </strong>Ronald Fagin, Amnon Lotem, and Moni Naor, Optimal Aggregation Algorithms for Middleware, Journal of Computer and System Sciences 66(4): 614–656, 2003.</p>
<p><strong>2013: </strong>Antoine Joux, A one round protocol for tripartite Diffie-Hellman, J. Cryptology 17(4): 263-276, 2004. Dan Boneh and Matthew K. Franklin, Identity-Based Encryption from the Weil pairing, SIAM J. Comput. 32(3): 586-615, 2003.</p></div>
    </content>
    <updated>2018-12-18T20:14:50Z</updated>
    <published>2018-12-18T20:14:50Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-01-07T22:43:20Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=332</id>
    <link href="https://tcsplus.wordpress.com/2018/12/06/tcs-talk-wednesday-december-12-julia-chuzhoy-ttic/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, December 12 — Julia Chuzhoy, TTIC</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, December 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Julia Chuzhoy from TTIC will speak about “Almost Polynomial Hardness of Node-Disjoint Paths in Grids” (abstract below). Please make sure you reserve a spot for your group to […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, December 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Julia Chuzhoy</strong> from TTIC will speak about “<em>Almost Polynomial Hardness of Node-Disjoint Paths in Grids</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: In the classical Node-Disjoint Paths (NDP) problem, we are given an n-vertex graph G, and a collection of pairs of its vertices, called demand pairs. The goal is to route as many of the demand pairs as possible, where to route a pair we need to select a path connecting it, so that all selected paths are disjoint in their vertices.</p>
<p>The best current algorithm for NDP achieves an <img alt="O(\sqrt{n})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7Bn%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(\sqrt{n})"/>-approximation, while, until recently, the best negative result was a roughly <img alt="\Omega(\sqrt{\log n})" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28%5Csqrt%7B%5Clog+n%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\Omega(\sqrt{\log n})"/>-hardness of approximation. Recently, an improved <img alt="2^{\Omega(\sqrt{\log n})}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B%5COmega%28%5Csqrt%7B%5Clog+n%7D%29%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="2^{\Omega(\sqrt{\log n})}"/>-hardness of approximation for NDP was shown, even if the underlying graph is a subgraph of a grid graph, and all source vertices lie on the boundary of the grid. Unfortunately, this result does not extend to grid graphs.</p>
<p>The approximability of NDP in grids has remained a tantalizing open question, with the best upper bound of <img alt="\tilde{O}(n^{1/4})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BO%7D%28n%5E%7B1%2F4%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\tilde{O}(n^{1/4})"/>, and the best lower bound of APX-hardness. In this talk we come close to resolving this question, by showing an almost polynomial hardness of approximation for NDP in grid graphs.</p>
<p>Our hardness proof performs a reduction from the 3COL(5) problem to NDP, using a new graph partitioning problem as a proxy. Unlike the more standard approach of employing Karp reductions to prove hardness of approximation, our proof is a Cook-type reduction, where, given an input instance of 3COL(5), we produce a large number of instances of NDP, and apply an approximation algorithm for NDP to each of them. The construction of each new instance of NDP crucially depends on the solutions to the previous instances that were found by the approximation algorithm.</p>
<p>Joint work with David H.K. Kim and Rachit Nimavat.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2018-12-06T21:04:07Z</updated>
    <published>2018-12-06T21:04:07Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-01-07T22:39:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/41958</id>
    <link href="https://cstheory.stackexchange.com/questions/41958/how-to-achieve-a-topological-sort-of-an-given-sequence-with-minimum-swaps" rel="alternate" type="text/html"/>
    <title>how to achieve a topological sort of an given sequence with minimum swaps</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>For example, given the constraints {<span class="math-container">$a&lt;b,c&lt;d$</span>} and a sequence <span class="math-container">$[b,a,c,d]$</span>. we just need swap <span class="math-container">$a$</span> with <span class="math-container">$b$</span> to get an topological sort, I want to ask how to find the sort solutions with minimum swaps</p></div>
    </summary>
    <updated>2018-11-29T15:28:42Z</updated>
    <published>2018-11-29T15:28:42Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="np-hardness"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="sorting"/>
    <author>
      <name>user51340</name>
      <uri>https://cstheory.stackexchange.com/users/51340</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-07T22:33:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=330</id>
    <link href="https://tcsplus.wordpress.com/2018/11/21/tcs-talk-wednesday-november-28-eric-balkanski-harvard-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, November 28 — Eric Balkanski, Harvard University</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, November 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Eric Balkanski from Harvard University will speak about “The Adaptive Complexity of Maximizing a Submodular Function” (abstract below). Please make sure you reserve a spot for your group […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, November 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Eric Balkanski</strong> from Harvard University will speak about “<em>The Adaptive Complexity of Maximizing a Submodular Function</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We introduce the study of submodular optimization in the adaptive complexity model to quantify the information theoretic complexity of black-box optimization in a parallel computation model. Informally, the adaptivity of an algorithm is the number of sequential rounds it makes when each round can execute polynomially-many function evaluations in parallel. Since submodular optimization is regularly applied on large datasets we seek algorithms with low adaptivity to enable speedups via parallelization. For the canonical problem of maximizing a monotone submodular function under a cardinality constraint, all that was known until recently is that the adaptivity needed to obtain a constant approximation is between 1 and <img alt="\Omega(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\Omega(n)"/>. Our main result is a tight characterization showing that the adaptivity needed is, up to lower order terms, <img alt="\Theta(\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CTheta%28%5Clog+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\Theta(\log n)"/>:</p>
<ul>
<li>We describe an algorithm which requires <img alt="O(\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Clog+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(\log n)"/> sequential rounds and achieves an approximation that is arbitrarily close to 1/3;</li>
<li>We show that no algorithm can achieve an approximation better than <img alt="O(1/\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%2F%5Clog+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(1/\log n)"/> with fewer than <img alt="O(\log n/\log\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Clog+n%2F%5Clog%5Clog+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(\log n/\log\log n)"/> rounds.<br/>
Thus, when allowing for parallelization, our algorithm achieves a constant factor approximation exponentially faster than any previous algorithm for submodular maximization. I will conclude the talk by surveying very recent results on submodular optimization in the adaptive complexity model.</li>
</ul>
<p>Joint work with Yaron Singer.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2018-11-22T02:40:57Z</updated>
    <published>2018-11-22T02:40:57Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-01-07T22:39:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=328</id>
    <link href="https://tcsplus.wordpress.com/2018/11/07/tcs-talk-wednesday-november-14-urmila-mahadev-uc-berkeley/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, November 14 — Urmila Mahadev, UC Berkeley</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, November 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Urmila Mahadev from UC Berkeley will speak about “Classical Homomorphic Encryption for Quantum Circuits” (abstract below). Please make sure you reserve a spot for your group to join […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, November 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Urmila Mahadev</strong> from UC Berkeley will speak about “<em>Classical Homomorphic Encryption for Quantum Circuits</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We present the first leveled fully homomorphic encryption scheme for quantum circuits with classical keys. The scheme allows a classical client to blindly delegate a quantum computation to a quantum server: an honest server is able to run the computation while a malicious server is unable to learn any information about the computation. We show that it is possible to construct such a scheme directly from a quantum secure classical homomorphic encryption scheme with certain properties. Finally, we show that a classical homomorphic encryption scheme with the required properties can be constructed from the learning with errors problem.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2018-11-07T18:52:11Z</updated>
    <published>2018-11-07T18:52:11Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-01-07T22:39:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1465</id>
    <link href="https://theorydish.blog/2018/11/05/5th-toca-sv-coming-this-friday/" rel="alternate" type="text/html"/>
    <title>5th TOCA-SV Coming this Friday</title>
    <summary>Our first TOCA-SV meeting of the academic year is coming up this Friday. Details are here, please come by if you are around. This time we will feature Shafi Goldwasser for the Motwani colloquium, telling us about Pseudo Deterministic Algorithms and Proofs, Avishay Tal  about Oracle Separation of BQP and the Polynomial Hierarchy, and Badih Ghazi about Resource-Efficient Common Randomness and Secret Key Generation. We will also have student talks, food and drink and a great and diverse group of theoreticians as usual.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Our first TOCA-SV meeting of the academic year is coming up this Friday. Details are <a href="https://sites.google.com/view/toca-sv-day-nov-2018/">here</a>, please come by if you are around. This time we will feature</p>
<p><strong>Shafi Goldwasser </strong>for the Motwani colloquium, telling us about <em>Pseudo Deterministic Algorithms and Proofs, </em><strong>Avishay Tal </strong> about <em>Oracle Separation of BQP and the Polynomial Hierarchy, and </em><strong>Badih Ghazi </strong>about <em>Resource-Efficient Common Randomness and Secret Key Generation. </em>We will also have student talks, food and drink and a great and diverse group of theoreticians as usual.</p></div>
    </content>
    <updated>2018-11-05T18:28:10Z</updated>
    <published>2018-11-05T18:28:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-01-07T22:43:20Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1455</id>
    <link href="https://theorydish.blog/2018/11/01/simons-cluster-on-algorithmic-fairness/" rel="alternate" type="text/html"/>
    <title>Simons Cluster on Algorithmic Fairness</title>
    <summary>Over the summer, the Simons Institute ran a short program (a “cluster”) on Algorithmic Fairness, which I briefly discuss: here. This was only one instance of a flurry of recent programs and events on Algorithmic Fairness that had a substantial representation of TOC researchers (often as part of a multidisciplinary collaboration). The growing interest within the theory community in Algorithmic Fairness and, more generally, in the societal implications of computation is highly motivated and timely given how prevalent computation is in every aspect of our lives (also see: here). I will devote several posts (by myself and others) to the (beautiful) “emerging theory of algorithmic fairness.” Most of these posts will be more technical, but I’d like to devote today’s post to a short discussion of what theoreticians can contribute to this multidisciplinary effort. My own belief is that computer scientists cannot solve Algorithmic Fairness (and privacy in data analysis or any other issue of this sort) on their own. On the other hand, these issues, in their current computation-driven large-scale incarnation, cannot be seriously addressed without major involvement of computer scientists. Furthermore, what is needed (as I will try to demonstrate in future posts) is a true collaboration, rather than [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Over the summer, the <a href="https://simons.berkeley.edu/">Simons Institute</a> ran a short program (a “cluster”) on Algorithmic Fairness, which I briefly discuss: <a href="https://simons.berkeley.edu/news/inside-summer-cluster-algorithmic-fairness">here</a>. This was only one instance of a flurry of recent programs and events on Algorithmic Fairness that had a substantial representation of TOC researchers (often as part of a multidisciplinary collaboration). The growing interest within the theory community in Algorithmic Fairness and, more generally, in the societal implications of computation is highly motivated and timely given how prevalent computation is in every aspect of our lives (also see: <a href="https://theorydish.blog/2018/09/10/the-technologists-and-society/">here</a>).</p>
<p>I will devote several posts (by myself and others) to the (beautiful) “<a href="https://royalsociety.org/science-events-and-lectures/2018/06/you-and-ai-fairness/">emerging theory of algorithmic fairness</a>.” Most of these posts will be more technical, but I’d like to devote today’s post to a short discussion of what theoreticians can contribute to this multidisciplinary effort.</p>
<p>My own belief is that computer scientists cannot solve Algorithmic Fairness (and privacy in data analysis or any other issue of this sort) on their own. On the other hand, these issues, in their current computation-driven large-scale incarnation, cannot be seriously addressed without major involvement of computer scientists. Furthermore, what is needed (as I will try to demonstrate in future posts) is a true collaboration, rather than a division of work, where one community sub-contracts another for specific expertise.</p>
<h2>All-or-nothing-ism</h2>
<p>One of the reasons the Theory of Computing is particularly suited to this challenge is our basic optimism in the face of complexities and even impossibilities. The topic of Algorithmic Fairness seems to be particularly entangled with such complexities. This is the source of a line of criticism on the inherent limitations of the “tech solutionist” approach to Algorithmic Fairness. For example, “discrimination is the result of biases in the data and cannot be addressed at the level of machine learning.” Another example: “unless we understand the causal structure we are analyzing, fairness cannot be obtained.” These criticisms (while not as devastating as they are sometimes presented) are not without merit, and they deserve a much more technical discussion (that will hopefully come in future posts). At this point I’d like to make two comments:</p>
<ol>
<li>The computational lens has served us well in the study of Cryptography, Game Theory, Learning , Privacy and beyond. There is already evidence that it is serving us well in the study of Algorithmic Fairness. I believe that the pessimistic view of what I would call “all-or-nothing-ism” ignores an incredible track record of Theory of Computing in addressing complicated human-involving subject areas, and ignores the progress already made on Algorithmic Fairness.</li>
<li>Furthermore, no one is planning to stop analyzing data (for example in medical research) because our data is imperfect or because we didn’t figure out causality, Algorithmic Fairness requires both the best solutions we can come up with right now, and a concerted research effort to guarantee better fairness in the future.</li>
</ol></div>
    </content>
    <updated>2018-11-01T20:10:52Z</updated>
    <published>2018-11-01T20:10:52Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-01-07T22:43:20Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5357</id>
    <link href="https://adamsheffer.wordpress.com/2018/10/29/an-uncitable-result/" rel="alternate" type="text/html"/>
    <title>An Uncitable Result?</title>
    <summary>My colleague Pablo Soberon just showed me an unusually problematic result to cite, and I wanted to share this weird story. If you have other weird citation stories, do tell! Yes, this is a second silly post in a row. Lately I’m not finding the time to write more serious ones. And the silly stories […]</summary>
    <updated>2018-10-29T21:10:40Z</updated>
    <published>2018-10-29T21:10:40Z</published>
    <category term="Silly"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2019-01-07T22:39:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=326</id>
    <link href="https://tcsplus.wordpress.com/2018/10/24/tcs-talk-wednesday-october-31-michal-koucky-charles-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 31 — Michal Koucky, Charles University</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 31th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). Michal Koucky from Charles University will speak about “Approximating Edit Distance Within Constant Factor in Truly Sub-Quadratic Time ” (abstract below). Please make sure you reserve a spot […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 31th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong>Michal Koucky</strong> from Charles University will speak about “<em>Approximating Edit Distance Within Constant Factor in Truly Sub-Quadratic Time </em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Edit distance is a measure of similarity of two strings based on the minimum number of character insertions, deletions, and substitutions required to transform one string into the other. The edit distance can be computed exactly using a dynamic programming algorithm that runs in quadratic time. Andoni, Krauthgamer and Onak (2010) gave a nearly linear time algorithm that approximates edit distance within approximation factor <img alt="\mathrm{poly}(\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bpoly%7D%28%5Clog+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\mathrm{poly}(\log n)"/>. In this talk I will present an algorithm with running time <img alt="O(n^{2-2/7})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2-2%2F7%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(n^{2-2/7})"/> that approximates the edit distance within a constant factor.</p>
<p>Joint work with Diptarka Chakraborty, Debarati Das, Elazar Goldenberg, and Mike Saks.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2018-10-24T23:49:41Z</updated>
    <published>2018-10-24T23:49:41Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-01-07T22:39:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=324</id>
    <link href="https://tcsplus.wordpress.com/2018/10/11/tcs-talk-wednesday-october-17-c-seshadhri-uc-santa-cruz/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 17 — C. Seshadhri, UC Santa Cruz</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). C. Seshadhri from UC Santa Cruz will speak about “Finding forbidden minors through random walks: (almost) -query one-sided testers for minor closed properties” (abstract below). Please make sure […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>C. Seshadhri</strong> from UC Santa Cruz will speak about “<em>Finding forbidden minors through random walks: (almost) <img alt="n^{1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2F2%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="n^{1/2}"/>-query one-sided testers for minor closed properties</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Let <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=fff&amp;fg=444444&amp;s=0" title="G"/> be an undirected, bounded degree graph with <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0" title="n"/> vertices. Fix a finite graph <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/>, and suppose one must remove <img alt="\varepsilon n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon+n&amp;bg=fff&amp;fg=444444&amp;s=0" title="\varepsilon n"/> edges from <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=fff&amp;fg=444444&amp;s=0" title="G"/> to make it <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/>-minor free (for some small constant <img alt="\varepsilon&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%3E0&amp;bg=fff&amp;fg=444444&amp;s=0" title="\varepsilon&gt;0"/>). We give a nearly <img alt="n^{1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2F2%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="n^{1/2}"/> time algorithm that, with high probability, finds an <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/>-minor in such a graph.</p>
<p>As an application, consider a graph <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=fff&amp;fg=444444&amp;s=0" title="G"/> that requires <img alt="\varepsilon n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon+n&amp;bg=fff&amp;fg=444444&amp;s=0" title="\varepsilon n"/> edge removals to make it planar. This result implies an algorithm, with the same running time, that produces a <img alt="K_{3,3}" class="latex" src="https://s0.wp.com/latex.php?latex=K_%7B3%2C3%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="K_{3,3}"/> or <img alt="K_5" class="latex" src="https://s0.wp.com/latex.php?latex=K_5&amp;bg=fff&amp;fg=444444&amp;s=0" title="K_5"/> minor in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=fff&amp;fg=444444&amp;s=0" title="G"/>. No prior sublinear time bound was known for this problem. By the graph minor theorem, we get an analogous result for any minor-closed property.</p>
<p>Up to <img alt="n^{o(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="n^{o(1)}"/> factors, this result resolves a conjecture of Benjamini-Schramm-Shapira (STOC 2008) on the existence of one-sided property testers for minor-closed properties. Furthermore, our algorithm is nearly optimal, by lower bounds of Czumaj et al (RSA 2014).</p>
<p>Joint work with Akash Kumar and Andrew Stolman</p></blockquote>
<p> </p></div>
    </content>
    <updated>2018-10-11T17:22:08Z</updated>
    <published>2018-10-11T17:22:08Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-01-07T22:39:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5352</id>
    <link href="https://adamsheffer.wordpress.com/2018/10/05/mathematical-energy-etymology/" rel="alternate" type="text/html"/>
    <title>Mathematical Energy: Etymology</title>
    <summary>This might be the silliest post I’ve written so far (yes – worse than “Was Disney trying to kill mathematicians?”). I urge you to stop reading now unless (i) you are quite familiar with the mathematical notion of energy (e.g., additive energy), and (ii) you have a horrible sense of humor. The term energy was […]</summary>
    <updated>2018-10-05T01:56:06Z</updated>
    <published>2018-10-05T01:56:06Z</published>
    <category term="Additive combinatorics"/>
    <category term="Silly"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2019-01-07T22:39:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=321</id>
    <link href="https://tcsplus.wordpress.com/2018/09/26/tcs-talk-wednesday-october-3-alex-andoni-columbia-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 3 — Alex Andoni, Columbia University</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Alex Andoni from Columbia University will speak about “Parallel Graph Connectivity in Log Diameter Rounds” (abstract below). Please make sure you reserve a spot for your group to […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong><a href="http://www.mit.edu/~andoni/">Alex Andoni</a></strong> from Columbia University will speak about “<em>Parallel Graph Connectivity in Log Diameter Rounds</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: The MPC model has emerged as a compelling model for modern parallel systems, such as MapReduce, Hadoop and Spark, capturing well coarse-grained computation on large data. Here, data is distributed to processors, each of which has a sublinear (in the input data) amount of memory and we alternate between rounds of computation and rounds of communication, where each machine can communicate an amount of data as large as the size of its memory. This model is stronger than the classical PRAM model, and the recent research program has been to design algorithms whose running time is smaller than in the PRAM model.</p>
<p>One now-classic challenge is the graph connectivity problem. On an undirected graph with <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0" title="n"/> nodes and <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=fff&amp;fg=444444&amp;s=0" title="m"/> edges, <img alt="O(\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Clog+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(\log n)"/> round connectivity algorithms have been known for over 35 years. However, no algorithms with better complexity bounds are known for MPC (in fact even impossible for some restricted algorithms).</p>
<p>We give a faster algorithms for the connectivity problem when parameterizing the time complexity as a function of the diameter of the graph. Our main result is a <img alt="O(\log D \cdot \log\log_{m/n} n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Clog+D+%5Ccdot+%5Clog%5Clog_%7Bm%2Fn%7D+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(\log D \cdot \log\log_{m/n} n)"/> time connectivity algorithm for  diameter-<img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=fff&amp;fg=444444&amp;s=0" title="D"/> graphs, using <img alt="O(m)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28m%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(m)"/> total memory.</p>
<p>Joint work with Zhao Song, Cliff Stein, Zhengyu Wang, Peilin Zhong (to appear in FOCS’18).</p></blockquote>
<p> </p></div>
    </content>
    <updated>2018-09-26T23:24:41Z</updated>
    <published>2018-09-26T23:24:41Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-01-07T22:39:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://nisheethvishnoi.wordpress.com/?p=78</id>
    <link href="https://nisheethvishnoi.wordpress.com/2018/09/19/the-dynamics-of-lagrange-and-hamilton/" rel="alternate" type="text/html"/>
    <title>The dynamics of Lagrange and Hamilton</title>
    <summary>In 1788, Lagrange presented a set of equations of motion that, unlike Newtonian mechanics, are independent of the choice of coordinates of the physical system, and ultimately led to the formulation of general relativity. Hamilton came up with a different set of equations of motion in 1833 that arguably led to the development of quantum mechanics. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In 1788, Lagrange presented a set of equations of motion that, unlike Newtonian mechanics, are independent of the choice of coordinates of the physical system, and ultimately led to the formulation of <a href="https://en.wikipedia.org/wiki/Einstein%E2%80%93Hilbert_action">general relativity</a>. Hamilton came up with a different set of equations of motion in 1833 that arguably led to the development of <a href="https://en.wikipedia.org/wiki/Hamiltonian_(quantum_mechanics)">quantum mechanics</a>. Remarkably, in classical mechanics, these sets of equations turn out to be <em>equivalent</em> via a beautiful duality due to <strong>Legendre</strong>.</p>
<p><img alt="AduC_057_Legendre_(L.,_1756-1797)" class="alignnone size-full wp-image-82" src="https://nisheethvishnoi.files.wordpress.com/2018/09/aduc_057_legendre_l-_1756-1797.jpg?w=1008"/></p>
<p><span style="color: #999999;"><em>A portrait of Legendre by H. Rousseau, E. Thomas, Augustin Challamel, and Desire Lacroix via Wikimedia Commons</em></span></p>
<p>Lagrangian and Hamiltonian dynamics have inspired several promising optimization and sampling algorithms such as <a href="https://arxiv.org/pdf/1603.04245.pdf">first-order methods in optimization</a>,  <a href="http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html">Hamiltonian Monte Carlo</a> (see also this <a href="https://arxiv.org/abs/1802.08898">paper</a> that will appear in NIPS 2018). Legendre duality also appears in convex optimization as <a href="https://en.wikipedia.org/wiki/Fenchel%27s_duality_theorem">Fenchel</a> <a href="https://nisheethvishnoi.wordpress.com/convex-optimization/">duality</a>.  <a href="https://nisheethvishnoi.files.wordpress.com/2018/09/lagrangehamiltonian.pdf" title="LagrangeHamiltonian">This</a> note, written primarily for optimization folks, introduces Lagrangian dynamics, Hamiltonian dynamics, and proves the duality that connects them.</p>
<p>I hope that these fundamental ideas inspire you as well to think about optimization from a physics perspective!</p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2018-09-19T22:15:10Z</updated>
    <published>2018-09-19T22:15:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>nisheethvishnoi</name>
    </author>
    <source>
      <id>https://nisheethvishnoi.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://nisheethvishnoi.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://nisheethvishnoi.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Algorithms, Nature, and Society</title>
      <updated>2019-01-07T22:44:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://nisheethvishnoi.wordpress.com/?p=63</id>
    <link href="https://nisheethvishnoi.wordpress.com/2018/09/16/fair-elections/" rel="alternate" type="text/html"/>
    <link href="http://radiochablais.ch//podcast/mp3/info_12h_13062018.mp3" length="0" rel="enclosure" type="audio/mpeg"/>
    <link href="http://radiochablais.ch//podcast/mp3/info_12h_13062018.mp3" length="0" rel="enclosure" type="audio/mpeg"/>
    <title>Fair Elections</title>
    <summary>Elisa Celis and Nisheeth Vishnoi Elections are the nervous system of a democracy and, hence, issues related to their transparency and fairness are central to healthy societies. However, the number of controversial elections has exploded since 2000, and the more recent ones involve the use of the Internet and algorithms — a good excuse for computer […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: center;"><em><a href="https://theory.epfl.ch/celis/HOME.html">Elisa Celis</a> and Nisheeth Vishnoi</em></p>
<p>Elections are the nervous system of a democracy and, hence, issues related to their transparency and fairness are central to healthy societies. However, the number of <a href="https://en.wikipedia.org/wiki/List_of_controversial_elections">controversial elections</a> has exploded since 2000, and the more recent ones involve the use of the Internet and algorithms — a good excuse for computer scientists to get involved in elections to save democracy!</p>
<p>Last year, the two of us and Lingxao Huang revisited the question of <em>fairness</em> in <em>multi-winner</em> elections<em>. </em>A multi-winner election is one where the objective is to select a committee from a set of candidates based on voter preferences. However, election systems for multi-winner elections have been shown to be <a href="https://www.representwomen.org/voting_rules_pose_barrier_for_women">biased against minorities</a> by giving preference to committees that under-represent their already-small numbers. When the candidates have certain sensitive attributes (such as gender), and different types (e.g., male, female and non-binary) we may instead wish to impose additional requirement so that the winning committee is <em>fair</em>. We considered the problem of finding the committee with the maximum number of votes, subject to such additional fairness requirements.</p>
<p style="text-align: center;"><em>But what does it mean to be fair?</em></p>
<p>Is a committee fair when types are equally represented? Or <a href="https://en.wikipedia.org/wiki/Proportional_representation">proportionally represented</a>? These questions have received more than a century’s worth of attention in social choice theory and are deeply entangled with culture and politics. In short, fairness means different things in different contexts and rather than addressing a specific instance of this question, we asked ourselves — <em>can we provide a framework in which a user can specify fairness constraints according to their needs and design algorithms that can compute the winning committee accordingly?</em></p>
<p>Because such a framework must handle general classes of constraints, the algorithmic task became more challenging than the usual unconstrained case. If you are interested in understanding our algorithmic and complexity results, they appear in our recent <a href="https://arxiv.org/pdf/1710.10057.pdf">paper</a> that was presented at IJCAI-ECAI this year.</p>
<p>Clearly, however, there are real merits to providing such generality — earlier this year, our paper caught the eye of members of a <a href="https://appelcitoyen.ch/">people’s movement</a> from the <a href="https://en.wikipedia.org/wiki/Canton_of_Valais">Valais</a> canton in Switzerland, who were in the process of rethinking elections in their region. They were happy to know that we had solved the exact problem they needed for a primary election and asked us if we would be willing to help them conduct their elections. We were, of course, thrilled!</p>
<p>After an intense collaboration for more than four months, 8 elections that used our framework concluded on September 9, 2018. For us, there were several eye-opening aspects of taking our algorithmic framework from theory to practice. Perhaps the most interesting one was coming up with an answer to:</p>
<p style="text-align: center;"><em>Who decides what is fair?</em></p>
<p>In this case, it was the voters! Before voting on the committee, a vote to determine the fairness constraints was conducted in each of the eight districts. This vote was in mid-June and also checked if the voters agree to use our algorithmic framework. Since transparency was the key motivation, prior to this vote,  we were invited to a press conference to educate the people about the whole process that resulted in coverage in newspapers <a href="https://www.letemps.ch/suisse/listes-ideales-constituante-valaisanne?itm_source=homepage&amp;itm_medium=position-12">Le Temps</a>,  <a href="https://www.lenouvelliste.ch/articles/valais/canton/appel-citoyen-lance-sa-primaire-digitale-en-vue-de-la-constituante-les-passionnes-de-democratie-ont-jusqu-a-jeudi-pour-s-inscrire-763669">Le Nouvelliste</a>, TV channel <a href="https://www.google.com/url?q=http://canal9.ch/appel-citoyen-va-choisir-ses-candidats-pour-la-lassemblee-constituante-en-deux-etapes-et-par-internet/&amp;source=gmail&amp;ust=1537194868256000&amp;usg=AFQjCNGQkC-CmcI2n0iBfAAzxP4GWbiBzQ">Canal 9</a>, and radio channels <a href="http://www.rhonefm.ch/fr/podcasts/journal-du-soir-une-election-primaire-digitale--c-est-l-aventure-dans-laquelle-se-lance-le-mouvement-apolitique-appel-citoyen-en-premiere-mondiale-1107827">Rhone FM</a> and  <a href="http://radiochablais.ch//podcast/mp3/info_12h_13062018.mp3">Radio Chablais</a>.</p>
<p><img alt="20180612_142435" class="alignnone size-full wp-image-65" src="https://nisheethvishnoi.files.wordpress.com/2018/09/20180612_142435.jpg?w=1008"/></p>
<p><em>An image from the press conference in Sion on June 12, 2018. Elisa Celis (center), accompanied by members of Appel Citoyen.</em></p>
<p>In the end, the voters decided to place constraints on the gender balance, age demographics, and regions. While all districts approved these same three attributes, the specific constraints varied by district according to their demographics and pre-defined regions. This was a truly open and democratic way to determine the criteria by which the committee was to be balanced. The constraints not only ensured that the outcome would be fair (as defined by the voters) but also encouraged many more women to run for election than the party originally anticipated — in fact, in several districts there were more women than men contesting the election!</p>
<p><img alt="20180909_153659" class="alignnone size-full wp-image-64" src="https://nisheethvishnoi.files.wordpress.com/2018/09/20180909_153659.jpg?w=1008"/></p>
<p style="text-align: left;"><em>Computing the winning committees in Sion on September 9, 2018. From L to R: Lingxiao Huang, Vijay Keswani, Elisa Celis, Florian Evequoz, Bernadette Morand-Aymon.</em></p>
<p>Everything about these elections, from votes, candidates, constraints, and code, to the output, is open for all to <a href="https://appelcitoyen.ch/blog/on-ouvre-les-urnes-donnees-brutes-de-la-primaire/">verify</a>. We have also made available a <a href="http://valais-elections.herokuapp.com/">demo</a> with pre-loaded constraints, candidate information, and votes for anyone to test and compute the results. To know more about the use of our algorithm in these elections, take a look at <a href="https://www.youtube.com/watch?v=X6M1fpcEBQE">this video</a>. A few links to the coverage of these elections can be found here: <a href="http://canal9.ch/constituante-appel-citoyen-a-designe-ses-96-candidats-une-primaire-digitale-realisee-grace-a-une-methode-concue-par-lepfl/">Canal 9</a>,  <a href="https://www.letemps.ch/suisse/valais-appel-citoyen-lance-primaire-constituante">Le Temps</a>, <a href="https://www.lenouvelliste.ch/dossiers/tout-savoir-sur-la-constituante/articles/constituante-la-primaire-numerique-d-appel-citoyen-a-rendu-son-verdict-782906">Le Nouvelliste</a>.</p>
<p>We feel fortunate that we were given this unique opportunity to take tools from computer science to the heart of a democratic process. It was an unparalleled learning experience for all of us and something that we increasingly look forward to in the future.</p>
<p><img alt="constraints" class="alignnone size-full wp-image-70" src="https://nisheethvishnoi.files.wordpress.com/2018/09/constraints.gif?w=1008"/></p>
<p><img alt="candidates" class="alignnone size-full wp-image-69" src="https://nisheethvishnoi.files.wordpress.com/2018/09/candidates.gif?w=1008"/></p>
<p>You can also use our <a href="https://theory.epfl.ch/bias/elections/">demo</a> to design your own elections and play around with our algorithms. This demo was developed with the help of Abhibhav Garg and Vijay Keswani and has limited functionality. If you are interested in using our full framework and/or working with us,  please feel free to <a href="https://theory.epfl.ch/bias/contact.html">contact us</a>.</p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2018-09-16T20:02:02Z</updated>
    <published>2018-09-16T20:02:02Z</published>
    <category term="AI and Society"/>
    <category term="Algorithms"/>
    <category term="Fairness"/>
    <author>
      <name>nisheethvishnoi</name>
    </author>
    <source>
      <id>https://nisheethvishnoi.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://nisheethvishnoi.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://nisheethvishnoi.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Algorithms, Nature, and Society</title>
      <updated>2019-01-07T22:44:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=311</id>
    <link href="https://tcsplus.wordpress.com/2018/09/15/tcs-talk-wednesday-september-19-avishay-tal-simons-institute/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, September 19 — Avishay Tal, Simons Institute</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, September 19th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Avishay Tal from the Simons Institute will speak about an “Oracle Separation of BQP and the Polynomial Hierarchy” (abstract below). Please make sure you reserve a spot for […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, September 19th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Avishay Tal</strong> from the Simons Institute will speak about an “<em>Oracle Separation of BQP and the Polynomial Hierarchy</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We present an oracle, <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=fff&amp;fg=444444&amp;s=0" title="A"/>, relative to which <img alt="\mathsf{BQP}^{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BBQP%7D%5E%7BA%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\mathsf{BQP}^{A}"/> is not contained in <img alt="\mathsf{PH}^{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BPH%7D%5E%7BA%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\mathsf{PH}^{A}"/>.</p>
<p>Following the approach of Aaronson [STOC, 2010], our result is obtained by finding a distribution <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=fff&amp;fg=444444&amp;s=0" title="D"/> over Boolean strings of length <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=fff&amp;fg=444444&amp;s=0" title="N"/> such that:</p>
<ol>
<li>There exists a quantum algorithm that runs in time<img alt="{\rm polylog}(N)" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crm+polylog%7D%28N%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="{\rm polylog}(N)"/> and distinguishes between<img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=fff&amp;fg=444444&amp;s=0" title="D"/> and the uniform distribution over Boolean strings of length<img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=fff&amp;fg=444444&amp;s=0" title="N"/>.</li>
<li>No Boolean circuit of quasi-polynomial size and constant depth can<br/>
distinguish between <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=fff&amp;fg=444444&amp;s=0" title="D"/> and the uniform distribution with advantage better than <img alt="{\rm polylog}(N)/\sqrt{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crm+polylog%7D%28N%29%2F%5Csqrt%7BN%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="{\rm polylog}(N)/\sqrt{N}"/>.</li>
</ol>
<p>Joint work with Ran Raz.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2018-09-16T01:02:41Z</updated>
    <published>2018-09-16T01:02:41Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-01-07T22:39:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1453</id>
    <link href="https://theorydish.blog/2018/09/10/the-technologists-and-society/" rel="alternate" type="text/html"/>
    <title>The “Technologists” and Society</title>
    <summary>Another round of technology leaders testifying in Congress last week brought about another round of complaints about the excessive power that “technologists” wield over today’s society. Are Computer Scientists following Economists in losing public support? Are we turning from heroes to villains in the public eye? Hopefully not, but it is becoming clear that the prosperity of Computer Science depends (at least in part) on Computer Scientists rising to meet the challenge in addressing the societal impact of computations. While all too common, the term “technologists” in this context is unfortunate. Who are those mysterious “technologists?” Are they software engineers? Are they computer scientists? (and which sub area: Machine Learning? Theory? Others?) Or perhaps CEOs of technology companies? Or perhaps this refers to the investment firms and Wall Street, who seem to have such a huge sway over technology companies? Perhaps users of technology are to blame? Each of those is a completely different group of individuals with completely different sets of constraints and incentives. Lumping them all together is close to meaningless. In a sequence of posts (by me and others and of increasing level of technical details), I hope to discuss the role of Theory of Computing in [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Another round of technology leaders testifying in Congress last week brought about another round of complaints about the excessive power that “technologists” wield over today’s society. Are Computer Scientists following Economists in losing public support? Are we turning from heroes to villains in the public eye? Hopefully not, but it is becoming clear that the prosperity of Computer Science depends (at least in part) on Computer Scientists rising to meet the challenge in addressing the societal impact of computations.</p>
<p>While all too common, the term “technologists” in this context is unfortunate. Who are those mysterious “technologists?” Are they software engineers? Are they computer scientists? (and which sub area: Machine Learning? Theory? Others?) Or perhaps CEOs of technology companies? Or perhaps this refers to the investment firms and Wall Street, who seem to have such a huge sway over technology companies? Perhaps users of technology are to blame? Each of those is a completely different group of individuals with completely different sets of constraints and incentives. Lumping them all together is close to meaningless.</p>
<p>In a sequence of posts (by me and others and of increasing level of technical details), I hope to discuss the role of Theory of Computing in the study of the particularly important societal issue of Algorithmic Fairness. In this post, I’d like to briefly discuss the role of Academia more generally.</p>
<p><strong>The power and weakness of education</strong></p>
<p>An idea that is getting traction is that ethics and the societal impact of computation should be embedded in essentially all Computer Science courses. I am all for it! (In fact, ethics should be a major part of every curriculum on campus, not just Computer Science). As these days a huge fraction of students take some Computer Science courses, this will improve the awareness of technology consumers to ethics in computation. It will also improve the awareness of software engineers and eventually also the leadership of technology companies and as importantly that of policy makers.</p>
<p>But awareness, in itself, may not have much of an impact. Software engineers often have very little flexibility in shaping the products they develop, even when it comes to topics that more clearly affect the bottom line of their companies (this has to do with the quick pace and incentive structure of companies). Even the most philanthropic CEOs seem to run companies that violate basic ethical considerations. Here too, the incentive structure is much more to blame than lack of awareness. And even consumers that want to punish violators, often do not, as many software companies are to a large extent a monopoly. In other cases, violators operate behind the scenes, hidden from consumers.</p>
<p><strong>Developing the Knowledge and Tools </strong></p>
<p>I would also add that topics like privacy and algorithmic fairness require significant sophistication and much of the required knowledge and tools are yet to be developed. This means that academia (and funding agencies) should perform and support much more research. But (big) companies (that make their living exploiting sensitive data) should also hire many more researchers (of various disciplines) to develop the tools they need.</p>
<p>The great breakthroughs in Machine Learning within industry did not occur because the employees of those companies increased their awareness to the importance of data analysis. It happened because those companies employed talented and knowledgeable individuals and poured a lot of money into machine learning. Unless companies invest much more resources in their ethics, we are going to see the same recurring failures in protecting their users.</p>
<p><strong>Regulations</strong></p>
<p>As we already mentioned that users are very limited in punishing big companies, it is unlikely that we will see the needed investment across the board (some companies are much better in this regard, but those companies are the exception rather than the rule). In addition to education, we need to enforce good behavior through legislation and regulation. Unfortunately, the direction of the current administration is to remove protections for consumers. Still, we can hope that Europe (as well as some of the more progressive U.S. states), will come to our rescue once again. As far of the role of scientists, we should work with policy makers to develop and advocate for the “right” regulations.</p>
<p> </p></div>
    </content>
    <updated>2018-09-10T14:00:43Z</updated>
    <published>2018-09-10T14:00:43Z</published>
    <category term="Uncategorized"/>
    <category term="Algorithmic Fairness"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-01-07T22:43:20Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=307</id>
    <link href="https://tcsplus.wordpress.com/2018/09/05/fall-is-coming/" rel="alternate" type="text/html"/>
    <title>Fall is Coming</title>
    <summary>Summer is over, and Fall is upon us. Trees changing color, pumpkins invading the Starbucks’ iced chai lattes, and a new exciting season of TCS+! The focus here is the latter, and specifically to remind you to (1) block your calendar, (2) book a room on a fortnightly fashion, and (3) sit back with your […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Summer is over, and Fall is upon us. Trees changing color, pumpkins invading the Starbucks’ iced chai lattes, and a new exciting season of TCS+!</p>
<p>The focus here is the latter, and specifically to remind you to (1) block your calendar, (2) book a room on a fortnightly fashion, and (3) sit back with your favorite talk-watching snack and prepare for a treat: as customary, the TCS+ seminars will take place every other Wednesday, starting exactly two weeks from now (Wed. 19th September), at the by-now-standard time of 1pm EST (10am PST).</p>
<p>The first speakers lined up for the Fall are:</p>
<ul>
<li>Sept. 19th: <a href="http://www.avishaytal.org/">Avishay Tal</a> (Simons Institute)</li>
<li>Oct. 3rd: (TBD)</li>
<li>Oct. 17th: <a href="https://users.soe.ucsc.edu/~sesh/">C. Seshadri</a> (UC Santa Cruz)</li>
<li>Oct. 31st: (TBD)</li>
<li>Nov. 14th: Urmila Mahadev (UC Berkeley)</li>
</ul>
<p>An up-to-date calendar will always appear on our <a href="https://sites.google.com/site/plustcs/home">webpage</a>, where you can also subscribe to our mailing-list and <a href="https://sites.google.com/site/plustcs/suggest">suggest talks</a>.</p>
<p>As always, TCS+ welcomes feedback, be it on the technical or scientific aspects. Please help us continue the success of our seminars by attending, suggesting speakers, and spreading the word to those universities worldwide that would most benefit from watching our seminars.</p>
<p style="text-align: right;">Best wishes for the new academic year,<br/>
The organizers.</p></div>
    </content>
    <updated>2018-09-05T23:06:29Z</updated>
    <published>2018-09-05T23:06:29Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-01-07T22:39:37Z</updated>
    </source>
  </entry>
</feed>
