<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-09-11T00:21:47Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17542</id>
    <link href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/" rel="alternate" type="text/html"/>
    <title>Hybrid Versus Remote Teaching</title>
    <summary>Which is best for students? Cropped from Wikipedia src Moshe Vardi holds multiple professorships at Rice University. He is also the Senior Editor of Communications of the ACM. His is therefore a voice to be reckoned with in the current debate over how best to teach during the pandemic. Much of the debate is over […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Which is best for students?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/330px-moshe_vardi_img_0010/" rel="attachment wp-att-17544"><img alt="" class="alignright wp-image-17544" height="150" src="https://rjlipton.files.wordpress.com/2020/09/330px-moshe_vardi_img_0010.jpg?w=121&amp;h=150" width="121"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Wikipedia <a href="https://en.wikipedia.org/wiki/Moshe_Vardi#/media/File:Moshe_Vardi_IMG_0010.jpg">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Moshe Vardi holds multiple professorships at Rice University. He is also the Senior Editor of <em>Communications of the ACM</em>. His is therefore a voice to be reckoned with in the current debate over how best to teach during the pandemic. Much of the debate is over whether all should hear his voice the same way, or some hear it in the classroom while others hear it remotely. </p>
<p>
Today we note his recent <a href="https://medium.com/@vardi/covid-19-the-ford-pinto-and-american-higher-ed-2b191920b065">column</a> for <em>Medium</em> advocating the former. Then I (Ken) give some of my own impressions.</p>
<p>
His September 5 column followed an August 8 <a href="https://www.ricethresher.org/article/2020/08/return-to-campus-but-to-what-end">opinion</a> given to the Rice student newspaper. Both begin with concern over the conflict between <em>safety</em> and <em>value</em> for students. Much of the value of college—<em>most</em> according to statistics he cites—comes from being collegial: outside the classroom. But many such activities, not only evening parties but informal games and gatherings, are the most unsafe. </p>
<p>
We will focus however on what Moshe says about the nature of instruction for lecture courses. Certainly for laboratory courses there is a sharp trade-off between safety and in-person interaction. But we focus here on what he says about the nature of teaching in the lecture hall, where one can take safety as a given requirement. </p>
<p>
</p><p/><h2> An In-Person Remoteness Paradox </h2><p/>
<p/><p>
I have just returned from sabbatical at the University at Buffalo (UB) and am teaching this fall a small elective 4xx/5xx theory course. It has 15 students, smaller than the 25 in the hypothetical class Moshe describes but of the same order of magnitude. In the spring I will be teaching a larger undergraduate course which is also on target for his concerns. I have taught such a class every spring for a decade. While this assignment is not a new to me, the issue of safety raises tough choices about the delivery options. My options are: </p>
<ol>
<li>
<em>remote-only</em>; <p/>
</li><li>
<em>in-person only</em>; <p/>
</li><li>
<em>hybrid</em> use of 1 and 2 for designated course components; <p/>
</li><li>
<em>hybrid-flexible</em>, meaning 1 and 2 are conducted simultaneously with students free to choose either option, even on a per-lecture basis.
</li></ol>
<p>
I have committed to hybrid-flexible. For my current fall course, I made this commitment in early summer when there was uncertainty over in-person instruction requirements for student visas and registration. I believe that my larger course will be implemented as safely in a large room as my current course. The question is quality.</p>
<p>
Moshe notes right away a paradox for his hypothetical class that could apply to any of modes 2–4; to include the last expressly, I’ve inserted the word “even”:</p>
<blockquote><p> <em> …I realized that [even] the students <b>in the classroom</b> will have to be communicating with me on Zoom, to be heard and recorded. All this, while both the students and I are wearing face masks. It dawned on me then that I will be conducting remote teaching <b>in the classroom</b>. </em>
</p></blockquote>
<p/><p/>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/lecture/" rel="attachment wp-att-17545"><img alt="" class="size-full wp-image-17545" src="https://rjlipton.files.wordpress.com/2020/09/lecture.jpg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><em>Business Insider</em> <a href="https://www.businessinsider.com/us-colleges-shutting-down-coronavirus-impact-when-classes-move-online-2020-3">source</a>—yet another variation</font>
</td>
</tr>
</tbody></table>
<p>
In fact, I have one volunteer now in the room logging into Zoom to help with interaction from those attending remotely. This helps because my podium has less space to catch faces and detect questions right away. I do repeat questions so they are picked up in the recording and often redirect them to the class. Still, the mere fact of my not seeing faces alongside the notes and interactive drawings I am sharing makes me feel Moshe’s paradox all the time. This is even though my room allows denser spacing than at Rice, so a class of 25 could sit closer.  Let me, however, say why I love stand-up teaching before addressing his paramount question of what is best for the students at this time.</p>
<p>
</p><p/><h2> From Whiteboards to Tutorials </h2><p/>
<p/><p>
Dick once wrote a <a href="https://rjlipton.wordpress.com/2013/11/07/in-praise-of-chalk-talks/">post</a>, “In Praise of Chalk Talks.” First, with reference to talks pre-made using PowerPoint or LaTeX slides, Dick wrote:</p>
<blockquote><p><b> </b> <em> Such talks can be informative and easy to follow, yet sometimes PowerPoint is not well suited to giving a proof. The slides do not hold enough <b>state</b> for us to easily follow the argument. </em>
</p></blockquote>
<p/><p>
Moreover, when I contributed to the open-problems session of the Princeton IAS workshop we <a href="https://rjlipton.wordpress.com/2018/06/06/princeton-is-invariant/">covered</a> two years ago, Avi Wigderson insisted that everyone use chalk, not slides. I’ve used slides for UB’s data-structures and programming languages courses, but I think students benefit from seeing proofs and problem-solving ideas <em>grow</em>.</p>
<p>
I find furthermore that the feel of immersion in a process of discovery is enhanced by an in-person presence. I had this in mind when I followed Dick’s post with a long one <a href="https://rjlipton.wordpress.com/2013/11/15/the-graph-of-math/">imagining</a> Kurt Gödel expounding the distinctive points of his set theory (joint with Paul Bernays and John von Neumann), all on one chalkboard. My classes are not as interactive as in that post, but I prepare junctures in lectures for posing questions and doing a little bit of Socratic method. And I try to lead this with body language as well as voice inflection, whether at a whiteboard or drawing on hard paper via a document camera. </p>
<p>
Still, it exactly this “extra” that gets diminished for those who are remote. When I share my screen for notes or a drawing (both in <a href="https://www.mathcha.io/">MathCha</a>), they see my movements only in a small second window if at all. They do hear my voice—but I do not hear theirs even if they unmute themselves. Nor can I read their state of following as I do in the room. Without reiterating the safety factor as Moshe does, I can reformulate his key question as:</p>
<blockquote><p><b> </b> <em> Does the non-uniformity and inequality of hybrid delivery outweigh the benefits of making in-person instruction available to some? </em>
</p></blockquote>
<p/><p>
I must quickly add that in-person teaching is perceived as a collective need at UB. The web form I filled for Spring 2021 stated that some in-person classes must be available at all levels, 1xx through 7xx. I am happy to oblige. But the fact that I chose a flexible structure, especially in a small class, does allow the students to give opinion on this question, as well as on something Moshe says next:</p>
<blockquote><p><b> </b> <em> “Remote teaching” actually can do a better job of reproducing the intimacy that we take for granted in small classes. </em>
</p></blockquote>
<p/><p>
Toward this end, I am implementing a remote version of the <a href="https://en.wikipedia.org/wiki/Tutorial_system">tutorial system</a> I was part of for two eight-week terms at Oxford while a junior fellow of Merton College. When Cambridge University <a href="https://www.bbc.com/news/education-52732814">declared</a> already last May that there would be no in-person lectures all the way through summer 2021, this is because most lectures there are formally optional anyway. The heart of required teaching is via weekly tutorial hours in groups of one-to-three students. They are organized separately by each of thirty-plus constituent colleges rather than by department-centered staff, so the numbers are divided to be manageable. In my math-course tutorials the expectation was for each student to present a solved problem and participate in discussions that build on the methods. </p>
<p>
I am doing this every other week this fall, alternating with weeks of problem-set review that will be strictly optional and classed as enhanced office hours. All UB office hours must be remote anyway. The tutorial requirement was agreed by student voice-vote in a tradeoff with lowering the material in timed exams to compensate for differences in home situations. After a few weeks of this, the class will take stock for opinions on which delivery options work best. UB has already committed to being remote-only after Thanksgiving, and it is possible that the on-campus medical situation will trigger an earlier conversion anyway.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We would like to throw the floor open for comment on Moshe’s matters that we’ve highlighted and on his other opinions about the university mission amid the current crisis more generally. </p>
<p>
[edited to reflect that at Rice too, the hypothetical class could be in any of modes 2–4, and that spacing is further than in my UB room.]</p></font></font></div>
    </content>
    <updated>2020-09-10T22:26:40Z</updated>
    <published>2020-09-10T22:26:40Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Teaching"/>
    <category term="education quality"/>
    <category term="hybrid instruction"/>
    <category term="Moshe Vardi"/>
    <category term="online courses"/>
    <category term="pandemic"/>
    <category term="remote instruction"/>
    <category term="teaching"/>
    <category term="universities"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-11T00:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7794</id>
    <link href="https://windowsontheory.org/2020/09/10/sigact-research-highlights-call-for-nominations/" rel="alternate" type="text/html"/>
    <title>SIGACT research highlights – call for nominations</title>
    <summary>TL;DR: Know of a great recent paper that should be highlighted to the theory community and beyond? Email a nomination to sigact.highlights.nominations@outlook.com by October 19th. The goal of the SIGACT Research Highlights Committee is to help promotetop computer science theory research via identifying results that are ofhigh quality and broad appeal to the general computer […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>TL;DR:</strong> Know of a great recent paper that should be highlighted to the theory community and beyond? Email a nomination to <a href="mailto:sigact.highlights.nominations@outlook.com" rel="noreferrer noopener" target="_blank">sigact.highlights.nominations@outlook.com</a> by October 19th.</p>



<p>The goal of the SIGACT Research Highlights Committee is to help promote<br/>top computer science theory research via identifying results that are of<br/>high quality and broad appeal to the general computer science audience.<br/>These results would then be recommended for consideration for the <a href="http://cacm.acm.org">CACM</a> <em>Research Highlights</em> section as well as other general-audience computer science research outlets.</p>



<p><strong>Nomination and Selection Process:</strong></p>



<p>The committee solicits two types of nominations:</p>



<p>1) <strong>Conference nominations.</strong> Each year, the committee will ask the PC<br/>chairs of theoretical computer science conferences to send a selection<br/>of up to three top papers from these conferences (selected based on both<br/>their technical merit and breadth of interest to non-theory audience)<br/>and forwarding them to the committee for considerations.</p>



<p>2) <strong>Community nominations. </strong>The committee will accept nominations from the members of the community. Each such nomination should summarize the contribution of the nominated paper and also argue why this paper is<br/>suitable for broader outreach. The nomination should be no more than a<br/>page in length and can be submitted at any time by emailing it to<br/><a href="mailto:sigact.highlights.nominations@outlook.com" rel="noreferrer noopener" target="_blank">sigact.highlights.nominations@outlook.com</a>. Self-nominations are<br/>discouraged.</p>



<p>The nomination deadline is <strong>Monday, October 19, 2020 </strong>.</p>



<p><strong>Committee:</strong></p>



<p>The SIGACT Research Highlights Committee currently comprises the<br/>following members:</p>



<p>Boaz Barak, Harvard University<br/>Irit Dinur, Weizmann Institute of Science<br/>Aleksander Mądry, Massachusetts Institute of Technology (chair)<br/>Jelani Nelson, University of California, Berkeley</p></div>
    </content>
    <updated>2020-09-10T14:17:37Z</updated>
    <published>2020-09-10T14:17:37Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-09-11T00:20:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8975826682758317937</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8975826682758317937/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8975826682758317937" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8975826682758317937" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html" rel="alternate" type="text/html"/>
    <title>When are both x^2+3y and y^2+3x both squares, and a more general question</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> In my last post (see <a href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html">here</a>) I asked two math questions. In this post I discuss one of them. (I will discuss the other one later, probably Monday Sept 14.)</p><p><br/>For which positive naturals x,y are x^2+3y and y^2+3x both squares?</p><p>I found this in a math contest book and could not solve it, so I posted it to see what my readers would come up with. They came up with two solutions, which you can either read in the comments on that post OR read my write up <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/sq3.pdf">here</a>.)</p><p>The problem raises two more general questions</p><p>1) I had grad student Daniel Smolyak write a program that showed that if  1\le x,y \le 1000 then the only solutions were (1,1) and (11,16) and (16,11).  (See write up for why the program did not have to look like anything close to all possibly (x,y).)  </p><p>Is there some way to prove that if the only solutions for 1\le x,y\le N (some N) are the three given above, then there are no other solutions?</p><p><br/></p><p>2) Is the following problem solvable: Given p,q in Z[x,y] determine if the number of a,b such that both p(a,b) and q(a,b) are squares is finite or infinite.  AND if finite then determine how many, or a bound on how many.</p><p><br/></p><p>Can replace squares with other sets, but lets keep it simple for now. </p></div>
    </content>
    <updated>2020-09-10T13:33:00Z</updated>
    <published>2020-09-10T13:33:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-11T00:02:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=448</id>
    <link href="https://tcsplus.wordpress.com/2020/09/09/tcs-talk-thursday-september-17-richard-peng-georgia-tech/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Thursday, September 17 — Richard Peng, Georgia Tech</title>
    <summary>The next TCS+ talk will take place this coming Thursday, September 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Richard Peng from Georgia Tech will speak about “Solving Sparse Linear Systems Faster than Matrix Multiplication” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Thursday, September 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Richard Peng</strong> from Georgia Tech will speak about “<em>Solving Sparse Linear Systems Faster than Matrix Multiplication</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>. </p>



<blockquote class="wp-block-quote"><p>Abstract: Can linear systems be solved faster than matrix multiplication? While there has been remarkable progress for the special cases of graph structured linear systems, in the general setting, the bit complexity of solving an n-by-n linear system <img alt="Ax=b" class="latex" src="https://s0.wp.com/latex.php?latex=Ax%3Db&amp;bg=fff&amp;fg=444444&amp;s=0" title="Ax=b"/> is <img alt="n^\omega" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%5Comega&amp;bg=fff&amp;fg=444444&amp;s=0" title="n^\omega"/>, where <img alt="\omega &lt; 2.372864" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega+%3C+2.372864&amp;bg=fff&amp;fg=444444&amp;s=0" title="\omega &lt; 2.372864"/> is the matrix multiplication exponent. Improving on this has been an open problem even for sparse linear systems with <img alt="\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\text{poly}(n)"/> condition number.</p><p>We present an algorithm that solves linear systems in sparse matrices asymptotically faster than matrix multiplication for any <img alt="\omega&gt;2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%3E2&amp;bg=fff&amp;fg=444444&amp;s=0" title="\omega&gt;2"/>. This speedup holds for any input matrix <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=fff&amp;fg=444444&amp;s=0" title="A"/> with <img alt="o(n^{\omega-1}/\log(\kappa(A)))" class="latex" src="https://s0.wp.com/latex.php?latex=o%28n%5E%7B%5Comega-1%7D%2F%5Clog%28%5Ckappa%28A%29%29%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="o(n^{\omega-1}/\log(\kappa(A)))"/> non-zeros, where <img alt="\kappa(A)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ckappa%28A%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\kappa(A)"/> is the condition number of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=fff&amp;fg=444444&amp;s=0" title="A"/>. For <img alt="\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\text{poly}(n)"/>-conditioned matrices with <img alt="O(n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(n)"/> nonzeros, and the current value of <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=fff&amp;fg=444444&amp;s=0" title="\omega"/>, the bit complexity of our algorithm to solve to within any <img alt="1/\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="1/\text{poly}(n)"/> error is <img alt="O(n^{2.331645})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2.331645%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(n^{2.331645})"/>.</p><p>Our algorithm can be viewed as an efficient randomized implementation of the block Krylov method via recursive low displacement rank factorizations. It is inspired by the algorithm of [Eberly-Giesbrecht-Giorgi-Storjohann-Villard ISSAC <code>06</code>07] for inverting matrices over finite fields. In our analysis of numerical stability, we develop matrix anti-concentration techniques to bound the smallest eigenvalue and the smallest gap in eigenvalues of semi-random matrices.</p><p>Joint work with Santosh Vempala, manuscript at <a href="https://arxiv.org/abs/2007.10254" rel="nofollow">https://arxiv.org/abs/2007.10254</a>.</p></blockquote></div>
    </content>
    <updated>2020-09-10T03:09:50Z</updated>
    <published>2020-09-10T03:09:50Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-09-11T00:21:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04294</id>
    <link href="http://arxiv.org/abs/2009.04294" rel="alternate" type="text/html"/>
    <title>Deterministic Linear Time Constrained Triangulation using Simplified Earcut</title>
    <feedworld_mtime>1599696000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Livesu:Marco.html">Marco Livesu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cherchi:Gianmarco.html">Gianmarco Cherchi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scateni:Riccardo.html">Riccardo Scateni</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Attene:Marco.html">Marco Attene</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04294">PDF</a><br/><b>Abstract: </b>Triangulation algorithms that conform to a set of non-intersecting input
segments typically proceed in an incremental fashion, by inserting points
first, and then segments. Inserting a segment amounts to delete all the
triangles it intersects, define two polygons that fill the so generated hole
and have the segment as shared basis, and then re-triangulate each polygon
separately. In this paper we prove that the polygons generated evacuating the
triangles that intersect a constrained segment are such that all their convex
vertices but two can be used to form triangles in an earcut fashion, without
the need to check whether other polygon points are located within each ear. The
fact that any simple polygon contains at least three convex vertices guarantees
the existence of a valid ear to cut, ensuring convergence. Not only this
translates to an optimal deterministic linear time triangulation algorithm, but
such algorithm is also trivial to implement. In this paper we formally prove
the correctness of our approach, also validating it in practical applications
and comparing it with prior art.
</p></div>
    </summary>
    <updated>2020-09-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04259</id>
    <link href="http://arxiv.org/abs/2009.04259" rel="alternate" type="text/html"/>
    <title>Completeness in Polylogarithmic Time and Space</title>
    <feedworld_mtime>1599696000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferrarotti:Flavio.html">Flavio Ferrarotti</a>, Senen Gonzalez, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schewe:Klaus=Dieter.html">Klaus-Dieter Schewe</a>, Jose Maria Turull-Torres <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04259">PDF</a><br/><b>Abstract: </b>Complexity theory can be viewed as the study of the relationship between
computation and applications, understood the former as complexity classes and
the latter as problems. Completeness results are clearly central to that view.
Many natural algorithms resulting from current applications have
polylogarithmic time (PolylogTime) or space complexity (PolylogSpace). The
classical Karp notion of complete problem however does not plays well with
these complexity classes. It is well known that PolylogSpace does not have
complete problems under logarithmic space many-one reductions. In this paper we
show similar results for deterministic and non-deterministic PolylogTime as
well as for every other level of the polylogarithmic time hierarchy. We achieve
that by following a different strategy based on proving the existence of proper
hierarchies of problems inside each class. We then develop an alternative
notion of completeness inspired by the concept of uniformity from circuit
complexity and prove the existence of a (uniformly) complete problem for
PolylogSpace under this new notion. As a consequence of this result we get that
complete problems can still play an important role in the study of the
interrelationship between polylogarithmic and other classical complexity
classes.
</p></div>
    </summary>
    <updated>2020-09-10T23:20:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04114</id>
    <link href="http://arxiv.org/abs/2009.04114" rel="alternate" type="text/html"/>
    <title>Adwords in a Panorama</title>
    <feedworld_mtime>1599696000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Zhiyi.html">Zhiyi Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Qiankun.html">Qiankun Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Yuhao.html">Yuhao Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04114">PDF</a><br/><b>Abstract: </b>Three decades ago, Karp, Vazirani, and Vazirani (STOC 1990) defined the
online matching problem and gave an optimal $1-\frac{1}{e} \approx
0.632$-competitive algorithm. %introduced the Ranking algorithm with the
optimal $1-\frac{1}{e}$ competitive ratio. Fifteen years later, Mehta, Saberi,
Vazirani, and Vazirani (FOCS 2005) introduced the first generalization called
\emph{AdWords} driven by online advertising and obtained the optimal
$1-\frac{1}{e}$ competitive ratio in the special case of \emph{small bids}. It
has been open ever since whether there is an algorithm for \emph{general bids}
better than the $0.5$-competitive greedy algorithm. This paper presents a
$0.5016$-competitive algorithm for AdWords, answering this open question on the
positive end. The algorithm builds on several ingredients, including a
combination of the online primal dual framework and the configuration linear
program of matching problems recently explored by Huang and Zhang (STOC 2020),
a novel formulation of AdWords which we call the panorama view, and a
generalization of the online correlated selection by Fahrbach, Huang, Tao, and
Zadimorghaddam (FOCS 2020) which we call the panoramic online correlated
selection.
</p></div>
    </summary>
    <updated>2020-09-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04013</id>
    <link href="http://arxiv.org/abs/2009.04013" rel="alternate" type="text/html"/>
    <title>Attribute Privacy: Framework and Mechanisms</title>
    <feedworld_mtime>1599696000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Wanrong.html">Wanrong Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ohrimenko:Olga.html">Olga Ohrimenko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cummings:Rachel.html">Rachel Cummings</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04013">PDF</a><br/><b>Abstract: </b>Ensuring the privacy of training data is a growing concern since many machine
learning models are trained on confidential and potentially sensitive data.
Much attention has been devoted to methods for protecting individual privacy
during analyses of large datasets. However in many settings, global properties
of the dataset may also be sensitive (e.g., mortality rate in a hospital rather
than presence of a particular patient in the dataset). In this work, we depart
from individual privacy to initiate the study of attribute privacy, where a
data owner is concerned about revealing sensitive properties of a whole dataset
during analysis. We propose definitions to capture \emph{attribute privacy} in
two relevant cases where global attributes may need to be protected: (1)
properties of a specific dataset and (2) parameters of the underlying
distribution from which dataset is sampled. We also provide two efficient
mechanisms and one inefficient mechanism that satisfy attribute privacy for
these settings. We base our results on a novel use of the Pufferfish framework
to account for correlations across attributes in the data, thus addressing "the
challenging problem of developing Pufferfish instantiations and algorithms for
general aggregate secrets" that was left open by \cite{kifer2014pufferfish}.
</p></div>
    </summary>
    <updated>2020-09-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.03996</id>
    <link href="http://arxiv.org/abs/2009.03996" rel="alternate" type="text/html"/>
    <title>Combining Determinism and Non-Determinism</title>
    <feedworld_mtime>1599696000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fiske:Michael_Stephen.html">Michael Stephen Fiske</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03996">PDF</a><br/><b>Abstract: </b>Our goal is to construct mathematical operations that combine non-determinism
measured from quantum randomness with computational determinism so that
non-mechanistic behavior is preserved in the computation. Formally, some
results about operations applied to computably enumerable (c.e.) and bi-immune
sets are proven here, where the objective is for the operations to preserve
bi-immunity. While developing rearrangement operations on the natural numbers,
we discovered that the bi-immune rearrangements generate an uncountable
subgroup of the symmetric group on the natural numbers. The structure of this
new subgroup is unknown.
</p></div>
    </summary>
    <updated>2020-09-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.03984</id>
    <link href="http://arxiv.org/abs/2009.03984" rel="alternate" type="text/html"/>
    <title>Automatic feature-preserving size field for 3D mesh generation</title>
    <feedworld_mtime>1599696000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Arthur Bawin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henrotte:Fran=ccedil=ois.html">François Henrotte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Remacle:Jean=Fran=ccedil=ois.html">Jean-François Remacle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03984">PDF</a><br/><b>Abstract: </b>This paper presents a methodology aiming at easing considerably the
generation of high-quality meshes for complex 3D domains. We show that the
whole mesh generation process can be controlled with only five parameters to
generate in one stroke quality meshes for arbitrary geometries. The main idea
is to build a meshsize field $h(x)$ taking local features of the geometry, such
as curvatures, into account. Meshsize information is then propagated from the
surfaces into the volume, ensuring that the magnitude of $\vert \nabla h \vert$
is always controlled so as to obtain a smoothly graded mesh. As the meshsize
field is stored in an independent octree data structure, the function h can be
computed separately, and then plugged in into any mesh generator able to
respect a prescribed meshsize field. The whole procedure is automatic, in the
sense that minimal interaction with the user is required. Applications examples
based on models taken from the very large ABC dataset, are then presented, all
treated with the same generic set of parameter values, to demonstrate the
efficiency and the universality of the technique.
</p></div>
    </summary>
    <updated>2020-09-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.03924</id>
    <link href="http://arxiv.org/abs/2009.03924" rel="alternate" type="text/html"/>
    <title>Variational wavefunctions for Sachdev-Ye-Kitaev models</title>
    <feedworld_mtime>1599696000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Arijit Haldar, Omid Tavakol, Thomas Scaffidi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03924">PDF</a><br/><b>Abstract: </b>Given a class of $q$-local Hamiltonians, is it possible to find a simple
variational state whose energy is a finite fraction of the ground state energy
in the thermodynamic limit? Whereas product states often provide an affirmative
answer in the case of bosonic (or qubit) models, we show that Gaussian states
fail dramatically in the fermionic case, like for the Sachdev-Ye-Kitaev (SYK)
models. This prompts us to propose a new class of wavefunctions for SYK models
inspired by the variational coupled cluster algorithm. We introduce a static
("0+0D") large-$N$ field theory to study the energy, two-point correlators, and
entanglement properties of these states. Most importantly, we demonstrate a
finite disorder-averaged approximation ratio of $r \approx 0.62$ between the
variational and ground state energy of SYK for $q=4$. Moreover, the variational
states provide an exact description of spontaneous symmetry breaking in a
related two-flavor SYK model.
</p></div>
    </summary>
    <updated>2020-09-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/135</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/135" rel="alternate" type="text/html"/>
    <title>TR20-135 |  Estimation of Graph Isomorphism Distance in the Query World | 

	Sayantan Sen, 

	Sourav Chakraborty, 

	Arijit Ghosh, 

	Gopinath Mishra</title>
    <summary>The graph isomorphism distance between two graphs $G_u$ and $G_k$ is the fraction of entries in the adjacency matrix that has to be changed to make $G_u$ isomorphic to $G_k$. We study the problem of estimating, up to a constant additive factor, the graph isomorphism distance between two graphs in the query model. In other words, if $G_k$ is a known graph and $G_u$ is an unknown graph whose adjacency matrix has to be accessed by querying the entries, what is the query complexity for testing whether the graph isomorphism distance between $G_u$ and $G_k$ is less than $\gamma_1$ or more than $\gamma_2$, where $\gamma_1$ and $\gamma_2$ are two constants with $0\leq \gamma_1 &lt; \gamma_2 \leq 1$. It is also called the tolerant property testing of graph isomorphism in the dense graph model. The non-tolerant version (where $\gamma_1$ is $0$) has been studied by Fischer and Matsliah (SICOMP'08). 


In this paper, we study both the upper and lower bounds of tolerant graph isomorphism testing. We prove an upper bound of $\widetilde{{\cal O}}(n)$ for this problem. Our upper bound algorithm crucially uses the tolerant testing of the well studied Earth Mover Distance (EMD), as the main subroutine, in a slightly different setting from what is generally studied in property testing literature.


Testing tolerant EMD between two probability distributions is equivalent to testing EMD between two multi-sets, where the multiplicity of each element is taken appropriately, and we sample elements from the unknown multi-set with replacement. In this paper, our (main conceptual) contribution is to introduce the problem of tolerant EMD testing between multi-sets (over Hamming cube) when we get samples from the unknown multi-set without replacement and to show that this variant of tolerant testing of EMD is as hard as tolerant testing of graph isomorphism between two graphs. Thus, while testing of equivalence between distributions is at the heart of the non-tolerant testing of graph isomorphism, we are showing that the estimation of the EMD over a Hamming cube (when we are allowed to sample without replacement) is at the heart of 
tolerant graph isomorphism. We believe that the introduction of the problem of testing EMD between multi-sets (when we get samples without replacement) opens an entirely new direction in the world of testing properties of distributions.</summary>
    <updated>2020-09-09T20:48:09Z</updated>
    <published>2020-09-09T20:48:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-11T00:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1387</id>
    <link href="https://ptreview.sublinear.info/?p=1387" rel="alternate" type="text/html"/>
    <title>News for August 2020</title>
    <summary>Last month saw action in property testing across the board: graphs, distributions and functions were all objects of study in papers that came out last month. Also included is a separation result between quantum and classical query complexities resolving a conjecture of Aaronson and Ambainis. Here is a brief report. Testing asymmetry in bounded degree […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Last month saw action in property testing across the board: graphs, distributions and functions were all objects of study in papers that came out last month. Also included is a separation result between quantum and classical query complexities resolving a conjecture of Aaronson and Ambainis. Here is a brief report.</p>



<p><strong>Testing asymmetry in bounded degree graphs</strong>, by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2020/118/">ECCC</a>). This paper studies a natural graph property hitherto not considered in the property testing literature. Namely, the question of testing whether a graph is <em>asymmetric</em> or whether it is far from being asymmetric. A graph is said to be asymmetric if its automorphism group is trivial (that is, it only contains the identity permutation). One of the results in the paper says that this problem is easy in the dense graph model – which is a side result of the paper. This is because all dense graphs are \(O(\log n/n)\)-close to being asymmetric. To see this, the paper points out that a simple randomized process which takes \(G\) as input and returns an asymmetric graph by changing very few edges. This process asks you to do the following: Take a set \(S \subseteq V\) with \(|S| = O(\log n)\) nodes and replace the subgraph they induce with a random graph. Moreover,  randomize all the edges between \(S\) and \(V \setminus S\). What you can show is that in this modified graph, any automorphism (whp) will map \(S\) to itself. And all the remaining vertices behave (whp) in a unique manner which is peculiar to it. In particular, this means that any automorphism better not map a vertex \(v\) in \(V \setminus S\) to any other vertex in \(V \setminus S\). And this finishes the argument. The main result explores the bounded degree model. By a simple argument, you can show that testing asymmetry is easy if all the connected components have size \(s(n) \leq o\left(\frac{\log n}{\log {\log n}}\right)\). Thus, the challenging case is when you have some connected components of a larger size. In this case, what Goldreich shows is the following: If all the components have size at most \(s(n)\) where \(s(n) \geq \Omega\left(\frac{\log n}{\log {\log n}}\right)\), then you can test asymmetry (with one-sided-error) in \(O(n^{1/2} \cdot s(n)/\epsilon)\) queries.  Moreover, the paper also shows a two-sided-lower bound of \(\Omega\left(n/s(n)\right)^{1/2}\) queries which holds as long as \(\epsilon \leq O(1/s(n)\). This leaves open the bounded degree question of determining the query complexity of testing asymmetry in the general case as the paper also points out.</p>



<p/>



<p><strong>On testability of first order properties in Bounded degree graphs</strong>, by Isolde Adler, Noleen Köhler and Pan Peng (<a href="https://arxiv.org/pdf/2008.05800.pdf">arXiv</a>). One of the well understood motivations for property testing begins in the following manner. Decision problems are typically hard to solve if they involve a universal quantifier — either \(\exists\) or  \(\forall\). One way around this hardness is to do the following ritual: relax the problem by dropping the universal quantifier, define a notion of distance between objects in your universe and ask a promise problem instead. Indeed, if you take your favorite property testing problem, you will note that it precisely fits in the template above. How about making this business more rigorous in bounded degree graph property model? This is precisely the content of this work which considers the face off between (First Order) logic and property testing in the bounded degree model. The authors show some interesting results. They begin by showing that in the bounded degree model, you can show that all first order graph properties which involve a single quantifier \(Q \in \{\forall, \exists\}\) are testable with constant query complexity.<br/>If you find this baffling, it would be good to remind yourself that not all graph properties can be expressed in the language of first order logic with a single quantifier! So, you can rest easy. The graph properties you know are not constant time testable are most assuredly not expressible with a single quantifier in First Order Logic. However, this work shows more. It turns out, that “any FO property that is defined by a formula with quantifier prefix \(\exists^* \forall^*\) is testable”. Moreover, there do exist FO graph properties defined by the quantifier prefix \(\forall^* \exists^*\) which are not testable. Thus, this work achieves results in the bounded degree model which are kind of analogous to the results in the dense graph model by Alon et al [1]. On a final note, I find the following one sentence summary of the techniques used to prove the lower bound rather intriguing: “[the paper] obtains the lower bound by a first-order formula that defines a class of bounded-degree expanders, based on zig-zag products of graphs.”  </p>



<p/>



<p><strong>On graphs of bounded degree that are far from being Hamiltonian</strong>, by Isolde Adler and Noleen Köhler (<a href="https://arxiv.org/abs/2008.05801">arXiv</a>). This paper explores the question of testing Hamiltonicity in the bounded degree model. The main result of the paper is that Hamiltonicity is not testable with one-sided error with \(o(n)\) queries. PTReview readers might recall from our <a href="https://ptreview.sublinear.info/?p=1371">July Post</a> a concurrent paper by Goldriech [2] which achieves the same lower bound on query complexity in the two-sided error model (the authors call attention to [2] as well). One of the interesting feature of this result is that the lower bounds are obtained by an explicit deterministic reduction as opposed to the usual randomized reduction. Like the authors point out, this offers more insights into structural complexity of instances that are far from being Hamiltonian. We point out that this also differs from how the lower bound is derived in [2] — which is via local hardness reductions to a promise problem of 3 CNF satisfiability.</p>



<p/>



<p><strong>An optimal tester for \(k\)-linear</strong>, by Nader Bshouty (<a href="https://eccc.weizmann.ac.il/report/2020/123/">ECCC</a>). This paper explores two related questions. We call a function \(f \colon \{0,1\}^n \to \{0,1\}\) \(k\)-linear if it equals the \(\sum_{i \in S} x_i\) for some \(S \subseteq [n]\) of size exactly \(k\). A boolean function is said to be \(k\)-linear<strong>*</strong> if it is \(j\) linear for a fixed  \(j\) where \(j \in \{0,1,2, \cdots, k\}\). The paper proves the following theorems.</p>



<ol><li>There exists a non-adaptive <em>one-sided</em> distribution free tester for \(k\)-linear<strong>*</strong> with query complexity being \(O\left(k \log k + \frac{1}{\varepsilon}\right)\). This matches the two-sided lower bound (where the underlying distribution is uniform) by Blais et al [3].</li><li>Using a reduction from \(k\)-linear<strong>*</strong> to \(k\)-linear, the paper shows one can obtain a non-adpative <em>two-sided</em> distribution free tester for \(k\)-linear with same query complexity as the above result. The lower bound from Blais et al applies here also (in fact, they prove a lower bound on \(k\)-linearity).</li><li>Next up, the paper has a couple of lower bound results to accompany this. One of these results reveals the price you pay for being <em>one-sided</em> and <em>exact</em> (that is, you insist on the function being exactly \(k\)-linear). Turns out, now you have a non-adaptive one-sided uniform distribution lower bound of \(\widetilde{\Omega}(k) \log n + \Omega(1/\varepsilon)\).  If you allow adaptivity instead, the paper shows a lower bound of \(\widetilde{\Omega}(\sqrt k)\log n + \Omega(1/\varepsilon)\).</li></ol>



<p/>



<p><strong>Amortized Edge Sampling</strong>, by Talya Eden, Saleet Mossel and Ronitt Rubinfeld (<a href="https://arxiv.org/abs/2008.08032">arXiv</a>). Consider the following setup. You are given query access to adjacency list of a graph \(G\) with \(n\) vertices and \(m\) edges. You can make degree queries and neighbor queries. Suppose I ask you to sample a single edge from this graph from a distribution that is pointwise \(\varepsilon\) close to the uniform distribution. Eden and Rosenbaum already showed how you can achieve this with a budget of \(\widetilde{\Theta}(n/\sqrt m)\) queries. Starting from this jump off point, the authors ask whether you can circumvent this lower bound if you want to return multiple samples from a distribution which is again pointwise close to uniform. The paper answers this question in the affirmative and shows that if you know the number of samples, \(q\), in advance you can get away with an amortized bound of \(O*(\sqrt q n/\sqrt m)\) on the total number of queries needed.</p>



<p/>



<p><strong>On the High Accuracy Limitation of Adaptive Property Estimation</strong>, by Yanjun Han (<a href="https://arxiv.org/abs/2008.11964">arXiv</a>). Take a discrete distribution \(\mathcal{P}\) with support size \(k\) and consider the task of estimating some symmetric property of \(\mathcal{P}\) to a small \(\pm \varepsilon\) additive error. Here, a symmetric property refers to a “nice” functional defined over the probability simplex, i.e., it refers to functions \(F \colon \Delta_k \to \mathbb{R}\) where \(F(p) = \sum_{i=1}^{k} f(p_i)\) where \(f \colon (0,1) \to \mathbb{R}\). A naive attack to these estimation tasks goes through the following ritual: you get your hands on the empirical distribution, you plug it in \(F\) and you hope for the best. Turns out, you are out of luck if the function \(f\) is non-smooth and in these cases you end up with a suboptimal estimator. Previous works have also looked at more sophisticated estimators (like the <em>local moment matching or LMM and profile maximum likelihood or PML</em> estimator). Turns out, using the LMM or PML estimator leads to optimal sample complexity for a handful of symmetric properties (as long as \(\varepsilon \geq n^{-1/3}\)). This paper considers the question of what can you say for supersmall values of \(\varepsilon\) where \(n^{-1/2} \leq \varepsilon \leq n^{-1/3}\). (The \(n^{-1/2}\) appears because there are estimators that use the knowledge of \(f\) and \(\varepsilon\) can be driven all the way down to \(n^{-1/2}\) for these estimators). The paper focuses on estimators which do not exploit any structure in \(f\). In particular, the paper specializes this question to PML and shows a fundamental limitation on PML which means that the PML approach fails to be sample optimal for the entire range of \(\varepsilon\) and is sample optimal only for \(\varepsilon &gt;&gt; n^{-1/3}\) — which also confirms a conjecture of Han and Shiragur (and refutes a conjecture of Acharya et al. who postulated this is sample optimal for the entire range of \(\varepsilon\)).</p>



<p/>



<p><strong>\(k\)-Forrelation Optimally Separates Quantum and Classical Query<br/>Complexity</strong>, by Nikhil Bansal and Makrand Sinha (<a href="https://arxiv.org/abs/2008.07003">arXiv</a>). Understanding the power of quantum query over classical queries is a well motivated problem with a rich history. One of the biggest questions in this area asks for the largest separation between classical and quantum query complexities. In a breakthrough, Aaronson and Ambainis [4] showed a fundamental simulation result which confirmed that you can simulate \(q\) quantum queries with \(O(N^{1 – 1/2q})\) classical queries in the randomized decision tree model of computation as long as \(q = O(1)\). In the same paper, the authors also showed that the standard <em>forrelation</em>* problem exhibits a \(1\) versus \(\widetilde{\Omega}(\sqrt n)\) separation. This means that for \(q = 1\), you essentially have optimal separation. But what about \(q &gt; 1\)? To this end, Aaronson and Ambainis conjectured that a certain problem which they called \(k\)-forrelation — which can be computed with \(q = k/2\) queries requires at least \(\widetilde{\Omega}(n^{1-1/k})\) classical queries. The current work precisely confirms this conjecture.</p>



<p>(*) The forrelation problem asks you to decide whether one Boolean function is highly correlated with the Fourier transform of a second function.</p>



<p><em><strong>(Edit:</strong> Added Later)</em>  Simon Apers points out a paper by Shrestov, Storozhenko and Wu that we missed. (Thanks Simon)! Here is a brief report on that paper.<br/></p>



<p><strong>An optimal separation of randomized and quantum query complexity</strong> (by Alexander Shrestov, Andrey Storozhenko and Pei Wu)(<a href="https://arxiv.org/abs/2008.10223">arXiv</a>) Similar to the paper by Bansal and Sinha [BS20] mentioned above, this paper also resolves the conjecture by Aaronson and Ambainis proving the same result. Like the paper also notes, the techniques in both of these works are completely different and incomparable. On the one hand [BS20] proves the separation for an explicit function as opposed to a function chosen uniformly at random from a certain set as considered in this work. On the other hand,  the separation result shown in [BS20] only applies when the query algorithm returns the correct answer with probability at least \(1/2 + 1/poly(\log n)\) — in contrast the results in this paper apply even when the query algorithm is required to have probability of correctness be a constant at least \(1/2\). In addition, this work also proves the \(\ell\)-Fourier weight conjecture of Tal which is of independent interest beyond quantum computing.</p>



<p/>



<p>So, it looks like all in all we had a great month with two concurrent papers both resolving Aaronson Ambainis conjecture (yet again after two concurrent papers on testing Hamiltonicity)!</p>



<p><strong>References</strong>:</p>



<p>[1] Noga Alon, Eldar Fischer, Michael Krivelevich, and Mario Szegedy. Efficient testing of<br/>large graphs. Combinatorica, 20(4):451–476, 2000.</p>



<p><br/>[2] Oded Goldreich. On testing hamiltonicity in the bounded degree graph model. Electronic Colloquium on Computational Complexity (ECCC), (18), 2020</p>



<p>[3] Eric Blais, Joshua Brody, and Kevin Matulef. Property testing lower bounds via communication complexity. <em>CCC 2011</em></p>



<p>[4] Scott Aaronson and Andris Ambainis. Forrelation: A problem that optimally separates quantum from classical computing. SIAM J. Comput., 47(3):982–1038, 2018</p></div>
    </content>
    <updated>2020-09-09T03:55:44Z</updated>
    <published>2020-09-09T03:55:44Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>akumar</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-09-10T23:27:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/134</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/134" rel="alternate" type="text/html"/>
    <title>TR20-134 |  Tight Bounds on Sensitivity and Block Sensitivity of Some Classes of Transitive Functions | 

	Anna Gal, 

	Siddhesh Chaubal</title>
    <summary>Nisan and Szegedy conjectured that block sensitivity is at most
polynomial in sensitivity for any Boolean function.
Until a recent breakthrough of Huang, the conjecture had been
wide open in the general case,
and was proved only for a few special classes
of Boolean functions.
Huang's result implies that block sensitivity is at most
the 4th power of sensitivity for any Boolean function.
It remains open if a tighter relationship between
sensitivity and block sensitivity holds for arbitrary Boolean functions;
the largest known gap between these measures is quadratic.

We prove tighter bounds showing that block sensitivity is at most
3rd power, and in some cases at most square of sensitivity for
subclasses of transitive functions,
defined by various properties of their DNF (or CNF) representation.
Our results improve and extend previous results regarding
transitive functions. We obtain these results by
proving tight (up to constant factors) lower bounds on the
smallest possible sensitivity of functions in these classes.

In another line of research, it has also been examined what is the
smallest possible block sensitivity of transitive functions.
Our results yield tight (up to constant factors) lower bounds
on the block sensitivity of the classes we consider.</summary>
    <updated>2020-09-08T23:39:46Z</updated>
    <published>2020-09-08T23:39:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-11T00:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/133</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/133" rel="alternate" type="text/html"/>
    <title>TR20-133 |  Query complexity lower bounds for local list-decoding and hard-core predicates (even for small rate and huge lists) | 

	Noga Ron-Zewi, 

	Ronen Shaltiel, 

	Nithin Varma</title>
    <summary>A binary code $\text{Enc}:\{0,1\}^k \rightarrow \{0,1\}^n$ is $(\frac{1}{2}-\varepsilon,L)$-list decodable if for every $w \in \{0,1\}^n$, there exists a set $\text{List}(w)$ of size at most $L$, containing all messages $m \in \{0,1\}^k$ such that the relative Hamming distance between $\text{Enc}(m)$ and $w$ is at most $\frac{1}{2}-\varepsilon$.
A $q$-query local list-decoder is a randomized procedure that when given oracle access to a string $w$, makes at most $q$ oracle calls, and for every message $m \in \text{List}(w)$, with high probability, there exists $j \in [L]$ such that for every $i \in [k]$, with high probability, $\text{Dec}^w(i,j)=m_i$.

We prove lower bounds on $q$, that apply even if $L$ is huge (say $L=2^{k^{0.9}}$) and the rate of $\text{Enc}$ is small (meaning that $n \ge 2^{k}$):

* For $\varepsilon = 1/k^{\nu}$ for some constant $\nu&gt;0$, we prove a lower bound of $q=\Omega(\frac{\log(1/\delta)}{\varepsilon^2})$, where $\delta$ is the error probability of the local list-decoder. This bound is tight as there is a matching upper bound by Goldreich and Levin (STOC 1989) of $q=O(\frac{\log(1/\delta)}{\varepsilon^2})$ for the Hadamard code (which has $n=2^k$). This bound extends an earlier work of Grinberg, Shaltiel and Viola (FOCS 2018) which only works if $n \le 2^{k^{\nu}}$ and the number of coins tossed by $\text{Dec}$ is small (and therefore does not apply to the Hadamard code, or other codes with low rate).

* For smaller $\varepsilon$, we prove a lower bound of roughly $q = \Omega(\frac{1}{\sqrt{\varepsilon}})$. To the best of our knowledge, this is the first lower bound on the number of queries of local list-decoders that gives $q \ge k$ for small $\varepsilon$.

Local list-decoders with small $\varepsilon$ form the key component in the celebrated theorem of Goldreich and Levin that extracts a hard-core predicate from a one-way function.
We show that black-box proofs cannot improve the Goldreich-Levin theorem and produce a hard-core predicate that is hard to predict with probability $\frac{1}{2}+\frac{1}{\ell^{\omega(1)}}$ when provided with a one-way function $f:\{0,1\}^{\ell} \rightarrow \{0,1\}^{\ell}$, such that circuits of size $\text{poly}(\ell)$ cannot invert $f$ with probability $\rho=1/2^{\sqrt{\ell}}$ (or even $\rho=1/2^{\Omega(\ell)}$). This limitation applies to any proof by black-box reduction (even if the reduction is allowed to use nonuniformity and has oracle access to $f$).</summary>
    <updated>2020-09-08T17:41:18Z</updated>
    <published>2020-09-08T17:41:18Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-11T00:20:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite</id>
    <link href="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite.html" rel="alternate" type="text/html"/>
    <title>Eberhard’s theorem for bipartite polyhedra with one big face</title>
    <summary>Eberhard’s theorem is a topic in the combinatorial theory of convex polyhedra that once saw a lot of research, but has faded from more recent interest. It’s named after Victor Eberhard, a German mathematician from the late 19th and early 20th century who worked in geometry despite becoming blind at age 12 or 13. I find this hard to imagine, as my own research in geometry is based very heavily on visual thinking, but he was far from the only successful blind mathematician; Leonhard Euler, Lev Pontryagin, and Bernard Morin also come to mind, and there are more.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://en.wikipedia.org/wiki/Eberhard%27s_theorem">Eberhard’s theorem</a> is a topic in the combinatorial theory of convex polyhedra that once saw a lot of research, but has faded from more recent interest. It’s named after <a href="https://en.wikipedia.org/wiki/Victor_Eberhard">Victor Eberhard</a>, a German mathematician from the late 19th and early 20th century who worked in geometry despite becoming blind at age 12 or 13. I find this hard to imagine, as my own research in geometry is based very heavily on visual thinking, but he was far from the only successful blind mathematician; <a href="https://en.wikipedia.org/wiki/Leonhard_Euler">Leonhard Euler</a>, <a href="https://en.wikipedia.org/wiki/Lev_Pontryagin">Lev Pontryagin</a>, and <a href="https://en.wikipedia.org/wiki/Bernard_Morin">Bernard Morin</a> also come to mind, and there are more.</p>

<p>Anyway, Eberhard’s theorem concerns the following question. Suppose I tell you that a polyhedron has a certain number of faces of certain types. For instance, after Archimedes’ work on polytopes was lost, all we knew about the <a href="https://en.wikipedia.org/wiki/Archimedean_solid">Archimedean solids</a> until their rediscovery in the Renaissance was a brief listing from <a href="https://en.wikipedia.org/wiki/Pappus_of_Alexandria">Pappus of Alexandria</a> giving this information: there is one with 8 triangles and 6 squares, etc. How can we tell that these counts of faces actually determine a polyhedron?</p>

<p>The given information for Eberhard’s theorem, then, is just a collection of counts of face types (triangles, quadrilaterals, etc.), without specifying the exact shapes of these faces. The goal is to use these faces to build a simple polyhedron, one for which three edges meet at every vertex (like a cube, unlike an octahedron). One necessary condition for this to be possible is that the polyhedron must obey Euler’s polyhedral formula \(v-e+f=2\). And it’s easy to calculate the numbers of vertices, edges, and faces appearing in this formula, from the face counts. Plugging these numbers into Euler’s formula leads to a linear equation that the face counts must obey. Crucially, this linear equation omits the count of hexagons: adding or removing hexagons will not change whether Euler’s formula holds. What Eberhard’s theorem states is that, as long as the face counts obey Euler’s formula in this way, there is always some number of hexagons that can be added or removed so that the remaining faces will form a polyhedron.</p>

<p>However, calculating the fewest number of hexagons needed, or even determining whether a given number of faces of all types (including hexagons) can be put together into a polyhedron, remains somewhat mysterious. So I thought I’d play with a case that would be both simple enough to solve and still interesting: the bipartite simple polyhedra (famous from <a href="https://en.wikipedia.org/wiki/Barnette%27s_conjecture">Barnette’s conjecture</a>), with one big face (a \(2n\)-gon for some \(n&gt;3\)), many small faces (\(n+3\) quadrilaterals, the number needed to make Euler’s formula hold), and a mysterious number of hexagons. What is the smallest number of hexagons that will allow the construction of a simple polyhedron with these face counts? The answer turns out to be \(\lfloor (3n-6)/2\rfloor\), achieved with polyhedra (or polyhedral graphs) in which the outer \(2n\)-gon surrounds a <a href="https://en.wikipedia.org/wiki/Cactus_graph">cactus tree</a> of 6-vertex cycles (and possibly one 4-vertex cycle), connected to each other by bridge edges:</p>

<p style="text-align: center;"><img alt="Three hexagon-minimal bipartite simple polyhedra" src="https://11011110.github.io/blog/assets/2020/eberhard.svg"/></p>

<p>The central cactus tree can be rearranged, as long as no two bridge edges have adjacent endpoints. For instance, in the graph with the dodecagon outer face, at the bottom of the figure, it’s possible for the middle six-vertex loop to have three connections to the outside polygon on one side and only one connection on the other side, or to have the four-vertex loop in the middle. But I can prove that all optimal solutions have the same overall central cactus tree structure.</p>

<p>I find it easier to think about the following equivalent rephrasing of the optimization problem: instead of finding a minimum number of hexagons that will allow us to build a polyhedron with those face counts, let’s build a polyhedron with one \(2n\)-gon face and the rest quadrilaterals and hexagons, and concentrate on minimizing the number of vertices in this polyhedron. The number of quadrilaterals will automatically come out right, and the number of hexagons will be minimized if the number of vertices is minimized.</p>

<p>Now suppose that we have any simple polyhedron with one \(2n\)-gon face and the rest quadrilaterals and hexagons. Remove the outer \(2n\)-gon from the graph, leaving a conncted subgraph, and look at the biconnected components of this subgraph. For any one component, its outer face in its induced planar embedding must be a simple cycle, with some vertices having degree two in the component (the endpoints of edges connecting the component to the rest of the graph) and some having degree three. If the component is a 4-cycle or 6-cycle, then all of its vertices have degree two. But if not, then at most four consecutive vertices of its outer cycle can have degree two, because they and the two vertices connected to them on both sides form part of the boundary of a face interior to the component, which can have at most six vertices. And the degree-three vertices of the outer cycle must come in consecutive pairs, which cannot be adjacent to the endpoints of bridge edges connecting to other biconnected components, because a degree-three vertex next to a bridge edge or next to two other degree-three vertices would combine with part of the outer \(2n\)-gon to form a face with seven or more vertices, and a degree-three vertex by itself would form a pentagon, neither of which is allowed.</p>

<p>So in a component that is not a 4-cycle or 6-cycle, the degree-two and degree-three vertices alternate around the outer cycle of the component in consecutive sequences of at most four and exactly two vertices. This implies that the number of degree-two vertices is even (because the whole cycle is even by bipartiteness) and that the number of degree-three vertices in the component (even just counting the ones on its boundary) is at least half of the number of degree-two vertices on its boundary. For the cactus trees that we’ve been using, on the other hand, the number of degree-three vertices in each cactus tree is strictly less than half of the number of degree-two vertices. So if we replace a whole non-cycle component by a cactus tree, we can get a graph with the same number of exposed degree-2 vertices, but fewer total vertices. After repeated replacement of biconnected components, at each step reducing the number of vertices, we would reach a state where the subgraph inside the \(2n\)-gon is a cactus tree. It might not meet the requirement that its bridge edges have nonadjacent endpoints, but it could always be rearranged to do so. And it might not be a cactus with at most one 4-cycle, but if not we could replace two 4-cycles by one 6-cycle and make it even smaller. So the only graphs that cannot be made smaller are the ones we started with, the cactus trees of 6-cycles and at most one 4-cycle, surrounded by an outer \(2n\)-gon.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104827671950147352">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-09-07T22:28:00Z</updated>
    <published>2020-09-07T22:28:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-09-08T07:49:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1108</id>
    <link href="http://corner.mimuw.edu.pl/?p=1108" rel="alternate" type="text/html"/>
    <title>IGAFIT Algorithmic Colloquium</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">We are excited to announce a new online seminar - IGAFIT Algorithmic Colloquium. This new event aims to integrate the European algorithmic community and keep it connected during the times of the pandemic. This online seminar will take place biweekly on … <a href="http://corner.mimuw.edu.pl/?p=1108">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are excited to announce a new online seminar - IGAFIT Algorithmic Colloquium. This new event aims to integrate the European algorithmic community and keep it connected during the times of the pandemic. This online seminar will take place biweekly on Thursday at 14:00 CET, with the talks lasting for 45 minutes. Each talk will be followed by a networking and discussion session on topics related to the talk. We cordially invite all participants to this session. The meeting will be run on <a href="https://www.airmeet.com/e/55923fa0-eee9-11ea-8530-b3eab1e75816" rel="noreferrer noopener" target="_blank">Airmeet</a>. More details on the event can be found on <a href="http://igafit.mimuw.edu.pl/?page_id=483786" rel="noreferrer noopener" target="_blank">IGAFIT web page</a>.</p>



<p>The first talk will be held on the 1st of October 2020.</p>



<p>October 1, 2020<br/>Vera Traub, University of Bonn<br/>Title: An improved approximation algorithm for ATSP<br/>Abstract: In a recent breakthrough, Svensson, Tarnawski, and Végh gave the first constant-factor approximation algorithm for the asymmetric traveling salesman problem (ATSP). In this work we revisit their algorithm. While following their overall framework, we improve on each part of it.</p>



<p>Svensson, Tarnawski, and Végh perform several steps of reducing ATSP to more and more structured instances. We avoid one of their reduction steps (to irreducible instances) and thus obtain a simpler and much better reduction to vertebrate pairs. Moreover, we show that a slight variant of their algorithm for vertebrate pairs has a much smaller approximation ratio.</p>



<p>Overall we improve the approximation ratio from 506 to 22 + ε for any ε &gt; 0. We also improve the upper bound on the integrality ratio of the standard LP relaxation from 319 to 22.</p>



<p>This is joint work with Jens Vygen.</p>



<p>Other upcoming talks include:</p>



<p>October 15, 2020<br/>Thatchaphol Saranurak, Toyota Technological Institute at Chicago<br/>Title: An almost-linear time deterministic algorithm for expander decomposition</p>



<p>October 29, 2020<br/>Nathan Klein, University of Bonn<br/>Title: A (Slightly) Improved Approximation Algorithm for Metric TSP</p>



<p>For more details please contact the Organization Committee:<br/>Nikhil Bansal<br/>Artur Czumaj<br/>Andreas Feldmann<br/>Adi Rosén<br/>Eva Rotenberg<br/>Piotr Sankowski<br/>Christian Sohler <br/></p></div>
    </content>
    <updated>2020-09-07T20:33:10Z</updated>
    <published>2020-09-07T20:33:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2020-09-10T23:26:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=2727</id>
    <link href="https://francisbach.com/integration-by-parts-randomized-smoothing-score-functions/" rel="alternate" type="text/html"/>
    <title>The many faces of integration by parts – II : Randomized smoothing and score functions</title>
    <summary>This month I will follow-up on last month blog post and look at another application of integration by parts, which is central to many interesting algorithms in machine learning, optimization and statistics. In this post, I will consider extensions in higher dimensions, where we take integrals on a subset of \(\mathbb{R}^d\), and focus primarily on...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">This month I will follow-up on last month blog post and look at another application of integration by parts, which is central to many interesting algorithms in machine learning, optimization and statistics. In this post, I will consider extensions in higher dimensions, where we take integrals on a subset of \(\mathbb{R}^d\), and focus primarily on property of the so-called “score function” of a density \(p: \mathbb{R}^d \to \mathbb{R}\), namely the gradient of its logarithm: $$\nabla  \log  p(z)  = \frac{1}{p(z)} \nabla p(z) \in \mathbb{R}^d,$$ or, done coordinate by coordinate, $$ \big(\nabla \log p(z)\big)_i = \frac{\partial [ \log p]}{\partial z_i}(z) = \frac{1}{p(z)} \frac{\partial  p }{\partial z_i}(z) .$$ Note here that we take derivatives with respect to \(z\) and not with respect to some hypothetical external parameter, which is often the case in statistics (see <a href="https://en.wikipedia.org/wiki/Score_(statistics)">here</a>).</p>



<p class="justify-text">As I will show below, this quantity comes up in many different areas, most often used with integration by parts. After a short review on integration by parts and its applications to score functions, I will present four quite diverse applications, to (1) optimization and randomized smoothing, (2) differentiable perturbed optimizers, (3) learning single-index models in statistics, and (4) score matching for density estimation.</p>



<h2>Integration by parts in multiple dimensions</h2>



<p class="justify-text">I will focus only on situations where we have some random variable \(Z\) defined on \(\mathbb{R}^d\), with differentiable strictly positive density \(p(\cdot)\) with respect to the Lebesgue measure (I could also consider bounded supports, but then I would need to use the <a href="https://en.wikipedia.org/wiki/Divergence_theorem">divergence theorem</a>). I will consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), and my goal is to provide an expression of \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] \in \mathbb{R}^d\) using the gradient of \(f\).</p>



<p class="justify-text">Assuming that \(f(z) p(z)\) goes to zero when \(\| z\| \to +\infty\), we have: $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big]  = \int_{\mathbb{R}^d} f(z)\Big( \frac{1}{p(z)} \nabla p (z) \Big) p(z) dz = \int_{\mathbb{R}^d}  f (z) \nabla p(z) dz .$$ We can then use integration by parts (together with the zero limit at infinity), to get $$\int_{\mathbb{R}^d} f (z) \nabla p(z) dz = \ – \int_{\mathbb{R}^d} p (z) \nabla f(z) dz.$$ This leads to $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\  – \mathbb{E} \big[ \nabla f(Z) \big]. \tag{1}$$ In other words, expectations of the gradient of \(f\) can be obtained through expectations of \(f\) times the negative of the score function.  </p>



<p class="justify-text">Note that Eq. (1) can be used in the two possible directions: to estimate the right hand side (expectation of gradients) when the score function is known, and vice-versa to estimate expectations (as a simple example, when \(f\) is constant equal to one, we get the traditional identity \(\mathbb{E} \big[ \nabla \log p(Z) \big] = 0\)).</p>



<p class="justify-text"><strong>Gaussian distribution.</strong> Assuming that \(p(z) = \frac{1}{(2\pi \sigma^2)^{d/2}} \exp\big( – \frac{1}{2 \sigma^2}\|  z – \mu\|_2^2 \big)\), that is, \(Z\) is normally distributed with mean vector \(\mu \in \mathbb{R}^d\) and covariance matrix \(\sigma^2 I\), we get a particularly simple expression $$\frac{1}{\sigma^2} \mathbb{E} \big[ f(Z) (Z-\mu)  \big] =  \mathbb{E} \big[ \nabla f(Z) \big],$$ which is often referred to as <a href="https://en.wikipedia.org/wiki/Stein%27s_lemma">Stein’s lemma</a> (see for example an application to <a href="https://en.wikipedia.org/wiki/Stein%27s_unbiased_risk_estimate">Stein’s unbiased risk estimation</a>).</p>



<p class="justify-text"><strong>Vector extension.</strong> If now \(f\) has values in \(\mathbb{R}^d\), still with the product \(f(z) p(z)\) going to zero when \(\| z\| \to +\infty\), we get $$\mathbb{E} \big[ f(Z)^\top \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla \!\cdot \! f(Z) \big], \tag{2}$$ where \(\nabla\! \cdot \! f\) is the <a href="https://en.wikipedia.org/wiki/Divergence">divergence</a> of \(f\) defined as \(\displaystyle \nabla\! \cdot\! f(z) = \sum_{i=1}^d \frac{\partial f}{\partial z_i}(z)\). </p>



<h2>Optimization and randomized smoothing</h2>



<p class="justify-text">We consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), which is  non-differentiable everywhere. There are several ways of <em>smoothing</em> it. A very traditional way is to convolve it with a smooth function. In our context, this corresponds to considering $$f_\varepsilon(x) = \mathbb{E} f(x+ \varepsilon Z) = \int_{\mathbb{R}^d} f(x+\varepsilon z) p(z) dz,$$ where \(z\) is a random variable with strictly positive sufficiently differentiable density, and \(\varepsilon \) is a positive parameter. Typically, if \(f\) is Lipschitz-continuous, \(| f – f_\varepsilon|\) is uniformly bounded by a constant times \(\varepsilon\).</p>



<p class="justify-text">Let us now assume that we can take gradients within the integral, leading to: $$\nabla f_\varepsilon(x) = \int_{\mathbb{R}^d}   \nabla f(x+\varepsilon z) p(z) dz = \mathbb{E} \big[  \nabla f(x+\varepsilon z) \big].$$ This derivation is problematic as the whole goal is to apply this to functions \(f\) which are not everywhere differentiable, so the gradient \(\nabla f\) is not always defined. It turns out that when \(p\) is sufficiently differentiable, integration by parts exactly provides an expression which does not imply the gradient of \(f\).</p>



<p class="justify-text">Indeed, still imagining that \(f\) is differentiable, we can apply Eq. (1) to the function \(z \mapsto \frac{1}{\varepsilon} f(x+\varepsilon z)\), whose gradient is the function \(z \mapsto \nabla f(x+\varepsilon z)\), and get $$\nabla f_\varepsilon(x) = \ – \frac{1}{\varepsilon} \int_{\mathbb{R}^d} f(x+\varepsilon z) \nabla p(z) dz = \frac{1}{\varepsilon} \mathbb{E} \big[ – f(x+\varepsilon Z) \nabla \log p(Z)\big].$$ These computations can easily be made rigorous and we obtain an expression of the gradient of \(f_\varepsilon\) without invoking the gradient of \(f\) (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>, <a href="https://arxiv.org/pdf/2002.08676">14</a>] for details).</p>



<p class="justify-text">Moreover, if \(p\) is a differentiable function, we can expect the expectation in the right hand side of the equation above to be bounded, and therefore the function \(f_\varepsilon\) has gradients bounded by \(\frac{1}{\varepsilon}\).</p>



<p class="justify-text">This can be used within (typically convex) optimization in two ways:</p>



<ul class="justify-text"><li><strong>Zero-th order optimization</strong>: if our goal is to minimize the function \(f\), which is non-smooth, and for which we only have access to function values (so-called “zero-th order oracle), then we can obtain an unbiased stochastic gradient of the smoothed version \(f_\varepsilon\) as \(– f(x+\varepsilon z) \nabla \log p(z)\) where \(z\) is sampled from \(p\). The variance of the stochastic gradient grows with \(1/\varepsilon\) and the bias due to the use of \(f_\varepsilon\) instead of \(f\) is proportional to \(\varepsilon\). There is thus a sweet spot for the choice of \(\varepsilon\), with many variations; see, e.g., [<a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">5</a>, <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">6</a>, <a href="https://arxiv.org/pdf/cs/0408007">7</a>]. </li><li><strong>Randomized smoothing with acceleration</strong> [<a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">8</a>, <a href="https://arxiv.org/pdf/1204.0665">9</a>]: Here the goal is to follow the “Nesterov smoothing” idea [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">10</a>] and minimize a non-smooth function \(f\) using accelerated gradient descent on the smoothed version \(f_\varepsilon\), but this time with a stochastic gradient. Stochastic versions of Nesterov accelerations are then needed; this is useful when a full deterministic smoothing of \(f\) is too costly, see [<a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">11</a>, <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">12</a>] for details.</li></ul>



<p class="justify-text"><strong>Example.</strong> We consider minimizing a quadratic function in two dimensions, and we compare below plain gradient descent, stochastic gradient descent (left) and zero-th order optimization where we take a step towards the direction \(– f(x+\varepsilon Z) \nabla \log p(Z)\) for a standard normal \(Z\). We compare stochastic zero-th order optimization to plain stochastic gradient descent (SGD) below: SGD is a first-order method requiring access to stochastic gradients with a variance that is bounded, while zero-th order optimization only requires function values, but with significantly higher variance and thus requiring more iterations to converge.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4633" height="254" src="https://francisbach.com/wp-content/uploads/2020/09/paths_video_zeroth_order.gif" width="556"/>Left: gradient descent (GD) and stochastic gradient descent (SGD). Right: zero-th order optimization. All with constant step-sizes.</figure></div>



<h2>Differentiable perturbed optimizers</h2>



<p class="justify-text">The randomized smoothing technique can be used in a different context with applications to differentiable programming. We now assume that the function \(f\) can be written as the <a href="https://en.wikipedia.org/wiki/Support_function">support function</a> of a polytope \(\mathcal{C}\), that is, for all \(u \in \mathbb{R}^d\), $$f(u) = \max_{y \in \mathcal{C}} u^\top y,$$ where \(\mathcal{C}\) is the convex hull of a finite family \((y_i)_{i \in I}\). </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4598" height="292" src="https://francisbach.com/wp-content/uploads/2020/09/polytope_intro-1024x842.png" width="357"/>Polytope \(C\), convex hull of 8 vectors in \(\mathbb{R}^2\).</figure></div>



<p class="justify-text">Typically, the family is very large (e.g., \(|I|\) is exponential in \(d\)), but a polynomial-time algorithm exists for computing an arg-max \(y^\ast(u)\) above. Classical examples, from simpler to more interesting, are:</p>



<ul class="justify-text"><li><strong>Simplex</strong>: \(\mathcal{C}\) is the set of vectors with non-negative components that sum to one, and is the convex hull of canonical basis vectors. Then \(f\) is the maximum function, and there are many classical ways of smoothing it (see link with the <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> below).</li><li><strong>Hypercube</strong>: \(\mathcal{C} = [0,1]^n\), which is the convex hull of all vectors in \(\{0,1\}^n\). The maximization of linear functions can then be done independently for each bit.</li><li><strong>Permutation matrices</strong>: \(\mathcal{C}\) is then the <a href="https://en.wikipedia.org/wiki/Birkhoff_polytope">Birkhoff polytope</a>, the convex hull of all <a href="https://en.wikipedia.org/wiki/Permutation_matrix">permutation matrices</a> (square matrices with elements in \(\{0,1\}\), and with exactly a single \(1\) in each row and column). Maximizing linear functions is the classical <a href="https://en.wikipedia.org/wiki/Assignment_problem">linear assignment problem</a>.</li><li><strong>Shortest paths</strong>: given a graph, a path is a sequence of vertices which are connected to each other in the graph. They can classically be represented as a vector of of 0’s and 1’s indicating the edges which are followed by the paths. Minimizing linear functions is then equivalent to <a href="https://en.wikipedia.org/wiki/Shortest_path_problem">shortest path</a> problems.</li></ul>



<p class="justify-text">In many supervised applications, the vector \(u\) is as a function of some input \(x\) and some parameter vector \(\theta\). In order to learn the pararameter \(\theta\) from data, one needs to be able to differentiate with respect to \(\theta\), and this is typically done through the chain rule by differentiating \(y^\ast(u)\) with respect to \(u\). There come two immediate obstacles: (1) the element \(y^\ast(u)\) is not even well-defined when the arg-max is not unique, which is not a real problem because this can only be the case for a set of \(u\)’s with zero Lebesgue measure; and (2) the function \(y^\ast(u)\) is locally constant for most \(u\)’s, that is, the gradient is equal to zero almost everywhere. Thus, in the context of differentiable programming, this is non informative and essentially useless.</p>



<p class="justify-text">Randomized smoothing provides a simple and generic way to define an approximation which is differentiable and with informative gradient everywhere (there are others, such as adding a strongly convex regularizer \(\psi(y)\), and maximizing \(u^\top y\  – \psi(y)\) instead, see [<a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">20</a>] for details. See also [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>]).</p>



<p class="justify-text">In order to obtain a differentiable function through randomized smoothing, we can consider \(y^\ast(u + \varepsilon z)\), for a random \(z\), which is an instance of the more general “perturb-and-MAP” paradigm [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242">21</a>, <a href="https://icml.cc/Conferences/2012/papers/528.pdf">22</a>].</p>



<p class="justify-text">Since \(y^\ast(u)\) is a subgradient of \(f\) at \(u\) and \(f_\varepsilon(u) = \int_{\mathbb{R}^d} f(u+\varepsilon z) p(z) dz\), by swapping integration (with respect to \(z\)) and differentiation (with respect to \(u\)), we have the following identities: $$ \mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big] = \nabla f_\varepsilon(u),$$ that is, the expectation of the perturbed arg-max is the gradient of the smoothed function \(f_\varepsilon\). I will use the notation \(y^\ast_\varepsilon(u) =\mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big]\) to denote this gradient; see an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4545" height="322" src="https://francisbach.com/wp-content/uploads/2020/08/polytope-1024x671.png" width="491"/>Polytope \(\mathcal{C}\), with a direction \(u\), the non-perturbed maximizer \(y^\ast(u)\), a perturbed direction \(u+\varepsilon Z\) and the perturbed maximizer \(y^\ast(u+\varepsilon Z)\). The areas of the red circles are proportional to the probability of selecting the corresponding extreme point after the perturbation. The expected perturbed maximizer \(y^\ast_\varepsilon(u)\) is in the interior of \(\mathcal{C}\).</figure></div>



<p class="justify-text">In a joint work with Quentin Berthet, Mathieu Blondel, Oliver Teboul, Marco Cuturi, and Jean-Philippe Vert [<a href="http://arxiv.org/pdf/2002.08676(opens in a new tab)">14</a>], we detail theoretical and practical properties of \(y^\ast_\varepsilon(u)\), in particular:</p>



<ul class="justify-text"><li>Estimation: \(y^\ast_\varepsilon(u)\) can be estimated by replacing the expectation by empirical averages.</li><li>Differentiability: if \(Z\) has a strictly positive density over \(\mathbb{R}^d\), then the function \(y^\ast_\varepsilon\) is infinitely differentiable, with simple expression of  the Jacobian, obtained by integration by parts (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>] for details).</li><li>The <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> is the simplest instance of such a smoothing technique, with \(\mathcal{C}\) being the simplex, and \(Z\) having independent Gumbel distributions. The function \(f_\varepsilon\) is then a “<a href="https://en.wikipedia.org/wiki/LogSumExp">log-sum-exp</a>” function.</li></ul>



<p class="justify-text"><strong>Illustration</strong>. Following [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>], this can be applied to learn the travel costs in graphs based on features. The vectors \(y_i\) represent shortest path between the top-left and bottom-right corners, with costs corresponding to the terrain type. See [<a href="https://arxiv.org/pdf/2002.08676">14</a>] for details on the learning procedure. Here I just want to highlight the effect of varying the amount of smoothing characterized by \(\varepsilon\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4605" height="225" src="https://francisbach.com/wp-content/uploads/2020/09/paths-1024x518.png" width="446"/>Left: Warcraft terrain. Right: Cost associated to each terrain type.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full"><img alt="" class="wp-image-4624" height="288" src="https://francisbach.com/wp-content/uploads/2020/09/anim_smoothed.gif" width="432"/>Shortest paths \(y^\ast_\varepsilon(u)\), from \(\varepsilon=0\) (no smoothing) to \(\varepsilon=2\). From an essentially single shortest path, as smoothing increases, we obtain a mixture of two potential paths, before having many extreme points.</figure></div>



<h2>Learning single-index models</h2>



<p class="justify-text">Given a random vector \((X,Y) \in \mathbb{R}^d \times \mathbb{R}\), we assume that \(Y = f(X) + \varepsilon\), where \(f(x) = \sigma(w^\top x)\) for some unknown function \(\sigma: \mathbb{R} \to \mathbb{R}\) and \(w \in \mathbb{R}^d\), with \(\varepsilon\) a zero-mean noise independent from \(X\).  Given some observations \((x_1,y_1), \dots, (x_n,y_n)\) in \(\mathbb{R}^d \times \mathbb{R}\), the goal is to estimate the direction \(w \in \mathbb{R}^d\). This model is referred to as single-index regression models in the statistics literature [<a href="https://www.jstor.org/stable/pdf/1913713.pdf">1</a>, <a href="https://www.jstor.org/stable/pdf/3035585.pdf">2</a>]</p>



<p class="justify-text">One possibility if \(\sigma\) was known would be to perform least-squares estimation and minimize with respect to \(w\) $$ \frac{1}{2n} \sum_{i=1}^n \big( y_i\  – \sigma(w^\top x_i) \big)^2, $$ which is a non-convex optimization problem in general. When \(\sigma\) is unknown, one could imagine adding the estimation of \(\sigma\) into the optimization, making it even more complicated.</p>



<p class="justify-text">Score functions provide an elegant solution that leads to the “average derivative method” (ADE) [<a href="https://www.jstor.org/stable/pdf/1914309.pdf">3</a>], which I will now describe. We consider \(p\) the density of \(X\). We then have, using Eq. (1): $$ \mathbb{E} \big[ Y \nabla \log p(X) \big] =\mathbb{E} \big[ f(X) \nabla \log p(X) \big] = \ – \mathbb{E} \big[ \nabla f(X)  \big] =\ –  \Big( \mathbb{E} \big[ \sigma'(w^\top X) \big] \Big) w, $$ which is proportional to \(w\). When replacing the expectation by an empirical mean, this provides a way to estimate \(w\) (up to a constant factor) without even knowing the function \(\sigma\), but assuming the density of \(X\) is known so that the score function is available.</p>



<p class="justify-text"><strong>Extensions.</strong> The ADE method can be extended in a number of ways to deal with more complex situations. Here are some examples below:</p>



<ul class="justify-text"><li><em>Multiple index models</em>: if the response/output \(Y\) is instead assumed of the form $$ Y = f(X) + \varepsilon =  \sigma(W^\top x) + \varepsilon, $$ where \(W \in \mathbb{R}^{d \times k}\) is a matrix with \(k\) columns, we obtained a “multiple index model”, for which a similar technique seems to apply since now \(\nabla f(x) = W \nabla  \sigma(W^\top x) \in \mathbb{R}^d\), and thus, for the assumed model \(\mathbb{E} \big[ Y \nabla \log p(X) \big]\) is in the linear span of the columns of \(W\); this is not enough for recovering the entire subspace if \(k&gt;1\) because we have only a single element of the span. There are two solutions for this. The first one is is to condition on some values of \(Y\) being in some set \(\mathcal{Y}\), where one can show that \(\mathbb{E} \big[ Y \nabla \log p(X) | Y \in \mathcal{Y} \big]\) is also in the desired subspace; thus, with several sets \(\mathcal{Y}\), one can generate several elements, and after \(k\) of these, one can expect to estimate the full \(k\)-dimensional subspace. The idea of conditioning on \(Y\) is called <a href="https://en.wikipedia.org/wiki/Sliced_inverse_regression">sliced inverse regression</a> [<a href="https://www.jstor.org/stable/pdf/2290563.pdf">15</a>], and the application to score function can be found in [<a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>]. The second one is to consider higher-order moments and derivatives of the score functions, that is, using integration by parts twice! (see [<a href="https://arxiv.org/pdf/1412.2863">17</a>, <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">18</a>, <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>] for details).</li><li><em>Neural networks</em>: when the function \(\sigma\) is the sum of functions that depends on single variables, multiple-index models are exactly one-hidden-layer neural networks. Similar techniques can be used for deep networks with more than a single hidden layer (see [<a href="https://arxiv.org/pdf/1506.08473">19</a>]).</li></ul>



<p class="justify-text"><strong>Moment matching vs. empirical risk minimization. </strong>In all cases mentioned above, the use of score functions can be seen as an instance of the <a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">method of moments</a>: we assume a specific model for the data, derive identities satisfied by expectations of some functions under the model, and use these identities to identify a parameter vector. In the situations above, direct empirical risk minimization would lead to a potentially hard optimization problem. However, moment matching techniques rely heavily on the model being well-specified, which is often not the case in practice, while empirical risk minimization techniques try to fit the data as much as the model allows, and is thus typically more robust to model misspecification.</p>



<h2>Score matching for density estimation</h2>



<p class="justify-text">We consider the problem of density estimation. That is, given some observations \(x_1,\dots,x_n \in \mathbb{R}^d\) sampled independently and identically distributed from some distribution with density \(p\), we want to estimate \(p\) from the data. Given a model \(q_\theta \) with some parameters \(\theta\), the most standard method is maximum likelihood estimation, which corresponds to the following optimization problem: $$\max_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n \log q_\theta(x_i).$$ It requires <em>normalized</em> densities that is, \(\int_{\mathbb{R}^d} q_\theta(x) dx = 1\), and dealing with normalized densities often requires to explicitly normalize them and thus to compute integrals, which is difficult when the underlying dimension \(d\) gets large.</p>



<p class="justify-text">Score matching is a recent method proposed by Aapo Hyvärinen [4] based on score functions. The simple (yet powerful) idea is to perform least-squares estimation on the score functions. That is, in the population case, the goal is to minimize $$\mathbb{E} \big\| \nabla \log p(X) \ – \nabla  \log q_\theta(X) \big\|_2^2 = \int_{\mathbb{R}^d} \big\| \nabla \log p(x)\  – \nabla  \log q_\theta(x) \big\|_2^2 p(x) dx.$$ Apparently, this expectation does not lead to an estimation procedure where \(p(x)\) is replaced by the empirical distribution of the data because of the presence of \(\nabla \log p(x)\). Integration by parts will solve this.</p>



<p class="justify-text">We can expand \(\mathbb{E} \big\| \nabla \log p(X) \ – \nabla \log q_\theta(X) \big\|_2^2\) as  $$ \mathbb{E} \big\| \nabla \log p(X) \|_2^2 + \mathbb{E} \big\|\nabla \log q_\theta(X) \big\|_2^2 – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big]. $$ The first term is independent of \(q_\theta\) so it does not count when minimizing. The second term is an expectation with respect to \(p(\cdot)\) so it can be replaced by the empirical mean. The third term can be dealt with with integration by parts, that is Eq. (2), leading to: $$ – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \nabla \cdot \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \Delta \log q_\theta(X) \big],$$ where \(\Delta\) is the <a href="https://en.wikipedia.org/wiki/Laplace_operator">Laplacian</a>.</p>



<p class="justify-text">We now have an expectation with respect to the data distribution \(p\), and we can replace the expectation with an empirical average to estimate the parameter \(\theta\) from data \(x_1,\dots,x_n\). We then use the cost function $$\frac{1}{n} \sum_{i=1}^n \big\|\nabla \log q_\theta(x_i) \big\|_2^2 + \frac{2}{n} \sum_{i=1}^n \Delta \log q_\theta(x_i), $$ which is linear in \(\log q_\theta\). Hence, when the unnormalized log-density is linearly parameterized, which is common, we obtain a quadratic problem. This procedure has a number of attractive properties, in particular consistency [<a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">4</a>], but the key benefit is to allow estimation without requiring normalizing constants.</p>



<h2>Conclusion</h2>



<p class="justify-text">Overall, the simple identity from Eq. (1), that is, \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla f(Z) \big]\), has many applications in diverse somewhat unrelated areas of machine learning, optimization and statistics. There are of course many other uses of integration by parts within this field. Feel free to add your preferred one as comment.</p>



<p class="justify-text">It has been a while since the last post on polynomial magic. I will revive the thread next month. I let you guess which polynomials will be the stars of my next blog post.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Quentin Berthet for producing the video of shortest paths, proofreading this blog post, and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] James L. Powell, James H. Stock, Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1913713.pdf">Semiparametric estimation of index coefficients</a>. <em>Econometrica: Journal of the Econometric Society</em>. 57(6):1403-1430, 1989.<br/>[2] Wolfgang Hardle, Peter Hall, Hidehiko Ichimura. <a href="https://www.jstor.org/stable/pdf/3035585.pdf">Optimal smoothing in single-index models</a>. <em>Annals of Statistics</em>. 21(1): 157-178(1993): 157-178.<br/>[3] Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1914309.pdf">Consistent Estimation of Scaled Coefficients</a>. <em>Econometrica</em>, 54(6):1461-1481, 1986.<br/>[4] Aapo Hyvärinen. <a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Estimation of non-normalized statistical models by score matching</a>. <em>Journal of Machine Learning Research</em>, <em>6</em>(Apr), 695-709, 2005.<br/>[5] Yurii Nesterov. <a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">Random gradient-free minimization of convex functions</a>. Technical report, Université Catholique de Louvain (CORE), 2011.<br/>[6] Boris T. Polyak and Alexander B. Tsybakov. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">Optimal order of accuracy of search algorithms in stochastic optimization</a>. <em>Problemy Peredachi Informatsii</em>, 26(2):45–53, 1990.<br/>[7]  Abraham D. Flaxman, Adam Tauman Kalai, H. Brendan McMahan. <a href="https://arxiv.org/pdf/cs/0408007">Online convex optimization in the bandit setting: gradient descent without a gradient</a>. In Proc. Symposium on Discrete algorithms (SODA), 2005.<br/>[8] John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. <a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">Randomized Smoothing for Stochastic Optimization</a>. SIAM Journal on Optimization, 22(2), 674–701, 2012.<br/>[9] Alexandre d’Aspremont, Nourredine El Karoui, <a href="https://www.di.ens.fr/~aspremon/stochsmooth.html">A Stochastic Smoothing Algorithm for Semidefinite Programming.</a> <em>SIAM Journal on Optimization</em>, 24(3): 1138-1177, 2014.<br/>[10] Yurii Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions</a>. Mathematical Programming, 103(1):127–152, 2005.<br/>[11] Lin Xiao. <a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a>. <em>Journal of Machine Learning Research</em>, 11(88): 2543−2596, 2010.<br/>[12] Guanghui Lan. <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">An optimal method for stochastic composite optimization</a>. <em>Mathematical Programming</em>, 133(1):365–397, 2012.<br/>[13] Tamir Hazan, George Papandreou, and Daniel Tarlow. <a href="https://mitpress.mit.edu/books/perturbations-optimization-and-statistics">Perturbation, Optimization, and Statistics</a>. MIT Press, 2016.<br/>[14] Quentin Berthet, Matthieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach, <a href="https://arxiv.org/pdf/2002.08676">Learning with differentiable perturbed optimizers</a>. Technical report arXiv 2002.08676, 2020.<br/>[15] Ker-Chau Li. <a href="https://www.jstor.org/stable/pdf/2290563.pdf">Sliced inverse regression for dimension reduction</a>. <em>Journal of the American Statistical Association</em>, <em>86</em>(414), 316-327, 1991.<br/>[16] Dmitry Babichev and Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">Slice inverse regression with score functions</a>. <em>Electronic Journal of Statistics</em>, 12(1):1507-1543, 2018.<br/>[17] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1412.2863">Score function features for discriminative learning: Matrix and tensor framework</a>. Technical report arXiv:1412.2863, 2014.<br/>[18] David R. Brillinger. <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">A generalized linear model with “Gaussian” regressor variables</a>.  <em>Selected Works of David Brillinger</em>, 589-606, 2012.<br/>[19] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1506.08473">Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods</a>.  Technical report arXiv:1506.08473, 2015.<br/>[20] Vlad Niculae, André F. T. Martins, Mathieu Blondel, and Claire Cardie. <a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">SparseMAP: Differentiable sparse structured inference</a>. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 2017.<br/>[21] George Papandreou and Alan L. Yuille.<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242"> Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models</a>. <em>International Conference on Computer Vision</em>, 2011.<br/>[22] Tamir Hazan and Tommi Jaakkola. <a href="https://icml.cc/Conferences/2012/papers/528.pdf">On the partition function and random maximum a-posteriori perturbations</a>. <em>Proceedings of the International Conference on International Conference on Machine Learning (ICML),</em> 2012.<br/>[23] Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. <a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">Perturbation techniques in online learning and optimization</a>. <em>Perturbations, Optimization, and Statistics</em>, 233-264, 2016.<br/>[24] Marin Vlastelica, Anselm Paulus, Vít Musil, Georg Martius, Michal Rolínek. <a href="https://openreview.net/pdf?id=BkevoJSYPB">Differentiation of Blackbox Combinatorial Solvers</a>. <em>International Conference on Learning Representations</em>. 2019.</p></div>
    </content>
    <updated>2020-09-07T19:06:36Z</updated>
    <published>2020-09-07T19:06:36Z</published>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-09-11T00:21:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/132</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/132" rel="alternate" type="text/html"/>
    <title>TR20-132 |  Towards Stronger Counterexamples to the Log-Approximate-Rank Conjecture | 

	Arkadev Chattopadhyay, 

	Ankit Garg, 

	Suhail Sherif</title>
    <summary>We give improved separations for the query complexity analogue of the log-approximate-rank conjecture i.e. we show that there are a plethora of total Boolean functions on $n$ input bits, each of which has approximate Fourier sparsity at most $O(n^3)$ and randomized parity decision tree complexity $\Theta(n)$. This improves upon the recent work of Chattopadhyay, Mande and Sherif (JACM '20) both qualitatively (in terms of designing a large number of examples) and quantitatively (improving the gap from quartic to cubic). We leave open the problem of proving a randomized communication complexity lower bound for XOR compositions of our examples. A linear lower bound would lead to new and improved refutations of the log-approximate-rank conjecture. Moreover, if any of these compositions had even a sub-linear cost randomized communication protocol, it would demonstrate that randomized parity decision tree complexity does not lift to randomized communication complexity in general (with the XOR gadget).</summary>
    <updated>2020-09-07T15:45:54Z</updated>
    <published>2020-09-07T15:45:54Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-11T00:20:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-83517349672236531</id>
    <link href="https://blog.computationalcomplexity.org/feeds/83517349672236531/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/83517349672236531" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/83517349672236531" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html" rel="alternate" type="text/html"/>
    <title>Two Math Problems of interest (at least to me)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I will give two math problems that are of interest to me.</p><p>These are not new problems, however you will have more fun if you work on them yourself and leave comments on what you find. So if you want to work on it without hints, don't read the comments.</p><p><br/></p><p>I will post about the answers (not sure I will post THE answers) on Thursday.</p><p><br/></p><p>1) Let x(1)&gt;0. Let x(n+1) = (  1 + (1/x(n))  )^n. </p><p><br/></p><p>For how many values of x(1) does this sequence go to infinity?</p><p><br/></p><p>2) Find all (x,y) \in N \times N such that x^2+3y and y^2+3x are both squares. </p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2020-09-06T21:05:00Z</updated>
    <published>2020-09-06T21:05:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-11T00:02:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/131</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/131" rel="alternate" type="text/html"/>
    <title>TR20-131 |  A Direct Product Theorem for One-Way Quantum Communication | 

	Srijita Kundu, 

	Rahul  Jain</title>
    <summary>We prove a direct product theorem for the one-way entanglement-assisted quantum communication complexity of a general relation $f\subseteq\mathcal{X}\times\mathcal{Y}\times\mathcal{Z}$. For any $\varepsilon, \zeta &gt; 0$ and any $k\geq1$, we show that
\[ \mathrm{Q}^1_{1-(1-\varepsilon)^{\Omega(\zeta^6k/\log|\mathcal{Z}|)}}(f^k) = \Omega\left(k\left(\zeta^5\cdot\mathrm{Q}^1_{\varepsilon + 12\zeta}(f) - \log\log(1/\zeta)\right)\right),\]
where $\mathrm{Q}^1_{\varepsilon}(f)$ represents the one-way entanglement-assisted quantum communication complexity of $f$ with worst-case error $\varepsilon$ and $f^k$ denotes $k$ parallel instances of $f$.

As far as we are aware, this is the first direct product theorem for quantum communication -- direct sum theorems were previously known for one-way quantum protocols. Our techniques are inspired by the parallel repetition theorems for the entangled value of two-player non-local games, under product distributions due to Jain, Pereszl\'{e}nyi and Yao, and under anchored distributions due to Bavarian, Vidick and Yuen, as well as message-compression for quantum protocols due to Jain, Radhakrishnan and Sen. In particular, we show that a direct product theorem holds for the distributional one-way quantum communication complexity of $f$ under any distribution $q$ on $\mathcal{X}\times\mathcal{Y}$ that is anchored on one side, i.e., there exists a $y^*$ such that $q(y^*)$ is constant and $q(x|y^*) = q(x)$ for all $x$. This allows us to show a direct product theorem for general distributions, since for any relation $f$ and any distribution $p$ on its inputs, we can define a modified relation $\tilde{f}$ which has an anchored distribution $q$ close to $p$, such that a protocol that fails with probability at most $\varepsilon$ for $\tilde{f}$ under $q$ can be used to give  a protocol that fails with probability at most $\varepsilon + \zeta$ for $f$ under $p$.

Our techniques also work for entangled non-local games which have input distributions anchored on any one side, i.e., either there exists a $y^*$ as previously specified, or there exists an $x^*$ such that $q(x^*)$ is constant and $q(y|x^*) = q(y)$ for all $y$. In particular, we show that for any game $G = (q, \mathcal{X}\times\mathcal{Y}, \mathcal{A}\times\mathcal{B}, V)$ where $q$ is a distribution on $\mathcal{X}\times\mathcal{Y}$ anchored on any one side with anchoring probability $\zeta$, then
\[ \omega^*(G^k) = \left(1 - (1-\omega^*(G))^5\right)^{\Omega\left(\frac{\zeta^2 k}{\log(|\mathcal{A}|\cdot|\mathcal{B}|)}\right)}\]
where $\omega^*(G)$ represents the entangled value of the game $G$. This is a generalization of the result of Bavarian, Vidick and Yuen, who proved a parallel repetition theorem for games anchored on both sides, i.e., where both a special $x^*$ and a special $y^*$ exist, and potentially a simplification of their proof.</summary>
    <updated>2020-09-06T05:33:09Z</updated>
    <published>2020-09-06T05:33:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-11T00:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/130</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/130" rel="alternate" type="text/html"/>
    <title>TR20-130 |  Optimal Inapproximability of Satisfiable k-LIN over Non-Abelian Groups | 

	Amey Bhangale, 

	Subhash Khot</title>
    <summary>A seminal result of H\r{a}stad [J. ACM, 48(4):798–859, 2001]  shows that it is NP-hard to find an assignment that satisfies $\frac{1}{|G|}+\varepsilon$ fraction of the constraints of a given $k$-LIN instance over an abelian group, even if there is an assignment that satisfies $(1-\varepsilon)$ fraction of the constraints, for any constant $\varepsilon&gt;0$.  Engebretsen et al. [Theoretical Computer Science, 312(1):17–45, 2004] later showed that the same hardness result holds for $k$-LIN instances over any finite non-abelian group.

Unlike the abelian case, where we can efficiently find a solution if the instance is satisfiable, in the non-abelian case, it is NP-complete to decide if a given system of linear equations is satisfiable or not, as shown by Russell and Goldmann [Information and Computation, 178(1):253–262, 2002].  

Surprisingly, for certain non-abelian groups $G$, given a satisfiable $k$-LIN instance over $G$, one can in fact do better than just outputting a random assignment using a simple but clever algorithm. The approximation factor achieved by this algorithm varies with the underlying group. In this paper, we show that this algorithm is {\em optimal} by proving a  tight hardness of approximation of satisfiable $k$-LIN instance over {\em any} non-abelian $G$, assuming $P \neq NP$.

As a corollary, we also get $3$-query probabilistically checkable proofs with perfect completeness over large alphabets with improved soundness.</summary>
    <updated>2020-09-06T04:42:45Z</updated>
    <published>2020-09-06T04:42:45Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-11T00:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17507</id>
    <link href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/" rel="alternate" type="text/html"/>
    <title>Closing An Open Problem</title>
    <summary>Crawl, then walk, then run. Bogdan Grechuk is a lecturer in the math department at the University of Leicester. His office is in the Michael Atiyah Building. Pretty cool. He works in risk analysis, but is more broadly interested in math of all kinds. See his wonderful book Theorems of the 21st Century. Or go […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Crawl, then walk, then run.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/bg/" rel="attachment wp-att-17512"><img alt="" class="alignright  wp-image-17512" src="https://rjlipton.files.wordpress.com/2020/09/bg.png?w=150" width="150"/></a></p>
<p>Bogdan Grechuk is a lecturer in the math department at the University of Leicester. His office is in the Michael Atiyah Building. Pretty cool. He works in risk analysis, but is more broadly interested in math of all kinds. See his wonderful book <a href="https://link.springer.com/book/10.1007%2F978-3-030-19096-5">Theorems of the 21st Century</a>. Or go to his web <a href="https://theorems.home.blog/theorems-list/">site</a>.</p>
<p>
Today Ken and I want to talk about solving open problems.<br/>
<span id="more-17507"/></p>
<p>
Grechuk’s site got us thinking about results that solve open problems. Most of us like to think our research solves an open problem. Personally I can say that I have tried to solve problems for the first time, but did not always succeed. </p>
<p>
Open problems usually mean something stronger. To be an open problem, a problem must be known to some community for some time. Advances in math and in complexity theory roughly fall into two categories: </p>
<ol>
<li>
Results that prove or disprove something that was explicitly stated before. The more who knew the problem the better. The longer the problem was known the better, too. <p/>
</li><li>
Results that prove something that is new. Something that no one had explicitly asked before.
</li></ol>
<p>Both type of results are important. The latter kind may ultimately be more important. They raise new questions, often contain new methods, and move the field ahead in a new direction. See our<br/>
<a href="https://rjlipton.wordpress.com/2011/02/01/godel&#x2019;s-lost-letter-is-two-years-old/">discussion</a> of Freeman Dyson and frogs and birds.</p>
<p>
Think of how Kurt Gödel’s incompleteness theorem was unsuspected, or how Alan Turing’s proof of undecidability of the halting problem came in tandem with settling the criterion for computability, or years latter the definition of public-key crypto-systems. But we will focus on open problems in the sense (1). </p>
<p/><h2> Claiming A Solution </h2><p/>
<p/><p>
Ken and I are amazed that when an open problem is claimed, especially for P versus NP, the claim swallows it whole. That is the claim is that the full problem is solved. We do not recall once when the claim was: </p>
<ul>
<li>
We are able to prove that SAT requires quadratic time, or <p/>
</li><li>
We can show that SAT is in co-NP.
</li></ul>
<p>Either of these would be a “stop the press” result. </p>
<p>
For a more concrete example, suppose you claim to have a polynomial-time algorithm for finding a maximum clique in an undirected graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. Of course this implies P<img alt="{=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{=}"/>NP. Your algorithm may require a chain of difficult lemmas that obscure its workings. Can you perhaps analyze its effectiveness more easily on <em>random</em> graphs? Here are two relevant facts:</p>
<ul>
<li>
In 1976, David Matula <a href="https://s2.smu.edu/~matula/Tech-Report76.pdf">proved</a> that with high probability for a random <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-vertex graph of edge probability <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/>, the size of the maximum clique is one of the two integers flanking <img alt="{2\log_{1/p}(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\log_{1/p}(n)}"/>. <p/>
</li><li>
As observed in 1976 by Dick Karp in his <a href="https://www.semanticscholar.org/paper/The-probabilistic-analysis-of-some-combinatorial-Karp/9a9558d79b93fd884354f1ae27463be2836d2ec0">paper</a>, “The Probabilistic Analysis of Some Combinatorial Search Algorithms,” no polynomial time algorithm is known to achieve size <img alt="{(1+\epsilon)\log_{1/p}(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2B%5Cepsilon%29%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1+\epsilon)\log_{1/p}(n)}"/>, for any <img alt="{\epsilon &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt; 0}"/> and sufficiently large <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>.
</li></ul>
<p>
You only need to close a gap of a factor of <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>, not to hit the maximum value exactly, and you do not need to succeed for all graphs. The behavior of random graphs should help your analysis. A more-recent mention of Karp’s open problem is in these 2005 <a href="https://www.math.cmu.edu/~af1p/MAA2005/L7.pdf">slides</a>.</p>
<p>
</p><p/><h2> Some Examples </h2><p/>
<p/><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b> Collatz Conjecture </b>: Terence Tao made important <a href="https://terrytao.wordpress.com/2019/09/10/almost-all-collatz-orbits-attain-almost-bounded-values/">progress</a> on this notorious <a href="https://terrytao.files.wordpress.com/2020/02/collatz.pdf">problem</a>. He said: </p>
<blockquote><p><b> </b> <em> In mathematics, when we cannot solve a problem completely, we look for partial results. Even if they do not lead to a complete solution, they often reveal insights about the problem. </em>
</p></blockquote>
<p>Recall this is also called the <img alt="{3n+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3n%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3n+1}"/> problem. It asks for the long-term behavior of the function: <img alt="f(n) " class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="f(n) "/> which is equal to <img alt="n/2 " class="latex" src="https://s0.wp.com/latex.php?latex=n%2F2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n/2 "/> for <img alt="n " class="latex" src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n "/> even and <img alt="3n+1 " class="latex" src="https://s0.wp.com/latex.php?latex=3n%2B1+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="3n+1 "/> for <img alt="n " class="latex" src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n "/> odd. </p>
<p>The conjecture is that for every <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, iterating the function eventually hits <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, i.e., <img alt="{f^i(n) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f^i(n) = 1}"/> for some <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>.</p>
<p>
There are two ways the conjecture can fail:</p>
<ul>
<li>
There is a finite cycle besides the trivial cycle <img alt="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Crightarrow+4+%5Crightarrow+2+%5Crightarrow+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}"/>. <p/>
</li><li>
For some <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, the sequence <img alt="{f^i(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f^i(n)}"/> goes off to infinity.
</li></ul>
<p>
What Tao proved is that “many” values <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> achieve: <img alt="f^i(n) &lt; \log^* n " class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ei%28n%29+%3C+%5Clog%5E%2A+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="f^i(n) &lt; \log^* n "/> for some <img alt="i " class="latex" src="https://s0.wp.com/latex.php?latex=i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i "/>.</p>
<p>Grechuk's <a href="https://theorems.home.blog/2020/04/29/almost-all-orbits-of-the-collatz-map-attain-almost-bounded-value/">page</a> includes the definition of “many,” which turns out to be <em>weaker</em> than saying a <img alt="{(1-\epsilon)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281-%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1-\epsilon)}"/> proportion of values <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> “swing low” in this sense. Moreover, Tao proved this for any unbounded function <img alt="{g(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(n)}"/> in place of the iterated logarithm, such as the inverse Ackermann function. Note this is a case where randomized analysis worked—in the hands of a master.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b> Twin Prime Conjecture </b>: Yitang Zhang made tremendous progress on this long standing <a href="https://en.wikipedia.org/wiki/Twin_prime">conjecture</a>. He proved that infinitely often is a prime <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> so there is another prime <img alt="{q&gt;p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%3Ep%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q&gt;p}"/> bounded above by <img alt="{p +C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%2BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p +C}"/>. His <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> was <img alt="{70}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{70}"/> million, but this was still a breakthrough. Previously no bounded <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> was known. We discussed this before <a href="https://rjlipton.wordpress.com/2013/05/21/twin-primes-are-useful/">here</a>. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b> Sensitivity Conjecture </b>: Hao Huang is given as a <a href="http://mentalfloss.com/article/52698/how-does-exception-prove-rule">counterexample</a> to partial progress—it is the exception that proves the rule. He solved the full <a href="https://arxiv.org/pdf/1907.00847.pdf">conjecture</a>. He proved that every <img alt="{2^{n}-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%7D-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{n}-1}"/> vertex induced subgraph of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-dimensional cube graph has maximum degree at least <img alt="{\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{n}}"/>. The previous best was only order logarithm in <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. But there does remain some slack: He proved a fourth-power bound, but can it be closed to cubic or even quadratic? We <a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/">discussed</a> this last year.</p>
<p>
By the <a href="https://www.mentalfloss.com/article/52698/how-does-exception-prove-rule">way</a>: </p>
<blockquote><p><b> </b> <em> The expression comes from the Latin legal principle exceptio probat regulam (the exception proves the rule), also rendered as exceptio firmat regulam (the exception establishes the rule) and exceptio confirmat regulam (the exception confirms the rule). The principle provides legal cover for inferences such as the following: if I see a sign reading “no swimming allowed after 10 pm,” I can assume swimming is allowed before that time. </em>
</p></blockquote>
<p>
</p><p/><h2> Advice To Claimers </h2><p/>
<p/><p>
Our general advice to claimers: </p>
<blockquote><p><b> </b> <em> <i>Okay you are sure you have solved the big problem. Write up the weakest new result that you can.</i> </em>
</p></blockquote>
<p/><p>
Use your methods, your insights, to minimize the work needed for someone to be 99.99% convinced that you have proved something <em>new</em>, rather than a lower confidence of your having proved something <em>huge</em>. For P<img alt="{=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{=}"/>NP show that you have a exponential algorithm that is better than known. Or for P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP give a non-linear lower bound. </p>
<p>
The rationale is: You are more likely to get someone to read your paper if you make a weaker claim. A paper titled: <i>A New SAT Algorithm that Runs in Sub-exponential Time</i> is more likely to get readers than a paper tiled <i>P=NP</i>. This shows that readership is non-monotone. </p>
<p>
This is consequence of two phenomena: One is believability. The weaker paper is more likely to be correct. One is human. The stronger paper, if correct, may not be easy to improve. A weaker paper could have results that the reader could improve and write a follow-on paper: </p>
<blockquote><p><b> </b> <em> In Carol Fletcher’s recent paper an <img alt="{2^{n^{1/3}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%5E%7B1%2F3%7D%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{2^{n^{1/3}}}"/> algorithm is found for SAT. She required the full Riemann Hypothesis. We remove that requirement and <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\dots}"/> </em>
</p></blockquote>
<p/><h2> Open Problems </h2><p/>
<p/><p>
What about our advice: what would you do if you solved a major open problem? Note that the examples we highlighted all have slack for improvement short of the optimum statements.</p></font></font></div>
    </content>
    <updated>2020-09-05T17:25:13Z</updated>
    <published>2020-09-05T17:25:13Z</published>
    <category term="News"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Advice"/>
    <category term="open problems"/>
    <category term="proof checking"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-11T00:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/129</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/129" rel="alternate" type="text/html"/>
    <title>TR20-129 |  A Lower Bound on Determinantal Complexity | 

	Mrinal Kumar, 

	Ben Lee Volk</title>
    <summary>The determinantal complexity of a polynomial $P \in \mathbb{F}[x_1,  \ldots, x_n]$ over a field $\mathbb{F}$ is the dimension of the smallest matrix $M$ whose entries are affine functions in $\mathbb{F}[x_1,  \ldots, x_n]$ such that $P = Det(M)$. We prove that the determinantal complexity of the polynomial $\sum_{i = 1}^n x_i^n$ is at least $1.5n - 3$. 

For every $n$-variate polynomial of degree $d$, the determinantal complexity is trivially at least $d$, and it is a long standing open problem to prove a lower bound which is super linear in $\max\{n,d\}$. Our result is the first lower bound for any explicit polynomial which is bigger by a constant factor than $\max\{n,d\}$, and improves upon the prior best bound of $n + 1$, proved by Alper, Bogart and Velasco [ABV17] for the same polynomial.</summary>
    <updated>2020-09-05T03:25:44Z</updated>
    <published>2020-09-05T03:25:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-11T00:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5519</id>
    <link href="https://adamsheffer.wordpress.com/2020/09/04/mathematics-for-human-flourishing/" rel="alternate" type="text/html"/>
    <title>Mathematics for Human Flourishing</title>
    <summary>I read a lot of “popular math” books. I also wrote about some in past posts. But I’ve never read a book similar to Mathematics for Human Flourishing by Francis Su. I already knew the math presented in this book and almost all other topics covered. I even knew some of Su’s personal stories from […]</summary>
    <updated>2020-09-04T21:21:41Z</updated>
    <published>2020-09-04T21:21:41Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2020-09-11T00:21:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20205</id>
    <link href="https://gilkalai.wordpress.com/2020/09/04/alef-corner-math-collaboration/" rel="alternate" type="text/html"/>
    <title>Alef Corner: Math Collaboration</title>
    <summary>Another artistic view by Alef on mathematical collaboration.   Other Alef’s corner posts</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Another artistic view by Alef on mathematical collaboration.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/09/mathcoll.jpg"><img alt="" class="alignnone size-full wp-image-20207" height="640" src="https://gilkalai.files.wordpress.com/2020/09/mathcoll.jpg?w=640&amp;h=640" width="640"/></a></p>
<p> </p>
<p>Other <a href="https://gilkalai.wordpress.com/tag/alefs-corner/">Alef’s corner</a> posts</p></div>
    </content>
    <updated>2020-09-04T07:17:13Z</updated>
    <published>2020-09-04T07:17:13Z</published>
    <category term="Art"/>
    <category term="Combinatorics"/>
    <category term="What is Mathematics"/>
    <category term="Alef's corner"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-09-11T00:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=52</id>
    <link href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/" rel="alternate" type="text/html"/>
    <title>Friday, Sept 11 — Bin Yu from UC Berkeley</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Friday, Sept 11th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  Bin Yu from UC Berkeley will speak about “Veridical Data Science”. Abstract: Building and expanding on principles of statistics, machine learning, and the sciences, we propose<a class="more-link" href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/">Continue reading <span class="screen-reader-text">"Friday, Sept 11 — Bin Yu from UC Berkeley"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next Foundations of Data Science virtual talk will take place on Friday, Sept 11th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Bin Yu </strong>from UC Berkeley will speak about “<em>Veridical Data Science</em>”.</p>



<p><strong>Abstract</strong>: Building and expanding on principles of statistics, machine learning, and the sciences, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework is comprised of both a workflow and documentation and aims to provide responsible, reliable, reproducible, and transparent results across the entire data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle for the data science life cycle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. We develop inference procedures that build on PCS, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations.</p>



<p>Moreover, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis.</p>



<p>The PCS framework will be illustrated through our DeepTune approach to model and characterize neurons in the difficult visual cortex area V4.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-09-04T00:47:54Z</updated>
    <published>2020-09-04T00:47:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-09-11T00:21:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsmath.wordpress.com/?p=2298</id>
    <link href="https://tcsmath.wordpress.com/2020/09/03/itcs-2021-call-for-papers-deadline-extension/" rel="alternate" type="text/html"/>
    <title>ITCS 2021 Call For Papers (deadline extension)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">NOTE: The deadline has been extended to Tuesday, September 8th to account for the Labor Day Holiday in the US. The 12th Innovations in Theoretical Computer Science (ITCS) conference will be held online from January 6-8, 2021. The submission deadline is now September 8, 2020. The program committee encourages you to send your papers our way! See the call for papers for information about submitting to the … <a class="more-link" href="https://tcsmath.wordpress.com/2020/09/03/itcs-2021-call-for-papers-deadline-extension/">Continue reading <span class="screen-reader-text">ITCS 2021 Call For Papers (deadline extension)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>NOTE</strong>:  The deadline has been extended to Tuesday, September 8th to account for the Labor Day Holiday in the US.</p>



<p>The <strong>12th Innovations in Theoretical Computer Science (ITCS)</strong> conference will be held <strong>online</strong> from <strong>January 6-8, 2021</strong>. The <strong>submission deadline</strong> is now <strong>September 8, 2020</strong>.</p>



<p>The <a href="http://itcs-conf.org/">program committee</a> encourages you to send your papers our way! See the <a href="http://itcs-conf.org/">call for papers</a> for information about submitting to the conference.</p>



<p>ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the greater theory community.</p>



<h3>Important dates</h3>



<ul><li><strong>Submission deadline: </strong>September 8, 2020 (05:59PM PDT)</li><li><strong>Notification to authors:</strong> November 1, 2020</li><li><strong>Conference dates: </strong>January 6-8, 2021</li></ul></div>
    </content>
    <updated>2020-09-03T21:53:56Z</updated>
    <published>2020-09-03T21:53:56Z</published>
    <category term="Math"/>
    <author>
      <name>James</name>
    </author>
    <source>
      <id>https://tcsmath.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsmath.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsmath.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsmath.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsmath.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>some mathematics &amp; computation</subtitle>
      <title>tcs math</title>
      <updated>2020-09-11T00:20:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4949</id>
    <link href="https://www.scottaaronson.com/blog/?p=4949" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4949#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4949" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">My Utility+ podcast with Matthew Putman</title>
    <summary xml:lang="en-US">Another Update (Sep. 8): A reader wrote to let me know about a fundraiser for Denys Smirnov, a 2015 IMO gold medalist from Ukraine who needs an expensive bone marrow transplant to survive Hodgkin’s lymphoma. I just donated and I hope you’ll consider it too! Update (Sep. 5): Here’s another quantum computing podcast I did, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Another Update (Sep. 8):</span></strong> A reader wrote to let me know about a <a href="https://www.gofundme.com/f/help-denis-smirnov">fundraiser for Denys Smirnov</a>, a 2015 IMO gold medalist from Ukraine who needs an expensive bone marrow transplant to survive Hodgkin’s lymphoma.  I just donated and I hope you’ll consider it too!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Sep. 5):</span></strong> <a href="https://dunctank.podbean.com/e/scott-aaronson-infinite-universes/">Here’s another quantum computing podcast I did</a>, “Dunc Tank” with Duncan Gammie.  Enjoy!</p>



<p/><hr/>
<p><br/>Thanks so much to <em>Shtetl-Optimized</em> readers, so far we’ve raised $1,371 for the <a href="https://secure.actblue.com/donate/duforjoe">Biden-Harris campaign</a> and $225 for the <a href="https://lincolnproject.us/donate/">Lincoln Project</a>, which I intend to match for $3,192 total.  If you’d like to donate by tonight (Thursday night), there’s still $404 to go!</p><p/>



<p>Meanwhile, a mere three days after declaring my <a href="https://www.scottaaronson.com/blog/?p=4942">“new motto,”</a> I’ve come up with a <em>new</em> new motto for this blog, hopefully a more cheerful one:</p>



<blockquote class="wp-block-quote"><p>When civilization seems on the brink of collapse, sometimes there’s nothing left to talk about but maximal separations between randomized and quantum query complexity.</p></blockquote>



<p>On that note, please enjoy my new <a href="https://nanotronics.co/thinkspace/24-scott-aaronson-before-and-after-the-machine/">one-hour podcast on Spotify</a> (if that link doesn’t work, <a href="https://overcast.fm/+UZFvS_CT8">try this one</a>) with <a href="https://en.wikipedia.org/wiki/Matthew_Putman_(scientist)">Matthew Putman</a> of Utility+.  Alas, my umming and ahhing were more frequent than I now aim for, but that’s partly compensated for by Matthew’s excellent decision to speed up the audio.  This was an unusually wide-ranging interview, covering everything from SlateStarCodex to quantum gravity to interdisciplinary conferences to the challenges of teaching quantum computing to 7-year-olds.  I hope you like it!</p></div>
    </content>
    <updated>2020-09-03T16:43:36Z</updated>
    <published>2020-09-03T16:43:36Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-09-08T18:48:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/128</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/128" rel="alternate" type="text/html"/>
    <title>TR20-128 |  An Optimal Separation of Randomized and Quantum Query Complexity | 

	Alexander A. Sherstov, 

	Andrey Storozhenko, 

	Pei Wu</title>
    <summary>We prove that for every decision tree, the absolute values of the Fourier coefficients of given order $\ell\geq1$ sum to at most $c^{\ell}\sqrt{{d\choose\ell}(1+\log n)^{\ell-1}},$ where $n$ is the number of variables, $d$ is the tree depth, and $c&gt;0$ is an absolute constant. This bound is essentially tight and settles a conjecture due to Tal (arxiv 2019; FOCS 2020). The bounds prior to our work degraded rapidly with $\ell,$ becoming trivial already at $\ell=\sqrt{d}.$

As an application, we obtain, for any positive integer $k,$ a partial Boolean function on $n$ bits that has bounded-error quantum query complexity at most $\lceil k/2\rceil$ and randomized query complexity $\tilde{\Omega}(n^{1-1/k}).$ This separation of bounded-error quantum versus randomized query complexity is best possible, by the results of Aaronson and Ambainis (STOC 2015). Prior to our work, the best known separation was polynomially weaker: $O(1)$ versus $n^{2/3-\epsilon}$ for any $\epsilon&gt;0$ (Tal, FOCS 2020).</summary>
    <updated>2020-09-03T13:24:26Z</updated>
    <published>2020-09-03T13:24:26Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-11T00:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/03/post-doc-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-october-26-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/03/post-doc-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-october-26-2020/" rel="alternate" type="text/html"/>
    <title>Post-doc at Basic Algorithms Research Copenhagen at IT University of Copenhagen (apply by October 26, 2020)</title>
    <summary>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a postdoc employed at IT University of Copenhagen (ITU). The ideal candidate will have proven their research ability through an outstanding PhD thesis, and by publishing in leading international venues for algorithms theory. Website: https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181207&amp;DepartmentId=3439&amp;MediaId=5 Email: pagh@itu.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a postdoc employed at IT University of Copenhagen (ITU).</p>
<p>The ideal candidate will have proven their research ability through an outstanding PhD thesis, and by publishing in leading international venues for algorithms theory.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181207&amp;DepartmentId=3439&amp;MediaId=5">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181207&amp;DepartmentId=3439&amp;MediaId=5</a><br/>
Email: pagh@itu.dk</p></div>
    </content>
    <updated>2020-09-03T08:36:42Z</updated>
    <published>2020-09-03T08:36:42Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-11T00:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/03/phd-student-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-september-28-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/03/phd-student-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-september-28-2020/" rel="alternate" type="text/html"/>
    <title>PhD student at Basic Algorithms Research Copenhagen at IT University of Copenhagen (apply by September 28, 2020)</title>
    <summary>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a PhD student, to be employed at IT University of Copenhagen. The ideal candidate will have proven potential through e.g.: 1) an outstanding MSc thesis, 2) publishing in leading international venues for algorithms theory, or 3) successful participation in international competitions in informatics or […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a PhD student, to be employed at IT University of Copenhagen.</p>
<p>The ideal candidate will have proven potential through e.g.: 1) an outstanding MSc thesis, 2) publishing in leading international venues for algorithms theory, or 3) successful participation in international competitions in informatics or mathematics.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181206&amp;DepartmentId=3439&amp;MediaId=1282">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181206&amp;DepartmentId=3439&amp;MediaId=1282</a><br/>
Email: pagh@itu.dk</p></div>
    </content>
    <updated>2020-09-03T08:35:16Z</updated>
    <published>2020-09-03T08:35:16Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-11T00:20:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-4174113397967272312</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/4174113397967272312/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=4174113397967272312" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/4174113397967272312" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/4174113397967272312" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2020/09/broad-testing-thank-you.html" rel="alternate" type="text/html"/>
    <title>Broad Testing Thank You</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I have a loose association with the Broad Institute, an institute created so that "complementary expertise of the genomic scientists and the chemical biologists across MIT and Harvard be brought together in one place to drive the transformation of medicine with molecular knowledge."  (See <a href="https://www.broadinstitute.org/history">https://www.broadinstitute.org/history</a> )</p><p>They recently passed an amazing milestone, having performed over 1 million Covid tests.  They weren't set up to be a Covid testing lab, but the converted their institute space to respond to the Covid crisis.  (See <a href="https://covid19-testing.broadinstitute.org/">https://covid19-testing.broadinstitute.org/</a> )  </p><p>In short, they stepped up.  They certainly didn't have to, but they did.  Maybe it will help them do good science, now and in the future.  But my understanding is that they saw a clear societal need (lots of <a href="https://www.amazon.com/gp/product/B07DFZ12KV/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=michaelmitzen-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=B07DFZ12KV&amp;linkId=5a5c250c33b2ba71012ff577c69e1a77">outbreak specialists there</a> ) and they realized they had the expertise and equipment to do good that went beyond science.  I just wanted to give a shout out to the Broad for their good works, scientific and societal.  </p></div>
    </content>
    <updated>2020-09-03T00:32:00Z</updated>
    <published>2020-09-03T00:32:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2020-09-10T08:21:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20194</id>
    <link href="https://gilkalai.wordpress.com/2020/09/02/alef-corner-math-collaboration-2/" rel="alternate" type="text/html"/>
    <title>Alef’s Corner: Math Collaboration 2</title>
    <summary>Other Alef’s corner posts</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2020/09/alef.jpg"><img alt="alef-math-collaboration" class="alignnone size-full wp-image-20202" src="https://gilkalai.files.wordpress.com/2020/09/alef-math-collaboration.jpg?w=640"/></a></p>
<p>Other <a href="https://gilkalai.wordpress.com/tag/alefs-corner/">Alef’s corner</a> posts</p></div>
    </content>
    <updated>2020-09-02T13:32:45Z</updated>
    <published>2020-09-02T13:32:45Z</published>
    <category term="Art"/>
    <category term="Combinatorics"/>
    <category term="Geometry"/>
    <category term="What is Mathematics"/>
    <category term="Alef's corner"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-09-11T00:20:29Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra</id>
    <link href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html" rel="alternate" type="text/html"/>
    <title>Isosceles polyhedra</title>
    <summary>My latest arXiv preprint is “On polyhedral realization with isosceles triangles”, arXiv:2009.00116. As the title suggests, it studies polyhedra whose faces are all isosceles triangles. Despite several new results in it, there’s a lot I still don’t know. The paper finds a sort-of-new1 infinite family of polyhedra with congruent isosceles faces, shown below, but I don’t know if there are any more such families. The family of polyhedra from the first image is only “sort-of-new” because the same combinatorial structure was previously described as a triangulation of the sphere by congruent spherical isosceles triangles: Dawson, Robert J. MacG. (2005), “Some new tilings of the sphere with congruent triangles”, Renaissance Banff. In exchange for re-purposing Dawson’s triangulation, my paper describes another infinite family of spherical triangulations by congruent spherical isosceles triangles, not given by Dawson, based on applying a similar \(2\pi/3\) twist to an infinite family of non-convex bipyramids with congruent isosceles faces like the one below. Again, I don’t know whether there are other such families of spherical triangulations. ↩</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest arXiv preprint is “On polyhedral realization with isosceles triangles”, <a href="http://arxiv.org/abs/2009.00116">arXiv:2009.00116</a>. As the title suggests, it studies polyhedra whose faces are all isosceles triangles. Despite several new results in it, there’s a lot I still don’t know. The paper finds a sort-of-new<sup id="fnref:1"><a class="footnote" href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fn:1">1</a></sup> infinite family of polyhedra with congruent isosceles faces, shown below, but I don’t know if there are any more such families.</p>

<p style="text-align: center;"><img alt="Twisted augmented bipyramid with isosceles-triangle faces" src="https://11011110.github.io/blog/assets/2020/twisted.svg"/></p>

<p>One of the other previously known families, shown below, has two integer parameters (the numbers of sides on the two half-bipyramids it combines), but I don’t know whether the same double parameterization is possible for the new family.</p>

<p style="text-align: center;"><img alt="Combination of two half-bipyramids with isosceles-triangle faces" src="https://11011110.github.io/blog/assets/2020/biarc.svg" width="80%"/></p>

<p>In 2001, Branko Grünbaum published an example of a polyhedron that could not be realized with congruent faces, even non-convexly,<sup id="fnref:2"><a class="footnote" href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fn:2">2</a></sup> but it can be realized as a convex polyhedron with all faces isosceles (or equilateral), as shown below. I don’t know whether there are polyhedra that cannot be realized with all faces isosceles, if one allows the realization to be non-convex (but non-self-crossing and combinatorially equivalent to a convex polyhedron) and the faces to be non-congruent.</p>

<p style="text-align: center;"><img alt="Gr&#xFC;nbaum's example of a polyhedron that cannot be realized with congruent faces, realized convexly with isosceles and equilateral triangle faces" src="https://11011110.github.io/blog/assets/2020/grunbaum.svg"/></p>

<p>My new preprint proves that there exist polyhedra (iterated Kleetopes) that cannot be realized as convex polyhedra with isosceles faces. But the construction is a little non-explicit and I don’t know how complicated these polyhedra need to be. For instance, I don’t know whether there is a convex isosceles-face realization of the double Kleetope of the octahedron, shown below.</p>

<p style="text-align: center;"><img alt="Double Kleetope of an octahedron" src="https://11011110.github.io/blog/assets/2020/kkoct.svg"/></p>

<p>Grünbaum’s example can be realized convexly with only two edge lengths, and my non-isosceles-faced polyhedra require at least three edge lengths in any convex realization. I don’t know whether the number of required edge lengths can be unbounded, or whether non-convex realizations ever require three lengths (although certain stacked polyhedra require at least two).</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The family of polyhedra from the first image is only “sort-of-new” because the same combinatorial structure was previously described as a triangulation of the sphere by congruent spherical isosceles triangles: Dawson, Robert J. MacG. (2005), “<a href="https://archive.bridgesmathart.org/2005/bridges2005-489.html">Some new tilings of the sphere with congruent triangles</a>”, Renaissance Banff. In exchange for re-purposing Dawson’s triangulation, my paper describes another infinite family of spherical triangulations by congruent spherical isosceles triangles, not given by Dawson, based on applying a similar \(2\pi/3\) twist to an infinite family of non-convex bipyramids with congruent isosceles faces like the one below. Again, I don’t know whether there are other such families of spherical triangulations.</p>

      <p style="text-align: center;"><img alt="Non-convex polyhedron with congruent isosceles-triangle faces" src="https://11011110.github.io/blog/assets/2020/nwb.svg"/> <a class="reversefootnote" href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>Grünbaum, Branko (2001), “<a href="https://sites.math.washington.edu/~grunbaum/Nonequifacettablesphere.pdf">A convex polyhedron which is not equifacettable</a>”, <em>Geombinatorics</em> 10: 165–171. I don’t know how to access old papers on this journal in general, but fortunately Grünbaum made his one available on his web site. <a class="reversefootnote" href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fnref:2">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/104794000861684463">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-09-01T23:18:00Z</updated>
    <published>2020-09-01T23:18:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-09-08T07:49:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4407</id>
    <link href="https://lucatrevisan.wordpress.com/2020/09/01/time-flies-when-the-world-is-falling-apart/" rel="alternate" type="text/html"/>
    <title>Time Flies When the World is Falling Apart</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">It has been exactly one year since I moved to Italy. Coming here a year ago, I expected that I would face some challenges and that there would be major life changes. Obviously, I did not know the half of … <a href="https://lucatrevisan.wordpress.com/2020/09/01/time-flies-when-the-world-is-falling-apart/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>It has been exactly one year since I moved to Italy.</p>
<p>Coming here a year ago, I expected that I would face some challenges and that there would be major life changes. Obviously, I did not know the half of it.</p>
<p><span id="more-4407"/></p>
<p>In March and April, being in the hottest hot spot of a global pandemic was not ideal. Something rather unexpected, however, has happened. Italy is not known for very effective governance and Italians are not known as a rule-following people. Yet the government implemented a strict lockdown, and held it long enough that the “reopening” took place with a safely low circulation of the virus. People have been mostly following the rules after the reopening, and for most of the summer Italy has been doing quite well. (There has been a recent surge of new cases  in the last few weeks, so it may be too soon to declare victory, but the numbers of intensive care hospitalizations and of deaths are still low).</p>
<p>Meanwhile, there have been “second waves” in several other European countries and the situation in the United States, as the President memorably said, is what it is.</p>
<p>This picture by Noah Berger for AP has been circulating as the perfect description of what is going on in Northern California this summer</p>
<p><img alt="IMG_0077" class="alignnone size-full wp-image-4409" src="https://lucatrevisan.files.wordpress.com/2020/09/img_0077.jpeg?w=584"/></p>
<p>This New York Times headline also does a good job of packing at least five things that are wrong in California right now (the spread of covid-19, the overcrowding of prisons because of excessive sentencing, the budget cuts affecting firefighting efforts, the exploitation of prisoners for underpaid work, and the spread of wildfires) in a single sentence:</p>
<p><img alt="california-inmates" class="alignnone size-full wp-image-4410" src="https://lucatrevisan.files.wordpress.com/2020/09/california-inmates.png?w=584"/></p>
<p>At the cost of resorting to banalities and cliches, Italians often do badly when confronted with good opportunities and lucky breaks, that are often wasted, but we do unexpectedly well in times of crisis. This can even be seen in the national football team, that is known for epic come-from-behind victories and for missing penalty kicks. The American spirit is to be ambitious and optimistic, and to pursue high-risk high-reward opportunities when they present themselves. The flip side is that adversity is often met in America with either despair or anger, typically counterproductively. All things considered, I am happy that I am getting to spend 2020 in Italy. Earlier this summer, the president and the rector of Bocconi announced the new long-term strategic plan for the university, which involves creating a new computer science department. If/when such plans come to fruition, there will be several faculty positions in computer science, at internationally competitive salaries, with advantageous taxation for people moving to Italy from other countries, and with English as the language of instructions, so maybe you too will consider such a move.</p>
<p>Bocconi is preparing to restart in-person teaching in a couple of weeks. The plan is to record and post lectures, to allow students to attend lectures remotely if they prefer, and to use classrooms at most at half capacity. If a class has to be scheduled in a classroom whose capacity is less than twice the class enrollment, students will be split in two groups. Each group will (be allowed to) physically attend in alternating weeks, and will (be required to) attend online at other times.</p>
<p>The space between campus buildings has been marked with well-spaced walking paths, and there are even pedestrian roundabouts.</p>
<p><img alt="IMG_0122" class="alignnone size-full wp-image-4412" src="https://lucatrevisan.files.wordpress.com/2020/09/img_0122.jpeg?w=584"/></p>
<p>Are we heading for a second-wave disaster? Will students really follow the rules? On Sunday we were in Rome and we took a bus to the city center. Every other seat in the bus was marked with a “do not sit” sign to create distancing between sitting people. All marked seats were empty even as the bus was filling up and several people were standing. Then a group of German tourists got in, and they sat in all the forbidden seats. This is the upside-down world of 2020, all bets are off.</p></div>
    </content>
    <updated>2020-09-01T17:36:01Z</updated>
    <published>2020-09-01T17:36:01Z</published>
    <category term="Bocconi"/>
    <category term="Italy"/>
    <category term="philosophy"/>
    <category term="politics"/>
    <category term="things that are terrible"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-09-11T00:20:11Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4647242799583898034</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4647242799583898034/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/a-well-known-theorem-that-has-not-been.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4647242799583898034" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4647242799583898034" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/a-well-known-theorem-that-has-not-been.html" rel="alternate" type="text/html"/>
    <title>A well known theorem that has not been written down- so I wrote it down- CLIQ is #P-complete</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>(The two proofs that CLIQ is #P-complete that I discuss in this blog are written up by Lance and myself and are <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/sharpclique.pdf">here</a>. I think both are well known but I have not been able to find a writeup,so Lance and I did one.)</p><p><br/></p><p> I have been looking at #P (see my last blog on it it, <a href="https://blog.computationalcomplexity.org/2020/08/sharp-p-and-issue-of-natural-problems.html">here</a>, for a refresher on this topic) since I will be teaching this topic in Spring 2021.  Recall</p><p>#SAT(\phi) = the number of satisfying assignments for phi</p><p>#CLIQ((G,k))= the number of cliques of size \ge k of G</p><p>#SAT is #P-complete by a cursory look at the proof of the Cook-Levin theorem.</p><p>A function is #P-complete if everything else in #P is Turing-Poly red to it. To show for some set A </p><p>in NP, #A is #P-complete one usually uses pars reductions. </p><p>I wanted a proof that #CLIQ is #P-complete, so I wanted a parsimonious reduction from SAT to CLIQ (Thats a reduction f: SAT--&gt; CLIQ such that the number of satisfying assignments of phi equals the number of cliques of size \ge k of G.)</p><p>I was sure there is such a reduction and that it was well known and would be on the web someplace. So I tried to find it.</p><p>ONE:</p><p>I tracked some references to a paper by Janos Simon (<a href="https://link.springer.com/content/pdf/10.1007%2F3-540-08342-1_37.pdf">see here</a>)  where he claims that the reduction of SAT to CLIQ  by Karp <a href="https://blog.computationalcomplexity.org/feeds/posts/{http://cgi.di.uoa.gr/~sgk/teaching/grad/handouts/karp.pdf">(see here)</a> was pars.  I had already considered that and decided that Karps reduction was NOT pars.  I looked at both Simon's paper and Karp's paper to make sure I wasn't missing something (e.g., I misunderstood what Simon Says or what Karp ... darn, I can't think of anything as good as `Simon Says'). It seemed to me that Simon was incorrect. If I am wrong let me know.</p><p>TWO</p><p>Someone told me that it was in Valiant's  paper (see <a href="https://epubs.siam.org/doi/10.1137/0208032">here</a>). Nope. Valiant's paper shows that counting the number of maximal cliques is #P-complete. Same Deal Here- if I am wrong let me know. One can modify Valiant's argument to get #CLIQ #P-complete, and I do so in the write up. The proof is a string of reductions and starts with PERM is #P-complete. This does prove that# CLIQ is  #P-complete, but is rather complicated. Also, the reduction is not pars.</p><p>THREE <br/>Lance told me an easy pars reduction that is similar to Karp's non-pars reduction, but it really is NOT Karp's reduction. Its in my write up. I think it is well known since I recall hearing it about 10 years ago. From Lance. Hmmm, maybe its just well known to Lance.</p><p><br/></p><p>But here is my question: I am surprised I didn't find it on the web. If someone can point to a place on the web where it is, please let me know. In any case, its on the web NOW (my write up) so hopefully in the future someone else looking for it can find it.</p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2020-09-01T15:29:00Z</updated>
    <published>2020-09-01T15:29:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-11T00:02:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/01/quics-hartree-postdoctoral-fellowships-at-joint-center-for-quantum-information-and-computer-science-apply-by-december-1-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/01/quics-hartree-postdoctoral-fellowships-at-joint-center-for-quantum-information-and-computer-science-apply-by-december-1-2020/" rel="alternate" type="text/html"/>
    <title>QuICS Hartree Postdoctoral Fellowships at Joint Center for Quantum Information and Computer Science (apply by December 1, 2020)</title>
    <summary>The Joint Center for Quantum Information and Computer Science (QuICS, http://quics.umd.edu) is seeking exceptional candidates for the QuICS Hartree Postdoctoral Fellowships in Quantum Information and Computer Science. Applications should be submitted through AcademicJobsOnline at https://academicjobsonline.org/ajo/jobs/16778. Website: https://academicjobsonline.org/ajo/jobs/16778 Email: quics-coordinator@umiacs.umd.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Joint Center for Quantum Information and Computer Science (QuICS, <a href="http://quics.umd.edu">http://quics.umd.edu</a>) is seeking exceptional candidates for the QuICS Hartree Postdoctoral Fellowships in Quantum Information and Computer Science.<br/>
Applications should be submitted through AcademicJobsOnline at <a href="https://academicjobsonline.org/ajo/jobs/16778.">https://academicjobsonline.org/ajo/jobs/16778.</a></p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/16778">https://academicjobsonline.org/ajo/jobs/16778</a><br/>
Email: quics-coordinator@umiacs.umd.edu</p></div>
    </content>
    <updated>2020-09-01T14:39:18Z</updated>
    <published>2020-09-01T14:39:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-11T00:20:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-25562705.post-7958692547156288391</id>
    <link href="http://aaronsadventures.blogspot.com/feeds/7958692547156288391/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=7958692547156288391" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/7958692547156288391" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/7958692547156288391" rel="self" type="application/atom+xml"/>
    <link href="http://aaronsadventures.blogspot.com/2020/07/the-existence-of-no-regret-learning.html" rel="alternate" type="text/html"/>
    <title>No Regret Algorithms from the Min Max Theorem</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The existence of no-regret learning algorithms can be used to prove <a href="https://en.wikipedia.org/wiki/Minimax_theorem">Von-Neumann's min-max theorem</a>. This argument is originally due to <a href="https://www.cs.princeton.edu/~schapire/papers/FreundScYY.pdf">Freund and Schapire</a>, and I <a href="https://www.cis.upenn.edu/~aaroth/courses/slides/agt20/lect08.pdf">teach it to my undergraduates </a>in my algorithmic game theory class. The min-max theorem also can be used to prove the existence of no-regret learning algorithms. Here is a constructive version of the argument (Constructive in that in the resulting algorithm, you only need to solve polynomially sized zero-sum games, so you can do it via linear programming).<div><br/></div><div>Recall the setting. Play proceeds in rounds $t \in \{1,\ldots,T\}$. At each day $t$, the learner chooses one of $k$ actions $i_t \in \{1,\ldots,k\}$, and the adversary chooses a loss vector $\ell^t \in [0,1]^k$. The learner incurs loss $\ell^t_{i_t}$, corresponding to the action he chose. At the end of the interaction, the regret of the learner is defined to be the difference between the cumulative loss he incurred and the cumulative loss of the best fixed action (played consistently)  in hindsight:</div><div>$$\textrm{Regret}_T = \max_j \left(\sum_{t=1}^T \ell^t_{i_t} - \ell^t_j\right)$$ </div><div>A classical and remarkable result is that there exist algorithms that can guarantee that regret grows only sublinearly with time: $\textrm{Regret}_T = O(\sqrt{T})$. Lets prove this. </div><div><br/></div><div><br/></div><div>Define the non-negative portion of our cumulative regret with respect to action $j$ up until day $d$ as:</div><div>$$V_d^j = \left(\sum_{t=1}^d\left(\ell^t_{i_t} - \ell^t_j\right)\right)^+$$</div><div>and our additional regret at day $d+1$ with respect to action $j$ as:</div><div>$$r_{j}^{d+1} = \ell_{i_{d+1}}^{d+1} - \ell^{d+1}_j$$</div><div>Observe that if $V_{d}^j \geq 1$  then $V_{d+1}^j = V_d^j + r_j^{d+1}$. </div><div><br/></div><div><br/></div><div>Define a surrogate loss function as our squared cumulative regrets, summed over all actions: </div><div>$$L_d = \sum_{j=1}^k (V_d^j)^2$$</div><div>Observe that we can write the expected gain in our loss on day $d+1$, conditioned on the history thus far:</div><div>$$\mathbb{E}[L_{d+1} - L_d] \leq \sum_{j : V_d^j \geq 1} \mathbb{E}[(V_d^j+r_j^{d+1})^2 - (V_d^j)^2) ] + 3k$$</div><div>$$= \sum_{j : V_d^j \geq 1} \left(2V_d^j \mathbb{E}[r_{j}^{d+1}] + \mathbb{E}[(r_{j}^{d+1})^2]\right) + 3k $$</div><div>$$\leq \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right) + 4k$$</div><div>where the expectations are taken over the randomness of both the learner and the adversary in round $d+1$. </div><div><br/></div><div>Now consider a zero-sum game played between the learner and the adversary in which the learner is the minimization player, the adversary is the maximization player, and the utility function is $$u(i_{d+1}, \ell^{d+1}) = \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right)$$ The min-max theorem says that the learner can guarantee the same payoff for herself in the following two scenarios:</div><div><br/></div><div><ol style="text-align: left;"><li>The learner first has to commit to playing a distribution $p_{d+1}$ over actions $i$, and then the adversary gets to best respond by picking the worst possible loss vectors, or</li><li>The adversary has to first commit to a distribution over loss vectors $\ell$ and then the learner gets the benefit of picking the best action $i_{d+1}$ to respond with. </li></ol>Scenario 1) is the scenario our learner finds herself in, when playing against an adaptive adversary. But 2) is much easier to analyze. If the adversary first commits to a distribution over loss vectors $\ell^{d+1}$, the learner can always choose action $i_{d+1} = \arg\min_j \mathbb{E}[\ell^{d+1}_j]$, which guarantees that $\mathbb{E}[r_{j}^{d+1}] \leq 0$, which in turn guarantees that the value of the game $ \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right) \leq 0$.  Hence, the min-max theorem tells us that the learner always has a distribution over actions $p_{d+1}$ that guarantees that $\mathbb{E}[L_{d+1} - L_d] \leq 4k$, <i>even in the worst case over loss functions</i>. If the learner always plays according to this distribution, then by a telescoping sum, we have that:</div><div>$$\mathbb{E}[L_T] \leq 4kT$$.</div><div>We therefore have by Jensen's inequality that:</div><div>$$\mathbb{E}[\max_j (V^j_T)] \leq \sqrt{\mathbb{E}[\max_j (V^j_T)^2]}\leq \sqrt{\mathbb{E}[\sum_{j=1}^k (V_j^T)^2]} \leq 2\sqrt{kT}$$.</div></div>
    </content>
    <updated>2020-09-01T14:16:00Z</updated>
    <published>2020-09-01T14:16:00Z</published>
    <author>
      <name>Aaron</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/09952936358739421126</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-25562705</id>
      <category term="game theory"/>
      <category term="news"/>
      <author>
        <name>Aaron</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/09952936358739421126</uri>
      </author>
      <link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <title>Adventures in Computation</title>
      <updated>2020-09-03T08:11:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/01/postdoc-at-tu-hamburg-apply-by-september-30-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/01/postdoc-at-tu-hamburg-apply-by-september-30-2020/" rel="alternate" type="text/html"/>
    <title>postdoc at TU Hamburg (apply by September 30, 2020)</title>
    <summary>The Institute for Algorithms and Complexity at TUHH: Hamburg University of Technology invites applications for a 5-year postdoc position (attached to DASHH: Data Science in Hamburg). The focus is on algorithms, applied to combinatorial optimization, operations research, and discrete mathematics in the natural sciences, particularly on big data obtained at Germany’s largest particle accelerator. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Institute for Algorithms and Complexity at TUHH: Hamburg University of Technology invites applications for a 5-year postdoc position (attached to DASHH: Data Science in Hamburg). The focus is on algorithms, applied to combinatorial optimization, operations research, and discrete mathematics in the natural sciences, particularly on big data obtained at Germany’s largest particle accelerator.</p>
<p>Website: <a href="https://www.tuhh.de/algo/jobs.html">https://www.tuhh.de/algo/jobs.html</a><br/>
Email: matthias.mnich@tuhh.de</p></div>
    </content>
    <updated>2020-09-01T08:46:00Z</updated>
    <published>2020-09-01T08:46:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-11T00:20:37Z</updated>
    </source>
  </entry>
</feed>
