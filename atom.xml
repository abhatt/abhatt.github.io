<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-11-14T20:21:57Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20366</id>
    <link href="https://gilkalai.wordpress.com/2020/11/14/to-cheer-you-up-in-difficult-times-13-triangulating-real-projective-spaces-with-subexponentially-many-vertices/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 13: Triangulating real projective spaces with subexponentially many vertices</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Wolfgang Kühnel Today I want to talk about a new result in a newly arXived paper: A subexponential size by Karim Adiprasito, Sergey Avvakumov, and Roman Karasev. Sergey Avvakumov gave about it a great zoom seminar talk about the result … <a href="https://gilkalai.wordpress.com/2020/11/14/to-cheer-you-up-in-difficult-times-13-triangulating-real-projective-spaces-with-subexponentially-many-vertices/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2020/11/wk.jpg"><img alt="" class="alignnone size-full wp-image-20381" src="https://gilkalai.files.wordpress.com/2020/11/wk.jpg?w=640"/></a></p>
<p><span style="color: #ff0000;">Wolfgang Kühnel</span></p>
<p>Today I want to talk about a new result in a newly arXived paper: <a href="https://arxiv.org/abs/2009.02703">A subexponential size <img alt="\mathbb RP^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+RP%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\mathbb RP^n"/> by Karim Adiprasito, Sergey Avvakumov, and Roman Karasev</a>. Sergey Avvakumov gave about it a <strong>great</strong> zoom seminar talk about the result in our combinatorics seminar. <a href="https://huji.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=602454e6-d104-42fe-849c-ac6000b7517b">Here is the link</a>.</p>
<p>The question is very simple: <strong>W<span style="color: #0000ff;">hat is the smallest number of vertices required to triangulate the <em>n</em>-dimensional real projective space?</span></strong></p>
<p>The short paper exhibits a triangulation with <img alt="\exp (\frac {1}{2} \sqrt n \log n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp+%28%5Cfrac+%7B1%7D%7B2%7D+%5Csqrt+n+%5Clog+n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\exp (\frac {1}{2} \sqrt n \log n)"/> vertices.  This is the first construction that requires subexponential number of vertices in the dimension. The best lower bound by Pierre Arnoux and Alexis Marin (from 1999) are quadratic, so there is quite a way to go with the problem. I am thankful to Ryan Alweiss who was the first to tell me about the new result.</p>
<h2>Reasons to care</h2>
<p>Representing a topological space using simplicial complexes arose in the early days of algebraic topology, but there are certainly more “efficient” representations. What is the reason to to care specifically about representations via simplicial complexes? Here are my answers</p>
<ol>
<li>Constructions of triangulated manifolds with few vertices are occasionally amazing mathematical objects.</li>
<li>Simplicial complexes arise in many combinatorial and algebraic contexts,</li>
<li>The study of face numbers of simplicial complexes with various topological properties is often related to deep questions in algebra, geometry, and topology,</li>
<li>We care also about other types of representation.</li>
</ol>
<h2>Kühnel and Lassmann’s complex projective plane with nine vertices and other miracles</h2>
<p>You may all be familiar with the 6-vertex triangulation of the projective space. It is obtained by identifying opposite faces of the icosahedron. It is 2-neighborly, namely every pair of vertices span an edge of the triangulation. (It also played a role in my high dimensional extension of Cayley’s counting trees formula.) The existence of 2-neighborly triangulations of other closed surfaces is essentially the <a href="https://en.wikipedia.org/wiki/Heawood_conjecture">Heawood 1890 conjecture</a> solved by Ringel and Young in the 1960s.</p>
<p>In 1980 Wolfgang Kühnel set out to construct a 3-neighborly triangulation of the complex projective plane with 9 vertices. The construction, achieved in <a href="https://www.sciencedirect.com/science/article/pii/0097316583900055?via%3Dihub">a paper by Kühnel and Lassmann</a> (who also proved uniqueness), and discussed in <a href="http://www.math.brown.edu/~banchoff/howison/newbanchoff/publications/pdfs/9Vertex.pdf">a paper of the same year by Kühnel and Banchof</a>, is, in my view, among the most beautiful constructions in mathematics with additional hidden structures and many connections. Some motivation to the work came from the study of tight embeddings of smooth manifolds and Morse theory.</p>
<p>Ulrich Brehm and Kühnel <a href="https://www.sciencedirect.com/science/article/pii/0040938387900425?via%3Dihub">proved</a> that if a <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3">d</span></span></span></span>-manifold has fewer than <span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-4"><span class="mrow" id="MathJax-Span-5"><span class="mn" id="MathJax-Span-6">3</span><span class="mo" id="MathJax-Span-7">⌈</span><span class="mi" id="MathJax-Span-8">d</span><span class="texatom" id="MathJax-Span-9"><span class="mrow" id="MathJax-Span-10"><span class="mo" id="MathJax-Span-11">/</span></span></span><span class="mn" id="MathJax-Span-12">2</span><span class="mo" id="MathJax-Span-13">⌉</span><span class="mo" id="MathJax-Span-14">+</span><span class="mn" id="MathJax-Span-15">3</span></span></span></span> vertices then it is homeomorphic to a sphere, and if it has exactly <span class="MathJax" id="MathJax-Element-3-Frame"><span class="math" id="MathJax-Span-16"><span class="mrow" id="MathJax-Span-17"><span class="mn" id="MathJax-Span-18">3</span><span class="mo" id="MathJax-Span-19">(</span><span class="mi" id="MathJax-Span-20">d</span><span class="texatom" id="MathJax-Span-21"><span class="mrow" id="MathJax-Span-22"><span class="mo" id="MathJax-Span-23">/</span></span></span><span class="mn" id="MathJax-Span-24">2</span><span class="mo" id="MathJax-Span-25">)</span><span class="mo" id="MathJax-Span-26">+</span><span class="mn" id="MathJax-Span-27">3</span></span></span></span> vertices and is not a sphere, then <span class="MathJax" id="MathJax-Element-4-Frame"><span class="math" id="MathJax-Span-28"><span class="mrow" id="MathJax-Span-29"><span class="mi" id="MathJax-Span-30">d</span><span class="mo" id="MathJax-Span-31">=</span><span class="mn" id="MathJax-Span-32">2</span><span class="mo" id="MathJax-Span-33">,</span><span class="mn" id="MathJax-Span-34">4</span><span class="mo" id="MathJax-Span-35">,</span><span class="mn" id="MathJax-Span-36">8</span></span></span></span> or <span class="MathJax" id="MathJax-Element-5-Frame"><span class="math" id="MathJax-Span-37"><span class="mrow" id="MathJax-Span-38"><span class="mn" id="MathJax-Span-39">16</span></span></span></span> and the manifold has a nondegenerate height function with exactly three critical points. The 6-vertex <img alt="\mathbb RP^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+RP%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\mathbb RP^2"/> and the 9-vertex $\mathbbCP^2$ are examples for dimensions 2 and 4. We can expect further such examples for projective planes over the quaternions and octonions. <a href="https://link.springer.com/article/10.1007%2FBF01934320">Brehm and Kühnel</a> constructed three 8-dimensional 5-neighborly “manifolds” which are not combinatorially isomorphic. It is conjectured but not known that they all triangulate the quaternionic projective plane.</p>
<h2>The result of <a href="https://arxiv.org/abs/2009.02703">Adiprasito, Avvakumov, and Karasev</a> as <a href="https://huji.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=602454e6-d104-42fe-849c-ac6000b7517b">told by Sergey Avvakumov</a>.</h2>
<p>Here is a brief description of Sergey Avvakumov’s lecture. (As far as I could see the proof of their result is fully presented along with  3 other proofs for basic related results.)</p>
<p><span style="color: #0000ff;"><strong>06:00</strong></span>  The lecture starts. A simple inductive constructions of  a triangulation of <img alt="\mathbb R P^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R+P%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\mathbb R P^n"/>.</p>
<p><span style="color: #0000ff;"><strong>08:00</strong></span>  Equivalent statement: we seek a centrally symmetric triangulation of the n-sphere with diameter 3.</p>
<p><strong><span style="color: #0000ff;">09:00</span></strong> 6-vertex triangulation of  <img alt="\mathbb R P^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R+P%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\mathbb R P^2"/></p>
<p><strong><span style="color: #0000ff;">11:00</span></strong> The construction: triangulate the positive facets of the cross polytope and symmetrically the negative facet. Follow a recipe to triangulate the side facets. Crucial question: what is needed from the triangulation T of the positive facet. Answer: <span style="color: #0000ff;"><strong>No edge between two disjoint faces</strong></span>.</p>
<p><span style="color: #0000ff;"><strong>23:00</strong> </span>Spherical interpretation and Delaunay triangulations of a certain configuration V</p>
<p><strong><span style="color: #0000ff;">31:00</span></strong>   A sufficient combinatorial condition</p>
<p><strong><span style="color: #0000ff;">39:00</span></strong> Proof of sufficiency</p>
<p><strong><span style="color: #0000ff;">54:00</span></strong> The construction!</p>
<p><strong><span style="color: #0000ff;">1:09:00</span></strong> Counting the number of vertices completes the proof of the theorem.</p>
<p>Additional matters</p>
<p><strong><span style="color: #0000ff;">1:11:00</span></strong> A proof of a quadratic lower bound by Kuhnel.</p>
<p><strong><span style="color: #0000ff;">1:21:00</span></strong> Proof of the facet lower bound (Barany and Lovasz) via Borsuk-Ulam</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/11/semtri7.png"><img alt="" class="alignnone size-full wp-image-20397" height="306" src="https://gilkalai.files.wordpress.com/2020/11/semtri7.png?w=640&amp;h=306" width="640"/></a></p>
<h2>Open problems and Low dimensions</h2>
<p>Of course a main question is what is the minimum number of vertices required for a triangulation of n dimensional real projective space. What about the n-dimensional complex projective space? What is the situation for low dimensions? See also the paper by Sonia Balagopalan, <a href="https://arxiv.org/abs/1409.6149">On a vertex-minimal triangulation of RP^4</a>.</p></div>
    </content>
    <updated>2020-11-14T17:52:54Z</updated>
    <published>2020-11-14T17:52:54Z</published>
    <category term="Algebra"/>
    <category term="Combinatorics"/>
    <category term="Geometry"/>
    <category term="Karim Adiprasito"/>
    <category term="Roman Karasev"/>
    <category term="Sergey Avvakumov"/>
    <category term="Wolfgang Kuhnel"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-11-14T20:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/11/14/postdoc-positions-at-foundations-of-data-science-institute-fodsi-apply-by-december-1-2020/</id>
    <link href="https://cstheory-jobs.org/2020/11/14/postdoc-positions-at-foundations-of-data-science-institute-fodsi-apply-by-december-1-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc positions at Foundations of Data Science Institute (FODSI) (apply by December 1, 2020)</title>
    <summary>The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. Multiple positions are available. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College. Website: https://academicjobsonline.org/ajo/jobs/17305 Email: see the URL</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. Multiple positions are available. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/17305">https://academicjobsonline.org/ajo/jobs/17305</a><br/>
Email: see the URL</p></div>
    </content>
    <updated>2020-11-14T17:52:02Z</updated>
    <published>2020-11-14T17:52:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-14T20:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/172</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/172" rel="alternate" type="text/html"/>
    <title>TR20-172 |  Optimal rate list decoding over bounded alphabets using algebraic-geometric codes | 

	Venkatesan Guruswami, 

	Chaoping Xing</title>
    <summary>We construct two classes of algebraic code families which are efficiently list decodable with small output list size from a fraction $1-R-\epsilon$ of adversarial errors where $R$ is the rate of the code, for any desired positive constant $\epsilon$. The alphabet size depends only on $\epsilon$ and is nearly-optimal.

The first class of codes are obtained by folding algebraic-geometric codes using automorphisms of the underlying function field. The second class of codes are obtained by restricting evaluation points of an algebraic-geometric code to rational points from a subfield. In both cases, we develop a linear-algebraic approach to perform list decoding, which pins down the candidate messages to a subspace with a nice "periodic" structure.

To prune this subspace and obtain a good bound on the list-size, we pick subcodes of these codes by pre-coding into certain subspace-evasive sets which are guaranteed to have small intersection with the sort of periodic subspaces that arise in our list decoding. We develop two approaches for constructing such subspace-evasive sets. The first is a Monte Carlo construction of hierearchical subspace-evasive (h.s.e) sets which leads to excellent list-size but is not explicit. The second approach exploits a further ultra-periodicity of our subspaces and uses a novel construct called subspace designs, which were subsequently constructed explicitly and also found further applications in pseudorandomness.

To get a family of codes over a fixed alphabet size, we instantiate our approach with algebraic-geometric codes based on the Garcia-Stichtenoth tower of function fields. Combining this with pruning via h.s.e sets yields codes list-decodable up to a $1-R-\epsilon$ error fraction with list size bounded by $O(1/\epsilon)$, matching the existential bound for random codes up to constant factors. Further, the alphabet size can be made $\exp(\tilde{O}(1/\epsilon^2))$ which is not much worse than the lower bound of $\exp(\Omega(1/\epsilon))$. The parameters we achieve are thus quite close to the  existential bounds in all three aspects---error-correction radius, alphabet size, and list-size---simultaneously. This construction is, however, Monte Carlo and the claimed list decoding property only holds with high probability. Once the code is (efficiently) sampled, the encoding/decoding algorithms are deterministic with a running time $O_\epsilon(N^c)$ for an absolute constant $c$, where $N$ is the code's block length.

Using subspace designs instead for the pruning, our approach yields a deterministic construction of an algebraic code family of rate $R$ with efficient list decoding from $1-R-\epsilon$ fraction of errors over an alphabet of constant size $\exp(\tilde{O}(1/\epsilon^2))$.  The list size bound is upper bounded by a very slowly growing function of the block length $N$; in particular, it is at most $O(\log^{(r)} N)$ (the $r$'th iterated logarithm) for any fixed integer $r$. The explicit construction avoids the shortcoming of the Monte Carlo sampling at the expense of a worse list size.</summary>
    <updated>2020-11-13T15:16:25Z</updated>
    <published>2020-11-13T15:16:25Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-11-14T20:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06585</id>
    <link href="http://arxiv.org/abs/2011.06585" rel="alternate" type="text/html"/>
    <title>Sparse PCA: Algorithms, Adversarial Perturbations and Certificates</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Tommaso d'Orsi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kothari:Pravesh_K=.html">Pravesh K. Kothari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Novikov:Gleb.html">Gleb Novikov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Steurer:David.html">David Steurer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06585">PDF</a><br/><b>Abstract: </b>We study efficient algorithms for Sparse PCA in standard statistical models
(spiked covariance in its Wishart form). Our goal is to achieve optimal
recovery guarantees while being resilient to small perturbations. Despite a
long history of prior works, including explicit studies of perturbation
resilience, the best known algorithmic guarantees for Sparse PCA are fragile
and break down under small adversarial perturbations.
</p>
<p>We observe a basic connection between perturbation resilience and
\emph{certifying algorithms} that are based on certificates of upper bounds on
sparse eigenvalues of random matrices. In contrast to other techniques, such
certifying algorithms, including the brute-force maximum likelihood estimator,
are automatically robust against small adversarial perturbation.
</p>
<p>We use this connection to obtain the first polynomial-time algorithms for
this problem that are resilient against additive adversarial perturbations by
obtaining new efficient certificates for upper bounds on sparse eigenvalues of
random matrices. Our algorithms are based either on basic semidefinite
programming or on its low-degree sum-of-squares strengthening depending on the
parameter regimes. Their guarantees either match or approach the best known
guarantees of \emph{fragile} algorithms in terms of sparsity of the unknown
vector, number of samples and the ambient dimension.
</p>
<p>To complement our algorithmic results, we prove rigorous lower bounds
matching the gap between fragile and robust polynomial-time algorithms in a
natural computational model based on low-degree polynomials (closely related to
the pseudo-calibration technique for sum-of-squares lower bounds) that is known
to capture the best known guarantees for related statistical estimation
problems. The combination of these results provides formal evidence of an
inherent price to pay to achieve robustness.
</p></div>
    </summary>
    <updated>2020-11-13T23:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06572</id>
    <link href="http://arxiv.org/abs/2011.06572" rel="alternate" type="text/html"/>
    <title>Relative Lipschitzness in Extragradient Methods and a Direct Recipe for Acceleration</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Michael_B=.html">Michael B. Cohen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sidford:Aaron.html">Aaron Sidford</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tian:Kevin.html">Kevin Tian</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06572">PDF</a><br/><b>Abstract: </b>We show that standard extragradient methods (i.e. mirror prox and dual
extrapolation) recover optimal accelerated rates for first-order minimization
of smooth convex functions. To obtain this result we provide fine-grained
characterization of the convergence rates of extragradient methods for solving
monotone variational inequalities in terms of a natural condition we call
relative Lipschitzness. We further generalize this framework to handle local
and randomized notions of relative Lipschitzness and thereby recover rates for
box-constrained $\ell_\infty$ regression based on area convexity and complexity
bounds achieved by accelerated (randomized) coordinate descent for smooth
convex function minimization.
</p></div>
    </summary>
    <updated>2020-11-13T23:05:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06551</id>
    <link href="http://arxiv.org/abs/2011.06551" rel="alternate" type="text/html"/>
    <title>Efficient Solution of Boolean Satisfiability Problems with Digital MemComputing</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>S. R. B. Bearden, Y. R. Pei, M. Di Ventra <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06551">PDF</a><br/><b>Abstract: </b>Boolean satisfiability is a propositional logic problem of interest in
multiple fields, e.g., physics, mathematics, and computer science. Beyond a
field of research, instances of the SAT problem, as it is known, require
efficient solution methods in a variety of applications. It is the decision
problem of determining whether a Boolean formula has a satisfying assignment,
believed to require exponentially growing time for an algorithm to solve for
the worst-case instances. Yet, the efficient solution of many classes of
Boolean formulae eludes even the most successful algorithms, not only for the
worst-case scenarios, but also for typical-case instances. Here, we introduce a
memory-assisted physical system (a digital memcomputing machine) that, when its
non-linear ordinary differential equations are integrated numerically, shows
evidence for polynomially-bounded scalability while solving "hard"
planted-solution instances of SAT, known to require exponential time to solve
in the typical case for both complete and incomplete algorithms. Furthermore,
we analytically demonstrate that the physical system can efficiently solve the
SAT problem in continuous time, without the need to introduce chaos or an
exponentially growing energy. The efficiency of the simulations is related to
the collective dynamical properties of the original physical system that
persist in the numerical integration to robustly guide the solution search even
in the presence of numerical errors. We anticipate our results to broaden
research directions in physics-inspired computing paradigms ranging from theory
to application, from simulation to hardware implementation.
</p></div>
    </summary>
    <updated>2020-11-13T22:43:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06545</id>
    <link href="http://arxiv.org/abs/2011.06545" rel="alternate" type="text/html"/>
    <title>Towards Better Approximation of Graph Crossing Number</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chuzhoy:Julia.html">Julia Chuzhoy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahabadi:Sepideh.html">Sepideh Mahabadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Zihan.html">Zihan Tan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06545">PDF</a><br/><b>Abstract: </b>Graph Crossing Number is a fundamental problem with various applications. In
this problem, the goal is to draw an input graph $G$ in the plane so as to
minimize the number of crossings between the images of its edges. Despite
extensive work, non-trivial approximation algorithms are only known for
bounded-degree graphs. Even for this special case, the best current algorithm
achieves a $\tilde O(\sqrt n)$-approximation, while the best current negative
result is APX-hardness. All current approximation algorithms for the problem
build on the same paradigm: compute a set $E'$ of edges (called a
\emph{planarizing set}) such that $G\setminus E'$ is planar; compute a planar
drawing of $G\setminus E'$; then add the drawings of the edges of $E'$ to the
resulting drawing. Unfortunately, there are examples of graphs, in which any
implementation of this method must incur $\Omega (\text{OPT}^2)$ crossings,
where $\text{OPT}$ is the value of the optimal solution. This barrier seems to
doom the only known approach to designing approximation algorithms for the
problem, and to prevent it from yielding a better than $O(\sqrt
n)$-approximation.
</p>
<p>In this paper we propose a new paradigm that allows us to overcome this
barrier. We show an algorithm that, given a bounded-degree graph $G$ and a
planarizing set $E'$ of its edges, computes another set $E''$ with $E'\subseteq
E''$, such that $|E''|$ is relatively small, and there exists a near-optimal
drawing of $G$ in which only edges of $E''$ participate in crossings. This
allows us to reduce the Crossing Number problem to \emph{Crossing Number with
Rotation System} -- a variant in which the ordering of the edges incident to
every vertex is fixed as part of input. We show a randomized algorithm for this
new problem, that allows us to obtain an $O(n^{1/2-\epsilon})$-approximation
for Crossing Number on bounded-degree graphs, for some constant $\epsilon&gt;0$.
</p></div>
    </summary>
    <updated>2020-11-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06535</id>
    <link href="http://arxiv.org/abs/2011.06535" rel="alternate" type="text/html"/>
    <title>Quantum Random Access Codes for Boolean Functions</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Doriguello:Jo=atilde=o_F=.html">João F. Doriguello</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Montanaro:Ashley.html">Ashley Montanaro</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06535">PDF</a><br/><b>Abstract: </b>An $n\overset{p}{\mapsto}m$ random access code (RAC) is an encoding of $n$
bits into $m$ bits such that any initial bit can be recovered with probability
at least $p$, while in a quantum RAC (QRAC), the $n$ bits are encoded into $m$
qubits. Since its proposal, the idea of RACs was generalized in many different
ways, e.g. allowing the use of shared entanglement (called
entanglement-assisted random access code, or simply EARAC) or recovering
multiple bits instead of one. In this paper we generalize the idea of RACs to
recovering the value of a given Boolean function $f$ on any subset of fixed
size of the initial bits, which we call $f$-random access codes. We study and
give protocols for $f$-random access codes with classical ($f$-RAC) and quantum
($f$-QRAC) encoding, together with many different resources, e.g. private or
shared randomness, shared entanglement ($f$-EARAC) and Popescu-Rohrlich boxes
($f$-PRRAC). The success probability of our protocols is characterized by the
\emph{noise stability} of the Boolean function $f$. Moreover, we give an
\emph{upper bound} on the success probability of any $f$-QRAC with shared
randomness that matches its success probability up to a multiplicative constant
(and $f$-RACs by extension), meaning that quantum protocols can only achieve a
limited advantage over their classical counterparts.
</p></div>
    </summary>
    <updated>2020-11-13T22:39:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06530</id>
    <link href="http://arxiv.org/abs/2011.06530" rel="alternate" type="text/html"/>
    <title>Towards Tight Bounds for Spectral Sparsification of Hypergraphs</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kapralov:Michael.html">Michael Kapralov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krauthgamer:Robert.html">Robert Krauthgamer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tardos:Jakab.html">Jakab Tardos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoshida:Yuichi.html">Yuichi Yoshida</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06530">PDF</a><br/><b>Abstract: </b>Cut and spectral sparsification of graphs have numerous applications,
including e.g. speeding up algorithms for cuts and Laplacian solvers. These
powerful notions have recently been extended to hypergraphs, which are much
richer and may offer new applications. However, the current bounds on the size
of hypergraph sparsifiers are not as tight as the corresponding bounds for
graphs.
</p>
<p>Our first result is a polynomial-time algorithm that, given a hypergraph on
$n$ vertices with maximum hyperedge size $r$, outputs an $\epsilon$-spectral
sparsifier with $O^*(nr)$ hyperedges, where $O^*$ suppresses $(\epsilon^{-1}
\log n)^{O(1)}$ factors. This size bound improves the two previous bounds:
$O^*(n^3)$ [Soma and Yoshida, SODA'19] and $O^*(nr^3)$ [Bansal, Svensson and
Trevisan, FOCS'19]. Our main technical tool is a new method for proving
concentration of the nonlinear analogue of the quadratic form of the Laplacians
for hypergraph expanders.
</p>
<p>We complement this with lower bounds on the bit complexity of any compression
scheme that $(1+\epsilon)$-approximates all the cuts in a given hypergraph, and
hence also on the bit complexity of every $\epsilon$-cut/spectral sparsifier.
These lower bounds are based on Ruzsa-Szemer\'edi graphs, and a particular
instantiation yields an $\Omega(nr)$ lower bound on the bit complexity even for
fixed constant $\epsilon$. This is tight up to polylogarithmic factors in $n$,
due to recent hypergraph cut sparsifiers of [Chen, Khanna and Nagda, FOCS'20].
</p>
<p>Finally, for directed hypergraphs, we present an algorithm that computes an
$\epsilon$-spectral sparsifier with $O^*(n^2r^3)$ hyperarcs, where $r$ is the
maximum size of a hyperarc. For small $r$, this improves over $O^*(n^3)$ known
from [Soma and Yoshida, SODA'19], and is getting close to the trivial lower
bound of $\Omega(n^2)$ hyperarcs.
</p></div>
    </summary>
    <updated>2020-11-13T23:00:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06516</id>
    <link href="http://arxiv.org/abs/2011.06516" rel="alternate" type="text/html"/>
    <title>Sample-driven optimal stopping: From the secretary problem to the i.i.d. prophet inequality</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Correa:Jos=eacute=.html">José Correa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cristi:Andr=eacute=s.html">Andrés Cristi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Epstein:Boris.html">Boris Epstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soto:Jos=eacute=.html">José Soto</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06516">PDF</a><br/><b>Abstract: </b>Two fundamental models in online decision making are that of competitive
analysis and that of optimal stopping. In the former the input is produced by
an adversary, while in the latter the algorithm has full distributional
knowledge of the input. In recent years, there has been a lot of interest in
bridging these two models by considering data-driven or sample-based versions
of optimal stopping problems. In this paper, we study such a version of the
classic single selection optimal stopping problem, as introduced by Kaplan et
al. [2020]. In this problem a collection of arbitrary non-negative numbers is
shuffled in uniform random order. A decision maker gets to observe a fraction
$p\in [0,1)$ of the numbers and the remaining are revealed sequentially. Upon
seeing a number, she must decide whether to take that number and stop the
sequence, or to drop it and continue with the next number. Her goal is to
maximize the expected value with which she stops.
</p>
<p>On one end of the spectrum, when $p=0$, the problem is essentially equivalent
to the secretary problem and thus the optimal algorithm guarantees a reward
within a factor $1/e$ of the expected maximum value. We develop an approach,
based on the continuous limit of a factor revealing LP, that allows us to
obtain the best possible rank-based (ordinal) algorithm for any value of $p$.
Notably, we prove that as $p$ approaches 1, our guarantee approaches 0.745,
matching the best possible guarantee for the i.i.d. prophet inequality. This
implies that there is no loss by considering this more general combinatorial
version without full distributional knowledge. Furthermore, we prove that this
convergence is very fast. Along the way we show that the optimal rank-based
algorithm takes the form of a sequence of thresholds $t_1,t_2,\ldots$ such that
at time $t_i$ the algorithm starts accepting values which are among the top $i$
values seen so far.
</p></div>
    </summary>
    <updated>2020-11-13T22:44:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06481</id>
    <link href="http://arxiv.org/abs/2011.06481" rel="alternate" type="text/html"/>
    <title>Communication Efficient Coresets for Maximum Matching</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kapralov:Michael.html">Michael Kapralov</a>, Gilbert Maystre, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tardos:Jakab.html">Jakab Tardos</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06481">PDF</a><br/><b>Abstract: </b>In this paper we revisit the problem of constructing randomized composable
coresets for bipartite matching. In this problem the input graph is randomly
partitioned across $k$ players, each of which sends a single message to a
coordinator, who then must output a good approximation to the maximum matching
in the input graph. Assadi and Khanna gave the first such coreset, achieving a
$1/9$-approximation by having every player send a maximum matching, i.e. at
most $n/2$ words per player. The approximation factor was improved to $1/3$ by
Bernstein et al.
</p>
<p>In this paper, we show that the matching skeleton construction of Goel,
Kapralov and Khanna, which is a carefully chosen (fractional) matching, is a
randomized composable coreset that achieves a $1/2-o(1)$ approximation using at
most $n-1$ words of communication per player. We also show an upper bound of
$2/3+o(1)$ on the approximation ratio achieved by this coreset.
</p></div>
    </summary>
    <updated>2020-11-13T22:46:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06475</id>
    <link href="http://arxiv.org/abs/2011.06475" rel="alternate" type="text/html"/>
    <title>Quantum algorithms for spectral sums</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Luongo:Alessandro.html">Alessandro Luongo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shao:Changpeng.html">Changpeng Shao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06475">PDF</a><br/><b>Abstract: </b>We propose and analyze new quantum algorithms for estimating the most common
spectral sums of symmetric positive definite (SPD) matrices. For a function $f$
and a matrix $A \in \mathbb{R}^{n\times n}$, the spectral sum is defined as
$S_f(A) :=\text{Tr}[f(A)] = \sum_j f(\lambda_j)$, where $\lambda_j$ are the
eigenvalues. Examples of spectral sums are the von Neumann entropy, the trace
of inverse, the log-determinant, and the Schatten-$p$ norm, where the latter
does not require the matrix to be SPD. The fastest classical randomized
algorithms estimate these quantities have a runtime that depends at least
linearly on the number of nonzero components of the matrix. Assuming quantum
access to the matrix, our algorithms are sub-linear in the matrix size, and
depend at most quadratically on other quantities, like the condition number and
the approximation error, and thus can compete with most of the randomized and
distributed classical algorithms proposed in recent literature. These
algorithms can be used as subroutines for solving many practical problems, for
which the estimation of a spectral sum often represents a computational
bottleneck.
</p></div>
    </summary>
    <updated>2020-11-13T23:06:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06268</id>
    <link href="http://arxiv.org/abs/2011.06268" rel="alternate" type="text/html"/>
    <title>FPT-Algorithms for the l-Matchoid Problem with Linear and Submodular Objectives</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Chien=Chung.html">Chien-Chung Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Ward:Justin.html">Justin Ward</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06268">PDF</a><br/><b>Abstract: </b>We design a fixed-parameter deterministic algorithm for computing a maximum
weight feasible set under a $\ell$-matchoid of rank $k$, parameterized by
$\ell$ and $k$. Unlike previous work that presumes linear representativity of
matroids, we consider the general oracle model. Our result, combined with the
lower bounds of Lovasz, and Jensen and Korte, demonstrates a separation between
the $\ell$-matchoid and the matroid $\ell$-parity problems in the setting of
fixed-parameter tractability.
</p>
<p>Our algorithms are obtained by means of kernelization: we construct a small
representative set which contains an optimal solution. Such a set gives us much
flexibility in adapting to other settings, allowing us to optimize not only a
linear function, but also several important submodular functions. It also helps
to transform our algorithms into streaming algorithms.
</p>
<p>In the streaming setting, we show that we can find a feasible solution of
value $z$ and the number of elements to be stored in memory depends only on $z$
and $\ell$ but totally independent of $n$. This shows that it is possible to
circumvent the recent space lower bound of Feldman et al., by parameterizing
the solution value. This result, combined with existing lower bounds, also
provides a new separation between the space and time complexity of maximizing
an arbitrary submodular function and a coverage function in the value oracle
model.
</p></div>
    </summary>
    <updated>2020-11-13T22:45:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06250</id>
    <link href="http://arxiv.org/abs/2011.06250" rel="alternate" type="text/html"/>
    <title>Online Virtual Machine Allocation with Predictions</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Buchbinder:Niv.html">Niv Buchbinder</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fairstein:Yaron.html">Yaron Fairstein</a>, Konstantina Mellou, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Menache:Ishai.html">Ishai Menache</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Naor:Joseph.html">Joseph Naor</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06250">PDF</a><br/><b>Abstract: </b>The cloud computing industry has grown rapidly over the last decade, and with
this growth there is a significant increase in demand for compute resources.
Demand is manifested in the form of Virtual Machine (VM) requests, which need
to be assigned to physical machines in a way that minimizes resource
fragmentation and efficiently utilizes the available machines. This problem can
be modeled as a dynamic version of the bin packing problem with the objective
of minimizing the total usage time of the bins (physical machines). Earlier
works on dynamic bin packing assumed that no knowledge is available to the
scheduler and later works studied models in which lifetime/duration of each
"item" (VM in our context) is available to the scheduler. This extra
information was shown to improve exponentially the achievable competitive
ratio.
</p>
<p>Motivated by advances in Machine Learning that provide good estimates of
workload characteristics, this paper studies the effect of having extra
information regarding future (total) demand. In the cloud context, since demand
is an aggregate over many VM requests, it can be predicted with high accuracy
(e.g., using historical data). We show that the competitive factor can be
dramatically improved by using this additional information; in some cases, we
achieve constant competitiveness, or even a competitive factor that approaches
1. Along the way, we design new offline algorithms with improved approximation
ratios for the dynamic bin-packing problem.
</p></div>
    </summary>
    <updated>2020-11-13T23:03:12Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06204</id>
    <link href="http://arxiv.org/abs/2011.06204" rel="alternate" type="text/html"/>
    <title>Approximating the Weighted Minimum Label $s$-$t$ Cut Problem</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Peng.html">Peng Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06204">PDF</a><br/><b>Abstract: </b>In the weighted (minimum) {\sf Label $s$-$t$ Cut} problem, we are given a
(directed or undirected) graph $G=(V,E)$, a label set $L = \{\ell_1, \ell_2,
\dots, \ell_q \}$ with positive label weights $\{w_\ell\}$, a source $s \in V$
and a sink $t \in V$. Each edge edge $e$ of $G$ has a label $\ell(e)$ from $L$.
Different edges may have the same label. The problem asks to find a minimum
weight label subset $L'$ such that the removal of all edges with labels in $L'$
disconnects $s$ and $t$.
</p>
<p>The unweighted {\sf Label $s$-$t$ Cut} problem (i.e., every label has a unit
weight) can be approximated within $O(n^{2/3})$, where $n$ is the number of
vertices of graph $G$. However, it is unknown for a long time how to
approximate the weighted {\sf Label $s$-$t$ Cut} problem within $o(n)$. In this
paper, we provide an approximation algorithm for the weighted {\sf Label
$s$-$t$ Cut} problem with ratio $O(n^{2/3})$. The key point of the algorithm is
a mechanism to interpret label weight on an edge as both its length and
capacity.
</p></div>
    </summary>
    <updated>2020-11-13T22:47:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06202</id>
    <link href="http://arxiv.org/abs/2011.06202" rel="alternate" type="text/html"/>
    <title>Optimal Private Median Estimation under Minimal Distributional Assumptions</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tzamos:Christos.html">Christos Tzamos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vlatakis=Gkaragkounis:Emmanouil=Vasileios.html">Emmanouil-Vasileios Vlatakis-Gkaragkounis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zadik:Ilias.html">Ilias Zadik</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06202">PDF</a><br/><b>Abstract: </b>We study the fundamental task of estimating the median of an underlying
distribution from a finite number of samples, under pure differential privacy
constraints. We focus on distributions satisfying the minimal assumption that
they have a positive density at a small neighborhood around the median. In
particular, the distribution is allowed to output unbounded values and is not
required to have finite moments. We compute the exact, up-to-constant terms,
statistical rate of estimation for the median by providing nearly-tight upper
and lower bounds. Furthermore, we design a polynomial-time differentially
private algorithm which provably achieves the optimal performance. At a
technical level, our results leverage a Lipschitz Extension Lemma which allows
us to design and analyze differentially private algorithms solely on
appropriately defined "typical" instances of the samples.
</p></div>
    </summary>
    <updated>2020-11-13T22:46:37Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06173</id>
    <link href="http://arxiv.org/abs/2011.06173" rel="alternate" type="text/html"/>
    <title>Note on 3-Coloring of $(2P_4,C_5)$-Free Graphs</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jel=iacute=nek:V=iacute=t.html">Vít Jelínek</a>, Tereza Klimošová, Tomáš Masařík, Jana Novotná, Aneta Pokorná <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06173">PDF</a><br/><b>Abstract: </b>We show that the 3-coloring problem is polynomial-time solvable on
$(2P_4,C_5)$-free graphs.
</p></div>
    </summary>
    <updated>2020-11-13T22:45:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06166</id>
    <link href="http://arxiv.org/abs/2011.06166" rel="alternate" type="text/html"/>
    <title>Comparing computational entropies below majority (or: When is the dense model theorem false?)</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Impagliazzo:Russell.html">Russell Impagliazzo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McGuire:Sam.html">Sam McGuire</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06166">PDF</a><br/><b>Abstract: </b>Computational pseudorandomness studies the extent to which a random variable
$\bf{Z}$ looks like the uniform distribution according to a class of tests
$\cal{F}$. Computational entropy generalizes computational pseudorandomness by
studying the extent which a random variable looks like a \emph{high entropy}
distribution. There are different formal definitions of computational entropy
with different advantages for different applications. Because of this, it is of
interest to understand when these definitions are equivalent.
</p>
<p>We consider three notions of computational entropy which are known to be
equivalent when the test class $\cal{F}$ is closed under taking majorities.
This equivalence constitutes (essentially) the so-called \emph{dense model
theorem} of Green and Tao (and later made explicit by Tao-Zeigler, Reingold et
al., and Gowers). The dense model theorem plays a key role in Green and Tao's
proof that the primes contain arbitrarily long arithmetic progressions and has
since been connected to a surprisingly wide range of topics in mathematics and
computer science, including cryptography, computational complexity,
combinatorics and machine learning. We show that, in different situations where
$\cal{F}$ is \emph{not} closed under majority, this equivalence fails. This in
turn provides examples where the dense model theorem is \emph{false}.
</p></div>
    </summary>
    <updated>2020-11-13T22:37:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06150</id>
    <link href="http://arxiv.org/abs/2011.06150" rel="alternate" type="text/html"/>
    <title>Total Completion Time Minimization for Scheduling with Incompatibility Cliques</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jansen:Klaus.html">Klaus Jansen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lassota:Alexandra.html">Alexandra Lassota</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maack:Marten.html">Marten Maack</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pikies:Tytus.html">Tytus Pikies</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06150">PDF</a><br/><b>Abstract: </b>This paper considers parallel machine scheduling with incompatibilities
between jobs. The jobs form a graph and no two jobs connected by an edge are
allowed to be assigned to the same machine. In particular, we study the case
where the graph is a collection of disjoint cliques. Scheduling with
incompatibilities between jobs represents a well-established line of research
in scheduling theory and the case of disjoint cliques has received increasing
attention in recent years. While the research up to this point has been focused
on the makespan objective, we broaden the scope and study the classical total
completion time criterion. In the setting without incompatibilities, this
objective is well known to admit polynomial time algorithms even for unrelated
machines via matching techniques. We show that the introduction of
incompatibility cliques results in a richer, more interesting picture.
Scheduling on identical machines remains solvable in polynomial time, while
scheduling on unrelated machines becomes APX-hard. Furthermore, we study the
problem under the paradigm of fixed-parameter tractable algorithms (FPT). In
particular, we consider a problem variant with assignment restrictions for the
cliques rather than the jobs. We prove that it is NP-hard and can be solved in
FPT time with respect to the number of cliques. Moreover, we show that the
problem on unrelated machines can be solved in FPT time for reasonable
parameters, e.g., the parameter pair: number of machines and maximum processing
time. The latter result is a natural extension of known results for the case
without incompatibilities and can even be extended to the case of total
weighted completion time. All of the FPT results make use of n-fold Integer
Programs that recently have received great attention by proving their
usefulness for scheduling problems.
</p></div>
    </summary>
    <updated>2020-11-13T22:39:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06135</id>
    <link href="http://arxiv.org/abs/2011.06135" rel="alternate" type="text/html"/>
    <title>Hardness of Approximate Nearest Neighbor Search under L-infinity</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Ko:Young_Kun.html">Young Kun Ko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Song:Min_Jae.html">Min Jae Song</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06135">PDF</a><br/><b>Abstract: </b>We show conditional hardness of Approximate Nearest Neighbor Search (ANN)
under the $\ell_\infty$ norm with two simple reductions. Our first reduction
shows that hardness of a special case of the Shortest Vector Problem (SVP),
which captures many provably hard instances of SVP, implies a lower bound for
ANN with polynomial preprocessing time under the same norm. Combined with a
recent quantitative hardness result on SVP under $\ell_\infty$ (Bennett et al.,
FOCS 2017), our reduction implies that finding a $(1+\varepsilon)$-approximate
nearest neighbor under $\ell_\infty$ with polynomial preprocessing requires
near-linear query time, unless the Strong Exponential Time Hypothesis (SETH) is
false. This complements the results of Rubinstein (STOC 2018), who showed
hardness of ANN under $\ell_1$, $\ell_2$, and edit distance.
</p>
<p>Further improving the approximation factor for hardness, we show that,
assuming SETH, near-linear query time is required for any approximation factor
less than $3$ under $\ell_\infty$. This shows a conditional separation between
ANN under the $\ell_1/ \ell_2$ norm and the $\ell_\infty$ norm since there are
sublinear time algorithms achieving better than $3$-approximation for the
$\ell_1$ and $\ell_2$ norm. Lastly, we show that the approximation factor of
$3$ is a barrier for any naive gadget reduction from the Orthogonal Vectors
problem.
</p></div>
    </summary>
    <updated>2020-11-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06112</id>
    <link href="http://arxiv.org/abs/2011.06112" rel="alternate" type="text/html"/>
    <title>Tree Embeddings for Hop-Constrained Network Design</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haeupler:Bernhard.html">Bernhard Haeupler</a>, D Ellis Hershkowitz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zuzic:Goran.html">Goran Zuzic</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06112">PDF</a><br/><b>Abstract: </b>Network design problems aim to compute low-cost structures such as routes,
trees and subgraphs. Often, it is natural and desirable to require that these
structures have small hop length or hop diameter. Unfortunately, optimization
problems with hop constraints are much harder and less well understood than
their hop-unconstrained counterparts. A significant algorithmic barrier in this
setting is the fact that hop-constrained distances in graphs are very far from
being a metric.
</p>
<p>We show that, nonetheless, hop-constrained distances can be approximated by
distributions over "partial tree metrics." We build this result into a powerful
and versatile algorithmic tool which, similarly to classic probabilistic tree
embeddings, reduces hop-constrained problems in general graphs to
hop-unconstrained problems on trees. We then use this tool to give the first
poly-logarithmic bicriteria approximations for the hop-constrained variants of
many classic network design problems. These include Steiner forest, group
Steiner tree, group Steiner forest, buy-at-bulk network design as well as
online and oblivious versions of many of these problems.
</p></div>
    </summary>
    <updated>2020-11-13T22:43:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2011.06108</id>
    <link href="http://arxiv.org/abs/2011.06108" rel="alternate" type="text/html"/>
    <title>An Optimal Rounding for Half-Integral Weighted Minimum Strongly Connected Spanning Subgraph</title>
    <feedworld_mtime>1605225600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>D Ellis Hershkowitz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kehne:Gregory.html">Gregory Kehne</a>, R. Ravi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2011.06108">PDF</a><br/><b>Abstract: </b>In the weighted minimum strongly connected spanning subgraph (WMSCSS) problem
we must purchase a minimum-cost strongly connected spanning subgraph of a
digraph. We show that half-integral linear program (LP) solutions for WMSCSS
can be efficiently rounded to integral solutions at a multiplicative $1.5$
cost. This rounding matches a known $1.5$ integrality gap lower bound for a
half-integral instance. More generally, we show that LP solutions whose
non-zero entries are at least a value $f &gt; 0$ can be rounded at a
multiplicative cost of $2 - f$.
</p></div>
    </summary>
    <updated>2020-11-13T23:05:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/11/12/postdoc-position-at-boston-college-apply-by-december-15-2020/</id>
    <link href="https://cstheory-jobs.org/2020/11/12/postdoc-position-at-boston-college-apply-by-december-15-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc Position at Boston College (apply by December 15, 2020)</title>
    <summary>Applications are invited for a postdoc position hosted by Hsin-Hao Su at Boston College. Areas of specific interests include but not limited to distributed graph algorithms, local algorithms, dynamic graph algorithms, gossip algorithms, and massively parallel computation algorithms. Website: https://sites.google.com/site/distributedhsinhao/postdoc Email: suhx@bc.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are invited for a postdoc position hosted by Hsin-Hao Su at Boston College. Areas of specific interests include but not limited to distributed graph algorithms, local algorithms, dynamic graph algorithms, gossip algorithms, and massively parallel computation algorithms.</p>
<p>Website: <a href="https://sites.google.com/site/distributedhsinhao/postdoc">https://sites.google.com/site/distributedhsinhao/postdoc</a><br/>
Email: suhx@bc.edu</p></div>
    </content>
    <updated>2020-11-12T22:58:17Z</updated>
    <published>2020-11-12T22:58:17Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-14T20:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17747</id>
    <link href="https://rjlipton.wordpress.com/2020/11/12/the-art-of-math/" rel="alternate" type="text/html"/>
    <title>The Art of Math</title>
    <summary>Art, history, and controversy Jemma Lorenat is an assistant professor at Pitzer College in Los Angeles. She teaches and does research on the history of mathematics. Today I thought we’d look at some of her work. History of math is one topic that we have focused on many times before. More on that in a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Art, history, and controversy</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p>
Jemma Lorenat is an assistant professor at <a href="https://www.pitzer.edu">Pitzer College</a> in Los Angeles. She teaches and does research on the history of mathematics.<br/>
<a href="https://rjlipton.wordpress.com/2020/11/12/the-art-of-math/jemma/" rel="attachment wp-att-17751"><img alt="" class="alignright size-full wp-image-17751" src="https://rjlipton.files.wordpress.com/2020/10/jemma.png?w=600"/></a></p>
<p>
Today I thought we’d look at some of her work. </p>
<p>
History of math is one topic that we have focused on many times before. More on that in a moment. </p>
<p>
</p><p/><h2> Her Art </h2><p/>
<p/><p>
But before we do that I wish to present Lorenat’s art work. My late father, Jack Lipton, was an artist and so perhaps I have a genetic interest in art. Lorenat is an artist besides being a mathematician. You can see some of her drawings of famous mathematicians <a href="https://www.maa.org/press/periodicals/convergence/portrait-gallery">here</a>. Her elegant style I find appealing. See if you like it as much as I do. My dad taught me: </p>
<blockquote><p><b> </b> <em> A clean drawing is more difficult to execute than a busy one. It is hard to hide flaws when your art is clean. </em>
</p></blockquote>
<p>Her drawings are clean indeed. </p>
<p>
Here are three of Lorenat’s drawings of the following three famous mathematicians in some order: Which is which? Prizes will not be given to those with correct answers.</p>
<ol>
<li>
Eric Temple Bell <p/>
</li><li>
Jacques Hadamard <p/>
</li><li>
Henri Lebesgue
</li></ol>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/11/12/the-art-of-math/group-2/" rel="attachment wp-att-17753"><img alt="" class="aligncenter size-full wp-image-17753" height="274" src="https://rjlipton.files.wordpress.com/2020/10/group.png?w=600&amp;h=274" width="600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
</p><p/><h2> Her Research </h2><p/>
<p/><p>
Lorenat’s research is on the history of mathematics. My first choice is to create math, but I am intrigued by the history of who did what, when, and why. We must understand history—at least in broad strokes—if we are to continue to make progress. History helps us understand how progress was made and how it was not. History teaches us much about our field, about mathematics. </p>
<p>
<a href="http://archives.math.utk.edu/topics/history.html">Several</a> online <a href="https://www.math.tamu.edu/~dallen/masters/hist_frame.htm">sources</a> show the field’s breadth and scope. Among issues and <a href="https://en.wikipedia.org/wiki/List_of_mathematics_history_topics">topics</a>, we note:</p>
<ul>
<li>
Who is a result named for? <p/>
</li><li>
Who gets the credit for a result? <p/>
</li><li>
What is the strongest result known? <p/>
</li><li>
Is this result correct?
</li></ul>
<p>
It is fun to see the process in action. One example I have been involved in for a long time is the study of vector addition systems and reachability problems. There continues to be exciting news, for instance, a <a href="https://arxiv.org/pdf/1809.07115.pdf">paper</a> last year showing that a central reachability problem is vastly harder than had been conjectured. I will discuss, however, an issue from two centuries ago that Lorenat has illuminated.</p>
<p>
</p><p/><h2> The Duality Controversy </h2><p/>
<p/><p>
Lorenat has a <a href="https://rjlipton.files.wordpress.com/2020/11/78167-culturalcontextconference2015.pdf">talk</a> on the geometric theory of duality. It was a prime example of a controversy in the discover of a basic math idea. Here duality means: Given a statement from projective geometry we can flip points and lines and still leave its correctness invariant. This is the duality: 	</p>
<p align="center"><img alt="\displaystyle  points \iff lines. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++points+%5Ciff+lines.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  points \iff lines. "/></p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/11/12/the-art-of-math/dual/" rel="attachment wp-att-17754"><img alt="" class="aligncenter size-full wp-image-17754" src="https://rjlipton.files.wordpress.com/2020/10/dual.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
In complexity theory we have our own duality. Instead of flipping points and lines we can exchange boolean operations 	</p>
<p align="center"><img alt="\displaystyle  AND \iff OR, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++AND+%5Ciff+OR%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  AND \iff OR, "/></p>
<p>and also exchange 	</p>
<p align="center"><img alt="\displaystyle  0 \iff 1. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0+%5Ciff+1.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  0 \iff 1. "/></p>
<p>	 Thus 	</p>
<p align="center"><img alt="\displaystyle  (x \wedge y) \vee (x \wedge z) \iff (x \vee y) \wedge (x \vee z). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x+%5Cwedge+y%29+%5Cvee+%28x+%5Cwedge+z%29+%5Ciff+%28x+%5Cvee+y%29+%5Cwedge+%28x+%5Cvee+z%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  (x \wedge y) \vee (x \wedge z) \iff (x \vee y) \wedge (x \vee z). "/></p>
<p>		 Lorenat’s talk highlights a controversy between: Joseph Diaz Gergonne, Jean-Victor Poncelet, and Julius Plucker. Her work is <a href="https://www.cambridge.org/core/journals/science-in-context/article/polemics-in-public-poncelet-gergonne-plucker-and-the-duality-controversy/C305E6B9326F2AC4C8AC31D4194FEDC6">here</a>. </p>
<blockquote><p><b> </b> <em> A plagiarism charge in 1827 sparked a public controversy centered between Jean-Victor Poncelet (1788-1867) and Joseph-Diez Gergonne (1771-1859) over the origin and applications of the principle of duality in geometry. Over the next three years and through the pages of various journals, monographs, letters, reviews, reports, and footnotes, vitriol between the antagonists increased as their potential publicity grew. While the historical literature offers valuable resources toward understanding the development, content, and applications of geometric duality, the hostile nature of the exchange seems to have deterred an in-depth textual study of the explicitly polemical writings. We argue that the necessary collective endeavor of beginning and ending this controversy constitutes a case study in the circulation of geometry. In particular, we consider how the duality controversy functioned as a medium of communicating new fundamental principles to a wider audience of practitioners. </em>
</p></blockquote>
<p/><p>
A further comment is <a href="https://en.wikipedia.org/wiki/Duality_(projective_geometry)">here</a>:</p>
<blockquote><p><b> </b> <em> Of this feud, Pierre Samuel has quipped that since both men were in the French army and Poncelet was a general while Gergonne a mere captain, Poncelet’s view prevailed, at least among their French contemporaries. </em>
</p></blockquote>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Did you see which drawing was which? </p>
<p><a href="https://rjlipton.wordpress.com/2020/11/12/the-art-of-math/answer-3/" rel="attachment wp-att-17763"><img alt="" class="aligncenter size-full wp-image-17763" src="https://rjlipton.files.wordpress.com/2020/10/answer-1.png?w=600"/></a></p>
<p/></font></font></div>
    </content>
    <updated>2020-11-12T19:14:31Z</updated>
    <published>2020-11-12T19:14:31Z</published>
    <category term="History"/>
    <category term="People"/>
    <category term="Results"/>
    <category term="controversy"/>
    <category term="duality"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-11-14T20:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5094</id>
    <link href="https://www.scottaaronson.com/blog/?p=5094" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5094#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5094" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Complexity Zoo needs a new home</title>
    <summary xml:lang="en-US">Since I’m now feeling better that the first authoritarian coup attempt in US history will probably sort itself out OK, here’s a real problem: Nearly a score years ago, I created the Complexity Zoo, a procrastination project turned encyclopedia of complexity classes. Nearly half a score years ago, the Zoo moved to my former employer, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Since I’m now feeling better that the first authoritarian coup attempt in US history will probably sort itself out OK, here’s a <em>real</em> problem:</p>



<p>Nearly a score years ago, I created the <a href="https://complexityzoo.uwaterloo.ca/Complexity_Zoo">Complexity Zoo</a>, a procrastination project turned encyclopedia of complexity classes.  Nearly half a score years ago, the Zoo moved to my former employer, the <a href="https://uwaterloo.ca/institute-for-quantum-computing/">Institute for Quantum Computing</a> in Waterloo, Canada, which graciously hosted it ever since.  Alas, IQC has decided that it can no longer do so.  The reason is new regulations in Ontario about the accessibility of websites, which the Zoo might be out of compliance with.  My students and I were willing to look into what was needed—like, does the <a href="https://complexityzoo.uwaterloo.ca/Complexity_Zoo:P#ph">polynomial hierarchy</a> need ramps between its levels or something?  The best would be if we heard from actual blind or other disabled complexity enthusiasts about how we could improve their experience, rather than trying to parse bureaucratese from the Ontario government.  But IQC informed us that in any case, they can’t deal with the potential liability and their decision is final.  I thank them for hosting the Zoo for eight years.</p>



<p>Now I’m looking for a volunteer for a new host.  The Zoo runs on the <a href="https://www.mediawiki.org/wiki/MediaWiki">MediaWiki</a> platform, which doesn’t work with my own hosting provider (Bluehost) but is apparently easy to set up if you, unlike me, are the kind of person who can do such things.  The IQC folks kindly offered to help with the transfer; I and my students can help as well.  It’s a small site with modest traffic.  The main things I need are just assurances that you can host the site for a long time (“forever” or thereabouts), and that you or someone else in your organization will be reachable if the site goes down or if there are other problems.  I own the complexityzoo.com domain and can redirect from there.</p>



<p>In return, you’ll get the immense prestige of hosting such a storied resource for theoretical computer science … plus free publicity for your cause or organization on <em>Shtetl-Optimized</em>, and the eternal gratitude of thousands of my readers.</p>



<p>Of course, if you’re <em>into</em> complexity theory, and you want to update or improve the Zoo while you’re at it, then so much the awesomer!  It could use some updates, badly.  But you don’t even <em>need</em> to know P from NP.</p>



<p>If you’re interested, leave a comment or shoot me an email.  Thanks!!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Unrelated Announcement:</span></strong> I’ll once again be doing an Ask-Me-Anything session at the <a href="https://q2b.qcware.com/">Q2B (“Quantum to Business”)</a> conference, December 8-10.  Other speakers include Umesh Vazirani, John Preskill, Jennifer Ouellette, Eric Schmidt, and many others.  Since the conference will of course be virtual this year, registration is a lot cheaper than in previous years.  Check it out!  (Full disclosure: Q2B is organized by <a href="https://qcware.com/">QC Ware, Inc.</a>, for which I’m a scientific advisor.)</p></div>
    </content>
    <updated>2020-11-12T19:10:44Z</updated>
    <published>2020-11-12T19:10:44Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Self-Referential"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-11-12T21:29:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/11/12/tenured-tenured-track-faculty-positions-at-cispa-saarbrucken-apply-by-november-30-2020/</id>
    <link href="https://cstheory-jobs.org/2020/11/12/tenured-tenured-track-faculty-positions-at-cispa-saarbrucken-apply-by-november-30-2020/" rel="alternate" type="text/html"/>
    <title>Tenured/tenured track faculty positions at CISPA, Saarbrücken (apply by November 30, 2020)</title>
    <summary>CISPA Helmholtz Center for Information Security (Saarbrücken, Germany) is inviting applications for tenured/tenure track faculty positions. The positions come with generous support including research staff positions. We have 4 separate calls: – Efficient Algorithms &amp; TCS Foundations – ML &amp; Data Science – Information Security &amp; Privacy – Software Engineering, Program Analysis &amp; Formal Methods. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>CISPA Helmholtz Center for Information Security (Saarbrücken, Germany) is inviting applications for tenured/tenure track faculty positions. The positions come with generous support including research staff positions. We have 4 separate calls:</p>
<p>– Efficient Algorithms &amp; TCS Foundations<br/>
– ML &amp; Data Science<br/>
– Information Security &amp; Privacy<br/>
– Software Engineering, Program Analysis &amp; Formal Methods.</p>
<p>Website: <a href="https://jobs.cispa.saarland/de_DE/jobs/detail/tenure-track-faculty-positions-in-all-areas-related-to-efficient-algorithms-and-the-foundations-of-theoretical-computer-science-f-m-d-69">https://jobs.cispa.saarland/de_DE/jobs/detail/tenure-track-faculty-positions-in-all-areas-related-to-efficient-algorithms-and-the-foundations-of-theoretical-computer-science-f-m-d-69</a><br/>
Email: marx@cispa.de</p></div>
    </content>
    <updated>2020-11-12T16:35:34Z</updated>
    <published>2020-11-12T16:35:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-14T20:20:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7166344296310393734</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7166344296310393734/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/11/recovery.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7166344296310393734" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7166344296310393734" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/11/recovery.html" rel="alternate" type="text/html"/>
    <title>Recovery</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Lance</b>: Perhaps we should do a post-election vidcast--what does it mean for complexity!</p><p><b>Bill</b>: Not sure if you are serious- but I doubt a Biden presidency will either speed up or slow down the proof that P NE NP or anything else in complexity. Did Trump give LESS money to the NSF and other funding agencies, and will Biden give more? I doubt it. </p><p><b>Lance: </b>Trump's budget did call for a massive cut for the NSF but luckily he doesn't control the purse strings. It's the immigration policy that worried me--both that it keeps good students from coming to the US and that it cuts off a revenue source that will cause many good schools to close down.</p><p><b>Bill: </b>Excellent point- but sounds more like a post you could write since you... know stuff, as opposed to a vidcast with me who... doesn't know stuff. </p><hr/><p>Bill sells himself short and me long but here is my post.</p><p>After Trump was elected in 2016 I thought maybe Trump with all his bluster will just be a typical conservative politician that we can live through until the next election. That didn't last long with his travel ban for Iranians just a few weeks after his inauguration <a href="https://blog.computationalcomplexity.org/2017/02/we-are-all-iranians.html">when I said</a> "This is not the America I believe in."</p><p>Judges have stopped the worst of Trump's travel bans but he has continued to whittle down the number of available visas, made it harder to get visas and get visas after they graduate. New students can't come to America if they take only on-line courses at a time many universities are online. Now students will have to get their visas renewed after two or four years. Few get a PhD in four years and would students come here and take the risk that their visas won't get renewed before they can finish? The <a href="https://cra.org/govaffairs/blog/2020/10/three-new-immigration-rule-changes/">CRA has some details</a> on the newest rules. Not to mention Trump's handling of COVID--would you send your kid to America now?</p><p>Trump hasn't hidden his disdain for higher education and had he won a second term his policies would greatly diminish the country's academic strength. Biden can and certainly will undo much of these changes and I hope it is not too late. </p><p>More from <a href="https://www.chronicle.com/article/bidens-victory-has-elated-international-students-but-the-road-to-lasting-reform-is-long">The Chronicle</a>.</p><p/></div>
    </content>
    <updated>2020-11-12T15:07:00Z</updated>
    <published>2020-11-12T15:07:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-11-14T20:19:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7892</id>
    <link href="https://windowsontheory.org/2020/11/12/mops-and-junior-senior-meeting-disc-2020/" rel="alternate" type="text/html"/>
    <title>MoPS and Junior-Senior Meeting (DISC 2020)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">(Guest post by Shir Cohen and Mouna Safir) The 34th International Symposium on Distributed Computing (DISC 2020) was held on October 12-16, 2020, as a virtual conference. As such, the opportunity for community members to get to know each other in an informal environment was lacking. To address this need, we arranged two types of … <a class="more-link" href="https://windowsontheory.org/2020/11/12/mops-and-junior-senior-meeting-disc-2020/">Continue reading <span class="screen-reader-text">MoPS and Junior-Senior Meeting (DISC 2020)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><br/>(Guest post by Shir Cohen and Mouna Safir)</p>



<p><br/>The 34th International Symposium on Distributed Computing (DISC 2020) was held on October 12-16, 2020, as a virtual conference. As such, the opportunity for community members to get to know each other in an informal environment was lacking. To address this need, we arranged two types of virtual networking events. We hope that these events planted the seeds for many future collaborations and that there will be an opportunity for those involved to meet in person next time.</p>



<p><strong>MoPS (Meet other Postdoc and Students) Sessions<br/></strong>Webpage: <a href="https://sites.google.com/view/disc2020mops/home" rel="noreferrer noopener" target="_blank">https://sites.google.com/view/disc2020mops/home</a></p>



<p>To allow junior members of the community to get to know one another, we arranged MoPS sessions, which we have not seen done before. There were more than 50 participants who took part in the sessions, with representation from a host of countries throughout the world. Sessions were held in 10-time slots before, during, and after DISC. In each session, there would typically be 5 members representing a mixture of Bachelor’s students, Masters students, PhDs, postdocs, and others. Care was taken to include at least one postdoc or Ph.D. in each session so that Bachelors and Masters students might benefit from their experience. Groups were formed with the goal of allowing participants from different countries and institutions to share their experiences and research journeys with one another. Based on the feedback for this event, it would appear that that goal was met and the participants came away with more of a sense of community.</p>



<p><strong>Junior-Senior Meetings<br/></strong>Webpage: <a href="https://sites.google.com/view/disc2020junior-seniormeeting/home" rel="noreferrer noopener" target="_blank">https://sites.google.com/view/disc2020junior-seniormeeting/home</a></p>



<p>The Junior-Senior meetings were organized to provide an opportunity for junior researchers to meet with senior researchers. Fourteen sessions were conducted, where each one brought together one senior and 3-5 juniors. In these sessions,  the juniors got a chance to interact with seniors in the field and profit from their experience. Discussions covered a variety of topics such as how to approach research, how to deal with the job market, or perhaps more personal concerns like work-life balance. We collected amazing feedback from the participants, who claimed that this was a fruitful and interesting experience.</p></div>
    </content>
    <updated>2020-11-12T13:53:04Z</updated>
    <published>2020-11-12T13:53:04Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-14T20:21:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/171</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/171" rel="alternate" type="text/html"/>
    <title>TR20-171 |  Comparing computational entropies below majority (or: When is the dense model theorem false?) | 

	Sam McGuire, 

	Russell Impagliazzo</title>
    <summary>Computational pseudorandomness studies the extent to which a random variable $\bf{Z}$ looks like the uniform distribution according to a class of tests $\cal{F}$. Computational entropy generalizes computational pseudorandomness by studying the extent which a random variable looks like a \emph{high entropy} distribution. There are different formal definitions of computational entropy with different advantages for different applications. Because of this, it is of interest to understand when these definitions are equivalent.
  We consider three notions of computational entropy which are known to be equivalent when the test class $\cal{F}$ is closed under taking majorities. This equivalence constitutes (essentially) the so-called \emph{dense model theorem} of Green and Tao (and later made explicit by Tao-Zeigler, Reingold et al., and Gowers). The dense model theorem plays a key role in Green and Tao's proof that the primes contain arbitrarily long arithmetic progressions and has since been connected to a surprisingly wide range of topics in mathematics and computer science, including cryptography, computational complexity, combinatorics and machine learning. We show that, in different situations where $\cal{F}$ is \emph{not} closed under majority, this equivalence fails. This in turn provides examples where the dense model theorem is \emph{false}.</summary>
    <updated>2020-11-12T02:49:07Z</updated>
    <published>2020-11-12T02:49:07Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-11-14T20:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=75</id>
    <link href="https://dstheory.wordpress.com/2020/11/11/friday-nov-20-himanshu-tyagi-for-the-indian-institute-of-science-iisc/" rel="alternate" type="text/html"/>
    <title>Friday Nov 20 — Himanshu Tyagi from the Indian Institute of Science (IISc)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Friday, Nov 20th at 10:00 AM Pacific Time (13:00 Eastern Time, 18:00 Central European Time, 17:00 UTC, 23:30 Indian Time).  Himanshu Tyagi from IISc will speak about “General lower bounds for estimation under information constraints”. Abstract:  We present very general lower bounds for parametric<a class="more-link" href="https://dstheory.wordpress.com/2020/11/11/friday-nov-20-himanshu-tyagi-for-the-indian-institute-of-science-iisc/">Continue reading <span class="screen-reader-text">"Friday Nov 20 — Himanshu Tyagi from the Indian Institute of Science (IISc)"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next Foundations of Data Science virtual talk will take place on Friday, Nov 20th at 10:00 AM Pacific Time (13:00 Eastern Time, 18:00 Central European Time, 17:00 UTC, 23:30 Indian Time).  <strong>Himanshu Tyagi </strong>from IISc will speak about “<strong>General lower bounds for estimation under information constraints</strong>”.</p>



<p><strong>Abstract</strong>:  We present very general lower bounds for parametric estimation when only limited information per sample is allowed. These limitations can arise, for example, in form of communication constraints, privacy constraints, or linear measurements. Our lower bounds hold for discrete distributions with large alphabet as well as continuous distributions with high-dimensional parameters, apply for any information constraint, and are valid for any $\ell_p$ loss function. Our bounds recover both strong data processing inequality based bounds and Cramér-Rao based bound as special cases.</p>



<p>This talk is based on joint work with Jayadev Acharya and Clément Canonne.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-11-11T21:47:20Z</updated>
    <published>2020-11-11T21:47:20Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-11-14T20:21:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/11/11/faculty-any-rank-at-penn-state-at-penn-state-apply-by-january-31-2021/</id>
    <link href="https://cstheory-jobs.org/2020/11/11/faculty-any-rank-at-penn-state-at-penn-state-apply-by-january-31-2021/" rel="alternate" type="text/html"/>
    <title>FACULTY (ANY RANK) AT PENN STATE at Penn State (apply by January 31, 2021)</title>
    <summary>Applications are invited for tenure-track positions at all levels across all areas of theoretical computer science. Website: https://apptrkr.com/2036821 Email: ablanca@cse.psu.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are invited for tenure-track positions at all levels across all areas of theoretical computer science.</p>
<p>Website: <a href="https://apptrkr.com/2036821">https://apptrkr.com/2036821</a><br/>
Email: ablanca@cse.psu.edu</p></div>
    </content>
    <updated>2020-11-11T21:17:02Z</updated>
    <published>2020-11-11T21:17:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-14T20:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/170</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/170" rel="alternate" type="text/html"/>
    <title>TR20-170 |  High Dimensional Expanders: Random Walks, Pseudorandomness, and Unique Games | 

	Max Hopkins, 

	Tali Kaufman, 

	Shachar Lovett</title>
    <summary>Higher order random walks (HD-walks) on high dimensional expanders have played a crucial role in a number of recent breakthroughs in theoretical computer science, perhaps most famously in the recent resolution of the Mihail-Vazirani conjecture (Anari et al. STOC 2019), which focuses on HD-walks on one-sided local-spectral expanders. In this work we study the spectral structure of walks on the stronger two-sided variant, which capture wide generalizations of important objects like the Johnson and Grassmann graphs. We prove that the spectra of these walks are tightly concentrated in a small number of strips, each of which corresponds combinatorially to a level in the underlying complex. Moreover, the eigenvalues corresponding to these strips decay exponentially with a measure we term the depth of the walk.
    
    Using this spectral machinery, we characterize the edge-expansion of small sets based upon the interplay of their local combinatorial structure and the global decay of the walk's eigenvalues across strips. Variants of this result for the special cases of the Johnson and Grassmann graphs were recently crucial both for the resolution of the 2-2 Games Conjecture (Khot et al. FOCS 2018), and for efficient algorithms for affine unique games over the Johnson graphs (Bafna et al. Arxiv 2020). For the complete complex, our characterization admits a low-degree Sum of Squares proof. Building on the work of Bafna et al., we provide the first polynomial time algorithm for affine unique games over the Johnson scheme. The soundness and runtime of our algorithm depend upon the number of strips with large eigenvalues, a measure we call High-Dimensional Threshold Rank that calls back to the seminal work of Barak, Raghavendra, and Steurer (FOCS 2011) on unique games and threshold rank.</summary>
    <updated>2020-11-11T17:13:41Z</updated>
    <published>2020-11-11T17:13:41Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-11-14T20:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/169</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/169" rel="alternate" type="text/html"/>
    <title>TR20-169 |  Efficient List-Decoding with Constant Alphabet and List Sizes | 

	Zeyu Guo, 

	Noga Ron-Zewi</title>
    <summary>We present an explicit and efficient algebraic construction of capacity-achieving list decodable codes with both constant alphabet and constant list sizes. More specifically, for any $R \in (0,1)$ and $\epsilon&gt;0$, we give an algebraic construction of an infinite family of error-correcting codes of rate $R$, over an alphabet of size $(1/\epsilon)^{O(1/\epsilon^2)}$, that can be list decoded from a $(1-R-\epsilon)$-fraction of errors with list size at most $\exp(\mathrm{poly}(1/\epsilon))$. Moreover, the codes can be encoded in time $\mathrm{poly}(1/\epsilon, n)$, the output list is contained in a linear subspace of dimension at most $\mathrm{poly}(1/\epsilon)$, and a basis for this subspace can be found in time $\mathrm{poly}(1/\epsilon, n)$. Thus, both encoding and list decoding can be performed in \emph{fully polynomial-time} $\mathrm{poly}(1/\epsilon, n)$, except for pruning the subspace and outputting the final list which takes time $\exp(\mathrm{poly}(1/\epsilon)) \cdot \mathrm{poly}(n)$. In contrast, prior explicit and efficient constructions of capacity-achieving list decodable codes either required a much higher complexity in terms of $1/\epsilon$ (and were additionally much less structured), or had super-constant alphabet or list sizes.

Our codes are quite natural and structured. Specifically, we use algebraic-geometric (AG) codes with evaluation points restricted to a subfield, and with the message space restricted to a (carefully chosen) linear subspace. Our main observation is that the output list of AG codes with subfield evaluation points is contained in an affine shift of the image of a \emph{block-triangular-Toeplitz (BTT) matrix}, and that the list size can potentially be reduced to a constant by restricting the message space to a \emph{BTT evasive subspace}, which is a large subspace that intersects the image of any BTT matrix in a constant number of points. We further show how to explicitly construct such BTT evasive subspaces, based on the explicit subspace designs of Guruswami and Kopparty (\emph{Combinatorica}, 2016), and composition.</summary>
    <updated>2020-11-11T17:07:44Z</updated>
    <published>2020-11-11T17:07:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-11-14T20:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/168</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/168" rel="alternate" type="text/html"/>
    <title>TR20-168 |  Improved List-Decodability of Reed–Solomon Codes via Tree Packings | 

	Zeyu Guo, 

	Ray Li, 

	Itzhak Tamo, 

	Mary Wootters, 

	Chong Shangguan</title>
    <summary>This paper shows that there exist Reed--Solomon (RS) codes, over large finite fields, that are combinatorially list-decodable well beyond the Johnson radius, in fact almost achieving list-decoding capacity. In particular, we show that for any $\epsilon\in (0,1]$ there exist RS codes with rate $\Omega(\frac{\epsilon}{\log(1/\epsilon)+1})$ that are list-decodable from radius of $1-\epsilon$. We generalize this result to obtain a similar result on list-recoverability of RS codes. Along the way we use our techniques to give a new proof of a result of Blackburn on optimal linear perfect hash matrices, and strengthen it to obtain a construction of strongly perfect hash matrices. 

To derive the results in this paper we show a surprising connection of the above problems to graph theory, and in particular to the tree packing theorem of Nash-Williams and Tutte. En route to our results on RS codes, we prove a generalization of the tree packing theorem to hypergraphs (and we conjecture that an even stronger generalization holds). We hope that this generalization to hypergraphs will be of independent interest.</summary>
    <updated>2020-11-11T17:06:16Z</updated>
    <published>2020-11-11T17:06:16Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-11-14T20:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://mycqstate.wordpress.com/?p=1262</id>
    <link href="https://mycqstate.wordpress.com/2020/11/11/lecture-notes-on-the-mahadev-verification-protocol/" rel="alternate" type="text/html"/>
    <title>Lecture notes on the Mahadev verification protocol</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">As announced earlier I am currently teaching a course on “interactive proofs with quantum devices” in Paris. The course is proceeding apace, even though the recent lockdown order in France means that we had to abandon our beautiful auditorium at … <a href="https://mycqstate.wordpress.com/2020/11/11/lecture-notes-on-the-mahadev-verification-protocol/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-image"><figure class="alignright size-large is-resized"><a href="https://mycqstate.files.wordpress.com/2020/11/index-1.jpg"><img alt="" class="wp-image-1266" height="206" src="https://mycqstate.files.wordpress.com/2020/11/index-1.jpg?w=1024" width="367"/></a>My street in lockdown </figure></div>



<p>As announced <a href="https://mycqstate.wordpress.com/2020/09/20/announcing-a-short-course-in-paris/">earlier</a> I am currently teaching a course on “interactive proofs with quantum devices” in Paris. The course is proceeding apace, even though the recent lockdown order in France means that we had to abandon our beautiful auditorium at the Institut Henri Poincaré and retreat behind the fanciful Zoom backgrounds whose pretension is a sad reminder of what our summers used to be (Banff, anyone?). A possible upshot is that more may be able to attend the now-online course; if you are interested see the <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/">course page</a> for info. (Things are actually fairly good here–in spite of the apparently strict restrictions on daily outings (max 1h, 1km) you can count on the French to bend things their way; most shops are closed but the streets remain quite busy.)</p>



<p>We just finished a sequence of four lectures on the Mahadev “classical verification of quantum computation” protocol. In the process of preparing the lectures I arrived at a presentation of the protocol that is fairly self-contained, so I decided to compile the associated lecture notes as a stand-alone group of 4 lectures that is available <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/fsmp_verification.pdf">here</a>. The notes are aimed at beginning graduate students with a first course in quantum computing and in complexity desiring to gain a concrete understanding of the inner workings of the result; the notes are a bit lengthy but this in part because they take the time to introduce related concepts and slowly build up to the main result. Overall, my hope is that these should be relatively easily readable and provide a good technical introduction to the Mahadev result on classical verification, including an almost complete analysis of her protocol (there are a few explicitly declared shortcuts that help simplify the presentation without hiding any important aspects). For additional background you can also consult the full <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/fsmp.pdf">10-week notes</a>. Comments on the notes are welcome; I’m afraid they most likely contain numerous typos so if you find any please feel free to correct them directly on the associated <a href="https://www.overleaf.com/9892211158zrwmqnxtvcxm">overleaf document</a>.</p>



<p>The last three lectures of the course will be devoted to the problem of testing under spatial assumptions, building up to an introduction to the proof of MIP* = RE. If all goes well I’ll aim to prepare some stand-alone notes for that part as well.</p></div>
    </content>
    <updated>2020-11-11T15:43:12Z</updated>
    <published>2020-11-11T15:43:12Z</published>
    <category term="Quantum"/>
    <category term="Science"/>
    <category term="teaching"/>
    <category term="Uncategorized"/>
    <category term="lecture notes"/>
    <category term="quantum verification"/>
    <author>
      <name>Thomas</name>
    </author>
    <source>
      <id>https://mycqstate.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://mycqstate.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://mycqstate.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://mycqstate.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://mycqstate.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>In superposition</subtitle>
      <title>MyCQstate</title>
      <updated>2020-11-14T20:21:36Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2020/11/11/instahide/</id>
    <link href="http://offconvex.github.io/2020/11/11/instahide/" rel="alternate" type="text/html"/>
    <title>How to allow deep learning on your data without revealing the data</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Today’s online world and the emerging internet of things is built around a Faustian bargain:  consumers (and their internet of things) hand over their data, and in return get customization of the world to their needs.  Is this exchange of privacy for convenience inherent? At first sight one sees no way around because, of course, to allow machine learning on our data we have to hand our data over to the training algorithm.</p>

<p>Similar issues arise in settings other than consumer devices. For instance, hospitals may wish to pool together their patient data to train a large deep model. But privacy laws such as HIPAA forbid them from sharing the data itself, so somehow they have to train a deep net on their data without revealing their data. Frameworks such as Federated Learning (<a href="https://arxiv.org/abs/1610.05492">Konečný et al., 2016</a>) have been proposed for this but it is known that sharing gradients in that environment leaks a lot of information about the data (<a href="https://arxiv.org/abs/1906.08935">Zhu et al., 2019</a>).</p>

<p>Methods to achieve some of the above  so could completely change the privacy/utility tradeoffs implicit in today’s organization of the online world.</p>

<p>This blog post discusses the current set of solutions,  how they don’t quite suffice for above questions, and the story of a new solution, <a href="http://arxiv.org/abs/2010.02772">InstaHide</a>, that we proposed, and takeaways from a recent attack on it by Carlini et al.</p>

<h2 id="existing-solutions-in-cryptography">Existing solutions in Cryptography</h2>

<p>Classic solutions in cryptography do allow you to in principle outsource any computation to the cloud without revealing your data. (A modern method is Fully Homomorphic Encryption.) Adapting these ideas to machine learning  presents two major obstacles: (a) (serious issue) huge computational overhead, which essentially rules it out for today’s large scale deep models (b) (less serious issue) need for special setups —e.g., requiring every user to sign up for public-key encryption.</p>

<p>Significant research efforts are being made to try to overcome these obstacles and we won’t survey them here.</p>

<h2 id="differential-privacy-dp">Differential Privacy (DP)</h2>

<p>Differential privacy (<a href="https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf">Dwork et al., 2006</a>, <a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">Dwork&amp;Roth, 2014</a>) involves adding carefully calculated amounts of noise during training. This is a modern and rigorous version of classic    <em>data anonymization</em> techniques whose canonical application is release of noised census data to protect privacy of individuals.</p>

<p>This notion was adapted to machine learning by positing that “privacy” in machine learning refers to trained classifiers not being dependent on data of individuals. In other words, if the classifier is trained on data from N individuals, it’s behavior should be essentially unchanged (statistically speaking) if we omit data from any individual. Note that this is a weak notion of privacy: it does not in any way hide the data from the company.</p>

<p>Many tech companies have adopted differential privacy in deployed systems but the following two caveats are important.</p>

<blockquote>
  <p>(Caveat 1): In deep learning applications, DP’s provable guarantees are very weak.</p>
</blockquote>

<p>Applying DP to deep learning involves noticing that the gradient computation amounts to adding gradients of the loss corresponding to individual data points, and that adding noise to those individual gradients in calculated doses can help make the overall classifier limit its dependence on the individual’s datapoint.</p>

<p>In practice provable bounds require adding so much gradient noise that accuracy of the trained classifier plummets. We do not know of any successful training that achieved accuracy &gt; 75 percent on CIFAR10 (or any that achieved accuracy even 10 percent on ImageNet). Furthermore, achieving this level of accuracy involves <strong>pretraining</strong> the classifier model on a large set of <strong>public</strong> images and then using the private/protected images  only to fine-tune the parameters.</p>

<p>Thus it is no surprise that firms today usually apply DP with very low noise level, which give essentially no guarantees. Which brings us to:</p>

<blockquote>
  <p>(Caveat 2): DP’s guarantees (and even weaker guarantees applying to deployment scenarios) possibly act as a fig leaf that allows firms to not address  the kinds of privacy violations that the person on the street actually worries about.</p>
</blockquote>

<p>DP’s provable guarantee (which as noted, does not hold in deployed systems due to the low noise level used) would only ensure that  a deployed ML software that was trained with data from tens of millions of users will not accidentally reveal private information of any user.</p>

<p>But that threat model would seem remote to the person on the street. The privacy issue they worry about more is that copious amounts of our data are continuously collected/stored/mined/sold, often by entities we do not even know about. While  lax regulation is primarily to blame,  there is also the technical hurdle  that  there  is no <strong>practical way</strong> for consumers to hide their data while at the same time benefiting from customized ML solutions that improve their lives.</p>

<p>Which brings us to the question we started with: <em>Could consumers allow machine learning to be done on their data without revealing their data?</em></p>

<h2 id="a-proposed-solution-instahide">A proposed solution: InstaHide</h2>

<p>InstaHide is a new concept: it hides or “encrypts” images to protect them somewhat,  while still allowing standard deep learning pipelines to be applied on them. The deep model is trained entirely on encrypted images.</p>

<ul>
  <li>
    <p>The training speed and accuracy is only slightly worse than vanilla training: one can achieve a test accuracy of ~ 90 percent on CIFAR10 using encrypted images with a computation overhead $&lt; 5$ percent.</p>
  </li>
  <li>
    <p>When it comes to privacy, like every other form of cryptography, its security is based upon conjectured difficulty of the underlying computational problem.
(But we don’t expect breaking it to be as difficult as say breaking RSA.)</p>
  </li>
</ul>

<h3 id="how-instahide-encryption-works">How InstaHide encryption works</h3>

<p>Here are some details. InstaHide  belongs to the class of subset-sum type encryptions (<a href="https://www.cs.cmu.edu/afs/cs/user/dwoodruf/www/biwx.pdf">Bhattacharyya et al., 2011</a>), and was inspired by a data augmentation technique called Mixup (<a href="https://arxiv.org/abs/1710.09412">Zhang et al., 2018</a>). It views images as vectors of pixel values. With vectors you can take linear combinations. The figure below shows the result of a typical MixUp: adding  0.6 times the bird image  with 0.4 times the airplane image. The image labels can also be treated as one-hot vectors, and they are mixed using the same coefficients in front of the image samples.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/mixup.png" width="60%"/>
</p>

<p>To encrypt the bird image, InstaHide does mixup (i.e., combination with nonnegative coefficients) with one other randomly chosen training image, and with two  other images chosen randomly from a large public dataset like imagenet.  The coefficients 0.6., 0.4 etc. in the figure  are also chosen at random. Then it takes this composite image and for every pixel value, it randomly flips the sign. With that, we get the encrypted images and labels. All random choices made in this encryption act as a one-time key that is never re-used to encrypt other images.</p>

<p>InstaHide has a parameter $k$ denoting how many images are mixed; in the picture, we have $k=4$. The figure below shows this encryption mechanism.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/instahide.png" width="80%"/>
</p>

<p>When plugged into the standard deep learning with a private dataset of $n$ images, in each epoch of training (say $T$ epochs in total), InstaHide will re-encrypt each image in the  dataset using a random one-time key. This will gives $n\times T$ encrypted images in total.</p>

<h3 id="the-security-argument">The security argument</h3>

<p>We conjectured, based upon intuitions from computational complexity of the k-vector-subset-sum problem (citations), that extracting information about the images could time $N^{k-2}$. Here $N$, the size of the public dataset, can be tens or hundreds of millions, so it might be infeasible for real-life attackers.</p>

<p>We also released a <a href="https://github.com/Hazelsuko07/InstaHide_Challenge">challenge dataset</a> with $k=6, n=100, T=50$ to enable further investigation of InstaHide’s security.</p>

<h2 id="carlini-et-als-recent-attack-on-instahide">Carlini et al.’s recent attack on InstaHide</h2>

<p>Recently, Carlini et al. have shared with us a manuscript with a two-step reconstruction attack (<a href="https://arxiv.org/pdf/2011.05315.pdf">Carlini et al., 2020</a>)  against InstaHide.</p>

<p><strong><em>TL;DR: They used 11 hours on Google’s best GPUs to get partial recovery of our 100 challenge encryptions and  120 CPU hours to break the encryption completely. Furthermore, the latter was possible entirely because we used an insecure random number generator, and they used exhaustive search over random seeds.</em></strong></p>

<p>Now the details.</p>

<p>The attack takes $n\times T$ InstaHide-encrypted images as the input, ($n$ is the size of the private dataset, $T$ is the number of training epochs), and returns a reconstruction of the private dataset. It goes as follows.</p>

<ul>
  <li>
    <p>Map $n \times T$ encryptions into $n$ private images, by clustering encryptions of a same private image as a group. This is achieved by firstly building a graph representing pairwise similarity between encrypted images, and then assign each encryption a private image. In their implementation, they train a neural network to annotate pairwise similarity between encryptions.</p>
  </li>
  <li>
    <p>Then, given the encrypted images and the mapping, they solve a nonlinear optimization problem via gradient desent to recover an approximation of the original private dataset.</p>
  </li>
</ul>

<p>Using Google’s powerful GPU, it took them 10 hours to train the neural network for similarity annotation, and about another hour to get an approximation of our challenge set of $100$ images with $k=6, n=100, T=50$. This gave them vaguely correct images, with significant unclear areas and color shift.</p>

<p>They also proposed a different strategy which abuses the vulnerability of NumPy and PyTorch’s random number generator (<em>Aargh; we didn’t use a secure random number generator.</em>) They did  brute force search of $2^{32}$ possible initial random seeds, which allows them to reproduce the randomness during encryption, and thus perform a pixel-perfect reconstruction. As they reported, this attack takes 120 CPU hours (they parallelize across 100 cores to obtain the solution in a little over an hour). We will have this implementation flaw fixed in an updated version.</p>

<h3 id="thoughts-on-this-attack">Thoughts on this attack</h3>

<p>Though the attack is clever and impressive, we feel that the long-term take-away is still unclear for several reasons.</p>

<blockquote>
  <p>Variants of InstaHide seem to evade the attack.</p>
</blockquote>

<p>The challenge set contained 50 encryptions each of 100 images. This corresponds to using encrypted images for 50 epochs. But as done in existing settings that use DP, one can pretrain the deep model using non-private images and then fine-tune it with fewer epochs of the private images. Using a similar pipeline DPSGD (<a href="https://arxiv.org/abs/1607.00133">Abadi et al., 2016</a>), by pretraining a ResNet-18 on CIFAR100 (the public dataset) and finetuning  for $10$ epochs on CIFAR10 (the private dataset)  gives accuracy of 83 percent, still far better than any provable guarantees using DP on this dataset. Carlini et al.\ team conceded that their attack probably would not work in this setting.</p>

<p>Similarly using InstaHide purely at inference time (i.e., using ML, instead of training ML) still should be completely secure since only one encryption of the image is released. The Google attack can’t work here at all.</p>

<blockquote>
  <p>InstaHide was never intended to be a mission-critical encryption like RSA (which by the way also has no provable guarantees).</p>
</blockquote>

<p>InstaHide is designed to give users and the internet of things a <em>light-weight</em> encryption method that allows them to use machine learning without giving eavesdroppers or servers access to their raw data. There is no other cost-effective alternative to InstaHide for this application. If it takes Google’s powerful computers a few hours  to break our challenge set of 100 images, this is not yet a cost-effective attack  in the intended settings.</p>

<p>More important, the challenge dataset corresponded to an ambitious form of security, where the encrypted images themselves are released to the world. The more typical application is a Federated Learning (<a href="https://arxiv.org/abs/1610.05492">Konečný et al., 2016</a>) scenario: the adversary observes shared gradients that are computed using encrypted images (he also has access to the trained model). The attacks in this paper do not currently apply to that scenario. This is also the idea in <a href="https://arxiv.org/abs/2010.06053"><strong>TextHide</strong></a>, an adaptation of InstaHide to text data.</p>

<h2 id="takeways">Takeways</h2>

<p>Users need lightweight encryptions that can be applied in real time to large amounts of data, and yet allow them to take benefit of Machine Learning on the cloud. Methods to do so could completely change the privacy/utility tradeoffs implicitly assumed in today’s tech world.</p>

<p>InstaHide is the only such tool right now, and we now know that it provides moderate security that may be enough for many applications.</p>

<!-- 
### References
[1] [**InstaHide: Instance-hiding Schemes for Private Distributed Learning**](http://arxiv.org/abs/2010.02772), *Yangsibo Huang, Zhao Song, Kai Li, Sanjeev Arora*, ICML 2020

[2] [**mixup: Beyond Empirical Risk Minimization**](https://arxiv.org/abs/1710.09412), *Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz*, ICLR 2018

[3] [**An Attack on InstaHide: Is Private Learning Possible with Instance Encoding?**](https://arxiv.org/pdf/2011.05315.pdf) *Nicholas Carlini, Samuel Deng, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Shuang Song, Abhradeep Thakurta, Florian Tramèr*, arxiv preprint

[4] [**Deep Learning with Differential Privacy**](https://arxiv.org/abs/1607.00133), *Martín Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang*, ACM CCS 2016

[5] [**Federated learning: Strategies for improving communication efficiency**](https://arxiv.org/abs/1610.05492), *Jakub Konečný, H. Brendan McMahan, Felix X. Yu, Peter Richtárik, Ananda Theertha Suresh, Dave Bacon*, NeurIPS Workshop 2016

[6] [**A method for obtaining digital signatures and public-key cryptosystems**](https://people.csail.mit.edu/rivest/Rsapaper.pdf), *R.L. Rivest, A. Shamir, and L. Adleman*, Communications of the ACM 1978

[7] [**Deep leakage from gradients**](https://arxiv.org/abs/1906.08935), *Ligeng Zhu, Zhijian Liu, and Song Han.* Neurips19. --></div>
    </summary>
    <updated>2020-11-11T10:00:00Z</updated>
    <published>2020-11-11T10:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2020-11-13T23:09:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/11/10/hex-books-queues</id>
    <link href="https://11011110.github.io/blog/2020/11/10/hex-books-queues.html" rel="alternate" type="text/html"/>
    <title>Hex, books, and queues</title>
    <summary>My latest preprint is “Stack-number is not bounded by queue-number”, arXiv:2011.04195, with Vida Dujmović, Robert Hickingbotham, Pat Morin, and David R. Wood; Hickingbotham is an undergraduate student of Wood at Monash University. It solves a question implicit in the work of Heath, Leighton and Rosenberg (1992) on whether graphs of bounded queue number have bounded stack number (they don’t), disproves a conjecture of Blankenship and Oporowski (1999) on whether subdividing a graph of unbounded stack number can reduce its stack number to bounded (it can), and answers a question of Bonnet, Geniet, Kim, Thomassé, and Watrigant (to appear at SODA 2021) on whether graphs of unbounded stacknumber can have bounded sparse twin-width (they can). And it does so in part using an old trick from combinatorial game theory from a mathematician who won a Nobel prize for his work in game theory. But what are stack number and queue number? And what could combinatorial games possibly have to do with these sorts of questions in structural graph theory?</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest preprint is “Stack-number is not bounded by queue-number”, <a href="https://arxiv.org/abs/2011.04195">arXiv:2011.04195</a>, with Vida Dujmović, Robert Hickingbotham, Pat Morin, and David R. Wood; Hickingbotham is an undergraduate student of Wood at Monash University. It solves a question implicit in the work of Heath, Leighton and Rosenberg (<a href="https://doi.org/10.1137/0405031">1992</a>) on whether graphs of bounded queue number have bounded stack number (they don’t), disproves <a href="https://en.wikipedia.org/wiki/Blankenship%E2%80%93Oporowski_conjecture">a conjecture of Blankenship and Oporowski</a> (1999) on whether subdividing a graph of unbounded stack number can reduce its stack number to bounded (it can), and answers a question of Bonnet, Geniet, Kim, Thomassé, and Watrigant (<a href="https://arxiv.org/abs/2006.09877">to appear at SODA 2021</a>) on whether graphs of unbounded stacknumber can have bounded sparse twin-width (they can). And it does so in part using an old trick from combinatorial game theory from a mathematician who won a Nobel prize for his work in game theory. But what are stack number and queue number? And what could combinatorial games possibly have to do with these sorts of questions in structural graph theory?</p>

<p>The game in question is <a href="https://en.wikipedia.org/wiki/Hex_(board_game)">Hex</a>, in which two players take turns placing their stones on hexagonal cells connected into a triangular grid and arranged in a rhombus. Each player tries to form a path of stones connecting two sides of the rhombus while simultaneously trying to block the other player from making a path. The rendering below from <a href="https://www.hexwiki.net/">HexWiki</a> shows a Hex board with a game in progress:</p>

<p style="text-align: center;"><img alt="CC-BY-SA POV-Ray image of a Hex game, by Twixter on HexWiki, from https://commons.wikimedia.org/wiki/File:Hexposition02.jpg and https://www.hexwiki.net/index.php/File:Hexposition02.jpg" src="https://11011110.github.io/blog/assets/2020/HexWiki.jpg"/></p>

<p>In tic-tac-toe, a similar game on a square grid, both players can end up blocking each other, producing a drawn game. But as John Nash famously proved in 1952, this can’t happen in Hex: every game ends with one or the other player forming a path. This inability to draw is usually explained as a topological property of disks in the plane, equivalent to the <a href="https://en.wikipedia.org/wiki/Brouwer_fixed-point_theorem">Brouwer fixed-point theorem</a>, although it also has a simple combinatorial proof involving walking along the boundary between the two colors of stones on a filled board, starting from a corner of the rhombus, and observing that the walk can only end at a non-opposite corner. But the lack of draws also implies the following statement about graphs: if you form a large <a href="https://en.wikipedia.org/wiki/Triangular_tiling">triangular grid</a> (the graph of positions and adjacencies of stones in Hex), and label its vertices with two colors (the colors of the stones of the two Hex players), then it must contain a long path: at least long enough to reach from one side of the Hex board to the other.</p>

<p>The same combinatorial proof for the Hex board works for any planar triangulation of a disk whose boundary can be divided into four paths with opposite pairs of paths far apart. Even some outerplanar graphs have the same long-path property. Suppose, for instance, that an outerplanar triangulated disk has a complete binary tree for its dual graph. Then we can form a long path by starting at a vertex of the root triangle and repeatedly walking down the tree, at each step moving to a neighbor of the same color whose triangle is closest in the dual tree. This walk takes a long step (such as the step on the top edge of the red path shown below) only when stepping from a vertex whose neighbors include a long path of the other color (the blue path shown below), and otherwise it takes many steps.</p>

<p style="text-align: center;"><img alt="An outerplanar graph in which every 2-coloring has a long monochromatic path" src="https://11011110.github.io/blog/assets/2020/outerplanar-path-property.svg"/></p>

<p>On the other hand, the square grid or other bipartite graphs don’t have the long path property, because if you color them bipartitely then there are no nontrivial monochromatic paths. Neither do subcubic graphs, because their maximum cuts always partition them into two subsets whose longest path has length at most one. The Hex board graph turns out to be the right choice for our purposes, because as well as the long path property it has bounded degree (unlike the outerplanar example) and a nice regular planar structure making stack and queue layouts easy.</p>

<p>So what are these layouts? Stack layout is another name for <a href="https://en.wikipedia.org/wiki/Book_embedding">book embedding</a>, in which the vertices of a graph are arranged in a line and its edges are placed without crossings into “pages”, half-planes bounded by the line. If you traverse the vertices of the graph in the order of the line, add an edge to its page when you traverse its first endpoint, and remove an edge from the page when you traverse the second endpoint, then the order of additions and removals is last-in-first-out, the same as a stack. The stack number or book thickness is the smallest number of pages you need to construct a layout like this. Below is an <a href="https://11011110.github.io/blog/2015/10/03/why-shallow-minors.html">example I’ve used before</a>, a 3-page book embedding of the complete graph \(K_5\), whose stack number is three:</p>

<p style="text-align: center;"><img alt="Book embedding" src="https://11011110.github.io/blog/assets/2015/3page-K5.svg"/></p>

<p>If you replace the last-in-first-out ordering of additions and removals in a stack by first-in-first-out ordering, you get a queue. So a <a href="https://en.wikipedia.org/wiki/Queue_number">queue layout</a> is just an ordering of the vertices into a line, and a partition of the edges into “pages”, so that the traversal of the vertices by their line order produces a queue ordering of additions and removals of edges within each page. As <a href="https://doi.org/10.1007%2F978-3-642-18469-7_7">Auer et al described at GD 2010</a>, these layouts can also be described topologically, by representing each page as a cylinder with the line going longitudinally along it, and requiring each edge to be placed in such a way that it loops all the way around the cylinder. The queue number is the minimum number of these queues, or cylindrical pages, that you need to organize the graph in this way. It’s also closely related to the compact layout of graphs in 3d.</p>

<p>Below is an example I recently drew for a new Wikipedia article on <a href="https://en.wikipedia.org/wiki/Shuffle-exchange_network">shuffle-exchange networks</a>. These are very nonplanar graphs, but the layout shown can almost be drawn without crossings on two cylinders (one for the edges that bend around the left side of the vertices, another for the edges on the right) or on a single surface with a figure-eight cross-section, formed by gluing two cylinders together on a line. However, this only takes care of the curved edges. If you look closely, there are also short horizontal edges between consecutive vertices, which in a queue layout would need to wrap around another third cylinder. So these graphs have queue number at most three.</p>

<p style="text-align: center;"><img alt="Shuffle-exchange network" src="https://11011110.github.io/blog/assets/2020/Order-4_shuffle-exchange.svg"/></p>

<p>With all that as background, the central example in our new preprint is the <a href="https://en.wikipedia.org/wiki/Cartesian_product_of_graphs">Cartesian product</a> of a triangular grid with a <a href="https://en.wikipedia.org/wiki/Star_(graph_theory)">star</a>. It looks like this, but possibly with a bigger grid or star:</p>

<p style="text-align: center;"><img alt="Cartesian product of a star with a triangular grid" src="https://11011110.github.io/blog/assets/2020/star-times-hex.svg"/></p>

<p>The reason Cartesian products are useful in this context is that they behave nicely for queue layouts but not quite as nicely for stack layouts. If two graphs \(G\) and \(H\) both have queue layouts with a bounded number of queues, and in addition the layout of \(H\) is <em>strict</em>, meaning that within each page, each vertex has at most one earlier and one later neighbor, then we can lay out their product by grouping the vertices of the product into copies of \(G\) and placing each copy separately, with the order of the copies determined by the layout of \(H\). The resulting layout will still have a bounded queue number.  The star has bounded queue number, but its layouts are not strict, because it has high degree at the center of the star. As a planar graph the triangular grid has bounded queue number, and because it also has bounded degree its layouts can be made strict. Therefore, the product graph again has bounded queue number.</p>

<p>For stack layouts, a similar product layout works, but only when \(H\) is bipartite. We can reverse the ordering within the copies of \(G\) coming from one side of the bipartition, and the edges going from one copy to another will still be stack-ordered. But when \(H\) is not bipartite, there is no way to consistently choose which copies of \(G\) to reverse. The long path property of Hex boards, in some sense, provides a quantitative measure for the fact that their graphs are far from being bipartite.</p>

<p>Of course, this only shows that a certain stack layout doesn’t work, not that all layouts fail. Our proof that all stack layouts of our product graph fail proceeds in a sequence of reductions in which we assume we are given a layout, and successively reduce it to more-constrained layouts on smaller graphs until reaching a contradiction. The first reductions use the pigeonhole principle to find products of the grid with a smaller star, in which all leaf copies of the grid have the same layout. Next we use the <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Szekeres_theorem">Erdős–Szekeres theorem</a> to find a product with an even smaller star, in which each copy of the star is consistently ordered by the layout in one of two ways. Finally, we use the result about long paths in Hex to find a product of this small star with a path, in which all copies of the star are consistently ordered in the same way, and prove that this ordering of this graph cannot have a bounded stack number.</p>

<p>Heath, Leighton and Rosenberg introduced both stack number and queue number in their 1992 work, and provided an example for which the stack number is exponentially bigger than the queue number, naturally raising the question whether even bigger separations are possible. Our work settles this question by showing that there is no function (exponential or otherwise) that can be used to bound stack number as a function of queue number. In the opposite direction, Heath, Leighton &amp; Rosenberg conjectured that queue number cannot be made to grow significantly more quickly than stack number, but this remains open.</p>

<p>In 1999, Blankenship and Oporowski observed that subdividing the edges of a graph can lead to better stack layouts: for instance adding a single subdivision point to each edge of a complete graph reduces the stack number from linear to the square root of the number of vertices. They conjectured that this improvement cannot be too extreme: reducing the stack number from non-constant to constant should require a non-constant number of subdivision points. Our example disproves the Blankenship–Oporowski conjecture. The star-triangular grid product graph has non-constant stack number, but because it has bounded queue number its subdivisions with three subdivision points per edge have bounded stack number, according to a result of Dujmović and Wood (<a href="https://dmtcs.episciences.org/346">2005</a>). The same paper of Dujmović and Wood also stated the question of whether stack number can be bounded by a function of queue number more explicitly.</p>

<p>The definition of twin-width is too technical to summarize here; see the <a href="https://arxiv.org/abs/2006.09877">forthcoming SODA paper by Bonnet et al.</a>, which shows among other results that it is bounded for graphs of bounded stack number. Our results show that this is not an equivalence: there exist graphs (including the star-triangular grid products) that have unbounded stack number, but still have bounded sparse twin-width.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105188872486464972">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-11-10T16:25:00Z</updated>
    <published>2020-11-10T16:25:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-11-11T00:34:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20368</id>
    <link href="https://gilkalai.wordpress.com/2020/11/10/benjamini-and-mossel-2000-account-sensitivity-of-voting-schemes-to-mistakes-and-manipulations/" rel="alternate" type="text/html"/>
    <title>Benjamini and Mossel’s 2000 Account: Sensitivity of Voting Schemes to Mistakes and Manipulations</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Here is a popular account by Itai Benjamini and Elchanan Mossel from 2000 written shortly after the 2000 US presidential election. Elchanan and Itai kindly agreed that I will publish it here,  for the first time, 20 years later!  I … <a href="https://gilkalai.wordpress.com/2020/11/10/benjamini-and-mossel-2000-account-sensitivity-of-voting-schemes-to-mistakes-and-manipulations/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="color: #0000ff;"><em><a href="https://gilkalai.files.wordpress.com/2020/11/ibem.png"><img alt="" class="alignnone size-full wp-image-20373" height="332" src="https://gilkalai.files.wordpress.com/2020/11/ibem.png?w=640&amp;h=332" width="640"/></a></em></span></p>
<p><em><span style="color: #0000ff;">Here is a popular account by Itai Benjamini and Elchanan Mossel from 2000 written shortly after the 2000 US presidential election. Elchanan and Itai kindly agreed that I will publish it here,  for the first time, 20 years later!  I left the documents as is so affiliations and email addresses are no longer valid. </span></em></p>
<p><span style="color: #0000ff;">__________________________________________________</span></p>
<p>This is a popular report by Dr. I. Benjamini and Dr. E. Mossel from<br/>
Microsoft Research at Redmond on some recent mathematical<br/>
studies which are relevant to the U.S. presidential elections.</p>
<p>For More information contact:<br/>
Dr. Itai Benjamini, Microsoft Research<br/>
e-mail: itai@microsoft.com<br/>
Tel: 1-425-7057024</p>
<h2>Sensitivity of voting schemes to mistakes and manipulations</h2>
<p>Do the results of the recent election accurately reflect public opinion?<br/>
Or are they the result of some minor local manipulations, and a few random<br/>
mistakes such as voters confusion, or counting errors.</p>
<p>There are many potential schemes for electing a president, each with its<br/>
own features. One important feature of a scheme is the agreement of the<br/>
actual outcome of the election with the outcome of the ideal<br/>
implementation of the election which disregards potential mistakes and<br/>
mischief. If it is probable that the ideal and the actual outcomes differ,<br/>
then frequently the outcome of the election will be in doubt.</p>
<p>In recent years, some mathematical efforts have been devoted to trying to<br/>
understand which procedures are “stable” to these potential perturbations<br/>
and which are “sensitive”. While the motivation for these studies came<br/>
from questions in probability and statistical physics, These mathematical<br/>
studies can shed some light on the current presidential election. In<br/>
particular, a recent paper by Professors Itai Benjamini and Oded Schramm<br/>
from Microsoft research and the Weizmann Institute and Gil Kalai from the<br/>
Hebrew University of Jerusalem, soon to be published in the prestigious<br/>
French journal Publication I.H.E.S., suggests that the “popular vote”<br/>
method is much more stable against mistakes than other voting method.</p>
<p>One example of a decision procedure that we encounter in nature is the<br/>
neuron. The neuron has to make its decisions based on many inputs; each<br/>
represents the strength of electrical currents at the synapses entering<br/>
the neuron. Based on these inputs, the neuron should decide whether to<br/>
fire its axon going out of the neuron. It is quite likely that some small<br/>
perturbations occur at each of the input synapses. It is therefore<br/>
expected that the decision procedure of the neuron will be “stable” to<br/>
these perturbations.</p>
<p>Experimental evidence indicates that neurons may be modeled as the<br/>
following simple procedure. The neuron looks at some weighted sum of its<br/>
inputs and makes a decision according to how large this sum is . If the<br/>
sum is large, then one decision is made , while if it is small another<br/>
decision is made .</p>
<p>The counterpart of this procedure for elections would be counting the<br/>
votes and deciding according to the majority of the votes. Mathematical<br/>
proofs have been given to show that this decision procedure is the most<br/>
stable among all decision procedures.</p>
<p>More complicated neural networks consist of a hierarchy of neurons where<br/>
the outputs of neurons in lower levels of the hierarchy are the inputs to<br/>
neurons in the higher levels. Some of these networks have been proven to<br/>
be much more sensitive to noise than a single neuron.</p>
<p>The counterpart of neural network in the political system may sound<br/>
familiar: The voters are divided into groups (states) and each group<br/>
chooses its candidate based on the majority vote. Then some weighted<br/>
majority of the votes of the states is taken to be the elected president.</p>
<p>The mathematical theorems imply that this system is much more sensitive to<br/>
minor mistakes.</p>
<p>If the overwhelming majority of the population is voting for the same<br/>
candidate then it doesn’t really matter which voting scheme we use. All of<br/>
the natural schemes are “stable”. On the other hand, when the population<br/>
is almost evenly split, different schemes behave differently.</p>
<p>The mathematical reasoning behind the stability of the majority vote goes<br/>
back to Abraham DeMoivre and Pierre-Simon Laplace (18th century). This<br/>
reasoning implies, for example, that in a population of 98,221,798 votes,<br/>
if there is a bias of 222,880 for one candidate, then this candidate will<br/>
be chosen, even if for each voter there is a small chance that his or her<br/>
vote is not counted or counted wrongly (the results of the last election<br/>
as of 11/13/00). As long as mistakes for both sides are equally likely,<br/>
the result will correctly reflect the bias in the public opinion.</p>
<p>Thus, in this presidential election while the gap between the two<br/>
candidates is only 0.2% of the votes it is still large enough so that the<br/>
outcome is immune even if a fairly large percent (10% for example) of the<br/>
votes were counted mistakenly (assuming the mistakes were random and<br/>
independent.)</p>
<p>On the other hand the gap of a 388 votes among the 6,000,000 million votes<br/>
in Florida (about 0.05% of the votes) may well be too small to overcome<br/>
the effect of random mistakes in counting the votes, even if the chance<br/>
for a mistake is fairly small (1%, say). It can well be argued that just<br/>
because of the (unavoidable) mistakes in counting the votes in Florida<br/>
(putting aside all other controversies surrounding the vote there) we will<br/>
never be able to know who got the majority of votes among the voters of<br/>
Florida. To understand why the picture is so different as far as the<br/>
popular vote is concerns in the entire nation and the popular vote in the<br/>
state of Florida we should note that the stability against mistakes<br/>
increases dramatically as the number of voters rise. (And also, of course,<br/>
as the gap between the candidates rise.)</p>
<p>The discrepancy between our ability to call the winner in the popular vote<br/>
and disability to call the winner in the electoral college (which is the<br/>
winner of the election according to the constitution) is not unexpected.</p>
<p>The new results by Benjamini, Schramm and Kalai show that for many models<br/>
majority is the scheme which is least sensitive to noise. In these models<br/>
it is assumed that voters make their decisions independently and the<br/>
mistakes are symmetric and independent for different voters. It is shown<br/>
then that for models resembling the current voting scheme in the U.S.A, if<br/>
the population is almost evenly split, the scheme is much more sensitive<br/>
to noise than the majority scheme. In particular a tiny fraction of<br/>
mistakes is very likely to reverse the ideal outcome of the election.<br/>
Moreover, if the elections are almost balanced, then the results are too<br/>
close to call.</p>
<p>If the outcomes do not show significant bias towards one of the candidates<br/>
then in the “popular-vote” method the probability of random independent<br/>
mistakes which effect one in every A votes has a chance of one in the<br/>
square-root of A to switch the outcome of the elections. In a method like<br/>
the “electoral college” the chance is increased to something like one in<br/>
the fourth root of A.</p>
<p>If the popular vote is significantly tilted towards one of the candidates<br/>
than the effect of mistakes becomes smaller for all voting methods but<br/>
much more so for the popular-vote method.</p>
<p>The study of the sensitivity of voting procedures to small errors for such<br/>
models is based on mathematical tools from “harmonic analysis”, and<br/>
provides a classification of the voting schemes which are very sensitive<br/>
to small amounts of noise and those which are more stable. In particular<br/>
among all symmetric voting procedures (i.e., all voters have the same<br/>
power), the popular majority is the most robust.</p>
<p>For further technical reading and the precise formulation of the<br/>
mathematical theorems see <a href="http://front.math.ucdavis.edu/math.PR/9811157" rel="nofollow">http://front.math.ucdavis.edu/math.PR/9811157</a></p>
<p>A word of warning is in place. The precise expected effect of mistakes<br/>
will depend on the specific statistical model for the voting patterns and<br/>
for the noise created by counting errors and biases. For example, more<br/>
recent work by Claire Kanyon from Orsay university, Elchanan Mossel from<br/>
Microsoft research and Yuval Peres from Berkeley and the Hebrew<br/>
University, gives quite a different picture for different models based on<br/>
the famous “Ising model” from Physics. We expect that the qualitative<br/>
conclusion of the research by Benjamini, Kalai and Schramm applied to the<br/>
case of U.S. presidential elections and that the popular vote method is<br/>
much more stable to noise and biases than the electoral college method.<br/>
For definite conclusions, however, some statistical work on actual voting<br/>
data should be carried out. Choosing the most appropriate voting method<br/>
involve, of course, many, primarily non-mathematical considerations.</p>
<p>Itai Benjamini, Microsoft Research and the Wizmann Institute for Science<br/>
Elchanan Mossel, Microsoft Research</p></div>
    </content>
    <updated>2020-11-10T15:57:54Z</updated>
    <published>2020-11-10T15:57:54Z</published>
    <category term="Combinatorics"/>
    <category term="Games"/>
    <category term="Probability"/>
    <category term="Rationality"/>
    <category term="Elchanan Mossel"/>
    <category term="Itai Benjamini"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-11-14T20:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/167</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/167" rel="alternate" type="text/html"/>
    <title>TR20-167 |  Approximate Hypergraph Vertex Cover and generalized Tuza&amp;#39;s conjecture | 

	Venkatesan Guruswami, 

	Sai Sandeep</title>
    <summary>A famous conjecture of Tuza states that the minimum number of edges needed to cover all the triangles in a graph is at most twice the maximum number of edge-disjoint triangles.  This conjecture was couched in a broader setting by Aharoni and Zerbib who proposed a hypergraph version of this conjecture, and also studied its implied fractional versions. We establish the fractional version of the Aharoni-Zerbib conjecture up to lower order terms. Specifically, we give a factor $t/2+ O(\sqrt{t \log t})$ approximation based on LP rounding for an algorithmic version of the hypergraph Tur\'{a}n problem (AHTP). The objective in AHTP is to pick the smallest collection of $(t-1)$-sized subsets of vertices of an input $t$-uniform hypergraph such that every hyperedge contains one of these subsets.

Aharoni and Zerbib also posed whether Tuza's conjecture and its hypergraph versions could follow from non-trivial duality gaps between vertex covers and matchings on hypergraphs that exclude certain sub-hypergraphs, for instance, a ``tent" structure that cannot occur in the incidence of triangles and edges. We give a strong negative answer to this question, by exhibiting tent-free hypergraphs, and indeed $\mathcal{F}$-free hypergraphs for any finite family $\mathcal{F}$ of excluded sub-hypergraphs, whose vertex covers must include almost all the vertices.


The algorithmic questions arising in the above study can be phrased as instances of vertex cover on \emph{simple} hypergraphs, whose hyperedges can pairwise share at most one vertex. We prove that the trivial factor $t$ approximation for vertex cover is hard to improve for simple $t$-uniform hypergraphs. However, for set cover on simple $n$-vertex hypergraphs, the greedy algorithm achieves a factor $(\ln n)/2$, better than the optimal $\ln n$ factor for general hypergraphs.</summary>
    <updated>2020-11-09T18:20:02Z</updated>
    <published>2020-11-09T18:20:02Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-11-14T20:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5091</id>
    <link href="https://www.scottaaronson.com/blog/?p=5091" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5091#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5091" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">On defeating a sociopath</title>
    <summary xml:lang="en-US">There are people who really, genuinely, believe, as far as you can dig down, that winning is everything—that however many lies they told, allies they betrayed, innocent lives they harmed, etc. etc., it was all justified by the fact that they won and their enemies lost. Faced with such sociopaths, people like me typically feel […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>There are people who really, genuinely, believe, as far as you can dig down, that winning is everything—that however many lies they told, allies they betrayed, innocent lives they harmed, etc. etc., it was all justified by the fact that <strong>they won and their enemies lost</strong>.  Faced with such sociopaths, people like me typically feel an irresistible compulsion to <em>counterargue</em>: to make the sociopath realize that winning is <em>not</em> everything, that truth and honor are terminal values as well; to subject the sociopath to the standards by which the rest of us are judged; to find the conscience that the sociopath buried even from himself and drag it out into the light.  Let me know if you can think of any case in human history where such efforts succeeded, because I’m having difficulty doing so.</p>



<p>Clearly, in the vast majority of cases if not in all, the only counterargument that a sociopath will ever understand is <em>losing</em>.  And yet not just any kind of losing suffices.  For victims, there’s an <em>enormous</em> temptation to turn the sociopath’s underhanded tools against him, to win with the same deceit and naked power that the sociopath so gleefully inflicted on others.  And yet, if that’s what it takes to beat him, then you have to imagine the sociopath deriving a certain perverse satisfaction from it.</p>



<p>Think of the movie villain who, as the panting hero stands over him with his lightsaber, taunts “Yes … yes … destroy me!  Do it now!  Feel the hate and the rage flow through you!”  What happens next, of course, is that the hero angrily decides to give the villain one more chance, the ungrateful villain lunges to stab the hero in the back or something, and only then does the villain die—either by a self-inflicted accident, or else killed by the hero in immediate self-defense.  Either way, the hero walks away with victory <em>and</em> honor.</p>



<p>In practice, it’s a tall order to arrange all of that.  This explains why sociopaths are so hard to defeat, and why I feel so bleak and depressed whenever I see one flaunting his power.  But, you know, the great upside of pessimism is that it doesn’t take much to beat your expectations!  Whenever a single sociopath <em>is</em> cleanly and honorably defeated, or even just rendered irrelevant—no matter that the sociopath’s friends and allies are still in power, no matter that they’ll be back to fight another day, etc. etc.—it’s a genuine occasion for rejoicing.</p>



<p>Anyway, that pretty much sums up my thoughts regarding Arthur Chu.  In other news, hooray about the election!</p></div>
    </content>
    <updated>2020-11-09T17:06:16Z</updated>
    <published>2020-11-09T17:06:16Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Uncategorized"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-11-12T21:29:58Z</updated>
    </source>
  </entry>
</feed>
