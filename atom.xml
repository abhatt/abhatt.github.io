<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-09-26T15:21:49Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/25/tenure-track-assistant-professor-at-university-of-vienna-apply-by-october-1-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/25/tenure-track-assistant-professor-at-university-of-vienna-apply-by-october-1-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track assistant professor at University of Vienna (apply by October 1, 2020)</title>
    <summary>We are looking for outstanding computer scientists with a research focus on the management of massive data. Examples for research topics of interest are high-performance data mining and machine learning methods in distributed and parallel environments and techniques for the analysis of high-dimensional data, management and analysis of high-throughput data streams. Website: https://informatik.univie.ac.at/en/news-events/article/news/new-tenure-track-professorship-for-the-field-of-management-of-massive-data/ Email: monika.henzinger@univie.ac.at</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are looking for outstanding computer scientists with a research focus on the management of massive data. Examples for research topics of interest are high-performance data mining and machine learning methods in distributed and parallel environments and techniques for the analysis of high-dimensional data, management and analysis of high-throughput data streams.</p>
<p>Website: <a href="https://informatik.univie.ac.at/en/news-events/article/news/new-tenure-track-professorship-for-the-field-of-management-of-massive-data/">https://informatik.univie.ac.at/en/news-events/article/news/new-tenure-track-professorship-for-the-field-of-management-of-massive-data/</a><br/>
Email: monika.henzinger@univie.ac.at</p></div>
    </content>
    <updated>2020-09-25T07:23:35Z</updated>
    <published>2020-09-25T07:23:35Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-26T15:20:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11840</id>
    <link href="http://arxiv.org/abs/2009.11840" rel="alternate" type="text/html"/>
    <title>Complexity of Scheduling Few Types of Jobs on Related and Unrelated Machines</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kouteck=yacute=:Martin.html">Martin Koutecký</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zink:Johannes.html">Johannes Zink</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11840">PDF</a><br/><b>Abstract: </b>The task of scheduling jobs to machines while minimizing the total makespan,
the sum of weighted completion times, or a norm of the load vector, are among
the oldest and most fundamental tasks in combinatorial optimization. Since all
of these problems are in general NP-hard, much attention has been given to the
regime where there is only a small number $k$ of job types, but possibly the
number of jobs $n$ is large; this is the few job types, high-multiplicity
regime. Despite many positive results, the hardness boundary of this regime was
not understood until now.
</p>
<p>We show that makespan minimization on uniformly related machines
($Q|HM|C_{\max}$) is NP-hard already with $6$ job types, and that the related
Cutting Stock problem is NP-hard already with $8$ item types. For the more
general unrelated machines model ($R|HM|C_{\max}$), we show that if either the
largest job size $p_{\max}$, or the number of jobs $n$ are polynomially bounded
in the instance size $|I|$, there are algorithms with complexity
$|I|^{\textrm{poly}(k)}$. Our main result is that this is unlikely to be
improved, because $Q||C_{\max}$ is W[1]-hard parameterized by $k$ already when
$n$, $p_{\max}$, and the numbers describing the speeds are polynomial in $|I|$;
the same holds for $R|HM|C_{\max}$ (without speeds) when the job sizes matrix
has rank $2$. Our positive and negative results also extend to the objectives
$\ell_2$-norm minimization of the load vector and, partially, sum of weighted
completion times $\sum w_j C_j$.
</p>
<p>Along the way, we answer affirmatively the question whether makespan
minimization on identical machines ($P||C_{\max}$) is fixed-parameter tractable
parameterized by $k$, extending our understanding of this fundamental problem.
Together with our hardness results for $Q||C_{\max}$ this implies that the
complexity of $P|HM|C_{\max}$ is the only remaining open case.
</p></div>
    </summary>
    <updated>2020-09-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11793</id>
    <link href="http://arxiv.org/abs/2009.11793" rel="alternate" type="text/html"/>
    <title>On the Parameterized Complexity of \textsc{Maximum Degree Contraction} Problem</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Saket Saurabh, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tale:Prafullkumar.html">Prafullkumar Tale</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11793">PDF</a><br/><b>Abstract: </b>In the \textsc{Maximum Degree Contraction} problem, input is a graph $G$ on
$n$ vertices, and integers $k, d$, and the objective is to check whether $G$
can be transformed into a graph of maximum degree at most $d$, using at most
$k$ edge contractions. A simple brute-force algorithm that checks all possible
sets of edges for a solution runs in time $n^{\mathcal{O}(k)}$. As our first
result, we prove that this algorithm is asymptotically optimal, upto constants
in the exponents, under Exponential Time Hypothesis (\ETH).
</p>
<p>Belmonte, Golovach, van't Hof, and Paulusma studied the problem in the realm
of Parameterized Complexity and proved, among other things, that it admits an
\FPT\ algorithm running in time $(d + k)^{2k} \cdot n^{\mathcal{O}(1)} =
2^{\mathcal{O}(k \log (k+d) )} \cdot n^{\mathcal{O}(1)}$, and remains \NP-hard
for every constant $d \ge 2$ (Acta Informatica $(2014)$). We present a
different \FPT\ algorithm that runs in time $2^{\mathcal{O}(dk)} \cdot
n^{\mathcal{O}(1)}$. In particular, our algorithm runs in time
$2^{\mathcal{O}(k)} \cdot n^{\mathcal{O}(1)}$, for every fixed $d$. In the same
article, the authors asked whether the problem admits a polynomial kernel, when
parameterized by $k + d$. We answer this question in the negative and prove
that it does not admit a polynomial compression unless $\NP \subseteq
\coNP/poly$.
</p></div>
    </summary>
    <updated>2020-09-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11789</id>
    <link href="http://arxiv.org/abs/2009.11789" rel="alternate" type="text/html"/>
    <title>A Case for Partitioned Bloom Filters</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Almeida:Paulo_S=eacute=rgio.html">Paulo Sérgio Almeida</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11789">PDF</a><br/><b>Abstract: </b>In a partitioned Bloom Filter the $m$ bit vector is split into $k$ disjoint
$m/k$ sized parts, one per hash function. Contrary to hardware designs, where
they prevail, software implementations mostly adopt standard Bloom filters,
considering partitioned filters slightly worse, due to the slightly larger
false positive rate (FPR). In this paper, by performing an in-depth analysis,
first we show that the FPR advantage of standard Bloom filters is smaller than
thought; more importantly, by studying the per-element FPR, we show that
standard Bloom filters have weak spots in the domain: elements which will be
tested as false positives much more frequently than expected. This is relevant
in scenarios where an element is tested against many filters, e.g., in packet
forwarding. Moreover, standard Bloom filters are prone to exhibit extremely
weak spots if naive double hashing is used, something occurring in several,
even mainstream, libraries. Partitioned Bloom filters exhibit a uniform
distribution of the FPR over the domain and are robust to the naive use of
double hashing, having no weak spots. Finally, by surveying several usages
other than testing set membership, we point out the many advantages of having
disjoint parts: they can be individually sampled, extracted, added or retired,
leading to superior designs for, e.g., SIMD usage, size reduction, test of set
disjointness, or duplicate detection in streams. Partitioned Bloom filters are
better, and should replace the standard form, both in general purpose libraries
and as the base for novel designs.
</p></div>
    </summary>
    <updated>2020-09-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11780</id>
    <link href="http://arxiv.org/abs/2009.11780" rel="alternate" type="text/html"/>
    <title>An Asymptotically Fast Polynomial Space Algorithm for Hamiltonicity Detection in Sparse Directed Graphs</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bj=ouml=rklund:Andreas.html">Andreas Björklund</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11780">PDF</a><br/><b>Abstract: </b>We present a polynomial space Monte Carlo algorithm that given a directed
graph on $n$ vertices and average outdegree $\delta$, detects if the graph has
a Hamiltonian cycle in $2^{n-\Omega(\frac{n}{\delta})}$ time. This asymptotic
scaling of the savings in the running time matches the fastest known
exponential space algorithm by Bj\"orklund and Williams ICALP 2019. By
comparison, the previously best polynomial space algorithm by Kowalik and
Majewski IPEC 2020 guarantees a $2^{n-\Omega(\frac{n}{2^\delta})}$ time bound.
</p>
<p>Our algorithm combines for the first time the idea of obtaining a fingerprint
of the presence of a Hamiltonian cycle through an inclusion--exclusion
summation over the Laplacian of the graph from Bj\"orklund, Kaski, and Koutis
ICALP 2017, with the idea of sieving for the non-zero terms in an
inclusion--exclusion summation by listing solutions to systems of linear
equations over $\mathbb{Z}_2$ from Bj\"orklund and Husfeldt FOCS 2013.
</p></div>
    </summary>
    <updated>2020-09-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11642</id>
    <link href="http://arxiv.org/abs/2009.11642" rel="alternate" type="text/html"/>
    <title>Fine-grained complexity of the list homomorphism problem: feedback vertex set and cutwidth</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Piecyk:Marta.html">Marta Piecyk</a>, Paweł Rzążewski <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11642">PDF</a><br/><b>Abstract: </b>For graphs $G,H$, a homomorphism from $G$ to $H$ is an edge-preserving
mapping from $V(G)$ to $V(H)$. In the list homomorphism problem, denoted by
\textsc{LHom}($H$), we are given a graph $G$ and lists $L: V(G) \to 2^{V(H)}$,
and we ask for a homomorphism from $G$ to $H$ which additionally respects the
lists $L$.
</p>
<p>Very recently Okrasa, Piecyk, and Rz\k{a}\.zewski [ESA 2020] defined an
invariant $i^*(H)$ and proved that under the SETH $\mathcal{O}^*\left
(i^*(H)^{\textrm{tw}(G)}\right)$ is the tight complexity bound for
\textsc{LHom}($H$), parameterized by the treewidth $\textrm{tw}(G)$ of the
instance graph $G$. We study the complexity of the problem under dirretent
parameterizations. As the first result, we show that $i^*(H)$ is also the right
complexity base if the parameter is the size of a minimum feedback vertex set
of $G$.
</p>
<p>Then we turn our attention to a parameterization by the cutwidth
$\textrm{ctw}(G)$ of $G$. Jansen and Nederlof~[ESA 2018] showed that
\textsc{List $k$-Coloring} (i.e., \textsc{LHom}($K_k$)) can be solved in time
$\mathcal{O}^*\left (c^{\textrm{ctw}(G)}\right)$ where $c$ does not depend on
$k$. Jansen asked if this behavior extends to graph homomorphisms. As the main
result of the paper, we answer the question in the negative. We define a new
graph invariant $mim^*(H)$ and prove that \textsc{LHom}($H$) problem cannot be
solved in time $\mathcal{O}^*\left
((mim^*(H)-\varepsilon)^{\textrm{ctw}(G)}\right)$ for any $\varepsilon &gt;0$,
unless the SETH fails. This implies that there is no $c$, such that for every
odd cycle the non-list version of the problem can be solved in time
$\mathcal{O}^*\left (c^{\textrm{ctw}(G)} \right)$.
</p>
<p>Finally, we generalize the algorithm of Jansen and Nederlof, so that it can
be used to solve \textsc{LHom}($H$) for every graph $H$; its complexity depends
on $\textrm{ctw}(G)$ and another invariant of $H$, which is constant for
cliques.
</p></div>
    </summary>
    <updated>2020-09-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11622</id>
    <link href="http://arxiv.org/abs/2009.11622" rel="alternate" type="text/html"/>
    <title>On Tractability of Ulams Metric in Highier Dimensions and Dually Related Hierarchies of Problems</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bala:Sebastian.html">Sebastian Bala</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kozik:Andrzej.html">Andrzej Kozik</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11622">PDF</a><br/><b>Abstract: </b>The Ulam's metric is the minimal number of moves consisting in removal of one
element from a permutation and its subsequent reinsertion in different place,
to go between two given permutations. Thet elements that are not moved create
longest common subsequence of permutations. Aldous and Diaconis, in their
paper, pointed that Ulam's metric had been introduced in the context of
questions concerning sorting and tossing cards. In this paper we define and
study Ulam's metric in highier dimensions: for dimension one the considered
object is a pair of permutations, for dimension k it is a pair of k-tuples of
permutations. Over encodings by k-tuples of permutations we define two dually
related hierarchies. Our very first motivation come from Murata at al. paper,
in which pairs of permutations were used as representation of topological
relation between rectangles packed into minimal area with application to VLSI
physical design. Our results concern hardness, approximability, and
parametrized complexity inside the hierarchies.
</p></div>
    </summary>
    <updated>2020-09-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11559</id>
    <link href="http://arxiv.org/abs/2009.11559" rel="alternate" type="text/html"/>
    <title>Dynamic Similarity Search on Integer Sketches</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kanda:Shunsuke.html">Shunsuke Kanda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tabei:Yasuo.html">Yasuo Tabei</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11559">PDF</a><br/><b>Abstract: </b>Similarity-preserving hashing is a core technique for fast similarity
searches, and it randomly maps data points in a metric space to strings of
discrete symbols (i.e., sketches) in the Hamming space. While traditional
hashing techniques produce binary sketches, recent ones produce integer
sketches for preserving various similarity measures. However, most similarity
search methods are designed for binary sketches and inefficient for integer
sketches. Moreover, most methods are either inapplicable or inefficient for
dynamic datasets, although modern real-world datasets are updated over time. We
propose dynamic filter trie (DyFT), a dynamic similarity search method for both
binary and integer sketches. An extensive experimental analysis using large
real-world datasets shows that DyFT performs superiorly with respect to
scalability, time performance, and memory efficiency. For example, on a huge
dataset of 216 million data points, DyFT performs a similarity search 6,000
times faster than a state-of-the-art method while reducing to one-thirteenth in
memory.
</p></div>
    </summary>
    <updated>2020-09-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11552</id>
    <link href="http://arxiv.org/abs/2009.11552" rel="alternate" type="text/html"/>
    <title>Parallel Graph Algorithms in Constant Adaptive Rounds: Theory meets Practice</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Behnezhad:Soheil.html">Soheil Behnezhad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dhulipala:Laxman.html">Laxman Dhulipala</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esfandiari:Hossein.html">Hossein Esfandiari</a>, Jakub Łącki, Vahab Mirrokni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schudy:Warren.html">Warren Schudy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11552">PDF</a><br/><b>Abstract: </b>We study fundamental graph problems such as graph connectivity, minimum
spanning forest (MSF), and approximate maximum (weight) matching in a
distributed setting. In particular, we focus on the Adaptive Massively Parallel
Computation (AMPC) model, which is a theoretical model that captures
MapReduce-like computation augmented with a distributed hash table.
</p>
<p>We show the first AMPC algorithms for all of the studied problems that run in
a constant number of rounds and use only $O(n^\epsilon)$ space per machine,
where $0 &lt; \epsilon &lt; 1$. Our results improve both upon the previous results in
the AMPC model, as well as the best-known results in the MPC model, which is
the theoretical model underpinning many popular distributed computation
frameworks, such as MapReduce, Hadoop, Beam, Pregel and Giraph.
</p>
<p>Finally, we provide an empirical comparison of the algorithms in the MPC and
AMPC models in a fault-tolerant distriubted computation environment. We
empirically evaluate our algorithms on a set of large real-world graphs and
show that our AMPC algorithms can achieve improvements in both running time and
round-complexity over optimized MPC baselines.
</p></div>
    </summary>
    <updated>2020-09-25T23:23:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11514</id>
    <link href="http://arxiv.org/abs/2009.11514" rel="alternate" type="text/html"/>
    <title>On One-way Functions and Kolmogorov Complexity</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yanyi.html">Yanyi Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pass:Rafael.html">Rafael Pass</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11514">PDF</a><br/><b>Abstract: </b>We prove that the equivalence of two fundamental problems in the theory of
computing. For every polynomial $t(n)\geq (1+\varepsilon)n, \varepsilon&gt;0$, the
following are equivalent:
</p>
<p>- One-way functions exists (which in turn is equivalent to the existence of
secure private-key encryption schemes, digital signatures, pseudorandom
generators, pseudorandom functions, commitment schemes, and more);
</p>
<p>- $t$-time bounded Kolmogorov Complexity, $K^t$, is mildly hard-on-average
(i.e., there exists a polynomial $p(n)&gt;0$ such that no PPT algorithm can
compute $K^t$, for more than a $1-\frac{1}{p(n)}$ fraction of $n$-bit strings).
</p>
<p>In doing so, we present the first natural, and well-studied, computational
problem characterizing the feasibility of the central private-key primitives
and protocols in Cryptography.
</p></div>
    </summary>
    <updated>2020-09-25T23:20:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11463</id>
    <link href="http://arxiv.org/abs/2009.11463" rel="alternate" type="text/html"/>
    <title>Algorithms for a Topology-aware Massively Parallel Computation Model</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hu:Xiao.html">Xiao Hu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koutris:Paraschos.html">Paraschos Koutris</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanas:Spyros.html">Spyros Blanas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11463">PDF</a><br/><b>Abstract: </b>Most of the prior work in massively parallel data processing assumes
homogeneity, i.e., every computing unit has the same computational capability,
and can communicate with every other unit with the same latency and bandwidth.
However, this strong assumption of a uniform topology rarely holds in practical
settings, where computing units are connected through complex networks. To
address this issue, Blanas et al. recently proposed a topology-aware massively
parallel computation model that integrates the network structure and
heterogeneity in the modeling cost. The network is modeled as a directed graph,
where each edge is associated with a cost function that depends on the data
transferred between the two endpoints. The computation proceeds in synchronous
rounds, and the cost of each round is measured as the maximum cost over all the
edges in the network.
</p>
<p>In this work, we take the first step into investigating three fundamental
data processing tasks in this topology-aware parallel model: set intersection,
cartesian product, and sorting. We focus on network topologies that are tree
topologies, and present both lower bounds, as well as (asymptotically) matching
upper bounds. The optimality of our algorithms is with respect to the initial
data distribution among the network nodes, instead of assuming worst-case
distribution as in previous results. Apart from the theoretical optimality of
our results, our protocols are simple, use a constant number of rounds, and we
believe can be implemented in practical settings as well.
</p></div>
    </summary>
    <updated>2020-09-25T23:24:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11435</id>
    <link href="http://arxiv.org/abs/2009.11435" rel="alternate" type="text/html"/>
    <title>Dynamic Near Maximum Independent Set with Time Independent of Graph Size</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Xiangyu.html">Xiangyu Gao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jianzhong.html">Jianzhong Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miao:Dongjing.html">Dongjing Miao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Xianmin.html">Xianmin Liu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11435">PDF</a><br/><b>Abstract: </b>Maximum Independent Set ({MaxIS}) problem is a fundamental problem in graph
theory, which is NP-hard. Since the underlying graphs are always changing in
numerous applications, computing a {MaxIS} over dynamic graphs has received
increasing attention in recent years. Due to the intractability to compute an
exact MaxIS or its good approximation over dynamic graphs, this paper studies
the problem to maintain a high-quality independent set, which is an
approximation of the MaxIS, over dynamic graphs. A framework based on swap
operations for resolving this problem is presented and two concrete update
algorithms based on one-swappable vertices and two-swappble vertex pairs are
designed. Both algorithms can compute high-quality independent sets over
dynamic graphs with $O(\Delta^3)$ time for general graphs and with $O(1)$ time
for bounded-degree graphs. Moreover, the lower bound of the size of the
solution maintained by our algorithms is derived if there is no swappable
vertex in it. Then the algorithms are extended under \textit{semi-external}
setting to maintained a high-quality independent set with limited memory
consumption. Extensive experiments are conducted over real graphs to confirm
the effectiveness and efficiency of the proposed methods.
</p></div>
    </summary>
    <updated>2020-09-25T23:26:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11391</id>
    <link href="http://arxiv.org/abs/2009.11391" rel="alternate" type="text/html"/>
    <title>Bad and good news for Strassen's laser method: Border rank of the 3x3 permanent and strict submultiplicativity</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Conner:Austin.html">Austin Conner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Hang.html">Hang Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Landsberg:J=_M=.html">J. M. Landsberg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11391">PDF</a><br/><b>Abstract: </b>We determine the border ranks of tensors that could potentially advance the
known upper bound for the exponent $\omega$ of matrix multiplication. The
Kronecker square of the small $q=2$ Coppersmith-Winograd tensor equals the
$3\times 3$ permanent, and could potentially be used to show $\omega=2$. We
prove the negative result for complexity theory that its border rank is $16$,
resolving a longstanding problem. Regarding its $q=4$ skew cousin in $
C^5\otimes C^5\otimes C^5$, which could potentially be used to prove
$\omega\leq 2.11$, we show the border rank of its Kronecker square is at most
$42$, a remarkable sub-multiplicativity result, as the square of its border
rank is $64$. We also determine moduli spaces $\underline{VSP}$ for the small
Coppersmith-Winograd tensors.
</p></div>
    </summary>
    <updated>2020-09-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.11338</id>
    <link href="http://arxiv.org/abs/2009.11338" rel="alternate" type="text/html"/>
    <title>Convergence of Gibbs Sampling: Coordinate Hit-and-Run Mixes Fast</title>
    <feedworld_mtime>1600992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Laddha:Aditi.html">Aditi Laddha</a>, Santosh Vempala <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.11338">PDF</a><br/><b>Abstract: </b>The Gibbs Sampler is a general method for sampling high-dimensional
distributions, dating back to 1971 [Turchin1971]. In each step, we pick a
random coordinate and re-sample that coordinate from the distribution induced
by fixing all other coordinates. While it has become widely used over the past
half-century, guarantees of efficient convergence have been elusive. Here we
show that for convex bodies in $\mathbb{R}^{n}$ with diameter $D$, the
resulting Coordinate Hit-and-Run (CHAR) algorithm mixes in poly$(n,D)$ steps.
This is the first polynomial guarantee for this widely-used algorithm. We also
give a lower bound on the mixing rate, showing that it is strictly worse than
hit-and-run or the ball walk in the worst case.
</p></div>
    </summary>
    <updated>2020-09-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/147</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/147" rel="alternate" type="text/html"/>
    <title>TR20-147 |  Batch Verification for Statistical Zero Knowledge Proofs | 

	Inbar Kaslasi, 

	Guy Rothblum, 

	Ron Rothblum, 

	Adam Sealfon, 

	Prashant Nalini Vasudevan</title>
    <summary>A statistical zero-knowledge proof (SZK) for a problem $\Pi$ enables a computationally unbounded prover to convince a polynomial-time verifier that $x \in \Pi$ without revealing any additional information about $x$ to the verifier, in a strong information-theoretic sense.

Suppose, however, that the prover wishes to convince the verifier that $k$ separate inputs $x_1,\dots,x_k$ all belong to $\Pi$ (without revealing anything else). A naive way of doing so is to simply run the SZK protocol separately for each input. In this work we ask whether one can do better -- that is, is efficient batch verification possible for SZK?

We give a partial positive answer to this question by constructing a batch verification protocol for a natural and important subclass of SZK -- all problems $\Pi$ that have a non-interactive SZK protocol (in the common random string model). More specifically, we show that, for every such problem $\Pi$, there exists an honest-verifier SZK protocol for batch verification of $k$ instances, with communication complexity $poly(n) + k \cdot poly(\log{n},\log{k})$, where $poly$ refers to a fixed polynomial that depends only on $\Pi$ (and not on $k$). This result should be contrasted with the naive solution, which has communication complexity $k \cdot poly(n)$.

Our proof leverages a new NISZK-complete problem, called Approximate Injectivity, that we find to be of independent interest. The goal in this problem is to distinguish circuits that are nearly injective, from those that are non-injective on almost all inputs.</summary>
    <updated>2020-09-24T13:41:26Z</updated>
    <published>2020-09-24T13:41:26Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-26T15:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/146</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/146" rel="alternate" type="text/html"/>
    <title>TR20-146 |  On the Hardness of Detecting Macroscopic Superpositions | 

	Scott Aaronson, 

	Yosi Atia, 

	Leonard Susskind</title>
    <summary>When is decoherence "effectively irreversible"? Here we examine this central question of quantum foundations using the tools of quantum computational complexity. We prove that, if one had a quantum circuit to determine if a system was in an equal superposition of two orthogonal states (for example, the $|$Alive$\rangle$ and $|$Dead$\rangle$ states of Schrodinger's cat), then with only a slightly larger circuit, one could also $\mathit{swap}$ the two states (e.g., bring a dead cat back to life). In other words, observing interference between the $|$Alive$\rangle$and $|$Dead$\rangle$ states is a "necromancy-hard" problem, technologically infeasible in any world where death is permanent. As for the converse statement (i.e., ability to swap implies ability to detect interference), we show that it holds modulo a single exception, involving unitaries that (for example) map $|$Alive$\rangle$ to $|$Dead$\rangle$ but $|$Dead$\rangle$ to -$|$Alive$\rangle$. We also show that these statements are robust---i.e., even a $\mathit{partial}$ ability to observe interference implies partial swapping ability, and vice versa. Finally, without relying on any unproved complexity conjectures, we show that all of these results are quantitatively tight. Our results have possible implications for the state dependence of observables in quantum gravity, the subject that originally motivated this study.</summary>
    <updated>2020-09-23T22:45:17Z</updated>
    <published>2020-09-23T22:45:17Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-26T15:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/145</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/145" rel="alternate" type="text/html"/>
    <title>TR20-145 |  An Improved Exponential-Time Approximation Algorithm for Fully-Alternating Games Against Nature | 

	Andrew Drucker</title>
    <summary>"Games against Nature" [Papadimitriou '85] are two-player games of perfect information, in which one player's moves are made randomly (here, uniformly); the final payoff to the non-random player is given by some $[0, 1]$-valued function of the move history.  Estimating the value of such games under optimal play, and computing near-optimal strategies, is an important goal in the study of decision-making under uncertainty, and has seen significant research in AI and allied areas [Hnich, Rossi, Tarim, Prestwich '11], with only experimental evaluation of most algorithms' performance.  The problem's PSPACE-completeness does not rule out nontrivial algorithms.  Improved algorithms with theoretical guarantees are known in various cases where the payoff function $F$ has special structure, and Littman, Majercik, and Pitassi [LMP'01] give a sampling-based improved algorithm for general $F$, for turn-orders which restrict the number of non-random player strategies.

We study the case of general $F$ for which the players strictly alternate with binary moves $(w_1, r_1, w_2, r_2, \ldots, w_{n/2}, r_{n/2})$---for which the approach of [LMP'01] does not improve over brute force.  We give a randomized algorithm to approximate the value of such games under optimal play, and to execute near-optimal strategies. Our algorithm achieves exponential savings over brute-force, making $2^{(1 - \delta) n}$ queries to $F$ for some absolute constant $\delta &gt; 0$, and certifies a lower bound $\hat{v}$ on the game value $v$ with additive expected error bounded as $E[v - \hat{v}] \leq \exp(-\Omega(n))$.  (On the downside, $\delta$ is tiny and the algorithm uses exponential space.)

Our algorithm is recursive, and bootstraps a "base case" algorithm for fixed-size inputs.  The method of recursive composition used, the specific base-case guarantees needed, and the steps to establish these guarantees are interesting and, we feel, likely to find uses beyond the present work.</summary>
    <updated>2020-09-23T21:48:34Z</updated>
    <published>2020-09-23T21:48:34Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-26T15:20:41Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7113260074896603448</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7113260074896603448/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/remembering-2000.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7113260074896603448" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7113260074896603448" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/remembering-2000.html" rel="alternate" type="text/html"/>
    <title>Remembering 2000</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="http://www.cs.cmu.edu/~FOCS2000/">FOCS 2000</a> took place in Redondo Beach, just south of Los Angeles, November 12-14. Certainly some great results such as the Reingold-Vadhan-Wigderson <a href="https://doi.org/10.1109/SFCS.2000.892006">Zig-Zag Graph Product Expander construction</a> that would lead to Omer Reingold's <a href="https://blog.computationalcomplexity.org/2014/02/favorite-theorems-connecting-in-log.html">Undirected Connectivity in Log Space</a>. Mostly though I remember the discussions about the presidential election held the week before and whether we might find out our next president during the conference. Spoiler alert: <a href="https://en.wikipedia.org/wiki/2000_United_States_presidential_election_recount_in_Florida">We didn't</a>. </p><p>Consider the following viewpoints for a person X</p><p>1. Did X support Bush or Gore?</p><p>2. Did X interpret the rules of the election that Bush won or Gore won?</p><p>These should be independent events. Your interpretation of the rules should not depend on who you supported. But in fact they were nearly perfectly correlated. Whether you were a politician, a newspaper editorial page writer, a supreme court justice, a computer scientist or pretty much everyone else, if you supported Gore, you believed he won the election and vice-versa. Everyone had their logic why they were right and I'm sure my readers who remember that election still believe their logic was correct. </p><p>As this upcoming election gets messy, as it already has, take care with trying to justify your desired endgame by choosing the logic that makes it work. Would you use the same logic if the candidates were reversed? Everyone says "yes" but it's rarely true. Just like Mitch McConnell, you'll just find some excuse why the opposite situation is different. Trust me, my logic is impeccable. </p></div>
    </content>
    <updated>2020-09-23T21:33:00Z</updated>
    <published>2020-09-23T21:33:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-26T13:46:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20259</id>
    <link href="https://gilkalai.wordpress.com/2020/09/23/to-cheer-you-up-in-difficult-times-12-asaf-ferber-and-david-conlon-found-new-lower-bounds-for-diagonal-ramsey-numbers/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 12:  Asaf Ferber and David Conlon found new lower bounds for diagonal Ramsey numbers</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Lower bounds for multicolor Ramsey numbers The Ramsey number r(t; ℓ) is the smallest natural number n such that every ℓ-coloring of the edges of the complete graph contains a monochromatic . (r(t;2) is often denoted by R(t,t) and r(t;3) … <a href="https://gilkalai.wordpress.com/2020/09/23/to-cheer-you-up-in-difficult-times-12-asaf-ferber-and-david-conlon-found-new-lower-bounds-for-diagonal-ramsey-numbers/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h2 class="title mathjax"><a href="https://arxiv.org/abs/2009.10458">Lower bounds for multicolor Ramsey numbers</a></h2>
<p>The Ramsey number <em>r(t; ℓ)</em> is the smallest natural number <em>n</em> such that every ℓ-coloring of the edges of the complete graph <img alt="K_n" class="latex" src="https://s0.wp.com/latex.php?latex=K_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K_n"/> contains a monochromatic <img alt="K_t" class="latex" src="https://s0.wp.com/latex.php?latex=K_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K_t"/>. (<em>r(t;2)</em> is often denoted by<em> R(t,t)</em> and <em>r(t;3)</em> by R<em>(t,t,t)</em> etc.) <a href="https://en.wikipedia.org/wiki/Ramsey%27s_theorem">Famously</a>, <em>R(3,3)=6</em>; <em>R(4,4)=18</em>;  and <em>R(3,3,3)=17</em>. Understanding <em>R(t,t)</em> is among the most famous problems in combinatorics. (Understanding if r(3; <em> ℓ</em>) is exponential or superexponential in<em> ℓ</em> is also a very famous problem.)</p>
<p>It is known since the 1940s that <img alt="t/2 +o(1) \le log_2 R(t,t) \le 2t" class="latex" src="https://s0.wp.com/latex.php?latex=t%2F2+%2Bo%281%29+%5Cle+log_2+R%28t%2Ct%29+%5Cle+2t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t/2 +o(1) \le log_2 R(t,t) \le 2t"/>.</p>
<p>Lower bounds for <em>R(t,t)</em> where used by Lefmann in 1987 to give lower bounds on <em>r(t; ℓ)</em> for <em> ℓ</em>&gt;2. Asaf Ferber and David Conlon gave now <span style="color: #ff0000;"><strong>exponential</strong></span> improvement. This is truly remarkable and <a href="https://arxiv.org/abs/2009.10458">the paper is just 4-page long!</a> congratulations Asaf and David!</p>
<p>Expect more cheering news of discrete geometry nature from Oberwolfach. (I take part remotely in the traditional meeting on Discrete and computational geometry, see pictures below).</p>
<p>Update: <a href="https://anuragbishnoi.wordpress.com/2020/09/23/improved-lower-bounds-for-multicolour-diagonal-ramsey-numbers/">An excellent blog post on Anurag math blog.</a> Anurag describes in details the construction, describes the connections with finite geometries, and improves the construction to get a better result.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/09/pak.png"><img alt="" class="alignnone size-medium wp-image-20264" height="188" src="https://gilkalai.files.wordpress.com/2020/09/pak.png?w=300&amp;h=188" width="300"/></a> <a href="https://gilkalai.files.wordpress.com/2020/09/ow2.png"><img alt="" class="alignnone size-medium wp-image-20265" height="188" src="https://gilkalai.files.wordpress.com/2020/09/ow2.png?w=300&amp;h=188" width="300"/></a></p></div>
    </content>
    <updated>2020-09-23T11:24:44Z</updated>
    <published>2020-09-23T11:24:44Z</published>
    <category term="Combinatorics"/>
    <category term="Asaf Ferber"/>
    <category term="David Conlon"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-09-26T15:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17612</id>
    <link href="https://rjlipton.wordpress.com/2020/09/22/puzzle-reviews-by-a-puzzle-writer/" rel="alternate" type="text/html"/>
    <title>Puzzle Reviews by a Puzzle Writer</title>
    <summary>Not puzzling reviews Princeton University Press page Jason Rosenhouse is professor in the Department of Mathematics at James Madison University. His research focuses on algebraic graph theory and analytic number theory involving exponential sums. The former includes a neat paper on expansion properties of a family of graphs associated to block designs, with two undergraduates […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Not puzzling reviews</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/09/jason-1.jpeg"><img alt="" class="alignright wp-image-17615" height="160" src="https://rjlipton.files.wordpress.com/2020/09/jason-1.jpeg?w=140&amp;h=160" width="140"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Princeton University Press <a href="https://press.princeton.edu/our-authors/rosenhouse-jason">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Jason Rosenhouse is professor in the Department of Mathematics at James Madison University. His research focuses on algebraic graph theory and analytic number theory involving exponential sums.  The former includes a neat <a href="https://www.researchgate.net/publication/220620901_Expansion_Properties_Of_Levi_Graphs">paper</a> on expansion properties of a family of graphs associated to block designs, with two undergraduates among its authors.  But besides his “real” research, he has written a number of books on puzzles such as <i><a href="https://www.amazon.com/s?k=Jason+Rosenhouse&amp;i=stripbooks&amp;ref=nb_sb_noss_2">The Monty Hall Problem</a>: The Remarkable Story of Math’s Most Contentious Brain Teaser</i>. Soon his book <i><a href="https://www.amazon.co.uk/Games-Your-Mind-History-Puzzles/dp/0691174075">Games for Your Mind</a>: The History and Future of Logic Puzzles</i> is to be published.</p>
<p><a href="https://rjlipton.files.wordpress.com/2020/09/revbook-1.png"><img alt="" class="aligncenter size-thumbnail wp-image-17626" height="150" src="https://rjlipton.files.wordpress.com/2020/09/revbook-1.png?w=103&amp;h=150" width="103"/></a></p>
<p>
Today Ken and I thought we would highlight his recent review of a book on math puzzles.</p>
<p>
I have mixed feelings about puzzles. I like them, and am happy when I can understand their solution. I am even happier when I can solve them. I sometimes feel that I should spend my limited brain cycles on “real” problems. But puzzles are fun. </p>
<p>
Rosenhouse’s <a href="https://www.ams.org/journals/notices/202009/rnoti-p1382.pdf">review</a> is in the recent <em>Notices of the AMS</em> on the book <i><a href="https://bookstore.ams.org/prb-36">Bicycles or Unicycles</a>: A Collection of Intriguing Mathematical Puzzles</i>. This book, the “Bicycle Book,” is authored by Daniel Velleman and Stan Wagon.</p>
<p><a href="https://rjlipton.files.wordpress.com/2020/09/maabook.jpg"><img alt="" class="aligncenter wp-image-17617" height="145" src="https://rjlipton.files.wordpress.com/2020/09/maabook.jpg?w=101&amp;h=145" width="101"/></a></p>
<p>
Their book is a collection of <img alt="{3 \times 5 \times 7}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+5+%5Ctimes+7%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3 \times 5 \times 7}"/> mathematical puzzles. Rosenhouse likes their book, which means a lot coming from an author of so many puzzle books himself. </p>
<p>
</p><p/><h2> A Cool Problem </h2><p/>
<p/><p>
Rosenhouse presents this problem from the Bicycle Book. </p>
<blockquote><p><b> </b> <em> You are playing solitaire in the first quadrant of the Cartesian plane, the lower corner of which is shown in Figure 1. You begin with a single checker on square a1. On each turn, a legal move consists of removing one checker from the board and then placing two new checkers in the cells immediately above and to the right of the original checker. If either of those two cells is occupied, then the move is illegal, and a different checker must be selected for removal. </em>
</p></blockquote>
<p/><p>
<a href="https://rjlipton.files.wordpress.com/2020/09/solitairepuzzle.png"><img alt="" class="aligncenter size-full wp-image-17618" src="https://rjlipton.files.wordpress.com/2020/09/solitairepuzzle.png?w=600"/></a></p>
<p>
Show that you can never make all of the <img alt="{3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3 \times 3}"/> lower-left squares empty. This is a complexity question. You describe a computation and assert that certain states cannot be reached. The challenge is two-fold: </p>
<ol>
<li>
The computation is nondeterministic. There can be more than one next state. <p/>
</li><li>
The computation can reach infinitely many states. The task is to prove that no reachable state has the lower nine squares empty.
</li></ol>
<p>
</p><p/><h2> A Cool Solution </h2><p/>
<p/><p>
I must admit I read the solution before I tried to solve the puzzle. I did find an alternative solution. It was not as clever as the one from the book. Let’s look at that solution first. </p>
<p>
The idea is to assign <i>magic</i> values to each square on the checkerboard. The value of a state is the sum over all the values of squares with a checker. We need these to hold: </p>
<ol>
<li>
The value of the initial square is <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. <p/>
</li><li>
The value of a move leaves the total sum over all the checkers the same. <p/>
</li><li>
The value of the squares <b>not</b> in the lower <img alt="{3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3 \times 3}"/> is less than <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>.
</li></ol>
<p>Then there can never be a reachable state that avoids all the lower <img alt="{3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3 \times 3}"/>. How can we do this? Assign the values as shown below. </p>
<p align="center"><img alt="\displaystyle  \begin{array}{ccccl} \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \\ 1/8 &amp; 1/16 &amp; 1/32 &amp; 1/64 &amp; \cdots\\ 1/4 &amp; 1/8 &amp; 1/16 &amp; 1/32 &amp; \cdots\\ 1/2 &amp; 1/4 &amp; 1/8 &amp; 1/16 &amp; \cdots\\ 1 &amp; 1/2 &amp; 1/4 &amp; 1/8 &amp; \cdots \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Bccccl%7D+%5Cvdots+%26+%5Cvdots+%26+%5Cvdots+%26+%5Cvdots+%26+%5C%5C+1%2F8+%26+1%2F16+%26+1%2F32+%26+1%2F64+%26+%5Ccdots%5C%5C+1%2F4+%26+1%2F8+%26+1%2F16+%26+1%2F32+%26+%5Ccdots%5C%5C+1%2F2+%26+1%2F4+%26+1%2F8+%26+1%2F16+%26+%5Ccdots%5C%5C+1+%26+1%2F2+%26+1%2F4+%26+1%2F8+%26+%5Ccdots+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{ccccl} \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \\ 1/8 &amp; 1/16 &amp; 1/32 &amp; 1/64 &amp; \cdots\\ 1/4 &amp; 1/8 &amp; 1/16 &amp; 1/32 &amp; \cdots\\ 1/2 &amp; 1/4 &amp; 1/8 &amp; 1/16 &amp; \cdots\\ 1 &amp; 1/2 &amp; 1/4 &amp; 1/8 &amp; \cdots \end{array} "/></p>
<p>
Ken remembers, as a teenager, seeing this puzzle in a collection by the master Martin Gardner, with the same proof. Ken thought of it again when considering problems in physics and combinatorics that involve defining an appropriate potential function as the first step. </p>
<p>
</p><p/><h2> An Uncool Solution </h2><p/>
<p/><p>
Let <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> be the lower-right <img alt="{3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3 \times 3}"/> corner board. Label the positions as usual with <img alt="{(i,j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(i,j)}"/> where <img alt="{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i,j}"/> both are in <img alt="{\{1,2,3\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C2%2C3%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{1,2,3\}}"/>.</p>
<p>
Let <img alt="{N(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N(t)}"/> be the number of checkers in <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. Of course <img alt="{N(0)=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%280%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N(0)=1}"/> and the checker is at <img alt="{(1,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1,1)}"/>.</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2020/09/config33v1.jpg"><img alt="" class="aligncenter wp-image-17621" height="107" src="https://rjlipton.files.wordpress.com/2020/09/config33v1.jpg?w=150&amp;h=107" width="150"/></a></p>
<p>
Suppose by way of contradiction that it is possible to make <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> empty. </p>
<p>
Our proof uses that the transition from <img alt="{N(t)&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%28t%29%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N(t)&gt;0}"/> to <img alt="{N(t+1)=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%28t%2B1%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N(t+1)=0}"/> requires that <img alt="{N(t)=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%28t%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N(t)=1}"/>. That is <img alt="{N(t)=2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%28t%29%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N(t)=2}"/> or even <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/> is impossible. The rule cannot remove two or more checkers from <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> in one move. </p>
<p>
<a href="https://rjlipton.files.wordpress.com/2020/09/config33v2.jpg"><img alt="" class="aligncenter wp-image-17622" height="97" src="https://rjlipton.files.wordpress.com/2020/09/config33v2.jpg?w=150&amp;h=97" width="150"/></a></p>
<p>
Let <img alt="{N(t)=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%28t%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N(t)=1}"/> and <img alt="{N(t+1)=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%28t%2B1%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N(t+1)=0}"/>. So where is the checker? A simple case analysis shows it must be at <img alt="{(3,3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%283%2C3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(3,3)}"/>. So now we know the last placement. But how did we get to this position? It is easy to see that it had to be previously at <img alt="{(3,2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%283%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(3,2)}"/> or <img alt="{(2,3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%282%2C3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(2,3)}"/>. By symmetry we can assume was <img alt="{(3,2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%283%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(3,2)}"/>. </p>
<p>
<a href="https://rjlipton.files.wordpress.com/2020/09/config33v3.jpg"><img alt="" class="aligncenter wp-image-17623" height="98" src="https://rjlipton.files.wordpress.com/2020/09/config33v3.jpg?w=150&amp;h=98" width="150"/></a></p>
<p>
Our goal to show that we cannot place one checker at <img alt="{(3,2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%283%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(3,2)}"/> and no other in <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. A little analysis shows that it must be the case that the previous state was one checker at <img alt="{(3,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%283%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(3,1)}"/>. But it is impossible to place a checker there and avoid having more checkers. This yields a contradiction. </p>
<p>
<a href="https://rjlipton.files.wordpress.com/2020/09/config33v4.jpg"><img alt="" class="aligncenter wp-image-17624" height="97" src="https://rjlipton.files.wordpress.com/2020/09/config33v4.jpg?w=150&amp;h=97" width="150"/></a></p>
<p/><h2> Another Solution </h2><p/>
<p/><p>
We could use finite state automata theory to supply another solution. The obvious issue is the full game is played on an infinite checkerboard. But we can use a standard trick to reduce the state space to a finite one. Imagine we play the game on just <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. When we have a move that creates checkers outside of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> just throw them away. It is simple to see that no move can place checkers inside <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. Thus if we cannot empty <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> in this finite version, then there is no way in the full game. </p>
<p>
Now the state space is bounded by <img alt="{2^{9}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B9%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{9}}"/>: each of the nine squares can have a checker or not. We know the initial state and we know the final state. So we can run a finite state search algorithm and decide the answer.</p>
<p>
The value of this solution is that it could handle more complex rules and larger squares. Well at least those within reason. </p>
<p>
</p><p/><h2> Other Puzzles </h2><p/>
<p/><p>
Rosenhouse covers nine other puzzles in his review. In our meta review of his review we will cover just two more. </p>
<p>
The third puzzle in his review comes from the challenge to prove that each matrix <img alt="{\{C_n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7BC_n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{C_n\}}"/> has determinant <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. This is a puzzle because the matrices <img alt="{C_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_{n}}"/> look like they could have some strange determinant, one that even varies with <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. The trick is to show that there are other families <img alt="{\{A_n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7BA_n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{A_n\}}"/> and <img alt="{\{B_n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7BB_n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{B_n\}}"/> of matrices, in which each matrix has determinant <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> and that 	</p>
<p align="center"><img alt="\displaystyle  C_n = A_n B_n. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C_n+%3D+A_n+B_n.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C_n = A_n B_n. "/></p>
<p>Of course this immediately proves that <img alt="{C_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_{n}}"/> also have determinant <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. The challenge is kind of a factorization problem. </p>
<p>
Another puzzle is to prove that a number <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> is prime if and only if there is exactly one pair of positive integers <img alt="{m,n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%2Cn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m,n}"/> such that </p>
<p align="center"><img alt="\displaystyle  \frac{1}{m} - \frac{1}{n} = \frac{1}{p}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B1%7D%7Bm%7D+-+%5Cfrac%7B1%7D%7Bn%7D+%3D+%5Cfrac%7B1%7D%7Bp%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{1}{m} - \frac{1}{n} = \frac{1}{p}. "/></p>
<p>This seems to be surprising in two ways: First who could think of this? Second who could think of this? Okay it should be why is it true? Indeed Rosenhouse says that the proof is complex. </p>
<p>
Rosenhouse adds that most puzzles in this book are less “bite-sized” than the ones typically posed by the master Gardner. This certainly goes for the title puzzle about whether a bicycle can possibly move along a curve—other than a straight line—that was made by a unicycle. It requires a foray into differential equations.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
My “uncool solution” was left somewhat incomplete. Do you see how to complete the analysis?</p>
<p/></font></font></div>
    </content>
    <updated>2020-09-22T22:01:06Z</updated>
    <published>2020-09-22T22:01:06Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Teaching"/>
    <category term="trick"/>
    <category term="book reviews"/>
    <category term="Daniel Velleman"/>
    <category term="Jason Rosenhouse"/>
    <category term="puzzles"/>
    <category term="Stan Wagon"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-26T15:20:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/144</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/144" rel="alternate" type="text/html"/>
    <title>TR20-144 |  Toward Probabilistic Checking against Non-Signaling Strategies with Constant Locality | 

	Mohammad Jahanara, 

	Sajin Koroth, 

	Igor Shinkar</title>
    <summary>Non-signaling strategies are a generalization of quantum strategies that have been studied in physics over the past three decades. Recently, they have found applications in theoretical computer science, including to proving inapproximability results for linear programming and to constructing protocols for delegating computation. A central tool for these applications is probabilistically checkable proof (PCPs) systems that are sound against non-signaling strategies.

In this paper we show, assuming a certain geometrical hypothesis about noise robustness of non-signaling proofs (or, equivalently, about robustness to noise of solutions to the Sherali-Adams linear program), that a slight variant of the parallel repetition of the exponential-length constant-query PCP construction due to Arora et al. (JACM 1998) is sound against non-signaling strategies with constant locality.

Our proof relies on the analysis of the linearity test and agreement test (also known as the direct product test) in the non-signaling setting.</summary>
    <updated>2020-09-22T11:49:39Z</updated>
    <published>2020-09-22T11:49:39Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-26T15:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/143</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/143" rel="alternate" type="text/html"/>
    <title>TR20-143 |  Characterizing Average-Case Complexity of PH by Worst-Case Meta-Complexity | 

	Shuichi Hirahara</title>
    <summary>We exactly characterize the average-case complexity of the polynomial-time hierarchy (PH) by the worst-case (meta-)complexity of GapMINKT(PH), i.e., an approximation version of the problem of determining if a given string can be compressed to a short PH-oracle efficient program.  Specifically, we establish the following equivalence:

  DistPH is contained in AvgP (i.e., PH is easy on average) if and only if GapMINKT(PH) is in P.

In fact, our equivalence is significantly broad: A number of statements on several fundamental notions of complexity theory, such as errorless and one-sided-error average-case complexity, sublinear-time-bounded and polynomial-time-bounded Kolmogorov complexity, and PH-computable hitting set generators, are all shown to be equivalent.

Our equivalence provides fundamentally new proof techniques for analyzing average-case complexity through the lens of *meta-complexity* of time-bounded Kolmogorov complexity and resolves, as immediate corollaries, questions of equivalence among different notions of average-case complexity of PH: low success versus high success probabilities (i.e., a hardness amplification theorem for DistPH against uniform algorithms) and errorless versus one-sided-error average-case complexity of PH.

Our results are based on a sequence of new technical results that further develops the proof techniques of the author's previous work on the non-black-box worst-case to average-case reduction and unexpected hardness results for Kolmogorov complexity (FOCS'18, CCC'20, ITCS'20, STOC'20).  Among other things, we prove the following.

  1.  If GapMINKT(NP) is in P, then P = BPP.
  At the core of the proof is a new black-box hitting set generator construction whose reconstruction algorithm uses few random bits, which also improves the approximation quality of the non-black-box worst-case to average-case reduction without using a pseudorandom generator.

  2.  If GapMINKT(PH) is in P, then DistPH is contained in AvgBPP = AvgP.

  3.  If MINKT(PH) is easy on a 1/poly(n)-fraction of inputs, then GapMINKT(PH) is in P.
  This improves the error tolerance of the previous non-black-box worst-case to average-case reduction.</summary>
    <updated>2020-09-21T10:16:37Z</updated>
    <published>2020-09-21T10:16:37Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-26T15:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4972</id>
    <link href="https://www.scottaaronson.com/blog/?p=4972" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4972#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4972" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Agent 3203.7: Guest post by Eliezer Yudkowsky</title>
    <summary xml:lang="en-US">In his day, Agent 3203.7 had stopped people from trying to kill Adolf Hitler, Richard Nixon, and even, in the case of one unusually thoughtful assassin, Henry David Thoreau. But this was a new one on him. “So…” drawled the seventh version of Agent 3203. His prosthetic hand crushed the simple 21st-century gun into fused […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>In his day, Agent 3203.7 had stopped people from trying to kill Adolf Hitler, Richard Nixon, and even, in the case of one unusually thoughtful assassin, Henry David Thoreau. But this was a new one on him.</p>



<p>“So…” drawled the seventh version of Agent 3203. His prosthetic hand crushed the simple 21st-century gun into fused metal and dropped it. “You traveled to the past in order to kill… of all people… Donald Trump. Care to explain why?”</p>



<p>The time-traveller’s eyes looked wild. Crazed. Nothing unusual. “How can you ask me that? You’re a time-traveler too! You know what he does!”</p>



<p>That was a surprising level of ignorance even for a 21st-century jumper. “Different timelines, kid. Some are pretty obscure. What the heck did Trump do in yours that’s worth taking your one shot at time travel to assassinate him of all people?”</p>



<p>“He’s destroying my world!”</p>



<p>Agent 3203.7 took a good look at where Donald Trump was pridefully addressing the unveiling of the Trump Taj Mahal in New Jersey, then took another good look at the errant time-traveler. “Destroying it how, exactly? Did Trump turn mad scientist in your timeline?”</p>



<p>“He’s President of the United States!”</p>



<p>Agent 3203.7 took another long stare at his new prisoner. He was apparently serious. “How did Trump become President in your timeline? Strangely advanced technology, subliminal messaging?”</p>



<p>“He was elected in the usual way,” the prisoner said bitterly.</p>



<p>Agent 3203.7 shook his head in amazement. Talk about shooting the messenger. “Kid, I doubt Trump was your timeline’s main problem.”</p>



<p><em>(thanks to Eliezer for giving me permission to reprint here)</em></p></div>
    </content>
    <updated>2020-09-21T04:01:10Z</updated>
    <published>2020-09-21T04:01:10Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-09-21T04:01:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3655236890828727429</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3655236890828727429/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/baseball-can-go-on-forever-it-doesnt.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3655236890828727429" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3655236890828727429" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/baseball-can-go-on-forever-it-doesnt.html" rel="alternate" type="text/html"/>
    <title>Baseball can go on forever, it doesn't just seem that way</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Most games have some way to make sure they cannot go on forever.</p><p>1) Chess: I had thought there was a 50-move rule and a 3-times-same-position rule, but its a byte more complicated than that, see <a href="https://en.wikipedia.org/wiki/Draw_(chess)">here</a>. There is also a chess clock. Suffice to say, Chess can never on forever (though it may seem like it does). </p><p>2) NIM: Eventually all of the stones are gone. There may be more complicated versions where you can add some stones, but in those versions I suspect that there is some parameter that goes to 0.</p><p>3) Basketball, Football, Hockey, Soccer: These all have a clock so they are time limited. For overtime there are also rules that make sure the game cannot go on forever. Or maybe its just very rare: what if the Superb Owl (spelled that way to avoid lawsuits, see <a href="https://www.vox.com/the-goods/2019/1/31/18202037/super-bowl-53-ads-trademark-the-big-game-2019">here</a>) is tied 0-0 at the end of the four quarters and goes into overtime and... nobody scores... ever. Could the game go on forever or would the referees declare it a tie? In the regular season there are ties, but in the in the superb owl? Actually this may be more a problem in the playoffs since you need to determine who goes to the next round.</p><p>4) Take your favorite game. I would bet dollars to doughnuts (what an odd phrase---see <a href="https://en.wiktionary.org/wiki/bet_a_dollar_to_a_doughnut">here</a> for more about the phrase) that there is some mechanism to make sure the game ends. An exception that Darling pointed out to me: If in Gin Rummy both players are terrible then the game can go on forever. This is probably true for other games as well and actually makes the question into two questions (a) will a game terminate no matter what the players do, and (b) (not sure how to formalize) will a game terminate if both players are trying to win and are making reasonable moves.</p><p>You may have noticed that in item 3 I left out Baseball. There is no clock in baseball. So one way the game can go on forever is to have a tie and extra innings and nobody scores. I think the umpire has the authority to call it a tie. (Incidentally, the shortened baseball season has a new extra inning rule---each inning starts with a runner on second. See <a href="https://www.mlb.com/news/reasons-new-extra-innings-rule-is-good">here</a>,) When Lance read an earlier version of this post he pointed me to 5 ways a game can go on forever, not counting the example I have later in this post. <a href="https://cs.nyu.edu/~gottlieb/tr/back-issues/1990s/1992/1-jan-scanned.pdf">Here</a> is where Lance found the question and answer (look on the first page under Speed Department for the question, and the very end of the second page for the answer). I also did my own writuep with more details, see <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/baseballforever.pdf">here</a>.  Also of interest (though not if you were actually at the game this happened), the record for number of times a player has a foul with 2 strikes is 16, see <a href="https://www.businessinsider.com/brandon-belts-record-at-bat-pop-fly-2018-4">here</a>. </p><p> However, I came across an  example more obscure than any of those. </p><p>Here is what happened (and you can see the video of it <a href="https://www.youtube.com/watch?v=yDyCRTlKllk">here</a>, though it really starts about a minute into it. Keep reading- it looks like its another post, but its part of this post: </p><div class="q-box qu-borderBottom qu-px--medium qu-py--small" style="border-bottom-style: solid; border-color: rgb(222, 224, 225); border-width: 1px; color: #282829; direction: ltr; font-size: 15px; padding: 8px 16px;"><div class="q-flex qu-justifyContent--space-between" style="direction: ltr; display: flex;"><div class="q-flex qu-alignItems--center" style="direction: ltr; display: flex;"><div class="q-text qu-fontSize--small qu-ml--small qu-color--gray" style="color: #636466; direction: ltr; font-size: 13px; margin-left: 8px;">From your Digest</div></div></div></div><div class="q-box" style="color: #282829; direction: ltr; font-size: 15px;"><div class="q-box" style="direction: ltr;"><div class="q-box qu-pt--medium qu-pb--tiny" style="direction: ltr; padding: 16px 16px 4px;"><div class="q-box" style="direction: ltr;"><div class="q-box" style="direction: ltr;"><div class="q-box" style="direction: ltr;"><div class="q-box" style="direction: ltr;"><div class="q-flex" style="direction: ltr; display: flex;"><div class="q-box qu-mb--small qu-pr--large" style="direction: ltr; margin-bottom: 8px; padding-right: 24px; width: 546px;"><div class="q-box spacing_log_answer_header" style="direction: ltr;"><div class="q-flex" style="direction: ltr; display: flex; width: 522px;"><div class="q-inlineFlex qu-mr--small qu-alignItems--center" style="direction: ltr; display: inline-flex; margin-right: 8px;"><div class="q-box qu-display--inline-block" style="direction: ltr; display: inline-block;"><div class="q-box qu-display--inline-block" style="direction: ltr; display: inline-block;"><div class="q-relative qu-display--inline-block" style="direction: ltr; display: inline-block;"><div class="q-box qu-display--inline-block" style="direction: ltr; display: inline-block;"><a class="q-box qu-display--inline-flex qu-color--gray_dark qu-cursor--pointer qu-hover--textDecoration--underline" href="https://www.quora.com/profile/Zev-Steinhardt" target="_blank"><div class="q-inlineFlex qu-flex--none" style="direction: ltr; display: inline-flex;"><div class="q-inlineFlex" style="direction: ltr; display: inline-flex;"><div class="q-inlineFlex qu-overflow--hidden qu-borderRadius--circle qu-borderWidth--retinaOverride"><div class="q-box qu-bg--white__ignore_dark_mode qu-borderRadius--circle" style="background-color: white; border-radius: 100%; direction: ltr;"/><img class="q-image qu-display--block qu-size--36 qu-minWidth--36" src="https://qph.fs.quoracdn.net/main-thumb-138599745-200-pbrgkfnbxdzyttabmtnmavtcwavrcktv.jpeg"/><div class="q-box qu-borderRadius--circle qu-borderAll Photo___StyledBox-sc-1x7c6d3-1 djSgZk"/></div></div></div></a></div></div></div></div></div><div class="q-box qu-flex--auto"><div class="q-flex qu-flexWrap--wrap" style="direction: ltr; display: flex;"><div class="q-box" style="direction: ltr;"><div class="q-text qu-bold qu-color--gray_dark qu-fontSize--small qu-passColorToLinks" style="direction: ltr; font-size: 13px; font-weight: bold;"><div class="q-box qu-display--inline" style="direction: ltr; display: inline;"><div class="q-box qu-display--inline" style="direction: ltr; display: inline;"><div class="q-relative qu-display--inline" style="direction: ltr; display: inline;"><div class="q-box qu-display--inline" style="direction: ltr; display: inline;"><a class="q-box qu-color--gray_dark qu-cursor--pointer qu-hover--textDecoration--underline" href="https://www.quora.com/profile/Zev-Steinhardt" target="_blank">Zev Steinhardt</a></div></div></div></div></div></div><span class="q-text qu-mx--tiny qu-color--gray qu-fontSize--small" style="color: #636466; direction: ltr; font-size: 13px; margin-left: 4px; margin-right: 4px;">·</span><div class="q-text qu-color--gray qu-fontSize--small qu-passColorToLinks qu-truncateLines--1"><a class="q-box qu-cursor--pointer qu-hover--textDecoration--underline" href="https://www.quora.com/Has-a-play-ever-happened-in-baseball-that-was-so-out-of-the-ordinary-that-no-written-umpiring-rule-at-the-time-covered-it/answer/Zev-Steinhardt" target="_top">July 9, 2019</a></div></div><div class="q-flex qu-flexWrap--wrap" style="direction: ltr; display: flex; margin-top: 2px;"><div class="q-text qu-truncateLines--2 qu-color--gray qu-passColorToLinks qu-fontSize--small">Studied at <span class="TopicName___StyledSpan-t3tegb-0 crUglW">Pace University</span></div></div></div></div></div></div><div class="q-box qu-pl--tiny" style="direction: ltr; margin-left: auto; padding-left: 4px;"><div class="q-relative qu-size--18" style="direction: ltr; height: 18px; width: 18px;"><div class="q-absolute"><div class="q-box qu-display--inline-block" style="direction: ltr; display: inline-block;"><div class="q-relative" style="direction: ltr;"><div class="q-click-wrapper qu-active--bg--darken qu-active--textDecoration--none qu-focus--bg--darken qu-focus--textDecoration--none qu-borderRadius--pill qu-whiteSpace--nowrap qu-display--inline-block qu-tapHighlight--white qu-textAlign--center qu-cursor--pointer qu-hover--bg--darken qu-hover--textDecoration--none" tabindex="0"><div class="q-flex qu-alignItems--center qu-justifyContent--center" style="direction: ltr; display: flex;"><div class="q-relative qu-display--flex qu-alignItems--center" style="direction: ltr; display: flex;"><span class="q-inlineBlock qu-verticalAlign--text-bottom" name="SmallClose" style="direction: ltr; display: inline-block; height: 24px; line-height: 0; vertical-align: text-bottom; width: 24px;"><span class="CssComponent__CssInlineComponent-sc-1oskqb9-1 Icon___StyledCssInlineComponent-sc-11tmcw7-0 eXDwse"/></span></div></div></div></div></div></div></div></div></div><div class="q-flex" style="direction: ltr; display: flex;"/><div class="q-flex qu-mb--tiny" style="direction: ltr; display: flex; margin-bottom: 4px;"><div class="q-text qu-bold qu-color--gray_dark_dim qu-passColorToLinks qu-userSelect--text qu-lineHeight--regular" style="direction: ltr; font-size: 16px; font-weight: bold; line-height: 1.4;"><span class="CssComponent__CssInlineComponent-sc-1oskqb9-1 TitleText___StyledCssInlineComponent-sc-1hpb63h-0 jPnwvF"><a class="q-box qu-cursor--pointer qu-hover--textDecoration--underline" href="https://www.quora.com/Has-a-play-ever-happened-in-baseball-that-was-so-out-of-the-ordinary-that-no-written-umpiring-rule-at-the-time-covered-it" target="_blank"><div class="q-flex qu-flexDirection--row" style="direction: ltr; display: flex;"><div class="q-inline qu-flexWrap--wrap" style="direction: ltr; display: inline;"><div class="q-text puppeteer_test_question_title" style="direction: ltr;"><span class="q-box qu-userSelect--text" style="direction: ltr;">Has a play ever happened in baseball that was so out of the ordinary that no written umpiring rule at the time covered it?</span></div></div></div></a></span></div></div><div class="q-relative spacing_log_answer_content" style="direction: ltr;"><div class="q-text" style="direction: ltr;"><span class="q-box qu-userSelect--text" style="direction: ltr;"><p class="q-text qu-display--block" style="direction: ltr; margin: 0px 0px 1em; padding: 0px;">Back in 2008, the Yankees drafted a pitcher named Pat Venditte. What made Venditte unusual is that he can throw with both hands. In other words, he’s a switch pitcher. When he was drafted, he was assigned to the Staten Island Yankees, a low A ball team.</p><p class="q-text qu-display--block" style="direction: ltr; margin: 0px 0px 1em; padding: 0px;">In his first game (against the Mets farm team, the Brooklyn Cyclones), Venditte came in to pitch. After getting the first two batters out and giving up a single, he then faced Ralph Henriquez, was a switch hitter. What happened next resembled an Abbott and Costello comedy routine. Venditte would put the glove on one hand (he had a specially made glove that could be worn on either hand) and Henriquez would then step across the plate to bat from the other side. Venditte would then switch his glove hand again and Henriquez went back to the other side.</p><p class="q-text qu-display--block" style="direction: ltr; margin: 0px 0px 1em; padding: 0px;">Eventually, after much discussion, the umpires ruled that Henriquez would have to choose a batting side first, before Venditte had to commit. Henriquez was mad and, after he struck out, he slammed the bat against the ground in frustration.</p><p class="q-text qu-display--block" style="direction: ltr; margin: 0px 0px 1em; padding: 0px;">The umpires were, in essence, winging it, because there was no rule to cover the situation. Eventually, the higher ups in baseball did write a rule to cover the situation — the opposite of the umpires’ decision.</p></span></div></div></div></div></div></div></div></div></div><p><br/></p><p><br/></p></div>
    </content>
    <updated>2020-09-20T18:56:00Z</updated>
    <published>2020-09-20T18:56:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-26T13:46:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/142</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/142" rel="alternate" type="text/html"/>
    <title>TR20-142 |  Relaxed Locally Correctable Codes with Improved Parameters | 

	Vahid Reza Asadi, 

	Igor Shinkar</title>
    <summary>Locally decodable codes (LDCs) are error-correcting codes $C : \Sigma^k \to \Sigma^n$ that admit a local decoding algorithm that recovers each individual bit of the message by querying only a few bits from a noisy codeword. An important question in this line of research is to understand the optimal trade-off between the query complexity of LDCs and their block length. Despite importance of these objects, the best known constructions of constant query LDCs have super-polynomial length, and there is a significant gap between the best constructions and the known lower bounds in terms of the block length.

For many applications it suffices to consider the weaker notion of relaxed LDCs (RLDCs), which allows the local decoding algorithm to abort if by querying a few bits it detects that the input is not a codeword. This relaxation turned out to allow decoding algorithms with constant query complexity for codes with almost linear length. Specifically, [Ben+06] constructed an $O(q)$-query RLDC that encodes a message of length $k$ using a codeword of block length $n = O(k^{1+1/\sqrt{q}})$.

In this work we improve the parameters of [Ben+06] by constructing an $O(q)$-query RLDC that encodes a message of length $k$ using a codeword of block length $O(k^{1+1/{q}})$. This construction matches (up to a multiplicative constant factor) the lower bounds of [KT00; Woo07] for constant query LDCs, thus making progress toward understanding the gap between LDCs and RLDCs in the constant query regime.

In fact, our construction extends to the stronger notion of relaxed locally correctable codes (RLCCs), introduced in [GRR18], where given a noisy codeword the correcting algorithm either recovers each individual bit of the codeword by only reading a small part of the input, or aborts if the input is detected to be corrupt.</summary>
    <updated>2020-09-20T18:00:19Z</updated>
    <published>2020-09-20T18:00:19Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-26T15:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://mycqstate.wordpress.com/?p=1244</id>
    <link href="https://mycqstate.wordpress.com/2020/09/20/announcing-a-short-course-in-paris/" rel="alternate" type="text/html"/>
    <title>Announcing a short course in Paris</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This coming academic year I am on sabbatical, in Paris. It’s certainly a funny year to be on sabbatical. (It’s a funny year to be doing anything, isn’t it? Or is “funny” not the appropriate word…Yet I can’t find any … <a href="https://mycqstate.wordpress.com/2020/09/20/announcing-a-short-course-in-paris/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This coming academic year I am on sabbatical, in Paris. It’s certainly a funny year to be on sabbatical. (It’s a funny year to be doing anything, isn’t it? Or is “funny” not the appropriate word…Yet I can’t find any other way to look at it that doesn’t send me straight into the abyss. So, let it be “funny”—knowing that, no, I’m not actually laughing right now.) On the one hand, I am lucky to have escaped the incessant debates on the format of teaching, how many people per square foot are allowed in each building on campus, what distance I should stay from my students were I to attempt to meet them in person, and so many other similar decisions that have come to take up a larger and larger fraction of our professional lives (not to mention of course the incommensurate challenges that many are facing at the personal and familial level). On the other hand, the situation makes it much harder to meet others and engage in new collaborations, one of the goals of my sabbatical. I’ll see how it plays out; I’ll be sure to write more on this blog as time progresses.</p>



<p>During the sabbatical I am being hosted successively by different French institutions. For the first 6 months I had the good fortune of being awarded a “chair” from the “<a href="https://www.sciencesmaths-paris.fr/en/">Fondation Sciences Mathématiques de Paris</a>” (FSMP), a private foundation which supports, in very general terms, the development of the mathematics community in Paris, from the organization of general-public conferences to the support of research collaborations. My only formal obligation during these 6 months is to give 20 hours of lecture on a theme of my choosing. The goal that I elected for the course is provide an in-depth introduction to two major works in quantum complexity and cryptography of the past few years: first, Mahavev’s 2018 result on <a href="https://arxiv.org/abs/1804.01082">classical verification of quantum computation</a> (a result for which I already shared my enthusiasm <a href="https://mycqstate.wordpress.com/2018/08/06/the-cryptographic-leash/">here</a>); second, my result <a href="https://arxiv.org/abs/2001.04383">MIP*=RE</a> with Ji, Natarajan, Wright and Yuen on the power of quantum multi-prover interactive proof systems, which I mentioned in the <a href="https://mycqstate.wordpress.com/2020/01/14/a-masters-project/">previous post</a>, and its consequences. For more about the course, including a tentative breakdown of lectures and some resources, see the <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/">course webpage</a>. </p>



<p>While at the time of writing the course is still scheduled to start as an in-person meeting (to take place in a very large layered amphitheater with ample space for social distancing), there is no telling how the situation, and regulations, will evolve in the near future. To accommodate participants who are unable or prefer not to travel in person, all lectures starting with the first one will be recorded. In addition I will post course materials, including lecture notes, <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/">here</a>. The purpose of this post is to advertise the course: participants from everywhere are welcome to watch the recorded videos, read the notes, and write to me with any questions in suggestions. In particular I plan to outsource the proof-reading of the notes via overleaf and I welcome any participant’s interest in helping with that; draft notes for the first lecture are already available <a href="https://www.overleaf.com/2293291658twkjfbtctsdb">here</a>. Anyone is welcome to make direct corrections, or add inline comments pointing to issues that may need my attention.</p>



<p>The program that I chose is ambitious, and we will see how far we get along. My goal is to start slow, so as to remain inclusive with respect to varying backgrounds in computer science, mathematics or physics. At first I will give complete definitions, state and prove simple lemmas, etc., in order to establish common language. As time progresses I expect that things will become a little more high-level, less self-contained, and more technical. Depending on your background and interests, you may find the first few lectures, or the last few ones, more interesting. Teaching the course will certainly be beneficial for me because I believe that there is a strong unity behind the two works I chose to present. I hope to make that unity apparent by presenting them together. Moreover, both works introduce new techniques that leave many avenues open; I hope that a “clean” presentation will help me, and others, build on them. </p>



<p>A side benefit of an “un-necessary” course such as this one is that it contributes to bringing a certain community together. (By “un-necessary” I mean that the course will not be required for any curriculum; if it did not take place, as long as it was replaced by other research-level activities its absence would not be felt.) COIVD-19 unfortunately turns that opportunity into a challenge. It is because of it that I insist–regulations allowing– on having the course take place in person: as much as we are getting used to Zoom, and as well as it may be working as a replacement for many aspects of our interactive lives, from in-person classes to conferences to research collaborations, a scientific event such as this one, with sustained involvement by a small set of participants coming from distant backgrounds, is probably one of the more challenging ones to make work online. I hope it doesn’t come to that. Even if it does, one of the lessons learned from the Spring 2020 semester on quantum computing at the Simons Institute in Berkeley, which was interrupted half-ways due to the pandemic, is that having an initial in-person phase was of great help to cement future online interactions. So, I hope that I am able to lecture on Tuesday; after that, we will see.</p></div>
    </content>
    <updated>2020-09-20T15:19:18Z</updated>
    <published>2020-09-20T15:19:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Thomas</name>
    </author>
    <source>
      <id>https://mycqstate.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://mycqstate.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://mycqstate.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://mycqstate.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://mycqstate.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>In superposition</subtitle>
      <title>MyCQstate</title>
      <updated>2020-09-26T15:21:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5573</id>
    <link href="https://adamsheffer.wordpress.com/2020/09/19/combinatorial-journals-are-changing/" rel="alternate" type="text/html"/>
    <title>Combinatorial Journals are Changing</title>
    <summary>I used to ask most combinatorialists I met for their opinion about the level of various journals. With this feedback, I compiled a rough journal ranking for combinatorics papers (for personal use). This was a very educational experience for me as a new combinatorialist. I learned that different people have rather different opinions. For example, […]</summary>
    <updated>2020-09-19T21:46:10Z</updated>
    <published>2020-09-19T21:46:10Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2020-09-26T15:21:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2020/09/19/beyondlogconvavesampling/</id>
    <link href="http://offconvex.github.io/2020/09/19/beyondlogconvavesampling/" rel="alternate" type="text/html"/>
    <title>Beyond log-concave sampling</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As the growing number of posts on this blog would suggest, recent years have seen a lot of progress in understanding optimization beyond convexity. However, optimization is only one of the basic algorithmic primitives in machine learning — it’s used by most forms of risk minimization and model fitting. Another important primitive is sampling, which is used by most forms of inference (i.e. answering probabilistic queries of a learned model).</p>

<p>It turns out that there is a natural analogue of convexity for sampling — <em>log-concavity</em>. Paralleling the state of affairs in optimization, we have a variety of (provably efficient) algorithms for sampling from log-concave distributions, under a variety of access models to the distribution. Log-concavity, however, is very restrictive and cannot model common properties of distributions we frequently wish to sample from in machine learning applications, for example multi-modality and manifold structure in the level sets, which is what we’ll focus on in this and the upcoming post.</p>

<p>Unlike non-convex optimization, the field of sampling beyond log-concavity is very nascent. In this post, we will survey the basic tools and difficulties for sampling beyond log-concavity. In the next post, we will survey recent progress in this direction, in particular with respect to handling multi-modality and manifold structure in the level sets, covering the papers <a href="https://arxiv.org/abs/1812.00793">Simulated tempering Langevin Monte Carlo</a> by Rong Ge, Holden Lee, and Andrej Risteski and <a href="https://arxiv.org/abs/2002.05576">Fast convergence for Langevin diffusion with matrix manifold structure</a> by Ankur Moitra and Andrej Risteski.</p>

<h1 id="formalizing-the-sampling-problem">Formalizing the sampling problem</h1>

<p>The formulation of the sampling problem we will consider is as follows:</p>

<blockquote>
  <p><strong>Problem</strong>: Sample from a distribution $p(x) \propto e^{-f(x)}$ given black-box access to $f$ and $\nabla f$.</p>
</blockquote>

<p>This formalization subsumes a lot of inference tasks involving different kinds of probabilistic models. We give several common examples:</p>

<p><em>1.Posterior inference</em>: Suppose our data is generated from a model with <em>unknown</em> parameters $\theta$ , such that the data-generation process is given by $p(x \mid \theta)$ and we have a prior $p(\theta)$ over the model parameters. Then the <em>posterior distribution</em> $p(\theta \mid x)$ , by Bayes’s Rule, is given by</p>

\[p(\theta \mid x) = \frac{p(x \mid \theta)p(x)}{p(x)}\propto p(x \mid \theta)p(\theta).\]

<p>A canonical example of this is a <em>noisy inference task</em> where a signal (parametrized by $\theta$ ) is perturbed by noise (as specified by $p(x \mid \theta)$ ).</p>

<p><em>2.Posteriors in latent-variable models</em>: If the data-generation process has a <em>latent (hidden) variable</em> $h$ associated to each data point, such that $h$ has a <em>known</em> prior $p(h)$ and a <em>known</em> conditional $p_\theta(x \mid h)$ , then again by Bayes’s rule, we have</p>

\[p_\theta(h \mid x) = \frac{p_\theta(x \mid h)p_\theta(h)}{p_\theta(x)}\propto p_\theta(x \mid h)p_\theta(h).\]

<p>In typical latent-variable models, $p_\theta(x \mid h)$ and $p_\theta(h)$ have a simple parametric form, which makes it easy to evaluate $p_\theta(x \mid h)p_\theta(h)$ . Some examples of latent-variable models are mixture models (where $h$ encodes which component a sample came from), topic models (where $h$ denote the topic proportions in a document), and noisy-OR networks (and latent-variable Bayesian belief networks).</p>

<p><em>3.Sampling from energy models</em>: in energy models, the distribution of the data is parametrized as $p(x) \propto \exp(-E(x))$ for some <em>energy</em> function $E(x)$ which is smaller on points in the data distribution. Recent works by <a href="https://arxiv.org/abs/1907.05600">(Song, Ermon 2019)</a> and <a href="https://arxiv.org/abs/1903.08689">(Du, Mordatch 2019)</a> have scaled up the training of these models on images so that the visual quality of the samples they produce is comparable to that of more popular generative models like GANs and flow models.</p>

<p>The “exponential form” $e^{-f(x)}$ is also helpful in making an analogy to optimization. Namely, if we sample from $p(x)\propto e^{-f(x)}$, a particular point $x$ is more likely to be sampled if $f(x)$ is small. The key difference between with optimization is that while in optimization, we only want to get to the minimum, in sampling, we want to pick points with the correct probabilities.</p>

<h1 id="comparison-with-optimization">Comparison with optimization</h1>

<p>The computational hardness landscape for our sampling problem parallels the one for black-box optimization, in which the goal is to find the minimum of a function $f$, given value/gradient oracle access. When $f$ is <em>convex</em>, there is a unique local minimum, so that local search algorithms like <em>gradient descent</em> are efficient. When $f$ is non-convex, gradient descent can get trapped in potentially poor local minima, and in the worst case, an exponential number of queries is needed.</p>

<p>Similarly, for sampling, when $p$ is <em>log-concave</em>, the distribution is unimodal and a Markov Chain which is a close relative of gradient descent — <em>Langevin Monte Carlo</em> —  is efficient. When $p$ is non-log-concave, Langevin Monte Carlo can get trapped in one of many modes, and and exponential number of queries may also be needed.</p>

<blockquote>
  <p>A distribution $p(x)\propto e^{-f(x)}$ is <strong>log-concave</strong> if $f(x) = -\log p(x)$ is convex. It is $\alpha$-strongly log-concave if $f(x)$ is $\alpha$-strongly convex.</p>
</blockquote>

<p>However, such worst-case hardness rarely stop practitioners from trying to solve the non-convex optimization or non-log-concave sampling problems which are ubiquitous in modern machine learning. Often they manage to do so with great success - for instance, in training deep neural networks, gradient descent and its relatives perform quite well. Similarly, Langevin Monte Carlo and its relatives can do quite well on non-log-concave problems, though they sometimes need to be aided by temperature heuristics and other tricks.</p>

<p>As theorists, we’d like to develop theory that will lead to a better understanding of why and when these heuristics work. Just like we’ve done for optimization, we need to be guided both by hardness results and relevant structure of real-world problems in this endeavour.</p>

<p>The following table summarizes the comparisons we have come up with:</p>

<p><img alt="" src="http://www.andrew.cmu.edu/user/aristesk/table_opt.jpg"/></p>

<p>Before we move on to non-log-concave distributions, though, we need to understand the basic algorithm for sampling and its guarantees for log-concave distributions.</p>

<h1 id="langevin-monte-carlo">Langevin Monte Carlo</h1>

<p>Just as gradient descent is the canonical algorithm for optimization, <em>Langevin Monte Carlo</em> (LMC) is the canonical algorithm for our sampling problem. In a nutshell, it is gradient descent that also injects Gaussian noise:</p>

\[\text{Gradient descent:}\quad 
x_{t+\eta} = x_t - \eta \nabla f(x_t)\]

\[\text{Langevin Monte Carlo:}\quad
x_{t+\eta} = x_t - \eta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I)\]

<p>Both of these processes can be considered as discretizations of a continuous process. For gradient descent, the limit is an <em>ordinary differential equation</em>, and for Langevin Monte Carlo a <em>stochastic differential equation</em>:</p>

\[\text{Gradient flow:} \quad dx_t = -\nabla f(x_t) dt\]

\[\text{Langevin diffusion:} \quad dx_t = -\nabla f(x_t) dt + \sqrt{2} dB_t\]

<p>where $B_t$ denotes Brownian motion of the appropriate dimension.</p>

<p>The crucial property of the above stochastic differential equation is that under fairly mild assumptions on $f$, the stationary distribution is $p(x) \propto e^{-f(x)}$. (If you’re more comfortable with optimization, note that while gradient descent generally converges to (local) minima, the Gaussian noise term prevents LMC from converging to a single point - rather, it converges to a <em>stationary distribution</em>. See animation below.)</p>

<p><img alt="" src="http://www.andrew.cmu.edu/user/aristesk/gd_ld_animated.gif"/></p>

<p>Langevin Monte Carlo fits in the <em>Markov Chain Monte Carlo</em> (MCMC) paradigm: design a random walk, so that the stationary distribution is the desired distribution. “Mixing” means getting close to the stationary distribution, and rapid mixing means this happens quickly.</p>

<p>Like in optimization, Langevin Monte Carlo is the most “basic” algorithm: for example, one can incorporate “acceleration” and obtain <em>underdamped</em> Langevin, or use the physics-inspired Hamiltonian Monte Carlo.</p>

<h1 id="tools-for-bounding-mixing-time-challenges-beyond-log-concavity">Tools for bounding mixing time, challenges beyond log-concavity</h1>

<p>To illustrate the difficulty in moving beyond log-concavity, we’ll describe the tools that are used to prove fast mixing for log-concave distributions, and where they fall short for non-log-concave distributions.</p>

<p>We will do this by an analogy to how we analyze random walks on graphs. One common way to prove rapid mixing of a random walk on a graph is to show the Laplacian has a spectral gap (equivalently, the transition matrix has a gap between the largest and next-to-largest eigenvalue). The analogue of this for Langevin diffusion is showing a <em>Poincaré inequality</em>. (A spectral gap of $1/C$ corresponds to Poincaré constant of $C$.)</p>

<blockquote>
  <p>We say that $p(x)$ satisfies a <strong>Poincaré inequality</strong> with constant $C$ if for all functions $g$ on $\mathbb R^d$ (such that $g$ and $\nabla g$ are square-integrable with respect to $p$),</p>
  <div> $$\text{Var}_p(g) \le C \int_{\mathbb R^d} ||\nabla g(x)||^2 p(x)\,dx.$$ </div>
</blockquote>

<p>A small constant $C$ implies fast mixing in $\chi^2$ divergence, which implies fast mixing in total variation distance. More precisely, the mixing time for Langevin diffusion is on the order of $C$. We note that other functional inequalities imply mixing with respect to other measures (such as log-Sobolev inequalities for KL divergence).</p>

<p>While it may not be obvious what the Poincaré inequality has to do with a spectral gap, it turns out that we can think of the right-hand side as a quadratic form involving the <em>infinitesimal generator</em> of Langevin process, which functions as the continuous analogue of a Laplacian for a graph random walk.</p>

<p>The following table shows the analogy: we can put the discrete and continuous processes on the same footing by defining a quadratic form called the Dirichlet form from the Laplacian or infinitesimal generator.</p>

<p><img alt="" src="http://www.andrew.cmu.edu/user/aristesk/table_mixing.jpg"/></p>

<p>To see how the Poincaré inequality represents a spectral gap in the discrete case, we write it in a more explicit form in a familiar special case: a lazy random walk (i.e. a random walk that with probability $1/2$ stays in the current vertex, and with probability $1/2$ goes to a random neighbor) on a regular graph with $n$ vertices. In this case, $p$ is the uniform distribution, and $v_1=\mathbf 1,\ldots, v_n$ are the eigenvectors of $A$ with eigenvalues $1=\lambda_1\ge \lambda_2\ge \cdots \ge \lambda_n\ge 0$; normalize $v_1,\ldots, v_n$ so they have unit norm with respect to $p$, i.e. $\Vert v_i\Vert_p^2=\frac 1n\sum_j v_{ij}^2=1$.</p>

<p>Writing $g= \sum_i a_i v_i$, since $v_2,\ldots, v_n$ are orthogonal to $v_1=\mathbf 1$, we have $\langle g, \mathbf 1\rangle_p =  a_1$, so</p>

\[\text{Var}_p(g) = \frac{1}{n}(\sum_i  g_i^2) - a_1^2 = \sum_{i=2}^n a_i^2\]

<p>Furthermore, we have</p>

\[\langle g, Lg \rangle_p = \langle \sum_i a_iv_i, (I- A)(\sum_i a_iv_i)\rangle_p=  \sum_{i=2}^n a_i^2(1-\lambda_i)\]

<p>These coefficients are all at most $1-\lambda_2$, i.e. the <em>spectral gap</em>, so</p>

\[\langle g, Lg \rangle_p \ge (1-\lambda_2)\text{Var}_p(g),\]

<p>which shows the Poincaré inequality with constant $(1-\lambda_2)^{-1}$.</p>

<p>A classic theorem establishes a Poincaré inequality for (strongly) log-concave distributions.</p>

<blockquote>
  <p><strong>Theorem (Bakry, Emery 1985)</strong>: If $p(x)$ is $\alpha$-strongly log-concave, then $p(x)$ satisfies a Poincaré inequality with constant $\frac1{\alpha}$.</p>
</blockquote>

<p>Hence, for strongly-log-concave distributions, Langevin diffusion mixes rapidly. To complete the picture, a line of recent works, starting with <a href="https://arxiv.org/abs/1412.7392">(Dalalyan 2014)</a> have established bounds for discretization error to obtain algorithmic guarantees for Langevin Monte Carlo.</p>

<p>However, guarantees break down when we don’t assume log-concavity. Generically, algorithms for sampling depend <em>exponentially</em> on the ambient dimension $d$, or on the “size” of the non-log-concave region (e.g., the distance between modes of the distribution). In terms of their dependence on $d$, they are not doing much better than if we split space into cells and sample each according to its probability, similar to “grid search” for optimization. This is unsurprising: we can’t hope for better guarantees without structural assumptions.</p>

<p>Toward this end, in the next blog post we will consider two kinds of structure that allow efficient sampling:</p>

<ol>
  <li>Simple multimodal distributions, such as a mixture of gaussians with equal variance.</li>
  <li>Manifold structure, arising from symmetries in the level sets of the distribution.</li>
</ol></div>
    </summary>
    <updated>2020-09-19T14:00:00Z</updated>
    <published>2020-09-19T14:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2020-09-25T23:29:08Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-09-19-living-with-asynchrony-brachas-reliable-broadcast/</id>
    <link href="https://decentralizedthoughts.github.io/2020-09-19-living-with-asynchrony-brachas-reliable-broadcast/" rel="alternate" type="text/html"/>
    <title>Living with Asynchrony: Bracha's Reliable Broadcast</title>
    <summary>In this series of posts, we explore what can be done in the Asynchronous model. This model seems challenging because the adversary can delay messages by any bounded time. By the end of this series, you will see that almost everything that can be done in synchrony can be obtained...</summary>
    <updated>2020-09-19T13:05:00Z</updated>
    <published>2020-09-19T13:05:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-09-26T15:21:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=457</id>
    <link href="https://tcsplus.wordpress.com/2020/09/18/tcs-talk-wednesday-september-23-fotis-iliopoulos-princeton-and-ias/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, September 23 — Fotis Iliopoulos, Princeton and IAS</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, September 23th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Fotis Iliopoulos from Princeton and IAS will speak about “Stochastic Local Search and the Lovász Local Lemma” (abstract below). You can reserve a spot as an individual or […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, September 23th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Fotis Iliopoulos</strong> from Princeton and IAS will speak about “<em>Stochastic Local Search and the Lovász Local Lemma</em>” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<blockquote class="wp-block-quote"><p>Abstract: The Lovasz Local Lemma (LLL) is a powerful tool in probabilistic combinatorics which can be used to establish the existence of objects that satisfy certain properties. The breakthrough of Moser and Tardos (who recently received the Godel Prize for their work) and follow-up works revealed that the LLL has intimate connections with a class of stochastic local search algorithms for finding such desirable objects.<br/><br/>In this talk, I will survey this line of work through the perspective of recent unifying results, and also talk about recent applications to solving pseudo-random constraint satisfaction problems.</p></blockquote>



<p/></div>
    </content>
    <updated>2020-09-18T16:22:59Z</updated>
    <published>2020-09-18T16:22:59Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-09-26T15:21:36Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/private-pac/</id>
    <link href="https://differentialprivacy.org/private-pac/" rel="alternate" type="text/html"/>
    <title>Differentially Private PAC Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The study of differentially private PAC learning runs all the way from
its introduction in 2008 <a href="https://arxiv.org/abs/0803.0924" title="Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What Can We Learn Privately? FOCS 2008"><strong>[KLNRS08]</strong></a> to a best paper award at the
Symposium on Foundations of Computer Science (FOCS) this year <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a>.
In this post, we’ll recap the history of this line of work, aiming for
enough detail for a rough understanding of the results and methods.</p>

<p>Before we get to the “what” and “how” of private PAC learning, it’s
worth thinking about the “why”. One motivation for this line of work is
that it neatly captures a fundamental question: does privacy in machine
learning come at a price? Machine learning is now sufficiently
successful and widespread for this question to have real import. But to
even start to address this question, we need a formalization of machine
learning that allows us to reason about possible trade-offs in a
rigorous way. Statistical learning theory, and its computational
formalization as PAC learning, provide one such clean and well-studied
model. We can therefore use PAC learning as a testbed whose insights we
might carry to other less idealized forms of learning.</p>

<p>With this motivation in mind, the rest of this post is structured as
follows. The first section covers the basics of the PAC model, and
subsequent sections gradually build up a chronology of results. When
possible, we give short sketches of the accompanying techniques.</p>

<h1 id="pac-learning">PAC Learning</h1>

<p>We’ll start with a brief overview of PAC learning absent any privacy
restrictions. Readers familiar with PAC learning can probably skip this
section while noting that</p>

<ol>
  <li>
    <p>(the cardinality version of) Occam’s razor is a baseline learner
using \(O(\log|\mathcal{H}|)\) samples,</p>
  </li>
  <li>
    <p>VC dimension characterizes non-private PAC learning,</p>
  </li>
  <li>
    <p>we’ll focus on the sample complexity of realizable PAC learning,</p>
  </li>
  <li>
    <p>we’ll usually omit dependencies on accuracy and success probability
parameters, and</p>
  </li>
  <li>
    <p>we’ll usually ignore computational efficiency.</p>
  </li>
</ol>

<p>For readers needing a refresher on PAC learning, the basic element of
the “probably approximately correct” (PAC) framework <a href="https://dl.acm.org/doi/10.1145/1968.1972" title="Leslie G Valiant. A theory of the learnable. Communications of the ACM, 1984"><strong>[Val84]</strong></a> is a
<em>hypothesis</em>. Each hypothesis is a function
\(h \colon \mathcal{X}\to \{-1,1\}\) mapping <em>examples</em> from some space
\(\mathcal{X}\) to binary labels. A collection of hypotheses is a
<em>hypothesis class</em> \(\mathcal{H}\), e.g., thresholds (a.k.a. perceptrons),
rectangles, conjunctions, and so on. In the <em>realizable</em> setting, a
learner receives examples drawn from some unknown distribution and
labeled by an unknown \(h^\ast \in \mathcal{H}\). The learner’s goal is to
with high probability (“probably”) output a hypothesis that mostly
matches the labels of \(h^\ast\) on future examples from the unknown example
distribution (“approximately correct”). In the <em>agnostic</em> setting,
examples are not necessarily labeled by any \(h
\in \mathcal{H}\), and the goal is only to output a hypothesis that
approximates the best error of any hypothesis from \(\mathcal{H}\). As
mentioned above, we focus on the realizable setting unless otherwise
specified. In the <em>proper</em> setting, the learner must output a hypothesis
from \(\mathcal{H}\) itself. In the <em>improper</em> setting, this requirement
is removed.</p>

<p>In general, we say an algorithm \((\alpha,\beta)\)-PAC learns
\(\mathcal{H}\) with sample complexity \(n\) if \(n\) samples are sufficient
to with probability at least \(1-\beta\) obtain error at most \(\alpha\)
over new examples from the distribution. For the purposes of this post,
we generally omit these dependencies on \(\alpha\) and \(\beta\), as they
typically vary little or not at all when switching between non-private
and private PAC learning.</p>

<p>Fortunately, we always have a simple baseline learner based on empirical
risk minimization: given a set of labeled examples, iterate over all
hypotheses \(h \in \mathcal{H}\), check how many of the labeled examples
each \(h\) mislabels, and output a hypothesis that mislabels the fewest
examples. Using this learner, which is sometimes called “Occam’s razor,”
\(O(\log|\mathcal{H}|)\) samples suffice to PAC learn \(\mathcal{H}\).</p>

<p>At the same time, \(|\mathcal{H}|\) is a pretty coarse measure of
hypothesis class complexity, as it would immediately rule out learning
any infinite hypothesis class (of which there are many). Thus, as you
might expect, we can do better. We do so using <em>VC dimension</em>.
\(\mathsf{VCD}\left(\mathcal{H}\right)\) is the size of the largest
possible collection of examples such that, for every labeling of the
examples, \(\mathcal{H}\) contains a hypothesis with that labeling. With
VC dimension, we can essentially swap \(\log|\mathcal{H}|\) with
\(\mathsf{VCD}\left(\mathcal{H}\right)\) in the Occam’s razor bound and
PAC learn with \(O(\mathsf{VCD}\left(\mathcal{H}\right))\) samples. In
fact, the “Fundamental Theorem of Statistical Learning” says that PAC
learnability (realizable or agnostic) is equivalent to finite VC
dimension. In this sense, \(\mathsf{VCD}\left(\mathcal{H}\right)\) is a
good measure of how hard it is to PAC learn \(\mathcal{H}\). As a
motivating example that will re-appear later, note that for the
hypothesis class of 1-dimensional thresholds over \(T\) points,
\(\log |\mathcal{H}| = \log T\), while
\(\mathsf{VCD}\left(\mathcal{H}\right)\) is only 1.</p>

<p><img alt="Example: a one-dimensional threshold function" src="https://differentialprivacy.org/images/thresh.png" style="margin: auto; display: block;" width="400"/>
An illustration of 1-dimensional thresholds. A given threshold is determined by some point \(x^\ast \in [T]\): any example \(x \leq x^\ast\) receives label \(-1\), and any example \(x &gt; x^\ast\) receives label 1.</p>

<h1 id="a-simple-private-pac-learner">A Simple Private PAC Learner</h1>

<p>It is straightforward to add a differential privacy constraint to the
PAC framework: the hypothesis output by the learner must be a
differentially private function of the labeled examples
\((x_1, y_1), \ldots, (x_n, y_n)\). That is, changing any one of the
examples — even to one with an inconsistent label — must not affect
the distribution over hypotheses output by the learner by too much.</p>

<p>Since we haven’t talked about any other PAC learner, we may as well
start with the empirical risk minimization-style Occam’s razor discussed
in the previous section, which simply selects a hypothesis that
minimizes empirical error. A private version becomes easy if we view
this algorithm in the right light. All it is doing is assigning a score
to each possible output (the hypothesis’ empirical error) and outputting
one with the best (lowest) score. This makes it a good candidate for
privatization by the <em>exponential mechanism</em> <a href="https://dl.acm.org/doi/10.1109/FOCS.2007.41" title="Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007."><strong>[MT07]</strong></a>.</p>

<p>Recall that the exponential mechanism uses a scoring function over
outputs to release better outputs with higher probability, subject to
the privacy constraint. More formally, the exponential mechanism
requires a scoring function \(u(X,h)\) mapping (database, output) pairs to
real-valued scores and then selects a given output \(h\) with probability
proportional to \(\exp\left(\tfrac{\varepsilon
u(X,h)}{2\Delta(u)}\right)\). Thus a lower \(\varepsilon\) (stricter
privacy requirement) and larger \(\Delta(u) := \sup_h \sup_{X \sim X’} u(X,h) - u(X’,h) \) (scoring function more sensitive to changing one element in the database \(X\) to make \(X’\)) both lead to a more uniform (more
private) output distribution.</p>

<p>Fortunately for our PAC learning setting, empirical error is not a very
sensitive scoring function: changing one sample only changes empirical
error by 1. We can therefore use (negative) empirical error as our
scoring function \(u(X,h)\), apply the exponential mechanism, and get a
“private Occam’s razor.” This was exactly what Kasiviswanathan, Lee,
Nissim, Raskhodnikova, and Smith <a href="https://arxiv.org/abs/0803.0924" title="Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What Can We Learn Privately? FOCS 2008"><strong>[KLNRS08]</strong></a> did when they introduced
differentially private PAC learning in 2008. The resulting sample
complexity bounds differ from the generic Occam’s razor only by an
\(\varepsilon\) factor in the denominator, and
\(O(\log|\mathcal{H}|/\varepsilon)\) samples suffice to privately PAC
learn \(\mathcal{H}\).</p>

<p>Of course, our experience with non-private PAC learning suggests that we
shouldn’t be satisfied with this \(\log
|\mathcal{H}|\) dependence. Maybe VC dimension characterizes private PAC
learning, too?</p>

<h1 id="characterizing-pure-private-pac-learning">Characterizing Pure Private PAC Learning</h1>

<p>As it turns out, answering this question will take some time. We start
with a partial negative answer. Specifically, we’ll see a class with VC
dimension 1 and (a restricted form of) private sample complexity
arbitrarily larger than 1. We’ll also cover the first in a line of
characterization results for private PAC learning.</p>

<p>We first consider learners that satisfy <em>pure</em> privacy. Recall that pure
\((\varepsilon,0)\)-differential privacy forces output distributions that
may only differ by a certain \(e^\varepsilon\) multiplicative factor (like
the exponential mechanism above). The strictly weaker notion of
approximate \((\varepsilon,\delta)\)-differential privacy also allows a
small additive \(\delta\) factor. Second, we restrict ourselves to
<em>proper</em> learners, which may only output hypotheses from the learned
class \(\mathcal{H}\).</p>

<p>With these assumptions in place, in 2010, Beimel, Kasiviswanathan, and
Nissim <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> studied a hypothesis class called \(\mathsf{Point}_d\).
\(\mathsf{Point}_d\) consists of \(2^d\) hypotheses, one for each vector in
\(\{0,1\}^d\). Taking the set of examples \(\mathcal{X}\) to be \(\{0,1\}^d\)
as well, we define each hypothesis in \(\mathsf{Point}_d\) to label only
its associated vector as 1, and the remaining \(2^d-1\) examples as
-1. <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> showed that the hypothesis class \(\mathsf{Point}_d\) requires
\(\Omega(d)\) samples for proper pure private PAC learning. In contrast,
\(\mathsf{VCD}\left(\mathsf{Point}_d\right) = 1\), so this \(\Omega(d)\)
lower bound shows us that VC dimension does <em>not</em> characterize proper
pure private PAC learning.</p>

<p>This result uses the classic “packing” lower bound method, which powers
many lower bounds for pure differential privacy. The general packing
method is to first construct a large collection of databases which are
all “close enough” to each other but nonetheless all have different
“good” outputs. Once we have such a collection, we use <em>group privacy</em>.
Group privacy is a corollary of differential privacy that requires
databases differing in \(k\) elements to have \(k\varepsilon\)-close output
distributions. Because of group privacy, if we start with a collection
of databases that are close together, then the output distributions for
any two databases in the collection cannot be too different. This
creates a tension: utility forces the algorithm to produce different
output distributions for different databases, but privacy forces
similarity. The packing argument comes down to arguing that, unless the
databases are large, privacy wins out, and when privacy wins out then
there is some database where the algorithm probably produces a bad
output.</p>

<p>For \(\mathsf{Point}_d\), we sketch the resulting argument as follows.
Suppose we have an \(\varepsilon\)-private PAC learner that uses \(m\)
samples. Then we can define a collection of different databases of size
\(m\), one for each hypothesis in \(\mathsf{Point}_d\). By group privacy,
the output distribution for our private PAC learner changes by at most
\(e^{m\varepsilon}\) between any two of the databases in this collection.
Thus we can pick any \(h \in \mathsf{Point}_d\) and know that the
probability of outputting the wrong hypothesis is at least roughly
\(2^d \cdot e^{-m\varepsilon}\). Since we need this probability to be
small, rearranging implies \(m =
\Omega(d/\varepsilon)\).</p>

<p><a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> then contrasted this result with an <em>improper</em> pure private PAC
learner. This learner applies the exponential mechanism to a class
\(\mathsf{Point}_d’\) of hypotheses derived from \(\mathsf{Point}_d\) —
but <em>not</em> necessarily a subset of \(\mathsf{Point}_d\) — gives an
improper pure private PAC learner with sample complexity \(O(\log
d)\). Since this learner is improper, it circumvents the “one database
per hypothesis” step of the packing lower bound. Moreover, <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> gave a
still more involved improper pure private PAC learner requiring only
\(O(1)\) samples. This separates proper pure private PAC learning from
improper pure private PAC learning. In contrast, the sample complexities
of proper and improper PAC learning absent privacy are the same up to
logarithmic factors in \(\alpha\) and \(\beta\).</p>

<p>In 2013, Beimel, Nissim, and Stemmer <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a> proved a more general
result. They gave the first characterization of pure (improper) private
PAC learning by defining a new hypothesis class measure called the
<em>representation dimension</em>, \(\mathsf{REPD}\left(\mathcal{H}\right)\).
Roughly, the representation dimension considers the collection of all
distributions \(\mathcal{D}\) over sets of hypotheses, not necessarily
from \(\mathcal{H}\), that “cover” \(\mathcal{H}\). By “cover,” we mean that
for any \(h
\in \mathcal{H}\), with high probability a set drawn from covering
distribution \(\mathcal{D}\) includes a hypothesis that mostly produces
labels that agree with \(h\). With this collection of distributions
defined, \(\mathsf{REPD}\left(\mathcal{H}\right)\) is the minimum over all
such covering distributions of the logarithm of the size of the largest
set in its support. Thus a hypothesis class that can be covered by a
distribution over small sets of hypotheses will have a small
representation dimension. With the notion of representation dimension in
hand, <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a> gave the following result:</p>

<blockquote>
  <p><strong>Theorem 1</strong> (<a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a>). The sample complexity to pure private PAC learn \(\mathcal{H}\) is \(\Theta(\mathsf{REPD}\left(\mathcal{H}\right))\).</p>
</blockquote>

<p>Representation dimension may seem like a strange definition, but a
sketch of the proof of this result helps illustrate the connection to
private learning. Recall from our private Occam’s razor, and the
improper pure private PAC learner above, that if we can find a good and
relatively small set of hypotheses to choose from, then we can apply the
exponential mechanism and call it a day. It is exactly this kind of
“good set of hypotheses” that representation dimension aims to capture.
A little more formally, given an upper bound on
\(\mathsf{REPD}\left(\mathcal{H}\right)\), we know there is some covering
distribution whose largest hypothesis set is not too big. That means we
can construct a learner that draws a hypothesis set from this covering
distribution and applies the exponential mechanism to it. Just as we
picked up a \(\log|\mathcal{H}|\) sample complexity dependence using
private Occam’s razor, since \(\mathsf{REPD}\left(\mathcal{H}\right)\)
measures the logarithm of the size of the largest hypothesis set in the
support, this pure private learner picks up a
\(\mathsf{REPD}\left(\mathcal{H}\right)\) sample complexity dependence
here. This gives us one direction of
Theorem 1.</p>

<p>This logic works in the other direction as well. To go from a pure
private PAC learner with sample complexity \(m\) to an upper bound on
\(\mathsf{REPD}\left(\mathcal{H}\right)\), we return to the group privacy
trick used by <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a>. Suppose we fix a database of size \(m\) and pass it
to the learner. By group privacy and the learner’s accuracy guarantee,
if we fix some concept \(c\), the learner has probability at least roughly
\(e^{-m}\) of outputting a hypothesis that mostly agrees with \(c\). Thus if
we repeat this process roughly \(e^{m}\) times, we probably get at least
one hypothesis that mostly agrees with \(c\). In other words, this
repeated calling of the learner on the arbitrary database yields a
covering distribution for \(\mathcal{H}\). Since we called the learner
approximately \(e^m\) times, the logarithm of this is \(m\), and we get our
upper bound on \(\mathsf{REPD}\left(\mathcal{H}\right)\).</p>

<p>To recap, we now know that proper pure private PAC learning is strictly
harder than improper pure private PAC learning, which is characterized
by representation dimension. A picture sums it up. Note the dotted line,
since we don’t yet have any evidence separating finite representation
dimension and finite VC dimension.</p>

<p><img alt="Landscape of Private PAC, take 1" src="https://differentialprivacy.org/images/private_pac_1.png" style="margin: auto; display: block;" width="400"/></p>

<h1 id="separating-pure-and-approximate-private-pac-learning">Separating Pure and Approximate Private PAC Learning</h1>

<p>So far, we’ve focused only on pure privacy. In this section, we move on
to the first separations between pure and approximate private PAC
learning, as well as the first connection between private learning and
<em>online</em> learning.</p>

<p>Our source is a pair of interconnected papers from around 2014. Among
other things, Feldman and Xiao <a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a> introduced <em>Littlestone
dimension</em> to private PAC learning. By connecting representation
dimension to results from communication complexity to Littlestone
dimension, they proved the following:</p>

<blockquote>
  <p><strong>Theorem 2</strong> (<a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a>). The sample complexity to pure private PAC learn \(\mathcal{H}\) is \(\Omega(\mathsf{LD}\left(\mathcal{H}\right))\).</p>
</blockquote>

<p>Littlestone dimension \(\mathsf{LD}\left(\mathcal{H}\right)\) is, roughly,
the maximum number of mistakes an adversary can force an <em>online</em>
PAC-learning algorithm to make <a href="https://link.springer.com/article/10.1023/A:1022869011914" title="Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine learning, 1988"><strong>[Lit88]</strong></a>. We always have
\(\mathsf{VCD}\left(\mathcal{H}\right) \leq \mathsf{LD}\left(\mathcal{H}\right) \leq \log|\mathcal{H}|\),
but these inequalities can be strict. For example, denoting by
\(\mathsf{Thresh_T}\) the class of thresholds over \(\{1, 2, \ldots,
T\}\), since an adversary can force \(\Theta(\log T)\) wrong answers from
an online learner binary searching over \(\{1,2, \ldots, T\}\),
\(\mathsf{LD}\left(\mathsf{Thresh_T}\right) = \Omega(\log T)\). In
contrast, \(\mathsf{VCD}\left(\mathsf{Thresh_T}\right) = 1\).</p>

<p>At first glance it’s not obvious what
Theorem 2 adds over
Theorem 1. After all,
Theorem 1 gives an equivalence, not just a lower bound. One
advantage of
Theorem 2 is that Littlestone dimension is a known
quantity that has already been studied in its own right. We can now
import results like the lower bound on
\(\mathsf{LD}\left(\mathsf{Thresh_T}\right)\), whereas bounds on
\(\mathsf{REPD}\left(\cdot\right)\) are not common. A second advantage is
that Littlestone dimension conceptually connects private learning and
online learning: we now know that pure private PAC learning is no easier
than online PAC learning.</p>

<p>A second paper by Beimel, Nissim, and Stemmer <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013"><strong>[BNS13b]</strong></a> contrasted this
\(\Omega(\log T)\) lower bound for pure private learning of thresholds
with a \(2^{O(\log^\ast T)}\) upper bound for <em>approximate</em> private PAC
learning \(\mathsf{Thresh_T}\). Here \(\log^\ast\) denotes the very
slow-growing iterated logarithm, the number of times we must take the
logarithm of the argument to bring it \(\leq 1\). (We’re not kidding about
“very slow-growing” either:
\(\log^\ast(\text{number of atoms in universe}) \approx
4\).) With Feldman and Xiao’s result, this separates pure private PAC
learning from approximate private PAC learning. It also shows that
representation dimension does <em>not</em> characterize approximate private PAC
learning.</p>

<p>At the same time, Feldman and Xiao observed that the connection between
pure private PAC learning and Littlestone dimension is imperfect. Again
borrowing results from communication complexity, they observed that the
hypothesis class \(\mathsf{Line_p}\) (which we won’t define here) has
\(\mathsf{LD}\left(\mathsf{Line_p}\right) = 2\) but
\(\mathsf{REPD}\left(\mathsf{Line_p}\right)
= \Theta(\log(p))\). In contrast, they showed that an <em>approximate</em>
private PAC learner can learn \(\mathsf{Line_p}\) using
\(O\left(\tfrac{\log(1/\beta)}{\alpha}\right)\) samples. Since this
entails no dependence on \(p\) at all, it improves the separation between
pure and approximate private PAC learning given by <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013"><strong>[BNS13b]</strong></a>.</p>

<p>Let’s pause to recap what’s happened so far. We learned in the last
section that representation dimension characterizes pure private PAC
learning <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a>. We learned in this section that Littlestone dimension
gives lower bounds for pure private PAC learning but, as shown by
\(\mathsf{Line_p}\), these bounds are sometimes quite loose <a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a>.
\(\mathsf{Thresh_T}\) shows that representation dimension does not
characterize approximate private PAC learning <strong>[<a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014">FX14</a>
; <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013">BNS13b</a>]</strong>, and we
still have no privacy-specific lower bounds for approximate private
learners. So the picture now looks like this:</p>

<p><img alt="Landscape of Private PAC, take 2" src="https://differentialprivacy.org/images/private_pac_2.png" style="margin: auto; display: block;" width="400"/></p>

<p>In particular, we might still find that VC dimension characterizes
approximate private PAC learning!</p>

<h1 id="lower-bounds-for-approximate-private-pac-learning">Lower Bounds for Approximate Private PAC Learning</h1>

<p>We now dash this hope. In 2015, Bun, Nissim, Stemmer, and
Vadhan <a href="https://arxiv.org/abs/1504.07553" title="Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. FOCS 2015"><strong>[BNSV15]</strong></a> gave the first nontrivial lower bound for approximate
private PAC learning. They showed that learning \(\mathsf{Thresh_T}\) has
<em>proper</em> approximate private sample complexity \(\Omega(\log^\ast(T))\) and
\(O(2^{\log^\ast(T)})\).</p>

<p>We’ll at least try to give some intuition for the presence of \(\log^\ast\)
in the lower bound. Informally, the lower bound relies on an inductive
construction of a sequence of hard problems for databases of size
\(n=1, 2,
\ldots\). The \(k^{th}\) hard problem relies on a distribution over
databases of size \(k\) whose data universe is of of size exponential in
the size of the data universe for the \((k-1)^{th}\) distribution. The
base case is the uniform distribution over the two singleton databases
\(\{0\}\) and \(\{1\}\), and they show how to inductively construct
successive problems such that a solution for the \(k^{th}\) problem
implies a solution for the \((k-1)^{th}\) problem. Unraveling the
recursive relationship between the problem domain sizes implies a
general lower bound of roughly \(\log^\ast|X|\) for domain \(X\).</p>

<p>The inclusion of \(\log^\ast\) makes this is an extremely mild lower bound.
However, \(\log^\ast(T)\) can still be arbitrarily larger than 1, so this is
the first definitive evidence that proper approximate privacy introduces
a cost over non-private PAC learning.</p>

<p>In 2018, Alon, Livni, Malliaris, and Moran <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a> extended this
\(\Omega(\log^\ast T)\) lower bound for \(\mathsf{Thresh_T}\) to <em>improper</em>
approximate privacy. More generally, they gave concrete evidence for the
importance of thresholds, which have played a seemingly outsize role in
the work so far. They did so by relating a class’ Littlestone dimension
to its ability to “contain” thresholds. Here, we say \(\mathcal{H}\)
“contains” \(m\) thresholds if there exist \(m\) (unlabeled) examples
\(x_1,\ldots,x_m\) and hypotheses \(h_1, \ldots, h_m \in \mathcal{H}\) such
that the hypotheses “behave like” thresholds on the \(m\) examples, i.e., 
\(h_i(x_j) = 1 \Leftrightarrow j \geq
i\). With this language, they imported a result from model theory to show
that any hypothesis class \(\mathcal{H}\) contains
\(\log(\mathsf{LD}\left(\mathcal{H}\right))\) thresholds. This implies
that learning \(\mathcal{H}\) is at least as hard as learning
\(\mathsf{Thresh_T}\) with
\(T = \log(\mathsf{LD}\left(\mathcal{H}\right))\). Since
\(\log^\ast(\log(\mathsf{LD}\left(\mathcal{H}\right)))
= \Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\), combining these
two results puts the following limit on private PAC learning:</p>

<blockquote>
  <p><strong>Theorem 3</strong> (<a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>). The sample complexity to approximate private PAC learn \(\mathcal{H}\) is \(\Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\).</p>
</blockquote>

<p>Littlestone dimension characterizes online PAC learning, so we now know
that online PAC learnability is necessary for private PAC learnability.
Sufficiency, however, remains an open question. This produces the
following picture, where the dotted line captures the question of
sufficiency.</p>

<p><img alt="Landscape of Private PAC, take 3" src="https://differentialprivacy.org/images/private_pac_3.png" style="margin: auto; display: block;" width="400"/></p>

<h1 id="characterizing-approximate-private-pac-learning">Characterizing Approximate Private PAC Learning</h1>

<p>Spurred by this question, several advances in private PAC learning have
appeared in the last year. First, Gonen, Hazan, and Moran strengthened
Theorem 3 by giving a constructive method for converting
<em>pure</em> private learners to online learners <a href="https://arxiv.org/abs/1905.11311" title="Alon Gonen, Elad Hazan, and Shay Moran. Private learning implies online learning: An efficient reduction. NeurIPS 2019"><strong>[GHM19]</strong></a>. Their result
reaches back to the 2013 characterization of pure private learning in
terms of representation dimension by using the covering distribution to
generate a collection of “experts” for online learning. Again revisiting
\(\mathsf{Thresh_T}\), Kaplan, Ligett, Mansour, Naor, and
Stemmer <a href="https://arxiv.org/abs/1911.10137" title="Haim Kaplan, Katrina Ligett, Yishay Mansour, Moni Naor, and Uri Stemmer. Privately learning thresholds: Closing the exponential gap. COLT 2020"><strong>[KLMNS20]</strong></a> significantly reduced the \(O(2^{\log^\ast(T)})\) upper
bound of <a href="https://arxiv.org/abs/1504.07553" title="Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. FOCS 2015"><strong>[BNSV15]</strong></a> to just \(O((\log^\ast(T))^{1.5})\). And Alon, Beimel,
Moran, and Stemmer <a href="https://arxiv.org/abs/2003.04509" title="Noga Alon, Amos Beimel, Shay Moran, and Uri Stemmer. Closure properties for private classification and online prediction. COLT 2020"><strong>[ABMS20]</strong></a> justified this post’s focus on realizable
private PAC learning by giving a transformation from a realizable
approximate private PAC learner to an agnostic one at the cost of
slightly worse privacy and sample complexity. This built on an earlier
transformation that only applied to <em>proper</em> learners <a href="https://arxiv.org/abs/1407.2662" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Learning privately with labeled and unlabeled examples. SODA 2015"><strong>[BNS15]</strong></a>.</p>

<p>Finally, Bun, Livni, and Moran <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a> answered the open question posed
by <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>:</p>

<blockquote>
  <p><strong>Theorem 4</strong> (<a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a>). The sample complexity to approximate private PAC learn \(\mathcal{H}\) is \(2^{O({\mathsf{LD}\left(\mathcal{H}\right)})}\).</p>
</blockquote>

<p>To prove this, <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a> introduced the notion of a <em>globally stable</em>
learner and showed how to convert an online learner to a globally stable
learner to a private learner. Thus, combined with the result of <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>,
we now know that the sample complexity of private PAC learning any
\(\mathcal{H}\) is at least
\(\Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\) and at most
\(2^{O({\mathsf{LD}\left(\mathcal{H}\right)})}\). In this sense, online
learnability characterizes private learnability.</p>

<p><img alt="Landscape of Private PAC, final take" src="https://differentialprivacy.org/images/private_pac_4.png" style="margin: auto; display: block;" width="400"/></p>

<p>Narrowing the gap between the lower and upper bounds above is an open
question. Note that we cannot hope to close the gap completely. For the
lower bound, the current \(\mathsf{Thresh_T}\) upper bound implies that no
general lower bound can be stronger than
\(\Omega((\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))^{1.5})\). For the
upper bound, there exist hypotheses classes \(\mathcal{H}\) with
\(\mathsf{VCD}\left(\mathcal{H}\right) = \mathsf{LD}\left(\mathcal{H}\right)\)
(e.g., \(\mathsf{VCD}\left(\mathsf{Point}_d\right) = \mathsf{LD}\left(\mathsf{Point}_d\right)= 1\)), so since non-private PAC learning requires
\(\Omega(\mathsf{VCD}\left(\mathcal{H}\right))\) samples, the best
possible private PAC learning upper bound is
\(O(\mathsf{LD}\left(\mathcal{H}\right))\). Nevertheless, proving either
bound remains open.</p>

<h1 id="conclusion">Conclusion</h1>

<p>This concludes our post, and with it our discussion of this fundamental
question: the price of privacy in machine learning. We now know that in
the PAC model, proper pure private learning, improper pure private
learning, approximate private learning, and non-private learning are all
strongly separated. By the connection to Littlestone dimension, we also
know that approximate private learnability is equivalent to online
learnability. However, many questions about computational efficiency and
tight sample complexity bounds remain open.</p>

<p>As mentioned in the introduction, we focused on the clean yet widely
studied and influential model of PAC learning. Having characterized how
privacy enters the picture in PAC learning, we can hopefully convey this
understanding to other models of learning, and now approach these
questions from a rigorous and grounded point of view.</p>

<p>Congratulations to Mark Bun, Roi Livni, and Shay Moran on their best
paper award — and to the many individuals who paved the way before
them!</p>

<h1 id="acknowledgments">Acknowledgments</h1>

<p>Thanks to Kareem Amin and Clément Canonne for helpful feedback while
writing this post.</p></div>
    </summary>
    <updated>2020-09-16T18:00:00Z</updated>
    <published>2020-09-16T18:00:00Z</published>
    <author>
      <name>Matthew Joseph</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-09-25T23:29:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/141</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/141" rel="alternate" type="text/html"/>
    <title>TR20-141 |  Candidate Tree Codes via Pascal Determinant Cubes | 

	Gil Cohen, 

	Inbar Ben Yaacov, 

	Anand Kumar Narayanan</title>
    <summary>Tree codes are combinatorial structures introduced by Schulman (STOC 1993) as key ingredients in interactive coding schemes. Asymptotically-good tree codes are long known to exist, yet their explicit construction remains a notoriously hard open problem. Even proposing a plausible construction, without the burden of proof, is difficult and the defining tree code property requires structure that remains elusive. To the best of our knowledge, only one candidate appears in the literature, due to Moore and Schulman (ITCS 2014).

We put forth a new candidate for an explicit asymptotically-good tree code. Our construction is an extension of the vanishing rate tree code by Cohen-Haeupler-Schulman (STOC 2018) combined with a vanishing distance tree code by Gelles et al. (SODA 2016). The correctness of our construction relies on a conjecture that we introduce on certain Pascal determinants indexed by the points of the Boolean hypercube. We furnish evidence supporting our conjecture through numerical computation, combinatorial arguments from planar path graphs and based on well-studied heuristics from arithmetic geometry.</summary>
    <updated>2020-09-16T17:46:59Z</updated>
    <published>2020-09-16T17:46:59Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-26T15:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/140</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/140" rel="alternate" type="text/html"/>
    <title>TR20-140 |  Optimal Testing of Discrete Distributions with High Probability | 

	Ilias Diakonikolas, 

	Themis Gouleakis, 

	Daniel Kane, 

	John Peebles, 

	Eric Price</title>
    <summary>We study the problem of testing discrete distributions with a focus on the high probability regime.
Specifically, given samples from one or more discrete distributions, a property $\mathcal{P}$, and 
parameters $0&lt; \epsilon, \delta &lt;1$, we want to distinguish {\em with probability at least $1-\delta$}
whether these distributions satisfy $\mathcal{P}$ or are $\epsilon$-far from $\mathcal{P}$
in total variation distance. Most prior work in distribution testing studied the constant confidence case 
(corresponding to $\delta = \Omega(1)$), and provided sample-optimal testers for a range of properties.
While one can always boost the confidence probability of any such tester by black-box amplification, 
this generic boosting method typically leads to sub-optimal sample bounds.

Here we study the following broad question: For a given property $\mathcal{P}$, can we {\em characterize} 
the sample complexity of testing $\mathcal{P}$ as a function of all relevant problem parameters, 
including the error probability $\delta$? Prior to this work, uniformity testing was the only statistical task
whose sample complexity had been characterized in this setting. As our main results,
we provide the first algorithms for closeness and independence testing that are sample-optimal, within 
constant factors, as a function of all relevant parameters. We also show matching
information-theoretic lower bounds on the sample complexity of these problems.
Our techniques naturally extend to give optimal testers for  related problems. To illustrate the generality of our methods, 
we give optimal algorithms for testing collections of distributions and testing closeness with unequal sized samples.</summary>
    <updated>2020-09-16T17:20:55Z</updated>
    <published>2020-09-16T17:20:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-26T15:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4962</id>
    <link href="https://www.scottaaronson.com/blog/?p=4962" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4962#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4962" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">In a world like this one, take every ally you can get</title>
    <summary xml:lang="en-US">Update (Sep. 17): Several people, here and elsewhere, wrote to tell me that while they completely agreed with my strategic and moral stance in this post, they think that it’s the ads of Republican Voters Against Trump, rather than the Lincoln Project, that have been most effective in changing Trump supporters’ minds. So please consider […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Update (Sep. 17):</span></strong> Several people, here and elsewhere, wrote to tell me that while they completely agreed with my strategic and moral stance in this post, they think that it’s the ads of <a href="https://www.youtube.com/channel/UC03-Q9vq-JyiStTnqasADVg">Republican Voters Against Trump</a>, rather than the Lincoln Project, that have been most effective in changing Trump supporters’ minds.  So please consider <a href="https://rvat.org/donate/">donating to RVAT</a> instead or in addition!  In fact, what the hell, I’ll match donations to RVAT up to $1000.</p>



<p/><hr/><p/>



<p>For the past few months, I’ve alternated between periods of debilitating depression and (thankfully) longer stretches when I’m more-or-less able to work.  Triggers for my depressive episodes include reading social media, watching my 7-year daughter struggle with prolonged isolation, and (especially) contemplating the ongoing apocalypse in the American West, the hundreds of thousands of pointless covid deaths, and an election in 48 days that <em>if I didn’t know such things were impossible in America</em> would seem <a href="https://www.washingtonpost.com/health/2020/09/14/michael-caputo-coronavirus-cdc/">likely</a> to produce a terrifying standoff as a despot and millions of his armed loyalists refuse to cede control.  Meanwhile, catalysts for my relatively functional periods have included teaching my undergrad <a href="https://www.scottaaronson.com/qclec.pdf">quantum information class</a>, Zoom calls with my students, <a href="https://www.nature.com/articles/s41550-020-1174-4">life on Venus?!?</a> (my guess is no, but almost entirely due to priors), learning new math (fulfilling a decades-old goal, I’m finally working my way through Paul Cohen’s celebrated <a href="https://en.wikipedia.org/wiki/Forcing_(mathematics)">proof</a> of the independence of the Continuum Hypothesis—more about that later!).</p>



<p>Of course, when you feel crushed by the weight of the world’s horribleness, it improves your mood to be able even just to prick the horribleness with a pin.  So I was gratified that, in response to a <a href="https://www.scottaaronson.com/blog/?p=4942">previous post</a>, <em>Shtetl-Optimized</em> readers contributed at least $3,000, the first $2,000 of which I matched, mostly to the <a href="https://secure.actblue.com/donate/duforjoe">Biden-Harris campaign</a> but a little to the <a href="https://lincolnproject.us/donate/">Lincoln Project</a>.</p>



<p>Alas, a <a href="https://www.scottaaronson.com/blog/?p=4942#comment-1856922">commenter</a> was unhappy with the latter:</p>



<blockquote class="wp-block-quote"><p>Lincoln Project? Really? … Pushing the Overton window rightward during a worldwide fascist dawn isn’t good. I have trouble understanding why even extremely smart people have trouble with this sort of thing.</p></blockquote>



<p>Since this is actually important, I’d like to spend the rest of this post responding to it.</p>



<p>For me it’s simple.</p>



<p>What’s the goal right now?  To defeat Trump.  In the US right now, that’s the prerequisite to <strong>every other</strong> sane political goal.</p>



<p>What will it take to achieve that goal? Turnout, energizing the base, defending the election process … but also, if possible, <em>persuading a sliver of Trump supporters in swing states to switch sides</em>, or at least vote third party or abstain.</p>



<p>Who is actually effective at that goal?  Well, no one knows for sure.  But while I thought the Biden campaign had some semi-decent ads, the Lincoln Project’s best stuff seems <a href="https://www.youtube.com/watch?v=2uLJkpH__os">better</a> to me, just savagely good.</p>



<p><em>Why</em> are they effective?  The answer seems obvious: for the same reason why a jilted ex is a more dangerous foe than a stranger.  If <em>anyone</em> understood how to deprogram a Republican from the Trump cult, who would it be: Alexandria Ocasio-Cortez, or a fellow Republican who successfully broke from the cult?</p>



<p>Do I agree with the Lincoln Republicans about most of the “normal” issues that Americans once argued about?  Not at all.  Do I hold them, in part, morally responsible for creating the preconditions to the current nightmare?  Certainly.</p>



<p>And should any of that cause me to boycott them? Not in my moral universe.  If Churchill and FDR could team up with Stalin, then surely we in the Resistance can temporarily ally ourselves with the rare Republicans who chose their stated principles over power when tested—their very rarity attesting to the nontriviality of their choice.</p>



<p>To my mind, turning one’s back on would-be allies, in a conflict whose stakes obviously overshadow what’s bad about those allies, is simultaneously one of the dumbest <em>and</em> the ugliest things that human beings can do.  It abandons reason for moral purity and ends up achieving neither.</p></div>
    </content>
    <updated>2020-09-16T07:51:47Z</updated>
    <published>2020-09-16T07:51:47Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-09-21T04:01:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/09/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/09/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Closed quasigeodesics on the dodecahedron (\(\mathbb{M}\)), paths that start at a vertex and go straight across each edge until coming back to the same vertex from the other side. Original paper, arXiv:1811.04131, doi:10.1080/10586458.2020.1712564. I saw this on Numberphile a few months back (video linked in article) but now it’s on Quanta.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-report-new-discovery-about-the-dodecahedron-20200831/">Closed quasigeodesics on the dodecahedron</a> (<a href="https://mathstodon.xyz/@11011110/104785420838924796">\(\mathbb{M}\)</a>), paths that start at a vertex and go straight across each edge until coming back to the same vertex from the other side. Original paper, <a href="https://arxiv.org/abs/1811.04131">arXiv:1811.04131</a>, <a href="https://doi.org/10.1080/10586458.2020.1712564">doi:10.1080/10586458.2020.1712564</a>. I saw this on Numberphile a few months back (video linked in article) but now it’s on <em>Quanta</em>.</p>
  </li>
  <li>
    <p><a href="https://blog.graphicine.com/lorenz-stoer-geometric-landscapes/">Lorenz Stöer’s geometric landscapes</a> (<a href="https://mathstodon.xyz/@11011110/104799765760054680">\(\mathbb{M}\)</a>). <a href="https://11011110.github.io/blog/2014/09/30/linkage-for-end.html">In 2014 I linked a different page</a> with a few of Stöer’s 16th-century proto-surrealist combinations of landscape and geometry, but they were black and white. This one has more of them, in color.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Ideal_polyhedron">Ideal polyhedron</a>, a polyhedron in hyperbolic space with all vertices at infinity, and <a href="https://en.wikipedia.org/wiki/Sylvester%E2%80%93Gallai_theorem">Sylvester–Gallai theorem</a>, that every finite set of points in the Euclidean plane has a line that either passes through all of them or through exactly two of them. Both newly promoted to Good Article status on Wikipedia (<a href="https://mathstodon.xyz/@11011110/104803212564257211">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://merveilles.town/@neauoire/104779168858836970">Escherian wiener-dog Cerberus fetches three impossible things</a>.</p>
  </li>
  <li>
    <p>Flamebait post of the day: <a href="http://nautil.us/issue/89/the-dark-side/why-mathematicians-should-stop-naming-things-after-each-other">Why mathematicians should stop naming things after each other</a> (<a href="https://mathstodon.xyz/@11011110/104813883920721252">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24385389">via</a>).  For once the via-link discussion is worth reading (main point: the alternative, using common English words to describe specialized technical concepts, can be even more confusing).</p>
  </li>
  <li>
    <p>Early Renaissance painter Piero della Francesca was also an accomplished mathematician, and his book on polyhedra, <em>De quinque corporibus regularibus</em> (subject of a <a href="https://en.wikipedia.org/wiki/De_quinque_corporibus_regularibus">new Wikipedia article</a>; <a href="https://mathstodon.xyz/@11011110/104820183749646319">\(\mathbb{M}\)</a>) has an interesting history that deserves to be better known. Rediscovery of the mathematics of Archimedes! “First full-blown case of plagiarism in the history of mathematics” (by Luca Pacioli, in Divina proportione)! Maybe owned by John Dee! Long lost and found centuries later in the Vatican Library!</p>
  </li>
  <li>
    <p><a href="https://cameroncounts.wordpress.com/2020/08/30/moonlighting/">Peter Cameron gives a nice roundup of two recent online conferences on group theory and combinatorics</a> (<a href="https://mathstodon.xyz/@11011110/104833470202099899">\(\mathbb{M}\)</a>) that he attended more-or-less simultaneously, something that would have been impossible for physical conferences. The parts on synchronizing automata and twin-width particularly caught my attention as stuff I should look up and find out more about.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/RodBogart/status/455123609195802624">An hourglass that demonstrates Archimedes’ theorem that the volume of a cylinder is the sum of the volumes of its inscribed sphere and cone</a> (<a href="https://mathstodon.xyz/@mjd/104836143207567957">\(\mathbb{M}\)</a>), from Rod Bogart’s twitter feed.</p>
  </li>
  <li>
    <p>The <a href="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite.html">hexagon-minimizing simple bipartite polyhedra of my recent blog post</a> make nice shapes when converted to <a href="https://arxiv.org/abs/0912.0537">simple orthogonal polyhedra</a> (<a href="https://mathstodon.xyz/@11011110/104842837266010559">\(\mathbb{M}\)</a>): a squared-off amphitheater with L-shaped terraces of increasing length as they rise, or a diagonal staircase with congruent L-shaped steps. In each case the outer \(2n\)-gon is the underside of the polygon and the inner cycles are the horizontal faces.</p>

    <p style="text-align: center;"><img alt="Hexagon-minimizing simple bipartite polyhedra represented as simple orthogonal polyhedra" src="https://11011110.github.io/blog/assets/2020/orthogonal-eberhard.svg"/></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.11933">Open is not forever: a study of vanished open access journals</a> (<a href="https://mathstodon.xyz/@11011110/104850663521092974">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24422593">via</a>, <a href="https://www.sciencemag.org/news/2020/09/dozens-scientific-journals-have-vanished-internet-and-no-one-preserved-them">via</a>). This study shows the need for systematic archiving and redundant copying of online open journals, but I suspect that the problem for small hand-run print-based journals without much library pickup might be much worse.</p>
  </li>
  <li>
    <p><a href="https://www.thisiscolossal.com/2018/10/a-prickly-structure-made-of-70000-reusable-hexapod-particles/">A prickly structure made of 70,000 reusable hexapod particles</a> (<a href="https://mathstodon.xyz/@11011110/104856358909259046">\(\mathbb{M}\)</a>). Sort of like those <a href="https://en.wikipedia.org/wiki/Tetrapod_(structure)">seawalls they build by jumbling together giant concrete caltrops</a>, only with pieces that are not quite so big and with usable spaces left void within it. Sometimes the article says “hexapod” and sometimes “decapod”; the pictures appear to show structures that mix two different kinds of particle.</p>
  </li>
  <li>
    <p><em><a href="http://math.sfsu.edu/beck/ct/board.php">Combinatorial Theory</a></em> (<a href="https://mathstodon.xyz/@bremner/104859257534118058">\(\mathbb{M}\)</a>, <a href="https://twitter.com/wtgowers/status/1305253478047068160">see also</a>), a new open-access combinatorics journal formed from the mass resignation of the Elsevier <em>JCTA</em> editorial board.</p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2020/09/14/us/caputo-virus.html">Trump officials are now telling their supporters to buy guns and ammunition to use against scientists for being anti-Trump</a>. <a href="https://thehill.com/policy/healthcare/516319-top-hhs-official-accuses-scientists-of-plotting-against-trump-tells">No, seriously</a> (<a href="https://mathstodon.xyz/@11011110/104865069277930004">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><em><a href="https://www.cambridge.org/us/academic/subjects/mathematics/recreational-mathematics/origametry-mathematical-methods-paper-folding">Origametry: Mathematical Methods in Paper Folding</a></em> (<a href="https://mathstodon.xyz/@11011110/104870325812873444">\(\mathbb{M}\)</a>), new book coming out October 31 by Tom Hull. I haven’t seen anything more than the blurb linked here and the <a href="https://books.google.com/books?id=LdX7DwAAQBAJ">limited preview on Google Books</a>, but it looks interesting and worth waiting for.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-09-15T22:15:00Z</updated>
    <published>2020-09-15T22:15:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-09-16T05:18:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17600</id>
    <link href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/" rel="alternate" type="text/html"/>
    <title>Ken Regan Turned 61</title>
    <summary>Happy birthday to Ken Ken Regan is of course my partner on GLL. He is faculty in the computer science department at the University of Buffalo. His PhD was in 1986 from Oxford University and it was titled On the separation of complexity classes. He was the PhD student of Dominic Welsh who was a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="color: #0044cc;"><br/>
<em>Happy birthday to Ken</em><br/>
</span></p>
<table class="image alignright">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/collage-2/" rel="attachment wp-att-17602"><img alt="" class="aligncenter size-full wp-image-17602" height="450" src="https://rjlipton.files.wordpress.com/2020/09/collage.jpg?w=600&amp;h=450" width="600"/></a></td>
</tr>
</tbody>
</table>
<p>Ken Regan is of course my partner on GLL. He is faculty in the computer science department at the University of Buffalo. His PhD was in 1986 from Oxford University and it was titled <i>On the separation of complexity classes</i>. He was the PhD student of Dominic Welsh who was a student of John Hammersley.</p>
<p>Today I would like to wish Ken a happy birthday.</p>
<p><span id="more-17600"/></p>
<p>He is now 61 years young. I hope you will join me and wish him many more birthdays. His age is <a href="https://en.wikipedia.org/wiki/61_(number)">special</a> for many reasons:</p>
<ul>
<li>It is a twin prime.</li>
<li>It is equal to <img alt="{5^{2} + 6^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B5%5E%7B2%7D+%2B+6%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{5^{2} + 6^{2}}"/>.</li>
<li>It is the ninth Mersenne prime: <img alt="{2^{61} - 1 = 2,305,843,009,213,693,951}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B61%7D+-+1+%3D+2%2C305%2C843%2C009%2C213%2C693%2C951%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{61} - 1 = 2,305,843,009,213,693,951}"/>.</li>
</ul>
<p>There are three big <b>I’s</b> in his life. Let’s talk about two of them.</p>
<h2>Interest in Cricket</h2>
<p>Ken loves sports in general and especially cricket. Last Sunday he told me he watched his Bills win their first NFL game while he watched a cricket match. I have no idea how cricket works, but here is Ken’s <a href="https://cse.buffalo.edu/~regan/Writing/CricketBaseball.html">explanation</a>: <i>Are Cricket and Baseball sister games?</i></p>
<table style="margin: auto;">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/cr1/" rel="attachment wp-att-17603"><img alt="" class="aligncenter size-full wp-image-17603" height="450" src="https://rjlipton.files.wordpress.com/2020/09/cr1.jpg?w=600&amp;h=450" width="600"/></a></td>
</tr>
</tbody>
</table>
<ul>
<li>In a baseball game you see pitchers on the field.<br/>
In a cricket match you see fielders on the pitch.</li>
<li>In baseball, a bad delivery is called a “Ball”.<br/>
In cricket, it’s a “No Ball”.</li>
<li>In baseball, if a batter carries his bat, he’s out.<br/>
In cricket, the batsmen always carry their bat, and an opening batsman who “carries his bat” is never out.</li>
<li>In baseball, an innings is called a half-inning.<br/>
In cricket, an inning is called an innings.</li>
<li>In baseball, a batter hit by a pitched ball gets a free pass to First Base.<br/>
In cricket, such a batter can be Out Leg Before Wicket.</li>
<li>In baseball, if a ball is caught over the boundary, “yer out!”<br/>
In cricket, you score 6 runs.</li>
<li>In baseball, when a batter “walks”, he gets a free pass to first and is not out.<br/>
In cricket, it means the batsman declares himself out before the umpire has a chance to make the call. This classic show of sportsmanship is considered unsportsmanlike in baseball.<p/>
<h2>Interest in Chess</h2>
<p>When the chess world wants to know if someone has cheated, they call Ken. He is an international chess master, and has worked on stopping cheating for years. It is important these days, since most tournaments are now online. And cheating is easier when no one is directly able to watch you. Ken is busy.</p>
<p>Let’s look at the cheating problem. Suppose that Alice and Bob are playing an online game of chess. Alice makes her own moves, but she wonders if Bob could be cheating. He could be using advice from another “player”, Sally. There are several points:</p>
<ol>
<li>Sally is a stronger player than anyone—she can easily beat Alice and Bob.</li>
<li>Sally not only says “here is my move”—she will sometimes give several good moves.</li>
<li>Sally is a program that is deterministic—given a position she gives the same answer.The issue for Ken is: When Alice played Bob did Bob make the moves or did he consult Sally?</li>
</ol>
<p>There are many complexities:</p>
<ol>
<li>What if Bob agreed with all Sally’s moves? Then he certainly did cheat.</li>
<li>What if Bob was just lucky and played above his strength? Then he did not cheat.</li>
<li>What if Bob used Sally for some positions but not others? Then he did cheat, but it may be hard to be sure.</li>
<li>And so on.</li>
</ol>
<p>What Ken has done is create both a theory and programs to determine whether Bob did indeed cheat. I find the general problem of telling if one cheats online at chess to be fascinating. See us <a href="https://rjlipton.wordpress.com/2014/06/18/the-problem-of-catching-chess-cheaters/">before</a> for more details and also see <a href="http://www.uschess.org/index.php/June/How-To-Catch-A-Chess-Cheater-Ken-Regan-Finds-Moves-Out-Of-Mind.html">this</a>.</p>
<h2>Open Problems</h2>
<p>Ken is one of the nicest people I know. Hope he has many more birthdays and many more twin primes.</p></li>
</ul></div>
    </content>
    <updated>2020-09-15T16:24:01Z</updated>
    <published>2020-09-15T16:24:01Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="People"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-26T15:20:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/14/postdoc-position-in-theoretical-computer-science-foundations-of-ai-at-aarhus-university-denmark-apply-by-october-9-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/14/postdoc-position-in-theoretical-computer-science-foundations-of-ai-at-aarhus-university-denmark-apply-by-october-9-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc position in Theoretical Computer Science/Foundations of AI at Aarhus University, Denmark (apply by October 9, 2020)</title>
    <summary>A 2-year postdoc position in Theoretical Computer Science/Algorithmic Foundations of AI is available in the Algorithms and Data Structures group at Aarhus University, Denmark. Candidates should have a recent PhD in Computer Science, Mathematics, or Economics on topics that fall within algorithmic game theory or computational social choice, and a strong publication record. Website: https://international.au.dk/about/profile/vacant-positions/job/department-of-computer-science-is-looking-for-a-post-doc-in-theoretical-computer-science-algorithm/ […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A 2-year postdoc position in Theoretical Computer Science/Algorithmic Foundations of AI is available in the Algorithms and Data Structures group at Aarhus University, Denmark.</p>
<p>Candidates should have a recent PhD in Computer Science, Mathematics, or Economics on topics that fall within algorithmic game theory or computational social choice, and a strong publication record.</p>
<p>Website: <a href="https://international.au.dk/about/profile/vacant-positions/job/department-of-computer-science-is-looking-for-a-post-doc-in-theoretical-computer-science-algorithm/">https://international.au.dk/about/profile/vacant-positions/job/department-of-computer-science-is-looking-for-a-post-doc-in-theoretical-computer-science-algorithm/</a><br/>
Email: iannis@cs.au.dk</p></div>
    </content>
    <updated>2020-09-14T17:10:58Z</updated>
    <published>2020-09-14T17:10:58Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-26T15:20:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6364597152143042105</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6364597152143042105/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/an-interesting-serendipitous-number.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6364597152143042105" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6364597152143042105" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/an-interesting-serendipitous-number.html" rel="alternate" type="text/html"/>
    <title>An interesting serendipitous number</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Last seek I blogged about two math problems of interest to me <a href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html">here</a>.</p><p>One of them two people posted answers, which was great since I didn't know how to solve them and now I do. Yeah! I blogged about that <a href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html">here</a>.</p><p><br/></p><p>The other problem got no comments, so I suppose it was of interest to me but not others. I was interested in it because the story behind it is interesting, and the answer is interesting.</p><p><br/></p><p>it is from the paper </p><p>An interesting and serendipitous number by John Ewing and Ciprian Foias, which is a chapter in the wonderful book </p><p>Finite vs Infinite: Contributions to an eternal dilemma</p><p>Here is the story, I paraphrase the article (I'll give pointers  later).</p><p>In the mid 1970's a student asked Ciprian about the following math-competition problem:</p><p>x(1)&gt;0    x(n+1) =  (1 + (1/x(n)))^n. For which x(1) does x(n) --&gt; infinity?</p><p>It turned out this was a misprint. The actual problem was</p><p>x(1)&gt;0  x(n+1)=(1+(1/x(n))^{x(n)}. For which x(1) does x(n) --&gt; infinity.</p><p><br/></p><p>The actual math-comp problem  (with exp x(n)) is fairly easy (I leave it to you.) But this left the misprinted problem (with exp n).  Crispian proved that there is exactly ONE x(1) such that x(n)--&gt; infinity. </p><p>Its approx 1.187... and may be trans.</p><p><br/></p><p>I find the story and the result interesting, but the proof is to long for a blog post.</p><p>I tried to find the article online and could not. A colleague found the following:</p><p><br/></p><p>A preview of the start of the article <a href="https://link.springer.com/chapter/10.1007/978-1-4471-0751-4_8">here</a></p><p>Wikipedia Page on the that number, called the Foias constant, <a href="https://en.wikipedia.org/wiki/Foias_constant">here</a></p><p>Mathworld page on that number <a href="https://mathworld.wolfram.com/FoiasConstant.html">here</a></p><p>Most of the article but skips two pages <a href="https://books.google.com/books?id=Bjb0BwAAQBAJ&amp;pg=PA119&amp;lpg=PA119&amp;dq=serendipitous+number+John+Ewing+and+Ciprian+Foias&amp;source=bl&amp;ots=4tn1sk3XEA&amp;sig=ACfU3U3GM9VtlyWxTjq302E5Uf7Tmr49Hw&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwiewLSF9OjrAhVkmXIEHRYEAMA4ChDoATAAegQICRAB#v=onepage&amp;q=serendipitous%20number%20John%20Ewing%20and%20Ciprian%20Foias&amp;f=false">here</a></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2020-09-14T16:16:00Z</updated>
    <published>2020-09-14T16:16:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-26T13:46:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-09-14-broadcast-from-agreement-and-agreement-from-broadcast/</id>
    <link href="https://decentralizedthoughts.github.io/2020-09-14-broadcast-from-agreement-and-agreement-from-broadcast/" rel="alternate" type="text/html"/>
    <title>Broadcast from Agreement and Agreement from Broadcast</title>
    <summary>In this post, we highlight the connection between Broadcast and Agreement in the synchronous model. Broadcast and Agreement: How can you implement one from the other? We defined Agreement and Broadcast in a previous post, here is a recap: Agreement A set of $n$ nodes where each node $i$ has...</summary>
    <updated>2020-09-14T14:07:00Z</updated>
    <published>2020-09-14T14:07:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-09-26T15:21:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/139</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/139" rel="alternate" type="text/html"/>
    <title>TR20-139 |  The Coin Problem with Applications to Data Streams | 

	Mark Braverman, 

	Sumegha Garg, 

	David Woodruff</title>
    <summary>Consider the problem of computing the majority of a stream of $n$ i.i.d. uniformly random bits. This problem, known as the {\it coin problem}, is central to a number of counting problems in different data stream models. We show that any streaming algorithm for solving this problem with large constant advantage must use $\Omega(\log n)$ bits of space. We extend our lower bound to proving tight lower bounds for solving multiple, randomly interleaved copies of the coin problem, as well as for solving the OR of multiple copies of a variant of the coin problem. Our proofs involve new measures of information complexity that are well-suited for data streams. 

We use these lower bounds to obtain a number of new results for data streams. In each case there is an underlying $d$-dimensional vector $x$ with additive updates to its coordinates given in a stream of length $m$. The input streams arising from our coin lower bound have nice distributional properties, and consequently for many problems for which we only had lower bounds in general turnstile streams, we now obtain the same lower bounds in more natural models, such as the bounded deletion model, in which $\|x\|_2$ never drops by a constant fraction of what it was earlier, or in the random order model, in which the updates are ordered randomly. In particular, in the bounded deletion model, we obtain nearly tight lower bounds for approximating $\|x\|_{\infty}$ up to additive error $\frac{1}{\sqrt{k}} \|x\|_2$, approximating $\|x\|_2$ up to a multiplicative $(1 + \epsilon)$ factor (resolving a question of Jayaram and Woodruff in PODS 2018), and solving the Point Query and $\ell_2$-Heavy Hitters Problems. In the random order model, we also obtain new lower bounds for the Point Query and $\ell_2$-Heavy Hitters Problems. 
We also give new algorithms complementing our lower bounds and illustrating the tightness of the models we consider, including an algorithm for approximating $\|x\|_{\infty}$ up to additive error $\frac{1}{\sqrt{k}} \|x\|_2$ in turnstile streams (resolving a question of Cormode in a 2006 IITK Workshop), and an algorithm for finding $\ell_2$-heavy hitters in randomly ordered insertion streams (which for random order streams, resolves a question of Nelson in a 2018 Warwick Workshop).</summary>
    <updated>2020-09-14T02:02:37Z</updated>
    <published>2020-09-14T02:02:37Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-26T15:20:41Z</updated>
    </source>
  </entry>
</feed>
