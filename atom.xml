<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-05-07T19:21:35Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7656443173792446291</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7656443173792446291/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/vidcast-on-conferences.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7656443173792446291" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7656443173792446291" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/vidcast-on-conferences.html" rel="alternate" type="text/html"/>
    <title>Vidcast on Conferences</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Bill and Lance have another socially-distanced <a href="https://youtu.be/VwvuTnE66xQ">vidcast</a>, this time with Lance telling the story of two conferences (<a href="http://ec20.sigecom.org/">ACM Economics and Computation</a> and the Game Theory Congress). As mentioned in the video the Game Theory Congress has been <a href="http://gametheorysociety.org/6th-world-congress-of-the-game-theory-society-in-budapest-july-13-17-2020/">postponed to next year</a>. Also mentioned in the video, for a limited time you can read Lance's <a href="https://goldenticket.fortnow.com/">book</a> on P v NP on <a href="https://muse.jhu.edu/book/36432">Project Muse</a>.<div><br/></div><div><br/></div></div>
    </content>
    <updated>2020-05-07T14:28:00Z</updated>
    <published>2020-05-07T14:28:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-05-07T14:28:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02970</id>
    <link href="http://arxiv.org/abs/2005.02970" rel="alternate" type="text/html"/>
    <title>Outlier-Robust Clustering of Non-Spherical Mixtures</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bakshi:Ainesh.html">Ainesh Bakshi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kothari:Pravesh.html">Pravesh Kothari</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02970">PDF</a><br/><b>Abstract: </b>We give the first outlier-robust efficient algorithm for clustering a mixture
of k statistically separated d-dimensional Gaussians (k-GMMs). Concretely, our
algorithm takes input an $\epsilon$-corrupted sample from a k-GMM and outputs
an approximate clustering that misclassifies at most $O(k\epsilon)+\eta$
fraction of the points whenever every pair of components is separated by
$1-\exp(-\textrm{poly}(k/\eta))$ in total variation distance. This is the
statistically weakest possible notion of separation and allows, for e.g.,
clustering of mixtures with components with the same mean with covariances
differing in a single unknown direction or separated in Frobenius distance. The
running time of our algorithm is $d^{O(\log(\kappa)) \textrm{poly}(k/\eta)}$
where $\kappa$ is a measure of spread of the mixture in any direction. For k=2,
our algorithms run in time and samples $\textrm{poly}(d)$ with no dependence on
the spread $\kappa$. Such a results were not known prior to our work, even for
k=2.
</p>
<p>More generally, our algorithm succeeds for mixtures of any distribution that
satisfies two well-studied analytic assumptions - certifiable
hypercontractivity and anti-concentration. Thus, they extend to clustering
mixtures of arbitrary affine transforms of the uniform distribution on the
$d$-dimensional unit sphere. Even the information theoretic clusterability of
distributions satisfying our analytic assumptions was not known and is likely
to be of independent interest.
</p>
<p>Our algorithms build on the recent flurry of work relying on certifiable
anti-concentration, first introduced in [KKK'19, RY'20]. Our techniques expand
the sum-of-squares toolkit to show robust certifiability of TV-separated
Gaussian clusters in data. This involves a low-degree sum-of-squares proof of
statements that relate parameter distance to total variation distance simply
relying on hypercontractivity and anti-concentration.
</p></div>
    </summary>
    <updated>2020-05-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02853</id>
    <link href="http://arxiv.org/abs/2005.02853" rel="alternate" type="text/html"/>
    <title>Sparktope: linear programs from algorithms</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Avis:David.html">David Avis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bremner:David.html">David Bremner</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02853">PDF</a><br/><b>Abstract: </b>In a recent paper Avis, Bremner, Tiwary and Watanabe gave a method for
constructing linear programs (LPs) based on algorithms written in a simple
programming language called Sparks. If an algorithm produces the solution $x$
to a problem in polynomial time and space then the LP constructed is also of
polynomial size and its optimum solution contains $x$ as well as a complete
execution trace of the algorithm. Their method led us to the construction of a
compiler called Sparktope which we describe in this paper. This compiler allows
one to generate polynomial sized LPs for problems in P that have exponential
extension complexity, such as matching problems in non-bipartite graphs.
</p>
<p>In this paper we describe Sparktope, the language Sparks, and the assembler
instructions and LP constraints it produces. This is followed by two concrete
examples, the makespan problem and the problem of testing if a matching in a
graph is maximum, both of which are known to have exponential extension
complexity. Computational results are given. In discussing these examples we
make use of visualization techniques included in Sparktope that may be of
independent interest. The extremely large linear programs produced by the
compiler appear to be quite challenging to solve using currently available
software. Since the optimum LP solutions can be computed independently they may
be useful as benchmarks. Further enhancements of the compiler and its
application are also discussed.
</p></div>
    </summary>
    <updated>2020-05-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02819</id>
    <link href="http://arxiv.org/abs/2005.02819" rel="alternate" type="text/html"/>
    <title>Geoopt: Riemannian Optimization in PyTorch</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kochurov:Max.html">Max Kochurov</a>, Rasul Karimov, Sergei Kozlukov <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02819">PDF</a><br/><b>Abstract: </b>Geoopt is a research-oriented modular open-source package for Riemannian
Optimization in PyTorch. The core of Geoopt is a standard Manifold interface
which allows for the generic implementation of optimization algorithms. Geoopt
supports basic Riemannian SGD as well as adaptive optimization algorithms.
Geoopt also provides several algorithms and arithmetic methods for supported
manifolds, which allow composing geometry-aware neural network layers that can
be integrated with existing models.
</p></div>
    </summary>
    <updated>2020-05-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02725</id>
    <link href="http://arxiv.org/abs/2005.02725" rel="alternate" type="text/html"/>
    <title>Incremental Multiple Longest Common Sub-Sequences</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Russo:Lu=iacute=s_M=_S=.html">Luís M. S. Russo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Francisco:Alexandre_P=.html">Alexandre P. Francisco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rocher:Tatiana.html">Tatiana Rocher</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02725">PDF</a><br/><b>Abstract: </b>We consider the problem of updating the information about multiple longest
common sub-sequences. This kind of sub-sequences is used to highlight
information that is shared across several information sequences, therefore it
is extensively used namely in bioinformatics and computational genomics. In
this paper we propose a way to maintain this information when the underlying
sequences are subject to modifications, namely when letters are added and
removed from the extremes of the sequence. Experimentally our data structure
obtains significant improvements over the state of the art.
</p></div>
    </summary>
    <updated>2020-05-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02645</id>
    <link href="http://arxiv.org/abs/2005.02645" rel="alternate" type="text/html"/>
    <title>Search for developments of a box having multiple ways of folding by SAT solver</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Riona Tadaki, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Amano:Kazuyuki.html">Kazuyuki Amano</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02645">PDF</a><br/><b>Abstract: </b>A polyomino is called a development if it can make a box by folding edges of
unit squares forming the polyomino. It is known that there are developments
that can fold into a box (or boxes) in multiple ways. In this work, we
conducted a computer search for finding such developments by using a SAT
solver. As a result, we found thousands of such developments including a
polyomino of area 52 that can fold into a box of size $1 \times 2 \times 8$ in
five different ways.
</p></div>
    </summary>
    <updated>2020-05-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02607</id>
    <link href="http://arxiv.org/abs/2005.02607" rel="alternate" type="text/html"/>
    <title>Towards quantum advantage for topological data analysis</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Casper Gyurik, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cade:Chris.html">Chris Cade</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dunjko:Vedran.html">Vedran Dunjko</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02607">PDF</a><br/><b>Abstract: </b>A particularly promising line of quantum machine leaning (QML) algorithms
with the potential to exhibit exponential speedups over their classical
counterparts has recently been set back by a series of "dequantization"
results, that is, quantum-inspired classical algorithms which perform equally
well in essence. This raises the important question whether other QML
algorithms are susceptible to such dequantization, or whether it can be
formally argued that they are out of reach of classical computers. In this
paper, we study the quantum algorithm for topological data analysis by Lloyd,
Garnerone and Zanardi (LGZ). We provide evidence that certain crucial steps in
this algorithm solve problems that are classically intractable by closely
relating them to the one clean qubit model, a restricted model of quantum
computation whose power is strongly believed to lie beyond that of classical
computation. While our results do not imply that the topological data analysis
problem solved by the LGZ algorithm (i.e., Betti number estimation) is itself
DQC1-hard, our work does provide the first steps towards answering the question
of whether it is out of reach of classical computers. Additionally, we discuss
how to extend the applicability of this algorithm beyond its original aim of
estimating Betti numbers and demonstrate this by looking into quantum
algorithms for spectral entropy estimation. Finally, we briefly consider the
suitability of the LGZ algorithm for near-term implementations.
</p></div>
    </summary>
    <updated>2020-05-07T01:21:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02578</id>
    <link href="http://arxiv.org/abs/2005.02578" rel="alternate" type="text/html"/>
    <title>Differentiable Greedy Submodular Maximization with Guarantees and Gradient Estimators</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sakaue:Shinsaku.html">Shinsaku Sakaue</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02578">PDF</a><br/><b>Abstract: </b>We consider making outputs of the greedy algorithm for monotone submodular
function maximization differentiable w.r.t. parameters of objective functions.
Due to the non-continuous behavior of the algorithm, we must use some smoothing
methods. Our contribution is a theoretically guaranteed and widely applicable
smoothing framework based on randomization. We prove that our smoothed greedy
algorithm almost recovers original approximation guarantees in expectation for
the cases of cardinality and $\kappa$-extensible system constrains. We also
show that unbiased gradient estimators of any expected output-dependent
quantities can be efficiently obtained by sampling outputs. We confirm the
utility and effectiveness of our framework by applying it to sensitivity
analysis of the greedy algorithm and decision-focused learning of parameterized
submodular models.
</p></div>
    </summary>
    <updated>2020-05-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02537</id>
    <link href="http://arxiv.org/abs/2005.02537" rel="alternate" type="text/html"/>
    <title>Conditional Cuckoo Filters</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Ting:Daniel.html">Daniel Ting</a>, Rick Cole <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02537">PDF</a><br/><b>Abstract: </b>Bloom filters, cuckoo filters, and other approximate set membership sketches
have a wide range of applications. Oftentimes, expensive operations can be
skipped if an item is not in a data set. These filters provide an inexpensive,
memory efficient way to test if an item is in a set and avoid unnecessary
operations. Existing sketches only allow membership testing for single set.
However, in some applications such as join processing, the relevant set is not
fixed and is determined by a set of predicates.
</p>
<p>We propose the Conditional Cuckoo Filter, a simple modification of the cuckoo
filter that allows for set membership testing given predicates on a
pre-computed sketch. This filter also introduces a novel chaining technique
that enables cuckoo filters to handle insertion of duplicate keys. We evaluate
our methods on a join processing application and show that they significantly
reduce the number of tuples that a join must process.
</p></div>
    </summary>
    <updated>2020-05-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02530</id>
    <link href="http://arxiv.org/abs/2005.02530" rel="alternate" type="text/html"/>
    <title>Approximation Algorithms for Multi-Robot Patrol-Scheduling with Min-Max Latency</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Afshani:Peyman.html">Peyman Afshani</a>, Mark De Berg, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Buchin:Kevin.html">Kevin Buchin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Jie.html">Jie Gao</a>, Maarten Loffler, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nayyeri:Amir.html">Amir Nayyeri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raichel:Benjamin.html">Benjamin Raichel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sarkar:Rik.html">Rik Sarkar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Haotian.html">Haotian Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Hao=Tsung.html">Hao-Tsung Yang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02530">PDF</a><br/><b>Abstract: </b>We consider the problem of finding patrol schedules for $k$ robots to visit a
given set of $n$ sites in a metric space. Each robot has the same maximum speed
and the goal is to minimize the weighted maximum latency of any site, where the
latency of a site is defined as the maximum time duration between consecutive
visits of that site. The problem is NP-hard, as it has the traveling salesman
problem as a special case (when $k=1$ and all sites have the same weight). We
present a polynomial-time algorithm with an approximation factor of $O(k \log
\frac{w_{max}}{w_{min}})$ to the optimal solution, where $w_{max}$ and
$w_{min}$ are the maximum and minimum weight of the sites respectively.
Further, we consider the special case where the sites are in 1D. When all sites
have the same weight, we present a polynomial-time algorithm to solve the
problem exactly. When the sites may have different weights, we use dynamic
programming to generate an $8$-approximate solution, which also runs in
polynomial time.
</p></div>
    </summary>
    <updated>2020-05-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02478</id>
    <link href="http://arxiv.org/abs/2005.02478" rel="alternate" type="text/html"/>
    <title>On the list recoverability of randomly punctured codes</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lund:Ben.html">Ben Lund</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potukuchi:Aditya.html">Aditya Potukuchi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02478">PDF</a><br/><b>Abstract: </b>We show that a random puncturing of a code with good distance is list
recoverable beyond the Johnson bound. In particular, this implies that there
are Reed-Solomon codes that are list recoverable beyond the Johnson bound. It
was previously known that there are Reed-Solomon codes that do not have this
property. As an immediate corollary to our main theorem, we obtain better
degree bounds on unbalanced expanders that come from Reed-Solomon codes.
</p></div>
    </summary>
    <updated>2020-05-07T01:21:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02421</id>
    <link href="http://arxiv.org/abs/2005.02421" rel="alternate" type="text/html"/>
    <title>Spoofing Linear Cross-Entropy Benchmarking in Shallow Quantum Circuits</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Barak:Boaz.html">Boaz Barak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chou:Chi=Ning.html">Chi-Ning Chou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Xun.html">Xun Gao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02421">PDF</a><br/><b>Abstract: </b>The linear cross-entropy benchmark (Linear XEB) has been used as a test for
procedures simulating quantum circuits. Given a quantum circuit $C$ with $n$
inputs and outputs and purported simulator whose output is distributed
according to a distribution $p$ over $\{0,1\}^n$, the linear XEB fidelity of
the simulator is $\mathcal{F}_{C}(p) = 2^n \mathbb{E}_{x \sim p} q_C(x) -1$
where $q_C(x)$ is the probability that $x$ is output from the distribution
$C|0^n\rangle$. A trivial simulator (e.g., the uniform distribution) satisfies
$\mathcal{F}_C(p)=0$, while Google's noisy quantum simulation of a 53 qubit
circuit $C$ achieved a fidelity value of $(2.24\pm0.21)\times10^{-3}$ (Arute
et. al., Nature'19).
</p>
<p>In this work we give a classical randomized algorithm that for a given
circuit $C$ of depth $d$ with Haar random 2-qubit gates achieves in expectation
a fidelity value of $\Omega(\tfrac{n}{L} \cdot 15^{-d})$ in running time
$\textsf{poly}(n,2^L)$. Here $L$ is the size of the \emph{light cone} of $C$:
the maximum number of input bits that each output bit depends on. In
particular, we obtain a polynomial-time algorithm that achieves large fidelity
of $\omega(1)$ for depth $O(\sqrt{\log n})$ two-dimensional circuits. To our
knowledge, this is the first such result for two dimensional circuits of
super-constant depth. Our results can be considered as an evidence that fooling
the linear XEB test might be easier than achieving a full simulation of the
quantum circuit.
</p></div>
    </summary>
    <updated>2020-05-07T01:20:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.01929</id>
    <link href="http://arxiv.org/abs/2005.01929" rel="alternate" type="text/html"/>
    <title>Edge-Weighted Online Bipartite Matching</title>
    <feedworld_mtime>1588809600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fahrbach:Matthew.html">Matthew Fahrbach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Zhiyi.html">Zhiyi Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tao:Runzhou.html">Runzhou Tao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zadimoghaddam:Morteza.html">Morteza Zadimoghaddam</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.01929">PDF</a><br/><b>Abstract: </b>Online bipartite matching and its variants are among the most fundamental
problems in the online algorithms literature. Karp, Vazirani, and Vazirani
(STOC 1990) introduced an elegant algorithm for the unweighted problem that
achieves an optimal competitive ratio of $1-1/e$. Later, Aggarwal et al. (SODA
2011) generalized their algorithm and analysis to the vertex-weighted case.
Little is known, however, about the most general edge-weighted problem aside
from the trivial $1/2$-competitive greedy algorithm. In this paper, we present
the first online algorithm that breaks the long-standing $1/2$ barrier and
achieves a competitive ratio of at least $0.5086$. In light of the hardness
result of Kapralov, Post, and Vondr\'ak (SODA 2013) that restricts beating a
$1/2$ competitive ratio for the more general problem of monotone submodular
welfare maximization, our result can be seen as strong evidence that
edge-weighted bipartite matching is strictly easier than submodular welfare
maximization in the online setting.
</p>
<p>The main ingredient in our online matching algorithm is a novel subroutine
called online correlated selection (OCS), which takes a sequence of pairs of
vertices as input and selects one vertex from each pair. Instead of using a
fresh random bit to choose a vertex from each pair, the OCS negatively
correlates decisions across different pairs and provides a quantitative measure
on the level of correlation. We believe our OCS technique is of independent
interest and will find further applications in other online optimization
problems.
</p></div>
    </summary>
    <updated>2020-05-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/075</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/075" rel="alternate" type="text/html"/>
    <title>TR20-075 |  Rigid Matrices From Rectangular PCPs | 

	Amey Bhangale, 

	Prahladh Harsha, 

	Orr Paradise, 

	Avishay Tal</title>
    <summary>We introduce a variant of PCPs, that we refer to as *rectangular* PCPs, wherein proofs are thought of as square matrices, and the random coins used by the verifier can be partitioned into two disjoint sets, one determining the *row* of each query and the other determining the *column*.

We construct PCPs that are *efficient*, *short*, *smooth* and (almost-)*rectangular*. As a key application, we show that proofs for hard languages in NTIME$(2^n)$, when viewed as matrices, are rigid infinitely often. This strengthens and considerably simplifies a recent result of Alman and Chen [FOCS, 2019] constructing explicit rigid matrices in FNP. Namely, we prove the following theorem:
- There is a constant $\delta \in (0,1)$ such that there is an FNP-machine that, for infinitely many $N$, on input $1^N$ outputs $N \times N$ matrices with entries in $\mathbb{F}_2$ that are $\delta N^2$-far (in Hamming distance) from matrices of rank at most $2^{\log  N/\Omega(\log \log N)}$.

Our construction of rectangular PCPs starts with an analysis of how randomness yields queries in the Reed--Muller-based outer PCP of Ben-Sasson, Goldreich, Harsha, Sudan and Vadhan [SICOMP, 2006; CCC, 2005]. We then show how to preserve rectangularity under PCP composition and a smoothness-inducing transformation. This warrants refined and stronger notions of rectangularity, which we prove for the outer PCP and its transforms.</summary>
    <updated>2020-05-06T19:28:27Z</updated>
    <published>2020-05-06T19:28:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-07T19:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/074</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/074" rel="alternate" type="text/html"/>
    <title>TR20-074 |  Depth-First Search in Directed Graphs, Revisited | 

	Eric Allender, 

	Archit Chauhan, 

	Samir Datta</title>
    <summary>We present an algorithm for constructing a depth-first search tree in planar digraphs; the algorithm can be implemented in the complexity class UL, which is contained in nondeterministic logspace NL, which in turn lies in NC^2. Prior to this (for more than a quarter-century), the fastest uniform deterministic parallel algorithm for this problem was O(log^10 n) (corresponding to the complexity class AC^10, which is contained in NC^11).

We also consider the problem of computing depth-first search trees in other classes of graphs, and obtain additional new upper bounds.</summary>
    <updated>2020-05-06T18:15:38Z</updated>
    <published>2020-05-06T18:15:38Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-07T19:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02369</id>
    <link href="http://arxiv.org/abs/2005.02369" rel="alternate" type="text/html"/>
    <title>The Expander Hierarchy and its Applications to Dynamic Graph Algorithms</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goranci:Gramoz.html">Gramoz Goranci</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/R=auml=cke:Harald.html">Harald Räcke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saranurak:Thatchaphol.html">Thatchaphol Saranurak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Zihan.html">Zihan Tan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02369">PDF</a><br/><b>Abstract: </b>We introduce a notion for hierarchical graph clustering which we call the
expander hierarchy and show a fully dynamic algorithm for maintaining such a
hierarchy on a graph with $n$ vertices undergoing edge insertions and deletions
using $n^{o(1)}$ update time. An expander hierarchy is a tree representation of
graphs that faithfully captures the cut-flow structure and consequently our
dynamic algorithm almost immediately implies several results including:
</p>
<p>(1) The first fully dynamic algorithm with $n^{o(1)}$ worst-case update time
that allows querying $n^{o(1)}$-approximate conductance, $s$-$t$ maximum flows,
and $s$-$t$ minimum cuts for any given $(s,t)$ in $O(\log^{1/6} n)$ time. Our
results are deterministic and extend to multi-commodity cuts and flows. The key
idea behind these results is a fully dynamic algorithm for maintaining a tree
flow sparsifier, a notion introduced by R\"acke [FOCS'02] for constructing
competitive oblivious routing schemes.
</p>
<p>(2) A deterministic fully dynamic connectivity algorithm with $n^{o(1)}$
worst-case update time. This significantly simplifies the recent algorithm by
Chuzhoy et al.~that uses the framework of Nanongkai et al. [FOCS'17].
</p>
<p>(3) The first non-trivial deterministic fully dynamic treewidth decomposition
algorithm on constant-degree graphs with $n^{o(1)}$ worst-case update time that
maintains a treewidth decomposition of width $\text{tw}(G)\cdot n^{o(1)}$ where
$\text{tw}(G)$ denotes the treewidth of the current graph.
</p>
<p>Our technique is based on a new stronger notion of the expander
decomposition, called the boundary-linked expander decomposition. This
decomposition is more robust against updates and better captures the clustering
structure of graphs. Given that the expander decomposition has proved extremely
useful in many fields, we expect that our new notion will find more future
applications.
</p></div>
    </summary>
    <updated>2020-05-06T22:29:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02368</id>
    <link href="http://arxiv.org/abs/2005.02368" rel="alternate" type="text/html"/>
    <title>Fast Dynamic Cuts, Distances and Effective Resistances via Vertex Sparsifiers</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Li.html">Li Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goranci:Gramoz.html">Gramoz Goranci</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henzinger:Monika.html">Monika Henzinger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Richard.html">Richard Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saranurak:Thatchaphol.html">Thatchaphol Saranurak</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02368">PDF</a><br/><b>Abstract: </b>We present a general framework of designing efficient dynamic approximate
algorithms for optimization on undirected graphs. In particular, we develop a
technique that, given any problem that admits a certain notion of vertex
sparsifiers, gives data structures that maintain approximate solutions in
sub-linear update and query time. We illustrate the applicability of our
paradigm to the following problems.
</p>
<p>(1) A fully-dynamic algorithm that approximates all-pair
maximum-flows/minimum-cuts up to a nearly logarithmic factor in
$\tilde{O}(n^{2/3})$ amortized time against an oblivious adversary, and
$\tilde{O}(m^{3/4})$ time against an adaptive adversary.
</p>
<p>(2) An incremental data structure that maintains $O(1)$-approximate shortest
path in $n^{o(1)}$ time per operation, as well as fully dynamic approximate
all-pair shortest path and transshipment in $\tilde{O}(n^{2/3+o(1)})$ amortized
time per operation.
</p>
<p>(3) A fully-dynamic algorithm that approximates all-pair effective resistance
up to an $(1+\epsilon)$ factor in $\tilde{O}(n^{2/3+o(1)} \epsilon^{-O(1)})$
amortized update time per operation.
</p>
<p>The key tool behind result (1) is the dynamic maintenance of an algorithmic
construction due to Madry [FOCS' 10], which partitions a graph into a
collection of simpler graph structures (known as j-trees) and approximately
captures the cut-flow and metric structure of the graph. The
$O(1)$-approximation guarantee of (2) is by adapting the distance oracles by
[Thorup-Zwick JACM `05]. Result (3) is obtained by invoking the random-walk
based spectral vertex sparsifier by [Durfee et al. STOC `19] in a hierarchical
manner, while carefully keeping track of the recourse among levels in the
hierarchy.
</p></div>
    </summary>
    <updated>2020-05-06T22:31:08Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02329</id>
    <link href="http://arxiv.org/abs/2005.02329" rel="alternate" type="text/html"/>
    <title>Many visits TSP revisited</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Łukasz Kowalik, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Shaohua.html">Shaohua Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nadara:Wojciech.html">Wojciech Nadara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smulewicz:Marcin.html">Marcin Smulewicz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wahlstr=ouml=m:Magnus.html">Magnus Wahlström</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02329">PDF</a><br/><b>Abstract: </b>We study the Many Visits TSP problem, where given a number $k(v)$ for each of
$n$ cities and pairwise (possibly asymmetric) integer distances, one has to
find an optimal tour that visits each city $v$ exactly $k(v)$ times. The
currently fastest algorithm is due to Berger, Kozma, Mnich and Vincze [SODA
2019, TALG 2020] and runs in time and space $\mathcal{O}^*(5^n)$. They also
show a polynomial space algorithm running in time $\mathcal{O}^*(16^{n+o(n)})$.
</p>
<p>In this work, we show three main results: (i) A randomized polynomial space
algorithm in time $\mathcal{O}^*(2^nD)$, where $D$ is the maximum distance
between two cities. By using standard methods, this results in
$(1+\epsilon)$-approximation in time $\mathcal{O}^*(2^n\epsilon^{-1})$.
Improving the constant $2$ in these results would be a major breakthrough, as
it would result in improving the $\mathcal{O}^*(2^n)$-time algorithm for
Directed Hamiltonian Cycle, which is a 50 years old open problem. (ii) A tight
analysis of Berger et al.'s exponential space algorithm, resulting in
$\mathcal{O}^*(4^n)$ running time bound. (iii) A new polynomial space
algorithm, running in time $\mathcal{O}(7.88^n)$.
</p></div>
    </summary>
    <updated>2020-05-06T22:58:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02300</id>
    <link href="http://arxiv.org/abs/2005.02300" rel="alternate" type="text/html"/>
    <title>Multistage Committee Election</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bredereck:Robert.html">Robert Bredereck</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fluschnik:Till.html">Till Fluschnik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaczmarczyk:Andrzej.html">Andrzej Kaczmarczyk</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02300">PDF</a><br/><b>Abstract: </b>Electing a single committee of a small size is a classical and
well-understood voting situation. Being interested in a sequence of committees,
we introduce and study two time-dependent multistage models based on simple
Plurality voting. Therein, we are given a sequence of voting profiles (stages)
over the same set of agents and candidates, and our task is to find a small
committee for each stage of high score. In the conservative model we
additionally require that any two consecutive committees have a small symmetric
difference. Analogously, in the revolutionary model we require large symmetric
differences. We prove both models to be NP-hard even for a constant number of
agents, and, based on this, initiate a parameterized complexity analysis for
the most natural parameters and combinations thereof. Among other results, we
prove both models to be in XP yet W[1]-hard regarding the number of stages, and
that being revolutionary seems to be "easier" than being conservative: If the
(upper- resp. lower-) bound on the size of symmetric differences is constant,
the conservative model remains NP-hard while the revolutionary model becomes
polynomial-time solvable.
</p></div>
    </summary>
    <updated>2020-05-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02238</id>
    <link href="http://arxiv.org/abs/2005.02238" rel="alternate" type="text/html"/>
    <title>Lower Bounds for Semi-adaptive Data Structures via Corruption</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Pavel Dvořák, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Loff:Bruno.html">Bruno Loff</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02238">PDF</a><br/><b>Abstract: </b>In a dynamic data structure problem we wish to maintain an encoding of some
data in memory, in such a way that we may efficiently carry out a sequence of
queries and updates to the data. A long-standing open problem in this area is
to prove an unconditional polynomial lower bound of a trade-off between the
update time and the query time of an adaptive dynamic data structure computing
some explicit function. Ko and Weinstein provided such lower bound for a
restricted class of {\em semi-adaptive\/} data structures, which compute the
Disjointness function. There, the data are subsets $x_1,\dots,x_k$ and $y$ of
$\{1,\dots,n\}$, the updates can modify $y$ (by inserting and removing
elements), and the queries are an index $i \in \{1,\dots,k\}$ (query $i$ should
answer whether $x_i$ and $y$ are disjoint, i.e., it should compute the
Disjointness function applied to $(x_i, y)$). The semi-adaptiveness places a
restriction in how the data structure can be accessed in order to answer a
query. We generalize the lower bound of Ko and Weinstein to work not just for
the Disjointness, but for any function having high complexity under the smooth
corruption bound.
</p></div>
    </summary>
    <updated>2020-05-06T23:20:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02218</id>
    <link href="http://arxiv.org/abs/2005.02218" rel="alternate" type="text/html"/>
    <title>On Reachable Assignments in Cycles and Cliques</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Luis Müller, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bentert:Matthias.html">Matthias Bentert</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02218">PDF</a><br/><b>Abstract: </b>The efficient and fair distribution of indivisible resources among agents is
a common problem in the field of \emph{Multi-Agent-Systems}. We consider a
graph-based version of this problem called Reachable Assignments, introduced by
Gourves, Lesca, and Wilczynski [AAAI, 2017]. The input for this problem
consists of a set of agents, a set of objects, the agent's preferences over the
objects, a graph with the agents as vertices and edges encoding which agents
can trade resources with each other, and an initial and a target distribution
of the objects, where each agent owns exactly one object in each distribution.
The question is then whether the target distribution is reachable via a
sequence of rational trades. A trade is rational when the two participating
agents are neighbors in the graph and both obtain an object they prefer over
the object they previously held. We show that Reachable Assignments is NP-hard
even when restricting the input graph to be a clique and develop an
$O(n^3)$-time algorithm for the case where the input graph is a cycle with $n$
vertices.
</p></div>
    </summary>
    <updated>2020-05-06T22:59:12Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.02143</id>
    <link href="http://arxiv.org/abs/2005.02143" rel="alternate" type="text/html"/>
    <title>A Space-Efficient Dynamic Dictionary for Multisets with Constant Time Operations</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bercea:Ioana_Oriana.html">Ioana Oriana Bercea</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Even:Guy.html">Guy Even</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.02143">PDF</a><br/><b>Abstract: </b>We consider the dynamic dictionary problem for multisets. Given an upper
bound $n$ on the total cardinality of the multiset (i.e., including
multiplicities) at any point in time, the goal is to design a data structure
that supports multiplicity queries and allows insertions and deletions to the
multiset (i.e., the dynamic setting). The data structure must be
space-efficient (the space is $1+o(1)$ times the information-theoretic lower
bound) and support all operations in constant time with high probability.
</p>
<p>In this paper, we present the first dynamic dictionary for multisets that
achieves these performance guarantees. This answers an open problem of
Arbitman, Naor and Segev (FOCS 2010). The previously best-known construction of
Pagh, Pagh and Rao (SODA 2005) supports membership in constant time,
multiplicity queries in $O(\log n)$ time in the worst case, and insertions and
deletions in constant expected amortized time. The main technical component of
our solution is a strategy for efficiently storing variable-length binary
counters using weighted balls-into-bins experiments in which balls have
logarithmic weights.
</p>
<p>We also obtain a counting filter that approximates multiplicity queries with
a one sided error, using the reduction of Carter et al. (STOC 1978). Counting
filters have received significant attention over the years due to their
applicability in practice.We present the first counting filter with constant
time operations.
</p></div>
    </summary>
    <updated>2020-05-06T22:55:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.01982</id>
    <link href="http://arxiv.org/abs/2005.01982" rel="alternate" type="text/html"/>
    <title>Envy-free cake cutting: A polynomial number of queries with high probability</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Ch=egrave=ze:Guillaume.html">Guillaume Chèze</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.01982">PDF</a><br/><b>Abstract: </b>In this article we propose a probabilistic framework in order to study the
fair division of a divisible good, e.g. a cake, between n players. Our
framework follows the same idea than the ''Full independence model'' used in
the study of fair division of indivisible goods. We show that, in this
framework, there exists an envy-free division algorithm satisfying the
following probability estimate:$$\mathbb{P}\big( C(\mu_1, \ldots,\mu_n) \geq
n^{7+b}\big) = \mathcal{O}\Big(n^{-\frac{b-1}{3}+1+o(1)}\Big),$$where
$\mu_1,\ldots, \mu_n$ correspond to the preferences of the $n$
players,$C(\mu_1, \ldots,\mu_n)$ is the number of queries used by the algorithm
and $b&gt;4$.In particular, this gives$$\lim_{n \rightarrow +
\infty}\mathbb{P}\big( C(\mu_1, \ldots,\mu_n) \geq n^{12}\big) = 0.$$It must be
noticed that nowadays few things are known about the complexity of envy-free
division algorithms. Indeed, Procaccia has given a lower bound in $\Omega(n^2)$
and Aziz and Mackenzie have given an upper bound in $n^{n^{n^{n^{n^{n}}}}}$. As
our estimate means that we have $C(\mu_1, \ldots, \mu_n)&lt;n^{12}$ with a high
probability, this gives a new insight on the complexity of envy-free cake
cutting algorithms.\\Our result follows from a study of Webb's algorithm and a
theorem of Tao and Vu about the smallest singular value of a random matrix.
</p></div>
    </summary>
    <updated>2020-05-06T23:20:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.01867</id>
    <link href="http://arxiv.org/abs/2005.01867" rel="alternate" type="text/html"/>
    <title>Advice for Online Knapsack With Removable Items</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=ouml=ckenhauer:Hans=Joachim.html">Hans-Joachim Böckenhauer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dreier:Jan.html">Jan Dreier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Frei:Fabian.html">Fabian Frei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rossmanith:Peter.html">Peter Rossmanith</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.01867">PDF</a><br/><b>Abstract: </b>In the proportional knapsack problem, we are given a knapsack of some
capacity and a set of variably sized items. The goal is to pack some of these
items such that they fill the knapsack as much as possible without ever
exceeding the capacity. The online version of this problem reveals the items
and their sizes not all at once but one by one. For each item, the algorithm
has to decide immediately whether to pack it or not. We consider a natural
variant of this online knapsack problem, which has been coined removable
knapsack and we denote by RemKnap. It differs from the classical variant by
allowing the removal of any packed item from the knapsack. Repacking is
impossible, however: Once an item is removed, it is gone for good.
</p>
<p>We analyze the advice complexity of this problem. It measures how many advice
bits an omniscient oracle needs to provide for an online algorithm to reach any
given competitive ratio, which is--understood in its strict sense--just the
algorithm's approximation factor. The online knapsack problem without
removability is known for its peculiar advice behavior involving three jumps in
competitivity. We show that the advice complexity of RemKnap is quite different
but just as interesting. The competitivity starts from the golden ratio when no
advice is given. It then drops down in small increments to (1 + epsilon) for a
constant amount of advice already, which requires logarithmic advice in the
classical version. Removability comes as no relief to the perfectionist,
however: Optimality still requires one full advice bit for every single item in
the instance as before.
</p>
<p>These results are particularly noteworthy from a structural viewpoint for the
exceptionally slow transition from near-optimality to optimality; such a steep
jump up from constant to full linear advice for just an infinitesimally small
improvement is unique among the online problems examined so far.
</p></div>
    </summary>
    <updated>2020-05-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.01824</id>
    <link href="http://arxiv.org/abs/2005.01824" rel="alternate" type="text/html"/>
    <title>Complexity of $C_k$-coloring in hereditary classes of graphs</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chudnovsky:Maria.html">Maria Chudnovsky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Shenwei.html">Shenwei Huang</a>, Paweł Rzążewski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Spirkl:Sophie.html">Sophie Spirkl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhong:Mingxian.html">Mingxian Zhong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.01824">PDF</a><br/><b>Abstract: </b>For a graph $F$, a graph $G$ is \emph{$F$-free} if it does not contain an
induced subgraph isomorphic to $F$. For two graphs $G$ and $H$, an
\emph{$H$-coloring} of $G$ is a mapping $f:V(G)\rightarrow V(H)$ such that for
every edge $uv\in E(G)$ it holds that $f(u)f(v)\in E(H)$. We are interested in
the complexity of the problem $H$-{\sc Coloring}, which asks for the existence
of an $H$-coloring of an input graph $G$. In particular, we consider $H$-{\sc
Coloring} of $F$-free graphs, where $F$ is a fixed graph and $H$ is an odd
cycle of length at least 5. This problem is closely related to the well known
open problem of determining the complexity of 3-{\sc Coloring} of $P_t$-free
graphs.
</p>
<p>We show that for every odd $k \geq 5$ the $C_k$-{\sc Coloring} problem, even
in the list variant, can be solved in polynomial time in $P_9$-free graphs. The
algorithm extends for the case of list version of $C_k$-{\sc Coloring}, where
$k$ is an even number of length at least 10.
</p>
<p>On the other hand, we prove that if some component of $F$ is not a subgraph
of a subdividecd claw, then the following problems are NP-complete in $F$-free
graphs: a)extension version of $C_k$-{\sc Coloring} for every odd $k \geq 5$,
b) list version of $C_k$-{\sc Coloring} for every even $k \geq 6$.
</p></div>
    </summary>
    <updated>2020-05-06T22:25:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.01778</id>
    <link href="http://arxiv.org/abs/2005.01778" rel="alternate" type="text/html"/>
    <title>Determining the Multiplicative Complexity of Boolean Functions using SAT</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soeken:Mathias.html">Mathias Soeken</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.01778">PDF</a><br/><b>Abstract: </b>We present a constructive SAT-based algorithm to determine the multiplicative
complexity of a Boolean function, i.e., the smallest number of AND gates in any
logic network that consists of 2-input AND gates, 2-input XOR gates, and
inverters. In order to speed-up solving time, we make use of several symmetry
breaking constraints; these exploit properties of XAGs that may be useful
beyond the proposed SAT-based algorithm. We further propose a heuristic
post-optimization algorithm to reduce the number of XOR gates once the optimum
number of AND gates has been obtained, which also makes use of SAT solvers. Our
algorithm is capable to find all optimum XAGs for representatives of all
5-input affine-equivalent classes, and for a set of frequently occurring
6-input functions.
</p></div>
    </summary>
    <updated>2020-05-06T22:56:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.01757</id>
    <link href="http://arxiv.org/abs/2005.01757" rel="alternate" type="text/html"/>
    <title>Sample Complexity of Uniform Convergence for Multicalibration</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Eliran Shabat, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Lee.html">Lee Cohen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mansour:Yishay.html">Yishay Mansour</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.01757">PDF</a><br/><b>Abstract: </b>There is a growing interest in societal concerns in machine learning systems,
especially in fairness. Multicalibration gives a comprehensive methodology to
address group fairness. In this work, we address the multicalibration error and
decouple it from the prediction error. The importance of decoupling the
fairness metric (multicalibration) and the accuracy (prediction error) is due
to the inherent trade-off between the two, and the societal decision regarding
the "right tradeoff" (as imposed many times by regulators). Our work gives
sample complexity bounds for uniform convergence guarantees of multicalibration
error, which implies that regardless of the accuracy, we can guarantee that the
empirical and (true) multicalibration errors are close. We emphasize that our
results: (1) are more general than previous bounds, as they apply to both
agnostic and realizable settings, and do not rely on a specific type of
algorithm (such as deferentially private), (2) improve over previous
multicalibration sample complexity bounds and (3) implies uniform convergence
guarantees for the classical calibration error.
</p></div>
    </summary>
    <updated>2020-05-06T22:50:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.00575</id>
    <link href="http://arxiv.org/abs/2005.00575" rel="alternate" type="text/html"/>
    <title>Approximating maximum integral multiflows on bounded genus graphs</title>
    <feedworld_mtime>1588723200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Chien-chung Huang, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mari:Mathieu.html">Mathieu Mari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mathieu:Claire.html">Claire Mathieu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vygen:Jens.html">Jens Vygen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.00575">PDF</a><br/><b>Abstract: </b>We devise the first constant-factor approximation algorithm for finding an
integral multi-commodity flow of maximum total value for instances where the
supply graph together with the demand edges can be embedded on an orientable
surface of bounded genus. This extends recent results for planar instances.
</p></div>
    </summary>
    <updated>2020-05-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/073</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/073" rel="alternate" type="text/html"/>
    <title>TR20-073 |  Lower Bounds on OBDD Proofs with Several Orders | 

	Dmitry Itsykson, 

	Sam Buss, 

	Dmitry Sokolov, 

	Alexander Knop, 

	Artur Riazanov</title>
    <summary>This paper is motivated by seeking lower bounds on OBDD($\land$, weakening, reordering) refutations, namely OBDD refutations that allow weakening and arbitrary reorderings. We first work with 1-NBP($\land$) refutations based on read-once nondeterministic branching programs. These generalize OBDD($\land$, reordering) refutations. There are polynomial size 1-NBP($\land$) refutations of the pigeonhole principle, hence 1-NBP($\land$) is strictly stronger than OBDD($\land$, reordering). There are also formulas that have polynomial size tree-like resolution refutations but require exponential size 1-NBP($\land$) refutations. As a corollary, OBDD($\land$, reordering) does not simulate tree-like resolution, answering a previously open question.

The system 1-NBP($\land$, $\exists$) uses projection inferences instead of weakening. 1-NBP($\land$, $\exists_k$) is the system restricted to projection on at most $k$ distinct variables. We construct explicit constant degree graphs $G_n$ on $n$ vertices and an $\epsilon &gt; 0$, such that 1-NBP($\land$, $\exists_{\epsilon n}$) refutations of the Tseitin formula for $G_n$ require exponential size.

Second, we study the proof system OBDD($\land$, weakening, reordering$_\ell$) which allows $\ell$ different variable orders in a refutation. We prove an exponential lower bound on the complexity of tree-like OBDD($\land$, weakening, reordering$_\ell$) refutations for $\ell = \epsilon \log n$, where $n$ is the number of variables and $\epsilon &gt; 0$ is a constant. The lower bound is based on multiparty communication complexity.</summary>
    <updated>2020-05-05T18:13:59Z</updated>
    <published>2020-05-05T18:13:59Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-07T19:20:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5609460581142399437</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5609460581142399437/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/why-is-there-no-dn-grid-for-hilberts.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5609460581142399437" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5609460581142399437" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/why-is-there-no-dn-grid-for-hilberts.html" rel="alternate" type="text/html"/>
    <title>Why is there no (d,n) grid for Hilbert's Tenth Problem?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
Hilbert's 10th problem, in modern language is:<br/>
<br/>
Find an algorithm that will, given a poly over Z in many variables, determine if it has a solution in Z.<br/>
<br/>
This problem was proven undecidable through the work of Davis, Putnam, Robinson and then<br/>
Matiyasevich supplied the last crucial part of the proof.<br/>
<br/>
Let H10(d,n) be the problem with degree d and n variables.<br/>
<br/>
I had assumed that somewhere on the web would be a grid where the dth row, nth col has<br/>
<br/>
U if  H10(d,n) is undecidable<br/>
<br/>
D if H10(d,n) is decidable<br/>
<br/>
? if the status of H10(d,n) was unknown.<br/>
<br/>
I found no grid. I then collected up all the results I could find <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/h10.pdf">here</a><br/>
<br/>
This lead to the (non-math) question: Why is there no grid out there? Here are my speculations.<br/>
<br/>
1) Logicians worked on proving particular (d,n) are undecidable. They sought solutions in N. By contrast number theorists worked on proving particular (d,n) decidable. They sought solutions in Z.. Hence a grid would need to reconcile these two related problems.<br/>
<br/>
<div>
<div>
2) Logicians and number theorists didn't talk to each other. Websites and books on Hilbert's Tenth problem do not mention any solvable cases of it.</div>
</div>
<div>
<br/></div>
<div>
<div>
3) There is a real dearth of positive results, so a grid would not be that interesting. Note that we do not even know if the following is decidable: given k in Z does there exists x,y,z in Z such that</div>
<div>
<br/></div>
<div>
x^3 +y^3+ z^3 = k. I blogged about that <a href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html">here</a></div>
</div>
<div>
<br/></div>
<div>
4) For an undecidable result for (d,n) if you make n small then all of the results make d very large.</div>
<div>
<br/></div>
<div>
For example</div>
<div>
<br/></div>
<div>
n=9, d= 1.6 x 10^{45}  is undecidable. The status of n=9, d=1.6 x 10^{45} -1 is unknown.</div>
<div>
<br/></div>
<div>
Hence the grid would be hard to draw.</div>
<div>
<br/></div>
<div>
Frankly I don't really want a grid. I really want a sense of what open problems might be solved. I think progress has gone in other directions- H10 over other domains. Oh well, I want to know about</div>
<div>
<br/></div>
<div>
n=9 and d=1.6 x 10^{45}-1. (parenthesis ambiguous but either way would be an advance.)</div>
<div>
<br/></div>
<div>
<br/></div>
<div>
<br/></div></div>
    </content>
    <updated>2020-05-05T04:39:00Z</updated>
    <published>2020-05-05T04:39:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-05-07T14:28:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=48</id>
    <link href="https://dstheory.wordpress.com/2020/05/05/friday-may-15-amin-karbasi-from-yale-university/" rel="alternate" type="text/html"/>
    <title>Friday, May 15 — Amin Karbasi from Yale University</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The fourth Foundations of Data Science virtual talk will take place on Friday, May 15th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC).  Amin Karbasi from Yale University will speak about “User-Friendly Submodular Maximization”. Abstract: Submodular functions model the intuitive notion of diminishing returns. Due to their far-reaching applications, they<a class="more-link" href="https://dstheory.wordpress.com/2020/05/05/friday-may-15-amin-karbasi-from-yale-university/">Continue reading <span class="screen-reader-text">"Friday, May 15 — Amin Karbasi from Yale University"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The fourth Foundations of Data Science virtual talk will take place on Friday, May 15th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC).  <strong>Amin Karbasi </strong>from Yale University will speak about “<em>User-Friendly Submodular Maximization</em>”.</p>



<p class="has-text-align-left"><strong>Abstract</strong>: Submodular functions model the intuitive notion of diminishing returns. Due to their far-reaching applications, they have been rediscovered in many fields such as information theory, operations research, statistical physics, economics, and machine learning. They also enjoy computational tractability as they can be minimized exactly or maximized approximately.</p>



<p>The goal of this talk is simple. We see how a little bit of randomness, a little bit of greediness, and the right combination can lead to pretty good methods for offline, streaming, and distributed solutions. I do not assume any background on submodularity and try to explain all the required details during the talk.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-05-05T01:30:02Z</updated>
    <published>2020-05-05T01:30:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-05-07T19:21:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/072</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/072" rel="alternate" type="text/html"/>
    <title>TR20-072 |  Locally testable codes via high-dimensional expanders | 

	Irit Dinur, 

	Prahladh Harsha, 

	Yotam Dikstein, 

	Noga Ron-Zewi</title>
    <summary>Locally testable codes (LTC) are error-correcting codes that have a local tester which can distinguish valid codewords from words that are far from all codewords, by probing a given word only at a very small (sublinear, typically constant) number of locations. Such codes form the combinatorial backbone of PCPs. A major open problem is whether there exist LTCs with positive rate, constant relative distance and testable with a constant number of queries. 

In this paper, we present a new approach towards constructing such LTCs using the machinery of high-dimensional expanders. 
To this end, we consider the Tanner representation of a code, which is specified by a graph and a base code. Informally, our result states that if this graph is part of an {\em agreement expander} then the local testability of the code follows from the local testability of the base code. Agreement expanders allow one to stitch together many mostly-consistent local functions into a single global function. High-dimensional expanders are known to yield agreement expanders with constant degree. 

This work unifies and generalizes the known results on testability of the Hadamard, Reed-Muller and lifted codes, all of which are proved via a single round of local self-correction: the corrected value at a vertex v depends on the values of all vertices that share a constraint with v. In the above codes this set includes all of the vertices. In contrast, in our setting the degree of a vertex might be a constant, so we cannot hope for one-round self-correction. We overcome this technical hurdle by performing iterative self correction with logarithmically many rounds and tightly controlling the error in each iteration using properties of the agreement expander.

Given this result, the missing ingredient towards constructing a constant-query LTC with positive rate and constant relative distance is an instantiation of a base code and a constant-degree agreement expander that interact well with each other.</summary>
    <updated>2020-05-05T00:09:11Z</updated>
    <published>2020-05-05T00:09:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-07T19:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/04/postdoc-position-at-university-of-alberta-apply-by-december-31-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/04/postdoc-position-at-university-of-alberta-apply-by-december-31-2020/" rel="alternate" type="text/html"/>
    <title>postdoc position at University of Alberta (apply by December 31, 2020)</title>
    <summary>The Theory Group in the Dept. of Computing Science at U. of Alberta invites applications for TWO postdoc positions. The successful applicants are expected to work closely with Zachary Friggstad and Mohammad Salavatipour in the areas of: approx algorithms, hardness of approximation, combinatorial optimization. For details see https://webdocs.cs.ualberta.ca/~mreza/pdf-ad7.pdf Email: mrs@ualberta.ca Website: http://www.cs.ualberta.ca Email: mrs@ualberta.ca</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Theory Group in the Dept. of Computing Science at U. of Alberta invites applications for TWO postdoc positions. The successful applicants are expected to work closely with Zachary Friggstad and Mohammad Salavatipour in the areas of: approx algorithms, hardness of approximation, combinatorial optimization. For details see <a href="https://webdocs.cs.ualberta.ca/~mreza/pdf-ad7.pdf">https://webdocs.cs.ualberta.ca/~mreza/pdf-ad7.pdf</a><br/>
Email: mrs@ualberta.ca</p>
<p>Website: <a href="http://www.cs.ualberta.ca">http://www.cs.ualberta.ca</a><br/>
Email: mrs@ualberta.ca</p></div>
    </content>
    <updated>2020-05-04T18:43:57Z</updated>
    <published>2020-05-04T18:43:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-07T19:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/071</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/071" rel="alternate" type="text/html"/>
    <title>TR20-071 |  A Tight Lower Bound on Adaptively Secure Full-Information Coin Flip | 

	Iftach Haitner, 

	Yonatan Karidi-Heller</title>
    <summary>In a distributed coin-flipping protocol, Blum [ACM Transactions on Computer Systems '83],
the parties try to output a common (close to) uniform bit, even when some adversarially chosen parties try to bias the common output. In an adaptively secure full-information coin flip, Ben-Or and Linial [FOCS '85], the parties communicate over a broadcast channel and a computationally unbounded adversary can choose which parties to corrupt during the protocol execution. Ben-Or and Linial proved that the $n$-party majority protocol is resilient to $o(\sqrt{n})$ corruptions (ignoring log factors), and conjectured this is a tight upper bound for any $n$-party protocol (of any round complexity). Their conjecture was proved to be correct for single-turn (each party sends a single message) single-bit (a message is one bit) protocols, Lichtenstein, Linial, and Saks [Combinatorica '89], symmetric protocols Goldwasser, Kalai, and Park [ICALP '15], and recently for (arbitrary message length) single-turn protocols Tauman Kalai, Komargodski, and Raz [DISC '18]. Yet, the question for many-turn (even single-bit) protocols was left completely open.

In this work we close the above gap, proving that no $n$-party protocol (of any round complexity) is resilient to $O(\sqrt{n})$ (adaptive) corruptions.</summary>
    <updated>2020-05-04T14:14:22Z</updated>
    <published>2020-05-04T14:14:22Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-07T19:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/070</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/070" rel="alternate" type="text/html"/>
    <title>TR20-070 |  On the list recoverability of randomly punctured codes | 

	Ben Lund, 

	Aditya Potukuchi</title>
    <summary>We show that a random puncturing of a code with good distance is list recoverable beyond the Johnson bound.
In particular, this implies that there are Reed-Solomon codes that are list recoverable beyond the Johnson bound.
It was previously known that there are Reed-Solomon codes that do not have this property. 
As an immediate corollary to our main theorem, we obtain better degree bounds on unbalanced expanders that come from Reed-Solomon codes.</summary>
    <updated>2020-05-04T09:06:23Z</updated>
    <published>2020-05-04T09:06:23Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-07T19:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1297</id>
    <link href="https://ptreview.sublinear.info/?p=1297" rel="alternate" type="text/html"/>
    <title>News for April 2020</title>
    <summary>April is now behind us, and we hope you and your families are all staying safe and healthy. We saw six seven property papers appear online last month, so at least there is some reading ahead of us! A mixture of privacy, quantum, high-dimensional distributions, and juntas (juntæ?). A lot of distribution testing, overall. Connecting […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>April is now behind us, and we hope you and your families are all staying safe and healthy. We saw <s>six</s> seven property papers appear online last month, so at least there is some reading ahead of us! A mixture of privacy, quantum, high-dimensional distributions, and juntas (juntæ?). A lot of distribution testing, overall.</p>



<p><strong>Connecting Robust Shuffle Privacy and Pan-Privacy</strong>, by Victor Balcer, Albert Cheu, Matthew Joseph, and Jieming Mao (<a href="https://arxiv.org/abs/2004.09481">arXiv</a>). This paper considers a recent notion of differential privacy called<em> shuffle privacy</em>, where users have sensitive data, a central untrusted server wants to do something with that data (for instance, say… testing its distribution), and a trusted middle-man/entity shuffles the users’ messages u.a.r. to bring in a bit more anonymity. As it turns out, testing uniformity (or identity) of distributions in the shuffle privacy model is (i) much harder than without privacy constraints; (ii) much harder than with ‘usual’ (weaker) differential privacy (iii) much easier than with local privacy; (iv) related to the sample complexity under another privacy notion, <em>pan-privacy</em>. It’s a brand exciting new world out there!</p>



<p><em>(Note: for the reader interested in keeping track of identity/uniformity testing of probability distributions under various privacy models, I wrote a very short summary of the current results <a href="https://github.com/ccanonne/probabilitydistributiontoolbox/blob/master/private-goodness-of-fit.pdf">here</a>.)</em></p>



<p><strong>Entanglement is Necessary for Optimal Quantum Property Testing, </strong>by Sebastien Bubeck, Sitan Chen, and Jerry Li (<a href="https://arxiv.org/abs/2004.07869">arXiv</a>). The analogue of uniformity testing, in the quantum world, is testing whether a quantum state is equal (or far from) the maximally mixed state. It’s known that this task  has “quantum sample complexity” (number of measurements) \(\Theta(d/\varepsilon^2)\) (i.e., square root dependence on  the dimension of the state, \(d^2\)). But this requires <em>entangled</em> measurements, which may be tricky to get (or, in my case, understand): what happens if the measurements can be adaptive, but not entangled? In this work, the authors show that, under this weaker access model \(\Omega(d^{4/3}/\varepsilon^2)\) measurements are necessary: adaptivity alone won’t cut it. It may still help though: without either entanglement <em>nor</em> adaptivity, the authors also show a \(\Omega(d^{3/2}/\varepsilon^2)\) measurements lower bound.</p>



<p><strong>Testing Data Binnings</strong>, by Clément Canonne and Karl Wimmer (<a href="https://eccc.weizmann.ac.il/report/2020/062/">ECCC</a>). More identity testing! Not private and not quantum for this one, but… not <em>quite</em> identity testing either. To paraphrase the abstract: this paper introduces (and gives near matching bounds for)  the related question of <em>identity up to binning</em>, where the reference distribution \(q\) is over \(k \ll n\) elements: the question is then whether there exists a suitable binning of the domain \([n]\) into \(k\) intervals such that, <em>once binned</em>, \(p\) is equal to \(q\).” </p>



<p><strong>Hardness of Identity Testing for Restricted Boltzmann Machines and Potts models</strong>, by Antonio Blanca, Zongchen Chen, Daniel Štefankovič, and Eric Vigoda (<a href="https://arxiv.org/abs/2004.10805">arXiv</a>). Back to identity testing of distributions, but for high-dimensional structured ones this one. Specifically, this paper focuses on the undirected graphical models known as <em>restricted Boltzmann machines, </em>and provides efficient algorithms for identity testing and conditional hardness lower bounds depending on the type of correlations allowed in the graphical models.</p>



<p><strong>Robust testing of low-dimensional functions</strong>, by Anindya De, Elchanan Mossel, and Joe Neeman (<a href="https://arxiv.org/abs/2004.11642">arXiv</a>). Junta testing is a classical, central problem in property testing, with motivations and applications in machine learning and complexity. The related (and equally well-motivated) question of junta testing of functions on \(\mathbb{R}^d\) (instead of the Boolean hypercube) was recently studied by the same authors; and the related (and, again, equally well-motivated) question of <em>tolerant</em> junta testing on the Boolean hypercube was also recently studied (among other works) by the same authors. Well, this paper does it all, and tackles the challenging (and, for a change, equally well-motivated!) question of <em>tolerant</em> testing of juntas  on \(\mathbb{R}^d\).</p>



<p><strong>Differentially Private Assouad, Fano, and Le Cam</strong>, by Jayadev Acharya, Ziteng Sun, and Huanyu Zhang (<a href="https://arxiv.org/abs/2004.06830">arXiv</a>). Back to probability distributions and privacy. This paper provides differentially private analogues of the classical eponymous statistical inference results (Assouad’s lemma, Fano’s inequality, and Le Cam’s method). In particular, it gives ready-to-use, blackbox tools to prove testing and learning lower bounds for distributions in the differentially private setting, and shows how to use them to easily derive, and rederive, several lower bounds.</p>



<p><strong>Edit: </strong>We missed one!</p>



<p><strong>Learning and Testing Junta Distributions with Subcube Conditioning</strong>, by Xi Chen, Rajesh Jayaram, Amit Levi, Erik Waingarten (<a href="https://arxiv.org/abs/2004.12496">arXiv</a>). This paper focuses on the <em>subcube conditioning</em> model of (high-dimensional) distribution testing, where the algorithm can fix some variables to values of its choosing and get samples conditioned on those variables. Extending and refining techniques from <a href="https://ptreview.sublinear.info/?p=1227">a previous work by a (sub+super)set of the authors</a>, the paper shows how to optimally learn and test <a href="http://proceedings.mlr.press/v49/aliakbarpour16.html">junta distributions</a> in this framework—with exponential savings with respect to the usual i.i.d. sampling model.</p></div>
    </content>
    <updated>2020-05-04T01:52:43Z</updated>
    <published>2020-05-04T01:52:43Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-05-06T23:02:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blog.simons.berkeley.edu/?p=164</id>
    <link href="https://blog.simons.berkeley.edu/2020/05/fine-grained-hardness-of-lattice-problems-open-questions/" rel="alternate" type="text/html"/>
    <title>Fine-grained hardness of lattice problems: Open questions</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">1 Introduction 1.1 Lattices and lattice-based cryptography Lattices are classically-studied geometric objects that in the past few decades have found a multitude of applications in computer science. The most important application area is lattice-based cryptography, the design of cryptosystems whose … <a href="https://blog.simons.berkeley.edu/2020/05/fine-grained-hardness-of-lattice-problems-open-questions/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h1>1 Introduction</h1>
<h2>1.1 Lattices and lattice-based cryptography</h2>
<p>Lattices are classically-studied geometric objects that in the past few decades have found a multitude of applications in computer science. The most important application area is <em>lattice-based cryptography</em>, the design of cryptosystems whose security is based on the apparent intractability of computational problems on lattices, even for quantum computers. Indeed, lattice-based cryptography has revolutionized the field because of its apparent quantum resistance and its other attractive security, functionality, and efficiency properties.</p>
<p>Intuitively, a lattice is a regular ordering of points in some (typically high-dimensional) space. More precisely, a <em>lattice</em> \( {{\cal{L}}}\) of rank \( {n}\) is the set of all integer linear combinations of some linearly independent vectors \( {\mathbf{b}_1, \ldots, \mathbf{b}_n}\), which are called a <em>basis</em> of \( {{\cal{L}}}\). We will be primarily interested in analyzing the running times of lattice algorithms as functions of the lattice’s rank \( {n}\).</p>
<h2>1.2. Computational lattice problems</h2>
<p>The two most important computational problems on lattices are the Shortest Vector Problem (SVP) and the Closest Vector Problem (CVP). SVP asks, given a basis of a lattice \( {{\cal{L}}}\) as input, to find a shortest non-zero vector in \( {{\cal{L}}}\). CVP, which can be viewed as an inhomogeneous version of SVP, asks, given a basis of a lattice \( {{\cal{L}}}\) and a target point \( {\mathbf{t}}\) as input, to find a closest vector in \( {{\cal{L}}}\) to \( {\mathbf{t}}\).</p>
<p>Algorithms for solving SVP form the core of the best known attacks on lattice-based cryptography both in theory and in practice. Accordingly, it is critical to understand the precise complexity of SVP as well as possible. The best provably correct algorithms for both SVP and CVP run in \( {2^{n + o(n)}}\)-time [<a href="https://arxiv.org/abs/1412.7994">ADRS15</a>, <a href="https://arxiv.org/abs/1504.01995">ADS15</a>, <a href="https://arxiv.org/abs/1709.01535">AS18a</a>]. The best heuristic algorithms for SVP run in \( {2^{cn + o(n)}}\)-time for \( {c = 0.292}\) classically [<a href="https://eprint.iacr.org/2015/1128">BDGL16</a>] and \( {c = 0.265}\) using quantum speedups [<a href="http://www.thijs.com/docs/phd-final.pdf">Laa15</a>] (see also [<a href="https://eprint.iacr.org/2019/1016">KMPR19</a>]), and most real-world lattice-based cryptosystems assume that these algorithms are close to optimal. Indeed, many of these cryptosystems assume what Bos et al. [<a href="https://eprint.iacr.org/2016/659">B+16</a>] call a “paranoid” worst-case estimate of \( {c = 0.2075}\) (based on the kissing number and assuming that sieving algorithms are optimal) as the fastest hypothetical running time for SVP algorithms when choosing parameters. (See also Albrecht et al. [<a href="https://estimate-all-the-lwe-ntru-schemes.github.io/paper.pdf">A+18</a>], which surveys the security assumptions made in a wide range of lattice-based cryptosystems.) Accordingly, the difference in being able to solve SVP in \( {2^{0.2075n}}\) versus \( {2^{n/20}}\) versus \( {2^{\sqrt{n}}}\) time may mean the difference between lattice-based cryptosystems being secure, insecure with current parameters, or effectively broken in practice.</p>
<p>There is a rank-preserving reduction from SVP to CVP [<a href="https://cseweb.ucsd.edu/~daniele/papers/GMSS.pdf">GMSS99</a>], so any algorithm for CVP immediately gives an essentially equally fast algorithm for SVP. In other words, CVP is at least as hard as SVP (and probably a bit harder). Indeed, historically, almost all lower bounds for SVP are proven via reduction from CVP (and nearly all algorithmic progress on CVP uses ideas originally developed for SVP).</p>
<h2>1.3. Fine-grained hardness</h2>
<p>The field of fine-grained complexity works to give strong, quantitative lower bounds on computational problems assuming standard complexity-theoretic assumptions. Proving such a (conditional) lower bound for an \( {{\mathsf{NP}}}\)-hard problem generally works by (1) assuming a stronger hardness assumption than \( {{\mathsf{P}} \neq {\mathsf{NP}}}\) about the complexity of \( {k}\)-SAT (such as ETH or SETH, defined below), and (2) giving a highly efficient reduction from \( {k}\)-SAT to the problem. The most important hardness assumptions for giving lower bounds on \( {{\mathsf{NP}}}\)-hard problems are the Exponential Time Hypothesis (ETH) and the Strong Exponential Time Hypothesis (SETH) of Impagliazzo and Paturi [<a href="https://cseweb.ucsd.edu/~paturi/myPapers/pubs/ImpagliazzoPaturi_2001_jcss.pdf">IP01</a>]. ETH asserts that there is no \( {2^{o(n)}}\)-time algorithm for \( {3}\)-SAT, and SETH asserts that for every \( {\epsilon &gt; 0}\) there exists \( {k \in {\mathbb Z}^+}\) such that there is no \( {2^{(1 – \epsilon)n}}\)-time algorithm for \( {k}\)-SAT, where \( {n}\) denotes the number of variables in the SAT instance.</p>
<p>Here by “highly efficient” reductions we mean linear ones, i.e., reductions that map a \( {3}\)-SAT or \( {k}\)-SAT formula on \( {n}\) variables to an SVP or CVP instance of rank \( {C n + o(n)}\) for some absolute constant \( {C &gt; 0}\). Indeed, by giving a reduction from \( {3}\)-SAT (respectively, \( {k}\)-SAT for any \( {k \in {\mathbb Z}^+}\)) instances on \( {n}\) variables to SVP or CVP instances of rank \( {C n + o(n)}\), we can conclude that there is no \( {2^{o(n)}}\)-time (resp., \( {2^{(1-\epsilon)n/C}}\)-time for any \( {\epsilon &gt; 0}\)) algorithm for the corresponding problem assuming ETH (resp., SETH). Note that the smaller the value of \( {C}\) for which one can show such a reduction, the stronger the conclusion. In particular, a reduction mapping \( {k}\)-SAT instances on \( {n}\) variables to SVP or CVP instances of rank \( {n + o(n)}\) would imply an essentially tight lower bound on the corresponding problem assuming SETH — as mentioned above, the best provably correct algorithms for both SVP and CVP run in time \( {2^{n + o(n)}}\).</p>
<h2>1.4. Fine-grained hardness of CVP (and SVP)</h2>
<p>It is relatively easy to show that CVP is “ETH-hard,” i.e., to show that a \( {2^{o(n)}}\)-time algorithm for CVP would imply a \( {2^{o(n)}}\)-time algorithm for \( {3}\)-SAT instances with \( {n}\) variables. This would falsify ETH. (It’s a nice exercise to show that the Subset Sum problem on a set of size \( {n}\) reduces to CVP on a lattice of rank \( {n}\), which implies the result.)</p>
<p>With some work, Divesh Aggarwal and Noah extended this to SVP [<a href="https://arxiv.org/abs/1712.00942">AS18b</a>]. In particular, we showed a reduction from CVP to SVP that only increases the rank of the lattice by some constant multiplicative factor. (Formally, the reduction only works with certain minor constraints on the CVP instance. The reduction originally relied on a geometric conjecture, which was open for decades. But, Serge Vlăduţ proved the conjecture [<a href="https://arxiv.org/abs/1802.00886">Vlă19</a>] shortly after we published!)</p>
<p>So, unless ETH is false, there is no \( {2^{o(n)}}\)-time algorithm for CVP or SVP. But, for cryptographic applications, even, say, a \( {2^{n/20}}\)-time algorithm would be completely devastating. If such an algorithm were found, cryptographic schemes that we currently think are secure against absurdly powerful attackers straight out of science fiction (say, one with a computer the size of the sun running until the heat death of the universe) would turn out to be easily broken (e.g., in seconds on our laptops).</p>
<p>In [<a href="https://arxiv.org/abs/1704.03928">BGS17</a>, <a href="https://arxiv.org/abs/1911.02440">ABGS20</a>], we <em>almost</em> showed that CVP is “SETH-hard,” i.e., that a \( {2^{(1-\epsilon)n}}\)-time algorithm for CVP would imply such an algorithm for \( {k}\)-SAT for <em>any</em> constant \( {k}\). This would falsify SETH. So, we <em>almost</em> showed that the [<a href="https://arxiv.org/abs/1504.01995">ADS15</a>] algorithm is optimal. The “almost” is because our proof works with \( {\ell_p}\) norms, that is, we show hardness for the version of CVP in which the distance from the target to a lattice vector is defined in terms of the \( {\ell_p}\) norm,</p>
<p align="center">\( \displaystyle \|\mathbf{x}\|_p := (|x_1|^p + \cdots + |x_d|^p)^{1/p} \; . \)</p>
<p>We call the corresponding problem \( {{\mathrm{CVP}}_p}\). In fact, our proof works for all \( {\ell_p}\) norms <em>except</em> when \( {p}\) is an even integer. (To see why this might happen, notice \( {\|\mathbf{x}\|_p^p}\) is a polynomial in the \( {x_i}\) if and only if \( {p}\) is an even integer. In fact, there’s some sense in which “\( {\ell_2}\) is the easiest norm,” because for any \( {p}\), there is a linear map \( {A \in {\mathbb R}^{d \times m}}\) such that \( {m}\) is not too large and \( {\|\mathbf{x}\|_2 \approx \|A \mathbf{x}\|_p}\).) Of course, we are most interested in the case \( {p= 2}\) (the only case for which the [<a href="https://arxiv.org/abs/1504.01995">ADS15</a>] algorithm works), which is an even integer! Indeed, for all \( {p \neq 2}\), the fastest known algorithm for CVP is still Ravi Kannan’s \( {n^{O(n)}}\)-time algorithm from 1987 [<a href="https://kilthub.cmu.edu/articles/Minkowski_s_convex_body_theorem_and_integer_programming/6607328/files/12097865.pdf">Kan87</a>]. (For SVP and for constant-factor approximate CVP, \(2^{O(n)}\)-time algorithms are known [<a href="https://arxiv.org/abs/1011.5666">DPV11</a>].)</p>
<p>In fact, we showed that for \( {p = 2}\), no “natural” reduction can rule out a \( {2^{3n/4}}\)-time algorithm for CVP under SETH. A “natural” reduction is one with a fixed bijection between witnesses. In particular, any “natural” reduction from \( {3}\)-SAT to CVP must reduce to a lattice with rank at least roughly \( {4n/3}\). So, new ideas will be needed to prove stronger hardness of CVP in the \( {\ell_2}\) norm.</p>
<h1>2. Open problems</h1>
<p>We now discuss some of the problems that we left open in [<a href="https://arxiv.org/abs/1704.03928">BGS17</a>, <a href="https://arxiv.org/abs/1911.02440">ABGS20</a>]. For simplicity, we ask for specific results (e.g., “prove that problem \( {A}\) is \( {T}\)-hard under hypothesis \( {B}\)“), but of course any similar results would be very interesting (e.g., “\( {A}\) is \( {T’}\)-hard under hypothesis \( {B’}\)“).</p>
<h2>2.1. Hardness in the \(\ell_2\) norm</h2>
<p>The most obvious question that we left open is, of course, to prove similar \( {2^n}\)-time hardness results for \( {{\mathrm{CVP}}_2}\) (and more generally for \( {{\mathrm{CVP}}_p}\) for even integers \( {p}\)).</p>
<blockquote>
<p><b>Open problem 1.</b> Show that there is no \( {2^{0.99 n}}\)-time algorithm for \( {{\mathrm{CVP}}_2}\) assuming SETH.</p>
</blockquote>
<p>Remember that we showed that any proof of such a strong result would have to use an “unnatural” reduction. So, a fundamentally different approach is needed. One potentially promising direction would be to find a Cook reduction, as our limitations only apply to Karp reductions.</p>
<p>Alternatively, one might try for a different result that gets around this “natural” reduction limitations. E.g., even the following much weaker result would be very interesting.</p>
<blockquote>
<p><b>Open problem 2.</b> Show an efficient reduction from \( {3}\)-SAT on \( {n}\) variables to \( {{\mathrm{CVP}}_2}\) on a lattice of rank \( {\approx 10n}\).</p>
</blockquote>
<p>Such a reduction to \( {{\mathrm{CVP}}_2}\) on a lattice of rank \( {Cn}\) for some large constant \( {C}\) is known by applying the Sparsification Lemma [<a href="https://cseweb.ucsd.edu/~russell/ipz.pdf">IPZ01</a>] to \( {3}\)-SAT, but showing such a reduction for any reasonably small \( {C}\) or even any explicit \( {C}\) using a different proof technique would be interesting.</p>
<p>Also, our limitations only apply to reductions that map satisfying assignments to <em>exact</em> closest vectors. So, one might try to get around our limitation by working directly with approximate versions of \( {3}\)-SAT and \( {{\mathrm{CVP}}_2}\). (In [<a href="https://arxiv.org/abs/1911.02440">ABGS20</a>], we show such reductions from Gap-\( {k}\)-SAT to constant-factor approximate \( {{\mathrm{CVP}}_p}\) for all \( {p \notin 2{\mathbb Z}}\) as well as all \( {k \leq p}\). We also show reductions from Gap-\( {k}\)-Parity that achieve relatively large approximation factors.)</p>
<blockquote>
<p><b>Open problem 3.</b> Show an efficient reduction from Gap-\( {3}\)-SAT on \( {n}\) variables to approximate \( {{\mathrm{CVP}}_2}\) on a lattice of rank \( {n}\).</p>
</blockquote>
<h2>2.2. Hardness in \(\ell_p\) norms</h2>
<p>Intuitively, one reason that we are able to prove such strong results for \( {\ell_p}\) norms for \( {p \neq 2}\) is because we can use lattices with large ambient dimension \( {d}\) but low rank \( {n}\). In other words, while our reductions produce lattices \( {{\cal{L}}}\) that live in some \( {n}\)-dimensional subspace of \( {\ell_p}\)-space, the ambient space itself has large dimension \( {d}\) relative to \( {n}\). Of course, any subspace of the \( {\ell_2}\) norm is an \( {\ell_2}\) subspace (i.e., every slice of a ball is a lower-dimensional ball), so in the \( {\ell_2}\) norm, one can assume without loss of generality that \( {d = n}\). In particular, if we were able to prove \( {2^n}\)-hardness for the \( {\ell_2}\) norm, then we would actually prove \( {2^d}\)-hardness for free. However, a potentially easier problem would be to improve the \( {2^n}\)-hardness of \( {{\mathrm{CVP}}_p}\) shown in [<a href="https://arxiv.org/abs/1704.03928">BGS17</a>, <a href="https://arxiv.org/abs/1911.02440">ABGS20</a>]  to \( {2^d}\)-hardness for some \( p \neq 2 \).</p>
<blockquote>
<p><b>Open problem 4.</b> Show that there is no \( {2^{0.99 d}}\)-time algorithm for \( {{\mathrm{CVP}}_p}\) (for some \( {p}\)) assuming SETH.</p>
</blockquote>
<p>More generally, it would be very interesting to settle the fine-grained complexity of \( {{\mathrm{CVP}}_p}\) for some \( {p \neq 2}\) (either in terms of rank \( {n} \) or dimension \( {d} \)). This could take the form either of showing improved algorithms (currently the fastest algorithms for \( {{\mathrm{CVP}}_p}\) for general \( {p}\) run in \( {n^{O(n)}}\)-time [<a href="https://kilthub.cmu.edu/articles/Minkowski_s_convex_body_theorem_and_integer_programming/6607328/files/12097865.pdf">Kan87</a>], and \( {2^{O(n)}}\)-time for a constant approximation factor [<a href="https://arxiv.org/abs/1011.5666">DPV11</a>]), or showing super-\( {2^n}\) hardness, or both.</p>
<blockquote>
<p><b>Open problem 5.</b> Show matching upper bounds and lower bounds (under SETH) for \( {{\mathrm{CVP}}_p}\) for some \( {p}\) (possibly with a constant approximation factor).</p>
</blockquote>
<p>The case where \( {p = \infty}\) is especially interesting. Indeed, because the kissing number in the \( {\ell_\infty}\) norm is \( {3^n-1}\), one might guess that the fastest algorithms for \( {{\mathrm{CVP}}_\infty}\) and \( {{\mathrm{SVP}}_\infty}\) actually run in time \( {3^{n + o(n)}}\) or perhaps \( {3^{d + o(d)}}\). (See [<a href="https://arxiv.org/pdf/1801.02358">AM18</a>], which essentially achieves this.) We therefore ask whether stronger lower bounds can be proven in this special case.</p>
<blockquote>
<p><b>Open problem 6.</b> Show that \( {{\mathrm{CVP}}_\infty}\) cannot be solved in time \( {3^{0.99n}}\) (under SETH).</p>
</blockquote>
<h2>2.3. Hardness closer to crypto</h2>
<p>The most relevant problem to cryptography is approximate \( {{\mathrm{SVP}}_2}\) with an approximation factor that is polynomial in the rank \( {n}\). Our fastest algorithms to solve this problem work via a reduction to exact (or near exact) \( {{\mathrm{SVP}}_2}\) with some lower rank \( {n’ = \Theta(n)}\), so that even for these polynomial approximation factors, our fastest algorithms run in time \( {2^{\Omega(n)}}\) (where the hidden constant depends on the polynomial; see <a href="https://blog.simons.berkeley.edu/2020/04/lattice-blog-reduction-part-i-bkz/">Michael’s post</a> for more on this topic). And, hardness results for exact SVP rule out attacks on cryptography that use such reductions. We currently only know how to rule out \( {2^{o(n)}}\)-time algorithms for \( {{\mathrm{SVP}}_2}\) (under the Gap-ETH assumption). We ask whether we can do better. (In [<a href="https://arxiv.org/abs/1712.00942">AS18b</a>], we proved the stronger result below for \( {\ell_p}\) norms for large enough \( {p \notin 2{\mathbb Z}}\).)</p>
<blockquote>
<p><b>Open problem 7.</b> Prove that there is no \( {2^{n/10}}\)-time algorithm for \( {{\mathrm{SVP}}_2}\) (under SETH).</p>
</blockquote>
<p>Of course, we would ideally like to directly rule out faster algorithms for approximate \( {{\mathrm{SVP}}_2}\) with the approximation factors that are most directly relevant to cryptography. There are serious complexity-theoretic barriers to overcome to get all the way there (e.g., \( {{\mathrm{CVP}}_p}\) and \( {{\mathrm{SVP}}_p}\) are known to be in \( {{\mathsf{NP}}} \cap {{\mathsf{coNP}}}\) for large enough polynomial approximation factors. But, we can still hope to get as close as possible, by proving stronger hardness results for approximate \( {{\mathrm{CVP}}_p}\) and approximate \( {{\mathrm{SVP}}_p}\). Indeed, a beautiful sequence of works showed hardness for approximation factors up to \( {n^{c/\log \log n}}\) (so “nearly polynomial) [<a href="http://www.wisdom.weizmann.ac.il/~dinuri/mypapers/cvpjournal.pdf">DKRS03</a>, <a href="https://arxiv.org/abs/1806.04087">HR12</a>], but these results are not fine grained.</p>
<p>The best <em>fine-grained</em> hardness of approximation results known rule out algorithms for small constant-factor approximations for \( {{\mathrm{CVP}}_p}\) with \( {p \notin 2{\mathbb Z}}\) in time \( {2^{0.99n}}\) for \( {{\mathrm{CVP}}_p}\) and \( {{\mathrm{SVP}}_p}\) for any \( {p}\) in time \( {2^{o(n)}}\). We ask whether we can do better.</p>
<blockquote>
<p><b>Open problem 8.</b> Prove that there is no \( {2^{0.99 n}}\)-time algorithm for \( {2}\)-approximate \( {{\mathrm{CVP}}_p}\) (under some form of Gap-SETH, see below).</p>
</blockquote>
<blockquote>
<p><b>Open problem 9.</b> Prove that there is no \( {2^{o(n)}}\)-time algorithm for \( {\gamma}\)-approximate \( {{\mathrm{CVP}}_p}\) for superconstant \( {\gamma = \omega(1)}\) (under Gap-ETH).</p>
</blockquote>
<h2>2.4. Gap-SETH?</h2>
<p>One issue that arose in our attempts to prove fine-grained hardness of approximation results is that we don’t even know the “right” complexity-theoretic assumption about approximate CSPs to use as a starting point. For fine-grained hardness of exact problems, ETH and SETH are very well established hypotheses, and they are in some sense “the weakest possible” assumptions of their form. E.g., it is easy to see that \( {k}\)-SAT is \( {2^{Cn}}\) hard if any \( {k}\)-CSP is. But, for hardness of approximation, the situation is less clear.</p>
<p>The analogue of ETH in the regime of hardness of approximation is the beautiful Gap-ETH assumption, which was defined independently by Irit Dinur [<a href="https://eccc.weizmann.ac.il/report/2016/128/">Din16</a>] and Pasin Manurangsi and Prasad Raghavendra [<a href="https://arxiv.org/pdf/1607.02986.pdf">MR17</a>]. This assumption says that there exists some constant approximation factor \( {\delta \neq 1}\) such that \( {\delta}\)-Gap-\( {3}\)-SAT cannot be solved in time \( {2^{o(n)}}\). (Formally, both Dinur and Manurangsi and Raghavendra say that there is no \( {2^{o(n)}}\)-time algorithm that distinguishes a satisfiable formula from a formula for which no assignment satisfies more than a \( {(1-\epsilon)}\) fraction of the clauses, but we ignore this requirement of perfect completeness here.) It is easy to see that this hypothesis is equivalent to a similar hypothesis about any \( {3}\)-CSP (or, indeed, any \( {k}\)-CSP for any constant\( {k}\)).</p>
<p>However, to prove hardness of approximation with the finest of grains, we need some “gap” analogue of SETH, i.e., we would like to assume that for large enough \( {k}\), some Gap-\( {k}\)-CSP is hard to approximate up to some constant factor \( {\delta \neq 1}\) in better than \( {2^{0.99n}}\)-time. (Formally, we should add an additional variable \( {\epsilon &gt; 0}\) and have such a hypothesis for every running time \( {2^{(1-\epsilon)n}}\), but we set \( {\epsilon = 0.01}\) here to keep things relatively simple.)</p>
<p>An issue arises here concerning the dependence of the approximation factor \( {\delta}\) on the arity \( {k}\). In particular, recall that \( {k}\)-SAT can be trivially approximated up to a factor of \( {1-2^{-k}}\) (since a random assignment satisfies a \( {1-2^{-k}}\) fraction of the clauses in expectation). So, if we define Gap-SETH in terms of Gap-\( {k}\)-SAT, then we must choose \( {\delta = \delta(k) \geq 1-2^{-k}}\) that converges to one as \(k\) increases. Manurangsi proposed such a version of Gap-SETH in his thesis [<a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-49.html">Man19</a>, Conjecture 12.1], specifically that for every large enough constant \( {k}\) there exists a constant \( {\delta = \delta(k) \neq 1}\) such that Gap-\( {k}\)-SAT cannot be approximated up to a factor of \( {\delta}\) in time \( {2^{0.99n}}\). (Again, we are leaving out an additional variable, \( {\epsilon}\).)</p>
<p>If we rely on this version of Gap-SETH, then our current techniques seem to get stuck at proving hardness of approximation for, say, \( {\gamma}\)-approximate \( {{\mathrm{CVP}}_p}\) for some non-explicit constant \( {\gamma_p &gt; 1}\) (and, if one works out the numbers, one can see immediately that \( {\gamma_p}\) must be really quite close to one). However, other Gap-\(k\)-CSPs are known to be (\(\mathsf{NP}\)-)hard to approximate up to much better approximation factors. E.g., for any \( {k}\), Gap-\(k\)-Parity is \( {{\mathsf{NP}}}\)-hard to approximate up to any constant approximation factor \( {1/2 &lt; \delta \leq 1}\) [<a href="http://kiosk.nada.kth.se/theory/projects/publications/optimaljh.pdf">Hås01</a>], and Gap-\( {k}\)-AND is \( {{\mathsf{NP}}}\)-hard to approximate for any constant approximation factor \( {\Omega(k/2^k) \leq \delta \leq 1}\) [<a href="https://eccc.weizmann.ac.il/report/2012/110/">Cha16</a>]. Indeed, Gap-\( {k}\)-AND is a quite natural problem to consider in this context since there is a fine-grained, approximation-factor preserving reduction from any Gap-\( {k}\)-CSP to Gap-\( {k}\)-AND. This generality motivates understanding the precise complexity of Gap-\( {k}\)-AND.</p>
<blockquote>
<p><b>Open problem 10.</b> What is the fine-grained complexity of the \( {\delta}\)-Gap-\( {k}\)-AND problem in terms of \( {n}\), \( {k}\), and \( {\delta}\)? In particular, if</p>
<p align="center">\( \displaystyle C_{k,\delta} := \inf \{ C &gt; 0 \ : \ \text{there is a $2^{C_{k,\delta}}$-time algorithm for algorithm for $\delta$-Gap-$k$-AND}\}\)</p>
<p>then what is the behavior of \( {C_{k,\delta}}\) as \( {k \rightarrow \infty}\) (for various functions \( {\delta = \delta(k)}\) of \( {k}\))?</p>
</blockquote>
<p>In particular, if one were to hypothesize sufficiently strong hardness of \( {\delta}\)-Gap-\( {k}\)-AND — i.e., to define an appropriate variant of Gap-SETH based on Gap-\( {k}\)-AND — then one might be able to use this hypothesis to prove very strong fine-grained hardness of approximation results. There is a fine-grained (but non-approximation preserving) reduction from Gap-\( {k}\)-AND to Gap-\( {k}\)-SAT, and so Manurangsi’s Gap-SETH is equivalent to the conjecture that there exists some non-explicit \( {\delta(k)}\) such that \( {\lim_{k \rightarrow \infty} C_{k,\delta} = 1}\).</p>


<ul><li>[<a href="https://arxiv.org/abs/1911.02440">ABGS20</a>] Aggarwal, Bennett, Golovnev, Stephens-Davidowitz. Fine-grained hardness of CVP(P)— Everything that we can prove (and nothing else)</li><li>[<a href="https://estimate-all-the-lwe-ntru-schemes.github.io/paper.pdf">A+18</a>] Albrecht, Curtis, Deo, Davidson, Player, Postlethwaite, Virdia, Wunderer. Estimate all the {LWE, NTRU} schemes! <em>SCN</em>, 2019.</li><li>[<a href="https://arxiv.org/abs/1412.7994">ADRS15</a>] Aggarwal, Dadush, Regev, Stephens-Davidowitz. Solving the Shortest Vector Problem in \(2^n\) time via discrete Gaussian sampling. <em>STOC</em>, 2015.</li><li>[<a href="https://arxiv.org/abs/1504.01995">ADS15</a>]  Aggarwal, Dadush, Stephens-Davidowitz. Solving the Closest Vector Problem in \(2^n\) time–The discrete Gaussian strikes again! <em>FOCS</em>, 2015.</li><li>[<a href="https://arxiv.org/pdf/1801.02358">AM18</a>] Aggarwal, Mukhopadhyay. Faster algorithms for SVP and CVP in the \(\ell_\infty\) norm. <em>ISAAC</em>, 2018.</li><li>[<a href="https://arxiv.org/abs/1709.01535">AS18a</a>] Aggarwal, Stephens-Davidowitz. Just take the average! An embarrassingly simple \(2^n\)-time algorithm for SVP (and CVP). <em>SOSA</em>, 2018.</li><li>[<a href="https://arxiv.org/abs/1712.00942">AS18b</a>] Aggarwal, Stephens-Davidowitz. (Gap/S)ETH hardness of SVP. <em>STOC</em>, 2018.</li><li>[<a href="https://eprint.iacr.org/2016/659">B+16</a>] Bos, Costello, Ducas, Mironov, Naehrig, Nikolaenko, Raghunathan, Stebila. Frodo: Take off the ring! Practical, Quantum-Secure Key Exchange from LWE. <em>CCS,</em> 2016.</li><li>[<a href="https://eprint.iacr.org/2015/1128">BDGL16</a>] Becker, Ducas, Gama, Laarhoven. New directions in nearest neighbor searching with applications to lattice sieving. <em>SODA</em>, 2016.</li><li>[<a href="https://arxiv.org/abs/1704.03928">BGS17</a>] Bennett, Golovnev, Stephens-Davidowitz. On the quantitative hardness of CVP. <em>FOCS</em>, 2017.</li><li>[<a href="https://eccc.weizmann.ac.il/report/2012/110/">Cha16</a>] Chan. Approximation resistance from pairwise-independent subgroups. <em>J. ACM</em>, 2016.</li><li>[<a href="https://eccc.weizmann.ac.il/report/2016/128/">Din16</a>] Dinur. Mildly exponential reduction from gap 3SAT to polynomial-gap label-cover.</li><li>[<a href="http://www.wisdom.weizmann.ac.il/~dinuri/mypapers/cvpjournal.pdf">DKRS03</a>] Dinur, Kindler, Raz, Safra. Approximating CVP to within almost-polynomial factors is NP-hard. <em>Combinatorica</em>, 2003.</li><li>[<a href="https://arxiv.org/abs/1011.5666">DPV11</a>] Dadush, Peikert, Vempala. Enumerative lattice algorithms in any norm via \(M\)-ellipsoid coverings. <em>FOCS</em>, 2011.</li><li>[<a href="https://cseweb.ucsd.edu/~daniele/papers/GMSS.pdf">GMSS99</a>] Goldreich, Micciancio, Safra, Seifert. Approximating shortest lattice vectors is not harder than approximating closest lattice vectors. <em>IPL</em>, 1999.</li><li>[<a href="http://kiosk.nada.kth.se/theory/projects/publications/optimaljh.pdf">Hås01</a>] Håstad. Some optimal inapproximability results. <em>J. ACM</em>, 2001.</li><li>[<a href="https://arxiv.org/abs/1806.04087">HR12</a>] Haviv, Regev. Tensor-based hardness of the Shortest Vector Problem to within almost polynomial factors. <em>TOC</em>, 2012.</li><li>[<a href="https://cseweb.ucsd.edu/~paturi/myPapers/pubs/ImpagliazzoPaturi_2001_jcss.pdf">IP01</a>] Impagliazzo, Paturi. On the complexity of \(k\)-SAT. <em>JCSS</em>, 2001.</li><li>[<a href="https://cseweb.ucsd.edu/~russell/ipz.pdf">IPZ01</a>] Impagliazzo, Paturi, Zane. Which problems have strongly exponential complexity? <em>JCSS</em>, 2001.</li><li>[<a href="http://www.thijs.com/docs/phd-final.pdf">Laa15</a>] Laarhoven. Search problems in cryptography. Ph.D thesis, 2015.</li><li>[<a href="https://kilthub.cmu.edu/articles/Minkowski_s_convex_body_theorem_and_integer_programming/6607328/files/12097865.pdf">Kan87</a>] Kannan. Minkowski’s convex body theorem and Integer Programming. <em>MOR</em>, 1987.</li><li>[<a href="https://eprint.iacr.org/2019/1016">KMPR19</a>] Kirshanova, Mårtensson, Postlethwaite, Roy Moulik. Quantum algorithms for the approximate \(k\)-list problem and their application to lattice sieving. <em>Asiacrypt</em>, 2019.</li><li>[<a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-49.html">Man19</a>] Manurangsi. Approximation and Hardness: Beyond P and NP.</li><li>[<a href="https://arxiv.org/pdf/1607.02986.pdf">MR17</a>] Manurangsi, Raghavendra. A Birthday Repetition Theorem and Complexity of Approximating Dense CSPs. <em>ICALP</em>, 17.</li><li>[<a href="https://arxiv.org/abs/1802.00886">Vlă19</a>] Vlăduţ. Lattices with exponentially large kissing numbers. <em>Moscow J. of Combinatorics and Number Theory</em>, 2019<em>.</em></li></ul></div>
    </content>
    <updated>2020-05-04T01:07:28Z</updated>
    <published>2020-05-04T01:07:28Z</published>
    <category term="General"/>
    <author>
      <name>Huck Bennett</name>
    </author>
    <source>
      <id>https://blog.simons.berkeley.edu</id>
      <link href="https://blog.simons.berkeley.edu/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blog.simons.berkeley.edu" rel="alternate" type="text/html"/>
      <subtitle>What's New at the Simons Institute for the Theory of Computing.</subtitle>
      <title>Calvin Café: The Simons Institute Blog</title>
      <updated>2020-05-07T00:21:13Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/05/03/hanoi-vs-sierpinski</id>
    <link href="https://11011110.github.io/blog/2020/05/03/hanoi-vs-sierpinski.html" rel="alternate" type="text/html"/>
    <title>Hanoi vs Sierpiński</title>
    <summary>The Hanoi graphs and Sierpiński graphs both look like the Sierpiński triangle, and have a very similar recursive construction from triples of smaller graphs of the same type, but they are not quite the same graphs as each other. The Sierpiński graphs (left, below) are the graphs of the vertices and boundary edges of partially-constructed Sierpiński triangles; they can also be formed from three smaller Sierpiński graphs by identifying pairs of extreme vertices (the vertices of degree two at the three corners of the triangular layout). The Hanoi graphs (right, below) are the state spaces of the tower of Hanoi puzzle, in which rings of different size are moved one at a time between three pegs, only allowing moves that keep the rings sorted on each peg. They also have a construction from three smaller Hanoi graphs, but where the pairs of extreme vertices are connected by an edge rather than identified.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Hanoi graphs and Sierpiński graphs both look like the <a href="https://en.wikipedia.org/wiki/Sierpi%C5%84ski_triangle">Sierpiński triangle</a>, and have a very similar recursive construction from triples of smaller graphs of the same type, but they are not quite the same graphs as each other. The Sierpiński graphs (left, below) are the graphs of the vertices and boundary edges of partially-constructed Sierpiński triangles; they can also be formed from three smaller Sierpiński graphs by identifying pairs of extreme vertices (the vertices of degree two at the three corners of the triangular layout). The <a href="https://en.wikipedia.org/wiki/Hanoi_graph">Hanoi graphs</a> (right, below) are the state spaces of the tower of Hanoi puzzle, in which rings of different size are moved one at a time between three pegs, only allowing moves that keep the rings sorted on each peg. They also have a construction from three smaller Hanoi graphs, but where the pairs of extreme vertices are connected by an edge rather than identified.</p>

<p style="text-align: center;"><img alt="Hanoi graphs and Sierpi&#x144;ski graphs" src="https://11011110.github.io/blog/assets/2020/hanoi-vs-sierpinski.svg"/></p>

<p>The difference between them comes out much more strongly when you generalize them to higher dimensions. The Sierpiński triangle generalizes to tetrahedra (a popular shape for kites) and higher-dimensional simplices; <a href="https://commons.wikimedia.org/wiki/File:Alexander_Graham_Bell_facing_his_wife,_Mabel_Hubbard_Gardiner_Bell,_who_is_standing_in_a_tetrahedral_kite,_Baddeck,_Nova_Scotia.tif">the photo below</a> shows <a href="https://en.wikipedia.org/wiki/Mabel_Gardiner_Hubbard">Mabel Bell</a> and <a href="https://en.wikipedia.org/wiki/Alexander_Graham_Bell">Alexander Graham Bell</a>, seemingly about to kiss, in a three-dimensional Sierpiński graph, the framework for a kite.</p>

<p style="text-align: center;"><img alt="Mabel Bell and Alexander Graham Bell kissing in a Sierpi&#x144;ski tetrahedron kite frame, from https://commons.wikimedia.org/wiki/File:Alexander_Graham_Bell_facing_his_wife,_Mabel_Hubbard_Gardiner_Bell,_who_is_standing_in_a_tetrahedral_kite,_Baddeck,_Nova_Scotia.tif" src="https://11011110.github.io/blog/assets/2020/bell-kite-kiss.jpg"/></p>

<p>Again, the -dimensional Sierpiński graph has a recursive construction from  smaller graphs of the same type, identified at extreme vertices (the vertices of degree  at the  corners of the layout). Because the number of vertices separating the subgraphs at each level of the recursion is so small, these graphs have bounded <a href="https://en.wikipedia.org/wiki/Treewidth">treewidth</a>, and a few years ago on the TCS stackexchange <a href="https://cstheory.stackexchange.com/q/36542">I calculated the treewidth of the Sierpiński triangle graphs explicitly as being four</a>. The same bound transfers easily enough to the three-peg Hanoi graphs.</p>

<p>The analogue of higher dimensions for the Hanoi graphs is to use more pegs. The Hanoi graph with  pegs and  rings has  states, more or less the same as the Sierpiński graph for -dimensional Sierpiński fractals with  levels of recursion. Here’s the one with two rings; each state is described by a pair of letters, using a capital letter for the peg holding the larger ring and a lowercase letter for the peg holding the smaller ring.</p>

<p style="text-align: center;"><img alt="Hanoi graph for two rings on four pegs" src="https://11011110.github.io/blog/assets/2020/Hanoi-4-2.svg"/></p>

<p>The recursive construction for these graphs combines  copies of a smaller graph of the same type: one copy for each position where the largest ring can be placed, and a smaller graph describing the placements of the smaller rings once the largest ring has been placed. These copies of the smaller graph are connected together by edges describing the movements of the largest ring. But I’ve only drawn an example for two rings because these graphs get messy and hard to draw very quickly. The reason is not the exponential number of total vertices, but the large number of connections from one recursive subgraph to another. Two recursive subgraphs are connected whenever the largest ring can move from its peg in one subgraph to its peg in the other, and this is allowed whenever these two pegs have no smaller rings on them. So in a Hanoi graph with  pegs,  rings, and  vertices, each pair of recursive subgraphs has  edges between them, one for each placement of the smaller rings on the  remaining pegs.</p>

<p>The recursive subdivision with  edges between subgraphs leads to a tree-decomposition with treewidth , and this naturally raises the question of whether this is tight or whether some other less-intuitive recursive decomposition has smaller cuts between its recursive subgraphs. This is the question studied in my newest preprint, “On the treewidth of Hanoi graphs” (<a href="https://arxiv.org/abs/2005.00179">arXiv:2005.00179</a>), with UCI student Daniel Frishberg and Oregon State student Will Maxwell, to appear at <a href="https://sites.google.com/view/fun2020/home">FUN 2020</a> (supposedly to be held in person in September in Italy after being rescheduled from June, but I’m not holding my breath). We don’t get a precise answer, but we succeed in proving bounds on the treewidth of the form <span style="white-space: nowrap;">.</span> This is nearly tight for fixed  and variable : we get the same exponential function of  as the upper bound, and are smaller than the upper bound by only a much lower-order polynomial factor. But the exact treewidth remains elusive.</p>

<p>To put it in a possibly more familiar form, when one of these graphs (for a fixed number of pegs and variable number of rings) has  vertices, it has separators of size , where . For the four-peg Hanoi graphs, this means separators of size , more or less the same as for planar graphs (although these graphs seem far from planar). But that nice exponent is just a coincidence caused by the fact that  is a power of . For other choices of , that doesn’t happen and we get a transcendental exponent . So these graphs don’t even act like -dimensional graphs, for which a reasonable separator exponent might be the rational number . And they certainly don’t act like the Sierpiński graphs, for which the exponent is zero.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104108481482094736">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-05-03T22:03:00Z</updated>
    <published>2020-05-03T22:03:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-04T05:19:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/069</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/069" rel="alternate" type="text/html"/>
    <title>TR20-069 |  Optimal Error Pseudodistributions for Read-Once Branching Programs | 

	Eshan Chattopadhyay, 

	Jyun-Jie Liao</title>
    <summary>In a seminal work, Nisan (Combinatorica'92) constructed a pseudorandom generator for length $n$ and width $w$ read-once branching programs  with seed length $O(\log n\cdot \log(nw)+\log n\cdot\log(1/\varepsilon))$ and  error $\varepsilon$. It remains a central question  to reduce the seed length to $O(\log (nw/\varepsilon))$, which would prove that $\mathbf{BPL}=\mathbf{L}$. However, there has been no improvement on Nisan's construction for the case $n=w$, which is most relevant to space-bounded derandomization.




Recently, in a beautiful work, Braverman, Cohen and Garg (STOC'18) introduced the notion of a \emph{pseudorandom pseudo-distribution} (PRPD) and gave an explicit construction of a PRPD with seed length $\tilde{O}(\log n\cdot \log(nw)+\log(1/\varepsilon))$. A PRPD is a relaxation of a pseudorandom generator, which suffices for derandomizing $\mathbf{BPL}$ and also implies a hitting set. Unfortunately, their construction is quite involved and complicated. Hoza and Zuckerman (FOCS'18) later constructed a much simpler hitting set generator with seed length $O(\log n\cdot \log(nw)+\log(1/\varepsilon))$, but their techniques are restricted to hitting sets.

In this work, we construct a PRPD with seed length 
$$O(\log n\cdot \log (nw)\cdot \log\log(nw)+\log(1/\varepsilon)).$$
This improves upon the construction in \cite{BCG18} by a $O(\log\log(1/\varepsilon))$ factor, and is optimal in the small error regime. In addition, we believe our construction and analysis to be   simpler than the work of Braverman, Cohen and Garg.</summary>
    <updated>2020-05-03T22:01:57Z</updated>
    <published>2020-05-03T22:01:57Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-07T19:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/068</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/068" rel="alternate" type="text/html"/>
    <title>TR20-068 |  One-Sided Error Testing of Monomials and Affine Subspaces | 

	Oded Goldreich, 

	Dana Ron</title>
    <summary>We consider the query complexity of three versions of the problem of testing monomials and affine (and linear) subspaces with one-sided error, and obtain the following results: 
\begin{itemize}
\item The general problem, in which the arity of the monomial (resp., co-dimension of the subspace) is not specified, has query complexity ${\widetilde{O}}(1/\epsilon)$, where $\epsilon$ denotes the proximity parameter. 
\item The bounded problem, in which the arity of the monomial (resp., co-dimension of the subspace) is upper bounded by a fixed parameter, has query complexity ${\widetilde{O}}(1/\epsilon)$.
\item The exact problem, in which the arity of the monomial (resp., co-dimension of the subspace) is required to equal a fixed parameter (e.g., equals~2), has query complexity ${\widetilde{\Omega}}(\log n)$, where $n$ denotes the length of the argument for the tested function.  
\end{itemize}
The running time of the testers in the positive results is linear in their query complexity.</summary>
    <updated>2020-05-03T20:04:44Z</updated>
    <published>2020-05-03T20:04:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-07T19:20:24Z</updated>
    </source>
  </entry>
</feed>
