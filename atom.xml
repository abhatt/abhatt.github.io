<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-05-07T16:39:45Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/</id>
    <link href="https://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/" rel="alternate" type="text/html"/>
    <title>Faculty Full Professor at University of Bamberg, Germany (apply by June 18, 2021)</title>
    <summary>The Faculty of Information Systems and Applied Computer Sciences invites applications for the position of Full Professor (W3 level) in Algorithms and Complexity Theory with a focus on algorithms and complexity theory for distributed and concurrent software systems as well for the acquisition, processing and visualisation of data in networked systems. Website: https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/ Email: michael.mendler@uni-bamberg.de</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Faculty of Information Systems and Applied Computer Sciences invites applications for the position of Full Professor (W3 level) in Algorithms and Complexity Theory with a focus on algorithms and complexity theory for distributed and concurrent software systems as well for the acquisition, processing and visualisation of data in networked systems.</p>
<p>Website: <a href="https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/">https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/</a><br/>
Email: michael.mendler@uni-bamberg.de</p></div>
    </content>
    <updated>2021-05-07T14:12:35Z</updated>
    <published>2021-05-07T14:12:35Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-05-07T16:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=1659</id>
    <link href="https://toc4fairness.org/fair-clustering-with-probabilistic-group-membership/" rel="alternate" type="text/html"/>
    <title>Fair Clustering with Probabilistic Group Membership</title>
    <summary>This post briefly summarizes a NeurIPS-20 paper, Probabilistic Fair Clustering, which I coauthored with Brian Brubach, Leonidas Tsepenekas, and John P. Dickerson. Clustering is possibly the most fundamental problem of ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This post briefly summarizes a NeurIPS-20 paper, <em><a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">Probabilistic Fair Clustering</a></em>, which I coauthored with <a href="https://bbrubach.github.io/">Brian Brubach</a>, Leonidas Tsepenekas, and <a href="http://jpdickerson.com/">John P. Dickerson</a>.<br/><br/>Clustering is possibly the most fundamental problem of unsupervised learning. Like many other paradigms of machine learning, there has been a focus on fair variants of clustering. Perhaps the definition which has received the most attention is the group fairness definition of [1]. The notion is based on disparate impact and simply states that each cluster should contain points belonging to the different demographic groups with “appropriate” proportions. A natural interpretation of appropriate would imply that each demographic group appears in close to population-level proportions in each cluster. More specifically, if we were to endow each point with a color <img alt="h \in {\cal H}" class="latex" src="https://s0.wp.com/latex.php?latex=h+%5Cin+%7B%5Ccal+H%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> to designate its group membership and we were to consider the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>-means clustering objective, then this notion of fair clustering amounts to the following constrained optimization problem:</p>



<p class="has-text-align-center"><img alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq |C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/></p>



<p>Here, <img alt="l_h" class="latex" src="https://s0.wp.com/latex.php?latex=l_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> and <img alt="u_h" class="latex" src="https://s0.wp.com/latex.php?latex=u_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> are the lower and upper pre-set proportionality bounds for color <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>, <img alt="C_i" class="latex" src="https://s0.wp.com/latex.php?latex=C_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> denotes the points in cluster <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>, and <img alt="C^h_i" class="latex" src="https://s0.wp.com/latex.php?latex=C%5Eh_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> denotes the subset of those points with color <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>. See figure 1 for a comparison between the outputs of color-agnostic and fair clustering.<br/></p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1735" height="151" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_1.png?resize=800%2C151&amp;ssl=1" width="800"/>Figure 1: The outputs of color-agnostic vs fair clustering. The clusters of the group-fair output have a proportional mixture of both colors whereas the color-agnostic clusters consist of only one color.</figure></div>



<p>If one were to use clustering for market segmentation and targeted advertisement, then the above definition of fair clustering would roughly ensure that each demographic group receives the same exposure to every type of ad. Similarly if we were to cluster news articles and let the source of each article indicate its membership then we could ensure that each cluster has a good mixture of news from different sources [2].</p>



<p>Significant progress has been made in this notion of fair clustering starting from only considering the two color case and under-representation bounds, to the multi-color case with both under- and over-representation bounds [3.4.5]. Scalable methods for larger datasets have also been proposed [6, 7].</p>



<p>Clearly, like the majority of the methods in group-fair supervised learning, it is assumed that the group membership of each point in the dataset is known. This setting conflicts with a common situation in practice where group memberships are either imperfectly known or completely unknown [8,9,10,11,12]. We take the first step in generalizing fair clustering to this setting; specifically, we assume that while we do not know the exact group membership of each point, we instead have a probability distribution over the group memberships. A natural generalization of the previous optimization problem would be the following:</p>



<p class="has-text-align-center"><img alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq \mathbb{E}|C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%5Cmathbb%7BE%7D%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/></p>



<p>Where the proportionality constraints were simply changed to hold in expectation instead of deterministically. Clearly, this constraint reduces to the original constraint when the group memberships are completely known. Figure 2 helps visualize how the input to probabilistic fair clustering looks like and the output we expect.</p>



<p><br/></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" class="wp-image-1737" height="210" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_2.png?resize=800%2C210&amp;ssl=1" width="800"/>Figure 2: In the above example, the given set of points in the top row are blue and red with probability almost 1 whereas the bottom are blue and red with probability around 0.6. To maintain almost equal color proportions in expectation probabilistic fair clustering would yield the given clustering.</figure></div>



<p> </p>



<p>Despite the innocuous modification to the constraint, the problem becomes significantly more difficult. In our <a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">paper</a>, we consider the center-based clustering objectives of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>-center, <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>-median, and <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>-means and produce solutions with approximation ratio guarantees for two given cases:</p>



<ul><li><strong>Two-Color Case</strong>: We see that even the two color case is not easy to handle. The key difficulty lies in the rounding method. However, we give a rounding method that maintains the fairness constraint with a worst-case additive violation of 1 matching the deterministic fair clustering case.</li><li><strong>Multi-Color Case with Large Enough Clusters</strong>: At a high level, if the clusters have a sufficiently large size then through a Chernoff bound we can show that independent sampling would result in a deterministic fair clustering instance which we could solve using deterministic fair clustering algorithms. This essentially forms a reduction from the probabilistic to the deterministic instance.</li></ul>



<p>While our solutions perform well empirically, we are left with a collection of problems. For example, guaranteeing that the color proportions are maintained in expectation is not the best constraint one should hope for, since when the colors are realized a cluster could entirely consist of one color. A more preferable constraint would instead bound the probability of obtaining an “unfair” clustering. Moreover, a setting that assumes access to the probability distribution for a given point over all colors could still be assuming too much. A more reasonable setting could instead take a robust-optimization-based approach, where we have the distribution of each point but allow the distribution of each point to belong to an uncertainty set. This effectively allows our probabilistic knowledge to be imperfect as well—as could be the case if, for example, a machine learning model were predicting group membership with a systematic bias against a particular subset of colors. Lastly, being able to handle the multi-color case in an assumption-free manner would also be interesting.</p>



<p><strong>References:</strong></p>



<ol><li>Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through fairlets. In Advances in Neural Information Processing Systems, 2017.</li><li>Sara Ahmadian, Alessandro Epasto, Ravi Kumar, and Mohammad Mahdian. Clustering without over-representation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 2019.</li><li>Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler. Fair coresets and streaming algorithms for fair k-means. In the International Workshop on Approximation and Online Algorithms, 2019.</li><li>Ioana O. Bercea, Martin Groß, Samir Khuller, <em>Aounon Kumar</em>, Clemens Rösner, Daniel R. Schmidt, Melanie Schmidt. On the cost of essentially fair clusterings, In the International Conference on Approximation Algorithms for Combinatorial Optimization Problems 2019.</li><li>Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani. Fair algorithms for clustering. In Advances in Neural Information Processing Systems, 2019.</li><li>Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. Scalable fair clustering. In the International Conference on Machine Learning, 2019.</li><li>Lingxiao Huang, Shaofeng Jiang, and Nisheeth Vishnoi. Coresets for clustering with fairness constraints. In Advances in Neural Information Processing Systems, 2019.</li><li>Pranjal Awasthi, Matth¨aus Kleindessner, and Jamie Morgenstern. Equalized odds postprocessing under imperfect group information. In the International Conference on Artificial Intelligence and Statistics, 2020.</li><li>Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, NithumThain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. In Advances in Neural InformationProcessing Systems, 2020.</li><li>David Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, AshwinMachanavajjhala, and Gerome Miklau. Fair decision making using privacy-protected data. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2020.</li><li>Hussein Mozannar, Mesrob Ohannessian, and Nathan Srebro. Fair learning with private demographic data. In the International Conference on Machine Learning, 2020.</li><li>Nathan Kallus, Xiaojie Mao, and Angela Zhou. Assessing algorithmic fairness with unobserved protected class using data combination. Management Science, 2021.</li></ol>



<p/></div>
    </content>
    <updated>2021-05-07T14:09:42Z</updated>
    <published>2021-05-07T14:09:42Z</published>
    <category term="Blog"/>
    <author>
      <name>seyed2357</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i1.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2021-05-07T16:39:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/05/07/ph-d-student-at-idsia-usi-supsi-lugano-switzerland-apply-by-june-30-2021/</id>
    <link href="https://cstheory-jobs.org/2021/05/07/ph-d-student-at-idsia-usi-supsi-lugano-switzerland-apply-by-june-30-2021/" rel="alternate" type="text/html"/>
    <title>Ph.D. Student at IDSIA, USI-SUPSI, Lugano, Switzerland (apply by June 30, 2021)</title>
    <summary>IDSIA opens two 4-year Ph.D. positions, starting on November 2021, in the area of algorithms and complexity, with a focus on approximation algorithms. The gross year salary is around 50K CHF. Candidates should hold a Master Degree in Computer Science or related areas. The interested candidates should email Prof. Fabrizio Grandoni a detailed CV and […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>IDSIA opens two 4-year Ph.D. positions, starting on November 2021, in the area of algorithms and complexity, with a focus on approximation algorithms.<br/>
The gross year salary is around 50K CHF. Candidates should hold a Master Degree in Computer Science or related areas.<br/>
The interested candidates should email Prof. Fabrizio Grandoni a detailed CV and contact details of 2-3 references.</p>
<p>Website: <a href="https://people.idsia.ch//~grandoni/">https://people.idsia.ch//~grandoni/</a><br/>
Email: fabrizio@idsia.ch</p></div>
    </content>
    <updated>2021-05-07T09:47:11Z</updated>
    <published>2021-05-07T09:47:11Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-05-07T16:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21716</id>
    <link href="https://gilkalai.wordpress.com/2021/05/06/alef-corner-icm2022/" rel="alternate" type="text/html"/>
    <title>Alef Corner: ICM2022</title>
    <summary>Alef’s new piece for ICM 2022 will surely cheer you up!</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><img alt="icm2022" class="alignnone size-full wp-image-21717" height="2100" src="https://gilkalai.files.wordpress.com/2021/05/icm2022.jpg" width="2100"/><strong><span style="color: #ff0000;">Alef’s new piece for ICM 2022 will surely cheer you up!</span> </strong></p>


<p/></div>
    </content>
    <updated>2021-05-06T19:45:22Z</updated>
    <published>2021-05-06T19:45:22Z</published>
    <category term="Art"/>
    <category term="Combinatorics"/>
    <category term="Geometry"/>
    <category term="ICM2022"/>
    <category term="Alef's corner"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-05-07T16:37:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.let-all.com/blog/?p=51</id>
    <link href="https://www.let-all.com/blog/2021/05/06/alt-highlights-a-report-on-the-first-alt-mentoring-workshop/" rel="alternate" type="text/html"/>
    <title>ALT Highlights – A Report on the First ALT Mentoring Workshop</title>
    <summary>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference ALT 2021, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by the <a href="https://www.let-all.com/">Learning Theory Alliance</a>, and overseen by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Learning Theory Alliance blog</a>.</p>



<p>This is the fifth post in the series, coverage of the first <a href="https://www.let-all.com/alt.html">ALT Mentoring Workshop</a> organized by the Learning Theory Alliance, written by <a href="https://www.let-all.com/blog/feed/knaggita@ttic.edu">Keziah Naggita</a> and <a href="https://www.comp.nus.edu.sg/~sutanu/">Sutanu Gayen</a>.</p>



<hr class="wp-block-separator"/>



<h2><strong>1 Introduction</strong></h2>



<p class="has-text-align-left has-text-align-justify">The Learning Theory Alliance (Let-All) is an online initiative aimed at developing a supportive learning theory community, founded by (1) <a href="https://www.cs.utexas.edu/~surbhi/" rel="noopener noreferrer" target="_blank">Surbhi Goel</a>, a postdoctoral researcher at Microsoft Research New York, (2) <a href="https://people.eecs.berkeley.edu/~nika/" rel="noopener noreferrer" target="_blank">Nika Haghtalab</a>, an assistant professor at UC Berkeley EECS, (3) and <a href="https://vitercik.github.io" rel="noopener noreferrer" target="_blank">Ellen Vitercik</a>, a Ph.D. Student at CMU; and advised by <a href="https://www.stat.berkeley.edu/~bartlett/" rel="noopener noreferrer" target="_blank">Peter Bartlett</a>, <a href="https://home.ttic.edu/~avrim/" rel="noopener noreferrer" target="_blank">Avrim Blum</a>, <a href="https://people.csail.mit.edu/stefje/" rel="noopener noreferrer" target="_blank">Stefanie Jegelka</a>, <a href="https://www.dpmms.cam.ac.uk/%7Epll28/" rel="noopener noreferrer" target="_blank">Po-Ling Loh</a>, and <a href="http://www.jennwv.com" rel="noopener noreferrer" target="_blank">Jenn Wortman Vaughan</a>. The goal of the alliance is to ensure healthy community growth by fostering inclusive community engagement and encouraging active contributions from researchers at all stages of their careers. Let-All’s efforts towards realizing these goals include a series of ongoing and future activities, such as the first ALT mentoring workshop, coordinating the ALT Highlights blog series, and other upcoming community initiatives. This article reports on  Let-All’s <a href="https://let-all.com/alt.html" rel="noopener noreferrer" target="_blank">first Mentoring Workshop</a>, which was affiliated with the 32<sup>nd</sup> International Conference on Algorithmic Learning Theory.</p>



<p class="has-text-align-left has-text-align-justify">The workshop had two main sessions to cater to the time zone differences of the participants.  These sessions had three main components: an academic program, which included how-to-talks, Ask Me Anythings (AMAs), and presentation dissections; a technical program, which included research talks; and a social program, which included discussion tables and other activities.</p>



<p class="has-text-align-left has-text-align-justify">The workshop participants included students, researchers, and industry professionals, all at different levels of familiarity with learning theory. Because of the ongoing COVID-19 pandemic, the workshop was virtual. It was held on the online platforms Zoom and Gather town, a virtual interactive environment that mimics an in-person workshop setting. For accessibility, the workshop organizers opened up the workshop free of cost to all registered participants. </p>



<h2><strong>2 Program Highlights</strong></h2>



<h3><strong>2.1 Academic Program</strong></h3>



<p class="has-text-align-left has-text-align-justify">To kick off the workshop, one of the organizers began with a welcome lecture: Surbhi in session one and Nika in session two.  They read out the code of conduct and who to contact in case of issues, outlined the workshop’s purpose, and gave attendees demographic information. They explained how participants could navigate the workshop-themed Gather town workspace and then ended the introduction with encouragement for participants to mingle. </p>



<p class="has-text-align-left has-text-align-justify">The <strong><em>How-to-Talks</em></strong> sessions covered writing papers, giving talks, and networking. In Session 1, <a href="https://www.cs.cmu.edu/~praveshk/" rel="noopener noreferrer" target="_blank">Pravesh Kothari</a> talked in great detail about the dos and don’ts of what to add in the abstract, overview, introduction, and appendix when advising participants on how to best structure research papers. He told attendees to always put effort into understanding their intended reader or talk audience.  Pravesh encouraged attendees to consider the expertise and interests of the reader or listener to capture their attention since these highly determine the attention span and interest in the information presented to them.  He strongly recommended attendees watch the <em><a href="https://www.youtube.com/watch?v=vtIzMaLkCaM" rel="noopener noreferrer" target="_blank">Leadership Lab: The Craft of Writing Effectively</a></em> by Larry McEnerney , Director of the University of Chicago Writing Program.<em> </em>In session 2 of the workshop, <a href="https://home.cs.colorado.edu/~raf/" rel="noopener noreferrer" target="_blank">Rafael Frongillo</a>, similar to Pravesh, discussed how to capture the intended audience when one writes a paper, reviews, and talks.</p>



<p class="has-text-align-left has-text-align-justify">In the first <strong><em>networking session</em></strong>, <a href="https://www.cc.gatech.edu/~jabernethy9/" rel="noopener noreferrer" target="_blank">Jacob Abernethy</a> encouraged participants to seek out horizontal and vertical networking, for example, through collaborations, talks, and reach outs. He said that currently, in academia, Ph.D. admissions, faculty hiring, and tenure appointments are heavily risk-averse. Therefore, people seek out candidates based on their network. For this reason, it is crucial for students to network from early on in their careers. He gave great examples of how junior researchers can reach out and forge relationships with other researchers. For example, when you meet academics, faculty/postdocs at events, ask to give a talk at their lab. Jacob also candidly talked about his earlier failures at MIT and how they shaped his journey. He talked about luck and how <a href="https://www.microsoft.com/en-us/research/people/jcl/" rel="noopener noreferrer" target="_blank">John Langford</a>, who was at Toyota Technological Institute at Chicago at the time, took a chance on him that forever changed his life. Jacob, therefore, advised academics to take chances on people as this would change the course of the field. </p>



<p class="has-text-align-left has-text-align-justify"><a href="https://jamiemorgenstern.com" rel="noopener noreferrer" target="_blank">Jamie Morgenstern</a> discussed different networking methods in the <strong><em>second How-to-talks session</em></strong>. She emphasized that for junior researchers, it’s important to attend conferences and to network with others, to advertise their research through talks, and to reach out to faculty for collaboration. To introduce oneself and capture the listener’s attention, Jamie said, for conferences, prepare to do so in two minutes, for social four minutes, bar 12 minutes, and faculty interview 25 minutes. Senior grad students may help introduce the juniors during lunch/poster sessions. Finally, when emailing faculty about research, she said one should avoid discussion about other people’s work and instead should stick to the recipient’s work – “showing deep understanding and possibly open questions which might lead to collaboration.” </p>



<p class="has-text-align-left has-text-align-justify">In both workshop sessions, there were<strong><em> two parallel talk dissections, </em></strong>in which senior faculty members gave both positive and constructive feedback on talks junior researchers presented. In the first session, <a href="https://www.cs.cornell.edu/~rdk/" rel="noopener noreferrer" target="_blank">Bobby Kleinberg</a> discussed <a href="https://www.emilyruthdiana.com" rel="noopener noreferrer" target="_blank">Emily Diana</a>‘s talk titled “Minimax and Lexicographically Fair Learning: Algorithms, Experiments, and Generalization”. He highlighted parts that were impressive, those that needed improvement, and gave general advice on structuring an audience-based presentation. When Bobby suggested including more diagrams than text, a few people made suggestions of free tools including tikz, matcha.io, PowerPoint, and draw.io. In parallel, <a href="http://cseweb.ucsd.edu/~kamalika/" rel="noopener noreferrer" target="_blank">Kamalika Chaudhuri</a> dissected a talk on “Efficient, Noise-Tolerant, and Private Learning via Boosting” by <a href="https://marco.ntime.org" rel="noopener noreferrer" target="_blank">Marco Carmosino</a>. Two main takeaways of this talk dissection were the balance of technical and nontechnical content (e.g., explaining ideas with fun pictures, etc.) and having one main and clear idea as the talk’s takeaway. </p>



<p class="has-text-align-left has-text-align-justify">In the second session, <a href="https://sites.google.com/site/acmonsterqiao/" rel="noopener noreferrer" target="_blank">Mingda Qiao</a> gave a talk titled: “Stronger Calibration Lower Bounds via Sidestepping” which <a href="https://praneethnetrapalli.org" rel="noopener noreferrer" target="_blank">Praneeth Netrapalli</a> dissected. Praneeth remarked that theory folks often jump into the problem straight away without covering much background. In conferences, this might be fine due to time pressure and specific interests. However, in broader settings such as departmental seminars, he advised the speaker to allocate more time to introduce the problem lucidly and concisely. In parallel, <a href="https://sites.google.com/site/marywootters/" rel="noopener noreferrer" target="_blank">Mary Wootters</a> dissected a talk titled “List-Decodable Subspace Recovery: Dimension Independent Error in Polynomial Time” that <a href="http://aineshbakshi.com" rel="noopener noreferrer" target="_blank">Ainesh Bakshi</a> presented. </p>



<p class="has-text-align-left has-text-align-justify">In the first <strong><em>AMA session</em></strong> moderated by <a href="https://www.stat.cmu.edu/~aramdas/" rel="noopener noreferrer" target="_blank">Aaditya Ramdas</a>, <a href="https://web.stanford.edu/~lmackey/" rel="noopener noreferrer" target="_blank">Lester Mackey</a> refreshingly answered several of the attendees’ well-curated questions about what makes strong collaborations, how to get into grad school, and whether or not he ever felt like quitting his Ph.D., among others.  He encouraged students to take classes with professors they are interested in as it makes it easy to ask for a mentorship opportunity. Lester talked about collaborations and imposter syndrome and encouraged attendees to look on the brighter side of things, to remember that we all are working towards one big goal, creating positive changes in the world. Therefore if someone discovers a result before us, we should applaud them, collaborate if possible, and move onto new problems. He said he did not necessarily plan to do a Ph.D. but got into it towards the end of his undergraduate degree due to an internship that made him fall in love with doing research. </p>



<p class="has-text-align-left has-text-align-justify">In the evening, there was an AMA session with <a href="http://people.csail.mit.edu/shafi/" rel="noopener noreferrer" target="_blank">Shafi Goldwasser</a> moderated by Nika. Shafi gave thoughtful and candid answers to attendees’ captivating questions about research, life in academia, collaborations, among others. Shafi told attendees that healthy competition, trust, and overlap of research interest, is crucial for successful research in the early stage of the career. She also asserted that fundamental science is always impactful. She mentioned that the high points of her career were working on problems she was curious about: cryptography, pseudo-randomness, and zero-knowledge proofs. Finally, when asked about what advice she wished she had during the early stage of her career, interestingly Shafi replied: “having good colleagues, good friends at work, very important, most important – having a listening, promoting and supportive cohort of friends rather than an individualistic path as a scholar is priceless.” </p>



<h3><strong>2.2 Technical Program</strong></h3>



<p class="has-text-align-left has-text-align-justify"><strong><em>Two research talks</em></strong> happened in the first session. First, Po-Ling Loh gave a talk titled “Mean estimation for entangled single-sample distributions.” Then <a href="https://web.stanford.edu/~vsharan/" rel="noopener noreferrer" target="_blank">Vatsal Sharan</a> talked about “Sample Amplification: Increasing Dataset Size even when Learning is Impossible”.<br/>Similarly in the second session, <a href="https://www.cohennadav.com" rel="noopener noreferrer" target="_blank">Nadav Cohen</a> gave the first talk about tensor and matrix completion problems and the importance of understanding the theory behind deep learning from theoretical and practical perspectives. After, <a href="https://i.cs.hku.hk/~zhiyi/" rel="noopener noreferrer" target="_blank">Zhiyi Huang</a> gave a talk titled “Setting the Sample Complexity of Single-parameter Revenue Maximization.” </p>



<h3><strong>2.3 Social Program</strong></h3>



<p class="has-text-align-left has-text-align-justify">In both sessions, during the social hours, <a href="https://sites.google.com/view/sumegha-garg/home" rel="noopener noreferrer" target="_blank">Sumegha Garg</a>, <a href="http://sgunasekar.github.io" rel="noopener noreferrer" target="_blank">Suriya Gunasekar</a>, and <a href="https://teddlyk.github.io" rel="noopener noreferrer" target="_blank">Thodoris Lykouris</a> organized the <strong><em>table topics</em></strong> to help attendees meet and interact with senior researchers and professors on different topics. The table topics included the following; starting on ML research, research agendas, ML+X: multidisciplinary research, advisor-advisee relationships, collaborators, communicating research and networking, beyond your institution: internships and research visits, planning after grad school: academia versus industry, Grad school applications, Work ethics, and Open research discussion. </p>



<p class="has-text-align-left has-text-align-justify">The table topics were chaired by; Jacob Abernethy, <a href="https://www.shivani-agarwal.net" rel="noopener noreferrer" target="_blank">Shivani Agarwal</a>, <a href="https://ericbalkanski.com" rel="noopener noreferrer" target="_blank">Eric Balkanski</a>, Peter Bartlett, Avrim Blum, <a href="http://sbubeck.com/" rel="noreferrer noopener" target="_blank">Sébastien Bubeck</a>, Kamalika Chaudhuri, Nadav Cohen, Sumegha Garg, Surbhi Goel, Suriya Gunasekar, Nika Haghtalab,  <a href="https://www.cs.columbia.edu/~djhsu/" rel="noopener noreferrer" target="_blank">Daniel Hsu</a>, <a href="https://www.prateekjain.org" rel="noopener noreferrer" target="_blank">Prateek Jain</a>, <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/jordan.html" rel="noopener noreferrer" target="_blank">Mike Jordan</a>, <a href="https://homes.cs.washington.edu/~sham/" rel="noopener noreferrer" target="_blank">Sham Kakade</a>, <a href="https://www.microsoft.com/en-us/research/people/adum/" rel="noopener noreferrer" target="_blank">Adam Kalai</a>, Pravesh Kothari, <a href="https://people.cs.umass.edu/~akshay/" rel="noopener noreferrer" target="_blank">Akshay Krishnamurthy</a>, <a href="https://jerryzli.github.io" rel="noopener noreferrer" target="_blank">Jerry Li</a>, Po-Ling Loh, Thodoris Lykouris, <a href="https://www.tau.ac.il/~mansour/" rel="noopener noreferrer" target="_blank">Yishay Mansour</a>, <a href="https://pasin30055.github.io" rel="noopener noreferrer" target="_blank">Pasin Manurangsi</a>, <a href="https://vmuthukumar.ece.gatech.edu" rel="noopener noreferrer" target="_blank">Vidya Muthukumar</a>, Praneeth Netrapalli, <a href="https://wensun.github.io" rel="noopener noreferrer" target="_blank">Wen Sun</a>, <a href="https://www.bowaggoner.com" rel="noopener noreferrer" target="_blank">Bo Waggoner</a>, <a href="https://mzampet.com" rel="noopener noreferrer" target="_blank">Manolis Zampetakis</a>, and <a href="https://cyrilzhang.com/">Cyril Zhang</a>. </p>



<p class="has-text-align-left has-text-align-justify">Lastly, at the end of the two general research talks in sessions one and two of the workshop, attendees assembled on Gather town to close the workshop. The social event included 1:1 social interactions with other attendees, an attempt at forging relationships, and activities like dancing.</p>



<h2><strong>3 Attendance Statistics, Testimonials and Feedback</strong></h2>



<h3><strong>3.1 Participants Statistics</strong></h3>



<p class="has-text-align-left has-text-align-justify">ALT mentoring workshop welcomed talented academics, researchers, and professionals from a wide array of backgrounds. Of the 438 registered to attend the workshop, 197 were new to the learning theory community, 37 attended at least one ALT/COLT conference in the past, and 146 hadn’t attended ALT/COLT but had attended machine learning conferences (STOC, NeurIPS, etc.) as shown in Figure 1. </p>



<p class="has-text-align-left has-text-align-justify">The workshop participants came from different parts of the world, were of different genders, races, and seniority levels. We use Figures 2, 3b, 4a, and 4b to highlight this demographic information about the participants. Some participants chose session one, and others chose session two, a choice driven by their schedules and time zones. The attendance composition is as shown in figure 3a.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-58" height="310" src="https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.28-AM.png?resize=515%2C310&amp;ssl=1" width="515"/>Figure 1: Registrants familiarity with the community</figure></div>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-61" height="401" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.11-AM-2.png?resize=678%2C401&amp;ssl=1" width="678"/>Figure 2: Career stages of participants</figure></div>



<figure class="wp-block-gallery columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img alt="" class="wp-image-70" height="414" src="https://i2.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.33.54-AM-1.png?resize=678%2C414&amp;ssl=1" width="678"/></figure></li><li class="blocks-gallery-item"><figure><img alt="" class="wp-image-71" height="406" src="https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.35.25-AM-1.png?resize=678%2C406&amp;ssl=1" width="678"/></figure></li></ul>Figure 3: Session preferences (a) and locations of participants (b)</figure>



<figure class="wp-block-gallery aligncenter columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img alt="" class="wp-image-75" height="404" src="https://i2.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.57-AM.png?resize=678%2C404&amp;ssl=1" width="678"/></figure></li><li class="blocks-gallery-item"><figure><img alt="" class="wp-image-77" height="404" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.42-AM-1.png?resize=678%2C404&amp;ssl=1" width="678"/></figure></li></ul>Figure 4: Race (a) and Gender (b) distribution of participants</figure>



<p/>



<h3><strong>3.2 Testimonials and Feedback</strong></h3>



<p class="has-text-align-left has-text-align-justify">In this section, we give a recount of testimonials from participants who we interviewed after the workshop. We also highlight some of the common themes in feedback from participants.</p>



<p class="has-text-align-left has-text-align-justify">In general, participants loved the content delivered in the sessions. They said it was informative, intuitive, and rare to find. Several participants loved interacting with peers and senior members and wished they had more time and activities to do it. The How-to-talks session (focused on networking skills, structuring papers, talks, and reviews) was the most popular session among attendees. 76.7% of the survey respondents said the session helped them gain new technical skills or hone existing skills, see opportunities in academia and how to use them, and see barriers in academia and ways to overcome them. Figure 5 highlights attendees’ ratings of the skills acquired from the workshop. </p>



<figure class="wp-block-image size-large is-resized is-style-default"><img alt="" class="wp-image-80" height="404" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.35.11-AM.png?resize=678%2C404&amp;ssl=1" width="678"/>Figure 5: Usefulness ratings of skills gained from the workshop</figure>



<p><strong>Below is a recount of the workshop experiences of the interviewed attendees.</strong></p>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My highlight was the How-to-Talks since they provided a lot of more personal information/inputs that you cannot easily find online and which was very valuable. The event helped me to remember and reflect upon which qualities are crucial to becoming a good researcher. I even made a list in a place that I see every day to keep them in mind.” – <em><a href="https://www.michaelaerni.com" rel="noopener noreferrer" target="_blank">Michael Aerni</a>, MSc student at ETH Zurich.</em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“The workshop highlights for me were the How-to-talks, the AMA session with Lester, and the social tables. The How-to-talks were extremely valuable as they discussed topics such as structuring papers and networking in the community. These are subtle aspects that are not often explicitly talked about in the community. I, therefore, learned a lot from them. The AMA session was refreshingly honest and open. Finally, the social tables were also great as I got to meet and talk to some well-established senior community members like Sebastien Bubeck, Shivani Agarwal, and Akshay Krishnamurthy.” – <em><a href="https://people.eecs.berkeley.edu/~tgautam23/" rel="noopener noreferrer" target="_blank">Tanmay Gautam</a>, a second-year Ph.D. student at UC Berkeley. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“It was awesome to have Lester Mackey answer my questions, indubitably. I learned about staying true to the research questions I genuinely believe in regardless of external opinions and rewards. As an NYU AI School organizer, I can appreciate how much effort went into organizing the workshop. The organizers did a stellar job! I think a version of the same event again would be perfect.” – <em><a href="https://swapneelm.github.io" rel="noopener noreferrer" target="_blank">Swapneel Mehta</a>, a Data Science Ph.D. student at New York University. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My highlights were getting to talk with senior members of the community. These opportunities rarely come by for someone who lives in a foreign country. For a timid person like myself, I am also thankful for the senior members for helping me (and other participants) breaking the ice and easing us into the conversations. Thanks to this event, I am now more confident in engaging with other researchers.”- <em><a href="http://www.donlapark.cmustat.com" rel="noopener noreferrer" target="_blank">Donlapark Ponnoprat</a>, a Statistics lecturer at Chiang Mai University. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My main highlights were Dr. Goldwasser’s session with Dr. Haghtalab and the socials. In the socials, I was able to ask professors and senior researchers for advice on varied topics based on the tables. Insights from Dr. Cohen’s lecture on tensor rank and implicit regularisation gave me several pointers to ideas in the literature that I was not aware of as a junior researcher from a slightly different AI specialty. These ideas might be beneficial for my research in the long term. </p><p class="has-text-align-left has-text-align-justify"> I send a sincere thank you to all the organizers. The mentorship workshop was a great event, and it models concrete actions, what it means to foster a welcoming community. It is clear how kind and dedicated folks are here as some researchers even stayed beyond midnight in their time zones to answer questions that attendees had. If an event like this happens again, I am most definitely signing up to come.” – <em><a href="https://github.com/esraa-saleh" rel="noopener noreferrer" target="_blank">Esra’a Saleh</a>, a Masters in Computer Science student at the University of Alberta, affiliated with AMII and RLAI.</em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My main takeaway from the event was an inside look at academia. As an undergrad, my only experience in academia has been the little experience I have with my advisory professors. While this is an invaluable experience, this event was nice as it was one of the very few that cater to students, including undergrads, with the intent of bringing them into the academia fold. Getting to know new people and talking to them was extremely interesting, especially during lockdown when connecting with others is a much more valuable commodity.” – <em><a href="https://pages.cs.wisc.edu/~shrey/" rel="noopener noreferrer" target="_blank">Shrey Shah</a>, a penultimate year undergraduate student at the University of Wisconsin-Madison. 
</em></p></blockquote>



<p class="has-text-align-left has-text-align-justify">Several participants enjoyed the workshop sessions and hoped that Let-All holds more similar themed workshops in conferences. Attendees suggested ways for attendees to interact more with each other. Some of the suggestions included the following: ‘beginner-friendly open problems sessions where attendees can collaborate’ – Esra’a Saleh, ‘an icebreaker session at the beginning that encourages attendees to mingle’ – Michael Aerni, and ‘a poster session for participants to present their work’ – Shrey Shah. </p>



<h2><strong>4 Conclusion</strong></h2>



<p class="has-text-align-left has-text-align-justify">The ALT mentorship workshop organized by the Learning Theory Alliance brought together many academics and researchers. It was, and we hope it continues to be, an opportunity for the budding researchers to learn about research and meta-research, forge collaborations, and be inspired. Kudos to the organizers and the Alliance in general for dreaming such a positive vision and then striving to make it a great success! </p>



<p class="has-text-align-left has-text-align-justify"><em>Thanks to <a href="http://www.gautamkamath.com" rel="noreferrer noopener" target="_blank">Gautam Kamath</a>, <a href="https://web.stanford.edu/~mglasgow/" rel="noreferrer noopener" target="_blank">Margalit Glasgow</a>, Surbhi Goel, Nika Haghtalab</em> <em>and Ellen Vitercik for helpful conversations and comments.</em></p></div>
    </content>
    <updated>2021-05-06T15:44:40Z</updated>
    <published>2021-05-06T15:44:40Z</published>
    <category term="ALT Highlights"/>
    <author>
      <name>Keziah</name>
    </author>
    <source>
      <id>https://www.let-all.com/blog</id>
      <logo>https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/04/logo.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://www.let-all.com/blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://www.let-all.com/blog" rel="alternate" type="text/html"/>
      <title>The Learning Theory Alliance Blog</title>
      <updated>2021-05-07T16:39:44Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-21129445.post-4927313116100787313</id>
    <link href="http://mysliceofpizza.blogspot.com/feeds/4927313116100787313/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=21129445&amp;postID=4927313116100787313" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/21129445/posts/default/4927313116100787313" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/21129445/posts/default/4927313116100787313" rel="self" type="application/atom+xml"/>
    <link href="http://mysliceofpizza.blogspot.com/2021/05/scaling-research-training.html" rel="alternate" type="text/html"/>
    <title>Scaling Research Training</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>There is a lot of need in the Industry for engineers who know the art (edited to <i>craft</i>)of research (pursue and find the right literature and algorithms, understand prior art, adopt and modify for a specific context with domain awareness, interpret results,  dive deeper into the unique insights), or researchers who can apply this art with engineering finesse. I wondered yesterday in a meeting if our technological lessons from COVID times have helped us identify a way to train many more in the ways of research, more than what we produce as PhDs in universities (the MS programs seem to train advanced engineers more than padawan researchers). <br/></p></div>
    </content>
    <updated>2021-05-06T13:46:00Z</updated>
    <published>2021-05-06T13:46:00Z</published>
    <category scheme="http://www.blogger.com/atom/ns#" term="aggregator"/>
    <author>
      <name>metoo</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/07192519900962182610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-21129445</id>
      <category term="aggregator"/>
      <category term="Non-CS"/>
      <author>
        <name>metoo</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/07192519900962182610</uri>
      </author>
      <link href="http://mysliceofpizza.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/21129445/posts/default/-/aggregator" rel="self" type="application/atom+xml"/>
      <link href="http://mysliceofpizza.blogspot.com/search/label/aggregator" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/21129445/posts/default/-/aggregator/-/aggregator?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>books, stories, poems, algorithms, math and computer science. 

some art and anecdotes too.</subtitle>
      <title>my slice of pizza</title>
      <updated>2021-05-06T15:39:21Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6832816500979478899</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6832816500979478899/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/negotiations.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6832816500979478899" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6832816500979478899" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/negotiations.html" rel="alternate" type="text/html"/>
    <title>Negotiations</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>So you got an offer to be an assistant professor in the computer science department at Prestigious U. Congratulations! </p><p>Time to negotiate your offer with the chair. Don't be nervous. This shouldn't be adversarial. Both of you have the same goal in mind--for you to come to Prestigious and be successful. </p><p>Let's discuss the different aspects of each package.</p><p><b>Research </b></p><p>Funds for supporting your research such as equipment, graduate student support, travel and postdocs. Here you should explain what you need to be successful. This will vary by subdiscipline, a systems researcher will need more equipment and students than a theorist. Keep in mind the university is giving you funds for 2-4 years to start your research, after which you are expected to fund your own research via grants.</p><p>I don't recommend taking on a postdoc right at the start of your first academic appointment. Postdocs require good mentoring while you need to spend the first year getting your research up and running. If you do ask for postdoc money, ask to have a flexible start time.</p><p>Many departments give course reductions to get your research going. I'd suggest asking to spend your first semester teaching a graduate topics course based on your thesis research to pick up some PhD students followed by a semester with no classes to get you research program going.</p><p><b>Salary</b></p><p>This includes actual salary, which is also the base for future raises, and summer salary in the first couple of years. Feel free to ask for more salary, but often these numbers are fixed for new assistant professors. There is more give if you take an academic job later in your career. You could also say something like, "Well if you can't give me more salary maybe you could give me another semester of grad student support?"</p><p><b>Partner</b></p><p>It seems 80% of the time, a job candidate has a partner that needs accommodating. Don't wait until the end of negotiations, bring it up early. The more time we have, the better we can help. Doesn't matter what job they want--we know people and we know people who know people.</p><p><b>Thesis</b></p><p>Many schools won't hire you as an assistant professor if you haven't finished your thesis. Has to do with college rankings work. Don't worry--they will generally give you some other role with the same package until you finish. This might delay your tenure clock though.</p><p><b>Delayed start time</b></p><p>A January start is usually fine with good reason but if you weren't planning to start until the fall of 2022 why are you on the market this year? If you do get the department to hold a position for you, remember you are also making a commitment--this is not an opportunity to try again for something better.</p><p><b>Overall</b></p><p>You may not get all that you want after a negotiation--don't take it personally. You shouldn't necessarily choose the place that gives you the biggest package. It's far more important in the long run that you pick a place where you can best succeed both professionally and personally, and the package is just a small piece of that puzzle.</p></div>
    </content>
    <updated>2021-05-06T13:07:00Z</updated>
    <published>2021-05-06T13:07:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-07T09:41:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=1628</id>
    <link href="https://toc4fairness.org/self-fulfilling-and-self-negating-predictions-a-short-tale-of-performativity-in-machine-learning/" rel="alternate" type="text/html"/>
    <title>Self-fulfilling and self-negating predictions: a short tale of performativity in machine learning</title>
    <summary>This post is based on results and discussions from a series of joint works with Moritz Hardt, Celestine Mendler-Dünner, John Miller, and Juan C. Perdomo. In 1998, Michel Callon wrote ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><blockquote class="wp-block-quote has-text-align-center"><p><em>This post is based on results and discussions from a series of joint works with Moritz Hardt, Celestine Mendler-Dünner, John Miller, and Juan C. Perdomo.</em></p></blockquote>



<p>In 1998, Michel Callon wrote what would be the first in an ongoing series of controversial publications in economic sociology [1]. He was the first to propose the idea that “the economy is not embedded in society but in economics”. With this, he challenged the conventional view that economic theories and models passively observe markets and infer their behavior, just like laws of physics passively describe the principles governing natural phenomena. Instead, Callon argued that economic theories are <em>performative:</em> they induce the economy, creating the phenomena they aim to describe.</p>



<p>One example that is often cited in support of Callon’s claims is the impact of the celebrated Black-Scholes-Merton options pricing model [2, 3]. MacKenzie and Millo [4] investigated the role of this model in the economy and found that it “made itself true”. In their words,</p>



<p class="has-text-align-center"><em>“Black, Scholes, and Merton’s model did not describe an already existing world: when first formulated, its assumptions were quite unrealistic, and empirical prices differed systematically from the model. Gradually, though, the financial markets changed in a way that fitted the model”.</em></p>



<p>Indeed, participants in the market started making decisions assuming the market obeys the mathematical laws implied by the Black-Scholes-Merton model. As MacKenzie and Millo put it, “pricing models came to shape the very way participants thought and talked about options”.</p>



<p>This phenomenon — whereby models and predictions inform decision-making and thus alter the target of prediction itself — is by no means special to economic forecasts.</p>



<p>Predictive policing, for example, develops algorithms that use historical data to estimate the likelihood of crime at a given location. Those locations where criminal behavior is deemed likely by the system typically get more police patrols and surveillance in general. In a kind of self-fulfilling prophecy [5], these actions resulting from prediction might further increase the <em>perceived</em> crime rate at the patrolled locations, thus biasing the data used for future decisions.</p>



<p>A similar feedback loop arises in traffic predictions, when drivers decide which route to take based on the estimated time of arrival (ETA) calculated by a traffic prediction system. If the predictive system estimates low ETA for a given route, many drivers take the route, potentially leading to an overflow of traffic and making the ETA prediction inaccurate as a result. Contrary to the previous example, traffic predictions arguably exhibit a self-negating prophecy: low ETA might imply a longer travel time, and vice versa.</p>



<p>While the previous examples deal with qualitatively different feedback mechanisms, the interplay of predictions and decision-making is similar. First, one uses historical data to build a predictive model. Then, the predictions of the model feed into and inform consequential decisions. Finally, these decisions trigger changes in the environment, making future observations differ from those in the initial dataset.</p>



<p class="has-text-align-center"><img height="81" src="https://lh3.googleusercontent.com/C3qD-SK6AD4gDpkJznllugJS8OZaJwGvXSNi96qYKPu2ALr5vSxPMhteCScLq-CCdKOvh8bZ8KB-gvTRVkbZALVEuML0WhSV0pX8YM1MHupTLPjZ8PKEydiNu_h3iUdlVAYUcFDT" width="624"/></p>



<p>We refer to prediction problems that exhibit this feedback-loop behavior as <em>performative prediction</em> problems.</p>



<p>In the language of machine learning, such a change in patterns would often be called <em>distribution shift</em>. Notably, however, performative distribution shift is not due to external factors independent of the model, such as, say, when traffic patterns change due to seasonal effects. Rather, the distribution shift is triggered directly by the choice of predictive model. (Of course, distribution shifts can also be caused by a combination of external factors and model choice.)</p>



<p>To formalize performative prediction mathematically, it is instructive to contrast performative prediction problems with supervised learning problems. In supervised learning, the decision-maker observes pairs of features and outcomes <img height="15" src="https://lh5.googleusercontent.com/2U92MhGBO6DJqe607HL2T4uWicPXKFgrU2PoSaeymNLxbOteL6r_dhkuuFo91W2AXoAzrhxt8Ndg1jfhf8KVqZMbID1koi01cpGcXz-CTACfH_b54DohHMqpO2hJ5bEtfdE6v6Ag" width="72"/> drawn from a <em>fixed</em> distribution <img height="13" src="https://lh6.googleusercontent.com/Rj1cE6eMb1304KzXnhY0e_dxrgXvPNd9q-IzgZLuKKxFYfn3q82FIiQUwb0_DO8G-xyuGdp8T5Y-EN5bKYPPxSd5QaMMO1irMwSeI6O5sy08Ubriy2cFliObESx_XOoFzPwZy6Wi" width="14"/>. The key difference in performative prediction is that there is no longer an unknown static distribution generating observations; rather, data is drawn from a <em>model-dependent distribution</em> <img height="17" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/>, where <img height="12" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/> is a parameter vector specifying the deployed model. For example, <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/> could be the weights of a neural network, or a vector of linear regression coefficients. For a given choice of parameters <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/>, <img height="17" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/> should be thought of as the distribution over features and outcomes that results from making decisions according to the model specified by <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/>. In the context of the traffic prediction example, <img height="17" src="https://lh4.googleusercontent.com/HDm0VWvPNAUIdPfMSLc0ptEDS7Tph1p00WbC6TlSbQ8oYjK2nJSuSMiLuKN8IAA89L2H8rDLRPf9GWwc37Zb_0_qLxN-UuVVsz_hkBB43hQcTOtV4HUg30BtsHel_hpCFHfYJa6E" width="32"/> could be a distribution over traffic conditions and travel times, given that drivers make routing decisions in response to ETA forecasts by model <img height="13" src="https://lh3.googleusercontent.com/TkPLvHJ9t0soCQkx43reFppYsx4b0sJrNUBWd_Yx_rarIU4nS92GpBFBn-D_jhvDNAU04Aee4NMsVzHEozOLxXN11DMl4h1O2TvHfNih9IqHi0R3wM1otXYEp1MbkZlbI0faSV1i" width="8"/>.</p>



<p>In supervised learning, the quality of a model <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/> is typically measured by its <em>risk</em>, namely, the expected loss of the model on instances from distribution <img height="13" src="https://lh6.googleusercontent.com/Rj1cE6eMb1304KzXnhY0e_dxrgXvPNd9q-IzgZLuKKxFYfn3q82FIiQUwb0_DO8G-xyuGdp8T5Y-EN5bKYPPxSd5QaMMO1irMwSeI6O5sy08Ubriy2cFliObESx_XOoFzPwZy6Wi" width="14"/> as measured via a loss function <img height="16" src="https://lh5.googleusercontent.com/iLwCF0E2qkddKEmiLhziUoOB171Na3z6aLjqWSYTPykfGKIx5PBtNT9qsZXIMPLc3hVXkkupbpqILbC4Fx9p_h5G8a1GmlYVL3rcx787SVO-24HKrL39LxAwPGaQ2i8wuPURBoyS" width="9"/>:</p>



<p class="has-text-align-center"><img height="26" src="https://lh5.googleusercontent.com/vP7Ll-u4VopJmW24L2lMvqwf8N9bOrKaoLFDDlmcZgmML4CM8bM53_lMEG--5Om2mTYOQ5uNqUBkzlp2Hgd9i3EO8Y8bzC8kvkFgQKSfqOgHoWiMpK2MVasGJmdFTdERwIUg0RzK" width="153"/></p>



<p>Since performative prediction does not admit one true data-generating distribution, but rather a family of distributions <img height="18" src="https://lh6.googleusercontent.com/un2s0qA36MFKtNGHoDb7sQdPfToUoZjYy45nbwmSy2jQhSDJbiouNjATQWsSwDlpUF-PfOk1ElFPDb8Rsr_TAAB0GUemVV5Y_bG1EISEUD7M9N5RGxug1gGdvpXa-D70ATQSafzT" width="58"/>, evaluating a model <img height="13" src="https://lh4.googleusercontent.com/zJsapVnRoEAmwRciJFyPDFM3pc3YOuB_6gM71GJX1Aa_9Tm09RLD4mj_DOEkH0CzKHFPV93LesWVf1AUH9QlwQoU0x07xusBHiT8-EeAApf2qMsufNc8Vuzc77LWu4bxEuYwgxXp" width="8"/> calls for a new risk concept. Arguably the most natural counterpart of the risk in supervised learning is the expected loss on the distribution that arises once the model is deployed and feeds into consequential decisions. This leads to the notion of <em>performative risk</em>, defined as:</p>



<p class="has-text-align-center"><img height="30" src="https://lh5.googleusercontent.com/zWCmnf2uG6j8LZG7w68p8y4I2wHcKc82ZULfh_MUmGc3YI6UByQP8dB3nBDugFmg8t0VybMz6RV4tzviUsIyHmVY1uUH1Xjv2XKTSEQGpFW3-o0e50-rxKeouEula7UNdqC6HYHr" width="181"/></p>



<p>Adopting the performative risk as the single overarching measure of quality, a model would be optimal if it minimizes the performative risk. While an appealing solution concept, performative optimality is difficult to achieve, seeing the double dependence of the risk function on <img height="13" src="https://lh3.googleusercontent.com/ywU1sy8RfVug9ZI2C0ETvBvXg7_gqi_Xx9a1-nILxAhSafCu1OvR5095fDFr3PJN9bmBa6qoPoLzz0bFrjPtTEw5-BX3vvKHqTUyvDFaDwt7mc0B41XPpPgZZ_T4gb72PAFMDwAK" width="8"/>. One of the main computational difficulties is the fact that, even if the loss <img height="16" src="https://lh5.googleusercontent.com/WZWkgjmj0CvaBREmhCbwBw1i5JfL1Tviq96JbXG1SePOG-BXJ8uv_fUUE2KKJCw2qvd5zpA_GUhHZ9azV6AUENm0qVsKnTPMg6WzLvT5QGGLNwcLyhRU0iOhYtRy6Fylx66uMGRS" width="9"/> is convex in <img height="13" src="https://lh3.googleusercontent.com/ywU1sy8RfVug9ZI2C0ETvBvXg7_gqi_Xx9a1-nILxAhSafCu1OvR5095fDFr3PJN9bmBa6qoPoLzz0bFrjPtTEw5-BX3vvKHqTUyvDFaDwt7mc0B41XPpPgZZ_T4gb72PAFMDwAK" width="8"/>, <img height="17" src="https://lh4.googleusercontent.com/-syiEeOCPlb5lZOfPAK063xD0s6EraAKMtHoPUR8cIfpnP0AdkrnINBFw6Ejb8nr1jF3G7QO7v5K7o28nSnnI74SPxaJH7apiBu7Bpd7DiWnZ6Hx79Yi4v3iwJQR8j0eUJh2Qzfk" width="43"/> need not be convex. Prior work on strategic classification [6] implies a set sufficient conditions for <img height="17" src="https://lh4.googleusercontent.com/-syiEeOCPlb5lZOfPAK063xD0s6EraAKMtHoPUR8cIfpnP0AdkrnINBFw6Ejb8nr1jF3G7QO7v5K7o28nSnnI74SPxaJH7apiBu7Bpd7DiWnZ6Hx79Yi4v3iwJQR8j0eUJh2Qzfk" width="43"/> to be convex in a binary classification context, and in recent work [7] we identified a complementary set of conditions when the family of distributions <img height="18" src="https://lh4.googleusercontent.com/Ti5C4ibszTKUqTM5evbcuaHojWHAN_Rq20XcnDlunJNG1M4lBKsjB7nJOKEN4lTSdWNmgKkcxqAwPPRahQcZ4uL8kCZHn_AuaB_Z9T35eztMk2CZzMcK50GMGnH69Jdjd0euwDtI" width="58"/> forms an appropriate location-scale family. That said, in many practical settings convexity might be an unrealistic and unnecessarily strong guarantee to aim for. As we know from present-day machine learning, even non-convex problems can sometimes be amenable to simple optimization algorithms. Understanding the optimization landscape of the performative risk beyond convex settings is a fruitful direction going forward.</p>



<p>A seemingly less ambitious target is to find a model that is <em>locally</em> optimal in some appropriate sense. For example, one could optimize for models that are optimal on the distribution that they induce:</p>



<p class="has-text-align-center"><img height="22" src="https://lh6.googleusercontent.com/ENuLNHEI7mNEDONR7rUA2FzwbkvqbINkwHJfO7jiiODrZ3_Ko9NZmJAt4Qv0cPNF4-_C68yI3CE4UMcFjzP8v5UHdu_LgBkdrCnUj4A-E6uccXlclWWZef2onVzsd5X2sHq5tTlh" width="233"/></p>



<p>We call a model that satisfies the fixed-point equation above <em>performatively stable</em>. Performative stability arises naturally when the decision-maker applies the heuristic of myopically updating the model based on the distribution resulting from the previous deployment:</p>



<p class="has-text-align-center"><img height="20" src="https://lh4.googleusercontent.com/RlZ0ZmF3E0VHyj_ktWb3QtAxsXTY_WjoGS29Jss3v_5cXnDVsWbeEvaTAA4l4tUYa5ccYvJPWcPXy5oLa3BnBT0Yqws89mNind-2UT8IA2ip3_pt3PbFhiNVcgIhgMS1omjfgzI1" width="248"/></p>



<p>If this retraining strategy converges, then it necessarily converges to a performatively stable solution. This is an appealing property, since it says that stability eliminates the need for retraining. Several existing works [8, 9, 10] have identified necessary and sufficient conditions for the above retraining heuristic, and some of its efficient approximations, to converge to a stable point. Roughly speaking, retraining converges to a stable solution if the loss is well-behaved and the performative feedback effects are not too strong. If either of those two conditions is violated, there is no guarantee of convergence.</p>



<p>In the language of game theory, one can think of performative prediction as a two-player game between a decision-maker, who decides which predictive model to deploy, and the model’s environment, which generates observations according to <img height="16" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/>. If <img height="16" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/> is thought of as the “best response” (according to some underlying utility) of the model’s environment to the deployment of model <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/>, then a performatively stable solution corresponds to a <em>Nash</em> equilibrium, while a performatively optimal solution corresponds to a <em>Stackelberg</em> equilibrium with the decision-maker acting as the leader.</p>



<p>Only in special cases, such as in well-behaved zero-sum games, it is known that Nash equilibria coincide with Stackelberg equilibria. Therefore, whenever performative prediction is a well-behaved zero-sum game, all stable solutions are also performatively optimal. However, <em>performative prediction is typically not a zero-sum game</em>. For example, if the decision-maker’s loss simply measures predictive accuracy, it seems odd that the environment’s primary objective is to hurt the model’s accuracy. Indeed, a typical performative prediction problem is a general-sum game without much structure. This implies that stable solutions and performative optima can be <em>very different</em>. And, since naive retraining strategies only converge to stability, this means that such myopic updates can be an inadequate method of overcoming performative distribution shifts and achieving low performative risk. This observation further motivates understanding the optimization landscape of the performative risk, as well as developing efficient algorithms for optimizing it. Recent work has explored several algorithmic solutions [11, 7], appropriate in convex settings.</p>



<p>Performative prediction relates to many other areas beyond game theory, including bandits, reinforcement learning, control theory. These frameworks are flexible enough to capture performative prediction as a special case, however performativity arises via distinctive feedback mechanisms and as such deserves its own specialized analysis. There is a long way to go in understanding the properties of performative distribution shifts, how they connect to feedback mechanisms in other disciplines, and how to tackle these shifts in practice. Furthermore, it is unclear whether a single distribution <img height="16" src="https://lh6.googleusercontent.com/a-1tKaIQT9pJwi6WQ4eV4ZO3xcFy1e_joEh0tz1QdG-cj7x-60nDwfRyyztRSzjEaL-0xbImihmV5LDB8njPUhSlsIeyJABtowIBOktz79sRW7hAY8B75CePuDxp47xEFOZ68gSA" width="32"/> is expressive enough to describe the observations after model deployment; in practice there are different kinds of memory effects [12] and self-reinforcing loops that make the data distribution evolve with time, even when the model is kept fixed. Finally, to make the existing theoretical insights actionable, going forward we need to think about what is the right solution concept — both statistically and ethically — to optimize for in performative settings.</p>



<p><br/>[1] M. Callon. Introduction: the embeddedness of economic markets in economics. <em>The Sociological Review</em>, 1998<br/>[2] F. Black, M. Scholes. The pricing of options and corporate liabilities. <em>The Journal of Political Economy</em>, 1973<br/>[3] R. C. Merton. Theory of rational option pricing. <em>The Bell Journal of Economics and Management Science</em>, 1973<br/>[4] D. MacKenzie, Y. Millo. Constructing a market, performing theory: The historical sociology of a financial derivatives exchange. <em>American Journal of Sociology</em>, 2003<br/>[5] D. Ensign, S. A. Friedler, S. Neville, C. Scheidegger, S. Venkatasubramanian. Runaway feedback loops in predictive policing. <em>ACM Conference on Fairness, Accountability and Transparency</em>, 2018<br/>[6] J. Dong, A. Roth, Z. Schutzman, B. Waggoner, Z. S. Wu. Strategic classification from revealed preferences. <em>ACM Conference on Economics and Computation</em>, 2018<br/>[7] J. Miller, J. C. Perdomo, T. Zrnic. Outside the echo chamber: Optimizing the performative risk. <em>arXiv preprint</em>, 2021<br/>[8] J. C. Perdomo, T. Zrnic, C. Mendler-Dünner, M. Hardt. Performative prediction. <em>International Conference on Machine Learning</em>, 2020<br/>[9] C. Mendler-Dünner, J. C. Perdomo, T. Zrnic, M. Hardt. Stochastic optimization for performative prediction. <em>Conference on Neural Information Processing Systems</em>, 2020<br/>[10] D. Drusvyatskiy, L. Xiao. Stochastic optimization with decision-dependent distributions. <em>arXiv preprint</em>, 2020<br/>[11] Z. Izzo, L. Ying, J. Zou. How to learn when data reacts to your model: Performative gradient descent. <em>arXiv preprint</em>, 2021<br/>[12] G. Brown, S. Hod, I. Kalemaj. Performative prediction in a stateful world. <em>arXiv preprint</em>, 2020</p></div>
    </content>
    <updated>2021-05-06T12:42:52Z</updated>
    <published>2021-05-06T12:42:52Z</published>
    <category term="Blog"/>
    <author>
      <name>tijanazrnic</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i1.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2021-05-07T16:39:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.02110</id>
    <link href="http://arxiv.org/abs/2105.02110" rel="alternate" type="text/html"/>
    <title>Essentiality of the Non-stoquastic Hamiltonians and Driver Graph Design in Quantum Optimization Annealing</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choi:Vicky.html">Vicky Choi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.02110">PDF</a><br/><b>Abstract: </b>One of the distinct features of quantum mechanics is that the probability
amplitude can have both positive and negative signs, which has no classical
counterpart as the classical probability must be positive. Consequently, one
possible way to achieve quantum speedup is to explicitly harness this feature.
Unlike a stoquastic Hamiltonian whose ground state has only positive amplitudes
(with respect to the computational basis), a non-stoquastic Hamiltonian can be
eventually stoquastic or properly non-stoquastic when its ground state has both
positive and negative amplitudes. In this paper, we describe that, for some
hard instances which are characterized by the presence of an anti-crossing (AC)
in a stoquastic quantum annealing (QA) algorithm, how to design an appropriate
XX-driver graph (without knowing the prior problem structure) with an
appropriate XX-coupler strength such that the resulting non-stoquastic QA
algorithm is proper-non-stoquastic with two bridged anti-crossings (a
double-AC) where the spectral gap between the first and second level is large
enough such that the system can be operated diabatically in polynomial time.
The speedup is exponential in the original AC-distance, which can be
sub-exponential or exponential in the system size, over the stoquastic QA
algorithm, and possibly the same order of speedup over the state-of-the-art
classical algorithms in optimization. This work is developed based on the novel
characterizations of a modified and generalized parametrization definition of
an anti-crossing in the context of quantum optimization annealing introduced in
[4].
</p></div>
    </summary>
    <updated>2021-05-06T23:05:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.02084</id>
    <link href="http://arxiv.org/abs/2105.02084" rel="alternate" type="text/html"/>
    <title>Local Algorithms for Bounded Degree Sparsifiers in Sparse Graphs</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomon:Shay.html">Shay Solomon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.02084">PDF</a><br/><b>Abstract: </b>In graph sparsification, the goal has almost always been of {global} nature:
compress a graph into a smaller subgraph ({sparsifier}) that maintains certain
features of the original graph. Algorithms can then run on the sparsifier,
which in many cases leads to improvements in the overall runtime and memory.
This paper studies sparsifiers that have bounded (maximum) degree, and are thus
{locally} sparse, aiming to improve local measures of runtime and memory. To
improve those local measures, it is important to be able to compute such
sparsifiers {locally}.
</p>
<p>We initiate the study of local algorithms for bounded degree sparsifiers in
unweighted sparse graphs, focusing on the problems of vertex cover, matching,
and independent set. Let $\epsilon &gt; 0$ be a slack parameter and $\alpha \ge 1$
be a density parameter. We devise local algorithms for computing: (1) A
$(1+\epsilon)$-vertex cover sparsifier of degree $O(\alpha / \epsilon)$, for
any graph of {arboricity} $\alpha$. (2) A $(1+\epsilon)$-maximum matching
sparsifier and also a $(1+\epsilon)$-maximal matching sparsifier of degree
$O(\alpha / \epsilon)$, for any graph of arboricity $\alpha$. (3) A
$(1+\epsilon)$-independent set sparsifier of degree $O(\alpha^2 / \epsilon)$,
for any graph of average degree $\alpha$.
</p>
<p>Our algorithms require only a single communication round in the standard
message passing models of distributed computing, and moreover, they can be
simulated locally in a trivial way. As an immediate application we can extend
results from distributed computing and local computation algorithms that apply
to graphs of degree bounded by $d$ to graphs of arboricity $O(d / \epsilon)$ or
average degree $O(d^2 / \epsilon)$, at the expense of increasing the
approximation guarantee by a factor of $(1+\epsilon)$. In particular, we can
extend the plethora of recent local computation algorithms [...]
</p></div>
    </summary>
    <updated>2021-05-06T22:47:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.02052</id>
    <link href="http://arxiv.org/abs/2105.02052" rel="alternate" type="text/html"/>
    <title>Scheduling with Testing on Multiple Identical Parallel Machines</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Albers:Susanne.html">Susanne Albers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eckl:Alexander.html">Alexander Eckl</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.02052">PDF</a><br/><b>Abstract: </b>Scheduling with testing is a recent online problem within the framework of
explorable uncertainty motivated by environments where some preliminary action
can influence the duration of a task. Jobs have an unknown processing time that
can be explored by running a test. Alternatively, jobs can be executed for the
duration of a given upper limit. We consider this problem within the setting of
multiple identical parallel machines and present competitive deterministic
algorithms and lower bounds for the objective of minimizing the makespan of the
schedule. In the non-preemptive setting, we present the SBS algorithm whose
competitive ratio approaches $3.1016$ if the number of machines becomes large.
We compare this result with a simple greedy strategy and a lower bound which
approaches $2$. In the case of uniform testing times, we can improve the SBS
algorithm to be $3$-competitive. For the preemptive case we provide a
$2$-competitive algorithm and a tight lower bound which approaches the same
value.
</p></div>
    </summary>
    <updated>2021-05-06T22:45:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.02022</id>
    <link href="http://arxiv.org/abs/2105.02022" rel="alternate" type="text/html"/>
    <title>Deep Multilevel Graph Partitioning</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gottesb=uuml=ren:Lars.html">Lars Gottesbüren</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heuer:Tobias.html">Tobias Heuer</a>, Peter Sanders, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schulz:Christian.html">Christian Schulz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seemaier:Daniel.html">Daniel Seemaier</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.02022">PDF</a><br/><b>Abstract: </b>Partitioning a graph into blocks of "roughly equal" weight while cutting only
few edges is a fundamental problem in computer science with a wide range of
applications. In particular, the problem is a building block in applications
that require parallel processing. While the amount of available cores in
parallel architectures has significantly increased in recent years,
state-of-the-art graph partitioning algorithms do not work well if the input
needs to be partitioned into a large number of blocks. Often currently
available algorithms compute highly imbalanced solutions, solutions of low
quality, or have excessive running time for this case. This is because most
high-quality general-purpose graph partitioners are multilevel algorithms which
perform graph coarsening to build a hierarchy of graphs, initial partitioning
to compute an initial solution, and local improvement to improve the solution
throughout the hierarchy. However, for large number of blocks, the smallest
graph in the hierarchy that is used for initial partitioning still has to be
large.
</p>
<p>In this work, we substantially mitigate these problems by introducing deep
multilevel graph partitioning and a shared-memory implementation thereof. Our
scheme continues the multilevel approach deep into initial partitioning --
integrating it into a framework where recursive bipartitioning and direct k-way
partitioning are combined such that they can operate with high performance and
quality. Our approach is stronger, more flexible, arguably more elegant, and
reduces bottlenecks for parallelization compared to other multilevel
approaches. For example, for large number of blocks our algorithm is on average
an order of magnitude faster than competing algorithms while computing balanced
partitions with comparable solution quality. For small number of blocks, our
algorithms are the fastest among competing systems with comparable quality.
</p></div>
    </summary>
    <updated>2021-05-06T23:05:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01963</id>
    <link href="http://arxiv.org/abs/2105.01963" rel="alternate" type="text/html"/>
    <title>One-way communication complexity and non-adaptive decision trees</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mande:Nikhil_S=.html">Nikhil S. Mande</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sanyal:Swagato.html">Swagato Sanyal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01963">PDF</a><br/><b>Abstract: </b>We study the relationship between various one-way communication complexity
measures of a composed function with the analogous decision tree complexity of
the outer function. We consider two gadgets: the AND function on 2 inputs, and
the Inner Product on a constant number of inputs. Let $IP$ denote Inner Product
on $2b$ bits.
</p>
<p>1) If $f$ is a total Boolean function that depends on all of its inputs, the
bounded-error one-way quantum communication complexity of $f \circ IP$ equals
$\Omega(n(b-1))$.
</p>
<p>2) If $f$ is a partial Boolean function, the deterministic one-way
communication complexity of $f \circ IP$ is at least $\Omega(b \cdot
D_{dt}^{\rightarrow}(f))$, where $D_{dt}^{\rightarrow}(f)$ denotes the
non-adaptive decision tree complexity of $f$.
</p>
<p>For our quantum lower bound, we show a lower bound on the VC-dimension of $f
\circ IP$, and then appeal to a result of Klauck [STOC'00]. Our deterministic
lower bound relies on a combinatorial result due to Frankl and Tokushige
[Comb.'99].
</p>
<p>It is known due to a result of Montanaro and Osborne [arXiv'09] that the
deterministic one-way communication complexity of $f \circ XOR_2$ equals the
non-adaptive parity decision tree complexity of $f$. In contrast, we show the
following with the gadget $AND_2$.
</p>
<p>1) There exists a function for which even the randomized non-adaptive AND
decision tree complexity of $f$ is exponentially large in the deterministic
one-way communication complexity of $f \circ AND_2$.
</p>
<p>2) For symmetric functions $f$, the non-adaptive AND decision tree complexity
of $f$ is at most quadratic in the (even two-way) communication complexity of
$f \circ AND_2$.
</p>
<p>In view of the first bullet, a lower bound on non-adaptive AND decision tree
complexity of $f$ does not lift to a lower bound on one-way communication
complexity of $f \circ AND_2$. The proof of the first bullet above uses the
well-studied Odd-Max-Bit function.
</p></div>
    </summary>
    <updated>2021-05-06T22:37:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01961</id>
    <link href="http://arxiv.org/abs/2105.01961" rel="alternate" type="text/html"/>
    <title>Stitch Fix for Mapper and Information Gains</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Youjia.html">Youjia Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saul:Nathaniel.html">Nathaniel Saul</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Safarli:Ilkin.html">Ilkin Safarli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krishnamoorthy:Bala.html">Bala Krishnamoorthy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Bei.html">Bei Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01961">PDF</a><br/><b>Abstract: </b>The mapper construction is a powerful tool from topological data analysis
that is designed for the analysis and visualization of multivariate data. In
this paper, we investigate a method for stitching a pair of univariate mappers
together into a bivariate mapper and study topological notions of information
gains during such a process. We further provide implementations that visualize
such information gains.
</p></div>
    </summary>
    <updated>2021-05-06T23:12:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01864</id>
    <link href="http://arxiv.org/abs/2105.01864" rel="alternate" type="text/html"/>
    <title>Tree Path Minimum Query Oracle via Boruvka Trees</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Tianqi.html">Tianqi Yang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01864">PDF</a><br/><b>Abstract: </b>Tree path minimum query problem is a fundamental problem while processing
trees, and is used widely in minimum spanning tree verification and randomized
minimum spanning tree algorithms. In this paper, we study the possibility of
building an oracle in advance, which is able to answer the queries efficiently.
We present an algorithm based on Boruvka trees. Our algorithm is the first to
achieve a near-optimal bound on query time, while matching the currently
optimal trade-off between construction time and the number of comparisons
required at query. Particularly, in order to answer each query within $2k$
comparisons, our algorithm requires $O(n \log \lambda_k(n))$ time and space to
construct the oracle, and the oracle can answer queries in $O(k + \log
\lambda_k(n))$ time. Here $\lambda_k(n)$ is the inverse of the Ackermann
function along the $k$-th column. This algorithm not only is simpler than the
previous ones, but also gives a completely different method of solving this
problem.
</p></div>
    </summary>
    <updated>2021-05-06T22:45:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01856</id>
    <link href="http://arxiv.org/abs/2105.01856" rel="alternate" type="text/html"/>
    <title>Identity testing under label mismatch</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Canonne:Cl=eacute=ment_L=.html">Clément L. Canonne</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wimmer:Karl.html">Karl Wimmer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01856">PDF</a><br/><b>Abstract: </b>Testing whether the observed data conforms to a purported model (probability
distribution) is a basic and fundamental statistical task, and one that is by
now well understood. However, the standard formulation, identity testing, fails
to capture many settings of interest; in this work, we focus on one such
natural setting, identity testing under promise of permutation. In this
setting, the unknown distribution is assumed to be equal to the purported one,
up to a relabeling (permutation) of the model: however, due to a systematic
error in the reporting of the data, this relabeling may not be the identity.
The goal is then to test identity under this assumption: equivalently, whether
this systematic labeling error led to a data distribution statistically far
from the reference model.
</p></div>
    </summary>
    <updated>2021-05-06T22:46:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01833</id>
    <link href="http://arxiv.org/abs/2105.01833" rel="alternate" type="text/html"/>
    <title>The Complexity of Symmetry Breaking in Massive Graphs</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Konrad:Christian.html">Christian Konrad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pemmaraju:Sriram_V=.html">Sriram V. Pemmaraju</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Riaz:Talal.html">Talal Riaz</a>, Peter Robinson <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01833">PDF</a><br/><b>Abstract: </b>The goal of this paper is to understand the complexity of symmetry breaking
problems, specifically maximal independent set (MIS) and the closely related
$\beta$-ruling set problem, in two computational models suited for large-scale
graph processing, namely the $k$-machine model and the graph streaming model.
We present a number of results. For MIS in the $k$-machine model, we improve
the $\tilde{O}(m/k^2 + \Delta/k)$-round upper bound of Klauck et al. (SODA
2015) by presenting an $\tilde{O}(m/k^2)$-round algorithm. We also present an
$\tilde{\Omega}(n/k^2)$ round lower bound for MIS, the first lower bound for a
symmetry breaking problem in the $k$-machine model. For $\beta$-ruling sets, we
use hierarchical sampling to obtain more efficient algorithms in the
$k$-machine model and also in the graph streaming model. More specifically, we
obtain a $k$-machine algorithm that runs in $\tilde{O}(\beta
n\Delta^{1/\beta}/k^2)$ rounds and, by using a similar hierarchical sampling
technique, we obtain one-pass algorithms for both insertion-only and
insertion-deletion streams that use $O(\beta \cdot n^{1+1/2^{\beta-1}})$ space.
The latter result establishes a clear separation between MIS, which is known to
require $\Omega(n^2)$ space (Cormode et al., ICALP 2019), and $\beta$-ruling
sets, even for $\beta = 2$. Finally, we present an even faster 2-ruling set
algorithm in the $k$-machine model, one that runs in
$\tilde{O}(n/k^{2-\epsilon} + k^{1-\epsilon})$ rounds for any $\epsilon$, $0
\le \epsilon \le 1$.
</p></div>
    </summary>
    <updated>2021-05-06T23:04:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01818</id>
    <link href="http://arxiv.org/abs/2105.01818" rel="alternate" type="text/html"/>
    <title>Dynamic Enumeration of Similarity Joins</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Agarwal:Pankaj_K=.html">Pankaj K. Agarwal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hu:Xiao.html">Xiao Hu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sintos:Stavros.html">Stavros Sintos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Jun.html">Jun Yang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01818">PDF</a><br/><b>Abstract: </b>This paper considers enumerating answers to similarity-join queries under
dynamic updates: Given two sets of $n$ points $A,B$ in $\mathbb{R}^d$, a metric
$\phi(\cdot)$, and a distance threshold $r &gt; 0$, report all pairs of points
$(a, b) \in A \times B$ with $\phi(a,b) \le r$. Our goal is to store $A,B$ into
a dynamic data structure that, whenever asked, can enumerate all result pairs
with worst-case delay guarantee, i.e., the time between enumerating two
consecutive pairs is bounded. Furthermore, the data structure can be
efficiently updated when a point is inserted into or deleted from $A$ or $B$.
</p>
<p>We propose several efficient data structures for answering similarity-join
queries in low dimension. For exact enumeration of similarity join, we present
near-linear-size data structures for $\ell_1, \ell_\infty$ metrics with
$\log^{O(1)} n$ update time and delay. We show that such a data structure is
not feasible for the $\ell_2$ metric for $d \ge 4$. For approximate enumeration
of similarity join, where the distance threshold is a soft constraint, we
obtain a unified linear-size data structure for $\ell_p$ metric, with
$\log^{O(1)} n$ delay and update time. In high dimensions, we present an
efficient data structure with worst-case delay-guarantee using locality
sensitive hashing (LSH).
</p></div>
    </summary>
    <updated>2021-05-06T22:48:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01785</id>
    <link href="http://arxiv.org/abs/2105.01785" rel="alternate" type="text/html"/>
    <title>An Optimal Algorithm for Triangle Counting</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jayaram:Rajesh.html">Rajesh Jayaram</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kallaugher:John.html">John Kallaugher</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01785">PDF</a><br/><b>Abstract: </b>We present a new algorithm for approximating the number of triangles in a
graph $G$ whose edges arrive as an arbitrary order stream. If $m$ is the number
of edges in $G$, $T$ the number of triangles, $\Delta_E$ the maximum number of
triangles which share a single edge, and $\Delta_V$ the maximum number of
triangles which share a single vertex, then our algorithm requires space: \[
\widetilde{O}\left(\frac{m}{T}\cdot \left(\Delta_E +
\sqrt{\Delta_V}\right)\right) \] Taken with the $\Omega\left(\frac{m
\Delta_E}{T}\right)$ lower bound of Braverman, Ostrovsky, and Vilenchik (ICALP
2013), and the $\Omega\left( \frac{m \sqrt{\Delta_V}}{T}\right)$ lower bound of
Kallaugher and Price (SODA 2017), our algorithm is optimal up to log factors,
resolving the complexity of a classic problem in graph streaming.
</p></div>
    </summary>
    <updated>2021-05-06T23:06:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01784</id>
    <link href="http://arxiv.org/abs/2105.01784" rel="alternate" type="text/html"/>
    <title>Sampling Colorings and Independent Sets of Random Regular Bipartite Graphs in the Non-Uniqueness Region</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Zongchen.html">Zongchen Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Galanis:Andreas.html">Andreas Galanis</a>, Daniel Štefankovič, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vigoda:Eric.html">Eric Vigoda</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01784">PDF</a><br/><b>Abstract: </b>For spin systems, such as the $q$-colorings and independent-set models,
approximating the partition function in the so-called non-uniqueness region,
where the model exhibits long-range correlations, is typically computationally
hard for bounded-degree graphs. We present new algorithmic results for
approximating the partition function and sampling from the Gibbs distribution
for spin systems in the non-uniqueness region on random regular bipartite
graphs. We give an $\mathsf{FPRAS}$ for counting $q$-colorings for even
$q=O\big(\tfrac{\Delta}{\log{\Delta}}\big)$ on almost every $\Delta$-regular
bipartite graph. This is within a factor $O(\log{\Delta})$ of the sampling
algorithm for general graphs in the uniqueness region and improves
significantly upon the previous best bound of
$q=O\big(\tfrac{\sqrt{\Delta}}{(\log\Delta)^2}\big)$ by Jenssen, Keevash, and
Perkins (SODA'19). Analogously, for the hard-core model on independent sets
weighted by $\lambda&gt;0$, we present an $\mathsf{FPRAS}$ for estimating the
partition function when $\lambda=\Omega\big(\tfrac{\log{\Delta}}{\Delta}\big)$,
which improves upon previous results by an $\Omega(\log \Delta)$ factor. Our
results for the colorings and hard-core models follow from a general result
that applies to arbitrary spin systems. Our main contribution is to show how to
elevate probabilistic/analytic bounds on the marginal probabilities for the
typical structure of phases on random bipartite regular graphs into efficient
algorithms, using the polymer method. We further show evidence that our result
for colorings is within a constant factor of best possible using current
polymer-method approaches.
</p></div>
    </summary>
    <updated>2021-05-06T23:06:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01782</id>
    <link href="http://arxiv.org/abs/2105.01782" rel="alternate" type="text/html"/>
    <title>Streaming approximation resistance of every ordering CSP</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singer:Noah.html">Noah Singer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sudan:Madhu.html">Madhu Sudan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Velusamy:Santhoshini.html">Santhoshini Velusamy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01782">PDF</a><br/><b>Abstract: </b>An ordering constraint satisfaction problem (OCSP) is given by a positive
integer $k$ and a constraint predicate $\Pi$ mapping permutations on
$\{1,\ldots,k\}$ to $\{0,1\}$. Given an instance of OCSP$(\Pi)$ on $n$
variables and $m$ constraints, the goal is to find an ordering of the $n$
variables that maximizes the number of constraints that are satisfied, where a
constraint specifies a sequence of $k$ distinct variables and the constraint is
satisfied by an ordering on the $n$ variables if the ordering induced on the
$k$ variables in the constraint satisfies $\Pi$. Natural ordering constraint
satisfaction problems include "Maximum acyclic subgraph (MAS)" and
"Betweenness". In this work we consider the task of approximating the maximum
number of satisfiable constraints in the (single-pass) streaming setting, where
an instance is presented as a stream of constraints.
</p>
<p>We show that for every $\Pi$, algorithms using $o(\sqrt{n})$ space cannot
distinguish streams where almost every constraint is satisfiable from streams
where no ordering beats the random ordering by a noticeable amount. In the case
of MAS our result shows that for every $\epsilon&gt;0$, MAS is not
$1/2+\epsilon$-approximable. The previous best inapproximability result only
ruled out a $3/4$ approximation.
</p>
<p>Our results build on a recent work of Chou, Golovnev, Sudan, and Velusamy who
show tight inapproximability results for some constraint satisfaction problems
over arbitrary (finite) alphabets. We show that the hard instances from this
earlier work have the property that in every partition of the hypergraph formed
by the constraints into small blocks, most of the hyperedges are incident on
vertices from distinct blocks. By exploiting this combinatorial property, in
combination with a natural reduction from CSPs over large finite alphabets to
OCSPs, we give optimal inapproximability results for all OCSPs.
</p></div>
    </summary>
    <updated>2021-05-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01780</id>
    <link href="http://arxiv.org/abs/2105.01780" rel="alternate" type="text/html"/>
    <title>Approximation schemes for bounded distance problems on fractionally treewidth-fragile graphs</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Zdeněk Dvořák, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lahiri:Abhiruk.html">Abhiruk Lahiri</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01780">PDF</a><br/><b>Abstract: </b>We give polynomial-time approximation schemes for monotone maximization
problems expressible in terms of distances (up to a fixed upper bound) and
efficiently solvable in graphs of bounded treewidth. These schemes apply in all
fractionally treewidth-fragile graph classes, a property that is true for many
natural graph classes with sublinear separators. We also provide
quasipolynomial-time approximation schemes for these problems in all classes
with sublinear separators.
</p></div>
    </summary>
    <updated>2021-05-06T23:08:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01778</id>
    <link href="http://arxiv.org/abs/2105.01778" rel="alternate" type="text/html"/>
    <title>Thinking Inside the Ball: Near-Optimal Minimization of the Maximal Loss</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carmon:Yair.html">Yair Carmon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jambulapati:Arun.html">Arun Jambulapati</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Yujia.html">Yujia Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sidford:Aaron.html">Aaron Sidford</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01778">PDF</a><br/><b>Abstract: </b>We characterize the complexity of minimizing $\max_{i\in[N]} f_i(x)$ for
convex, Lipschitz functions $f_1,\ldots, f_N$. For non-smooth functions,
existing methods require $O(N\epsilon^{-2})$ queries to a first-order oracle to
compute an $\epsilon$-suboptimal point and $\tilde{O}(N\epsilon^{-1})$ queries
if the $f_i$ are $O(1/\epsilon)$-smooth. We develop methods with improved
complexity bounds of $\tilde{O}(N\epsilon^{-2/3} + \epsilon^{-8/3})$ in the
non-smooth case and $\tilde{O}(N\epsilon^{-2/3} + \sqrt{N}\epsilon^{-1})$ in
the $O(1/\epsilon)$-smooth case. Our methods consist of a recently proposed
ball optimization oracle acceleration algorithm (which we refine) and a careful
implementation of said oracle for the softmax function. We also prove an oracle
complexity lower bound scaling as $\Omega(N\epsilon^{-2/3})$, showing that our
dependence on $N$ is optimal up to polylogarithmic factors.
</p></div>
    </summary>
    <updated>2021-05-06T23:07:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01751</id>
    <link href="http://arxiv.org/abs/2105.01751" rel="alternate" type="text/html"/>
    <title>Reconstruction Algorithms for Low-Rank Tensors and Depth-3 Multilinear Circuits</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhargava:Vishwas.html">Vishwas Bhargava</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saraf:Shubhangi.html">Shubhangi Saraf</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Volkovich:Ilya.html">Ilya Volkovich</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01751">PDF</a><br/><b>Abstract: </b>We give new and efficient black-box reconstruction algorithms for some
classes of depth-$3$ arithmetic circuits. As a consequence, we obtain the first
efficient algorithm for computing the tensor rank and for finding the optimal
tensor decomposition as a sum of rank-one tensors when then input is a
constant-rank tensor. More specifically, we provide efficient learning
algorithms that run in randomized polynomial time over general fields and in
deterministic polynomial time over the reals and the complex numbers for the
following classes:
</p>
<p>(1) Set-multilinear depth-$3$ circuits of constant top fan-in
$\Sigma\Pi\Sigma\{\sqcup_j X_j\}(k)$ circuits). As a consequence of our
algorithm, we obtain the first polynomial time algorithm for tensor rank
computation and optimal tensor decomposition of constant-rank tensors. This
result holds for $d$ dimensional tensors for any $d$, but is interesting even
for $d=3$.
</p>
<p>(2) Sums of powers of constantly many linear forms ($\Sigma\wedge\Sigma$
circuits). As a consequence we obtain the first polynomial-time algorithm for
tensor rank computation and optimal tensor decomposition of constant-rank
symmetric tensors.
</p>
<p>(3) Multilinear depth-3 circuits of constant top fan-in (multilinear
$\Sigma\Pi\Sigma(k)$ circuits). Our algorithm works over all fields of
characteristic 0 or large enough characteristic. Prior to our work the only
efficient algorithms known were over polynomially-sized finite fields (see.
Karnin-Shpilka 09').
</p>
<p>Prior to our work, the only polynomial-time or even subexponential-time
algorithms known (deterministic or randomized) for subclasses of
$\Sigma\Pi\Sigma(k)$ circuits that also work over large/infinite fields were
for the setting when the top fan-in $k$ is at most $2$ (see Sinha 16' and Sinha
20').
</p></div>
    </summary>
    <updated>2021-05-06T22:38:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01738</id>
    <link href="http://arxiv.org/abs/2105.01738" rel="alternate" type="text/html"/>
    <title>Priority Promotion with Parysian Flair</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Benerecetti:Massimo.html">Massimo Benerecetti</a>, Daniele Dell'Erba, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mogavero:Fabio.html">Fabio Mogavero</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schewe:Sven.html">Sven Schewe</a>, Dominik Wojtczak Università di Napoli Federico II, University of Liverpool) <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01738">PDF</a><br/><b>Abstract: </b>We develop an algorithm that combines the advantages of priority promotion -
the leading approach to solving large parity games in practice - with the
quasi-polynomial time guarantees offered by Parys' algorithm. Hybridising these
algorithms sounds both natural and difficult, as they both generalise the
classic recursive algorithm in different ways that appear to be irreconcilable:
while the promotion transcends the call structure, the guarantees change on
each level. We show that an interface that respects both is not only effective,
but also efficient.
</p></div>
    </summary>
    <updated>2021-05-06T22:42:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01699</id>
    <link href="http://arxiv.org/abs/2105.01699" rel="alternate" type="text/html"/>
    <title>Determining 4-edge-connected components in linear time</title>
    <feedworld_mtime>1620259200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nadara:Wojciech.html">Wojciech Nadara</a>, Mateusz Radecki, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smulewicz:Marcin.html">Marcin Smulewicz</a>, Marek Sokołowski <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01699">PDF</a><br/><b>Abstract: </b>In this work, we present the first linear time deterministic algorithm
computing the 4-edge-connected components of an undirected graph. First, we
show an algorithm listing all 3-edge-cuts in a given 3-edge-connected graph,
and then we use the output of this algorithm in order to determine the
4-edge-connected components of the graph.
</p></div>
    </summary>
    <updated>2021-05-06T22:47:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/066</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/066" rel="alternate" type="text/html"/>
    <title>TR21-066 |  Dimension-free Bounds and Structural Results in Communication Complexity | 

	Lianna Hambardzumyan, 

	Hamed Hatami, 

	Pooya Hatami</title>
    <summary>The purpose of this article is to initiate a systematic study of dimension-free relations between basic communication and query complexity measures  and various  matrix norms.  In other words, our goal is to obtain    inequalities that bound a parameter   solely as a function of another parameter. This is in contrast to perhaps the more common framework in communication complexity  where  poly-logarithmic dependencies on the number of input bits are   tolerated. 


Dimension-free bounds are also closely related to structural results, where one seeks to describe the structure of Boolean matrices and functions that have low complexity.  We prove such  theorems for several communication and query complexity measures as well as various matrix and operator norms. In several other cases we show that such bounds do not exist. 


We propose several conjectures, and establish that, in addition to applications in complexity theory, these   problems are central to  characterization of the idempotents of the algebra of Schur multipliers, and could lead to new extensions of  Cohen's celebrated idempotent theorem regarding the Fourier algebra.</summary>
    <updated>2021-05-05T18:59:23Z</updated>
    <published>2021-05-05T18:59:23Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-07T16:37:33Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/tpdp21-cfp/</id>
    <link href="https://differentialprivacy.org/tpdp21-cfp/" rel="alternate" type="text/html"/>
    <title>Call for Papers - Workshop on the Theory and Practice of Differential Privacy (TPDP 2021)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Work on differential privacy spans a number of different research communities, including theoretical computer science, machine learning, statistics, security, law, databases, cryptography, programming languages, social sciences, and more.
Each of these communities may choose to publish their work in their own community’s venues, which could result in small groups of differential privacy researchers becoming isolated.
To alleviate these issues, we have the Workshop on the <a href="https://tpdp.journalprivacyconfidentiality.org/">Theory and Practice of Differential Privacy</a> (TPDP), which is intended to bring these subcommunities together under one roof (well, a virtual one at least for 2020 and 2021).</p>

<p>We have just posted the <a href="https://tpdp.journalprivacyconfidentiality.org/2021/TPDP2021CfP.pdf">Call for Papers</a> for <a href="https://tpdp.journalprivacyconfidentiality.org/2021/">TPDP 2021</a>, which will be a workshop affiliated with <a href="https://icml.cc/Conferences/2021/">ICML 2021</a>.
The submission deadline is Friday, May 28, 2021, Anywhere on Earth (conveniently, two days after the deadline for NeurIPS 2021).
Submissions are extended abstracts of up to four pages in length, and will undergo a lightweight review process, based mostly on relevance and interest to the differential privacy community.
The workshop is non-archival, so feel free to submit recent work at any stage of publication.
Submissions will be on <a href="https://openreview.net/group?id=ICML.cc/2021/Workshop/TPDP">OpenReview</a>, but since submitted work may be preliminary, the process will be “closed” similar to traditional review processes.
One goal of the workshop is to be inclusive and welcoming to newcomers to the differential privacy community, so please consider participating even if you are new to the field.</p>

<p>Most papers will be presented as posters at a (virtual) poster session, while a few papers will be selected for spotlight talks.
There will also be plenary talks by <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a> (Hebrew University of Jerusalem) and <a href="https://www2.math.upenn.edu/~ryrogers/">Ryan Rogers</a> (LinkedIn).
The program co-chairs are <a href="https://sites.gatech.edu/rachel-cummings/">Rachel Cummings</a> and <a href="http://www.gautamkamath.com/">myself</a>.
Please submit your best work on differential privacy, and hope to see you there!</p>
<p align="center">
  <img src="https://differentialprivacy.org/images/Ligett.png"/>
  <img src="https://differentialprivacy.org/images/Rogers.png"/> <br/>
    <i>Invited speakers Katrina Ligett and Ryan Rogers</i>
</p></div>
    </summary>
    <updated>2021-05-05T14:00:00Z</updated>
    <published>2021-05-05T14:00:00Z</published>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-05-06T23:15:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18701</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/05/05/how-to-teach-math/" rel="alternate" type="text/html"/>
    <title>How to Teach Math?</title>
    <summary>The only way to learn mathematics is to do mathematics—Paul Halmos Angie Hodge is an Associate Professor of mathematics at Northern Arizona University. Her interests as stated on her website are focused mainly on education, mentoring, and equity in the STEM disciplines. Today I thought we would discuss the issue of teaching and learning math […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>The only way to learn mathematics is to do mathematics—Paul Halmos</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/05/how-to-teach-math/an/" rel="attachment wp-att-18704"><img alt="" class="alignright size-thumbnail wp-image-18704" height="150" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/an-150x150.jpg?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
Angie Hodge is an Associate Professor of mathematics at Northern Arizona University. Her interests as stated on her <a href="https://directory.nau.edu/person/amh952">website</a> are focused mainly on education, mentoring, and equity in the STEM disciplines. </p>
<p>
Today I thought we would discuss the issue of teaching and learning math and complexity theory.</p>
<p>
I am now retired from Georgia Tech. But I continue to be interested in how we can teach math. Dually how we can learn math. I still try to learn math—not for formal classes, nor in formal classes—but to help advance my own research. Even now I find that I need to learn some topics to help write a post or to solve a problem. </p>
<p>
On this blog with Ken I am also interested in still helping others learn. Some call that teaching. This is therefore the topic for today.</p>
<p>
</p><p/><h2> First An Issue </h2><p/>
<p/><p>
One thing that drives me nuts about learning new math is what I will call the </p>
<blockquote><p><b> </b> <em> <i>“It is too obvious to state” principle.</i> </em>
</p></blockquote>
<p>What I mean is that when you start to learn material from some corner of math you get the key definitions and the main results. What I do not always get is some totally obvious ideas. Does this make any sense?</p>
<p>
Here is an example. Consider the class of sequences that are defined by linear <a href="https://en.wikipedia.org/wiki/Recurrence_relation">recurrences</a>. That is: a recursion that defines a sequence as a linear combination of earlier terms. One of the most famous is the Fibonacci numbers: 	</p>
<p align="center"><img alt="\displaystyle  F_n = F_{n-1} + F_{n-2}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F_n+%3D+F_%7Bn-1%7D+%2B+F_%7Bn-2%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>And <img alt="{F_0=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF_0%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{F_1=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF_1%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
The property we are interested in is: </p>
<blockquote><p><b> </b> <em> <i>Is a product of two such sequences also of the same form?</i> </em>
</p></blockquote>
<p>This is a basic question, but it is not trivial to find the <a href="https://reader.elsevier.com/reader/sd/pii/0012365X79901869?token=C0E36B1EE6735F9AB6140E86924EB1A19F236C9507028E02CB912D21952F2438487C78DDBB68CC1541D2E9CF5754B8C1&amp;originRegion=us-east-1&amp;originCreation=20210415000540">answer</a>. </p>
<p>
</p><p/><h2> How To Teach? </h2><p/>
<p/><p>
This is an ancient question that we all probably have thought about from time to time. It is complicated by the recent issue of most classes being done via online. Hodge has some nice stuff on teaching, especially on Inquiry-Based Learning (IBL). </p>
<ul>
<li>
See her <a href="https://www.artofmathematics.org/blogs/cvonrenesse/guest-blog-by-angie-hodge">blog</a>. <p/>
</li><li>
See <a href="http://math.uchicago.edu/~boller/IBL/">this</a> for her comments on <em>Inquiry-Based Learning</em>. <p/>
</li><li>
See her thoughts on <a href="https://www.maa.org/sites/default/files/Programs/MathFest2018_IPS_Hodge2.pdf">IBL</a>.
</li></ul>
<p>
</p><p/><h2> Ken’s Similar Question </h2><p/>
<p/><p>
Ken writes: I am interested in manipulating logistic curves. Such curves not only undergird the chess rating system and the theory of standardized tests, they relate directly to the design of chess programs which I use to take my data. This decade’s neural chess-playing programs, following on from AlphaZero, express values directly in terms of the likelihood of winning, as numbers between 0 and 1, rather than traditional evaluation schemes built on counting 1.00 for a pawn, 3–3.5 for a knight or bishop, and so on. </p>
<p>
The new likelihood numbers follow a logistic curve. I especially want to convert from the evaluation numbers to them. After doing this conversion for various major chess programs, I would like to average their values as input to my predictive model. This involves taking averages of logistic curves. The curves can be generalized to the <a href="https://en.wikipedia.org/wiki/Generalised_logistic_function">form</a> named for Francis Richards: </p>
<p align="center"><img alt="\displaystyle  f(x) = A + \frac{K-A}{(C + Qe^{-Bx})^{1/\nu}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+A+%2B+%5Cfrac%7BK-A%7D%7B%28C+%2B+Qe%5E%7B-Bx%7D%29%5E%7B1%2F%5Cnu%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>for constant parameters <img alt="{A,K,B,C,Q,\nu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CK%2CB%2CC%2CQ%2C%5Cnu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The standard family has <img alt="{A=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{K=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{C=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{\nu=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnu%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{Q=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, with <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> the central parameter determining the slope of the curve <img alt="{f(t) = \frac{1}{1+e^{-Bx}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28t%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-Bx%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> at <img alt="{x=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We can ask the basic question about intermediate families between the standard family and the most general kind:</p>
<blockquote><p><b> </b> <em> When does a linear combination of logistic curves belong to the same family? And if it doesn’t belong, how close to a member of the family does it come? </em>
</p></blockquote>
<p/><p>
My point is that I have not been able to find an easy source for answers. This strikes me as exactly the kind of question for which sites like MathOverflow and StackExchange exist. But it is also a nice instructional exercise for training students in both the grit and adventure of mathematical research. </p>
<p>
One can also pose literally the same as Dick’s question above: how about a product of two curves <img alt="{f_1(x),f_2(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%28x%29%2Cf_2%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from the family? The product still has values that run from <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> goes from <img alt="{-\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to <img alt="{+\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. If we want the curves all to have value <img alt="{f(0) = 0.5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%280%29+%3D+0.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then we can compose the product with a square root, viz. <img alt="{f(x) = \sqrt{f_1(x) f_2(x)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%3D+%5Csqrt%7Bf_1%28x%29+f_2%28x%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, thus taking a geometric rather than arithmetic mean. I mentioned other nuts-and-bolts problems about logistic curves in this <a href="https://rjlipton.wpcomstaging.com/2018/08/25/do-you-want-to-know-a-secret/">post</a> and its longer <a href="https://rjlipton.wpcomstaging.com/2018/09/07/sliding-scale-problems/">followup</a>.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
The following video shows how not to teach math: <a href="https://www.youtube.com/watch?v=vU5LoCLGMdQ&amp;t=109s">The Kettles</a> do math.</p>
<p/><p><br/>
[some format and word tweaks]</p></font></font></div>
    </content>
    <updated>2021-05-05T13:56:56Z</updated>
    <published>2021-05-05T13:56:56Z</published>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Angie Hodge"/>
    <category term="Fibonacci sequence"/>
    <category term="IBL"/>
    <category term="learn"/>
    <category term="logistic curves"/>
    <category term="teach"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-05-07T16:37:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/065</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/065" rel="alternate" type="text/html"/>
    <title>TR21-065 |  One-way communication complexity and non-adaptive decision trees | 

	Nikhil Mande, 

	Swagato Sanyal</title>
    <summary>We study the relationship between various one-way communication complexity measures of a composed function with the analogous decision tree complexity of the outer function. We consider two gadgets: the AND function on 2 inputs, and the Inner Product on a constant number of inputs. Let $IP$ denote Inner Product on $2b$ bits.

1) If $f$ is a total Boolean function that depends on all of its inputs, the bounded-error one-way quantum communication complexity of $f \circ IP$ equals $\Omega(n(b-1))$.

2) If $f$ is a partial Boolean function, the deterministic one-way communication complexity of $f \circ IP$ is at least $\Omega(b \cdot D_{dt}^{\rightarrow}(f))$, where $D_{dt}^{\rightarrow}(f)$ denotes the non-adaptive decision tree complexity of $f$.

For our quantum lower bound, we show a lower bound on the VC-dimension of $f \circ IP$, and then appeal to a result of Klauck [STOC'00].
Our deterministic lower bound relies on a combinatorial result due to Frankl and Tokushige [Comb.'99].

It is known due to a result of Montanaro and Osborne [arXiv'09] that the deterministic one-way communication complexity of $f \circ XOR_2$ equals the non-adaptive parity decision tree complexity of $f$.
In contrast, we show the following with the gadget $AND_2$.

1) There exists a function for which even the randomized non-adaptive AND decision tree complexity of $f$ is exponentially large in the deterministic one-way communication complexity of $f \circ AND_2$.

2) For symmetric functions $f$, the non-adaptive AND decision tree complexity of $f$ is at most quadratic in the (even two-way) communication complexity of $f \circ AND_2$.

In view of the first point, a lower bound on non-adaptive AND decision tree complexity of $f$ does not lift to a lower bound on one-way communication complexity of $f \circ AND_2$.
The proof of the first point above uses the well-studied Odd-Max-Bit function.
For the second bullet, we first observe a connection between the one-way communication complexity of $f$ and the M\"obius sparsity of $f$, and then use a known lower bound on the M\"obius sparsity of symmetric functions. An upper bound on the non-adaptive AND decision tree complexity of symmetric functions follows implicitly from prior work on combinatorial group testing; for the sake of completeness, we include a proof of this result.</summary>
    <updated>2021-05-05T13:16:19Z</updated>
    <published>2021-05-05T13:16:19Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-07T16:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3524</id>
    <link href="https://agtb.wordpress.com/2021/05/05/netecon-2021/" rel="alternate" type="text/html"/>
    <title>NetEcon 2021</title>
    <summary>NetEcon’21, the 16th Workshop on the Economics of Networks, Systems and Computation, will take place on July 23, 2021. NetEcon’21 is a workshop of EC’21, the 22nd ACM Conference on Economics and Computation, which will be held on July 19-23, 2021, co-located with the 6th World Congress of the Game Theory Society. The aim of NetEcon […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>NetEcon’21,</strong> the 16th Workshop on the Economics of Networks, Systems and Computation, will take place on July 23, 2021. NetEcon’21 is a workshop of EC’21, the 22nd ACM Conference on Economics and Computation, which will be held on July 19-23, 2021, co-located with the 6th World Congress of the Game Theory Society. The aim of NetEcon is to foster discussions on the application of economic and game-theoretic models and principles to address challenges in the development of networks and network-based applications and services.</p>



<p>Details regarding submission rules and dates can be found at <a href="https://netecon21.gametheory.online/" rel="noreferrer noopener" target="_blank">https://netecon21.gametheory.online/</a>. A novelty compared with prior editions of the workshop is that papers that were already formatted for and submitted to EC’21 or SIGMETRICS’21 may retain this format (for submission) if submitted together with all the reviews (see submission guidelines for details).</p></div>
    </content>
    <updated>2021-05-05T01:12:21Z</updated>
    <published>2021-05-05T01:12:21Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Kevin Leyton-Brown</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2021-05-07T16:37:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/064</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/064" rel="alternate" type="text/html"/>
    <title>TR21-064 |  Streaming approximation resistance of every ordering CSP | 

	Santhoshini Velusamy, 

	Noah Singer, 

	Madhu Sudan</title>
    <summary>An ordering constraint satisfaction problem (OCSP) is given by a positive integer $k$ and a constraint predicate $\Pi$ mapping permutations on $\{1,\ldots,k\}$ to $\{0,1\}$. Given an instance of OCSP$(\Pi)$ on $n$ variables and $m$ constraints, the goal is to find an ordering of the $n$ variables that maximizes the number of constraints that are satisfied, where a constraint specifies a sequence of $k$ distinct variables and the constraint is satisfied by an ordering on the $n$ variables if the ordering induced on the $k$ variables in the constraint satisfies $\Pi$. Ordering constraint satisfaction problems capture natural problems including ''Maximum acyclic subgraph (MAS)'' and ''Betweenness''. 

In this work we consider the task of approximating the maximum number of satisfiable constraints in the (single-pass) streaming setting, where an instance is presented as a stream of constraints. We show that for every $\Pi$, OCSP$(\Pi)$ is approximation-resistant to $o(\sqrt{n})$-space streaming algorithms, i.e., algorithms using $o(\sqrt{n})$ space cannot distinguish streams where almost every constraint is satisfiable from streams where no ordering beats the random ordering by a noticeable amount. In the case of MAS our result shows that for every $\epsilon&gt;0$, MAS is not $1/2+\epsilon$-approximable. The previous best inapproximability result only ruled out a $3/4$ approximation.

Our results build on a recent work of Chou, Golovnev, Sudan, and Velusamy who show tight inapproximability results for some constraint satisfaction problems over arbitrary (finite) alphabets. We show that the hard instances from this earlier work have the following ''small-set expansion'' property: in every partition of the hypergraph formed by the constraints into small blocks, most of the hyperedges are incident on vertices from distinct blocks. By exploiting this combinatorial property, in combination with a natural reduction from CSPs over large finite alphabets to OCSPs, we give optimal inapproximability results for all OCSPs.</summary>
    <updated>2021-05-04T21:39:50Z</updated>
    <published>2021-05-04T21:39:50Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-07T16:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.let-all.com/blog/?p=55</id>
    <link href="https://www.let-all.com/blog/2021/05/04/alt-highlights-an-interview-with-the-pc-chairs-of-alt-2021/" rel="alternate" type="text/html"/>
    <title>ALT Highlights – An Interview with the PC Chairs of ALT 2021</title>
    <summary>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference ALT 2021, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by the <a href="https://www.let-all.com/">Learning Theory Alliance</a>, and overseen by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Learning Theory Alliance blog</a>.</p>



<p>This is the fourth post in the series, an interview with ALT 2021 PC Chairs <a href="http://vtaly.net/">Vitaly Feldman</a> and <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a>, written by <a href="https://www.comp.nus.edu.sg/~sutanu/">Sutanu Gayen</a> and <a href="https://sites.google.com/view/michal-moshkovitz">Michal Moshkovitz</a>.</p>



<hr class="wp-block-separator"/>



<p>We had the great opportunity to attend <em>The 32nd International Conference on Algorithmic Learning Theory</em>, held online between March 16-19, 2021, and co-chaired by Vitaly Feldman and Katrina Ligett. Vitaly is a research scientist at Apple AI Research and has done foundational works in machine learning and privacy-preserving data analysis. Katrina is an Associate Professor of Computer Science at the Hebrew University of Jerusalem and has done pivotal works in data privacy, algorithmic fairness, algorithmic game theory, and online algorithms. We asked for an interview with them about their experiences and perspectives as co-chairs, to which they kindly agreed. We are happy to share with the readers the excerpts of this interview.</p>



<p class="has-text-align-center"><img src="https://lh4.googleusercontent.com/7RUOfC-v06amphwIhfCYsQNH4jUg82AVsePZcbWO0D40DhSRk-Cf_wnXx9y5RI-qVYz6D-IRAxRzsQ9lWBpraofuggrTYC_sG40GehOCvSrQyZfj1khlMTVqK23NKOZueGcbK_xa" style="width: 225px;"/>           <img height="252" src="https://lh4.googleusercontent.com/lUMmpb6Bcfq3bPljS7jYaF2TFaXRks64--IeslcdR3WzqnCtUETu-ymoOSxm7ys_6eyRFb2mGmd1mCe1boNHdxii0d1_UCthAEJy_eTsWsGQsFKpsV5snZe3Nm9g-QDy0JTnzPKx" width="194"/></p>



<p><strong>How it started</strong></p>



<p>How are chairs and program committees chosen? </p>



<p class="has-text-color" style="color: #0000ff;">Katrina: The chairs are selected by the Association for Algorithmic Learning Theory (AALT) Steering Committee (<a href="http://algorithmiclearningtheory.org/alt-steering-committee/">http://algorithmiclearningtheory.org/alt-steering-committee/</a>), and the chairs then select the program committee members. Vitaly and I brainstormed potential PC member names, solicited additional suggestions, and also considered the lists of people who have served on recent ALT and COLT PCs. In building the PC, we had many considerations in mind, including coverage of research areas, and various metrics of diversity.</p>



<p><strong>The chairs’ role</strong></p>



<p>What are the different tasks a chair has? What is the most difficult task?</p>



<p class="has-text-color" style="color: #0000ff;">Katrina: At a high level, the roles of the PC chairs are to build the PC, oversee the reviewing process and create the conference program. Practically, though, there are a lot of decisions that need to be discussed, emails to be sent, and a lot of organizational aspects to tend to—configuring the reviewing platform, sending reminders, chasing down late reviews, and so on. Vitaly and I have a very, very long joint “to do” list—and luckily, it’s now almost all crossed off! We also had additional responsibilities this year because of the move to the virtual conference format, including selecting the technologies, overseeing the pre-recording process, and much more.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I think the hardest and probably the most time-consuming is making the accept/reject decisions on papers.  For a large fraction of the papers arriving at the decision requires getting a sense of the results; understanding the main points in reviews, author responses and discussion (while calibrating them to the PC members and reviewers); ensuring that each paper is properly discussed by chasing reviewers, asking questions and often soliciting additional opinions. We also needed to come up with a set of criteria for deciding on borderline cases and make sure that these criteria are applied as consistently as possible. At the end it is a rather long and iterative process that luckily for us has converged to a program we are happy with.</p>



<p>How much time do you spend doing chair tasks? How do you balance chairing a conference (a massive amount of work) with all your other commitments? Do you turn down other service items you would generally accept, etc.?</p>



<p class="has-text-color" style="color: #0000ff;">Katrina: It’s difficult to estimate the number of hours, but I think we have been meeting regularly since early June 2020, and we’re only wrapping up our work now, in late March 2021. It’s a much longer-timeframe commitment than serving as a PC member. I actually am chairing a second conference this year, FORC, and together it makes for a pretty serious load. As a result, I have been declining all other conference-related service. I also have a couple of other pretty substantial service commitments, as well, so I just don’t have bandwidth this year for additional PC and Area Chair-type roles.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that it’s hard to tell how much time we spent in total. My rough estimate is that it’s about a month of full-time work. I also had to decline most other service commitments during that period some of which I’d normally accept. Naturally, it also slows down other work so I definitely had to lean more on my collaborators in some of the ongoing projects <img alt="&#x1F642;" class="wp-smiley" src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f642.png" style="height: 1em;"/></p>



<p>Can chairs bring their own personality into the conference? How? </p>



<p class="has-text-color" style="color: #0000ff;">Katrina: One area where the chairs enjoy freedom is in selecting the keynote and tutorial speakers. I’m biased, of course, but I think we chose very well, and all of these speakers (Joelle Pineau, Shay Moran, and Costis Daskalakis) gave excellent talks (they were recorded—check them out if you missed them)! We also were fortunate to be able to work with amazing partners who organized the mentoring workshop (Surbhi Goel, Nika Hagtalab, and Ellen Vitercik) and the Women in ML Theory event (Tosca Lechner and Ruth Urner). These aspects of the conference beyond the papers are a way for the chairs to express their priorities.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: The chairs have a lot of freedom in choosing how to run the review process, design the conference program and who else will be involved. So, inevitably, the chairs’ personalities and tastes end up being reflected in the final results. </p>



<p>Does the online conference impact the chair job? How? </p>



<p class="has-text-color" style="color: #0000ff;">Katrina: Typically, the PC chairs build the program, and then many details of organizing and running the conference get handed off to local organization chairs. But this year, since ALT was held virtually, there were many unusual tasks that fell to the PC chairs—not just the obvious ones like choosing the technologies and format and negotiating those contracts, but smaller things like chasing down authors who failed to upload their recordings, and developing instructions for people in various roles to interact with the conference platform.</p>



<p class="has-text-color" style="color: #0000ff;">In addition, COVID times placed strains on many people, which made it more challenging to recruit PC members, and resulted in a higher than usual rate of late reviews and PC drop-outs, which of course left us scrambling.</p>



<p>What motivates you to spend time on a conference service?</p>



<p class="has-text-color" style="color: #0000ff;">Katrina: We all rely on the conference system for our personal professional advancement, and for the advancement of our field as a whole. So we all owe that system our service, of course, according to our abilities, availability, and seniority. Also, it’s fun to get this different perspective on the conference review process and on the field. And it’s an honor to be entrusted with shepherding a conference for a year and hopefully nurturing its growth.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that it’s a mix of (1) contribution to the community I’m a member of and, perhaps, an opportunity to improve some of its processes (2) a learning experience that gives one a higher-level view of the research that is happening and people who do it (3) honor and recognition that come with the job.</p>



<p><strong>Awards </strong></p>



<p>How do you decide which papers were chosen as awards? </p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: A necessary condition for a paper to receive an award is that at least one of the PC members/reviewers assigned to the paper is excited about the results.  So we start by looking at papers with the highest scores (typically all papers that received at least one “strong accept”) and reading their reviews. This allowed us to narrow down the list to a set of 5-6 candidates. From those we selected the winners by learning more about the results and selecting those, we found the most significant and interesting for the community.</p>



<p><strong>The review process</strong></p>



<p>What are your thoughts about the current peer review process in ALT? What are the downsides and advantages? Do you have suggestions for improvement?</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: It is common that confidence in correctness of the results in a conference submission is based on higher-level sanity checks and general intuition of the reviewers. Naturally, the more interesting and important result the more likely it is to be scrutinized. At this ALT we did not run into a situation where the authors’ reputation affected our confidence in the correctness of the results. In case of concerns about correctness of an interesting result we would ask either an expert on the PC or an external expert to try to verify the result.</p>



<p class="has-luminous-vivid-orange-color has-text-color">ALT currently relies on a traditional theory conference model of reviewing and for a typical submission has several PC members who are experts in the subarea. The reviewing load is also relatively light (8 papers per PC member). So I think that the overall reviewing quality is pretty much as good as it gets in ML (and is similar to COLT). Naturally, the model is not perfect and there is still variation in the quality of individual reviews. This year many more reviewers and PC members were under unusual time pressure due to the pandemic so perhaps the variation was higher than usual.</p>



<p><strong>The future</strong></p>



<p>What are your suggestions for the next chair? </p>



<p class="has-text-color" style="color: #0000ff;">Katrina: Make sure you have a good co-chair. <img alt="&#x1F642;" class="wp-smiley" src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f642.png" style="height: 1em;"/> Vitaly has been a great partner for this process—fun to work with, reliable, always willing to pitch in even on the less-fun tasks, and I have great respect for his technical perspective.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that diversity of perspectives and expertise is useful in several ways. Most notably, it gives the chairs a wider network of people to select the PC from. But I completely agree with Katrina, that the most important thing is the ability of co-chairs to work well together: after all, it’s a lot of work and complicated decisions that need to be made jointly. Here, I couldn’t have asked for more: Katrina is amazing both professionally and personally. Working with her was definitely the highlight of being the ALT co-chair and learned a lot from her in the process as well.</p></div>
    </content>
    <updated>2021-05-04T16:40:08Z</updated>
    <published>2021-05-04T16:40:08Z</published>
    <category term="ALT Highlights"/>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://www.let-all.com/blog</id>
      <logo>https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/04/logo.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://www.let-all.com/blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://www.let-all.com/blog" rel="alternate" type="text/html"/>
      <title>The Learning Theory Alliance Blog</title>
      <updated>2021-05-07T16:39:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=559</id>
    <link href="https://tcsplus.wordpress.com/2021/05/04/tcs-talk-wednesday-may-12-santhoshini-velusamy-harvard-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 12 — Santhoshini Velusamy, Harvard University</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Santhoshini Velusamy from Harvard University will speak about “Classification of the approximability of all finite Max-CSPs in the dynamic streaming setting” (abstract below). You can reserve a spot […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://scholar.harvard.edu/santhoshiniv/home"><strong>Santhoshini Velusamy</strong></a> from Harvard University will speak about “<em>Classification of the approximability of all finite Max-CSPs in the dynamic streaming setting</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk)</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: A maximum constraint satisfaction problem, Max-CSP(F), is specified by a finite family of constraints F, where each constraint is of arity k. An instance of the problem on n variables is given by m applications of constraints from F to length-k subsequences of the n variables, and the goal is to find an assignment to the n variables that satisfies the maximum number of constraints. The class of Max-CSP(F) includes optimization problems such as Max-CUT, Max-DICUT, Max-3SAT, Max-q-Coloring, Unique Games, etc.</p>
<p>In this talk, I will present our recent dichotomy theorem on the approximability of Max-CSP(F) for every finite family F, in the single-pass dynamic streaming setting. In this setting, at each time step, a constraint is either added to or deleted from the stream. In the end, the streaming algorithm must estimate the maximum number of constraints that can be satisfied using space that is only polylogarithmic in n. No background in streaming algorithms or constraint satisfaction problems will be needed to enjoy this talk!</p>
<p>The talk will be based on <a href="https://eccc.weizmann.ac.il/report/2021/011/">this paper</a>, and <a href="https://eccc.weizmann.ac.il/report/2021/063/">this paper</a> with Chi-Ning Chou, Alexander Golovnev, and Madhu Sudan.</p></blockquote></div>
    </content>
    <updated>2021-05-04T06:48:59Z</updated>
    <published>2021-05-04T06:48:59Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-05-07T16:38:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/05/03/postdoc-at-uc-irvine-apply-by-june-4-2021/</id>
    <link href="https://cstheory-jobs.org/2021/05/03/postdoc-at-uc-irvine-apply-by-june-4-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at UC Irvine (apply by June 4, 2021)</title>
    <summary>One Post-doctoral position is available under the guidance of Ioannis Panageas. The appointment is for one year and may be renewable if funding permits. Requirement is a Ph.D. in TCS/Theory of ML, or related field. Expertise can be demonstrated by 3 top-tier publications in venues like ICML, NeurIPS, AISTATS, STOC, FOCS, SODA, ICALP, EC. Anticipated […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>One Post-doctoral position is available under the guidance of Ioannis Panageas. The appointment is for one year and may be renewable if funding permits. Requirement is a Ph.D. in TCS/Theory of ML, or related field. Expertise can be demonstrated by 3 top-tier publications in venues like ICML, NeurIPS, AISTATS, STOC, FOCS, SODA, ICALP, EC. Anticipated starting date is October 1 2021 (negotiable).</p>
<p>Website: <a href="https://recruit.ap.uci.edu/JPF06615">https://recruit.ap.uci.edu/JPF06615</a><br/>
Email: ipanagea@ics.uci.edu</p></div>
    </content>
    <updated>2021-05-03T22:59:14Z</updated>
    <published>2021-05-03T22:59:14Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-05-07T16:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/063</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/063" rel="alternate" type="text/html"/>
    <title>TR21-063 |  Approximability of all finite CSPs in the dynamic streaming setting | 

	Chi-Ning  Chou, 

	Alexander Golovnev, 

	Madhu Sudan, 

	Santhoshini Velusamy</title>
    <summary>A constraint satisfaction problem (CSP), Max-CSP$({\cal F})$, is specified by a finite set of constraints ${\cal F} \subseteq \{[q]^k \to \{0,1\}\}$ for positive integers $q$ and $k$. An instance of the problem on $n$ variables is given by $m$ applications of constraints from ${\cal F}$ to subsequences of the $n$ variables, and the goal is to find an assignment to the variables that satisfies the maximum number of constraints. In the $(\gamma,\beta)$-approximation version of the problem for parameters $0 \leq \beta &lt; \gamma \leq 1$, the goal is to distinguish instances where at least $\gamma$ fraction of the constraints can be satisfied from instances where at most $\beta$ fraction of the constraints can be satisfied. 

In this work we consider the approximability of this problem in the context of streaming algorithms and give a dichotomy result in the dynamic setting, where constraints can be inserted or deleted. Specifically,  for every family ${\cal F}$ and every $\beta &lt; \gamma$,  we show that either the approximation problem is solvable with polylogarithmic space in the dynamic setting, or not solvable with $o(\sqrt{n})$ space. We also establish tight inapproximability results for a broad subclass in the streaming insertion-only setting. Our work builds on, and significantly extends previous work by the authors who consider the special case of Boolean variables ($q=2$), singleton families ($|{\cal F}| = 1$) and where constraints may be placed on variables or their negations. Our framework extends non-trivially the previous work allowing us to appeal to richer norm estimation algorithms to get our algorithmic results. For our negative results we introduce new variants of the communication problems studied in the previous work, build new reductions for these problems, and extend the technical parts of previous works. In particular, previous works used Fourier analysis over the Boolean cube to prove their results and the results seemed particularly tailored to functions on Boolean literals (i.e., with negations). Our techniques surprisingly allow us to get to general $q$-ary CSPs without negations by appealing to the same Fourier analytic starting point over Boolean hypercubes.</summary>
    <updated>2021-05-03T20:10:43Z</updated>
    <published>2021-05-03T20:10:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-07T16:37:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7474459772601125760</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7474459772601125760/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/the-mythical-man-month-hen-day-and-cat.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7474459772601125760" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7474459772601125760" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/the-mythical-man-month-hen-day-and-cat.html" rel="alternate" type="text/html"/>
    <title>The Mythical Man-Month, Hen-Day, and Cat-Minute (Fred Brooks Turned 90)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><i> The Mythical Man-Month </i>is a great book which talks about the (obvious in retrospect) fact that putting more people on a project may slow it down. It was by Fred Brooks who turned 90 in April (he is still alive). It's a good read. I actually read it many years ago when I exchanged books with a Software Engineer I was dating- She lent me <i>The Mythical Man Month </i>which I found interesting, and I lent her <i>What is the name of this book by Smullyan </i>which she found amusing. Did this exchange of books help our relationship? We have now been married for many years, though its not clear if we can trace this to the exchange of books OR to the fact that she had KNUTH Volumes 1 and 3, and I had KNUTH Volume 2. </p><p> Fred Brooks: You have my thanks and of course Happy Birthday!</p><p>When I read The Mythical Man-Month  I was reminded of a math problem I heard as a kid: </p><p>If a hen-and-half lays an egg-and-a-half in a day-and-a-half then how many eggs can seven hen lay in seven days? </p><p>My answer: if (3/2) hens lay (3/2) eggs in (3/2) days then that's 2/3 of an egg per hen-day, so the answer is </p><p>49* 2/3 = 32 and 2/3 eggs.</p><p>It did not bother me one whit that (1) you can't have 2/3 of an egg, and (2) Just like adding more people might slow down a project, adding more hens might end up being a bad idea-- especially if they are all crowded into the same chicken-coop and hence don't feel much like laying eggs.</p><p>Who was the first person to note that adding <i>more</i> people or hens might be a bad idea? I do not know, but here is an amusing, yet realistic, article by Mark Twain on what I would call <i>The mythical</i> <i>cat-minute. </i>My advisor Harry Lewis send it to me in the midst of an email exchange about <i>The Mythical</i> <i>Man-Month.</i> He got it from a student of his, Larry Denenberg. Here it is: </p><p><br/></p><p>CATS AND RATS</p><pre>The following piece first appeared in ``The Monthly Packet'' of February
1880 and is reprinted in _The_Magic_of_Lewis_Carroll_, edited by John
Fisher, Bramhall House, 1973.


   If 6 cats kill 6 rats in 6 minutes, how many will be needed to kill
   100 rats in 50 minutes?

   This is a good example of a phenomenon that often occurs in working
   problems in double proportion; the answer looks all right at first, but,
   when we come to test it, we find that, owing to peculiar circumstances in
   the case, the solution is either impossible or else indefinite, and needing
   further data.  The 'peculiar circumstance' here is that fractional cats or
   rats are excluded from consideration, and in consequence of this the
   solution is, as we shall see, indefinite.

   The solution, by the ordinary rules of Double Proportion, is 12 cats.
   [Steps of Carroll's solution, in the notation of his time, omitted.]

   But when we come to trace the history of this sanguinary scene through all
   its horrid details, we find that at the end of 48 minutes 96 rats are dead,
   and that there remain 4 live rats and 2 minutes to kill them in: the
   question is, can this be done?

   Now there are at least *four* different ways in which the original feat,
   of 6 cats killing 6 rats in 6 minutes, may be achieved.  For the sake of
   clearness let us tabulate them:
      A.  All 6 cats are needed to kill a rat; and this they do in one minute,
          the other rats standing meekly by, waiting for their turn.
      B.  3 cats are needed to kill a rat, and they do it in 2 minutes.
      C.  2 cats are needed, and do it in 3 minutes.
      D.  Each cat kills a rat all by itself, and takes 6 minutes to do it.

   In cases A and B it is clear that the 12 cats (who are assumed to come
   quite fresh from their 48 minutes of slaughter) can finish the affair in
   the required time; but, in case C, it can only be done by supposing that 2
   cats could kill two-thirds of a rat in 2 minutes; and in case D, by
   supposing that a cat could kill one-third of a rat in two minutes.  Neither
   supposition is warranted by the data; nor could the fractional rats (even
   if endowed with equal vitality) be fairly assigned to the different cats.
   For my part, if I were a cat in case D, and did not find my claws in good
   working order, I should certainly prefer to have my one-third-rat cut off
   from the tail end.

   In cases C and D, then, it is clear that we must provide extra cat-power.
   In case C *less* than 2 extra cats would be of no use.  If 2 were supplied,
   and if they began killing their 4 rats at the beginning of the time, they
   would finish them in 12 minutes, and have 36 minutes to spare, during which
   they might weep, like Alexander, because there were not 12 more rats to
   kill.  In case D, one extra cat would suffice; it would kill its 4 rats in
   24 minutes, and have 26 minutes to spare, during which it could have killed
   another 4.  But in neither case could any use be made of the last 2
   minutes, except to half-kill rats---a barbarity we need not take into
   consideration.

   To sum up our results.  If the 6 cats kill the 6 rats by method A or B,
   the answer is 12; if by method C, 14; if by method D, 13.

   This, then, is an instance of a solution made `indefinite' by the
   circumstances of the case.  If an instance of the `impossible' be desired,
   take the following: `If a cat can kill a rat in a minute, how many would be
   needed to kill it in the thousandth part of a second?'  The *mathematical*
   answer, of course, is `60,000,' and no doubt less than this would *not*
   suffice; but would 60,000 suffice?  I doubt it very much.  I fancy that at
   least 50,000 of the cats would never even see the rat, or have any idea of
   what was going on.

   Or take this: `If a cat can kill a rat in a minute, how long would it be
   killing 60,000 rats?'  Ah, how long, indeed!  My private opinion is that
   the rats would kill the cat.
</pre><div><br/></div><p><br/></p></div>
    </content>
    <updated>2021-05-02T19:33:00Z</updated>
    <published>2021-05-02T19:33:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-07T09:41:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/062</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/062" rel="alternate" type="text/html"/>
    <title>TR21-062 |  Improved Hitting Set for Orbit of ROABPs | 

	Vishwas Bhargava, 

	Sumanta Ghosh</title>
    <summary>The orbit of an $n$-variate polynomial $f(\mathbf{x})$ over a field $\mathbb{F}$ is the set $\{f(A \mathbf{x} +  b)\,\mid\, A\in \mathrm{GL}({n,\mathbb{F}})\mbox{ and }\mathbf{b} \in \mathbb{F}^n\}$, and the orbit of a polynomial class is the union of orbits of all the polynomials in it. In this paper, we give improved constructions of hitting-sets for the orbit of read-once oblivious algebraic branching programs (ROABPs) and a related model. Over field with characteristic zero or greater than $d$, we construct a hitting set of size  $(ndw)^{O(w^2\log n\cdot \min\{w^2, d\log w\})}$  for the orbit of ROABPs in unknown variable order where $d$ is the individual degree and $w$ is the width of ROABPs. We also give hitting set of size $(ndw)^{O(\min\{w^2,d\log w\})}$ for the orbit of polynomials  computed by $w$-width ROABPs in any variable order. Our hitting sets improve upon the results of Saha and Thankey \cite{Saha-Thankey'21} who gave an $(ndw)^{O(d\log w)}$ size hitting set for the orbit of commutative ROABPs (a subclass of \textit{any-order} ROABPs) and $(nw)^{O(w^6\log n)}$ size hitting set for the orbit of multilinear ROABPs. Designing better hitting sets in large individual degree regime, for instance $d&gt;n$, was asked as an open problem by \cite{Saha-Thankey'21} and this work solves it in  small width setting. 

We prove some new rank concentration results by establishing \emph{low-cone concentration} for the polynomials over vector spaces, and they strengthen some previously known \emph{low-support} based rank concentrations shown in \cite{FSS14}. These new low-cone concentration results are crucial in our hitting set construction, and may be of independent interest. To the best of our knowledge, this is the first time when low-cone rank concentration has been used for designing hitting sets.</summary>
    <updated>2021-05-02T07:04:05Z</updated>
    <published>2021-05-02T07:04:05Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-07T16:37:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18674</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/" rel="alternate" type="text/html"/>
    <title>Test of Time</title>
    <summary>Time is the ultimate critic. What future generations think of us and our work ultimately determines our standing or lack of it— Stewart Stafford Bobby Kleinberg just reached out to those of us who post from time to time. He wanted some help in announcing a new STOC Test of Time Award. So today, Ken […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Time is the ultimate critic. What future generations think of us and our work ultimately determines our standing or lack of it— Stewart Stafford</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/bk/" rel="attachment wp-att-18676"><img alt="" class="alignright size-thumbnail wp-image-18676" height="150" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/bk-150x150.png?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
Bobby Kleinberg just reached out to those of us who post from time to time. He wanted some help in announcing a new STOC Test of Time Award. </p>
<p>
So today, Ken and I put this together. </p>
<p>Bobby said: </p>
<blockquote><p><b> </b> <em> As with FOCS, three awards will be given: one for papers from approximately 10 years ago, one for approximately 20 years ago, and one for approximately 30 years ago. The selection committee for this year’s award will be Joe Halpern, Mihalis Yannakakis, and Salil Vadhan. </em>
</p></blockquote>
<p/><p>
I would suggest that one of Bobby’s papers could fit this award: </p>
<p>Group-theoretic algorithms for matrix multiplication <br/>
Henry Cohn, Robert Kleinberg, Balazs Szegedy, Christopher Umans </p>
<p>
Well, I can say that without being out of order <em>here</em> because that paper was in FOCS, not STOC.</p>
<p>
</p><p/><h2> Criteria </h2><p/>
<p>
</p><li>
<i>Area</i>: Opening up a new area of research <p/>
</li><li>
<i>Proof</i>: Introducing new proof techniques <p/>
</li><li>
<i>Use</i>: Solving a problem of lasting importance <p/>
</li><li>
<i>Else</i>: Stimulating advances in other areas of computer science or in other disciplines. <p/>
<p>
Go here for details on how to <a href="https://sigact.org/prizes/stoc_tot.html">nominate</a>. By the way: Another test of Time is that the nominations are due relatively soon—May 24. So if you wish to nominate some paper please act soon. </p>
<p>
Ken notes that the first criterion could also be called <i>Leadership</i>, the second always comes with an element of <i>Surprise</i>, and the last two have aspects of <i>Applicability</i> and <i>Practicality</i>.  Adding those to my terms makes a double acronym <i>APPLAUSE</i>.</p>
<p>
</p><p/><h2> Early Early Years </h2><p/>
<p/><p>
I have been around long enough to fit the a 30++ years category, and Ken almost <a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/">ditto</a>. Here are some opinions on the early days. Those papers with<br/>
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img alt="" class="alignleft  wp-image-18682" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1"/></a><br/>
are an absolute must include—I hope you agree.</p>
<p>
<a href="https://dblp.org/db/conf/stoc/stoc81.html">1981</a>:</p>
<p>
Space-Bounded Probabilistic Turing Machine Complexity Classes Are Closed under Complement <br/>
<i>Use</i>.</p>
<p>
<a href="https://dblp.org/db/conf/stoc/stoc82.html">1982</a>:</p>
<p>
Shafi Goldwasser, Silvio Micali <br/>
Probabilistic Encryption and How to Play Mental Poker Keeping Secret All Partial Information <br/>
<i>Area</i>.</p>
<p>
<a href="https://dl.acm.org/doi/proceedings/10.1145/800061">1983</a>:</p>
<p>
Miklós Ajtai, Janos Komlós, and Endre Szemerédi <br/>
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img alt="" class="alignleft  wp-image-18682" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1"/></a>An <img alt="{0(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> sorting network <br/>
<i>Use</i> and <i>Proof</i>.</p>
<p>
Larry Stockmeyer <br/>
The complexity of approximate counting <br/>
<i>Use</i>.</p>
<p>
<a href="https://dl.acm.org/doi/proceedings/10.1145/800057">1984</a>:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img alt="" class="alignleft  wp-image-18682" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1"/></a>Les Valiant <br/>
A theory of the learnable <br/>
<i>Area</i> and <i>Else</i>.</p>
<p>
Narendra Karmarkar <br/>
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img alt="" class="alignleft  wp-image-18682" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1"/></a>A new polynomial-time algorithm for linear programming <br/>
<i>Use</i> and <i>Proof</i>.</p>
<p>
Of course, these are my own opinions (with concurrence from Ken) and do not reflect those of organizations we belong to.</p>
<p/><h2> Open Problems </h2><p/>
<p/><p>
Ken thinks that one way not to be asked here about my own papers is to mention one, so here goes:</p>
<p>
<a href="https://dblp.org/db/conf/stoc/stoc80.html">1980</a></p>
<p>
Ravindran Kannan, Richard Lipton <br/>
The Orbit Problem is Decidable <br/>
<i>Proof</i>.  Ken adds: Could also be <i>Surprise</i> because we not only showed decidable but in polynomial time.  But the real test of time here may be whether (despite some technical limitations) it proves useful to the great recent interest in adjacent kinds of orbit problems in <a href="https://rjlipton.wpcomstaging.com/2018/06/06/princeton-is-invariant/">invariant</a> and <a href="https://rjlipton.wpcomstaging.com/2015/07/12/the-long-reach-of-reachability/">reachability</a> theory.</p></li></font></font></div>
    </content>
    <updated>2021-04-30T22:53:16Z</updated>
    <published>2021-04-30T22:53:16Z</published>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="People"/>
    <category term="award"/>
    <category term="Bobby Kleinberg"/>
    <category term="Prize"/>
    <category term="STOC"/>
    <category term="Test of Time"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-05-07T16:37:43Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-3713824207379370168</id>
    <link href="http://processalgebra.blogspot.com/feeds/3713824207379370168/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=3713824207379370168" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/3713824207379370168" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/3713824207379370168" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/04/polyconc-online-collaboration-to.html" rel="alternate" type="text/html"/>
    <title>PolyConc: Online collaboration to improve on a result on the equational theory of CCS modulo bisimilarity</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The aim of this post is to try and start an online collaboration to improve the solution to a problem in the equational logic of processes that I posed in a <a href="https://www.brics.dk/NS/03/2/BRICS-NS-03-2.pdf" target="_blank">survey paper in 2003</a>, namely    </p><blockquote>  Can one obtain a finite axiomatisation of the parallel composition operator in bisimulation semantics by adding only one binary operator to the signature of (recursion, restriction, and relabelling free) CCS? </blockquote><p>     Valentina Castiglioni, Wan Fokkink, Anna Ingólfsdóttir, Bas Luttik and I published a partial, negative answer to the above question in a <a href="https://doi.org/10.4230/LIPIcs.CSL.2021.8" target="_blank">paper at CSL 2021</a>. (See the <a href="https://arxiv.org/pdf/2010.01943.pdf" target="_blank">arXiv version</a> for details and for the historical context for the above question.) Our solution is based on three simplifying assumptions that are described in detail in Section 3 of the <a href="https://arxiv.org/pdf/2010.01943.pdf" target="_blank">above-mentioned paper</a>. We'd be very interested in hearing whether any member of the research community in process algebra, universal algebra and equational logic can relax or remove any of our simplifying assumptions. In particular, one can start with assumptions 3 and 2. </p><p>We would also welcome any comments and suggestions on whether some version of that problem can be solved using existing results from equational logic and universal algebra. In particular, are there any general results guaranteeing that, under certain conditions, the reduct of a finitely based algebra is also finitely based? Or, conversely, that if some algebra is not finitely based, then so its expansion with a new operator?</p><p>To start with, add any contributions you might have as comments to this post. If ever we make substantial enough progress on the above question, anyone who has played a positive role in extending our results will be a co-author of the resulting paper. </p><p>Let PolyConc begin!<br/></p><p/></div>
    </content>
    <updated>2021-04-30T20:41:00Z</updated>
    <published>2021-04-30T20:41:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-05-06T22:25:47Z</updated>
    </source>
  </entry>
</feed>
