<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-07-25T15:22:55Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17669</id>
    <link href="https://gilkalai.wordpress.com/2019/07/25/tyi-39-can-a-coalition-of-children-guarantees-being-in-the-same-class/" rel="alternate" type="text/html"/>
    <title>TYI 39 : Can a coalition of children guarantees all being in the same class?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">There is a class of children that have just finished elementary school. Now they all move from elementary school to high school and classes are reshuffled. Each child lists three friends, and the assignment of children into classes ensures that … <a href="https://gilkalai.wordpress.com/2019/07/25/tyi-39-can-a-coalition-of-children-guarantees-being-in-the-same-class/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2019/07/class_school.jpg"><img alt="" class="alignnone size-medium wp-image-17676" height="213" src="https://gilkalai.files.wordpress.com/2019/07/class_school.jpg?w=300&amp;h=213" width="300"/></a></p>
<p>There is a class of children that have just finished elementary school. Now they all move from elementary school to high school and classes are reshuffled. Each child lists three friends, and the assignment of children into classes ensures that each child will have at least one of these three friends in his class.</p>
<p>One of the children heard from five of his schoolmates that they found that they can make their selections in a way that will ensure that all five will be assigned to the same class!</p>
<h3><span style="color: #993366;">Test your intuition: Is there a strategy for five of the children that will ensure that all five will be assigned to the same class?</span></h3>
<p>Can a larger group of children coordinate their choices to ensure that they will all necessarily be assigned to the same class?</p>
<a name="pd_a_10371327"/><div class="CSS_Poll PDS_Poll" id="PDI_container10371327" style="display: inline-block;"/><div id="PD_superContainer"/><noscript>&lt;a href="https://polldaddy.com/p/10371327" target="_blank"&gt;Take Our Poll&lt;/a&gt;</noscript></div>
    </content>
    <updated>2019-07-25T12:59:05Z</updated>
    <published>2019-07-25T12:59:05Z</published>
    <category term="Combinatorics"/>
    <category term="Economics"/>
    <category term="Mathematics to the rescue"/>
    <category term="Test your intuition"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-25T15:20:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10531</id>
    <link href="http://arxiv.org/abs/1907.10531" rel="alternate" type="text/html"/>
    <title>Sampling and Optimization on Convex Sets in Riemannian Manifolds of Non-Negative Curvature</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goyal:Navin.html">Navin Goyal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shetty:Abhishek.html">Abhishek Shetty</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10531">PDF</a><br/><b>Abstract: </b>The Euclidean space notion of convex sets (and functions) generalizes to
Riemannian manifolds in a natural sense and is called geodesic convexity.
Extensively studied computational problems such as convex optimization and
sampling in convex sets also have meaningful counterparts in the manifold
setting. Geodesically convex optimization is a well-studied problem with
ongoing research and considerable recent interest in machine learning and
theoretical computer science. In this paper, we study sampling and convex
optimization problems over manifolds of non-negative curvature proving
polynomial running time in the dimension and other relevant parameters. Our
algorithms assume a warm start. We first present a random walk based sampling
algorithm and then combine it with simulated annealing for solving convex
optimization problems. To our knowledge, these are the first algorithms in the
general setting of positively curved manifolds with provable polynomial
guarantees under reasonable assumptions, and the first study of the connection
between sampling and optimization in this setting.
</p></div>
    </summary>
    <updated>2019-07-25T01:26:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10499</id>
    <link href="http://arxiv.org/abs/1907.10499" rel="alternate" type="text/html"/>
    <title>P-SLOCAL-Completeness of Maximum Independent Set Approximation</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maus:Yannic.html">Yannic Maus</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10499">PDF</a><br/><b>Abstract: </b>We prove that the maximum independent set approximation problem with
polylogarithmic approximation factor is P-SLOCAL-complete. Thus an efficient
algorithm for the maximum independent set approximation in the LOCAL model
efficient algorithms for many problems in the LOCAL model including the
computation of (polylog n, polylog n) network decompositions.
</p></div>
    </summary>
    <updated>2019-07-25T01:28:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10468</id>
    <link href="http://arxiv.org/abs/1907.10468" rel="alternate" type="text/html"/>
    <title>The Complexity of Computational Problems about Nash Equilibria in Symmetric Win-Lose Games</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bil=ograve=:Vittorio.html">Vittorio Bilò</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mavronicolas:Marios.html">Marios Mavronicolas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10468">PDF</a><br/><b>Abstract: </b>We revisit the complexity of deciding, given a {\it bimatrix game,} whether
it has a {\it Nash equilibrium} with certain natural properties; such decision
problems were early known to be ${\mathcal{NP}}$-hard~\cite{GZ89}. We show that
${\mathcal{NP}}$-hardness still holds under two significant restrictions in
simultaneity: the game is {\it win-lose} (that is, all {\it utilities} are $0$
or $1$) and {\it symmetric}. To address the former restriction, we design
win-lose {\it gadgets} and a win-lose reduction; to accomodate the latter
restriction, we employ and analyze the classical {\it
${\mathsf{GHR}}$-symmetrization}~\cite{GHR63} in the win-lose setting. Thus,
{\it symmetric win-lose bimatrix games} are as complex as general bimatrix
games with respect to such decision problems. As a byproduct of our techniques,
we derive hardness results for search, counting and parity problems about Nash
equilibria in symmetric win-lose bimatrix games.
</p></div>
    </summary>
    <updated>2019-07-25T01:20:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10444</id>
    <link href="http://arxiv.org/abs/1907.10444" rel="alternate" type="text/html"/>
    <title>Constant Delay Traversal of Grammar-Compressed Graphs with Bounded Rank</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maneth:Sebastian.html">Sebastian Maneth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peternek:Fabian.html">Fabian Peternek</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10444">PDF</a><br/><b>Abstract: </b>We present a pointer-based data structure for constant time traversal of the
edges of an edge-labeled (alphabet $\Sigma$) directed hypergraph (a graph where
edges can be incident to more than two vertices, and the incident vertices are
ordered) given as hyperedge-replacement grammar $G$. It is assumed that the
grammar has a fixed rank $\kappa$ (maximal number of vertices connected to a
nonterminal hyperedge) and that each vertex of the represented graph is
incident to at most one $\sigma$-edge per direction ($\sigma \in \Sigma$).
Precomputing the data structure needs $O(|G||\Sigma|\kappa r h)$ space and
$O(|G||\Sigma|\kappa rh^2)$ time, where $h$ is the height of the derivation
tree of $G$ and $r$ is the maximal rank of a terminal edge occurring in the
grammar.
</p></div>
    </summary>
    <updated>2019-07-25T01:29:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10398</id>
    <link href="http://arxiv.org/abs/1907.10398" rel="alternate" type="text/html"/>
    <title>Medians in median graphs in linear time</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Laurine Bénéteau, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chalopin:J=eacute=r=eacute=mie.html">Jérémie Chalopin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chepoi:Victor.html">Victor Chepoi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vax=egrave=s:Yann.html">Yann Vaxès</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10398">PDF</a><br/><b>Abstract: </b>The median of a graph $G$ is the set of all vertices $x$ of $G$ minimizing
the sum of distances from $x$ to all other vertices of $G$. It is known that
computing the median of dense graphs in subcubic time refutes the APSP
conjecture and computing the median of sparse graphs in subquadratic time
refutes the HS conjecture. In this paper, we present a linear time algorithm
for computing medians of median graphs, improving over the existing quadratic
time algorithm. Median graphs constitute the principal class of graphs
investigated in metric graph theory, due to their bijections with other
discrete and geometric structures (CAT(0) cube complexes, domains of event
structures, and solution sets of 2-SAT formulas). Our algorithm is based on the
known majority rule characterization of medians in a median graph $G$ and on a
fast computation of parallelism classes of edges ($\Theta$-classes) of $G$. The
main technical contribution of the paper is a linear time algorithm for
computing the $\Theta$-classes of a median graph $G$ using Lexicographic
Breadth First Search (LexBFS). Namely, we show that any LexBFS ordering of the
vertices of a median graph $G$ has the following \emph{fellow traveler
property}: the fathers of any two adjacent vertices of $G$ are also adjacent.
Using the fast computation of the $\Theta$-classes of a median graph $G$, we
also compute the Wiener index (total distance) of $G$ in linear time.
</p></div>
    </summary>
    <updated>2019-07-25T01:26:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10376</id>
    <link href="http://arxiv.org/abs/1907.10376" rel="alternate" type="text/html"/>
    <title>Reducing Path TSP to TSP</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Traub:Vera.html">Vera Traub</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vygen:Jens.html">Jens Vygen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zenklusen:Rico.html">Rico Zenklusen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10376">PDF</a><br/><b>Abstract: </b>We present a black-box reduction from the path version of the Traveling
Salesman Problem (Path TSP) to the classical tour version (TSP). More
precisely, we show that given an $\alpha$-approximation algorithm for TSP,
then, for any $\epsilon &gt;0$, there is an $(\alpha+\epsilon)$-approximation
algorithm for the more general Path TSP. This reduction implies that the
approximability of Path TSP is the same as for TSP, up to an arbitrarily small
error. This avoids future discrepancies between the best known approximation
factors achievable for these two problems, as they have existed until very
recently.
</p>
<p>A well-studied special case of TSP, Graph TSP, asks for tours in unit-weight
graphs. Our reduction shows that any $\alpha$-approximation algorithm for Graph
TSP implies an $(\alpha+\epsilon)$-approximation algorithm for its path
version. By applying our reduction to the $1.4$-approximation algorithm for
Graph TSP by Seb\H{o} and Vygen, we obtain a polynomial-time
$(1.4+\epsilon)$-approximation algorithm for Graph Path TSP, improving on a
recent $1.497$-approximation algorithm of Traub and Vygen.
</p>
<p>We obtain our results through a variety of new techniques, including a novel
way to set up a recursive dynamic program to guess significant parts of an
optimal solution. At the core of our dynamic program we deal with instances of
a new generalization of (Path) TSP which combines parity constraints with
certain connectivity requirements. This problem, which we call $\Phi$-TSP, has
a constant-factor approximation algorithm and can be reduced to TSP in certain
cases when the dynamic program would not make sufficient progress.
</p></div>
    </summary>
    <updated>2019-07-25T01:28:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10367</id>
    <link href="http://arxiv.org/abs/1907.10367" rel="alternate" type="text/html"/>
    <title>DispVoxNets: Non-Rigid Point Set Alignment with Supervised Learning Proxies</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shimada:Soshi.html">Soshi Shimada</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golyanik:Vladislav.html">Vladislav Golyanik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tretschk:Edgar.html">Edgar Tretschk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stricker:Didier.html">Didier Stricker</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Theobalt:Christian.html">Christian Theobalt</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10367">PDF</a><br/><b>Abstract: </b>We introduce a supervised-learning framework for non-rigid point set
alignment of a new kind - Displacements on Voxels Networks (DispVoxNets) -
which abstracts away from the point set representation and regresses 3D
displacement fields on regularly sampled proxy 3D voxel grids. Thanks to
recently released collections of deformable objects with known intra-state
correspondences, DispVoxNets learn a deformation model and further priors
(e.g., weak point topology preservation) for different object categories such
as cloths, human bodies and faces. DispVoxNets cope with large deformations,
noise and clustered outliers more robustly than the state-of-the-art. At test
time, our approach runs orders of magnitude faster than previous techniques.
All properties of DispVoxNets are ascertained numerically and qualitatively in
extensive experiments and comparisons to several previous methods.
</p></div>
    </summary>
    <updated>2019-07-25T01:30:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10363</id>
    <link href="http://arxiv.org/abs/1907.10363" rel="alternate" type="text/html"/>
    <title>Classification of linear codes using canonical augmentation</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bouyukliev:Iliya.html">Iliya Bouyukliev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bouyuklieva:Stefka.html">Stefka Bouyuklieva</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10363">PDF</a><br/><b>Abstract: </b>We propose an algorithm for classification of linear codes over different
finite fields based on canonical augmentation. We apply this algorithm to
obtain classification results over fields with 2, 3 and 4 elements.
</p></div>
    </summary>
    <updated>2019-07-25T01:28:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10333</id>
    <link href="http://arxiv.org/abs/1907.10333" rel="alternate" type="text/html"/>
    <title>Anti-unification in Constraint Logic Programming</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Gonzague Yernaux, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vanhoof:Wim.html">Wim Vanhoof</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10333">PDF</a><br/><b>Abstract: </b>Anti-unification refers to the process of generalizing two (or more) goals
into a single, more general, goal that captures some of the structure that is
common to all initial goals. In general one is typically interested in
computing what is often called a most specific generalization, that is a
generalization that captures a maximal amount of shared structure. In this work
we address the problem of anti-unification in CLP, where goals can be seen as
unordered sets of atoms and/or constraints. We show that while the concept of a
most specific generalization can easily be defined in this context, computing
it becomes an NP-complete problem. We subsequently introduce a generalization
algorithm that computes a well-defined abstraction whose computation can be
bound to a polynomial execution time. Initial experiments show that even a
naive implementation of our algorithm produces acceptable generalizations in an
efficient way. Under consideration for acceptance in TPLP.
</p></div>
    </summary>
    <updated>2019-07-25T01:22:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10308</id>
    <link href="http://arxiv.org/abs/1907.10308" rel="alternate" type="text/html"/>
    <title>Scalable and Secure Computation Among Strangers: Resource-Competitive Byzantine Protocols</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Augustine:John.html">John Augustine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/King:Valerie.html">Valerie King</a>, Anisur R. Molla Gopal Pandurangan, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saia:Jared.html">Jared Saia</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10308">PDF</a><br/><b>Abstract: </b>Motivated, in part, by the rise of permissionless systems such as Bitcoin
where arbitrary nodes (whose identities are not known apriori) can join and
leave at will, we extend established research in scalable Byzantine agreement
to a more practical model where each node (initially) does not know the
identity of other nodes. A node can send to new destinations only by sending to
random (or arbitrary) nodes, or responding (if it chooses) to messages received
from those destinations. We assume a synchronous and fully-connected network,
with a full-information, but static Byzantine adversary. A general drawback of
existing Byzantine protocols is that the communication cost incurred by the
honest nodes may not be proportional to those incurred by the Byzantine nodes;
in fact, they can be significantly higher. Our goal is to design Byzantine
protocols for fundamental problems which are {\em resource competitive}, i.e.,
the number of bits sent by honest nodes is not much more than those sent by
Byzantine nodes.
</p>
<p>We describe a randomized scalable algorithm to solve Byzantine agreement,
leader election, and committee election in this model. Our algorithm sends an
expected $O((T+n)\log n)$ bits and has latency $O(polylog(n))$, where $n$ is
the number of nodes, and $T$ is the minimum of $n^2$ and the number of bits
sent by adversarially controlled nodes. The algorithm is resilient to
$(1/4-\epsilon)n$ Byzantine nodes for any fixed $\epsilon &gt; 0$, and succeeds
with high probability. Our work can be considered as a first application of
resource-competitive analysis to fundamental Byzantine problems.
</p>
<p>To complement our algorithm we also show lower bounds for
resource-competitive Byzantine agreement. We prove that, in general, one cannot
hope to design Byzantine protocols that have communication cost that is
significantly smaller than the cost of the Byzantine adversary.
</p></div>
    </summary>
    <updated>2019-07-25T01:22:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10250</id>
    <link href="http://arxiv.org/abs/1907.10250" rel="alternate" type="text/html"/>
    <title>Learning Embedding of 3D models with Quadric Loss</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Agarwal:Nitin.html">Nitin Agarwal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoon:Sung=eui.html">Sung-eui Yoon</a>, M Gopi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10250">PDF</a><br/><b>Abstract: </b>Sharp features such as edges and corners play an important role in the
perception of 3D models. In order to capture them better, we propose quadric
loss, a point-surface loss function, which minimizes the quadric error between
the reconstructed points and the input surface. Computation of Quadric loss is
easy, efficient since the quadric matrices can be computed apriori, and is
fully differentiable, making quadric loss suitable for training point and mesh
based architectures. Through extensive experiments we show the merits and
demerits of quadric loss. When combined with Chamfer loss, quadric loss
achieves better reconstruction results as compared to any one of them or other
point-surface loss functions.
</p></div>
    </summary>
    <updated>2019-07-25T01:36:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10230</id>
    <link href="http://arxiv.org/abs/1907.10230" rel="alternate" type="text/html"/>
    <title>An FPT algorithm for orthogonal buttons and scissors</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsur:Dekel.html">Dekel Tsur</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10230">PDF</a><br/><b>Abstract: </b>We study the puzzle game Buttons and Scissors in which the goal is to remove
all buttons from an $n\times m$ grid by a series of horizontal and vertical
cuts. We show that the corresponding parameterized problem has an algorithm
with time complexity $2^{O(k^2 \log k)} (n+m)^{O(1)}$, where $k$ is an upper
bound on the number of cuts.
</p></div>
    </summary>
    <updated>2019-07-25T01:28:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10121</id>
    <link href="http://arxiv.org/abs/1907.10121" rel="alternate" type="text/html"/>
    <title>SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Pauli Virtanen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gommers:Ralf.html">Ralf Gommers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oliphant:Travis_E=.html">Travis E. Oliphant</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haberland:Matt.html">Matt Haberland</a>, Tyler Reddy, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cournapeau:David.html">David Cournapeau</a>, Evgeni Burovski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peterson:Pearu.html">Pearu Peterson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weckesser:Warren.html">Warren Weckesser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bright:Jonathan.html">Jonathan Bright</a>, Stéfan J. van der Walt, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brett:Matthew.html">Matthew Brett</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wilson:Joshua.html">Joshua Wilson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Millman:K=_Jarrod.html">K. Jarrod Millman</a>, Nikolay Mayorov, Andrew R. J. Nelson, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Eric.html">Eric Jones</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kern:Robert.html">Robert Kern</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Larson:Eric.html">Eric Larson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carey:CJ.html">CJ Carey</a>, İlhan Polat, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Yu.html">Yu Feng</a>, Eric W. Moore, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/VanderPlas:Jake.html">Jake VanderPlas</a>, Denis Laxalde, Josef Perktold, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cimrman:Robert.html">Robert Cimrman</a>, Ian Henriksen, E. A. Quintero, Charles R Harris, Anne M. Archibald, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ribeiro:Ant=ocirc=nio_H=.html">Antônio H. Ribeiro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pedregosa:Fabian.html">Fabian Pedregosa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mulbregt:Paul_van.html">Paul van Mulbregt</a>, SciPy 1.0 Contributors <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10121">PDF</a><br/><b>Abstract: </b>SciPy is an open source scientific computing library for the Python
programming language. SciPy 1.0 was released in late 2017, about 16 years after
the original version 0.1 release. SciPy has become a de facto standard for
leveraging scientific algorithms in the Python programming language, with more
than 600 unique code contributors, thousands of dependent packages, over
100,000 dependent repositories, and millions of downloads per year. This
includes usage of SciPy in almost half of all machine learning projects on
GitHub, and usage by high profile projects including LIGO gravitational wave
analysis and creation of the first-ever image of a black hole (M87). The
library includes functionality spanning clustering, Fourier transforms,
integration, interpolation, file I/O, linear algebra, image processing,
orthogonal distance regression, minimization algorithms, signal processing,
sparse matrix handling, computational geometry, and statistics. In this work,
we provide an overview of the capabilities and development practices of the
SciPy library and highlight some recent technical developments.
</p></div>
    </summary>
    <updated>2019-07-25T01:23:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/097</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/097" rel="alternate" type="text/html"/>
    <title>TR19-097 |  Reversible Pebble Games and the Relation Between Tree-Like and General Resolution Space | 

	Florian Wörz, 

	Jacobo Toran</title>
    <summary>We show a new connection between the space measure in tree-like resolution and the reversible pebble game in graphs. Using this connection we provide several formula classes for which there is a logarithmic factor separation between the space complexity measure in tree-like and general resolution. We show that these separations are almost optimal by proving upper bounds for tree-like resolution space in terms of general resolution clause and variable space. In particular we show that for any formula F, its tree-like resolution is upper bounded by space(?)log time(?) where ? is any general resolution refutation of F. This holds considering as space(?) the clause space of the refutation as well as considering its variable space. For the concrete case of Tseitin formulas we are able to improve this bound to the optimal bound space(?)log(n), where n is the number of vertices of the corresponding graph.</summary>
    <updated>2019-07-24T12:09:27Z</updated>
    <published>2019-07-24T12:09:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-25T15:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.09994</id>
    <link href="http://arxiv.org/abs/1907.09994" rel="alternate" type="text/html"/>
    <title>Local and Union Page Numbers</title>
    <feedworld_mtime>1563926400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Laura Merker, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Ueckerdt:Torsten.html">Torsten Ueckerdt</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.09994">PDF</a><br/><b>Abstract: </b>We introduce the novel concepts of local and union book embeddings, and, as
the corresponding graph parameters, the local page number ${\rm pn}_\ell(G)$
and the union page number ${\rm pn}_u(G)$. Both parameters are relaxations of
the classical page number ${\rm pn}(G)$, and for every graph $G$ we have ${\rm
pn}_\ell(G) \leq {\rm pn}_u(G) \leq {\rm pn}(G)$. While for ${\rm pn}(G)$ one
minimizes the total number of pages in a book embedding of $G$, for ${\rm
pn}_\ell(G)$ we instead minimize the number of pages incident to any one
vertex, and for ${\rm pn}_u(G)$ we instead minimize the size of a partition of
$G$ with each part being a vertex-disjoint union of crossing-free subgraphs.
While ${\rm pn}_\ell(G)$ and ${\rm pn}_u(G)$ are always within a multiplicative
factor of $4$, there is no bound on the classical page number ${\rm pn}(G)$ in
terms of ${\rm pn}_\ell(G)$ or ${\rm pn}_u(G)$.
</p>
<p>We show that local and union page numbers are closer related to the graph's
density, while for the classical page number the graph's global structure can
play a much more decisive role. We introduce tools to investigate local and
union book embeddings in exemplary considerations of the class of all planar
graphs and the class of graphs of tree-width $k$. As an incentive to pursue
research in this new direction, we offer a list of intriguing open problems.
</p></div>
    </summary>
    <updated>2019-07-24T23:27:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.09834</id>
    <link href="http://arxiv.org/abs/1907.09834" rel="alternate" type="text/html"/>
    <title>Managing Multiple Mobile Resources</title>
    <feedworld_mtime>1563926400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldkord:Bj=ouml=rn.html">Björn Feldkord</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Knollmann:Till.html">Till Knollmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Malatyali:Manuel.html">Manuel Malatyali</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heide:Friedhelm_Meyer_auf_der.html">Friedhelm Meyer auf der Heide</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.09834">PDF</a><br/><b>Abstract: </b>We extend the Mobile Server Problem, introduced in SPAA'17, to a model where
k identical mobile resources, here named servers, answer requests appearing at
points in the Euclidean space. In order to reduce communication costs, the
positions of the servers can be adapted by a limited distance m_s per round for
each server. The costs are measured similar to the classical Page Migration
Problem, i.e., answering a request induces costs proportional to the distance
to the nearest server, and moving a server induces costs proportional to the
distance multiplied with a weight D.
</p>
<p>We show that, in our model, no online algorithm can have a constant
competitive ratio, i.e., one which is independent of the input length n, even
if an augmented moving distance of (1+\delta)m_s is allowed for the online
algorithm. Therefore we investigate a restriction of the power of the adversary
dictating the sequence of requests: We demand locality of requests, i.e., that
consecutive requests come from points in the Euclidean space with distance
bounded by some constant m_c. We show constant lower bounds on the
competitiveness in this setting (independent of n, but dependent on k, m_s and
m_c).
</p>
<p>On the positive side, we present a deterministic online algorithm with
bounded competitiveness when augmented moving distance and locality of requests
is assumed. Our algorithm simulates any given algorithm for the classical
k-Page Migration problem as guidance for its servers and extends it by a greedy
move of one server in every round. The resulting competitive ratio is
polynomial in the number of servers k, the ratio between m_c and m_s, the
inverse of the augmentation factor 1/\delta and the competitive ratio of the
simulated k-Page Migration algorithm.
</p></div>
    </summary>
    <updated>2019-07-24T23:23:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.09759</id>
    <link href="http://arxiv.org/abs/1907.09759" rel="alternate" type="text/html"/>
    <title>Level-sets persistence and sheaf theory</title>
    <feedworld_mtime>1563926400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berkouk:Nicolas.html">Nicolas Berkouk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ginot:Gr=eacute=gory.html">Grégory Ginot</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oudot:Steve.html">Steve Oudot</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.09759">PDF</a><br/><b>Abstract: </b>In this paper we provide an explicit connection between level-sets
persistence and derived sheaf theory over the real line. In particular we
construct a functor from 2-parameter persistence modules to sheaves over
$\mathbb{R}$, as well as a functor in the other direction. We also observe that
the 2-parameter persistence modules arising from the level sets of Morse
functions carry extra structure that we call a Mayer-Vietoris system. We prove
classification, barcode decomposition, and stability theorems for these
Mayer-Vietoris systems, and we show that the aforementioned functors establish
a pseudo-isometric equivalence of categories between derived constructible
sheaves with the convolution or (derived) bottleneck distance and the
interleaving distance of strictly pointwise finite-dimensional Mayer-Vietoris
systems. Ultimately, our results provide a functorial equivalence between
level-sets persistence and derived pushforward for continuous real-valued
functions.
</p></div>
    </summary>
    <updated>2019-07-24T23:26:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.09582</id>
    <link href="http://arxiv.org/abs/1907.09582" rel="alternate" type="text/html"/>
    <title>The $k$-Dimensional Weisfeiler-Leman Algorithm</title>
    <feedworld_mtime>1563926400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Immerman:Neil.html">Neil Immerman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sengupta:Rik.html">Rik Sengupta</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.09582">PDF</a><br/><b>Abstract: </b>In this note, we provide details of the $k$-dimensional Weisfeiler-Leman
Algorithm and its analysis from Immerman-Lander (1990). In particular, we
present an optimized version of the algorithm that runs in time $O(n^{k+1}\log
n)$, where $k$ is fixed (not varying with $n$).
</p></div>
    </summary>
    <updated>2019-07-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.09531</id>
    <link href="http://arxiv.org/abs/1907.09531" rel="alternate" type="text/html"/>
    <title>Between the deterministic and non-deterministic query complexity</title>
    <feedworld_mtime>1563926400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gerbner:D=aacute=niel.html">Dániel Gerbner</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.09531">PDF</a><br/><b>Abstract: </b>We consider problems that can be solved by asking certain queries. The
deterministic query complexity $D(P)$ of a problem $P$ is the smallest number
of queries needed to ask in order to find the solution (in the worst case),
while the non-deterministic query complexity $D_0(P)$ is the smallest number of
queries needed to ask, in case we know the solution, to prove that it is indeed
the solution (in the worst case). Equivalently, $D(P)$ is the largest number of
queries needed to find the solution in case an Adversary is answering the
queries, while $D_0(P)$ is the largest number of queries needed to find the
solution in case an Adversary chooses the input. We define a series of
quantities between these two values, $D_k(P)$ is the largest number of queries
needed to find the solution in case an Adversary chooses the input, and answers
the queries, but he can change the input at most $k$ times. We give bounds on
$D_k(P)$ for various problems $P$.
</p></div>
    </summary>
    <updated>2019-07-24T23:22:14Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.09049</id>
    <link href="http://arxiv.org/abs/1907.09049" rel="alternate" type="text/html"/>
    <title>Multiple Server SRPT with speed scaling is competitive</title>
    <feedworld_mtime>1563926400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vaze:Rahul.html">Rahul Vaze</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nair:Jayakrishnan.html">Jayakrishnan Nair</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.09049">PDF</a><br/><b>Abstract: </b>Can the popular shortest remaining processing time (SRPT) algorithm achieve a
constant competitive ratio on multiple servers when server speeds are
adjustable (speed scaling) with respect to the flow time plus energy
consumption metric? This question has remained open for a while, where a
negative result in the absence of speed scaling is well known. The main result
of this paper is to show that multi-server SRPT can be constant competitive,
with a competitive ratio that only depends on the power-usage function of the
servers, but not on the number of jobs/servers or the job sizes (unlike when
speed scaling is not allowed). When all job sizes are unity, we show that
round-robin routing is optimal and can achieve the same competitive ratio as
the best known algorithm for the single server problem. Finally, we show that a
class of greedy dispatch policies, including policies that route to the least
loaded or the shortest queue, do not admit a constant competitive ratio. When
job arrivals are stochastic, with Poisson arrivals and i.i.d. job sizes, we
show that random routing and a simple gated-static speed scaling algorithm
achieves a constant competitive ratio.
</p></div>
    </summary>
    <updated>2019-07-24T23:25:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/096</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/096" rel="alternate" type="text/html"/>
    <title>TR19-096 |  On the $\text{AC}^0[\oplus]$ complexity of Andreev&amp;#39;s Problem | 

	Aditya Potukuchi</title>
    <summary>Andreev's Problem asks the following: Given an integer $d$ and a subset of $S \subseteq \mathbb{F}_q \times \mathbb{F}_q$, is there a polynomial $y = p(x)$ of degree at most $d$ such that  for every $a \in \mathbb{F}_q$, $(a,p(a)) \in S$? We show an $\text{AC}^0[\oplus]$ lower bound for this problem. 

This problem appears to be similar to the list recovery problem for degree $d$-Reed-Solomon codes over $\mathbb{F}_q$ which asks the following: Given subsets $A_1,\ldots,A_q$ of $\mathbb{F}_q$, output all (if any) Reed-Solomon codewords contained in $A_1\times \cdots \times A_q$. For our purpose, we study this problem when $A_1, \ldots, A_q$ are random subsets of a given size, which may be of independent interest.</summary>
    <updated>2019-07-23T17:29:25Z</updated>
    <published>2019-07-23T17:29:25Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-25T15:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17642</id>
    <link href="https://gilkalai.wordpress.com/2019/07/23/matan-harel-frank-mousset-and-wojciech-samotij-and-the-the-infamous-upper-tail-problem/" rel="alternate" type="text/html"/>
    <title>Matan Harel, Frank Mousset, and Wojciech Samotij and the “the infamous upper tail” problem</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Let me report today on a major breakthrough in random graph theory and probabilistic combinatorics. Congratulations to Matan, Frank, and Vojtek! Artist: Heidi Buck. “Catch a Dragon by the Tail 2” ( source ) Upper tails via high moments and entropic … <a href="https://gilkalai.wordpress.com/2019/07/23/matan-harel-frank-mousset-and-wojciech-samotij-and-the-the-infamous-upper-tail-problem/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Let me report today on a major breakthrough in random graph theory and probabilistic combinatorics. Congratulations to Matan, Frank, and Vojtek!</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/07/dragon_tail_2_web.png"><img alt="" class="alignnone size-full wp-image-17644" height="496" src="https://gilkalai.files.wordpress.com/2019/07/dragon_tail_2_web.png?w=640&amp;h=496" width="640"/></a></p>
<p>Artist: Heidi Buck.<strong><span style="color: #ff0000;"> “Catch a Dragon by the Tail 2”</span></strong> ( <a href="https://www.hbdragon.com/main/content/catch-dragon-tail-2">source</a> )</p>
<p class="title mathjax"><a href="https://arxiv.org/abs/1904.08212">Upper tails via high moments and entropic stability</a> by Matan Harel, Frank Mousset, and Wojciech Samotij</p>
<p><strong>Abstract:</strong></p>
<p>Suppose that <em>X</em> is a bounded-degree polynomial with nonnegative coefficients on the <em>p</em>-biased discrete hypercube. Our main result gives sharp estimates on the logarithmic upper tail probability of X whenever an associated extremal problem satisfies a certain entropic stability property. We apply this result to solve two long-standing open problems in probabilistic combinatorics: the upper tail problem for the number of arithmetic progressions of a fixed length in the <em>p</em>-random subset of the integers and the upper tail problem for the number of cliques of a fixed size in the random graph <img alt="G_{n,p}" class="latex" src="https://s0.wp.com/latex.php?latex=G_%7Bn%2Cp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{n,p}"/>. We also make significant progress on the upper tail problem for the number of copies of a fixed regular graph <em>H</em> in <img alt="G_{n,p}." class="latex" src="https://s0.wp.com/latex.php?latex=G_%7Bn%2Cp%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{n,p}."/> To accommodate readers who are interested in learning the basic method, we include a short, self-contained solution to the upper tail problem for the number of triangles in <img alt="G_{n,p}" class="latex" src="https://s0.wp.com/latex.php?latex=G_%7Bn%2Cp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{n,p}"/> for all <em>p=p(n)</em> satisfying <img alt="\frac {\log n}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac+%7B%5Clog+n%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac {\log n}{n}"/> <img alt="\ll" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cll&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ll"/> <img alt="p \ll 1" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cll+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p \ll 1"/>.</p>
<p>The introduction gives very nicely the rich history of the problem.  Here is a 2002 paper by Svante Janson and Andrzej Ruciński on <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/rsa.10031?casa_token=7FoHUVq0w5IAAAAA:AqmK_4tpjDzXErWnGh4qnE2kn4NTupyoW27eSXW7Uwr5l0JgpBP5SG7PaVnJ6Lvr4JhCaSIH2HPv-QTsUg">the infamous upper tail</a>.  (And a lot have happened since then on this problem and on non linear large deviation theory). Following, there is a lovely section with a short solution for the case of triangles.</p>
<p>Forthcoming reference [48] talks about lower tails! Stay tuned!</p></div>
    </content>
    <updated>2019-07-23T06:51:02Z</updated>
    <published>2019-07-23T06:51:02Z</published>
    <category term="Combinatorics"/>
    <category term="Probability"/>
    <category term="Frank Mousset"/>
    <category term="Matan Harel"/>
    <category term="Wojciech Samotij"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-25T15:20:53Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7828284719883166611</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7828284719883166611/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/answer-to-both-infinite-hats-problems.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7828284719883166611" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7828284719883166611" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/answer-to-both-infinite-hats-problems.html" rel="alternate" type="text/html"/>
    <title>Answer to both Infinite Hats Problems from the last post</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
(This is a joint post with David Marcus. You'll see why later.)<br/>
<br/>
In a prior  I posed two infinite hat problems. Today I post the solutions. Actually this is a copy of my last post with the solutions added, so it is self contained.<br/>
<br/>
A Hat Problem that you have probably seen:<br/>
<br/>
1) There are an infinite number of people, numbered 1,2,3,...  There are 2 colors of hats. They can all see everyone's hat but their own. <br/>
<br/>
2) The adversary is going to put hats on all the people. They will guess their own hat color<i> at the same time</i>. <br/>
<br/>
3) The people can discuss strategy ahead of time, but must use a deterministic strategy and the adversary knows the strategy.<br/>
<br/>
4) The people want to minimize how many they get wrong. <br/>
<br/>
5) The adversary puts on hats to maximize how many they get wrong.<br/>
<br/>
I ask two questions (the answers are in a document I point to) and one meta-question:<br/>
<br/>
Q1: Is there a solution where they get all but a finite number of the guesses right? (If you have read my prior post on hat puzzles, <a href="https://blog.computationalcomplexity.org/2017/07/two-hat-problems-you-may-or-may-not.html">here</a> then you can do this one.) <br/>
<br/>
Q2: Is there a solution where they get all but at most (say) 18 wrong.<br/>
<br/>
<br/>
Answers to Q1 and Q2 are <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/infinitehats.pdf">here</a>.<br/>
<br/>
How did I get into this problem? I was looking at hat problems a while back. Then  I began discussing Q1 and Q2 by email  (Does the term <i>discussing</i> have as a default that it is by email?) with David Marcus who had just read the chapter of <a href="https://www.amazon.com/Problems-Point-Exploring-Computer-Science/dp/9813279974">Problems with a Point</a> on hat puzzles. After a few emails back and fourth, he began looking on the web for answers. He found one. There is a website of hat puzzles! It was MY website papers on  Hat Puzzles! It is  <a href="https://www.cs.umd.edu/users/gasarch/TOPICS/hats/hats.html">here</a>. And on it was a relevant paper <a href="https://www.cs.umd.edu/users/gasarch/TOPICS/hats/infinite-hats-and-ac.pdf">here</a>. We did not find any other source of the problem or its solution. <br/>
<br/>
Q3: How well known is problem Q2 and the solution?  I've seen Q1 around but the only source on Q2 that I know of is that paper, and now this blog post. So, please leave a comment telling me if you have seen Q2 and/or the solution someplace else, and if so where.<br/>
<br/>
The responses to my last post indicated that YES the problem was out there, but the proof that you could not get all-but-18 was not well known. <br/>
<br/>
I THINK that all of the proofs that you can't do all-but-18 in the comment of the last post were essentially the same as the solution I pointed to in this blog. I would be interested if there is an alternative proof. <br/></div>
    </content>
    <updated>2019-07-22T03:00:00Z</updated>
    <published>2019-07-22T03:00:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-07-25T15:03:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/095</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/095" rel="alternate" type="text/html"/>
    <title>TR19-095 |  Unambiguous Catalytic Computation | 

	Chetan Gupta, 

	Rahul Jain, 

	Vimal Raj Sharma, 

	Raghunath Tewari</title>
    <summary>The catalytic Turing machine is a model of computation defined by Buhrman, Cleve,
Kouck, Loff, and Speelman (STOC 2014). Compared to the classical space-bounded Turing
machine, this model has an extra space which is filled with arbitrary content in addition
to the clean space. In such a model we study if this additional filled space can be used to
increase the power of computation or not, with the condition that the initial content of this
extra filled space must be restored at the end of the computation.
In this paper, we define the notion of unambiguous catalytic Turing machine and prove
that under a standard derandomization assumption, the class of problems solved by an
unambiguous catalytic Turing machine is same as the class of problems solved by a general
nondeterministic catalytic Turing machine in the logspace setting.</summary>
    <updated>2019-07-21T11:21:02Z</updated>
    <published>2019-07-21T11:21:02Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-25T15:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17574</id>
    <link href="https://gilkalai.wordpress.com/2019/07/20/isabella-novik-and-hailun-zheng-neighborly-centrally-symmetric-spheres-exist-in-all-dimensions/" rel="alternate" type="text/html"/>
    <title>Isabella Novik and Hailun Zheng: Neighborly centrally symmetric spheres exist in all dimensions!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A twit long summary: The cyclic polytope is wonderful and whenever we construct an analogous object we are happy. Examples: Neighborly cubic polytopes; The amplituhedron; and as of last week, the Novik-Zheng new construction of neighborly centrally symmetric spheres! At … <a href="https://gilkalai.wordpress.com/2019/07/20/isabella-novik-and-hailun-zheng-neighborly-centrally-symmetric-spheres-exist-in-all-dimensions/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="color: #0000ff;">A twit long summary: The cyclic polytope is wonderful and whenever we construct an analogous object we are happy. Examples: Neighborly cubic polytopes; The amplituhedron; and as of last week, the Novik-Zheng new construction of neighborly centrally symmetric spheres!</span></p>
<h2>At last: Neighborly CS spheres!</h2>
<p>The news: Isabella Novik and Hailun Zheng’ paper  <a href="https://arxiv.org/abs/1907.06115">Highly neighborly centrally symmetric spheres</a>, resolves an old standing problem in this field.</p>
<p>Here is the abstract:</p>
<blockquote><p>In 1995, Jockusch constructed an infinite family of centrally symmetric 3-dimensional simplicial spheres that are cs-<em>2</em>-neighborly. Here we generalize his construction and show that for all d ≥ 4 and n ≥ d, there exists a centrally symmetric (d − 1)-dimensional simplicial sphere with <em>2n</em> vertices that is cs-[d/2]-neighborly. This result combined with work of Adin and Stanley completely resolves the upper bound problem for centrally symmetric simplicial spheres.</p></blockquote>
<p>Congratulations to Isabella and Hailun!</p>
<h2>Some background to the Novik and Zheng breakthrough</h2>
<p><strong>Centrally symmetric bodies:</strong> A centrally symmetric (cs) polytope convex body in <img alt="\mathbb R^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb R^d"/> satisfies <img alt="x \in P" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in P"/> implies <img alt="-x \in P" class="latex" src="https://s0.wp.com/latex.php?latex=-x+%5Cin+P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="-x \in P"/>. Centrally symmetric bodies are the unit balls of normed spaces.</p>
<p><strong>Centrally symmetric simplicial spheres:</strong> A triangulation <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> of a  <img alt="(d-1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28d-1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(d-1)"/>-dimensional sphere with a set <img alt="V" class="latex" src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V"/> of vertices is centrally symmetric if there is an involution <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/> on <img alt="V" class="latex" src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V"/> that <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/> maps a face of <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> to a face of <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> and for every vertex <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/>, <img alt="\phi(v) \ne v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28v%29+%5Cne+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(v) \ne v"/> and <img alt="\{v , \phi (v)\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bv+%2C+%5Cphi+%28v%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{v , \phi (v)\}"/> is not an edge of <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/>.  The boundary complex of a cs <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-polytopes is a cs triangulation of <img alt="S^{d-1}" class="latex" src="https://s0.wp.com/latex.php?latex=S%5E%7Bd-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S^{d-1}"/>.</p>
<p><strong>Neighborliness.</strong> A simplicial complex <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/>  is <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/>-neighborly of every set of <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> vertices of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> form a face.  (The definition was first considered  for simplicial <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-polytopes <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/>.) The cyclic <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-polytope with <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> vertices is <img alt="[d/2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%2F2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[d/2]"/>-neighborly. (The only (<img alt="[d/2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%2F2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[d/2]"/>+1)-neighborly simplicial <img alt="(d-1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28d-1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(d-1)"/>-sphere is the simplex. There are many other <img alt="[d/2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%2F2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[d/2]"/>-neighborly simplicial <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-polytopes and <img alt="(d-1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28d-1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(d-1)"/>-spheres.</p>
<p><strong>cs-Neighborliness. </strong>Let <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> be a simplicial complex with an involution <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/> on its vertices which acts on <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> (maps faces to faces) and has the property that <img alt="\phi(v)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28v%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(v)"/> is not adjacent to <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> (and <img alt="\phi (v) \ne v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi+%28v%29+%5Cne+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi (v) \ne v"/>). We will call <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> and <img alt="\phi (v)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi+%28v%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi (v)"/> <strong>antipodal</strong>. <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> is cs-<img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/>-neighborly if every set of <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> vertices that contains no pair of antipodal vertices is a face of <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/>. The only cs-<img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/>-neighborly simplicial sphere is the boundary complex of the cross polytope.</p>
<p><strong>The existence of cs-<img alt="[d/2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%2F2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[d/2]"/>-neighborly spheres. </strong>It was an important open question to understand if cs <img alt="[d/2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%2F2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[d/2]"/>-neighborly simplicial spheres exist. (The only cs-<img alt="([d/2]+1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Bd%2F2%5D%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="([d/2]+1)"/>-neighborly spheres is the boundary complex of the cross polytope.)  The first example (whoch is not a cross polytope) was given by Grunbaum in 1969 in his paper “The importance of being straight(?).” In 1995  Jockusch constructed an infinite family cs-2-neighborly centrally symmetric 3-dimensional simplicial spheres. This problem has now been solved by Novik and Zheng.</p>
<p><strong>Neighborly centrally symmetric polytopes.  </strong>In the 1960s Grünbaum noted the big difference between neighborly centrally symmetric spheres and centrally symmetric polytopes. He proved (This is Theorem 4.1 in his book “Convex polytopes”) that no cs-2-neighborly 4 polytope with 12 vertices exists.  This is an example of the important themes of “streightening” or “linearizing” combimatorial objects and  of extending theorems from the “streight” or “linear” case to more general combinatorial settings.)</p>
<p>This result by Grünbaum was extended in various directions. Let me mention two major results in the field:</p>
<p><strong>Theorem</strong> McMullen and Shephard (1968): A cs <em>d</em>-dimensional polytope with <em>2(d + 2)</em> vertices cannot be more than cs-<em>⌊(d + 1)/3⌋</em>-neighborly.</p>
<p><strong>Theorem</strong> <a href="https://arxiv.org/abs/math/0507280">Linial and Novik (2006):</a> A cs-2-neighborly <em>d</em>-dimensional polytope has at most  <img alt="2^d" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^d"/>;</p>
<p>Novik (2017) <a href="https://arxiv.org/abs/1712.09489">constructed</a>  cs-2-neighborly d polytopes with <img alt="2^{d-1}+1" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7Bd-1%7D%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{d-1}+1"/> vertices. She used a 2017 <a href="https://arxiv.org/abs/1709.03411">breakthrough construction</a> by Gerencsér–Harang of an acute set of size <img alt="2^{d-1}+1" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7Bd-1%7D%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{d-1}+1"/> in <img alt="\mathbb R^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb R^d"/>. (A set <em>S</em> is <em>acute</em> if every three points from <em>S </em>determine an acute triangle.)</p>
<p><strong>Face numbers of centrally-symmetric polytopes and spheres. </strong>As the abstract asserts the new construction is related to questions about face numbers of centrally symmetric polytopes, spheres and other cellular objects. In fact, this was the next item in our planned posts on algebraic combinatorics of cellular objects. (The first and only post so far<a href="https://gilkalai.wordpress.com/2018/06/20/beyond-the-g-conjecture-algebraic-combinatorics-of-cellular-spaces-i/"> is here</a>.) Here is a recent survey by Isabella Novik  <a href="https://arxiv.org/abs/1711.09310">A tale on centrally symmetric  polytopes and spheres</a>.</p>
<p><strong>Upper bound theorems.</strong> Neighborly polytopes and spheres are the equality cases of the upper bound theorem (proved by McMullen for polytopes and by Stanley for spheres). A version of the upper bound inequality for centrally symmetric spheres was proved by Adin and Stanley and the new construction shows that the Adin-Stanley inequality is tight. For more on the upper bound theorem and neighborliness see Section 2 of my 2000 survey  <a href="http://www.ma.huji.ac.il/~kalai/VIS.pdf">Combinatorics with geometric flavor. </a> See also the post <a href="https://gilkalai.wordpress.com/2009/04/04/how-the-g-conjecture-came-about/">How the g-conjecture came about</a> and  and the post <a href="https://gilkalai.wordpress.com/2013/09/10/how-the-proof-of-the-upper-bound-theorem-for-spheres-was-found/" rel="bookmark">Richard Stanley: How the Proof of the Upper Bound Theorem (for spheres) was Found.</a></p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2019-07-20T18:33:03Z</updated>
    <published>2019-07-20T18:33:03Z</published>
    <category term="Combinatorics"/>
    <category term="Convexity"/>
    <category term="Hailun Zheng"/>
    <category term="Isabella Novik"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-25T15:20:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4267</id>
    <link href="https://www.scottaaronson.com/blog/?p=4267" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4267#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4267" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Fake it till you make it (to the moon)</title>
    <summary xml:lang="en-US">While I wait to board a flight at my favorite location on earth—Philadelphia International Airport—I figured I might as well blog something to mark the 50th anniversary of Apollo 11. (Thanks also to Joshua Zelinsky for a Facebook post that inspired this.) I wasn’t alive for Apollo, but I’ve been alive for 3/4 of the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>While I wait to board a flight at my favorite location on earth—Philadelphia International Airport—I figured I might as well blog something to mark the 50<sup>th</sup> anniversary of Apollo 11.  (Thanks also to Joshua Zelinsky for a Facebook post that inspired this.)</p>



<p>I wasn’t alive for Apollo, but I’ve been alive for 3/4 of the time <em>after</em> it, even though it now seems like ancient history—specifically, like a Roman cathedral being gawked at by a medieval peasant, like an achievement by some vanished, more cohesive civilization that we can’t even replicate today, let alone surpass.</p>



<p>Which brings me to a depressing mystery: why do so many people now deny that humans walked on the moon at all?  Like, why <em>that</em> specifically?  While they’re at it, why don’t they also deny that WWII happened, or that the Beatles existed?</p>



<p>Surprisingly, skepticism of the reality of Apollo seems to have gone all the way back to the landings themselves.  One of my favorite stories growing up was of my mom, as a teenager, working as a waitress at an Israeli restaurant in Philadelphia, on the night of Apollo 11 landing.  My mom asked for a few minutes off to listen to news of the landing on the radio.  The owners wouldn’t grant it—explaining that it was all Hollywood anyway, just some actors in spacesuits on a sound stage, and obviously my mom wasn’t so naïve as to think anyone was <em>actually</em> walking to the moon?</p>



<p>Alas, as we get further and further from the event, with no serious prospect of ever replicating it past the stage of announcing an optimistic timetable (nor, to be honest, any scientific <em>reason</em> to replicate it), as the people involved die off, and as our civilization becomes ever more awash in social-media-fueled paranoid conspiracies, I fear that moon-landing denalism will become more common.</p>



<p>Because here’s the thing: Apollo could happen, but <em>only</em> because of a wildly improbable, once-in-history confluence of social and geopolitical factors.  It was economically insane, taking 100,000 people and 4% of the US federal budget for some photo-ops, a flag-planting, some data and returned moon rocks that had genuine scientific value but could’ve been provided much more cheaply by robots.  It was dismantled immediately afterwards like a used movie set, rather than leading to any greater successes. Indeed, manned spaceflight severely <em>regressed</em> afterwards, surely mocking the expectations of every last science fiction fan and techno-utopian who was alive at that time.</p>



<p>One could summarize the situation by saying that, in certain respects, the Apollo program really <strong>was</strong> “faked.”  It’s just that the way they “faked” it, involved actually landing people on the moon!</p></div>
    </content>
    <updated>2019-07-19T21:40:43Z</updated>
    <published>2019-07-19T21:40:43Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Procrastination"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-07-19T23:00:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blogs.princeton.edu/imabandit/?p=1397</id>
    <link href="https://blogs.princeton.edu/imabandit/2019/07/17/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-ii/" rel="alternate" type="text/html"/>
    <title>Guest post by Julien Mairal: A Kernel Point of View on Convolutional Neural Networks, part II</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is a continuation of Julien Mairal‘s guest post on CNNs, see part I here. Stability to deformations of convolutional neural networks In their ICML paper Zhang et al. introduce a functional space for CNNs with one layer, by noticing … <a href="https://blogs.princeton.edu/imabandit/2019/07/17/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-ii/">Continue reading <span class="meta-nav">→</span></a></p></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="liimagelink" href="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/Mairal2.jpg?ssl=1"><img alt="" class="alignnone wp-image-1399" height="324" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/Mairal2.jpg?resize=639%2C324&amp;ssl=1" width="639"/></a></p>
<p>This is a continuation of <a class="liinternal" href="https://lear.inrialpes.fr/people/mairal/">Julien Mairal</a>‘s guest post on CNNs, see <a class="liinternal" href="https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/">part I here.</a></p>
<p><strong>Stability to deformations of convolutional neural networks</strong></p>
<p>In their <a class="lipdf" href="http://proceedings.mlr.press/v70/zhang17f/zhang17f.pdf">ICML paper</a> Zhang et al. introduce a functional space for CNNs with one layer, by noticing that for some dot-product kernels, smoothed variants of rectified linear unit activation functions (ReLU) live in the corresponding RKHS, see also <a class="lipdf" href="http://proceedings.mlr.press/v48/zhangd16.pdf">this paper</a> and <a class="lipdf" href="https://www.cs.cornell.edu/~sridharan/sicomp.pdf">that one</a>. By following a similar reasoning with multiple layers, it is then possible to show that the functional space described in <a class="liinternal" href="https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/">part I</a> <img alt="\{ f_w: x \mapsto \langle w , \Phi_n(x_0) \rangle; w \in L^2(\Omega,\mathcal{H}_n) \}" class="ql-img-inline-formula " height="20" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3e321e5f0406c9879f25b6b1d69a5fc3_l3.png?resize=298%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="298"/> contains CNNs with such smoothed ReLU, and that the norm <img alt="\|f_w\|" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-62e1a48032624994ba16c4e26421676e_l3.png?resize=34%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="34"/> of such networks can be controlled by the spectral norms of filter matrices. This is consistent with previous measures of complexity for CNNs, see <a class="lipdf" href="https://papers.nips.cc/paper/7204-spectrally-normalized-margin-bounds-for-neural-networks.pdf">this paper</a> by Bartlett et al.</p>
<p>A perhaps more interesting finding is that the abstract representation <img alt="\Phi_n(x)" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ca9a5351b772e88bebe85e7e4a13632_l3.png?resize=45%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="45"/>, which only depends on the network architecture, may provide near-translation invariance and stability to small image deformations while preserving information—that is, <img alt="x" class="ql-img-inline-formula " height="8" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b9fbfb207b6d17d74b33c6d8342a1a4_l3.png?resize=10%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="10"/> can be recovered from <img alt="\Phi_n(x)" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ca9a5351b772e88bebe85e7e4a13632_l3.png?resize=45%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="45"/>. The original characterization we use was introduced by Mallat in <a class="lipdf" href="https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf">his paper</a> on the scattering transform—a multilayer architecture akin to CNNs based on wavelets, and was extended to <img alt="\Phi_n" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-273242b8e92b3a9f4dc13c62b2785bd3_l3.png?resize=21%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="21"/> by Alberto Bietti, who should be credited for all the hard work here.</p>
<p>Our goal is to understand under which conditions it is possible to obtain a representation that (i) is near-translation invariant, (ii) is stable to deformations, (iii) preserves signal information. Given a <img alt="C^1" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2e9ea203bbd77c5cd8bee967e2729d8b_l3.png?resize=20%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="20"/>-diffeomorphism <img alt="\tau: \mathbb{R}^2 \to \mathbb{R}^2" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c6f8f1dde2ee4682653c2a6b37d8a42d_l3.png?resize=93%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="93"/> and denoting by <img alt="L_\tau x(u) = x(u-\tau(u))" class="ql-img-inline-formula " height="18" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-835d9f864f712213ee317332b3f3675a_l3.png?resize=167%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="167"/> its action operator (for an image defined on the continuous domain <img alt="\mathbb{R}^2" class="ql-img-inline-formula " height="15" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d5abe0f29e8cc710ae26f4f0af5a0859_l3.png?resize=20%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="20"/>), the main stability bound we obtain is the following one, see Theorem 7 in <a class="lipdf" href="https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf">Mallat’s paper</a> if <img alt="\|\nabla \tau\|_\infty \leq 1/2" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-758b3cac273166048ed1879acf427860_l3.png?resize=104%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="104"/>, for all <img alt="x" class="ql-img-inline-formula " height="8" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b9fbfb207b6d17d74b33c6d8342a1a4_l3.png?resize=10%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="10"/>,</p>
<p class="ql-center-displayed-equation" style="line-height: 43px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \| \Phi_n(L_\tau x) - \Phi_n(x)\| \leq \left ( C_1 (1+n) \|\nabla \tau\|_\infty + \frac{C_2}{\sigma_n} \|\tau\|_\infty \right) \|x\|, \]" class="ql-img-displayed-equation " height="43" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-51041d0067a72066938e31b1f00529fa_l3.png?resize=455%2C43&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="455"/></p>
<p>where <img alt="C_1, C_2" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-08d1f29fa9c0981e916619b6c6bc7eee_l3.png?resize=48%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="48"/> are universal constants, <img alt="\sigma_n" class="ql-img-inline-formula " height="11" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c0142846a2999e170f7beec7be1523f2_l3.png?resize=18%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="18"/> is the scale parameter of the pooling operator <img alt="A_n" class="ql-img-inline-formula " height="15" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e1872c7d7a65e0dc92f8a4a04608b88a_l3.png?resize=21%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="21"/> corresponding to the “amount of pooling” performed up to the last layer, <img alt="\|\tau\|_\infty" class="ql-img-inline-formula " height="19" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c1a5effb150d36de3c7074eaa980c357_l3.png?resize=39%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="39"/> is the maximum pixel displacement and <img alt="\|\nabla \tau\|_\infty" class="ql-img-inline-formula " height="19" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ab4c5d3fe8fd25af25beb4f58a55c938_l3.png?resize=53%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="53"/> represents the maximum amount of deformation, see <a class="lipdf" href="https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf">the paper</a> for the precise definitions of all these quantities. Note that when <img alt="C_2/\sigma_n \to 0" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b732bf857c5f04c7d10dda247f1a5022_l3.png?resize=85%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="85"/>, the representation <img alt="\Phi_n" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-273242b8e92b3a9f4dc13c62b2785bd3_l3.png?resize=21%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="21"/> becomes translation invariant: indeed, consider the particular case of <img alt="\tau" class="ql-img-inline-formula " height="8" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3af6c51247895b176bb502f0ee0857ee_l3.png?resize=10%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="10"/> being a translation, then <img alt="\nabla \tau=0" class="ql-img-inline-formula " height="14" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-aa1278a7149925a4f299de0dbb85cec0_l3.png?resize=57%2C14&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="57"/> and <img alt="\|\Phi_n(L_\tau x) - \Phi_n(x)\| \to 0" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cd1d650abd9970e357384c0653960577_l3.png?resize=186%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="186"/>.</p>
<p>The stability bound and a few additional results tell us a few things about the network architecture: (a) small patches lead to more stable representations (the dependency is hidden in <img alt="C_1" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-782c65cbd411fb8862688afc92bc1eea_l3.png?resize=19%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="19"/>); (b) signal preservation for discrete signals requires small subsampling factors (and thus small pooling) between layers. In such a setting, the scale parameter <img alt="\sigma_n" class="ql-img-inline-formula " height="11" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c0142846a2999e170f7beec7be1523f2_l3.png?resize=18%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="18"/> still grows exponentially with <img alt="n" class="ql-img-inline-formula " height="8" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a63eb5ff0272d3119fa684be6e7acce8_l3.png?resize=11%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> and near translation invariance may be achieved with several layers.</p>
<p>Interestingly, we may now come back to the Cauchy-Schwarz inequality from part 1, and note that if <img alt="\Phi_n" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-273242b8e92b3a9f4dc13c62b2785bd3_l3.png?resize=21%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="21"/> is stable, the RKHS norm <img alt="\|f\|" class="ql-img-inline-formula " height="19" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-afe70184469e7e3a14405a7193eedf29_l3.png?resize=24%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="24"/> is then a natural quantity that provides stability to deformations to the prediction function <img alt="f" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png?resize=10%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="10"/>, in addition to measuring model complexity in a traditional sense.</p>
<p><strong>Feature learning in RKHSs and convolutional kernel networks</strong></p>
<p>The previous paragraph is devoted to the characterization of convolutional architectures such as CNNs but the previous kernel construction can in fact be used to derive more traditional kernel methods. After all, why should one spend efforts defining a kernel between images if not to use it?</p>
<p>This can be achieved by considering finite-dimensional approximations of the previous feature maps. In order to shorten the presentation, we simply describe the main idea based on the Nystrom approximation and refer to <a class="lipdf" href="http://papers.nips.cc/paper/6184-end-to-end-kernel-learning-with-supervised-convolutional-kernel-networks.pdf">the paper</a> for more details. Approximating the infinite-dimensional feature maps <img alt="x_k" class="ql-img-inline-formula " height="11" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ad23c5c360c3f33031a5d000d37416f_l3.png?resize=17%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> (see the figure at the top of <a class="liinternal" href="https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/">part I</a>) can be done by projecting each point in <img alt="\mathcal{H}_k" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc26f0de4084a72b9e625a080bd5d674_l3.png?resize=22%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="22"/> onto a <img alt="p_k" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5c72bc331dc0008f57d454e7071dc39e_l3.png?resize=17%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/>-dimensional subspace <img alt="\mathcal{F}_k" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b08b176c0bf0adbd9cbe41b31147e1f7_l3.png?resize=20%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="20"/> leading to a finite-dimensional feature map <img alt="\tilde{x}_k" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5f55b75318f3b8da67917ee0b0e190ce_l3.png?resize=17%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> akin to CNNs, see the figure at the top of the post.</p>
<p>By parametrizing <img alt="\mathcal{F}_k=\text{span}(\varphi_k(z_1),\varphi_k(z_2),\ldots,\varphi_k(z_{p_k}))" class="ql-img-inline-formula " height="20" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5dd8802df8efddb9acc5056af47339d7_l3.png?resize=297%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="297"/> with <img alt="p_k" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5c72bc331dc0008f57d454e7071dc39e_l3.png?resize=17%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> anchor points <img alt="Z=[z_1,\ldots,z_{p_k}]" class="ql-img-inline-formula " height="19" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4614e1cdba47dc6a6db7957fb1d82632_l3.png?resize=123%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="123"/>, and using a dot-product kernel, a patch <img alt="z" class="ql-img-inline-formula " height="8" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d9d772a59543419785ce66946592259a_l3.png?resize=9%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="9"/> from <img alt="\tilde{x}_{k-1}" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6419748397a324cd2a2ebc3f119b7f80_l3.png?resize=35%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/> is encoded through the mapping function</p>
<p class="ql-center-displayed-equation" style="line-height: 43px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \psi_k(z) = \|z\| \kappa_k( Z^\top Z)^{-1/2} \kappa_k\left( Z^\top \frac{z}{\|z\|} \right), \]" class="ql-img-displayed-equation " height="43" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cc43da382f024d96cb50e3dc3f051d6f_l3.png?resize=306%2C43&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="306"/></p>
<p>where <img alt="\kappa_k" class="ql-img-inline-formula " height="11" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-684fcf23472c51919624049fb4e0129a_l3.png?resize=17%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> is applied pointwise. Then, computing <img alt="\tilde{x}_k" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5f55b75318f3b8da67917ee0b0e190ce_l3.png?resize=17%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> from <img alt="\tilde{x}_{k-1}" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6419748397a324cd2a2ebc3f119b7f80_l3.png?resize=35%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/> admits a CNN interpretation, where only the normalization and the matrix multiplication by <img alt="\kappa_k( Z^\top Z)^{-1/2}" class="ql-img-inline-formula " height="21" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a74cffbbd51922298a13f864fbedaa98_l3.png?resize=103%2C21&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="103"/> are not standard operations. It remains now to choose the anchor points:</p>
<ul>
<li><strong>kernel approximation:</strong> a first approach consists of using a variant of the Nystrom method, see <a class="lipdf" href="https://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines.pdf">this paper</a> and <a class="lipdf" href="http://home.cse.ust.hk/~twinsen/nystrom.pdf">that one</a>. When plugging the corresponding image representation in a linear classifier, the resulting approach behaves as a classical kernel machine. Empirically, we observe that the higher the number of anchor points, the better the kernel approximation, and the higher the accuracy. For instance, a two-layer network with a <img alt="300k" class="ql-img-inline-formula " height="13" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-81a4466abb5fecba81f8a3aa055a1a14_l3.png?resize=36%2C13&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="36"/>-dimensional representations achieves about <img alt="86\%" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0eea28372ada596bc618b4b94fee69ec_l3.png?resize=32%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="32"/> accuracy on CIFAR-10 without data augmentation (see <a class="liinternal" href="https://gitlab.inria.fr/mairal/ckn-cudnn-matlab">here</a>).</li>
<li><strong>back-propagation, feature selection</strong>: learning the anchor points <img alt="Z" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cc9f8fff9fd24060bc054e78f01d5bfb_l3.png?resize=12%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="12"/> can also be done as in a traditional CNN, by optimizing them end-to-end. This allows using deeper lower-dimensional architectures and empirically seems to perform better when enough data is available, e.g., <img alt="92\%" class="ql-img-inline-formula " height="15" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-659ab3cccda2422f955af880d20646cf_l3.png?resize=32%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="32"/> accuracy on CIFAR-10 with simple data augmentation. There, the subspaces <img alt="\mathcal{F}_k" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b08b176c0bf0adbd9cbe41b31147e1f7_l3.png?resize=20%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="20"/> are not learned anymore to provide the best kernel approximation, but the model seems to perform a sort of feature selection in each layer’s RKHS <img alt="\mathcal{H}_k" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc26f0de4084a72b9e625a080bd5d674_l3.png?resize=22%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="22"/>, which is not well understood yet (This feature selection interpretation is due to my collaborator Laurent Jacob).</li>
</ul>
<p>Note that the first CKN model published <a class="lipdf" href="https://papers.nips.cc/paper/5348-convolutional-kernel-networks.pdf">here</a> was based on a different approximation principle, which was not compatible with end-to-end training. We found this to be less scalable and effective.</p>
<p><strong>Other links between neural networks and kernel methods</strong></p>
<p>Finally, other links between kernels and infinitely-wide neural networks with random weights are classical, but they were not the topic of this blog post (they should be the topic of another one!). In a nutshell, for a large collection of weights distributions and nonlinear functions <img alt="s: \mathbb{R} \to \mathbb{R}" class="ql-img-inline-formula " height="13" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b4680e3f9e8274687d2d04f0a262ed00_l3.png?resize=76%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="76"/>, the following quantity admits an analytical form</p>
<p class="ql-center-displayed-equation" style="line-height: 22px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ K(x,x') = \E_{w}[ s(w^\top x) s(w^\top x')], \]" class="ql-img-displayed-equation " height="22" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9144f2d0b847adb69db90629ed805148_l3.png?resize=229%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="229"/></p>
<p>where the terms <img alt="s(w^\top x)" class="ql-img-inline-formula " height="19" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-da82e444bbc3a5e594b7edbf0b1ba3a0_l3.png?resize=56%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="56"/> may be seen as an infinitely-wide single-layer neural network. The first time such a relation appears is likely to be in <a class="liexternal" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf">the PhD thesis</a> of Radford Neal with a Gaussian process interpretation, and it was revisited later by <a class="lipdf" href="http://proceedings.mlr.press/v2/leroux07a/leroux07a.pdf">Le Roux and Bengio</a> and by <a class="lipdf" href="http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">Cho and Saul</a> with multilayer models.</p>
<p>In particular, when <img alt="s" class="ql-img-inline-formula " height="8" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3bcfb3f0b6b04be3b598743cd774dd78_l3.png?resize=8%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="8"/> is the rectified linear unit and <img alt="w" class="ql-img-inline-formula " height="8" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-78d46af3f19bae0d88ac0cabd450a296_l3.png?resize=13%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="13"/> follows a Gaussian distribution, it is known that we recover the arc-cosine kernel. We may also note that <a class="lipdf" href="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">random Fourier features</a> also yield a similar interpretation.</p>
<p>Other important links have also been drawn recently between kernel regression and strongly over-parametrized neural networks, see <a class="lipdf" href="http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">this paper</a> and <a class="lipdf" href="https://arxiv.org/pdf/1812.07956.pdf">that one</a>, which is another exciting story.</p></div>
    </content>
    <updated>2019-07-17T16:02:03Z</updated>
    <published>2019-07-17T16:02:03Z</published>
    <category term="Machine learning"/>
    <author>
      <name>Sebastien Bubeck</name>
    </author>
    <source>
      <id>https://blogs.princeton.edu/imabandit</id>
      <link href="https://blogs.princeton.edu/imabandit/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blogs.princeton.edu/imabandit" rel="alternate" type="text/html"/>
      <subtitle>Random topics in optimization, probability, and statistics. By Sébastien Bubeck</subtitle>
      <title>I’m a bandit</title>
      <updated>2019-07-24T23:29:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://minimizingregret.wordpress.com/?p=207</id>
    <link href="https://minimizingregret.wordpress.com/2019/07/17/boosting-for-dynamical-systems/" rel="alternate" type="text/html"/>
    <title>Boosting for Dynamical Systems</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">by Nataly Brukhim, Naman Agarwal and Elad Hazan, based on this paper  In a famous 1906 competition at a local fair in Plymouth, England, participants were asked to guess the weight of an ox. Out of a crowd of hundreds, no one came close to the ox’s actual weight, but the average of all guesses was … <a class="more-link" href="https://minimizingregret.wordpress.com/2019/07/17/boosting-for-dynamical-systems/">Continue reading <span class="screen-reader-text">Boosting for Dynamical Systems</span> <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>by Nataly Brukhim, Naman Agarwal and Elad Hazan, based on <a href="https://arxiv.org/abs/1906.08720">this paper</a> </em></p>
<p>In a famous 1906 competition at a local fair in Plymouth, England, participants were asked to guess the weight of an ox. Out of a crowd of hundreds, no one came close to the ox’s actual weight, but the average of all guesses was almost correct. How is it that combining the opinions of laymen can somehow arrive at highly reasoned decisions, despite the weak judgment of individual members? This concept of harnessing wisdom from weak rules of thumb to form a highly accurate prediction rule, is the basis of ensemble methods and <b>boosting</b>. Boosting is a theoretically sound methodology that has transformed machine learning across a variety of applications; in classification and regression tasks, online learning, and many more.</p>
<p>In the case of online learning, examples for training a predictor are not available in advance, but are revealed one at a time. Online boosting combines a set of online prediction rules, or <i>weak learners. </i>At every time step, each weak learner outputs a prediction, suffers some loss and is then updated accordingly. The performance of an online learner is measured using the <i>regret</i> criterion, which compares the accumulated loss over time with that of the best fixed decision in hindsight. A <i>Boosting</i> algorithm can choose which examples are fed to each of the weak learners, as well as the losses they incur. Intuitively, the online booster can encourage some weak learners to become really good in predicting certain common cases, while allowing others to focus on edge cases that are harder to predict. Overall, the <a href="http://proceedings.mlr.press/v37/beygelzimer15.pdf">online</a> <a href="https://arxiv.org/abs/1506.04820">boosting</a> framework can achieve low regret guarantees based on the learners’ individual regret values.</p>
<p>However, online learning can become more challenging when our actions have consequences on the environment. This can be illustrated with the following experiment: imagine learning to balance a long pole on your hand. When you move your hand slightly, the pole tilts. You then move your hand in the opposite direction, and it bounces back and tilts to the other side. One jerk the wrong way might have you struggling for a good few seconds to rebalance. In other words, a <u>sequence of decisions</u> you made earlier determines whether or not the pole is balanced at any given time, rather than the single decision you make at that point.<img alt="" class=" aligncenter" height="129" src="https://minimizingregret.files.wordpress.com/2019/07/image.jpeg?w=136&amp;h=129" title="" width="136"/></p>
<p>More generally, consider cases when our environment has a <b>state, </b>and is in some sense “remembering” our past choices. A stateful framework, able to model a wide range of such phenomena, is a <i>dynamical system</i>. A dynamical system can be thought of as a function that determines, given the current state, what the state of the system will be in the next time step. Think of the physical dynamics that determines our pole’s position based on sequential hand movements. Other intuitive examples are the fluctuations of stock prices in the stock market, or the local weather temperatures; these can all be modeled with dynamical systems.</p>
<p>So how can boosting help us make better predictions for a dynamical system? In <a href="https://arxiv.org/abs/1906.08720">recent work</a> we propose an algorithm, which we refer to as DynaBoost, that achieves this goal. In the paper we provide theoretical regret bounds, as well as an empirical evaluation in a variety of applications, such as online control and time-series prediction.</p>
<p><b>Learning for Online Control</b></p>
<p>Control theory is a field of applied mathematics that deals with the control of various physical processes and engineering systems. The objective is to design an action rule, or <i>controller</i>, for a dynamical system such that steady state values are achieved quickly, and the system maintains stability.</p>
<p>Consider a simple Linear Dynamical System (LDS):</p>
<p style="text-align: center;"><img alt="x_{t+1} = A x_t + B u_t + w_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+%3D+A+x_t+%2B+B+u_t+%2B+w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x_{t+1} = A x_t + B u_t + w_t"/></p>
<p>where <img alt="x_t,u_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_t%2Cu_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x_t,u_t"/> are the state and control values at time t, respectively. Assume a known transition dynamics specified by the matrices A and B, and an arbitrary disturbance to the system given by <img alt="w_t" class="latex" src="https://s0.wp.com/latex.php?latex=w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w_t"/>. The goal of the controller is to minimize a convex cost function <img alt="c_t(x_t,u_t)" class="latex" src="https://s0.wp.com/latex.php?latex=c_t%28x_t%2Cu_t%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_t(x_t,u_t)"/>.</p>
<p>A provably optimal controller for the Gaussian noise case (where <img alt="w_t" class="latex" src="https://s0.wp.com/latex.php?latex=w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w_t"/> are normally distributed) and when the cost functions are quadratic, is the Linear Quadratic Regulator (LQR). LQR computes a pre-fixed matrix K such that <img alt="u_t^{LQR} = K x_t" class="latex" src="https://s0.wp.com/latex.php?latex=u_t%5E%7BLQR%7D+%3D+K+x_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="u_t^{LQR} = K x_t"/>. In other words, LQR computes a linear controller – which linearly maps the state into a control at every time step.</p>
<p>A <a href="http://proceedings.mlr.press/v97/agarwal19c/agarwal19c.pdf">recent advancement</a> in online control considers <i>arbitrary</i> disturbances, as opposed to normally distributed noise. In this more general setting, there is no closed form for the optimal controller. Instead, it is proposed to use a weighted sum of previously observed noises, i.e., <img alt="u_t^{WL} = K x_t + \sum_{i=1}^H M_i w_{t-i} " class="latex" src="https://s0.wp.com/latex.php?latex=u_t%5E%7BWL%7D+%3D+K+x_t+%2B+%5Csum_%7Bi%3D1%7D%5EH+M_i+w_%7Bt-i%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="u_t^{WL} = K x_t + \sum_{i=1}^H M_i w_{t-i} "/> , where <img alt="M_1,...,M_H" class="latex" src="https://s0.wp.com/latex.php?latex=M_1%2C...%2CM_H&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="M_1,...,M_H"/> are learned parameters, updated in an online fashion. This method is shown to attain vanishing average regret compared to the best fixed linear controller in hindsight, and is applicable for general convex cost functions as opposed to only quadratics.</p>
<p>Crucially, the state-dependent term <img alt="Kx_t" class="latex" src="https://s0.wp.com/latex.php?latex=Kx_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="Kx_t"/> is not learned. Since the learned parameters of the above controller therefore considers only a fixed number of recent disturbances, we can apply existing <a href="http://ocobook.cs.princeton.edu/">online convex optimization</a> techniques developed for <a href="https://papers.nips.cc/paper/6025-online-learning-for-adversaries-with-memory-price-of-past-mistakes">learning with loss functions that have bounded memory</a>.</p>
<p><strong>Boosting for Online Control</strong></p>
<p>Using the insights described above to remove state in online control, we can now use techniques from online boosting. DynaBoost maintains multiple copies of the base-controller above, with each copy corresponding to one stage in boosting. At every time step, the control <img alt="u_t" class="latex" src="https://s0.wp.com/latex.php?latex=u_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="u_t"/> is obtained from a convex combination of the base-controllers’ outputs <img alt="u_t^{WL(1)},...,u_t^{WL(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=u_t%5E%7BWL%281%29%7D%2C...%2Cu_t%5E%7BWL%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="u_t^{WL(1)},...,u_t^{WL(N)}"/>. To update each base-controller’s parameters, DynaBoost feeds each controller with a <i>residual</i> <i>proxy </i>cost function, and seeks to obtain a minimizing point in the direction of the residual loss function’s gradient. Stability ensures that minimizing regret over the proxy costs (which have finite memory) suffices to minimize overall regret. See <a href="https://arxiv.org/abs/1906.08720">our paper</a> for the detailed description of the algorithm and its regret guarantees.</p>
<p><strong>Sanity check experiment</strong></p>
<p>We first conducted experiments for the standard LQR setting with i.i.d. Gaussian noise and known dynamics. We applied our boosting method to the non-optimal controller with learned parameters (Control-WL), and we observe that boosting improves its loss and achieves near-optimal performance (here the optimal controller is given by the fixed LQR solution). We have tested an LDS of different dimensions <img alt="d = 1,10,100" class="latex" src="https://s0.wp.com/latex.php?latex=d+%3D+1%2C10%2C100&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="d = 1,10,100"/>, and averaged results over multiple runs.</p>
<p><img alt="" height="182" src="https://minimizingregret.files.wordpress.com/2019/07/null.png?w=624&amp;h=182" title="" width="624"/></p>
<p><strong>Correlated noise experiment</strong></p>
<p>When the disturbances are not independently drawn, the LQR controller is not guaranteed to perform optimally. We experimented with two LDS settings with correlated disturbances in which (a) the disturbances <img alt="w_t" class="latex" src="https://s0.wp.com/latex.php?latex=w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w_t"/> are generated from a Gaussian random-walk, and (b) where they are generated by a sine function applied to the time index. In these cases, boosted controllers perform better compared to the “weak” learned controller, and can also outperform the fixed LQR solution. We have also tested a Recurrent Neural Network, and observed that boosting is effective for RNNs as well.</p>
<p><img alt="" height="266" src="https://minimizingregret.files.wordpress.com/2019/07/null-1.png?w=624&amp;h=266" title="" width="624"/></p>
<p><strong>Inverted Pendulum experiment</strong></p>
<p>A more challenging experiment with a non-linear dynamical system is the Inverted Pendulum experiment. This is very similar to the pole balancing example we discussed above, and is a standard benchmark for control methods. The goal is to balance the inverted pendulum by applying torque that will stabilize it in a vertically upright position, in the presence of noise. In our experiments, we used correlated disturbances from a Gaussian random-walk. We follow the dynamics implemented in <a href="https://gym.openai.com/">OpenAI Gym</a>, and test the performance of different controllers: LQR, a learned controller, and boosting. The video below visualizes this experiment:</p>
<div class="jetpack-video-wrapper"/>
<p>When averaging the loss value over multiple experiment runs, we get the following plot:</p>
<p style="text-align: center;"><img alt="" height="276" src="https://minimizingregret.files.wordpress.com/2019/07/null-2.png?w=369&amp;h=276" title="" width="369"/></p>
<p>It can be seen that the learned controller performs much better than the LQR in the presence of correlated noise, and that boosting can improve its stability and achieve lower average loss.</p>
<p><b>Boosting for Time-Series Prediction</b></p>
<p>Similarly to the control setting, in time-series prediction tasks it is sufficient to use fixed horizons, and online boosting can be efficiently applied here as well. In time-series prediction, the data is often assumed to be generated from an autoregressive moving average (ARMA) model:</p>
<p style="text-align: center;"><img alt="x_{t} = \sum_{i=1}^k \alpha_i x_{t-i} + \sum_{j=1}^q \beta_j w_j + w_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bt%7D+%3D+%5Csum_%7Bi%3D1%7D%5Ek+%5Calpha_i+x_%7Bt-i%7D+%2B+%5Csum_%7Bj%3D1%7D%5Eq+%5Cbeta_j+w_j+%2B+w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x_{t} = \sum_{i=1}^k \alpha_i x_{t-i} + \sum_{j=1}^q \beta_j w_j + w_t"/></p>
<p>In words, each data point <img alt="x_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x_t"/> is given by a weighted sum of previous points, previous noises and a new noise term <img alt="w_t" class="latex" src="https://s0.wp.com/latex.php?latex=w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w_t"/>, where <img alt="\alpha,\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%2C%5Cbeta&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\alpha,\beta"/> are the coefficients vectors.</p>
<p>To test our boosting method, We experimented with 4 simulated settings: 1) normally distributed noises, 2) coefficients of the dynamical system slowly change over time, 3) a single, abrupt, change of the coefficients, and, 4) correlated noise: Gaussian random walk.</p>
<p>The weak learners tested here are the ARMA-ONS (online newton step) and ARMA-OGD (online gradient descent) algorithms for time-series prediction (See <a href="http://proceedings.mlr.press/v30/Anava13.pdf">this</a> paper for more details). We applied our boosting method, as well as a fast version of it, which applies to quadratic loss functions (we used squared difference in this case).</p>
<p><i><b>1) Gaussian Noise </b></i> <i><b>2) Changing Coefficients</b></i></p>
<p><img alt="" height="217" src="https://minimizingregret.files.wordpress.com/2019/07/null-3.png?w=289&amp;h=217" title="" width="289"/><img alt="" height="217" src="https://minimizingregret.files.wordpress.com/2019/07/null-4.png?w=289&amp;h=217" title="" width="289"/><img alt="" height="218" src="https://minimizingregret.files.wordpress.com/2019/07/null-5.png?w=292&amp;h=218" title="" width="292"/><img alt="" height="217" src="https://minimizingregret.files.wordpress.com/2019/07/null-6.png?w=290&amp;h=217" title="" width="290"/></p>
<p><i><b>3) Abrupt Change </b></i> <i><b>4) Correlated Noise</b></i></p>
<p>We can see in the plots above that all weak learners’ loss values (red) can be improved by online boosting methods (blue). A similar observation arises when experimenting with real-world data; we experimented with the Air Quality dataset from the <a href="https://archive.ics.uci.edu/ml/index.php">UCI Machine Learning repository</a>, that contains hourly averaged measurements of air quality properties from an Italian city throughout one year, as measured by chemical sensors. We apply similar weak learners to this task, as well as our boosting algorithms. Here we again obtain better averaged losses for boosted methods (blue) compared to the baselines (red).</p>
<p style="text-align: center;"><img alt="" height="373" src="https://minimizingregret.files.wordpress.com/2019/07/null-7.png?w=498&amp;h=373" title="" width="498"/></p></div>
    </content>
    <updated>2019-07-17T14:24:42Z</updated>
    <published>2019-07-17T14:24:42Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Elad Hazan</name>
    </author>
    <source>
      <id>https://minimizingregret.wordpress.com</id>
      <logo>https://minimizingregret.files.wordpress.com/2017/08/cropped-pu1.png?w=32</logo>
      <link href="https://minimizingregret.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://minimizingregret.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://minimizingregret.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://minimizingregret.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Google Princeton AI and Hazan Lab @ Princeton University</subtitle>
      <title>Minimizing Regret</title>
      <updated>2019-07-25T15:22:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17564</id>
    <link href="https://gilkalai.wordpress.com/2019/07/17/dan-romik-on-the-riemann-zeta-function/" rel="alternate" type="text/html"/>
    <title>Dan Romik on the Riemann zeta function</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post about the Rieman zeta function, among the most important and mysterious mathematical objects is kindly written by Dan Romik. It is related to his paper Orthogonal polynomial expansions for the Riemann xi function,  that we mentioned in this … <a href="https://gilkalai.wordpress.com/2019/07/17/dan-romik-on-the-riemann-zeta-function/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>This post about the Rieman zeta function, among the most important and mysterious mathematical objects is kindly written by Dan Romik. It is related to his paper <a href="https://arxiv.org/abs/1902.06330">Orthogonal polynomial expansions for the Riemann xi function</a>,  that we mentioned in <a href="https://gilkalai.wordpress.com/2019/02/28/dan-romik-studies-the-riemanns-zeta-function-and-other-zeta-news/">this post</a>.</em></p>
<h2>Dan Romik on the Riemann zeta function</h2>
<p>Recently when I was thinking about the Riemann zeta function, I had the double thrill of discovering some new results about it, and then later finding out that my new ideas were closely related to some very classical ideas due to two icons of twentieth-century mathematics, George Pólya and Pál Turán. When you are trying to stand on the shoulders of giants, it’s nice to see other giants right there beside you trying to do the same!</p>
<p>It all goes back to one of the most famous problems in mathematics, the Riemann Hypothesis (RH). Both Pólya and Turán were rather enamored with this problem and published about it extensively; Pólya was said to have been preoccupied with the problem to the very end of his life.(1) And they both recognized that an important first step in trying to prove something about the zeros of the zeta function is having a good representation<br/>
for the Riemann zeta function. After all, there are many different formulas that can be used to define or compute the zeta function. If you don’t choose the right one, you probably won’t get very far with your analysis.</p>
<p>Pólya in one of his famous attacks on the problem considered the representation of the zeta function (or more precisely of the Riemann xi function, which is a symmetrized and better-behaved version of the zeta function; see below) as a Fourier transform—a standard representation due (essentially) to Riemann. I’ll have more to say about that later.</p>
<p>Turán also looked at the Riemann xi function, and instead of working with one of the standard “named” representations such as the Fourier transform or Taylor series, looked around a bit more intentionally for a representation of the function that seemed particularly suited to answering the specific question of whether the zeros all lie on a line. In a 1950 address to the Hungarian Academy of Sciences, he put forward his ideas about what he thought was the correct representation to look at: the infinite series expansion of the xi function in the Hermite polynomials. About eighty years after Turán’s discovery, my own investigations led me to discover [5] that the Hermite polynomials are not the only polynomials in which it’s interesting to expand the Riemann xi function. It turns out that there are at least two other families of polynomials for which the respective expansions are no less (and, in some ways, more) well-behaved. My motto for these polynomial families, which are known to experts in special functions but have until now been somewhat esoteric (though I hope that is about to change), is that they are “the coolest polynomials that you never heard of.”</p>
<p>Let’s look at some of the technical details so that I can explain why these new expansions are interesting, and how they relate to Turán’s work and ultimately back to Pólya’s ideas and one of the particular threads that grew out of them. First, define the Riemann xi function as</p>
<p><img alt="\displaystyle \xi(s) = \frac12 s(s-1) \pi^{-s/2} \Gamma\left(\frac{s}{2}\right) \zeta(s) \qquad (s\in\mathbb{C}), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cxi%28s%29+%3D+%5Cfrac12+s%28s-1%29+%5Cpi%5E%7B-s%2F2%7D+%5CGamma%5Cleft%28%5Cfrac%7Bs%7D%7B2%7D%5Cright%29+%5Czeta%28s%29+%5Cqquad+%28s%5Cin%5Cmathbb%7BC%7D%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \xi(s) = \frac12 s(s-1) \pi^{-s/2} \Gamma\left(\frac{s}{2}\right) \zeta(s) \qquad (s\in\mathbb{C}), "/></p>
<p>where <img alt="{\Gamma(z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Gamma(z)}"/> is the Euler gamma function and <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\zeta(s)}"/> is the Riemann zeta function. It’s also common to denote<br/>
<img alt="\displaystyle \Xi(t) = \xi\left(\frac12+it\right) \qquad (t\in\mathbb{C}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CXi%28t%29+%3D+%5Cxi%5Cleft%28%5Cfrac12%2Bit%5Cright%29+%5Cqquad+%28t%5Cin%5Cmathbb%7BC%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \Xi(t) = \xi\left(\frac12+it\right) \qquad (t\in\mathbb{C}). "/></p>
<p>This is Riemann’s “capital xi” function, which is still usually referred to as Riemann’s xi function. (This seems reasonable: the two functions are the same up to a trivial linear change of variables.) The main point of these definitions is that <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/> is an entire function of the complex variable <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, and that RH can now be reformulated as the statement that the zeros of <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/> all lie on the real line. Moreover, the famous functional equation satisfied by the Riemann zeta function maps to the statement that <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/> is an even function.<br/>
Now consider the following four ways of representing the xi function:</p>
<p><img alt="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n a_{2n} t^{2n},~~~~~(1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CXi%28t%29+%3D+%5Csum_%7Bn%3D0%7D%5E%5Cinfty+%28-1%29%5En+a_%7B2n%7D+t%5E%7B2n%7D%2C%7E%7E%7E%7E%7E%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n a_{2n} t^{2n},~~~~~(1)"/></p>
<p><img alt="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n b_{2n} H_{2n}(t),~~~~~(2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CXi%28t%29+%3D+%5Csum_%7Bn%3D0%7D%5E%5Cinfty+%28-1%29%5En+b_%7B2n%7D+H_%7B2n%7D%28t%29%2C%7E%7E%7E%7E%7E%282%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n b_{2n} H_{2n}(t),~~~~~(2)"/></p>
<p><img alt="\displaystyle\Xi(t) = \sum_{n=0}^\infty (-1)^n c_{2n} f_{2n}\left(\frac{t}{2}\right),~~~~~(3)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%5CXi%28t%29+%3D+%5Csum_%7Bn%3D0%7D%5E%5Cinfty+%28-1%29%5En+c_%7B2n%7D+f_%7B2n%7D%5Cleft%28%5Cfrac%7Bt%7D%7B2%7D%5Cright%29%2C%7E%7E%7E%7E%7E%283%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle\Xi(t) = \sum_{n=0}^\infty (-1)^n c_{2n} f_{2n}\left(\frac{t}{2}\right),~~~~~(3)"/></p>
<p><img alt="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n d_{2n} g_{2n}\left(\frac{t}{2}\right).~~~~~(4)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CXi%28t%29+%3D+%5Csum_%7Bn%3D0%7D%5E%5Cinfty+%28-1%29%5En+d_%7B2n%7D+g_%7B2n%7D%5Cleft%28%5Cfrac%7Bt%7D%7B2%7D%5Cright%29.%7E%7E%7E%7E%7E%284%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n d_{2n} g_{2n}\left(\frac{t}{2}\right).~~~~~(4)"/></p>
<p>Here, the first representation (1) is simply the Taylor expansion of <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/>, which contains only even terms since <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/> is an even function. The numbers <img alt="{a_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_{2n}}"/> are (up to the <img alt="{(-1)^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28-1%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(-1)^n}"/> sign factor) the Taylor coefficients. Some attempts have been made to understand them, and one interesting and fairly trivial observation (again going back to facts already known to Riemann) is that they are all positive. Some additional and less trivial things can be said—see for example Section 6.1 of my paper [5], and the recent paper by Griffin, Ono, Rolen and Zagier [2]. But at the end of the day, no one has yet succeeded in using the Taylor expansion to prove anything new about the location of the zeros.</p>
<p>The second representation (2) is the infinite series expansion of <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/> in the classical sequence of Hermite polynomials, defined by the well-known formula</p>
<p><img alt="\displaystyle H_n(t) = (-1)^n e^{t^2} \frac{d^n}{dt^n} \left( e^{-t^2} \right). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+H_n%28t%29+%3D+%28-1%29%5En+e%5E%7Bt%5E2%7D+%5Cfrac%7Bd%5En%7D%7Bdt%5En%7D+%5Cleft%28+e%5E%7B-t%5E2%7D+%5Cright%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle H_n(t) = (-1)^n e^{t^2} \frac{d^n}{dt^n} \left( e^{-t^2} \right). "/></p>
<p>This is the representation whose use was advocated by Turán. His reasoning was that expanding a function of a complex variable (for example, in the simplest case, a polynomial) in monomials <img alt="{t^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t^n}"/> doesn’t provide useful information to easily decide if the function has only real zeros, because the monomials have, roughly speaking, radial symmetry: their level curves are concentric circles. The Hermite polynomials on the other hand, at least heuristically, have level curves that are closer to being straight lines parallel to the real axis, Turán argued; thus, they are more suited to the geometry of the problem we are trying to solve.</p>
<p>Turán’s case for supporting the Hermite polynomials as the right basis to use is quite detailed—you can read about it in his papers [6,7,8] (and no, he was not able to actually prove anything about the zeros of <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/>; this is a common theme in most of the attacks on RH to date…). I’ll simply mention that again one interesting and fairly easy observation is that the coefficients <img alt="{b_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b_{2n}}"/> in the expansion (2)—adjusted through the introduction of the sign factor <img alt="{(-1)^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28-1%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(-1)^n}"/>—end up being positive numbers. Their asymptotic behavior can also be analyzed: I prove a result about this in my paper (though it’s not particularly pretty).</p>
<p>Now comes the part that to me seems the most exciting, involving the expansions (3) and (4). These are the expansions in the more exotic families of polynomials</p>
<p><img alt="\displaystyle f_n(x)=(-i)^n \sum_{k=0}^n 2^k\binom{n+\frac12}{n-k}\binom{-\frac34+ix}{k}," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f_n%28x%29%3D%28-i%29%5En+%5Csum_%7Bk%3D0%7D%5En+2%5Ek%5Cbinom%7Bn%2B%5Cfrac12%7D%7Bn-k%7D%5Cbinom%7B-%5Cfrac34%2Bix%7D%7Bk%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle f_n(x)=(-i)^n \sum_{k=0}^n 2^k\binom{n+\frac12}{n-k}\binom{-\frac34+ix}{k},"/></p>
<p><img alt="\displaystyle g_n(x)= (-i)^n \sum_{k=0}^n \frac{(n+k+1)!}{(n-k)!(3/2)_k^2} \binom{-\frac34+ix}{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+g_n%28x%29%3D+%28-i%29%5En+%5Csum_%7Bk%3D0%7D%5En+%5Cfrac%7B%28n%2Bk%2B1%29%21%7D%7B%28n-k%29%21%283%2F2%29_k%5E2%7D+%5Cbinom%7B-%5Cfrac34%2Bix%7D%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle g_n(x)= (-i)^n \sum_{k=0}^n \frac{(n+k+1)!}{(n-k)!(3/2)_k^2} \binom{-\frac34+ix}{k}"/></p>
<p>(where <img alt="{(3/2)_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%283%2F2%29_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(3/2)_n}"/> is a <a href="https://en.wikipedia.org/wiki/Falling_and_rising_factorials">Pochhammer symbol</a>), mildly rescaled by replacing <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>  with <img alt="{t/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t/2}"/>. In the terminology of the theory of orthogonal polynomials, the family <img alt="{f_n(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_n%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_n(x)}"/> is a special case of a two-parameter family <img alt="{P_n^{(\lambda)}(x;\phi)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_n%5E%7B%28%5Clambda%29%7D%28x%3B%5Cphi%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P_n^{(\lambda)}(x;\phi)}"/> known as the Meixner-Pollaczek polynomials, with the parameters taking the particular values <img alt="{\phi=\frac{\pi}{2}, \lambda=\frac{3}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%3D%5Cfrac%7B%5Cpi%7D%7B2%7D%2C+%5Clambda%3D%5Cfrac%7B3%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi=\frac{\pi}{2}, \lambda=\frac{3}{4}}"/>. Similarly, the family <img alt="{g_n(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_n%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_n(x)}"/> is a special case of the four-parameter family <img alt="{p_n(x;a,b,c,d)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_n%28x%3Ba%2Cb%2Cc%2Cd%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_n(x;a,b,c,d)}"/> known as the continuous Hahn polynomials, with the parameters taking the particular values <img alt="{a=b=c=d=\frac{3}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%3Db%3Dc%3Dd%3D%5Cfrac%7B3%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a=b=c=d=\frac{3}{4}}"/>. Their main characterizing property is that they are orthogonal sequences of polynomials for two specific weight functions on <img alt="{\mathbb{R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{R}}"/>: the <img alt="{f_n(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_n%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_n(x)}"/> are orthogonal with respect to the weight function <img alt="{w_1(x)=\left|\Gamma\left(\frac34+ix\right)\right|^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_1%28x%29%3D%5Cleft%7C%5CGamma%5Cleft%28%5Cfrac34%2Bix%5Cright%29%5Cright%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_1(x)=\left|\Gamma\left(\frac34+ix\right)\right|^2}"/>, and the <img alt="{g_n(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_n%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_n(x)}"/> are orthogonal with respect to <img alt="{w_2(x)=\left|\Gamma\left(\frac34+ix\right)\right|^4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_2%28x%29%3D%5Cleft%7C%5CGamma%5Cleft%28%5Cfrac34%2Bix%5Cright%29%5Cright%7C%5E4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_2(x)=\left|\Gamma\left(\frac34+ix\right)\right|^4}"/>. Again, fairly esoteric. But interesting!</p>
<p>There are several things that make the expansions (3)–(4) well-behaved. First, the coefficients <img alt="{c_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_{2n}}"/>, <img alt="{d_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{2n}}"/> are again positive. This actually seems pretty relevant for questions like RH: for example, if we consider “toy” versions of (1)–(3) in which the coefficient sequences <img alt="{a_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_n}"/>, <img alt="{b_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b_n}"/> and <img alt="{c_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_n}"/> are replaced by the sequence <img alt="{\alpha^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha^n}"/> for fixed <img alt="{0&lt;\alpha&lt;1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%3C%5Calpha%3C1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0&lt;\alpha&lt;1}"/>, all three expansions sum up to rescaled cosines, which are entire functions that of course have only real zeros. (Without the <img alt="{(-1)^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28-1%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(-1)^n}"/> factor, we would get a hyperbolic cosine, which has imaginary zeros.)</p>
<p>Second, one can derive asymptotics for <img alt="{c_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_{2n}}"/> and <img alt="{d_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{2n}}"/>, and they are quite a bit nicer than the asymptotic formulas for the Taylor and Hermite expansion coefficients. In my paper, I proved that <img alt="\displaystyle c_{2n} \sim A \sqrt{n} e^{-B \sqrt{n}}, \qquad d_{2n} \sim C n^{4/3} e^{-D n^{2/3}} \qquad \textrm{as }n\rightarrow\infty, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+c_%7B2n%7D+%5Csim+A+%5Csqrt%7Bn%7D+e%5E%7B-B+%5Csqrt%7Bn%7D%7D%2C+%5Cqquad+d_%7B2n%7D+%5Csim+C+n%5E%7B4%2F3%7D+e%5E%7B-D+n%5E%7B2%2F3%7D%7D+%5Cqquad+%5Ctextrm%7Bas+%7Dn%5Crightarrow%5Cinfty%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle c_{2n} \sim A \sqrt{n} e^{-B \sqrt{n}}, \qquad d_{2n} \sim C n^{4/3} e^{-D n^{2/3}} \qquad \textrm{as }n\rightarrow\infty, "/> where <img alt="{A,B,C,D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%2CD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B,C,D}"/> are the constants <img alt="\displaystyle A = 16\sqrt{2}\pi^{3/2}, \qquad B = 4\sqrt{\pi}, \qquad C = \frac{128 \times 2^{1/3} \pi^{2/3} e^{-2\pi /3}}{\sqrt{3}}, \qquad D = 3 (4\pi)^{1/3}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+A+%3D+16%5Csqrt%7B2%7D%5Cpi%5E%7B3%2F2%7D%2C+%5Cqquad+B+%3D+4%5Csqrt%7B%5Cpi%7D%2C+%5Cqquad+C+%3D+%5Cfrac%7B128+%5Ctimes+2%5E%7B1%2F3%7D+%5Cpi%5E%7B2%2F3%7D+e%5E%7B-2%5Cpi+%2F3%7D%7D%7B%5Csqrt%7B3%7D%7D%2C+%5Cqquad+D+%3D+3+%284%5Cpi%29%5E%7B1%2F3%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle A = 16\sqrt{2}\pi^{3/2}, \qquad B = 4\sqrt{\pi}, \qquad C = \frac{128 \times 2^{1/3} \pi^{2/3} e^{-2\pi /3}}{\sqrt{3}}, \qquad D = 3 (4\pi)^{1/3}. "/></p>
<p>Third, the expansions have some conceptual meaning: (3) turns out to be equivalent to the expansion of the elementary function <img alt="{\frac{d^2}{du^2} (u \coth(\pi u))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bd%5E2%7D%7Bdu%5E2%7D+%28u+%5Ccoth%28%5Cpi+u%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{d^2}{du^2} (u \coth(\pi u))}"/>, <img alt="{u&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u&gt;0}"/>, in an orthogonal basis of functions related to the Laguerre polynomials <img alt="{L_n^{1/2}(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_n%5E%7B1%2F2%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_n^{1/2}(x)}"/>. And analogously, (4) arises out of the expansion of a certain auxiliary function <img alt="{\tilde{\nu}(u)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7B%5Cnu%7D%28u%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tilde{\nu}(u)}"/> (I won’t define it here) in yet another classical family of orthogonal polynomials, the Chebyshev polynomials of the second kind.</p>
<p>Fourth (and fifth, sixth, …): the expansions are just… nice, in the sense that they arise in a way that seems natural when one asks certain questions, that they have excellent convergence properties, and that the coefficients <img alt="{c_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_{2n}}"/> and <img alt="{d_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{2n}}"/> have several elegant formulas, each revealing something interesting about them. Read the paper to understand more.</p>
<p>I said I will get back to Pólya’s work on RH. This post is already quite long so I will say only a little bit about this. One of Pólya’s major discoveries was that there are operations on entire functions that (under certain mild assumptions) preserve the property of the function having only real zeros. Specifically this is the case for the operation of multiplying the Fourier transform of the function by the factor <img alt="{e^{\lambda u^2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B%5Clambda+u%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{\lambda u^2}}"/> for <img alt="{\lambda&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda&gt;0}"/>  (where <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> is the frequency variable). This opens the way to defining a family of deformations <img alt="{\Xi_\lambda(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi_%5Clambda%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi_\lambda(t)}"/> of the Riemann xi function arising out of this operation, and trying to generalize RH by asking for which values of <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> it is the case that <img alt="{\Xi_\lambda(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi_%5Clambda%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi_\lambda(t)}"/> has only real zeros. Since Pólya’s work, and important later extensions of it by De Bruijn and Newman, this has become a very active topic of research, nowadays referred to under the name of the De Bruijn-Newman constant.<br/>
See the recent survey of Newman and Wu [3], a 2018 paper by Rodgers and Tao [4] proving a major conjecture of Newman, and the recent paper [9] by the <a href="https://terrytao.wordpress.com/2018/12/28/polymath-15-eleventh-thread-writing-up-the-results-and-exploring-negative-t/">Polymath15 project</a> (mentioned by Gil in his <a href="https://gilkalai.wordpress.com/2019/02/28/dan-romik-studies-the-riemanns-zeta-function-and-other-zeta-news/">earlier post</a>), for the latest on this subject.</p>
<p>The connection I found between this topic and the idea of expanding the Riemann xi function in families of orthogonal polynomials is the following: expansions such as (2)–(4) suggest yet another natural way of “deforming” the Riemann xi function by adding a parameter <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/>: simply multiply the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>th term in the expansion by <img alt="{\alpha^{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%5E%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha^{2n}}"/> (the linear operator that does this is called the Poisson kernel, and generalizes the standard Poisson kernel from complex analysis and the theory of harmonic functions). It turns out—and is actually easy to prove, and really isn’t terribly surprising in the grand scheme of things—that in the case of the Hermite expansion (2), this family of deformations is the same, up to some trivial reparametrization, as the family of deformations <img alt="{\Xi_\lambda(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi_%5Clambda%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi_\lambda(t)}"/> that was studied in connection with the work of Pólya, De Bruijn, Newman and their successors. A nice connection between two threads of research that were not previously recognized as being related to each other, I think. Furthermore, this suggests that the Poisson kernel and associated deformations may yet have an important role to play in the context of the new expansions in the orthogonal polynomial families <img alt="{f_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_n}"/> and <img alt="{g_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_n}"/>, where we get genuinely new families of deformations of the Riemann xi function. I explore this idea in my paper and it leads to some interesting things.</p>
<p>So let’s summarize. The key questions you are no doubt wondering about are: where does any of this lead? And do these new ideas say anything really useful or especially relevant for the Riemann hypothesis? The answer is that I don’t know (and I’m wondering about the same things). That being said, these orthogonal polynomial expansions seem quite interesting in their own right. The Riemann zeta function is a mysterious object, and there are <a href="https://en.wikipedia.org/wiki/Lindel%C3%B6f_hypothesis">other things</a> we wish to understand about it beside where its zeros are, so it’s always good to have additional points of view from which to approach it. Moreover, even on the question of the zeros there are reasons to be cautiously optimistic that this approach may have something useful to offer; see Chapter 7 of my paper for a brief discussion of why that is the case.</p>
<h2/>
<h2>References</h2>
<p>[1] D. Albers and G. L. Alexanderson, editors. Mathematical People: Profiles and Interviews. A K Peters, 2008.</p>
<p>[2] M. Griffin, K. Ono, L. Rolen and D. Zagier. Jensen polynomials for the Riemann zeta function and other sequences. Preprint (2019), <a href="https://arxiv.org/abs/1902.07321">arXiv:1902.07321</a>.</p>
<p>[3] C. M. Newman. Constants of de Bruijn-Newman type in analytic number theory and statistical physics. To appear in Bull. Amer. Math. Soc.</p>
<p>[4] B. Rodgers and T. Tao. The De Bruijn-Newman constant is nonnegative. Preprint (2018), <a href="https://arxiv.org/abs/1801.05914">arXiv:1801.05914</a>.</p>
<p>[5] D. Romik. <a href="https://arxiv.org/abs/1902.06330">Orthogonal polynomial expansions for the Riemann xi function</a>. Preprint (2019), <a href="https://arxiv.org/abs/1902.06330">arXiv:1902.06330</a>.</p>
<p>[6] P. Turán. Sur l’algèbre fonctionelle. Pages 279–290 in: Comptes Rendus du Premier Congrès des Mathématiciens Hongrois, 27 Août–2 Septembre 1950. Akadémiai Kiadó, 1952. Reprinted in Collected Papers of Paul Turán, Ed. P. Erdos, Vol. 1, pp. 677–688. Akadémiai Kiadó, 1990. An English translation of the paper by Dan Romik <a href="http://math.ucdavis.edu/~romik/data/uploads/misc/turan1952-english.pdf">On functional algebra</a>.</p>
<p>[7] P. Turán. Hermite-expansion and strips for zeros of polynomials. Arch. Math. 5 (1954), 148–152. Reprinted in Collected Papers of Paul Turán, Ed. P. Erdos, Vol. 1, pp. 738–742. Akadémiai Kiadó, 1990.</p>
<p>[8]  P. Turán. To the analytical theory of algebraic equations. Bulgar. Akad. Nauk. Otd. Mat. Fiz. Nauk. Izv. Mat. Inst. 3 (1959), 123–137. Reprinted in Collected Papers of Paul Turán, Ed. P. Erdos, Vol. 2, pp. 1080–1090. Akadémiai Kiadó, 1990.</p>
<p>[9] D.H.J. Polymath. Effective approximation of heat flow evolution of the Riemann ξ function, and a new upper bound for the de Bruijn-Newman constant. Preprint (2019), <a href="https://arxiv.org/abs/1904.12438">arXiv:1904.12438</a>.</p>
<h2>Notes:</h2>
<p>(1) Alexanderson writes in [1, p. 259]: “A week or so before he died, Pólya asked me to look on his desk at home for some papers on which he said he had written down some interesting ideas he had for proving RH. Of course I could find no such notes, but until the day he died he was thinking about that famous problem.”</p>
<p> </p>
<p>(2) Turán’s Hungarian Academy of Sciences talk was published in a rather obscure French-language paper [6] that seems to have been largely forgotten. It’s an interesting read nonetheless, and to make it more accessible to anyone who may be interested, I recently translated it to English.</p>
<p> </p>
<p>(3) Turán mentions in [8] that he discovered the results on the Hermite expansion in 1938–39, but they were not published until much later. Clearly this was not a convenient time in history for publishing such discoveries; Turán, a Hungarian Jew, spent much of World War II interned in labor camps in Hungary.</p></div>
    </content>
    <updated>2019-07-17T06:22:33Z</updated>
    <published>2019-07-17T06:22:33Z</published>
    <category term="Combinatorics"/>
    <category term="Guest blogger"/>
    <category term="Number theory"/>
    <category term="Dan Romik"/>
    <category term="George Polya"/>
    <category term="Paul Turan"/>
    <category term="Riemann Hypothesis"/>
    <category term="Riemann zeta function"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-25T15:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16114</id>
    <link href="https://rjlipton.wordpress.com/2019/07/16/summer-reading-in-theory/" rel="alternate" type="text/html"/>
    <title>Summer Reading in Theory</title>
    <summary>Some formative books in mathematics and computing theory LSE source: “Calculus on Clay?” Norman Biggs is the author of the wonderful book Algebraic Graph Theory. Both Ken and I read it long ago, and both of us have it out now because of its relevance to Hao Huang’s beautiful short proof of the Boolean Sensitivity […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Some formative books in mathematics and computing theory</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/07/biggs-norman-150x150.jpg"><img alt="" class="alignright size-full wp-image-16115" src="https://rjlipton.files.wordpress.com/2019/07/biggs-norman-150x150.jpg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">LSE <a href="https://blogs.lse.ac.uk/maths/2016/04/15/norman-biggs-calculus-on-clay/">source</a>: <i>“Calculus on Clay?”</i></font></td>
</tr>
</tbody>
</table>
<p>
Norman Biggs is the author of the wonderful <a href="https://www.cambridge.org/core/books/algebraic-graph-theory/6C70471342F19680068C35EF174075DC">book</a> <em>Algebraic Graph Theory</em>. Both Ken and I read it long ago, and both of us have it out now because of its relevance to Hao Huang’s beautiful short <a href="http://www.mathcs.emory.edu/~hhuan30/papers/sensitivity_1.pdf">proof</a> of the Boolean Sensitivity Conjecture. </p>
<p>
Today we wish to ask, <i>What are your top five favorite books on mathematics and theory for summer reading?</i></p>
<p>
There’s an <a href="https://en.wikipedia.org/wiki/Aporia">aporia</a> in that question. A working definition of aporia is: “a self-contradiction that isn’t.” The point is that books for summer reading should be new, so how would you already know which are your favorites? Well, we are thinking of books that are so rich you can always find new things in them—and that also played formative roles earlier in our careers.</p>
<p>
Ken knew Biggs during his first year at Oxford when Biggs was visiting there from London. He took part in a weekly sitting-room seminar organized by Peter Neumann. Biggs’s book was a central reference for Ken’s undergraduate senior thesis at Princeton, and both he and Ken presented material based on it. </p>
<p>
</p><p/><h2> Best Five Books—Dick </h2><p/>
<p/><p>
Here are my votes for all-time best books in mathematics and in computer science theory.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <a href="https://www.cambridge.org/core/books/algebraic-graph-theory/6C70471342F19680068C35EF174075DC"><i>Algebraic Graph Theory</i></a>, by Norman Biggs. A wonderful book. First appeared in 1974.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <a href="https://www.amazon.com/Introduction-Probability-Theory-Applications-Vol/dp/0471257087/ref=sr_1_1?keywords=William+Feller&amp;qid=1563190401&amp;s=books&amp;sr=1-1"><i> An Introduction to Probability Theory and Its Applications, Vol. 1</i></a>, by William Feller. This is the book I used to learn probability theory.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <a href="https://www.amazon.com/gp/product/0199219869/ref=as_li_tl?ie=UTF8&amp;tag=mathblog05-20&amp;camp=1789&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=0199219869&amp;linkId=a71963a143733e948f50588526d624c0"><i> An Introduction to the Theory of Numbers</i></a>, by Godfrey Hardy and Edward Wright. Now updated by Andrew Wiles, Roger Heath-Brown, and Joseph Silverman. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <a href="https://www.amazon.com/Elements-Number-Theory-Dover-Mathematics/dp/0486781658/ref=sr_1_1?keywords=Vinogradov&amp;qid=1563190340&amp;s=books&amp;sr=1-1"><i>Elements of Number Theory</i></a>, by Ivan Vinogradov. Another small book that is loaded with ideas. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <a href="https://www.abebooks.com/Paul-Erds-Art-Counting-Erdos-Joel/13380002114/bd"><i>The Art of Counting</i></a>, by Paul Erdős and Joel Spencer. This book changed my life. Today the book is of course <a href="https://www.amazon.com/dp/0470170204/?tag=stackoverfl08-20"><i>The Probabilistic Method</i></a>, by Noga Alon and Joel Spencer. </p>
<p>
</p><p/><h2> Best Five Books—Ken </h2><p/>
<p/><p>
Ken reaches back to his teen years but it’s still the same span of years as my list. Here he tells it:</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> All books by Martin Gardner—in particular, the books of collections of his “Mathematical Games” columns in <em>Scientific American</em>. Here is an <a href="https://blogs.scientificamerican.com/guest-blog/the-top-10-martin-gardner-scientific-american-articles/">overview</a>.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <a href="https://www.lybrary.com/scarne-on-dice-p-655.html"><i>Scarne on Dice</i></a> and <a href="https://www.lybrary.com/scarne-on-cards-p-759.html"><i> Scarne on Cards</i></a>. Originally it was neither of these books—nor John Scarne’s <em>Complete Guide to Gambling</em>—but a different book on in which both Scarne and Gardner figured prominently. Alas I, Ken, cannot trace it. That’s what I used to learn probability theory.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <a href="https://www.amazon.com/Spectra-Graphs-Application-Applied-Mathematics/dp/0121951502"><i>Spectra of Graphs</i></a>, by Dragoš Cvetković, Michael Doob, and Horst Sachs. I could put Biggs’s book here, but this is the one that got me on to the whole subject just before my senior year at Princeton. It was fresh out in 1980—I recall the tactile sensation of the dark green spanking new cover in the Fine Hall Library’s copy. A great book with pictures and algebra. </p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <a href="https://www.amazon.com/Ideals-Varieties-Algorithms-Computational-Undergraduate/dp/0387356509"><i> Ideals, Varieties, and Algorithms</i></a>, by David Cox, John Little, and Donal O’Shea. Fast forward to 1997. Having realized that techniques from algebraic geometry could surmount the “Natural Proofs” <a href="https://rjlipton.wordpress.com/2009/03/25/whos-afraid-of-natural-proofs/">barrier</a> (see also <a href="https://en.wikipedia.org/wiki/Geometric_complexity_theory">GCT</a>), I went whole-hog after it. See “Manic Monomials” in this <a href="https://rjlipton.wordpress.com/2012/07/04/july-fourth-sale-of-ideas/">post</a> for one thing that tripped it up. The book remains incredibly stimulating. It has a <a href="https://www.amazon.com/Using-Algebraic-Geometry-Graduate-Mathematics/dp/0387984879/">sequel</a>, <em>Using Algebraic Geometry</em>.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <a href="https://en.wikipedia.org/wiki/Quantum_Computation_and_Quantum_Information"><i>Quantum Computation and Quantum Information</i></a> by Michael Nielsen and Isaac Chuang. As with Hardy and Wright, it has its own Wikipedia page. Dick and I can say this is nominating a competitor, but Chaung &amp; Nielsen is really in a class by itself for the sheer richness and writing style. One odd mark of its influence: In 2006 when I reacted to the sensational and frightening accusations of cheating at the world championship <a href="https://en.wikipedia.org/wiki/World_Chess_Championship_2006">match</a>, my first thought was to apply distributional distance measures of the kind used in its later chapters. Among such measures is (quantum) <a href="https://en.wikipedia.org/wiki/Fidelity_of_quantum_states">fidelity</a>, and although I focused more on Jensen-Shannon divergence before deciding on simpler stuff, my chess research <a href="https://cse.buffalo.edu/~regan/chess/fidelity/">website</a> retains “fidelity” in its name as part of a multi-way reference to <a href="https://en.wikipedia.org/wiki/FIDE">FIDE</a>, faith, and playing in good faith.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What books most influenced you? What are your votes for the best books that might influence others?	 </p></font></font></div>
    </content>
    <updated>2019-07-17T04:26:38Z</updated>
    <published>2019-07-17T04:26:38Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Teaching"/>
    <category term="algebraic"/>
    <category term="books"/>
    <category term="Hao Huang"/>
    <category term="Norman Biggs"/>
    <category term="probabilistic"/>
    <category term="spectral graph theory"/>
    <category term="summer reading"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-07-25T15:21:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6069637759837834972</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6069637759837834972/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/guest-post-by-samir-khuller-on.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6069637759837834972" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6069637759837834972" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/guest-post-by-samir-khuller-on.html" rel="alternate" type="text/html"/>
    <title>Guest post by Samir Khuller on attending The TCS Women 2019 meeting</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
(I will post the solution to the problem in the last blog later in the week---probably Thursday. Meanwhile, enjoy these thoughts from Samir Khuller on the TCS Women 2019 meeting.)<br/>
<br/>
Guest Post by Samir Khuller:<br/>
<br/>
Am I even allowed here?” was the first thought that crossed my mind when I entered the room. It was packed with women (over 95%), however a few minutes later, several men had trickled in. I was at the TCS Women spotlight workshop on the day before STOC. Kudos to Barna Saha, Sofya Raskhodnikova, and Virginia Vassilevska Williams for putting this grand (and long needed) event together, which serves as a role model and showcases some of the recent work by rising stars. In addition to the Sun afternoon workshop, the event was followed by both an all women panel and a poster session (which I sadly did not attend).<br/>
<br/>
<br/>
The rising stars talks were given by Naama Ben-David (CMU), Andrea Lincoln (MIT), Debarati Das (Charles University) and Oxana Poburinnaya (Boston U). After a short break the inspirational talk was by Ronitt Rubinfeld from MIT.  Ronitt’s talk was on the topic of Program Checking, but she made it inspirational by putting us in her shoes as a young graduate student, three decades back, trying to make a dent in research by working on something that her advisor Manuel Blum, and his senior graduate student Sampath Kannan had been working on, and I must say she made a pretty big dent in the process! She also related those ideas to other pieces of work done since in a really elegant manner and how these pieces of work lead to work on property testing.<br/>
<br/>
<br/>
I am delighted to say that NSF supported the workshop along with companies such as Amazon, Akamai, Google and Microsoft. SIGACT plans to be a major sponsor next year.<br/>
<br/>
<br/>
The Full program for the workshop is at the following URL<a href="https://sigact.org/tcswomen/tcs-women-2019/">here.</a><br/>
<br/></div>
    </content>
    <updated>2019-07-16T23:38:00Z</updated>
    <published>2019-07-16T23:38:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-07-25T15:03:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/094</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/094" rel="alternate" type="text/html"/>
    <title>TR19-094 |  Rainbow coloring hardness via low sensitivity polymorphisms | 

	Venkatesan Guruswami, 

	Sai Sandeep</title>
    <summary>A $k$-uniform hypergraph is said to be $r$-rainbow colorable if there is an $r$-coloring of its vertices such that every hyperedge intersects all $r$ color classes. Given as input such a hypergraph, finding a $r$-rainbow coloring of it is NP-hard for all $k \ge 3$ and $r \ge 2$. Therefore, one settles for finding a rainbow coloring with fewer colors (which is an easier task).  When $r=k$ (the maximum possible value), i.e., the hypergraph is $k$-partite, one can efficiently $2$-rainbow color the hypergraph, i.e., $2$-color its vertices so that there are no monochromatic edges. In this work we consider the next smaller value of $r=k-1$, and prove that in this case it is NP-hard to rainbow color the hypergraph with $q :=  \lceil \frac{k-2}{2} \rceil$ colors. In particular, for $k \le 6$, it is NP-hard to $2$-color $(k-1)$-rainbow colorable $k$-uniform hypergraphs.

Our proof follows the algebraic approach to promise constraint satisfaction problems. It proceeds by characterizing the polymorphisms associated with the approximate rainbow coloring problem, which are rainbow colorings of some product hypergraphs on vertex set $[r]^n$. We prove that any such polymorphism $f: [r]^n \to [q]$ must be $C$-fixing, i.e., there is a small subset $S$ of $C$ coordinates and a setting $a \in [q]^S$ such that fixing $x_{|S} = a$ determines the value of $f(x)$. The key step in our proof is bounding the sensitivity of certain rainbow colorings, thereby arguing that they must be juntas. Armed with the $C$-fixing characterization, our NP-hardness is obtained via a reduction from smooth Label Cover.</summary>
    <updated>2019-07-16T01:19:56Z</updated>
    <published>2019-07-16T01:19:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-25T15:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/093</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/093" rel="alternate" type="text/html"/>
    <title>TR19-093 |  Improved 3LIN Hardness via Linear Label Cover | 

	Euiwoong Lee, 

	Subhash Khot, 

	Prahladh Harsha, 

	Devanathan Thiruvenkatachari</title>
    <summary>We prove that for every constant $c$ and $\epsilon = (\log n)^{-c}$, there is no polynomial time algorithm that when given an instance of 3LIN with $n$ variables where an $(1 - \epsilon)$-fraction of the clauses are satisfiable, finds an assignment that satisfies at least $(\frac{1}{2} + \epsilon)$-fraction of clauses unless $\mathbf{NP} \subseteq \mathbf{BPP}$. The previous best hardness using a polynomial time reduction achieves $\epsilon = (\log \log n)^{-c}$, which is obtained by the Label Cover hardness of Moshkovitz and Raz [J. ACM, 57(5), 2010] followed by the reduction from Label Cover to 3LIN of Hastad [J. ACM, 48(4):798--859, 2001].

Our main idea is to prove a hardness result for Label Cover similar to Moshkovitz and Raz where each projection has a linear structure. This linear structure of Label Cover allows us to use Hadamard codes instead of long codes, making the reduction more efficient. For the hardness of Linear Label Cover, we follow the work of Dinur and Harsha [SIAM J. Comput., 42(6):2452--2486, 2013] that simplified the construction of Moshkovitz and Raz, and observe that running their reduction from a hardness of the problem LIN (of unbounded arity) instead of the more standard problem of solving quadratic equations ensures the linearity of the resultant Label Cover.</summary>
    <updated>2019-07-16T01:10:59Z</updated>
    <published>2019-07-16T01:10:59Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-25T15:20:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/07/15/linkage</id>
    <link href="https://11011110.github.io/blog/2019/07/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Turkey has charged over 700 academics with terrorism for signing a peace petition (). Among the most severely penalized is Tuna Altınel, a mathematician in France who was arrested visiting family in Turkey, and who has now been imprisoned for over 50 days (via).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.insidehighered.com/news/2019/07/01/about-700-academics-have-been-criminally-charged-turkey-their-signatures-petition">Turkey has charged over 700 academics with terrorism for signing a peace petition</a> (<a href="https://mathstodon.xyz/@11011110/102367088461002886"/>). Among the most severely penalized is <a href="https://en.wikipedia.org/wiki/Tuna_Alt%C4%B1nel">Tuna Altınel</a>, a mathematician in France who was arrested visiting family in Turkey, and who <a href="http://math.univ-lyon1.fr/SoutienTunaAltinel/?lang=en">has now been imprisoned for over 50 days</a> (<a href="https://cameroncounts.wordpress.com/2019/05/31/tuna-altnel/">via</a>).</p>
  </li>
  <li>
    <p><a href="http://web.cs.elte.hu/~lovasz/bookxx/geomgraphbook/geombook2019.01.11.pdf">László Lovász’s book “Graphs and Geometry”, on geometric representations of graphs</a> (<a href="https://mathstodon.xyz/@11011110/102374639115017977"/>, <a href="https://news.ycombinator.com/item?id=20317825">via</a>). <a href="https://bookstore.ams.org/coll-65">The print version</a> should appear in a month or so from the AMS.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/quicktakes/2019/06/28/huge-budget-cut-university-alaska">University of Alaska budget gutted by 40%</a> (<a href="https://mathstodon.xyz/@11011110/102381510293748063"/>, <a href="https://www.metafilter.com/181768/We-dont-need-no-stinkin-edumaction">see also</a>). The total amount cut over the past five years (including this new biggest cut) is <a href="https://www.chronicle.com/article/Unprecedented-in-Our/246596">more like 63%, from $522M to $192M</a>. And <a href="https://www.npr.org/2019/07/03/738569508/university-of-alaska-readies-for-budget-slash-we-may-likely-never-recover">the likely response is to close one of its three main campuses and all 13 smaller community campuses</a>. Ironically, the cause is right-wing insistence on a universal basic income of $3000/person from fuel extraction revenues.</p>
  </li>
  <li>
    <p><a href="https://aperiodical.com/category/the-big-internet-math-off/">The annual Big Internet Math-off — view and vote on your favorites!</a> (<a href="https://mathstodon.xyz/@11011110/102385386477969762"/>). 
The first few matches include <a href="https://aperiodical.com/2019/07/the-big-internet-math-off-2019-group-1-alex-corner-vs-lucy-rycroft-smith/">commutativity of log-exponentiation vs weather infovis</a>, the <a href="https://aperiodical.com/2019/07/the-big-internet-math-off-2019-group-2-marianne-rachel-vs-vincent-pantaloni/">geometry of the Sydney Opera House vs straight lines on a donut</a>, <a href="https://aperiodical.com/2019/07/the-big-internet-math-off-2019-group-3-vicky-neale-vs-jim-propp/">multiplication tables and muffins</a> and <a href="https://aperiodical.com/2019/07/the-big-internet-math-off-2019-group-4-colin-beveridge-vs-kyle-d-evans/">a video on shapes in La Sagrada Familia (shot on location?!) vs an introduction to fractals</a>. More daily for roughly a month.</p>
  </li>
  <li>
    <p><a href="https://www.scmp.com/magazines/post-magazine/long-reads/article/3016267/chinese-scientists-guilty-researching-while">Chinese scientists guilty of “researching while Asian” in Trump’s America</a> (<a href="https://mathstodon.xyz/@11011110/102390640744960409"/>, <a href="https://news.ycombinator.com/item?id=20319936">via</a>, <a href="https://www.bloomberg.com/news/features/2019-06-13/the-u-s-is-purging-chinese-americans-from-top-cancer-research">see also</a>). The story focuses on star cancer researcher <a href="https://en.wikipedia.org/wiki/Xifeng_Wu">Xifeng Wu</a>, forced to resign from the University of Texas, apparently because she fostered collaboration with Chinese cancer research institutions at the behest of her higher administration.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@ccppurcell/102133405425258779">Chris Purcell thinks about graphs with degree sequence </a>. They have to have at least one cycle of each parity.</p>
  </li>
  <li>
    <p><a href="https://petapixel.com/2019/07/05/goodbye-aberration-physicist-solves-2000-year-old-optical-problem/">A formula for designing lenses with no spherical aberration</a> (<a href="https://mathstodon.xyz/@11011110/102401888807980335"/>, <a href="https://news.ycombinator.com/item?id=20369960">via</a>). This seems to have little practical value as there was already a numerical solution, and I don’t think it handles chromatic aberration, but it’s interesting that there is an analytic formula for these shapes.</p>
  </li>
  <li>
    <p>Last week I traveled to Milan for the <a href="https://sgp2019.di.unimi.it/">Symposium on Geometry Processing</a> (<a href="https://mathstodon.xyz/@11011110/102407039766140849"/>). They also have an <a href="https://twitter.com/geometryprocess">official twitter stream</a>, mostly consisting of event photos. The sightseeing highlight of my trip was seeing pages of <a href="https://www.ambrosiana.it/en/discover/codex-atlanticus/">Da Vinci’s Codex Atlanticus at the Ambrosian Library</a>.</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2019/07/08/check-out-this-cool-synthesize.html">Evoboxx</a> (<a href="https://mathstodon.xyz/@11011110/102412280507996705"/>), a retro-styled portable device that does only two things: run Conway’s Game of Life and generate sounds from it. Not very practical in these days of cell phones but then maybe that’s what makes it a fun project.</p>
  </li>
  <li>
    <p><a href="http://jdh.hamkins.org/modal-model-theory/">Modal model theory</a> (<a href="https://mathstodon.xyz/@11011110/102418646790775178"/>). For graphs, this extends first order logic (where the only quantification is over vertices and the only predicate is adjacency) with operators  and .  is true when all supergraphs model  and  is true when at least one supergraph models . This can express nontrivial graph properties like -colorability, and comes in two variants depending on whether you can quantify outside the operators.</p>
  </li>
  <li>
    <p><a href="https://www.instagram.com/p/ByH-R4Ql-NA/">Very quick video tutorial on how to make the Miura-ori fold</a>, by Polly Verity (<a href="https://mathstodon.xyz/@11011110/102429787082169299"/>, <a href="https://www.thisiscolossal.com/2019/07/new-polly-verity/">via</a>).</p>
  </li>
  <li>
    <p><a href="http://jtra.cz/stuff/essays/math-self-reference-smooth/index.html">Trávník’s smooth self-referential formula</a> (<a href="https://mathstodon.xyz/@11011110/102435762025301458"/>, <a href="https://twitter.com/johncarlosbaez/status/1141376710551601152">via</a>). It is actually a linked set of formulas, described in a typeset image, that when plotted as described in the image produces the image itself. It follows the same ideas as earlier self-referential formulas like <a href="https://en.wikipedia.org/wiki/Tupper%27s_self-referential_formula">Tupper’s self-referential formula</a> but unlike them describes a smooth vector image based on splines instead of a pixelated bitmap.</p>
  </li>
  <li>
    <p><a href="http://muurformules.nl/">Leiden wall formulas</a> (<a href="https://mathstodon.xyz/@11011110/102444110227078604"/>). The last time I was in Leiden they were decorating the exterior walls of all their buildings with poems of many different languages. Now they’ve moved on to the language of mathematics.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=eYfpSAxGakI">Numberphile video on Dehn invariants</a> (15-minutes; <a href="https://mathstodon.xyz/@11011110/102448538574760818"/>). The Dehn invariant is a value derived from a polyhedron that doesn’t change if you cut up the polyhedron into smaller polyhedral pieces and rearrange them into a different polyhedron. It’s 0 for the cube and nonzero for other Platonic solids, proving that they can’t be cut and rearranged into a cube. See <a href="https://en.wikipedia.org/wiki/Dehn_invariant">the Wikipedia article</a> for more technical details.</p>
  </li>
</ul></div>
    </content>
    <updated>2019-07-15T21:57:00Z</updated>
    <published>2019-07-15T21:57:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-07-16T04:58:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=16778</id>
    <link href="https://gilkalai.wordpress.com/2019/07/15/itai-benjamini-and-jeremie-brieussel-noise-sensitivity-meets-group-theory/" rel="alternate" type="text/html"/>
    <title>Itai Benjamini and Jeremie Brieussel: Noise Sensitivity Meets Group Theory</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The final  version of my ICM 2018 paper Three puzzles on mathematics computation and games is available for some time. (This proceeding’s version unlike the arXived version has a full list of references.)  In this post I would like to … <a href="https://gilkalai.wordpress.com/2019/07/15/itai-benjamini-and-jeremie-brieussel-noise-sensitivity-meets-group-theory/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The final  version of my ICM 2018 paper <a href="https://gilkalai.files.wordpress.com/2019/07/main-pf.pdf">Three puzzles on mathematics computation and games</a> is available for some time. (This proceeding’s version unlike the arXived version has a full list of references.)  In this post I would like to advertise one problem that I mentioned in the paper. You can read more about it in the paper  by Itai Benjamini and  Jeremie Brieussel  <a href="https://arxiv.org/abs/1901.03617">Noise sensitivity of random walks on groups</a> and learn about it also from the videotaped lecture by Jeremie. BTW, the name of my ICM paper is a tribute to Avi Wigdeson’s great book <strong><a href="https://www.math.ias.edu/files/Website03-25-19.pdf#page=1" target="&#x201D;_blank&#x201D;">Mathematics and Computation</a> </strong>(see <a href="https://gilkalai.wordpress.com/2017/10/27/must-read-book-by-avi-wigderson/">this post</a>). Click on the title for an  almost final draft of Avi’s book (March, 25, 2019) soon to be published by Princeton University Press<strong>. </strong>(We are negotiating with Avi on showing here first how the cover of his book will look like.)</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/07/friends-of-noise.png"><img alt="" class="alignnone size-full wp-image-17590" height="381" src="https://gilkalai.files.wordpress.com/2019/07/friends-of-noise.png?w=640&amp;h=381" width="640"/></a></p>
<p><a href="http://friendsofnoise.org/about/">source</a></p>
<h2>The problem of Benjamini and Brieussel and their conjecture</h2>
<p> </p>
<p/>
<p>Consider an <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>-step simple random walk (SRW) <img alt="X_n" class="latex" src="https://s0.wp.com/latex.php?latex=X_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_n"/> on a Cayley graph of a finitely generated infinite group <img alt="\Gamma" class="latex" src="https://s0.wp.com/latex.php?latex=%5CGamma&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Gamma"/>. Refresh independently each step with probability <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/>, to get <img alt="Y_n" class="latex" src="https://s0.wp.com/latex.php?latex=Y_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y_n"/> from <img alt="X_n" class="latex" src="https://s0.wp.com/latex.php?latex=X_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_n"/>. Are there groups for which at time <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> the positions <img alt="X_n" class="latex" src="https://s0.wp.com/latex.php?latex=X_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_n"/> and <img alt="Y_n" class="latex" src="https://s0.wp.com/latex.php?latex=Y_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y_n"/> are asymptotically independent? That is, does the <img alt="l_1" class="latex" src="https://s0.wp.com/latex.php?latex=l_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="l_1"/> (total variation) distance between the chain <img alt="(X_n, Y_n)" class="latex" src="https://s0.wp.com/latex.php?latex=%28X_n%2C+Y_n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(X_n, Y_n)"/> and two independent copies <img alt="(X'_n, X''_n)" class="latex" src="https://s0.wp.com/latex.php?latex=%28X%27_n%2C+X%27%27_n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(X'_n, X''_n)"/> go to 0, as <img alt="n \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n \to \infty"/>?</p>
<p>Note that on the line <img alt="\mathbb Z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb Z"/>, they are uniformally correlated, and therefore also on any group with a nontrivial homomorphism to <img alt="\mathbb R" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb R"/>, or on any group that has a finite index subgroup with a nontrivial homomorphism to <img alt="\mathbb R" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb R"/>. On the free group and for any non-Liouville group, <img alt="X_n" class="latex" src="https://s0.wp.com/latex.php?latex=X_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_n"/> and <img alt="Y_n" class="latex" src="https://s0.wp.com/latex.php?latex=Y_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y_n"/> are correlated as well, but for a different reason: both <img alt="X_n" class="latex" src="https://s0.wp.com/latex.php?latex=X_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_n"/> and <img alt="Y_n" class="latex" src="https://s0.wp.com/latex.php?latex=Y_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y_n"/> have a nontrivial correlation with <img alt="X_1" class="latex" src="https://s0.wp.com/latex.php?latex=X_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_1"/>.</p>
<p>Itai Benjamini and Jeremie Brieussel conjecture that these are the only ways not to be noise sensitive. That is, if a Cayley graph is Liouville and the group does not have a finite index subgroup with a homomorphism to the reals, then the Cayley graph is noise sensitive for the simple random walk. In particular, the Grigorchuk group is noise sensitive for the simple random walk!</p>
<h3>A paragraph of philosophical nature from Benjamini and Brieussel’s paper.</h3>
<p>“Physically, an <em>ℓ</em><img alt="^1" class="latex" src="https://s0.wp.com/latex.php?latex=%5E1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="^1"/>-noise sensitive process can somewhat not be observed, since the observation <img alt="Y^\rho_n" class="latex" src="https://s0.wp.com/latex.php?latex=Y%5E%5Crho_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y^\rho_n"/> does not provide any significant information on the actual output <img alt="X_n" class="latex" src="https://s0.wp.com/latex.php?latex=X_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_n"/>. Speculatively, this could account for the rarity of Liouville groups in natural science. Indeed besides virtually nilpotent ones, all known Liouville groups are genuinely mathematical objects .”</p>
<h3>Polytope integrality gap: An update</h3>
<p>An update on polytope integrality gap:  In my ICM paper and also in <a href="https://gilkalai.wordpress.com/2018/01/21/hardness-of-approximating-vertex-cover-polytope-integrality-gap-the-alswede-kachaterian-theorem-and-more/">this post</a>  I asked the beautiful problem that I learned from Anna Karlin if for vertex cover for every graph G and every vector of weights, there is an efficient algorithm achieving the “polytope integrality gap”.  Anna Karlin kindly informed me that <a href="https://www2.isye.gatech.edu/~msingh94/publications.html">Mohit Singh</a> got in touch with her after seeing the conjecture on my blog and pointed out that the hope for approximating the polytope integrality gap for vertex cover is unlikely to be possible because of its relationship to fractional chromatic number. Mohit noted that fractional chromatic number is hard to approximate even when it is constant assuming UGC. I still think that  the notion of polytope integrality gap for vertex cover as well as for more general problems is important and worth further study.</p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2019-07-15T08:13:08Z</updated>
    <published>2019-07-15T08:13:08Z</published>
    <category term="Algebra"/>
    <category term="Combinatorics"/>
    <category term="Probability"/>
    <category term="Itai Benjamini"/>
    <category term="Jeremie Brieussel"/>
    <category term="Noise-sensitivity"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-25T15:20:53Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4254868758435665114</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4254868758435665114/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/two-infinite-hat-problem-and-question.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4254868758435665114" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4254868758435665114" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/two-infinite-hat-problem-and-question.html" rel="alternate" type="text/html"/>
    <title>Two infinite hat problem and a question about what is ``well known''</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
This is a joint post with David Marcus. You will see how he is involved in my next post.<br/>
<br/>
Two infinite hat problems based on one scenario. I am also curious if they are well known.<br/>
<br/>
1) There are an infinite number of people, numbered 1,2,3,...  There are 2 colors of hats. They can all see everyone's hat but their own. <br/>
<br/>
2) The adversary is going to put hats on all the people. They will guess their own hat color<i> at the same time</i>. <br/>
<br/>
3) The people can discuss strategy ahead of time, but must use a deterministic strategy and the adversary knows the strategy.<br/>
<br/>
4) The people want to minimize how many they get wrong. <br/>
<br/>
5) The adversary puts on hats to maximize how many they get wrong.<br/>
<br/>
I ask two questions  and one meta-question:<br/>
<br/>
Q1: Is there a solution where they get all but a finite number of the guesses right? (I have blogged about a variant of this one a while back.)<br/>
<br/>
Q2: Is there a solution where they get all but at most (say) 18 wrong. (My students would say <i>the answer has to be YES or he</i> <i>wouldn't ask it</i>. They don't realize that I work on upper AND lower bounds!)<br/>
<br/>
Q3: How well known is problem Q1 and the solution?  Q2 and the solution? I've seen Q1 and its solution around (not sure where), but the only source on Q2 that I know of is CAN'T TELL YOU IN THIS POST, WILL IN THE NEXT POST. So, please leave a comment telling me if you have seen Q1 or Q2 and solutions. And if so then where.<br/>
<br/>
Feel free to leave any comments you want; however, I warn readers who want to solve it themselves to not look at the comments, or at my next post.<br/></div>
    </content>
    <updated>2019-07-15T03:12:00Z</updated>
    <published>2019-07-15T03:12:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-07-25T15:03:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4253</id>
    <link href="https://www.scottaaronson.com/blog/?p=4253" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4253#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4253" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">On two blog posts of Jerry Coyne</title>
    <summary xml:lang="en-US">A few months ago, I got to know Jerry Coyne, the recently-retired biologist at the University of Chicago who writes the blog “Why Evolution Is True.” The interaction started when Jerry put up a bemused post about my thoughts on predictability and free will, and if I pointed out that if he wanted to engage […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>A few months ago, I got to know <a href="https://en.wikipedia.org/wiki/Jerry_Coyne">Jerry Coyne</a>, the recently-retired biologist at the University of Chicago who writes the blog <a href="https://whyevolutionistrue.wordpress.com/">“Why Evolution Is True.”</a>  The interaction started when Jerry put up a <a href="https://whyevolutionistrue.wordpress.com/2019/01/15/a-computer-scientist-finds-the-question-of-free-will-uninteresting-for-bad-reasons/">bemused post about my thoughts on predictability and free will</a>, and if I pointed out that if he wanted to engage me on those topics, there was <a href="https://www.scottaaronson.com/papers/giqtm3.pdf">more to go on</a> than an 8-minute YouTube video.  I told Coyne that it would be a shame to get off on the wrong foot with him, since perusal of his blog made it obvious that whatever he and I disputed, it was dwarfed by our areas of agreement.  He and I exchanged more emails and had lunch in Chicago.</p>



<p>By way of explaining how he hadn’t read <a href="https://www.scottaaronson.com/papers/giqtm3.pdf">“The Ghost in the Quantum Turing Machine,”</a> Coyne emphasized the difference in my and his turnaround times: while these days I update my blog only a couple times per month, Coyne often updates multiple times per <em>day</em>.  Indeed the sheer volume of material he posts, on subjects from biology to culture wars to <a href="https://whyevolutionistrue.wordpress.com/2019/02/27/the-chicago-hot-dog-museum-and-our-wonderful-hot-dogs/">Chicago hot dogs</a>, would take months to absorb.</p>



<p>Today, though, I want to comment on just two posts of Jerry’s.</p>



<p>The <a href="https://whyevolutionistrue.wordpress.com/2019/05/17/computer-scientist-david-gelertner-drinks-the-academic-kool-aid-buys-into-intelligent-design/">first post</a>, from back in May, concerns <a href="https://en.wikipedia.org/wiki/David_Gelernter">David Gelernter</a>, the computer science professor at Yale who was infamously injured in a 1993 attack by the Unabomber, and who’s now mainly known as a right-wing commentator.  I don’t know Gelernter, though I did once attend a small interdisciplinary workshop in the south of France that Gelernter also attended, wherein I gave a talk about quantum computing and computational complexity in which Gelernter showed no interest.  Anyway, Gelernter, in an <a href="https://www.claremont.org/crb/article/giving-up-darwin/">essay in May for the <em>Claremont Review of Books</em></a>, argued that recent work has definitively disproved Darwinism as a mechanism for generating new species, and until something better comes along, Intelligent Design is the best available alternative.</p>



<p>Curiously, I think that Gelernter’s argument falls flat not for detailed reasons of biology, but mostly just because it indulges in <em>bad math and computer science</em>—in fact, in precisely the sorts of arguments that I was trying to answer in <a href="https://www.scottaaronson.com/blog/?p=1487">my segment on Morgan Freeman’s </a><em><a href="https://www.scottaaronson.com/blog/?p=1487">Through the Wormhole</a></em> (see also Section 3.2 of <a href="https://www.scottaaronson.com/papers/philos.pdf">Why Philosophers Should Care About Computational Complexity</a>).  Gelernter says that</p>



<ol><li>a random change to an amino acid sequence will pretty much always make it worse,</li><li>the probability of finding a useful new such sequence by picking one at random is at most ~1 in 10<sup>77</sup>, and</li><li>there have only been maybe ~10<sup>40</sup> organisms in earth’s history.</li></ol>



<p>Since 10<sup>77</sup> &gt;&gt; 10<sup>40</sup>, Darwinism is thereby refuted—not in principle, but as an explanation for life on earth.  QED. </p>



<p>The most glaring hole in the above argument, it seems to me, is that it simply ignores <em>intermediate</em> possible numbers of mutations.  How hard would it be to change, not 1 or 100, but 5 amino acids in a given protein to get a usefully different one—as might happen, for example, with local optimization methods like simulated annealing run at nonzero temperature?  And how many chances were there for <em>that</em> kind of mutation in the earth’s history?</p>



<p>Gelernter can’t personally see how a path could cut through the exponentially large solution space in a polynomial amount of time, so he asserts that it’s impossible.  Many of the would-be P≠NP provers who email me every week do the same.  But this particular kind of “argument from incredulity” has an abysmal track record: it would’ve applied equally well, for example, to problems like maximum matching that turned out to have efficient algorithms.  This is why, in CS, we demand better evidence of hardness—like completeness results or black-box lower bounds—neither of which seem however to apply to the case at hand.  Surely Gelernter understands all this, but had he not, he could’ve learned it from my lecture at the workshop in France!</p>



<p>Alas, online debate, as it’s wont to do, focused less on Gelernter’s actual arguments and the problems with them, than on the tiresome questions of “standing” and “status.”  In particular: does Gelernter’s authority, as a noted computer science professor, somehow lend new weight to Intelligent Design?  Or conversely: does the very fact that a computer scientist endorsed ID prove that computer science itself isn’t a real science at all, and that its practitioners should never be taken seriously in any statements about the real world?</p>



<p>It’s hard to say which of these two questions makes me want to bury my face deeper into my hands.  <a href="https://en.wikipedia.org/wiki/Serge_Lang">Serge Lang</a>, the famous mathematician and textbook author, spent much of his later life fervently denying the connection between HIV and AIDS.  <a href="https://en.wikipedia.org/wiki/Lynn_Margulis">Lynn Margulis</a>, the discoverer of the origin of mitochondria (and Carl Sagan’s first wife), died a 9/11 truther.  What broader lesson should we draw from any of this?  And anyway, what percentage of computer scientists actually do doubt evolution, and how does it compare to the percentage in other academic fields and other professions?  Isn’t the question of how divorced we computer scientists are from the real world an … ahem … <strong>empirical</strong> matter, one hard to answer on the basis of armchair certainties and anecdotes?</p>



<p>Speaking of empiricism, if you check Gelernter’s <a href="https://dblp.uni-trier.de/pers/hd/g/Gelernter:David">publication list on DBLP</a> and his <a href="https://scholar.google.ca/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=david+gelernter&amp;btnG=&amp;oq=david+geler">Google Scholar page</a>, you’ll find that he did influential work in programming languages, parallel computing, and other areas from 1981 through 1997, and then in the past 22 years published a grand total of … <strong>two</strong> papers in computer science.  One with four coauthors, the other a review/perspective piece about his earlier work.  So it seems fair to say that, some time after receiving tenure in a CS department, Gelernter pivoted (to put it mildly) away from CS and toward conservative punditry.  His recent offerings, in case you’re curious, include the book <a href="https://www.amazon.com/America-Lite-Imperial-Academia-Dismantled-Obamacrats/dp/1594036063/ref=sr_1_1?keywords=david+gelernter&amp;qid=1563047627&amp;s=gateway&amp;sr=8-1">America-Lite: How Imperial Academia Dismantled Our Culture (and Ushered In the Obamacrats)</a>.</p>



<p>Some will claim that this case underscores what’s wrong with the tenure system itself, while others will reply that it’s precisely what tenure was designed for, even if in this instance you happen to disagree with what Gelernter uses his tenured freedom to say.  The point I wanted to make is different, though.  It’s that the question “what kind of a field is computer science, anyway, that a guy can do high-level CS research on Monday, and then on Tuesday reject Darwinism and unironically use the word ‘Obamacrat’?”—well, even if I accepted the immense weight this question places on one atypical example (which I don’t), and even if I dismissed the power of compartmentalization (which I again don’t), the question <em>still</em> wouldn’t arise in Gelernter’s case, since getting from “Monday” to “Tuesday” seems to have taken him 15+ years.</p>



<p>Anyway, the second post of Coyne’s that I wanted to talk about is <a href="https://whyevolutionistrue.wordpress.com/2019/07/12/tarring-steve-pinker-and-others-with-jeffrey-epstein/">from just yesterday</a>, and is about Jeffrey Epstein—the financier, science philanthropist, and confessed sex offender, whose appalling crimes you’ll have read all about this week if you weren’t on a long sea voyage without Internet or something.</p>



<p>For the benefit of my many fair-minded friends on Twitter, I should clarify that I’ve never met Jeffrey Epstein, let alone accepted any private flights to his sex island or whatever.  I doubt he has any clue who I am either—even if he did once claim to be <a href="https://web.archive.org/web/20101112131000/http://www.jeffreyepsteinscience.com/2010/10/the-value-of-quantum-computing-to-jeffrey-epstein/">“intrigued”</a> by quantum information.</p>



<p>I do know a few of the scientists who Epstein once hung out with, including Seth Lloyd and Steven Pinker.  Pinker, in particular, is now facing vociferous attacks on Twitter, similar in magnitude perhaps to what I faced in the comment-171 affair, for having been photographed next to Epstein at a 2014 luncheon that was hosted by Lawrence Krauss (a physicist who later faced sexual harassment allegations of his own).  By the evidentiary standards of social media, this photo suffices to convict Pinker as basically a child molester himself, and is <em>also</em> a devastating refutation of any data that Pinker might have adduced in his books about the Enlightenment’s contributions to human flourishing.</p>



<p>From my standpoint, what’s surprising is not that Pinker is up against this, but that it <em>took this long</em> to happen, given that Pinker’s pro-Enlightenment, anti-blank-slate views have had the effect of painting a giant red target on his back.  Despite the near-inevitability, though, you can’t blame Pinker for wanting to defend himself, as I did when it was my turn for the struggle session.</p>



<p>Thus, in response to an emailed inquiry by Jerry Coyne, Pinker shared some detailed reflections about Epstein; Pinker then gave Coyne permission to post those reflections on his blog (though they were originally meant for Coyne only).  Like everything Pinker writes, they’re <a href="https://whyevolutionistrue.wordpress.com/2019/07/12/tarring-steve-pinker-and-others-with-jeffrey-epstein/">worth reading in full</a>.  Here’s the opening paragraph:</p>



<blockquote class="wp-block-quote"><p>The annoying irony is that I could never stand the guy [Epstein], never took research funding from him, and always tried to keep my distance. Friends and colleagues described him to me as a quantitative genius and a scientific sophisticate, and they invited me to salons and coffee klatches at which he held court. But I found him to be a kibitzer and a dilettante — he would abruptly change the subject ADD style, dismiss an observation with an adolescent wisecrack, and privilege his own intuitions over systematic data.</p></blockquote>



<p>Pinker goes on to discuss his record of celebrating, and extensively documenting, the forces of modernity that led to dramatic reductions in violence against women and that have the power to continue doing so.  On Twitter, Pinker had <a href="https://twitter.com/sapinker/status/1149154274787627010">already written</a>: “Needless to say I condemn Epstein’s crimes in the strongest terms.”</p>



<p>I probably should’ve predicted that Pinker would then be attacked again—this time, for having prefaced his condemnation with the phrase “needless to say.”  The argument, as best I can follow, runs like this: given all the isms of which woke Twitter has already convicted Pinker—scientism, neoliberalism, biological determinism, etc.—how could Pinker’s being against Epstein’s crimes (which we recently learned probably include the <a href="https://www.cnn.com/videos/us/2019/07/10/jeffrey-epstein-accuser-speaks-out-today-show-nbc-intv-sot-newday-vpx.cnn">rape</a>, and not only statutorily, of a 15-year-old) <em>possibly</em> be assumed as a given?</p>



<p>For the record, just as Epstein’s friends and enablers weren’t confined to one party or ideology, so the public condemnation of Epstein strikes me as a matter that is (or should be) beyond ideology, with all reasonable dispute now confined to the space between “very bad” and “extremely bad,” between “lock away for years” and “lock away for life.”</p>



<p>While I didn’t need Pinker to tell me <em>that</em>, one reason I personally appreciated his comments is that they helped to answer a question that had bugged me, and that none of the mountains of other condemnations of Epstein had given me a clear sense about.  Namely: supposing, hypothetically, that I’d met Epstein around 2002 or so—without, of course, knowing about his crimes—would I have been as taken with him as many other academics seem to have been?  (Would <em>you</em> have been?  How sure are you?)</p>



<p>Over the last decade, I’ve had the opportunity to meet some titans and semi-titans of finance and business, to discuss quantum computing and other nerdy topics.  For a few (by no means all) of these titans, my overriding impression was <em>precisely</em> their unwillingness to concentrate on any one point for more than about 20 seconds—as though they wanted the crust of a deep intellectual exchange without the meat filling.  My experience with them fit Pinker’s description of Epstein to a T (though I hasten to add that, as far as I know, none of these others ran teenage sex rings).</p>



<p>Anyway, given all the anger at Pinker for having intersected with Epstein, it’s ironic that I could easily imagine Pinker’s comments rattling Epstein the most of anyone’s, if Epstein hears of them from his prison cell.  It’s like: Epstein must have developed a skin like a rhinoceros’s by this point about being called a child abuser, a creep, and a thousand similar (and similarly deserved) epithets.  But “a kibitzer and a dilettante” who merely lured famous intellectuals into his living room, with wads of cash not entirely unlike the ones used to lure teenage girls to his massage table?  Ouch!</p>



<p>OK, but what about Alan Dershowitz—the man who apparently used to be Epstein’s close friend, who still is Pinker’s friend, and who played a crucial role in securing Epstein’s 2008 plea bargain, the one now condemned as a travesty of justice?  I’m not sure how I feel about Dershowitz.  It’s like: I understand that our system requires attorneys willing to mount a vociferous defense even for clients who they privately know or believe to be guilty—and even to get those clients off on technicalities or bargaining whenever they can.  I’m also incredibly grateful that I chose CS rather than law school, because I don’t think I could last an hour advocating causes that I knew to be unjust.  Just like my fellow CS professor, the intelligent design advocate David Gelernter, I have the privilege and the burden of speaking only for myself.</p></div>
    </content>
    <updated>2019-07-13T23:33:12Z</updated>
    <published>2019-07-13T23:33:12Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Obviously I'm Not Defending Aaronson"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-07-19T23:00:05Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/07/13/connectivity-finiteness-modal</id>
    <link href="https://11011110.github.io/blog/2019/07/13/connectivity-finiteness-modal.html" rel="alternate" type="text/html"/>
    <title>Connectivity and finiteness in modal graph logic</title>
    <summary>I read with interest Joel David Hamkins’ recent blog post on modal model theory. This week, on a long plane flight home from Italy, I was inspired to play with the modal logic of graphs, in which one describes properties of graphs by simpler properties of their (induced) supergraphs. My interest is less in what this says about set theory and model theory, and more in how expressive this language is: which graph properties can it describe? Joel showed in his post how to describe -colorability in this theory, but I thought it would be of interest to start with something simpler than an -complete problem. And what could be simpler for graphs than testing whether a graph is connected or finite?</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I read with interest Joel David Hamkins’ recent blog post on <a href="http://jdh.hamkins.org/modal-model-theory/">modal model theory</a>.
This week, on a long plane flight home from Italy, I was inspired to play with the modal logic of graphs, in which one describes properties of graphs by simpler properties of their (induced) supergraphs. My interest is less in what this says about set theory and model theory, and more in how expressive this language is: which graph properties can it describe? Joel showed in his post how to describe -colorability in this theory, but I thought it would be of interest to start with something simpler than an -complete problem. And what could be simpler for graphs than testing whether a graph is connected or finite?</p>

<h1 id="the-basics-of-modal-graph-logic">The basics of modal graph logic</h1>

<p><a href="https://en.wikipedia.org/wiki/Modal_logic">Modal logic</a> is a logic of “possible worlds” with two operators on formulas,  (it is possible for  to be true, in at least one possible world) and  (it is necessary for  to be true, in all possible worlds).
Modal graph logic applies this idea to the first-order <a href="https://en.wikipedia.org/wiki/Logic_of_graphs">logic of graphs</a>, in which one can quantify over variables that represent graph vertices and use a binary predicate  representing adjacency of pairs of vertices. I’ll assume here that all graphs are simple and undirected (i.e.,  is irreflexive and commutative). A given graph  models a given first-order formula  (written as ) if the formula becomes true when you evaluate it in the obvious way for the given graph. In the modal logic of graphs, the possible worlds are graphs containing  as an induced subgraph. So  means that it is possible for  to be modeled by one of these larger graphs and  means that it is necessary for  to be modeled by all of these larger graphs. If we apply a modal operator to a formula  that is itself modal, the possible worlds of the modal operators within  are supergraphs of these larger graphs, in the same way (in modal logic terminology, we are assuming S4 accessibility of possible worlds).</p>

<p>In some ways this is enormously powerful: the class of all graphs containing  is so big that it’s not even a set, and it can encode arbitrarily complicated structures. In other ways this is more highly constrained than other graph logics like monadic second-order logic (in which one can describe and quantify sets of vertices or edges, but not more complicated structures like sequences of vertices or strategy trees over games defined on the graph). The issue is that in a possible world, one can’t tell the vertices that were part of the base graph apart from the ones that were added later, except possibly for a finite set of vertices named in variables outside the modal operator. So while the possible worlds that model a given formula can be very large and complicated, it can be difficult to anchor these castles in the air to the base graph that you started with.</p>

<p>Joel’s description of how to express -colorability suggests a path around this difficulty: Suppose we want to test a hereditary property  of graphs (one that extends from any graph to its induced subgraphs) such as colorability. Then we should look for a family of self-verifyingly- graphs: a family of graphs with property  such that membership in the family can be tested by a first-order formula  and such that every graph with property  is an induced subgraph of a larger graph in this family. If we can find such a family and first-order formula , then  will describe property  itself. For instance, for colorability, the self-verifyingly--colorable graphs are graphs in which each color class has a universal vertex, every vertex is adjacent to all but one of the universal vertices, and no two adjacent vertices are both non-adjacent to the same universal vertex.</p>

<p>Similar ideas can also work when the property is not hereditary (for instance, a graph has chromatic number 3 when it is 3-colorable but not 2-colorable) or when checking membership in the family of self-verifying graphs itself involves modal logic (as we’ll see for testing finiteness).</p>

<h1 id="connectivity">Connectivity</h1>

<p>Connectivity is not hereditary: every connected graph is part of a larger disconnected graph and vice versa. But the property that some particular pair of vertices  is separated is hereditary: if  is separated from , the two vertices remain separated in any induced subgraph that contains them both. And while it’s not possible to verify this directly in a first order formula, for all graphs, it is possible in a special family of disconnected graphs, the ones containing a <em>transitive vertex</em>, one whose neighborhood forms a connected component of the graph. We can define a formula</p>



<p>which characterizes these transitive vertices. (Here I am using  to mean syntactic equivalence of formulas or definition of the name of a formula, rather than its meaning in Joel’s post, equivalence of models.) Then we can test whether  is separated from  by the formula</p>



<p>(Here, the instance of “transitive” on the right hand side is not a unary predicate, even though it looks like one; it should be expanded by the definition of the “transitive” formula to produce the resulting “separated” formula. Think of it as being like a C preprocessor macro.)
If  is indeed separated from , there is a possible world modeling the formula, in which we add an extra vertex  to the starting graph, adjacent to everything in the connected component of . And in any possible world modeling the formula,  is separated from , and this must remain true in every induced subgraph of this possible world, including the base graph. Finally, a graph is connected if and only if it has no separated pair:</p>



<p>In MSO logic, one of the standard tools is the <em>method of syntactic interpretations</em>. This allows you to modify your base graph  to form a different graph  (for any of certain standard types of modification) and test whether the resulting graph models a given formula . To do this, you instead modify the formula (in certain purely mechanical ways derived from how you were modifying ) and test whether your original graph models the modified formula . The same thing works in first-order logic and in modal logic, and allows such modifications as adding or removing an edge between given vertices, removing any given vertex, restricting to a logically-specified induced subgraph, or adding a new vertex with a logically-specified adjacency relation. I’ll write  for the modified formula that simulates formula  on the modified graph . We can use this idea to extend connectedness to other properties; for instance, a graph is a forest if it has no cycle, and this is true if every edge removal disconnects it:</p>



<h1 id="finiteness">Finiteness</h1>

<p>Following the earlier outline,
I’d like to find a simple family of finite graphs for which their finiteness is
so obvious that it can be tested by a simple logical formula, and then embed
every finite graph into one of these simple finite graphs.</p>

<p>The first natural choice of such a family is the family of paths.
We can define a path to be a connected graph with exactly two degree-one vertices in which all remaining vertices have degree two. Checking the degrees is first order, but I’ll spare you the messy details of the formula. All such graphs are finite, because a graph is connected if and only if every two vertices are a finite distance apart, and when the endpoints of a path are a finite distance apart there can be only finitely many other degree-two vertices between them. Any other vertices outside of this finite set must belong to a different component.</p>

<p>Not every finite graph can be embedded into a path. However, every finite graph has a perfect matching to the vertices of a path. To check the existence of an induced perfect matching between two sets of vertices in a graph, it’s helpful to have an extra vertex  that is not part of the matching, but that distinguishes one side of the matching (the neighbors of ) from the other side:</p>



<p>Here,  is shorthand for the <a href="https://en.wikipedia.org/wiki/Uniqueness_quantification">existence of exactly one thing</a>. With this test for an induced perfect matching in hand, we can check for a perfect matching to a finite path by</p>



<p>where  denotes the open neighborhood of , the graph induced by the vertices adjacent to .</p>

<p>I don’t think it’s possible to test finiteness in the same way in MSO.
We can define paths in MSO, and force them to have a perfect induced matching to the remaining vertices. But the recipe above breaks down at the point where it embeds the given graph into a supergraph, not generally possible in MSO. More generally I don’t think it’s possible in MSO to distinguish the family of finite complete graphs from their limit, the countable complete graph.</p>

<h1 id="additional-properties">Additional properties</h1>

<p>The same technology of paths and matchings can also be used to formulate not-very-natural properties of finite graphs that are definitely not expressible in MSO. For instance, we can check whether a graph  is the disjoint union of two equal-length paths, by first checking that it is a forest with four degree-one vertices and the rest degree-two, and then checking that both paths can be simultaneously perfectly matched to a third path of remaining vertices (with an additional vertex used to distinguish the sides of the matching as above). When this structure exists, the whole graph forms a polyhedron in the form of a  grid with the sides of the grid connected to the distinguishing vertex, and this polyhedral structure can be used to prove that we cannot trick the formula by adding new paths connecting the original path endpoints. In contrast, MSO cannot express equality of path lengths, because (per <a href="https://en.wikipedia.org/wiki/Courcelle%27s_theorem">Courcelle’s theorem</a>) it can only express properties of path-like graphs that can be expressed as regular languages (over bounded-width path-decompositions of the graph), and equality of length is context-free but not regular for path-decompositions that concatenate the two paths separately.</p>

<p>It’s also possible to express that the <a href="https://en.wikipedia.org/wiki/Degeneracy_(graph_theory)">degeneracy of a graph</a> is at most some constant  in modal logic. As special cases, the graphs of degeneracy zero are the independent sets (first-order expressible) and the graphs of degeneracy one are the forests, which we’ve already seen how to express. Otherwise, we can represent sequences of vertices by <em>ladders</em>, paths of degree-three vertices matched to independent sets of degree-two vertices, with the represented sequence being the other endpoints of these degree-two vertices. Adding a ladder to a graph of degeneracy at least two doesn’t change its degeneracy, and we can express the existence of a degeneracy ordering of a finite graph (a sequence of the vertices such that all vertices have at most  neighbors that are later in the sequence) by the addition of a ladder with a designated starting vertex that touches all non-ladder vertices.
Each non-ladder vertex  should have at most  neighbors with the property that, within the ladder, the ladder vertex nearest  separates the top of the ladder from the ladder vertices nearest these neighbors.</p>

<p>We can <a href="https://11011110.github.io/blog/2019/01/17/orientations-infinite-graphs.html">extend the concept of degeneracy from finite to infinite graphs</a>
by requiring that every finite subgraph have degeneracy at most .
With this definition, we can identify a finite subgraph as the neighbors of a ladder, and then use the formula for finite-graph degeneracy on these neighbors.
This leads to a modal logic expression for infinite graph degeneracy in which the overall structure of the formula is that it is necessary that, whenever a vertex  forms the start of a ladder, it should be possible for there to exist another ladder defining a degeneracy ordering for ’s ladder and its neighbors.</p>

<p>I suspect, although I haven’t worked out all the details, that planarity testing is also expressible in modal logic. Here’s the outline of an idea for proving this. First, we can check that the graph is finite, to avoid complications of which infinite graphs should be considered to be planar or what a drawing of an infinite planar graph might look like. Next, a graph is planar if and only if it is an induced subgraph of a maximal planar graph, one in which all edges belong to exactly two <a href="https://en.wikipedia.org/wiki/Peripheral_cycle">peripheral triangles</a>. A graph with this two-peripheral-triangle property is planar if and only if one can partition its edges into two subsets, one of which forms a spanning tree for the graph and the other of which forms a spanning tree for the dual graph of the peripheral cycles. (The two-peripheral-triangle property defines a surface embedding which, if non-planar, would have some leftover edges that are neither part of a spanning tree nor a complementary dual spanning tree; see my paper “<a href="https://www.ics.uci.edu/~eppstein/pubs/p-dyngen.html">Dynamic generators of topologically embedded graphs</a>”.) And it should be possible to describe this partition in a planarity-preserving way by decorating the edges on one side of the partition by additional small planar graphs (maybe even just attaching a triangle to each decorated edge). So all we need to check is that it’s possible for the given graph to be part of a larger graph
that looks like a maximal planar graph with some decorated edges (ignoring the decorations, each edge belongs to two peripheral triangles) and that the decorations describe a spanning tree and a dual spanning tree. We already know how to describe spanning trees and dual spanning trees should also be possible using similar logic.</p>

<p>One natural and simple property that I don’t see how to express in modal logic is regularity. One can ask: do each two vertices have the same degree? And equality of sets of vertices can be checked by the existence of perfect matchings, as in the two-equal-paths example. But how do we know, in a possible world, which neighbors of two given vertices are original and which are added? For the same reason, the property of having a perfect matching seems difficult to express in modal logic, even though it is easy in MSO. Again, it seems difficult to impose any extra structure on a supergraph without losing too much information about which parts of the graph are original.</p>

<h1 id="standard-and-nonstandard-models">Standard and nonstandard models</h1>

<p>I have been (deliberately) naive here about what kind of set theory I am using to define my graphs, what “all induced supergraphs” means (do I consider graphs only over some set of candidate vertices, or the proper class of all graphs in some set theory), and whether there is always a “correct” value of  that our models of modal graph logic should produce for a given graph and formula. If these naive assumptions are not valid, the description of what these formulas express may be inaccurate.</p>

<p>In particular, the claim that a graph is connected if and only if every two vertices are a finite distance apart uses concepts of distance that go beyond the first-order theory of graphs. In non-standard models of set theory, or in standard models of set theory but with non-standard collections of possible worlds that are not really the collection of all induced supergraphs of a given graph, that claim may fail to be true. In such cases, the finiteness formula may determine that an infinite graph (one that models the first-order logical formulas stating that there exist at least  distinct vertices, for every finite integer ) is finite. It’s not a bug in the formula, just an indication that you need to be careful about your models. Or in computer science terms, <a href="https://en.wikipedia.org/wiki/Garbage_in,_garbage_out">GIGO</a>.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102438063877916451">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-07-13T21:57:00Z</updated>
    <published>2019-07-13T21:57:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-07-16T04:58:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16085</id>
    <link href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/" rel="alternate" type="text/html"/>
    <title>Tools and Sensitivity</title>
    <summary>Cutting right through a 30-year-old conjecture Cropped from Emory homepage Hao Huang is a mathematician and computer scientist at Emory University. Last week he released a paper of only six pages that solves the Boolean Sensitivity Conjecture, which goes back at least to a 1992 paper by Noam Nisan and Mario Szegedy. Today we discuss […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Cutting right through a 30-year-old conjecture</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/haohuangcropped/" rel="attachment wp-att-16086"><img alt="" class="alignright wp-image-16086" height="212" src="https://rjlipton.files.wordpress.com/2019/07/haohuangcropped.jpg?w=148&amp;h=212" width="148"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Emory <a href="http://www.mathcs.emory.edu/~hhuan30/">homepage</a></font></td>
</tr>
</tbody>
</table>
<p>
Hao Huang is a mathematician and computer scientist at Emory University. Last week he released a <a href="http://www.mathcs.emory.edu/~hhuan30/papers/sensitivity_1.pdf">paper</a> of only six pages that solves the Boolean Sensitivity Conjecture, which goes back at least to a 1992 <a href="https://www.researchgate.net/publication/2508255_On_the_Degree_of_Boolean_Functions_as_Real_Polynomials">paper</a> by Noam Nisan and Mario Szegedy.</p>
<p>
Today we discuss his brilliant proof and what it means for sensitivity of the <em>tools</em> one employs.<br/>
<span id="more-16085"/></p>
<p>
Several of our blogging friends have <a href="https://www.scottaaronson.com/blog/?p=4229">covered</a> this <a href="https://gilkalai.wordpress.com/2019/07/02/amazing-hao-huang-proved-the-sensitivity-conjecture/">news</a> in <a href="https://blog.computationalcomplexity.org/2019/07/local-kid-makes-history.html">posts</a> <a href="https://windowsontheory.org/2019/07/02/sensitivity-conjecture-proved/">already</a>, and Ryan O’Donnell even summarized the proof in one <a href="https://twitter.com/BooleanAnalysis/status/1145837576487612416">tweet</a>. Scott Aaronson’s thread includes a <a href="https://www.scottaaronson.com/blog/?p=4229#comment-1813116">comment</a> by Huang on how he came by his proof. </p>
<p>
We will try to draw implications for the related matter of how <em>you</em> might come by proofs of <em>other</em> conjectures. We have previously <a href="https://rjlipton.wordpress.com/2016/04/09/missing-mate-in-ten/">discussed</a> the possibility of overlooking short solutions to major problems. Here we will discuss how to <em>find</em> them.</p>
<p>
</p><p/><h2> A Graph Puzzle </h2><p/>
<p/><p>
To get a flavor of what Huang proved, consider the graph of an ordinary <a href="https://commons.wikimedia.org/wiki/File:Cube_graph.png">cube</a>:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/cube_graph/" rel="attachment wp-att-16087"><img alt="" class="aligncenter wp-image-16087" height="181" src="https://rjlipton.files.wordpress.com/2019/07/cube_graph.png?w=192&amp;h=181" width="192"/></a></p>
<p/><p><br/>
The question is, <em>can you color 5 vertices red so that no red node has 3 red neighbors?</em> Your first impulse might be to color 4 nodes red according to parity so that none has a red neighbor, per below left:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/cube_graph_tries/" rel="attachment wp-att-16088"><img alt="" class="aligncenter wp-image-16088" height="175" src="https://rjlipton.files.wordpress.com/2019/07/cube_graph_tries.png?w=450&amp;h=175" width="450"/></a></p>
<p/><p><br/>
But then any 5th node will have 3 red neighbors. Another “greedy” idea is to pack a subgraph of the allowed degree 2 into half the cube, as at right. Any 5th node will again create a degree-3 vertex in the subgraph induced by the red nodes.</p>
<p>
The answer is that actually one can pack 6 nodes that induce a simple cycle:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/cube_graph_solved/" rel="attachment wp-att-16089"><img alt="" class="aligncenter wp-image-16089" height="181" src="https://rjlipton.files.wordpress.com/2019/07/cube_graph_solved.png?w=192&amp;h=181" width="192"/></a></p>
<p/><p><br/>
Now let’s up the dimension by one—that is, take <img alt="{n = 4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 4}"/> and <img alt="{N = 2^n = 16}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+2%5En+%3D+16%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N = 2^n = 16}"/>. How many nodes can we color red and keep the induced degree 2? </p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/hypercube_graph/" rel="attachment wp-att-16091"><img alt="" class="aligncenter wp-image-16091" height="230" src="https://rjlipton.files.wordpress.com/2019/07/hypercube_graph-1.png?w=300&amp;h=230" width="300"/></a></p>
<p/><p><br/>
Again the parity trick gives us degree 0 with 8 nodes, but then we can’t add a 9th. We can greedily try to pack the outer cube with our 6-node solution, but then—perhaps surprisingly—we can add only 2 more red nodes from the inner cube. So we can only do 5 from the outer cube. We can get 9 overall by:</p>
<p><a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/hypercube_graph_try10/" rel="attachment wp-att-16092"><img alt="" class="aligncenter wp-image-16092" height="230" src="https://rjlipton.files.wordpress.com/2019/07/hypercube_graph_try10.png?w=300&amp;h=230" width="300"/></a></p>
<p/><p><br/>
The fact that one red node is isolated seems to give room to improve, but there is no way to make 10. </p>
<p>
</p><p/><h2> The Theorem </h2><p/>
<p/><p>
The calculations have left an interesting jump from degree 0 with eight red nodes and degree 2 with nine. How about degree 1? Can we do that with 9 nodes? We can pack four disjoint edges but then there is nowhere to stick an isolated node. </p>
<p>
So for 9 nodes, which is <img alt="{\frac{N}{2} + 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BN%7D%7B2%7D+%2B+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{N}{2} + 1}"/>, the best we can do is degree 2, which is <img alt="{\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{n}}"/>. This is what Huang proved:</p>
<blockquote><p><b>Theorem 1</b> <em><a name="graphs"/> Every subgraph induced by <img alt="{\frac{N}{2} + 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BN%7D%7B2%7D+%2B+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\frac{N}{2} + 1}"/> nodes of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/>-dimensional hypercube graph has a node of degree at least <img alt="{\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\sqrt{n}}"/>. </em>
</p></blockquote>
<p/><p>
This is completely tight. When <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is a perfect square there is a way to achieve <img alt="{\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{n}}"/> as the maximum degree (shown <a href="https://pdfs.semanticscholar.org/3917/3e0cb4e028c94328f1355bf02febea132127.pdf">here</a>). Otherwise the least integer above <img alt="{\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{n}}"/> is best. Thus every subgraph of the <img alt="{5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{5}"/>-cube induced by 17 nodes has a node with three neighbors, but you can go as high as 257 nodes in the <img alt="{9}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B9%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{9}"/>-cube while keeping the maximum degree to 3.</p>
<p>
We will mention the relation to Boolean sensitivity only briefly. The nodes of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-cube correspond to truth assignments in <img alt="{\{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{0,1\}^n}"/>. Since every red node <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> has <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> neighbors in the cube but at most <img alt="{\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{n}}"/> red neighbors, the color function is highly sensitive to bitflips. But every flip also changes the parity of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Hence the <em>exclusive-or</em> of the color function with the parity function has <em>low</em> sensitivity. </p>
<p>
But not too low: Huang proved it is at least <img alt="{\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{n}}"/>. That was enough to prove the conjecture. I’ve cut two sections on Boolean sensitivity from this post’s <a href="https://cse.buffalo.edu/~regan/cse705/wsensitivity.pdf">original draft</a>—let’s just say the connection to the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-cube and graph degree was known since this 1992 <a href="https://www.sciencedirect.com/science/article/pii/0097316592900608">paper</a>. Here we’ll focus on what it took to prove this theorem.</p>
<p>
</p><p/><h2> The Proof </h2><p/>
<p/><p>
From my undergrad days I’ve kept an interest in spectral graph theory. One of the basic facts is that the degree <img alt="{d(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d(G)}"/> of a graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is always at least as great as the largest eigenvalue <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> of its adjacency matrix <img alt="{A_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_G}"/>. For a <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d}"/>-regular graph they are equal. Huang’s first trick is to note that the classic proof of this also allows <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1}"/> values on edges:</p>
<blockquote><p><b>Lemma 2</b> <em> Let <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> be a symmetric matrix obtained from <img alt="{A_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A_G}"/> by multiplying some entries by <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{-1}"/> and <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\lambda}"/> any of its eigenvalues. Then <img alt="{d(G) \geq |\lambda|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%28G%29+%5Cgeq+%7C%5Clambda%7C%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{d(G) \geq |\lambda|}"/>. </em>
</p></blockquote>
<p/><p>
<em>Proof:</em>  Choose an eigenvector <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> such that <img alt="{Av = \lambda v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAv+%3D+%5Clambda+v%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Av = \lambda v}"/> and take an index <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> that maximizes <img alt="{|v_i|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cv_i%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|v_i|}"/>. Then </p>
<p align="center"><img alt="\displaystyle  |\lambda v_i| = |(A v)_i| = |\sum_j A_{i,j} v_j| \leq |\sum_j A_{i,j}| \cdot |v_i| \leq \sum_{(i,j) \in E(G)} |A_{i,j}|\cdot |v_i| \leq d(G)|v_i|. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%5Clambda+v_i%7C+%3D+%7C%28A+v%29_i%7C+%3D+%7C%5Csum_j+A_%7Bi%2Cj%7D+v_j%7C+%5Cleq+%7C%5Csum_j+A_%7Bi%2Cj%7D%7C+%5Ccdot+%7Cv_i%7C+%5Cleq+%5Csum_%7B%28i%2Cj%29+%5Cin+E%28G%29%7D+%7CA_%7Bi%2Cj%7D%7C%5Ccdot+%7Cv_i%7C+%5Cleq+d%28G%29%7Cv_i%7C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |\lambda v_i| = |(A v)_i| = |\sum_j A_{i,j} v_j| \leq |\sum_j A_{i,j}| \cdot |v_i| \leq \sum_{(i,j) \in E(G)} |A_{i,j}|\cdot |v_i| \leq d(G)|v_i|. "/></p>
<p>Dividing out <img alt="{|v_i|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cv_i%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|v_i|}"/> gives the lemma. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p/><p><br/>
So now what we want to do is find conditions that force <img alt="{\lambda = \sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda+%3D+%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda = \sqrt{n}}"/> when <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is a <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/>-vertex subgraph of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-cube with <img alt="{m \geq \frac{N}{2} + 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%5Cgeq+%5Cfrac%7BN%7D%7B2%7D+%2B+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m \geq \frac{N}{2} + 1}"/>, where <img alt="{N = 2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N = 2^n}"/>. The trick that Huang realized is that he could do this by making <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> sit inside a matrix <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_N}"/> with at least <img alt="{\frac{N}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BN%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{N}{2}}"/> eigenvalues of <img alt="{+\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+\sqrt{n}}"/>. </p>
<p>
To see how, form <img alt="{A_{N-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7BN-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{N-1}}"/> by knocking out the last row and column of <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_N}"/>. Since <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_N}"/> and <img alt="{A_{N-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7BN-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{N-1}}"/> are both real and symmetric, their eigenvalues are real, so we can order them <img alt="{\lambda_1,\dots,\lambda_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_1%2C%5Cdots%2C%5Clambda_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda_1,\dots,\lambda_N}"/> and <img alt="{\mu_1,\dots,\mu_{N-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu_1%2C%5Cdots%2C%5Cmu_%7BN-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu_1,\dots,\mu_{N-1}}"/> in nonincreasing order. The basic fact is that they always <em>interlace</em>: </p>
<p align="center"><img alt="\displaystyle  \lambda_1 \geq \mu_1 \geq \lambda_2 \geq \mu_2 \geq \lambda_3 \geq \cdots \geq \mu_{N-1} \geq \lambda_N. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_1+%5Cgeq+%5Cmu_1+%5Cgeq+%5Clambda_2+%5Cgeq+%5Cmu_2+%5Cgeq+%5Clambda_3+%5Cgeq+%5Ccdots+%5Cgeq+%5Cmu_%7BN-1%7D+%5Cgeq+%5Clambda_N.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \lambda_1 \geq \mu_1 \geq \lambda_2 \geq \mu_2 \geq \lambda_3 \geq \cdots \geq \mu_{N-1} \geq \lambda_N. "/></p>
<p>See <a href="https://arxiv.org/pdf/math/0502408.pdf">this</a> for a one-page proof. The neat point is that you can repeat this: if you get <img alt="{A''}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A''}"/> by knocking out another row and corresponding column, and <img alt="{[\nu_i]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5B%5Cnu_i%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{[\nu_i]}"/> are its eigenvalues in order, then </p>
<p align="center"><img alt="\displaystyle  \mu_1 \geq \nu_1 \geq \mu_2 \geq \nu_2 \geq \mu_3 \cdots. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu_1+%5Cgeq+%5Cnu_1+%5Cgeq+%5Cmu_2+%5Cgeq+%5Cnu_2+%5Cgeq+%5Cmu_3+%5Ccdots.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mu_1 \geq \nu_1 \geq \mu_2 \geq \nu_2 \geq \mu_3 \cdots. "/></p>
<p>It follows that <img alt="{\lambda_1 \geq \nu_1 \geq \lambda_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_1+%5Cgeq+%5Cnu_1+%5Cgeq+%5Clambda_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda_1 \geq \nu_1 \geq \lambda_3}"/>. If you do this again, you get a matrix whose leading eigenvalue is still at least as big as <img alt="{\lambda_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda_4}"/>. Do it <img alt="{\frac{N}{2} - 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BN%7D%7B2%7D+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{N}{2} - 1}"/> times inside <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_N}"/>, and you’re still above <img alt="{\lambda_{N/2}(A_N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_%7BN%2F2%7D%28A_N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda_{N/2}(A_N)}"/>, which we just said we will arrange to be <img alt="{+\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+\sqrt{n}}"/>. Thus if we knock out the <img alt="{\frac{N}{2} - 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BN%7D%7B2%7D+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{N}{2} - 1}"/> white nodes, we will get the graph on the red nodes with adjacency matrix <img alt="{A_m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_m}"/> and conclude: </p>
<p align="center"><img alt="\displaystyle  \lambda_1(A_N) \geq \lambda_1(A_m) \geq \lambda_{N/2}(A_N) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_1%28A_N%29+%5Cgeq+%5Clambda_1%28A_m%29+%5Cgeq+%5Clambda_%7BN%2F2%7D%28A_N%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \lambda_1(A_N) \geq \lambda_1(A_m) \geq \lambda_{N/2}(A_N) "/></p>
<p>Plugging into the lemma gives: </p>
<p align="center"><img alt="\displaystyle  d(G) \geq \lambda_1(A_m) \geq \lambda_{N/2}(A_N) = \sqrt{n}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++d%28G%29+%5Cgeq+%5Clambda_1%28A_m%29+%5Cgeq+%5Clambda_%7BN%2F2%7D%28A_N%29+%3D+%5Csqrt%7Bn%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  d(G) \geq \lambda_1(A_m) \geq \lambda_{N/2}(A_N) = \sqrt{n}. "/></p>
<p>(In fact, as also <a href="https://www.scottaaronson.com/blog/?p=4229#comment-1813084">noted</a> on Scott’s blog, this case of interlacing can be inferred from simpler reasoning—but our point is that the interlacing theorem was in Huang’s bag of tricks.) </p>
<p>
</p><p/><h2> Building the Matrix </h2><p/>
<p/><p>
Finally, how do we lay hands on <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_N}"/>? We want a matrix of trace zero such that <img alt="{A_N^2 = nI}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%5E2+%3D+nI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_N^2 = nI}"/>. Then all its eigenvalues are <img alt="{+\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+\sqrt{n}}"/> and <img alt="{-\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-\sqrt{n}}"/>.  They come in equal numbers because they sum to the trace which is zero. So we will have <img alt="{N/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N/2}"/> eigenvalues of <img alt="{+\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+\sqrt{n}}"/>, as needed. And we would want <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_N}"/> to be the matrix of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-cube but that doesn’t work: each <img alt="{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i,j}"/> entry of its square counts all paths of length 2 from node <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> to node <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> and that number can be nonzero.</p>
<p>
This is where the trick of putting <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1}"/> on edges comes in, and we can explain it in a way familiar from quantum. We arrange that every 4-cycle of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-cube has exactly one edge with <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1}"/>. Then the pairs of paths from one corner to the opposite corner will always <em>cancel</em>, leaving <img alt="{A^2_{i,j} = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E2_%7Bi%2Cj%7D+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^2_{i,j} = 0}"/> whenever <img alt="{i \neq j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%5Cneq+j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i \neq j}"/>. And <img alt="{A^2_{i,j} = n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E2_%7Bi%2Cj%7D+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^2_{i,j} = n}"/> because there are <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> ways to go out and come back along the same edge, always contributing <img alt="{1\cdot 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%5Ccdot+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1\cdot 1}"/> or <img alt="{(-1)\cdot(-1) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28-1%29%5Ccdot%28-1%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(-1)\cdot(-1) = 1}"/> either way. Huang defines the needed labeling explicitly by the recursion: </p>
<p align="center"><img alt="\displaystyle  A_2 = \begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix},\quad\text{and for } N &gt; 2,\quad A_N = \begin{bmatrix} A_{N/2} &amp; I \\ I &amp; -A_{N/2} \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A_2+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%5C%5C+1+%26+0+%5Cend%7Bbmatrix%7D%2C%5Cquad%5Ctext%7Band+for+%7D+N+%3E+2%2C%5Cquad+A_N+%3D+%5Cbegin%7Bbmatrix%7D+A_%7BN%2F2%7D+%26+I+%5C%5C+I+%26+-A_%7BN%2F2%7D+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A_2 = \begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix},\quad\text{and for } N &gt; 2,\quad A_N = \begin{bmatrix} A_{N/2} &amp; I \\ I &amp; -A_{N/2} \end{bmatrix}. "/></p>
<p>This puts a <img alt="{-}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-}"/> sign on exactly one-fourth of the entries in the needed way. OK, we changed Huang’s subscripts for consistency with “<img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_N}"/>” above and also to note that the basis could be <img alt="{A_1 = [0]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_1+%3D+%5B0%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_1 = [0]}"/>.  Anyway, he verifies <img alt="{A_N^2 = nI}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%5E2+%3D+nI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_N^2 = nI}"/> directly by simple algebra and induction.  That’s it—that’s the proof.</p>
<p>
Why was it hard to spot? Dick and I believe it was the <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1}"/> trick. In the 1980s, I thought about ways to convert undirected graphs into directed ones by putting arrows on the edges, but not <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1}"/> signs. The chance of thinking of it maybe rises with knowing quantum ideas such as interference and amplification. Now we can see, OK, <img alt="{A_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_2}"/> is the quantum NOT gate and the recursion treats signs in similar fashion to the recursion defining Hadamard matrices.  The matrix <img alt="{\frac{1}{\sqrt{n}}A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{\sqrt{n}}A_N}"/> is unitary, so it defines a quantum operator. This all goes to our main point about having tools at one’s command—the more tools, the better. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Huang’s theorem still leaves a gap between a quadratic lower bound and his 4th-power upper bound (my longer <a href="https://cse.buffalo.edu/~regan/cse705/wsensitivity.pdf">draft</a> lays this out).  Can this gap be closed?  In discussing this, Huang notes that his spectral methods need not be confined to sub-matrices of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-cube, and our thoughts of involving quantum are similar. Can quantum tools improve the results even further?</p>
<p/></font></font></div>
    </content>
    <updated>2019-07-12T10:51:42Z</updated>
    <published>2019-07-12T10:51:42Z</published>
    <category term="algorithms"/>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="Teaching"/>
    <category term="trick"/>
    <category term="Boolean functions"/>
    <category term="Boolean sensitivity conjecture"/>
    <category term="complexity"/>
    <category term="concrete complexity"/>
    <category term="eigenvalues"/>
    <category term="Hao Huang"/>
    <category term="linear algebra"/>
    <category term="matrices"/>
    <category term="quantum"/>
    <category term="spectral methods"/>
    <category term="tricks"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-07-25T15:21:05Z</updated>
    </source>
  </entry>
</feed>
