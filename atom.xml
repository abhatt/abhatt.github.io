<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-06-04T15:39:38Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/076</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/076" rel="alternate" type="text/html"/>
    <title>TR21-076 |  Pseudorandom Generators, Resolution and Heavy Width | 

	Dmitry Sokolov</title>
    <summary>Following the paper of Alekhnovich, Ben-Sasson, Razborov, Wigderson \cite{ABRW04} we call a pseudorandom generator $\mathrm{PRG}\colon \{0, 1\}^n \to \{0, 1\}^m$ hard for for a propositional proof system $\mathrm{P}$ if $\mathrm{P}$ cannot efficiently prove the (properly encoded) statement $b \notin \mathrm{Im}(\mathrm{PRG})$ for any string $b \in \{0, 1\}^m$.

In \cite{ABRW04} authors suggested the ``functional encoding'' of considered statement for Nisan--Wigderson generator that allows the introduction of ``local'' extension variables. These extension variables may potentially significantly increase the power of the proof system. In \cite{ABRW04} authors gave a lower bound $\exp\left[\frac{n^2}{m \Omega\left(2^{2^{\Delta}}\right)}\right]$ on the length of Resolution proofs where $\Delta$ is the degree of the dependency graph of the generator. This lower bound meets the barrier for the restriction technique.

In this paper, we introduce a ``heavy width'' measure for Resolution that allows showing a lower bound $\exp\left[\frac{n^2}{m 2^{O(\varepsilon \Delta)}}\right]$ on the length of Resolution proofs of the considered statement for the Nisan--Wigderson generator. This gives an exponential lower bound up to $\Delta := \log^{2 - \delta} n$ (the bigger degree the more extension variables we can use). It is a solution to an open problem from \cite{ABRW04}.</summary>
    <updated>2021-06-04T14:38:44Z</updated>
    <published>2021-06-04T14:38:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-04T15:37:53Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/inference-is-not-a-privacy-violation/</id>
    <link href="https://differentialprivacy.org/inference-is-not-a-privacy-violation/" rel="alternate" type="text/html"/>
    <title>Statistical Inference is Not a Privacy Violation</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>On April 28, 2021, the US Census Bureau <a href="https://www.census.gov/programs-surveys/decennial-census/decade/2020/planning-management/process/disclosure-avoidance/2020-das-updates.html">released</a> a new demonstration of its differentially private Disclosure Avoidance System (DAS) for the 2020 US Census. The public were given a month to submit feedback before the system is finalized.
This demonstration data and the feedback has generated a lot of discussion, including media coverage on <a href="https://www.npr.org/2021/05/19/993247101/for-the-u-s-census-keeping-your-data-anonymous-and-useful-is-a-tricky-balance">National Public Radio</a>, in <a href="https://www.washingtonpost.com/local/social-issues/2020-census-differential-privacy-ipums/2021/06/01/6c94b46e-c30d-11eb-93f5-ee9558eecf4b_story.html">the Washington Post</a>, and via <a href="https://apnews.com/article/business-census-2020-technology-e701e313e841674be6396321343b7e49">the Associated Press</a>. The DAS is also the subject of an <a href="https://www.courtlistener.com/docket/59728874/state-v-united-states-department-of-commerce/">ongoing lawsuit</a>.</p>

<p>The following is a response from experts on differential privacy and cryptography to the <a href="https://alarm-redist.github.io/posts/2021-05-28-census-das/Harvard-DAS-Evaluation.pdf">working paper of Kenny et al.</a> on the impact of the 2020 U.S. Census Disclosure Avoidance System (DAS) on redistricting.</p>

<p>This paper makes a <a href="https://github.com/frankmcsherry/blog/blob/master/posts/2016-06-14.md">common but serious mistake</a>, from which the authors wrongfully conclude the Census Bureau should not modernize its privacy-protection technology.  Not only do the results not support this conclusion, but they instead show the power of the methodology, known as differential privacy, adopted by the Bureau, precisely the opposite of the authors’ erroneous conclusions.</p>

<p>Trust is essential; once destroyed it can be nearly impossible to rebuild, and getting privacy wrong in this Census will have an impact on all future government surveys.  The Census Bureau has shown that their <a href="https://desfontain.es/privacy/index.html">2010 (DAS) does not survive modern privacy threats</a>, and in fact was roughly equivalent to publishing nearly three quarters of the responses.  The Census Bureau’s decision to modernize its Disclosure Avoidance System (DAS) for the 2020 Decennial Census to be differentially private is the correct response to decades of theoretical and empirical work on the privacy risks inherent in releasing large numbers of statistics derived from a dataset.</p>

<p>The importance of the Census, and the reality that no technology competing with differential privacy exists for meeting their confidentiality obligations, makes it very important that the public and policy makers have accurate information. We imagine you will be reporting on this topic in the future.  Others have <a href="https://gerrymander.princeton.edu/DAS-evaluation-Kenny-response">addressed flaws</a> in the paper regarding implications for redistricting; we want to provide you with an understanding of the privacy mistake in the study.</p>

<p>To understand the flaw in the paper’s argument, consider the role of smoking in determining cancer risk.  Statistical study of medical data has taught us that smoking causes cancer.   Armed with this knowledge, if we are told that 40 year old Mr. S is a smoker, we can conclude that he has an elevated cancer risk.  The statistical inference of elevated cancer risk—made before Mr. S was born—did not violate Mr. S’s privacy. To conclude otherwise is to define science to be a privacy attack.  This is the mistake made in the paper.</p>

<p>This is basically what Kenny et al. found.</p>

<p>The authors looked at three different predictors: one built directly from (swapped) 2010 Census data and the other two built using differential privacy applied to (swapped) 2010 Census data, and evaluated all three “on approximately 5.8 million registered voters included in the North Carolina February 2021 voter file.”  What did they find?</p>

<blockquote>
  <p>“Our analysis shows that across three main racial and ethnic groups, the predictions based on the [differential privacy based] DAS data appear to be as accurate as those based on the 2010 Census data.”</p>
</blockquote>

<p>This makes perfect sense. Bayesian Improved Surname Geocoding, or BISG, is a statistical method of building a predictor inferring ethnicity (or race) from name and geography.  Here, name and geography play the role of the information as to whether or not one smokes, and the prediction of ethnicity corresponds to the cancer risk prediction.  The predictor is constructed from census data on the ethnic makeup of individual census blocks and statistical information about the popularity of individual surnames within different ethnic groups.  With such a predictor, moving across the country can change the outcome, as can changing one’s name.  But a BISG prediction is not about the individual, it is about the statistical—population-level—relationship between name, geography, and ethnicity.</p>

<p>The differentially private DAS enabled learning to make statistical inferences about ethnicity from name and geography, without compromising the privacy of any Census respondent, exactly as it was intended to do.  In other words, the paper establishes fitness-for-use of the DAS data for the BISG statistical method!  Because differential privacy permits learning statistical patterns without compromising the privacy of individual members of the dataset, it should not interfere with learning the predictor, which is exactly what the authors found. Returning to our “smoking causes cancer” example, the researchers found that it was just as easy to detect this statistical pattern with a modern disclosure avoidance system in place as it was with the older, less protective system.</p>

<p>The authors’ conclusions –“ the DAS data may not provide universal privacy protection” – are simply not supported by their findings.</p>

<p>They have confused learning that smoking causes cancer—and applying this predictor to an individual smoker—with learning medical details of individual patients in the dataset. Change the input to the predictor—replace “smoker” with “non-smoker” or move across the country, for example—and the prediction changes.</p>

<p>The BISG prediction is not about the individual, it does not accompany her as she relocates from one neighborhood to another, it is a statistical relationship between name, geography, and ethnicity.  It is not a privacy compromise, it is science.</p>

<p>Signed:</p>
<ul>
  <li>Mark Bun, Assistant Professor of Computer Science, Boston University</li>
  <li>Damien Desfontaines, Privacy Engineer, Google</li>
  <li>Cynthia Dwork, Professor of Computer Science, Harvard University</li>
  <li>Moni Naor, Professor of Computer Science, The Weizmann Institute of Science</li>
  <li>Kobbi Nissim, Professor of Computer Science, Georgetown University</li>
  <li>Aaron Roth, Professor of Computer and Information Science, University of Pennsylvania</li>
  <li>Adam Smith, Professor of Computer Science, Boston University</li>
  <li>Thomas Steinke, Research Scientist, Google</li>
  <li>Jonathan Ullman, Assistant Professor of Computer Science, Northeastern University</li>
  <li>Salil Vadhan, Professor of Computer Science and Applied Mathematics, Harvard University</li>
</ul>

<p>Please contact Cynthia Dwork for contact information for authors happy to speak about this on the record.</p></div>
    </summary>
    <updated>2021-06-03T23:30:00Z</updated>
    <published>2021-06-03T23:30:00Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-06-03T23:14:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/075</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/075" rel="alternate" type="text/html"/>
    <title>TR21-075 |  Affine Extractors for Almost Logarithmic Entropy | 

	Eshan Chattopadhyay, 

	Jesse Goodman, 

	Jyun-Jie Liao</title>
    <summary>We give an explicit construction of an affine extractor (over $\mathbb{F}_2$) that works for affine sources on $n$ bits with min-entropy $k \ge~  \log n \cdot (\log \log n)^{1 + o(1)}$. This improves prior work of Li (FOCS'16) that requires  min-entropy at least $\mathrm{poly}(\log n)$.
    
Our construction is based on the framework of using correlation breakers and resilient functions, a paradigm that was also used by Li. On a high level, the key sources of our improvement are based on the following new ingredients: (i) A new construction of  an affine somewhere random extractor, that we use in a crucial step instead of a linear seeded extractor (for which optimal constructions are not known) that was used by Li. (ii) A near optimal construction of a correlation breaker for linearly correlated sources. The construction of our correlation breaker takes inspiration from an exciting line of recent work that constructs two-source extractors for near logarithmic min-entropy.</summary>
    <updated>2021-06-03T23:12:08Z</updated>
    <published>2021-06-03T23:12:08Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-04T15:37:52Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-583257573096655269</id>
    <link href="https://blog.computationalcomplexity.org/feeds/583257573096655269/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/what-happened-to-self-driving-cars.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/583257573096655269" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/583257573096655269" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/what-happened-to-self-driving-cars.html" rel="alternate" type="text/html"/>
    <title>What happened to self-driving cars?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In 2014, I wrote a blog post about a fake company <a href="https://blog.computationalcomplexity.org/2014/07/elfdrive.html">Elfdrive</a>.</p><blockquote style="border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;"><p>With a near record-setting investment announced last week, the self-driving car service Elfdrive is the hottest, most valuable technology start-up on the planet. It is also one of the most controversial.</p><p>The company, which has been the target of protests across Europe this week, has been accused of a reckless attitude toward safety, of price-gouging its customers, of putting existing cabbies out of work and of evading regulation. And it has been called trivial. In The New Yorker last year, George Packer huffed that Elfdrive typified Silicon Valley’s newfound focus on “solving all the problems of being 20 years old, with cash on hand.”</p><p>It is impossible to say whether Elfdrive is worth the $117 billion its investors believe it to be; like any start-up, it could fail. But for all its flaws, Elfdrive is anything but trivial. It could well transform transportation the way Amazon has altered shopping — by using slick, user-friendly software and mountains of data to completely reshape an existing market, ultimately making many modes of urban transportation cheaper, more flexible and more widely accessible to people across the income spectrum.</p></blockquote><p>It was a spoof on Uber but now it looks more like Tesla, expect that Tesla's market value is over half a trillion, about six times larger than General Motors.</p><p>The post was really about self-driving cars which I thought at the time would be commonplace by 2020. We are mostly there but there are issues of silent communication between drivers or between a driver and a pedestrian on who goes first that's hard to duplicate for a self-driving car. There is the paradox that if we make a car that will always stop if someone runs in front of it, then some people will run in front of it.</p><p>There is also the man-bites-dog problem. Any person killed by a self-driving car will be a major news item while the person killed by a human-driven car while you've been reading this post will never be reported.</p><p>We'll get to self-driving cars eventually, it just won't be all at once. We're already have basically self-driving cars on highways and in many other roads as well. As the technology improves and people see that it's safe at some point people will say, "So why do we even need the steering wheel anymore?"</p></div>
    </content>
    <updated>2021-06-03T20:17:00Z</updated>
    <published>2021-06-03T20:17:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-06-04T12:10:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/074</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/074" rel="alternate" type="text/html"/>
    <title>TR21-074 |  Space characterizations of complexity measures and size-space trade-offs in propositional proof systems | 

	Theodoros Papamakarios, 

	Alexander Razborov</title>
    <summary>We identify two new big clusters of proof complexity measures equivalent up to
polynomial and $\log n$ factors. The first cluster contains, among others,
the logarithm of tree-like resolution size, regularized (that is, multiplied
by the logarithm of proof length) clause and monomial space, and clause
space, both ordinary and regularized, in regular and tree-like resolution. As
a consequence, separating clause or monomial space from the (logarithm of)
tree-like resolution size is the same as showing a strong trade-off between
clause or monomial space and proof length, and is the same as showing a
super-critical trade-off between clause space and depth. The second cluster
contains width, $\Sigma_2$ space (a generalization of clause
space to depth 2 Frege systems), both ordinary and regularized, as well as
the logarithm of tree-like size in the system $R(\log)$. As an application of some of
these simulations, we improve a known size-space trade-off for polynomial calculus with resolution. In
terms of lower bounds, we show a quadratic lower bound on tree-like
resolution size for formulas refutable in clause space $4$. We introduce on
our way yet another proof complexity measure intermediate between depth and
the logarithm of tree-like size that might be of independent interest.</summary>
    <updated>2021-06-03T16:20:36Z</updated>
    <published>2021-06-03T16:20:36Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-04T15:37:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/073</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/073" rel="alternate" type="text/html"/>
    <title>TR21-073 |  Lower bounds for samplers and data structures via the cell-probe separator | 

	Emanuele Viola</title>
    <summary>Suppose that a distribution $S$ can be approximately sampled by an
efficient cell-probe algorithm. It is shown to be possible to restrict
the input to the algorithm so that its output distribution is still
not too far from $S$, and at the same time many output coordinates
are almost pairwise independent.

Building on this several results are obtained, including:

- A lower bound for sampling prefix sums.

- A lower bound for sampling a variant of the predecessor problem.

- A separation between AC0 and cell-probe sampling.

- A separation between sampling with $O(q)$ and $q$ probes.

- A new proof of the Patrascu-Viola data-structure lower bound for
prefix sums, demonstrating the feasibility of obtaining data-structure
lower bounds via sampling.

- A separation between data structures making $O(q)$ and $q$ probes.

The only previous cell-probe lower bounds for sampling followed from
the AC0 lower bounds and applied to pseudorandom objects like error-correcting
and extractors, making them inadequate for the above applications.</summary>
    <updated>2021-06-03T14:15:11Z</updated>
    <published>2021-06-03T14:15:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-04T15:37:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=565</id>
    <link href="https://tcsplus.wordpress.com/2021/06/03/tcs-talk-wednesday-june-9-ankur-moitra-mit-and-pravesh-kothari-cmu/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, June 9 — Pravesh Kothari (CMU) and Ankur Moitra (MIT)</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, June 9th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Pravesh Kothari  and Ankur Moitra from CMU and MIT will (jointly) speak about “Robustly Learning Mixtures of Gaussians” (abstract below). Note that the seminar will be a bit […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, June 9th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC).<strong> <a href="https://www.cs.cmu.edu/~praveshk/">Pravesh Kothari</a>  and <a href="https://people.csail.mit.edu/moitra/">Ankur Moitra</a> </strong>from CMU and MIT will (jointly) speak about “<em>Robustly Learning Mixtures of Gaussians</em>” (abstract below).</p>
<p>Note that the seminar will be a bit longer than the usual: it’s a double feature!</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: For a while now the problem of robustly learning a high-dimensional mixture of Gaussians has had a target on its back. The first works in algorithmic robust statistics gave provably robust algorithms for learning a single Gaussian. Since then there has been steady progress, including algorithms for robustly learning mixtures of spherical Gaussians, mixtures of Gaussians under separation conditions, and arbitrary mixtures of two Gaussians. In this talk we will discuss two recent works that essentially resolve the general problem. There are important differences in their techniques, setup, and overall quantitative guarantees, which we will discuss.</p>
<p>The talk will cover the following independent works:</p>
<ul>
<li>Liu, Moitra, “Settling the Robust Learnability of Mixtures of Gaussians”</li>
<li>Bakshi, Diakonikolas, Jia, Kane, Kothari, Vempala, “Robustly Learning Mixtures of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> Arbitrary Gaussians”</li>
</ul>
</blockquote></div>
    </content>
    <updated>2021-06-03T04:30:54Z</updated>
    <published>2021-06-03T04:30:54Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-06-04T15:38:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.01340</id>
    <link href="http://arxiv.org/abs/2106.01340" rel="alternate" type="text/html"/>
    <title>Transaction Fee Mechanism Design</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roughgarden:Tim.html">Tim Roughgarden</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.01340">PDF</a><br/><b>Abstract: </b>Demand for blockchains such as Bitcoin and Ethereum is far larger than
supply, necessitating a mechanism that selects a subset of transactions to
include "on-chain" from the pool of all pending transactions. EIP-1559 is a
proposal to make several tightly coupled changes to the Ethereum blockchain's
transaction fee mechanism, including the introduction of variable-size blocks
and a burned base fee that rises and falls with demand. These changes are
slated for deployment in Ethereum's "London fork," scheduled for late
summer~2021, at which point it will be the biggest economic change made to a
major blockchain to date.
</p>
<p>The first goal of this paper is to formalize the problem of designing a
transaction fee mechanism, taking into account the many idiosyncrasies of the
blockchain setting (ranging from off-chain collusion between miners and users
to the ease of money-burning). The second goal is to situate the specific
mechanism proposed in EIP-1559 in this framework and rigorously interrogate its
game-theoretic properties. The third goal is to suggest competing designs that
offer alternative sets of trade-offs. The final goal is to highlight research
opportunities for the EC community that could help shape the future of
blockchain transaction fee mechanisms.
</p></div>
    </summary>
    <updated>2021-06-03T22:39:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.01336</id>
    <link href="http://arxiv.org/abs/2106.01336" rel="alternate" type="text/html"/>
    <title>Improved Rates for Differentially Private Stochastic Convex Optimization with Heavy-Tailed Data</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Gautam Kamath, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Xingtu.html">Xingtu Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Huanyu.html">Huanyu Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.01336">PDF</a><br/><b>Abstract: </b>We study stochastic convex optimization with heavy-tailed data under the
constraint of differential privacy. Most prior work on this problem is
restricted to the case where the loss function is Lipschitz. Instead, as
introduced by Wang, Xiao, Devadas, and Xu, we study general convex loss
functions with the assumption that the distribution of gradients has bounded
$k$-th moments. We provide improved upper bounds on the excess population risk
under approximate differential privacy of
$\tilde{O}\left(\sqrt{\frac{d}{n}}+\left(\frac{d}{\epsilon
n}\right)^{\frac{k-1}{k}}\right)$ and
$\tilde{O}\left(\frac{d}{n}+\left(\frac{d}{\epsilon
n}\right)^{\frac{2k-2}{k}}\right)$ for convex and strongly convex loss
functions, respectively. We also prove nearly-matching lower bounds under the
constraint of pure differential privacy, giving strong evidence that our bounds
are tight.
</p></div>
    </summary>
    <updated>2021-06-03T22:47:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.01236</id>
    <link href="http://arxiv.org/abs/2106.01236" rel="alternate" type="text/html"/>
    <title>Improved Spanning on Theta-5</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bose:Prosenjit.html">Prosenjit Bose</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hill:Darryl.html">Darryl Hill</a>, Aurélien Ooms Carleton University) <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.01236">PDF</a><br/><b>Abstract: </b>We show an upper bound of $\frac{
</p>
<p>\sin\left(\frac{3\pi}{10}\right)
</p>
<p>}{
</p>
<p>\sin\left(\frac{2\pi}{5}\right)-\sin\left(\frac{3\pi}{10}\right)
</p>
<p>}
</p>
<p>&lt;5.70$ on the spanning ratio of $\Theta_5$-graphs, improving on the previous
best known upper bound of $9.96$ [Bose, Morin, van Renssen, and Verdonschot.
The Theta-5-graph is a spanner. Computational Geometry, 2015.]
</p></div>
    </summary>
    <updated>2021-06-03T22:59:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.01215</id>
    <link href="http://arxiv.org/abs/2106.01215" rel="alternate" type="text/html"/>
    <title>Visual Analysis of Electronic Densities and Transitions in Molecules</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Masood:Talha_Bin.html">Talha Bin Masood</a>, Signe Sidwall Thygesen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Linares:Mathieu.html">Mathieu Linares</a>, Alexei I. Abrikosov, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Natarajan:Vijay.html">Vijay Natarajan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hotz:Ingrid.html">Ingrid Hotz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.01215">PDF</a><br/><b>Abstract: </b>The study of electronic transitions within a molecule connected to the
absorption or emission of light is a common task in the process of the design
of new materials. The transitions are complex quantum mechanical processes and
a detailed analysis requires a breakdown of these processes into components
that can be interpreted via characteristic chemical properties. We approach
these tasks by providing a detailed analysis of the electron density field.
This entails methods to quantify and visualize electron localization and
transfer from molecular subgroups combining spatial and abstract
representations. The core of our method uses geometric segmentation of the
electronic density field coupled with a graph-theoretic formulation of charge
transfer between molecular subgroups. The design of the methods has been guided
by the goal of providing a generic and objective analysis following fundamental
concepts. We illustrate the proposed approach using several case studies
involving the study of electronic transitions in different molecular systems.
</p></div>
    </summary>
    <updated>2021-06-03T23:10:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.01173</id>
    <link href="http://arxiv.org/abs/2106.01173" rel="alternate" type="text/html"/>
    <title>On the approximation ratio of LZ-End to LZ77</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Takumi Ideue, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mieno:Takuya.html">Takuya Mieno</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Funakoshi:Mitsuru.html">Mitsuru Funakoshi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakashima:Yuto.html">Yuto Nakashima</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inenaga:Shunsuke.html">Shunsuke Inenaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takeda:Masayuki.html">Masayuki Takeda</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.01173">PDF</a><br/><b>Abstract: </b>A family of Lempel-Ziv factorizations is a well-studied string structure. The
LZ-End factorization is a member of the family that achieved faster extraction
of any substrings (Kreft &amp; Navarro, TCS 2013). One of the interests for LZ-End
factorizations is the possible difference between the size of LZ-End and LZ77
factorizations. They also showed families of strings where the approximation
ratio of the number of LZ-End phrases to the number of LZ77 phrases
asymptotically approaches 2. However, the alphabet size of these strings is
unbounded. In this paper, we analyze the LZ-End factorization of the
period-doubling sequence. We also show that the approximation ratio for the
period-doubling sequence asymptotically approaches 2 for the binary alphabet.
</p></div>
    </summary>
    <updated>2021-06-03T22:50:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.01135</id>
    <link href="http://arxiv.org/abs/2106.01135" rel="alternate" type="text/html"/>
    <title>MNL-Bandit with Knapsacks</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Abdellah Aznag, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goyal:Vineet.html">Vineet Goyal</a>, Noemie Perivier <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.01135">PDF</a><br/><b>Abstract: </b>We consider a dynamic assortment selection problem where a seller has a fixed
inventory of $N$ substitutable products and faces an unknown demand that
arrives sequentially over $T$ periods. In each period, the seller needs to
decide on the assortment of products (of cardinality at most $K$) to offer to
the customers. The customer's response follows an unknown multinomial logit
model (MNL) with parameters $v$. The goal of the seller is to maximize the
total expected revenue given the fixed initial inventory of $N$ products. We
give a policy that achieves a regret of $\tilde O\left(K \sqrt{K N T}\left(1 +
\frac{\sqrt{v_{\max}}}{q_{\min}}\text{OPT}\right) \right)$ under a mild
assumption on the model parameters. In particular, our policy achieves a
near-optimal $\tilde O(\sqrt{T})$ regret in the large inventory setting.
</p>
<p>Our policy builds upon the UCB-based approach for MNL-bandit without
inventory constraints in [1] and addresses the inventory constraints through an
exponentially sized LP for which we present a tractable approximation while
keeping the $\tilde O(\sqrt{T})$ regret bound.
</p></div>
    </summary>
    <updated>2021-06-03T22:57:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.01108</id>
    <link href="http://arxiv.org/abs/2106.01108" rel="alternate" type="text/html"/>
    <title>Efficient Deterministic Leader Election for Programmable Matter</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dufoulon:Fabien.html">Fabien Dufoulon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kutten:Shay.html">Shay Kutten</a>, William K. Moses Jr. <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.01108">PDF</a><br/><b>Abstract: </b>It was suggested that a programmable matter system (composed of multiple
computationally weak mobile particles) should remain connected at all times
since otherwise, reconnection is difficult and may be impossible. At the same
time, it was not clear that allowing the system to disconnect carried a
significant advantage in terms of time complexity. We demonstrate for a
fundamental task, that of leader election, an algorithm where the system
disconnects and then reconnects automatically in a non-trivial way (particles
can move far away from their former neighbors and later reconnect to others).
Moreover, the runtime of the temporarily disconnecting deterministic leader
election algorithm is linear in the diameter. Hence, the disconnecting --
reconnecting algorithm is as fast as previous randomized algorithms. When
comparing to previous deterministic algorithms, we note that some of the
previous work assumed weaker schedulers. Still, the runtime of all the previous
deterministic algorithms that did not assume special shapes of the particle
system (shapes with no holes) was at least quadratic in $n$, where $n$ is the
number of particles in the system. (Moreover, the new algorithm is even faster
in some parameters than the deterministic algorithms that did assume special
initial shapes.)
</p>
<p>Since leader election is an important module in algorithms for various other
tasks, the presented algorithm can be useful for speeding up other algorithms
under the assumption of a strong scheduler. This leaves open the question: "can
a deterministic algorithm be as fast as the randomized ones also under weaker
schedulers?"
</p></div>
    </summary>
    <updated>2021-06-03T22:48:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.01079</id>
    <link href="http://arxiv.org/abs/2106.01079" rel="alternate" type="text/html"/>
    <title>Using Predicted Weights for Ad Delivery</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lavastida:Thomas.html">Thomas Lavastida</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moseley:Benjamin.html">Benjamin Moseley</a>, R. Ravi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Chenyang.html">Chenyang Xu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.01079">PDF</a><br/><b>Abstract: </b>We study the performance of a proportional weights algorithm for online
capacitated bipartite matching modeling the delivery of impression ads. The
algorithm uses predictions on the advertiser nodes to match arriving impression
nodes fractionally in proportion to the weights of its neighbors. This paper
gives a thorough empirical study of the performance of the algorithm on a
data-set of ad impressions from Yahoo! and shows its superior performance
compared to natural baselines such as a greedy water-filling algorithm and the
ranking algorithm. The proportional weights algorithm has recently received
interest in the theoretical literature where it was shown to have strong
guarantees beyond the worst-case model of algorithms augmented with
predictions. We extend these results to the case where the advertisers'
capacities are no longer stationary over time. Additionally, we show the
algorithm has near optimal performance in the random-order arrival model when
the number of impressions and the optimal matching are sufficiently large.
</p></div>
    </summary>
    <updated>2021-06-03T22:58:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.01036</id>
    <link href="http://arxiv.org/abs/2106.01036" rel="alternate" type="text/html"/>
    <title>Ultra-Sparse Near-Additive Emulators</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Elkin:Michael.html">Michael Elkin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matar:Shaked.html">Shaked Matar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.01036">PDF</a><br/><b>Abstract: </b>Near-additive (aka $(1+\epsilon,\beta)$-) emulators and spanners are a
fundamental graph-algorithmic construct, with numerous applications for
computing approximate shortest paths and related problems in distributed,
streaming and dynamic settings.
</p>
<p>Known constructions of near-additive emulators enable one to trade between
their sparsity (i.e., number of edges) and the additive stretch $\beta$.
Specifically, for any pair of parameters $\epsilon &gt;0$, $ \kappa=1,2,\dots$,
one can have a $(1+\epsilon,\beta)$-emulator with $O(n^{1+1/\kappa})$ edges,
with $\beta = \left(\frac{\log \kappa}{\epsilon}\right)^{\log \kappa}$. At
their sparsest, these emulators employ $c\cdot n$ edges, for some constant
$c\geq 2$.
</p>
<p>We tighten this bound, and show that in fact precisely $n^{1+1/\kappa}$ edges
suffice.
</p>
<p>In particular, our emulators can be \emph{ultra-sparse}, i.e., we can have an
emulator with $n+o(n)$ edges and $\beta = \left(\frac{\log {\log n}}{\epsilon
}\right)^{{\log {\log n}}(1+o(1))}$.
</p>
<p>We also devise a distributed deterministic algorithm in the CONGEST model
that builds these emulators in low polynomial time (i.e., in $O(n^\rho)$ time,
for an arbitrarily small constant parameter $\rho &gt;0$).
</p>
<p>Finally, we also improve the state-of-the-art distributed deterministic
\congest-model construction of
</p>
<p>$(1+\epsilon,\beta)$-spanners devised in the PODC'19 paper
</p>
<p>[ElkinM19]. Specifically, the spanners of [ElkinM19] have $O(\beta\cdot
n^{1+1/\kappa})$ edges, i.e., at their sparsest they employ
</p>
<p>$ O\left(\frac{\log {\log n}}{\epsilon }\right)^{{\log {\log n}}}\cdot n$
edges. In this paper, we devise an efficient distributed deterministic
CONGEST-model algorithm that builds such spanners with $O(n^{1+1/\kappa})$
edges for $\kappa = O\left(\frac{\log n}{\log ^{(3)}n}\right)$. At their
sparsest, these spanners employ only $O(n\cdot {\log {\log n}})$ edges.
</p></div>
    </summary>
    <updated>2021-06-03T22:46:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.00937</id>
    <link href="http://arxiv.org/abs/2106.00937" rel="alternate" type="text/html"/>
    <title>Putting the Squeeze on Array Programs: Loop Verification via Inductive Rank Reduction</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Oren Ish Shalom, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Itzhaky:Shachar.html">Shachar Itzhaky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rinetzky:Noam.html">Noam Rinetzky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shoham:Sharon.html">Sharon Shoham</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.00937">PDF</a><br/><b>Abstract: </b>Automatic verification of array manipulating programs is a challenging
problem because it often amounts to the inference of in ductive quantified loop
invariants which, in some cases, may not even be firstorder expressible. In
this paper, we suggest a novel verification tech nique that is based on
induction on userdefined rank of program states as an alternative to
loopinvariants. Our technique, dubbed inductive rank reduction, works in two
steps. Firstly, we simplify the verification problem and prove that the program
is correct when the input state con tains an input array of length B or less,
using the length of the array as the rank of the state. Secondly, we employ a
squeezing function g which converts a program state sigma with an array of
length &gt; B to a state g(sigma) containing an array of length minus 1 or less.
We prove that when g satisfies certain natural conditions then if the program
violates its specification on sigma then it does so also on g(sigma). The
correctness of the program on inputs with arrays of arbitrary lengths follows
by induction. We make our technique automatic for array programs whose length
of execution is proportional to the length of the input arrays by (i) perform
ing the first step using symbolic execution, (ii) verifying the conditions
required of g using Z3, and (iii) providing a heuristic procedure for syn
thesizing g. We implemented our technique and applied it successfully to
several interesting arraymanipulating programs, including a bidirec tional
summation program whose loop invariant cannot be expressed in firstorder logic
while its specification is quantifier free.
</p></div>
    </summary>
    <updated>2021-06-03T22:37:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.00909</id>
    <link href="http://arxiv.org/abs/2106.00909" rel="alternate" type="text/html"/>
    <title>The Generalized Mean Densest Subgraph Problem</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Veldt:Nate.html">Nate Veldt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Benson:Austin_R=.html">Austin R. Benson</a>, Jon Kleinberg <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.00909">PDF</a><br/><b>Abstract: </b>Finding dense subgraphs of a large graph is a standard problem in graph
mining that has been studied extensively both for its theoretical richness and
its many practical applications. In this paper we introduce a new family of
dense subgraph objectives, parameterized by a single parameter $p$, based on
computing generalized means of degree sequences of a subgraph. Our objective
captures both the standard densest subgraph problem and the maximum $k$-core as
special cases, and provides a way to interpolate between and extrapolate beyond
these two objectives when searching for other notions of dense subgraphs. In
terms of algorithmic contributions, we first show that our objective can be
minimized in polynomial time for all $p \geq 1$ using repeated submodular
minimization. A major contribution of our work is analyzing the performance of
different types of peeling algorithms for dense subgraphs both in theory and
practice. We prove that the standard peeling algorithm can perform arbitrarily
poorly on our generalized objective, but we then design a more sophisticated
peeling method which for $p \geq 1$ has an approximation guarantee that is
always at least $1/2$ and converges to 1 as $p \rightarrow \infty$. In
practice, we show that this algorithm obtains extremely good approximations to
the optimal solution, scales to large graphs, and highlights a range of
different meaningful notions of density on graphs coming from numerous domains.
Furthermore, it is typically able to approximate the densest subgraph problem
better than the standard peeling algorithm, by better accounting for how the
removal of one node affects other nodes in its neighborhood.
</p></div>
    </summary>
    <updated>2021-06-03T22:48:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.00875</id>
    <link href="http://arxiv.org/abs/2106.00875" rel="alternate" type="text/html"/>
    <title>The Hardest Explicit Construction</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Korten:Oliver.html">Oliver Korten</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.00875">PDF</a><br/><b>Abstract: </b>We investigate the complexity of explicit construction problems, where the
goal is to produce a particular object of size $n$ possessing some pseudorandom
property in time polynomial in $n$. We give overwhelming evidence that ${\bf
APEPP}$, defined originally by Kleinberg et al., is the natural complexity
class associated with explicit constructions for objects whose existence
follows from the probabilistic method, by proving that a host of well-studied
explicit construction problems lie in this class. We then observe that a result
of Je\v{r}\'{a}bek on provability in Bounded Arithmetic, when reinterpreted as
a reduction between search problems, shows that constructing a truth table of
high circuit complexity is complete for ${\bf APEPP}$ under ${\bf P}^{\bf NP}$
reductions. This demonstrates that constructing a hard truth table is a
universal explicit construction problem in a concrete sense. This result in
fact gives a precise algorithmic characterization of proving $2^{\Omega(n)}$
circuit lower bounds for ${\bf E}^{\bf NP}$: the complete problem for ${\bf
APEPP}$ has a ${\bf P}^{\bf NP}$ algorithm if and only if such a lower bound
holds. Together with our proof that pseudorandom generators can be constructed
in ${\bf APEPP}$, this also yields a self-contained and significantly
simplified proof of the celebrated result of Impagliazzo and Wigderson that
worst-case-hard truth tables can be used to derandomize algorithms (although
the conclusion is weaker as our derandomization requires an ${\bf NP}$ oracle).
As another corollary of this completeness result, we show that ${\bf E}^{\bf
NP}$ contains a language of circuit complexity $2^{\Omega(n)}$ if and only if
it contains a language of circuit complexity $\frac{2^n}{3n}$. Finally, for
several of the problems shown to lie in ${\bf APEPP}$, we demonstrate direct
polynomial time reductions to the explicit construction of hard truth tables.
</p></div>
    </summary>
    <updated>2021-06-03T22:37:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.00730</id>
    <link href="http://arxiv.org/abs/2106.00730" rel="alternate" type="text/html"/>
    <title>Enabling Efficiency-Precision Trade-offs for Label Trees in Extreme Classification</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Baharav:Tavor_Z=.html">Tavor Z. Baharav</a>, Daniel L. Jiang, Kedarnath Kolluri, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sanghavi:Sujay.html">Sujay Sanghavi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dhillon:Inderjit_S=.html">Inderjit S. Dhillon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.00730">PDF</a><br/><b>Abstract: </b>Extreme multi-label classification (XMC) aims to learn a model that can tag
data points with a subset of relevant labels from an extremely large label set.
Real world e-commerce applications like personalized recommendations and
product advertising can be formulated as XMC problems, where the objective is
to predict for a user a small subset of items from a catalog of several million
products. For such applications, a common approach is to organize these labels
into a tree, enabling training and inference times that are logarithmic in the
number of labels. While training a model once a label tree is available is well
studied, designing the structure of the tree is a difficult task that is not
yet well understood, and can dramatically impact both model latency and
statistical performance. Existing approaches to tree construction fall at an
extreme point, either optimizing exclusively for statistical performance, or
for latency. We propose an efficient information theory inspired algorithm to
construct intermediary operating points that trade off between the benefits of
both. Our algorithm enables interpolation between these objectives, which was
not previously possible. We corroborate our theoretical analysis with numerical
results, showing that on the Wiki-500K benchmark dataset our method can reduce
a proxy for expected latency by up to 28% while maintaining the same accuracy
as Parabel. On several datasets derived from e-commerce customer logs, our
modified label tree is able to improve this expected latency metric by up to
20% while maintaining the same accuracy. Finally, we discuss challenges in
realizing these latency improvements in deployed models.
</p></div>
    </summary>
    <updated>2021-06-03T22:42:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.00718</id>
    <link href="http://arxiv.org/abs/2106.00718" rel="alternate" type="text/html"/>
    <title>Public Good Games in Directed Networks</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Papadimitriou:Christos.html">Christos Papadimitriou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Binghui.html">Binghui Peng</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.00718">PDF</a><br/><b>Abstract: </b>Public goods games in undirected networks are generally known to have pure
Nash equilibria, which are easy to find. In contrast, we prove that, in
directed networks, a broad range of public goods games have intractable
equilibrium problems: The existence of pure Nash equilibria is NP-hard to
decide, and mixed Nash equilibria are PPAD-hard to find. We define general
utility public goods games, and prove a complexity dichotomy result for finding
pure equilibria, and a PPAD-completeness proof for mixed Nash equilibria. Even
in the divisible goods variant of the problem, where existence is easy to
prove, finding the equilibrium is PPAD-complete. Finally, when the treewidth of
the directed network is appropriately bounded, we prove that polynomial-time
algorithms are possible.
</p></div>
    </summary>
    <updated>2021-06-03T22:46:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.00715</id>
    <link href="http://arxiv.org/abs/2106.00715" rel="alternate" type="text/html"/>
    <title>A Theory for Locus Ellipticity of Poncelet 3-Periodics</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Helman:Mark.html">Mark Helman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Laurain:Dominique.html">Dominique Laurain</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reznik:Dan.html">Dan Reznik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garcia:Ronaldo.html">Ronaldo Garcia</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.00715">PDF</a><br/><b>Abstract: </b>We provide a theory as to why the locus of a triangle center over Poncelet
3-periodics in an ellipse pair is an ellipse or not. For the confocal pair
(elliptic billiard), we show that if the center can be expressed as a fixed
affine combination of barycenter, circumcenter, and mittenpunkt (which is
stationary over the confocal family), then its locus will be an ellipse. We
also provide conditions under which a particular locus will be a circle or a
segment. We also analyze locus turning number and monotonicity with respect to
vertices of the 3-periodic family. Finally we write out expressions for the
convex quartic locus of the incenter for a generic Poncelet family,
conjecturing it can only be an ellipse if the pair is confocal.
</p></div>
    </summary>
    <updated>2021-06-03T23:09:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.00714</id>
    <link href="http://arxiv.org/abs/2106.00714" rel="alternate" type="text/html"/>
    <title>Parallel Polynomial Permanent Mod Powers of 2 and Shortest Disjoint Cycles</title>
    <feedworld_mtime>1622678400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Datta:Samir.html">Samir Datta</a>, Kishlaya Jaiswal <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.00714">PDF</a><br/><b>Abstract: </b>We present a parallel algorithm for permanent mod 2^k of a matrix of
univariate integer polynomials. It places the problem in ParityL subset of
NC^2. This extends the techniques of [Valiant], [Braverman, Kulkarni, Roy] and
[Bj\"orklund, Husfeldt], and yields a (randomized) parallel algorithm for
shortest 2-disjoint paths improving upon the recent result from (randomized)
polynomial time.
</p>
<p>We also recognize the disjoint paths problem as a special case of finding
disjoint cycles, and present (randomized) parallel algorithms for finding a
shortest cycle and shortest 2-disjoint cycles passing through any given fixed
number of vertices or edges.
</p></div>
    </summary>
    <updated>2021-06-03T22:38:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-06-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-9052295508162981109</id>
    <link href="http://processalgebra.blogspot.com/feeds/9052295508162981109/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=9052295508162981109" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/9052295508162981109" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/9052295508162981109" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/06/frank-p-ramsey-on-research-and.html" rel="alternate" type="text/html"/>
    <title>Frank P. Ramsey on research and publication rates</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Spurred by the excellent 1978 radio programme <a href="https://sms.cam.ac.uk/media/20145" target="_blank">'Better than the Stars'</a> by <a href="https://en.wikipedia.org/wiki/Hugh_Mellor" target="_blank">D. H. Mellor</a> about <a href="https://plato.stanford.edu/entries/ramsey/" target="_blank">Frank Ramsey</a> and by the <a href="https://traffic.libsyn.com/secure/philosophybites/Cheryl_Misak_on_Frank_Ramsey_and_Ludwig_Wittgenstein.mp3" target="_blank">Philosophy Bites interview with Cheryl Misak on Frank Ramsey and Ludwig Wittgenstein</a>, I started reading <a href="https://www.cherylmisak.com/" target="_blank">Cheryl Misak</a>'s <a href="https://global.oup.com/academic/product/frank-ramsey-9780198755357?cc=is&amp;lang=en&amp;" target="_blank">biography of Frank Ramsey</a>. (FWIW, I strongly recommend the radio programme, the podcast and the book.)<br/></p><p>The following quote from pages 169-170 of Cheryl Misak's book describes Ramsey's views on publications and research, as stated in a letter to his father Arthur:<br/></p><div style="margin-left: 40px; text-align: left;">Arthur tried a different tack, suggesting that Frank was going to be in trouble for wasting all this time on analysis, rather than on his career. On 24 September, in what seems to be his last letter home before he left Vienna, Frank wrote: </div><blockquote style="margin-left: 80px; text-align: left;">I don’t see how there can be any such inquisition into my conduct in Vienna as you suppose seem to want to guard against.... No one can suppose that you can’t research for six months without having a paper ready by the end. If everyone wrote a paper every six months the amount of trivial literature would swell beyond all bounds. Given time I shall produce a good paper. But if I hurry it will be ill written and unintelligible and unconvincing. <br/></blockquote><blockquote style="margin-left: 80px; text-align: left;">It seems to me perfectly proper to spend a scholarship being analysed, as it is likely to make me cleverer in the future, and discoveries of importance are made by remarkable people not by remarkable diligence. </blockquote><div style="margin-left: 40px; text-align: left;"> While it may not be persuasive that psychoanalysis makes one cleverer, Frank was prescient that the numbers of journal articles would eventually swell and he was right that diligence isn't enough to produce discoveries of importance.</div><div style="margin-left: 40px; text-align: left;"> </div><div style="text-align: left;">Of course, academia has changed since those times and I do value "remarkable diligence". However, I will try to remember Ramsey's words next time someone proposes to make academic hirings and promotions conditional to having a certain number of papers or citations or whatever per year on average. <br/></div></div>
    </content>
    <updated>2021-06-01T21:42:00Z</updated>
    <published>2021-06-01T21:42:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-06-01T21:43:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5536</id>
    <link href="https://www.scottaaronson.com/blog/?p=5536" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5536#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5536" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Three updates</title>
    <summary xml:lang="en-US">Hooray, I’m today’s “Featured ACM Member”! Which basically means, yet another interview with me about quantum computing, with questions including what’s most surprised me about the development of QC, and what students should do to get into the field. I’m proud to announce that An Automated Approach to the Collatz Conjecture, a paper by Emre […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><ol><li>Hooray, I’m <a href="https://www.acm.org/articles/people-of-acm/2021/scott-aaronson">today’s “Featured ACM Member”</a>!  Which basically means, yet another interview with me about quantum computing, with questions including what’s most surprised me about the development of QC, and what students should do to get into the field.</li><li>I’m proud to announce that <a href="https://arxiv.org/abs/2105.14697">An Automated Approach to the Collatz Conjecture</a>, a paper by Emre Yolcu, myself, and Marijn Heule that we started working on over four years ago, is finally available on the arXiv, and will be presented at the 2021 <a href="https://www.cs.cmu.edu/~mheule/CADE28/">Conference on Automated Deduction</a>.  Long story short: no, we didn’t prove <a href="https://en.wikipedia.org/wiki/Collatz_conjecture">Collatz</a>, but we have an approach that can for the first time prove certain Collatz-like statements in a fully automated way, so hopefully that’s interesting!  There was also a <a href="https://www.quantamagazine.org/can-computers-solve-the-collatz-conjecture-20200826/"><em>Quanta</em> article</a> even before our paper had come out (I wasn’t thrilled about the timing).</li><li>The legendary <a href="https://en.wikipedia.org/wiki/Baba_Brinkman">Baba Brinkman</a> has a <a href="https://www.youtube.com/watch?v=kVcOx9Bg3a4">new rap about quantum computing</a> (hat tip to blog commenter YD).  Having just watched the music video, I see it as one of the better popularization efforts our field has seen in the past 25 years—more coherent than the average journalistic account and with a <em>much</em> better backbeat.  (I do, however, take a more guarded view than Brinkman of the potential applications, especially to e.g. autonomous driving and supply-chain optimization.)</li></ol>



<p/></div>
    </content>
    <updated>2021-06-01T20:00:21Z</updated>
    <published>2021-06-01T20:00:21Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-06-01T20:00:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3526</id>
    <link href="https://agtb.wordpress.com/2021/06/01/wine21-call-for-papers/" rel="alternate" type="text/html"/>
    <title>WINE’21 Call for Papers</title>
    <summary>WINE 2021: The 17th Conference on Web and Internet Economics  December 14-17, 2021Hasso Plattner Institute, Potsdam, Germany Overview Over the past two decades, researchers in theoretical computer science, artificial intelligence, operations research, and economics have joined forces to understand the interplay of incentives and computation. These issues are of particular importance in the Web and […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>WINE 2021: The 17th Conference on Web and Internet Economics </strong></p>



<p><strong>December 14-17, 2021</strong><br/><strong>Hasso Plattner Institute, Potsdam, Germany</strong></p>



<p/><blockquote class="wp-embedded-content"><a href="https://hpi.de/wine2021/">Overview</a></blockquote><p/>



<p>Over the past two decades, researchers in theoretical computer science, artificial intelligence, operations research, and economics have joined forces to understand the interplay of incentives and computation. These issues are of particular importance in the Web and the Internet that enable the interaction of large and diverse populations. The Conference on Web and Internet Economics (WINE) is an interdisciplinary forum for the exchange of ideas and results on incentives and computation arising from these various fields. WINE 2021 continues the successful tradition of the Conference on Web and Internet Economics (named Workshop on Internet &amp; Network Economics until 2013), which was held annually from 2005 to present.</p>



<p>The program will feature invited talks, tutorials, paper presentations, and a poster session. All paper submissions will be peer-reviewed and evaluated on the basis of the quality of their contribution, originality, soundness, and significance. Submissions are invited in, but not limited to, the following topics:</p>



<ul><li>Algorithmic Game Theory</li><li>Algorithmic Mechanism Design</li><li>Auction Algorithms and Analysis</li><li>Computational Advertising</li><li>Computational Aspects of Equilibria</li><li>Computational Social Choice</li><li>Learning in Markets and Mechanism Design</li><li>Learning under Strategic Behavior</li><li>Coalitions, Coordination, and Collective Action</li><li>Economic Aspects of Security and Privacy</li><li>Economic Aspects of Distributed Computing and Cryptocurrencies</li><li>Econometrics, ML, and Data Science</li><li>Behavioral Economics and Behavioral Modeling</li><li>Fairness and Trust in Games and Markets</li><li>Price Differentiation and Price Dynamics</li><li>Revenue Management</li><li>Social Networks and Network Games</li></ul>



<p><strong>Authors of the accepted papers will have a choice to attend the conference virtually.</strong></p>



<p><strong>Important Dates</strong></p>



<p>Paper submission deadline: July 12, 2021, 11:59pm Pacific Time</p>



<p>Author notification: September (exact date TBA)</p>



<p><strong>Submission Format</strong></p>



<p>Authors are invited to submit extended abstracts presenting original research on any of the research fields related to WINE 2021.</p>



<p>An extended abstract submitted to WINE 2021 should start with the title of the paper, each author’s name, affiliation and e-mail address, followed by a one-paragraph summary of the results to be presented. This should then be followed by a technical exposition of the main ideas and techniques used to achieve these results, including motivation and a clear comparison with related work.</p>



<p>The extended abstract should not exceed 18 single-spaced pages (excluding references) using reasonable margins (at least one-inch margins all around) and at least 11-point font. If the authors believe that more details are essential to substantiate the claims of the paper, they may include a clearly marked appendix (with no space limit) that will be read at the discretion of the Program Committee. It is strongly recommended that submissions adhere to the specified format and length. Submissions that are clearly too long may be rejected immediately. The above specifications are meant to provide more freedom to the authors at the time of submission. Note that accepted papers will be allocated 14 pages (including references) in the LNCS format in the proceedings (see below).</p>



<p>The proceedings of the conference will be published by Springer-Verlag in the ARCoSS/LNCS series, and will be available for distribution at the conference. Accepted papers will be allocated 14 pages total in the LNCS format in the proceedings. Submissions are encouraged, though not required, to follow the LNCS format (Latex, Word). More information about the LNCS format can be found on the <a href="https://www.springer.com/cn/computer-science/lncs/conference-proceedings-guidelines" rel="noreferrer noopener" target="_blank">author instructions page of Springer-Verlag</a>. </p>



<p><strong>Best Paper Award</strong></p>



<p>The program committee will decide upon a best paper award and a best student paper award.</p>



<p><strong>Important Notice</strong></p>



<p>To accommodate the publishing traditions of different fields, authors of accepted papers can ask that only a one-page abstract of the paper appear in the proceedings, along with a URL pointing to the full paper. The authors should guarantee the link to be reliable for at least two years. This option is available to accommodate subsequent publication in journals that would not consider results that have been published in preliminary form in conference proceedings. Such papers must be submitted and formatted just like papers submitted for full-text publication.</p>



<p>Simultaneous submission of results to another conference with published proceedings is not allowed. Results previously published or presented at another archival conference prior to WINE 2021, or published (or accepted for publication) at a journal prior to the submission deadline of WINE 2021, will not be considered. Simultaneous submission of results to a journal is allowed only if the authors intend to publish the paper as a one-page abstract in WINE 2021. Papers that are accepted and appear as a one-page abstract can be subsequently submitted for publication in a journal but may not be submitted to any other conference that has a published proceeding.</p>



<p>Program PC co-chairs: Michal Feldman (chair), Hu Fu and Inbal Talgam-Cohen</p></div>
    </content>
    <updated>2021-06-01T09:39:25Z</updated>
    <published>2021-06-01T09:39:25Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>michalfeldman</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2021-06-04T15:37:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=1768</id>
    <link href="https://toc4fairness.org/forc-2021-is-coming-up/" rel="alternate" type="text/html"/>
    <title>FORC 2021 is coming up!</title>
    <summary>The Second annual Symposium on the Foundations of Responsible Computing (FORC) will be held virtually (on Gather.town) June 9-11, 2021. Registration is free, but required, by June 7. Instructions for ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Second annual Symposium on the Foundations of Responsible Computing (FORC) will be held virtually (on Gather.town) June 9-11, 2021.<a href="https://docs.google.com/forms/d/e/1FAIpQLSfELj0MvgcI83dmYThUPiynqWj8-G9Xve2dVf84EqKzgXXsHQ/viewform" rel="noreferrer noopener" target="_blank"> Registration is free, but required, by June 7</a>. Instructions for joining the event will be emailed to registered participants and posted online on June 8.</p>



<p>The program will feature 24 papers, six exciting panel discussions, three social hours featuring interactive board games, keynotes by Julie Owono (of the Facebook Oversight Board) and Kate Crawford (on her new book, the Atlas of AI), and a mentoring meetup.</p>



<p>The full program is here: <a href="https://responsiblecomputing.org/forc-2021-program/" rel="noreferrer noopener" target="_blank">https://responsiblecomputing.org/forc-2021-program/</a></p>



<p>The Symposium on Foundations of Responsible Computing (FORC) is a forum for mathematical research in computation and society writ large. The Symposium aims to catalyze the formation of a community supportive of the application of theoretical computer science, statistics, economics and other relevant analytical fields to problems of pressing and anticipated societal concern.</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-1770" height="858" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/white-logo-no-background.png?resize=800%2C858&amp;ssl=1" width="800"/></figure></div>
    </content>
    <updated>2021-06-01T06:49:04Z</updated>
    <published>2021-06-01T06:49:04Z</published>
    <category term="Blog"/>
    <author>
      <name>galoosh33</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i1.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2021-06-04T15:39:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/05/31/linkage</id>
    <link href="https://11011110.github.io/blog/2021/05/31/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Ememem’s mosaic interventions in the potholes of Lyon (\(\mathbb{M}\), see also, and also). I find this kind of urban kintsugi pleasing, not just for its aspect of guerilla art, but for the sharp geometric patterns of the mosaics and the rough boundaries where they meet the landscape around them.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.thisiscolossal.com/2021/05/ememem-street-mosaics/">Ememem’s mosaic interventions in the potholes of Lyon</a> (<a href="https://mathstodon.xyz/@11011110/106251596474278206">\(\mathbb{M}\)</a>, <a href="https://www.ememem-flacking.net/flacking">see also</a>, <a href="https://boingboing.net/2021/05/17/this-artist-repairs-damaged-sidewalks-with-beautiful-mosaic-tiles.html">and also</a>). I find this kind of urban kintsugi pleasing, not just for its aspect of guerilla art, but for the sharp geometric patterns of the mosaics and the rough boundaries where they meet the landscape around them.</p>
  </li>
  <li>
    <p><a href="https://tilings.math.uni-bielefeld.de/">Tilings encyclopedia</a> (<a href="https://mathstodon.xyz/@11011110/106255097813445827">\(\mathbb{M}\)</a>). An online collection of many pretty substitution tilings. What’s missing to make it closer to OEIS in usefulness is a way to search by tiling rather than by keyword in the description of the tiling.</p>
  </li>
  <li>
    <p>At UC Irvine, we’ve been using Piazza for online course forums, but Piazza insists on serving ads to students (despite being paid), so we’re moving to <a href="https://edstem.org/us/">Ed Discussion</a> instead (<a href="https://mathstodon.xyz/@11011110/106258490049583719">\(\mathbb{M}\)</a>). You can see similar moves at <a href="https://rtl.berkeley.edu/news/piazza-and-new-online-discussion-platform-update">Berkeley</a> and <a href="https://news.psu.edu/story/641227/2020/12/07/academics/piazza-class-discussion-platform-start-displaying-advertisements">Penn State</a>. Usually I mistrust campus choices of software infrastructure — they tend to go for big crufty hard-to-use packages that are a poor fit for our needs — but this appears to be for good reason and I support it.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-answer-old-question-about-odd-graphs-20210519/"><em>Quanta</em> highlights one of my UCI colleagues, Asaf Ferber, for his work with Michael Krivelevich proving that every graph without isolated vertices has an induced subgraph of \(\Omega(n)\) vertices in which all vertices have odd degree</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/106265785749549559">\(\mathbb{M}\)</a>).</span> As usual, they also get important details wrong, forgetting to mention that the subgraphs must be induced, and also not mentioning the requirement of no isolated vertices. For the actual preprint see <a href="https://arxiv.org/abs/2009.05495">arXiv:2009.05495</a>.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2021/05/20/unc-chapel-hill-board-doesnt-approve-tenure-noted-journalist">Academic freedom dead at  U. of North Carolina</a> (<a href="https://mathstodon.xyz/@11011110/106269430445568583">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/191525/Nikole-Hannah-Jones-Denied-Tenure-at-UNC">via</a>, <a href="https://www.nytimes.com/2021/05/19/business/media/nikole-hannah-jones-unc.html">see also</a>): right-wing board of trustees cancel-cultures MacArthur and Pulitzer winner Nikole Hannah-Jones for racist/political reasons (they don’t want anyone to talk about US slavery and ongoing systemic racism). To be clear, she still has a position at UNC; after the trustees denied her tenure, her department gave her a five-year-renewable spot not requiring trustee approval.</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2021/05/20/to-cheer-you-up-in-difficult-times-25-some-mathematical-news-part-2/">Some mathematical news</a> (<a href="https://mathstodon.xyz/@11011110/106277146476845581">\(\mathbb{M}\)</a>). Gil Kalai rounds up a lot of recent developments in knot recognition complexity, exotic spheres, numbers of fixed points, graph theory, and additive combinatorics.</p>
  </li>
  <li>
    <p><a href="https://www34.homepage.villanova.edu/robert.jantzen/notes/torus/cavatappo20/">Tilted cavatappo surfaces</a> (<a href="https://mathstodon.xyz/@11011110/106280088634102191">\(\mathbb{M}\)</a>). Robert Jantzen studies the shape of <a href="https://en.wikipedia.org/wiki/Cavatappi">corkscrew tube pasta</a> and of shortest paths on its surface. See also his preprints <a href="https://arxiv.org/abs/1301.0013">arXiv:1301.0013</a> and <a href="https://arxiv.org/abs/1402.3284">arXiv:1402.3284</a>.</p>
  </li>
  <li>
    <p>Here’s an unexpected property of hyperbolic geometry (<a href="https://mathstodon.xyz/@11011110/106288718744948273">\(\mathbb{M}\)</a>), or at least, it struck me as counterintuitive: all lines through two opposite quadrants of two perpendicular lines pass near their crossing, within distance \(\ln(1+\sqrt2)\). The figure below uses the upper halfplane model to show two quadrants (dark yellow), their convex hull (light yellow, from which the lines cannot escape), and a circle of radius \(\ln(1+\sqrt2)\) centered at the crossing, separating the quadrants.</p>

    <p style="text-align: center;"><img alt="Convex hull of the quadrants formed in the hyperbolic plane by two perpendicular lines, and a circle centered at the crossing separating the quadrants" src="https://11011110.github.io/blog/assets/2021/double-wedge-hull.svg"/></p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=HeQX2HjkcNo">Veritaserium on Cantor, Gödel, and Turing</a> (<a href="https://mathstodon.xyz/@gnivasch/106290995069247640">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>At a time when national borders have largely shut down, significantly decreasing the opportunities for higher education for people with the misfortune to be born in the wrong part of the world, <a href="https://www.latimes.com/california/story/2021-05-25/bold-plan-for-uc-admissions-reduce-out-of-state-students">California’s legislators are pushing to enact the same barriers to education across state lines</a> (<a href="https://mathstodon.xyz/@11011110/106297102605074297">\(\mathbb{M}\)</a>), preventing access to education and hurting the state itself by discouraging the best and brightest from coming here.</p>
  </li>
  <li>
    <p>Cellphone snapshot of the UC Irvine Ecological Preserve on a recent cloudy day (<a href="https://mathstodon.xyz/@11011110/106305347918888735">\(\mathbb{M}\)</a>). The foreground is mostly mustard, of no particular ecological significance; the ecological part is the coastal sage scrub on the sunny hillside in the background.</p>

    <p style="text-align: center;"><img alt="UCI Ecological Preserve" src="https://www.ics.uci.edu/~eppstein/pix/uciep2/DriedMustard-m.jpg" style="border-style: solid; border-color: black;" width="80%"/></p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1007/BF02764015">Aperiodic tilings of the hyperbolic plane by convex polygons</a> (<a href="https://mathstodon.xyz/@11011110/106314334930093898">\(\mathbb{M}\)</a>), Margulis and Mozes, 1998. I knew about the <a href="https://en.wikipedia.org/wiki/Binary_tiling">binary tilings</a> but not this, which gets aperiodicity differently: if a tile’s area isn’t a rational multiple of \(\pi\), it cannot be periodic. This works even for certain hyperbolic rhombi, with angles chosen so they can tile. The tilings can still have 1d symmetry. If some single tile  avoids all symmetries, I don’t know about it.</p>
  </li>
  <li>
    <p><a href="http://www.geometrictomography.com/">Geometric tomography</a> (<a href="https://mathstodon.xyz/@11011110/106317414925334494">\(\mathbb{M}\)</a>). A nicely presented brief web survey of this subject, on the reconstruction of 3d shapes from 2d information (such as its brightness function, the areas of its perpendicular projections), by Richard J. Gardner based on his 1995 book of the same title.</p>
  </li>
  <li>
    <p><a href="https://cacm.acm.org/magazines/2021/6/252840-collusion-rings-threaten-the-integrity-of-computer-science-research/fulltext">Collusion rings threaten the integrity of computer science research</a> (<a href="https://mathstodon.xyz/@11011110/106320077489874249">\(\mathbb{M}\)</a>), Michael L. Littman, CACM. Relatedly: <a href="https://www.chemistryworld.com/news/publishers-grapple-with-an-invisible-foe-as-huge-organised-fraud-hits-scientific-journals/4013652.article">“huge organised fraud” in scientific journals</a>, <a href="https://retractionwatch.com/2021/05/29/weekend-reads-gibberish-papers-persist-the-academic-who-faked-cherokee-heritage-organised-fraud-hits-scientific-journals/">via Retraction Watch</a>.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=8Sv_FaEN8zE">Five talks from CanaDAM 2021 introducing graph product structure theory and its applications</a> (<a href="https://mathstodon.xyz/@patmorin/106300452133561408">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://isohedral.ca/heesch-numbers-of-unmarked-polyforms/">Big progress in classifying polyforms by their Heesch numbers, obtained by using a SAT solver in place of an ad-hoc backtracking search for tilings</a> (<a href="https://mathstodon.xyz/@11011110/106331731989277653">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/191587/TikTok-teen-points-to-inside-elbow-bites-lip-Heeeeeeeesch">via</a>). See also <a href="https://twitter.com/mathpuzzle/status/1397040707706236928">Ed Pegg on an infinite set of polyforms with Heesch = 3</a>.</p>
  </li>
</ul></div>
    </content>
    <updated>2021-05-31T14:09:00Z</updated>
    <published>2021-05-31T14:09:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-31T21:10:15Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6576209065051505306</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6576209065051505306/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/what-is-natural-question-who-should.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6576209065051505306" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6576209065051505306" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/what-is-natural-question-who-should.html" rel="alternate" type="text/html"/>
    <title>What is a natural question? Who should decide?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><br/></p><p>(Thanks to Timothy Chow for inspiring this post.)</p><p>My survey on Hilbert's Tenth Problem(see  <a href="https://arxiv.org/abs/2104.07220">here</a>) is about variants of the problem. One of them is as follows: </p><p>For which degrees d and number-of-vars n, is Hilbert's tenth problem decidable? undecidable? unknown? </p><p> I wondered why there was not a website with this information. More generally, the problem didn't seem to be getting much attention. (My survey does report on the attention it has gotten.) </p><p>I got several emails telling me it was the wrong question. I didn't quite know what they meant until Timothy Chow emailed me the following eloquent explanation:</p><p>-----------------------------------</p><p><i>One reason there isn't already a website of the type you envision is that from a number-theoretic (or decidability) point of view, parameterization by  degree and number of variables is not as natural as it might seem at first glance. The most fruitful lines of research have been geometric, and so geometric concepts such as smoothness, dimension, and genus are more natural than, say, degree. A nice survey by a number theorist is the book Rational Points on Varieties by Bjorn Poonen. Much of it is highly technical; however, reading the preface is very enlightening. Roughly speaking, the current state of the art is that there is really only one known way to prove that a system of Diophantine equations has no rational solution.</i></p><p>----------------------------------</p><p>AGAINST THE NUMBER THEORISTS VIEWPOINT:</p><p>1) ALICE: Why are you looking for your keys under the lamppost instead of where you dropped them?</p><p>   BOB: The light is better here.</p><p>2) I can imagine the following conversation:</p><p>BILL: I want to know about what happens with degree 3, and number of variables 3.</p><p>MATHPERSON: That's the wrong question you moron. The real question is what happens for fixed length of cohomology subchains.</p><p>BILL: Why is that more natural?</p><p>MATHPERSON: Because that is what we can solve. And besides, I've had 10 papers on it.</p><p><br/></p><p>FOR THE NUMBER THEORISTS VIEWPOINT</p><p>1) They are working on really hard problems so it is natural to gravitate towards those that can be solved.</p><p>2) I suspect that the math that comes out of studying classes of equations based on smoothness, dimension, genus is more interesting than what comes out of degree and number of vars. Or at least it has been so far. </p><p>META QUESTION</p><p>Who gets to decide what problems are natural?</p><p>People outside the field (me in this case) are asking the kind of questions that a layperson would ask and there is some merit to that.</p><p>People inside the field KNOW STUFF and hence their opinion of what's interesting to study has some merit. But they can also mistake `I cannot solve X' for `X is not interesting'</p><div><br/></div></div>
    </content>
    <updated>2021-05-30T23:31:00Z</updated>
    <published>2021-05-30T23:31:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-06-04T12:10:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21758</id>
    <link href="https://gilkalai.wordpress.com/2021/05/30/to-cheer-you-up-in-difficult-times-24-borodins-colouring-conjecture/" rel="alternate" type="text/html"/>
    <title>To Cheer You Up in Difficult times 24: Borodin’s colouring conjecture!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">An acyclic colouring of a graph is a colouring of its vertices so that the subgraph spanned on union of every two colour classes is acyclic (a forest). Grunbaum conjectured in 1973 that Every planar graph has acyclic colouring with … <a href="https://gilkalai.wordpress.com/2021/05/30/to-cheer-you-up-in-difficult-times-24-borodins-colouring-conjecture/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>An <a href="https://en.wikipedia.org/wiki/Acyclic_coloring">acyclic colouring</a> of a graph is a colouring of its vertices so that the subgraph spanned on union of every two colour classes is acyclic (a forest). Grunbaum conjectured in 1973 that </p>



<p class="has-text-align-center"><strong><span class="has-inline-color" style="color: #a30042;">Every planar graph has acyclic colouring with five colours.</span> </strong></p>



<p>This was proved by Borodin in 1976.</p>



<p>Borodin made the following stronger conjecture. Recall that a graph is <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/>-degenerate if every  subgraph has a vertex of degree at most <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/>. </p>



<p class="has-text-align-center"><strong><span class="has-inline-color" style="color: #a3001e;">Every planar graph <em>G</em> can be coloured with five colours so that the union of every <em>k</em> colour classes, <img alt="1 \le k \le 4" class="latex" src="https://s0.wp.com/latex.php?latex=1+%5Cle+k+%5Cle+4&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/> induces a <em>(k-1)</em>-degenerate graph. </span></strong></p>



<p>Let me mention an intermediate conjecture in-between Grunbaum’s conjecture and Borodin’s </p>



<p class="has-text-align-center"><strong><span class="has-inline-color" style="color: #a30011;">Every planar graph <em>G</em> can be coloured with five colours so that the union of every <em>k</em> colour classes, <img alt="1 \le k \le 4" class="latex" src="https://s0.wp.com/latex.php?latex=1+%5Cle+k+%5Cle+4&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/> induces a stress free graph for generic embedding into <img alt="R^{k-1}" class="latex" src="https://s0.wp.com/latex.php?latex=R%5E%7Bk-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/>.</span></strong> <br/><br/></p>



<p>Grunbaum’s 1973 paper <a href="https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/BF02764716&amp;casa_token=vnWD-T_S73sAAAAA:mUenckyg9cyx5mrJI0hGdRqZ5WznXX-keyu4jYW61z-CkI1pslMA4w7SheVILEa1g1yHmjmFrP63EoqPODY">Acyclic<strong> </strong>colorings of planar graphs‏</a> is a very imaginative and highly cited paper. It was the first paper I ever refereed and I remember spending a lot of time reading it.  Here is the link to Borodin’s paper <a href="https://www.sciencedirect.com/science/article/pii/0012365X79900773">On acyclic colorings of planar graphs‏</a>. I am not sure what is the current world-record for the number of colours.</p></div>
    </content>
    <updated>2021-05-30T20:44:18Z</updated>
    <published>2021-05-30T20:44:18Z</published>
    <category term="Combinatorics"/>
    <category term="Branko Grunbaum"/>
    <category term="Oleg Borodin"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-06-04T15:37:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=2235</id>
    <link href="https://theorydish.blog/2021/05/26/few-lessons-from-the-history-of-multiparty-computation/" rel="alternate" type="text/html"/>
    <title>A few lessons from the history of multiparty computation</title>
    <summary>As part of my Ph.D. qualifying exam, I gave a survey talk on some of the recent progress in practical multi-party computation (MPC). After the talk, Omer Reingold asked me why so much progress on MPC happened in the last decade or so, given that the main theoretical groundwork for multiparty computation was laid out in the 1980s.  Omer’s question got me interested, so I looked at some MPC papers from over the years and talked to a few people who have been working on MPC for some time now. In this post, I want to share some of what I’ve learned about the history of the progress of MPC, a few answers to Omer’s question, and potentially a few takeaways on progress in science more broadly. To give a bit of background, MPC is one of the most fundamental problems in cryptography. In particular, MPC protocols allow a set of parties, each of which holds its private input, to compute a joint function over their inputs, in a way that protects the confidentiality and integrity of the computation from an adversary that controls a subset of the parties and the communication channels. Defining this precisely requires a lot of [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As part of my Ph.D. qualifying exam, I gave a survey talk on some of the recent progress in practical multi-party computation (MPC). After the talk, Omer Reingold asked me why so much progress on MPC happened in the last decade or so, given that the main theoretical groundwork for multiparty computation was laid out in the 1980s.  Omer’s question got me interested, so I looked at some MPC papers from over the years and talked to a few people who have been working on MPC for some time now. In this post, I want to share some of what I’ve learned about the history of the progress of MPC, a few answers to Omer’s question, and potentially a few takeaways on progress in science more broadly.</p>



<p>To give a bit of background, MPC is one of the most fundamental problems in cryptography. In particular, MPC protocols allow a set of parties, each of which holds its private input, to compute a joint function over their inputs, in a way that protects the confidentiality and integrity of the computation from an adversary that controls a subset of the parties and the communication channels. Defining this precisely requires a lot of care, yet for now, you can think of an MPC protocol as being secure, if the adversary can only cause as much damage as it can in an “ideal world” in which the computation is performed with the aid of a trusted incorruptible party. Intuitively, in such an ideal world, the adversary cannot do much more than choose its inputs, modify its output, or choose not to participate in the computation at all. </p>



<p>In the 1980s, a <a href="https://ieeexplore.ieee.org/abstract/document/4568207">sequence</a> <a href="https://dl.acm.org/doi/10.1145/28395.28420">of</a> <a href="https://dl.acm.org/doi/10.1145/62212.62213">works</a> <a href="https://dl.acm.org/doi/10.1145/62212.62214">initiated</a> the study of multiparty computation and established very powerful and general results, showing the feasibility of constructing protocols for securely computing any multi-party functionality in a variety of settings. Those results were mostly theoretical. In contrast, the last 15 years saw a growing interest in MPC from a more practical perspective. To put this rising interest in context, consider the following two graphs (data for the graphs is <a href="https://docs.google.com/spreadsheets/d/1IzfKDRqv6C9hSeD9TyosvCl9fknN2V3Ja4tQjy54y0A/edit?usp=sharing">here</a>).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2245" height="372" src="https://theorydish.files.wordpress.com/2021/05/comptime.png?w=620" width="620"/></figure></div>



<p>This graph shows the time of a two-party secure computation of the AES block cipher over the years. That is the time it takes to evaluate AES(k,x), where one party holds a 128-bit key k to the block cipher, the other party holds a 256-bit data block x, and the output is 256-bit long. The measurements are from the original papers. There are different variants of this “benchmark”, in terms of the number of parties (2PC/MPC), the number of corrupted parties (honest/dishonest majority), precise security model (semi-honest/fully malicious), hardware, network (LAN/WAN), and cost model (single/batch execution), but, for concreteness, the graph only includes results in the batch-evaluation setting in the fully malicious two-party setting. </p>



<p class="has-text-align-center"><img height="375" src="https://lh4.googleusercontent.com/lfa7-L5D3d5U8ZCHbAmUoqRH-RVaUmpFfSiFhBl7yF38ieoUmdo-LL0xH-gzXEaPauRXkCUmOYWDovfJvE792a0ikVxojM6g4P7EFD-mg4A2XbvzJWAslBug8UE6W4CnRhtIt4el" width="624"/></p>



<p>The second graph shows the number of papers on two-party computation, over the years. This is not a very well-defined characterization, but for simplicity, I looked at all the papers referenced from the relevant chapters in the <a href="https://securecomputation.org/">book on MPC</a> by Evans, Kolesnikov, and Rosulek. Admittedly, these are not all the papers in the area, there might be valuable indirect contributions that are missing, and, more importantly, the number of papers is a bit of a superficial metric. Yet, even with these caveats, I think this is helpful to get a general picture. On top of the flurry of academic research, MPC has also been thriving in the industry, with companies like <a href="https://www.curv.co/">Curv</a>, <a href="https://partisia.com/">Partisia</a>, <a href="https://sharemind.cyber.ee/">Sharemind</a>, <a href="https://www.unboundsecurity.com/">Unbound</a>, and others building MPC-based products and services.</p>



<p>The first graph shows quite impressive progress. In less than a decade, the time to securely compute AES in the two-party setting decreased by more than 60,000x, from more than 10 minutes to less than 10 milliseconds. (For reference, in the same period, CPU speeds increased <a href="https://www.cpubenchmark.net/cpu.php?cpu=Intel+Core+i7-975+%40+3.33GHz&amp;id=841">roughly</a> <a href="https://www.cpubenchmark.net/cpu.php?cpu=Intel+Core+i7-7740X+%40+4.30GHz&amp;id=3041">twofold</a>, and typical local-area network speeds increased from 1GB/S to 10GB/S.) The second graph illustrates the increase in the volume of research, which has enabled this progress. This includes advances such as <a href="https://dl.acm.org/doi/10.1145/100216.100287">various</a> <a href="https://dl.acm.org/doi/10.1145/336992.337028">increasingly</a> <a href="https://link.springer.com/chapter/10.1007/978-3-540-70583-3_40">efficient</a> <a href="https://link.springer.com/chapter/10.1007/978-3-662-46803-6_8">garbling</a> <a href="https://dl.acm.org/doi/10.1145/3133956.3134053">techniques</a>, <a href="https://link.springer.com/chapter/10.1007%2F11745853_30">more</a> <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.140.2627&amp;rep=rep1&amp;type=pdf">efficient</a> <a href="https://link.springer.com/chapter/10.1007/978-3-540-72540-4_4">cut</a>–<a href="https://link.springer.com/chapter/10.1007/978-3-642-20465-4_22">and</a>–<a href="https://link.springer.com/chapter/10.1007/978-3-642-19571-6_20">choose</a> <a href="https://link.springer.com/chapter/10.1007/978-3-642-40084-1_3">techniques</a> <a href="https://link.springer.com/chapter/10.1007/978-3-642-00457-5_22">for</a> <a href="https://dl.acm.org/doi/10.1145/2508859.2516698">malicious security</a>, <a href="https://link.springer.com/chapter/10.1007/978-3-642-20465-4_11">protocols</a> <a href="https://link.springer.com/chapter/10.1007/978-3-642-32009-5_38">based on</a> <a href="https://link.springer.com/chapter/10.1007/978-3-642-32009-5_40">information-theoretic</a> <a href="https://dl.acm.org/doi/10.1145/2976749.2978357">message-authentication</a> <a href="https://link.springer.com/chapter/10.1007/978-3-319-78372-7_6">codes</a>, <a href="https://crypto.stanford.edu/craig/craig-thesis.pdf">homomorphic</a> <a href="https://dl.acm.org/doi/10.1145/2090236.2090262">encryption</a>, and many others. (See the EKR book for an overview of these techniques and more references.) The second graph also clearly shows the inflection point that happened about 15 years ago. This inflection point illustrates the premise of the question that has triggered this blog post.</p>



<p>I use the term inflection point, since progress in MPC never really stopped between the late 80s and the mid-2000s, and there was a lot of research in the interim period. For example, <a href="https://eprint.iacr.org/2005/187.pdf">oblivious</a> <a href="https://dl.acm.org/doi/10.1145/1008908.1008920">transfer</a>–one of the fundamental primitives used <a href="https://dl.acm.org/doi/10.1145/3812.3818">to</a> <a href="https://link.springer.com/content/pdf/10.1007%2F3-540-47721-7_17.pdf">build</a> <a href="https://dl.acm.org/doi/10.1145/62212.62215">MPC</a>–<a href="https://link.springer.com/chapter/10.1007/3-540-44750-4_9">received</a> <a href="https://dl.acm.org/doi/abs/10.1145/237814.237996">a</a> <a href="https://link.springer.com/article/10.1007/BF00208002">lot</a> <a href="https://link.springer.com/chapter/10.1007/BFb0054139">of</a> <a href="https://dl.acm.org/doi/10.1145/301250.301312">attention</a> <a href="https://link.springer.com/chapter/10.1007/3-540-44987-6_8">and</a> <a href="https://dl.acm.org/doi/abs/10.5555/365411.365502">saw</a> <a href="https://link.springer.com/chapter/10.1007/978-3-540-45146-4_9">significant</a> <a href="https://link.springer.com/chapter/10.1007/978-3-540-40061-5_27">progress</a>. There <a href="https://link.springer.com/article/10.1007/BF02252866">have</a> <a href="https://link.springer.com/chapter/10.1007/3-540-38424-3_6">been</a> <a href="https://link.springer.com/article/10.1007/BF00196771">many</a> <a href="https://link.springer.com/chapter/10.1007/3-540-46766-1_32">works</a> <a href="https://dl.acm.org/doi/10.1145/167088.167109">developing</a> <a href="https://dl.acm.org/doi/abs/10.1145/195058.195408">the</a> <a href="https://dl.acm.org/doi/10.1145/259380.259412">theoretical</a> <a href="https://link.springer.com/chapter/10.1007/3-540-44598-6_5">framework</a> <a href="https://dl.acm.org/doi/10.1145/352600.352639">of</a> <a href="https://link.springer.com/chapter/10.1007/3-540-44448-3_13">secure</a> <a href="https://dl.acm.org/doi/abs/10.1145/380752.380855">computation</a>, studying <a href="https://eprint.iacr.org/2000/067">secure composition</a>, and <a href="https://dl.acm.org/doi/10.1145/72981.72995">reducing</a> <a href="https://dl.acm.org/doi/10.1145/100216.100287">the</a> <a href="https://link.springer.com/chapter/10.1007/3-540-38424-3_5">round</a> <a href="https://ieeexplore.ieee.org/document/595170">complexity</a> <a href="https://ieeexplore.ieee.org/document/814630">of</a> <a href="https://link.springer.com/chapter/10.1007/3-540-45022-X_43">secure</a> <a href="https://link.springer.com/chapter/10.1007/3-540-45539-6_23">multiparty</a> <a href="https://ieeexplore.ieee.org/abstract/document/892118">computation</a> <a href="https://dl.acm.org/doi/10.5555/646766.704286">protocols</a>. Other works developed special-purpose secure computation protocols for concrete applications such as <a href="https://ieeexplore.ieee.org/document/502223">secure auctions</a> and  <a href="https://dl.acm.org/doi/10.1145/293347.293350">private information retrieval</a>. Many of the techniques and ideas in these works were instrumental for the more recent progress.</p>



<p>Having said that, there still seems to have been a dip in the attention devoted to MPC for a certain period. One explanation that I’ve found interesting is that the powerful feasibility results from the early days made the problem seem “solved” from a theoretical point of view. As a result, interest in MPC within the theory community decreased. It took time before the interest ramped up again, this time more within the more-practically oriented crypto community. (Interestingly, since then, MPC found renewed interest in the theoretical cryptography community as well.)</p>



<p>One work that, to me, is a key point in the transition of MPC from theory to practice is the 2004 <a href="https://www.usenix.org/conference/13th-usenix-security-symposium/fairplay%E2%80%94-secure-two-party-computation-system">Fairplay paper</a>. This was the first full-fledged system that implemented generic secure function evaluation. I believe that, together with other early implementations of MPC-based systems (such as <a href="https://link.springer.com/chapter/10.1007/978-3-642-03549-4_20">the system</a> developed for the Danish sugar-beet auction and the <a href="https://link.springer.com/chapter/10.1007/978-3-540-88313-5_13">Sharemind system</a>), it played a pivotal role in accelerating practical research. I view it as a lesson on the value of “first systems”: they help to identify bottlenecks and discover new concerns, which are hard to predict without building an actual system. Moreover, such systems put “theory problems” on the radar of practitioners and demonstrate which ideas are closer to being practical.  In some sense, a system is an evidence that “the constants in the big-O have been worked out…and they are not astronomical.” Furthermore, when code is made public, follow-up works can build upon an initial system and improve on it more rapidly. A first system can mark the transition of an idea from pure theory to practice. It’s still fair to ask what determines the timing of the appearance of such a first system. One answer is that it often takes the right type of collaboration of researchers from different research communities, as seems to have been the case with “Fairplay”. </p>



<p>I think that external factors also played a role in the timing of events. Specifically, the growth of the Internet and the demand for security on the Internet stimulated a lot of interest and progress in applied cryptography. The late 1990s saw large-scale adoption of public-key cryptography with the successful development and deployment of SSL. More complex Internet applications began to appear later, including those that are inherently security-sensitive, like e-commerce. These applications stimulated interest in more advanced cryptographic tools, MPC being one of them. Indeed, MPC papers from that period discuss new Internet-driven applications as part of their motivation. </p>



<p>A final factor that I want to mention ties back to the first graph above. The fact that many works demonstrate their performance by measuring AES evaluation is, to me, an example of the value of standardized benchmarks. Such benchmarks help to evaluate new techniques and ideas within a certain area and can help progress on a particular dimension, partly because of the competitive element they add. As with any measurement, there is a risk of overfitting to the benchmark, rather than focusing on the true goal, but I think they are very helpful overall.</p>



<p>I am sure there are other valuable lessons to be learned from this case study. Moreover, I am quite certain that folks with more experience in the area have a much better perspective on the history of these developments than I do. I will be very happy to hear other perspectives!</p>



<p><strong>Acknowledgements:</strong> I would like to thank my quals committee, Dan Boneh, Omer Reingold, and Mary Wootters, for an interesting discussion that has led to this blog post. I am grateful to Yuval Ishai and Benny Pinkas for interesting conversations on the progress of MPC and to Henry Corrigan-Gibbs and Saba Eskandarian for providing me with helpful comments.</p></div>
    </content>
    <updated>2021-05-27T01:29:16Z</updated>
    <published>2021-05-27T01:29:16Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Dima Kogan</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-06-04T15:39:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18791</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/05/26/the-voting-paradox/" rel="alternate" type="text/html"/>
    <title>The Voting Paradox</title>
    <summary>Trust, but verify—Ronald Reagan are researchers in the area of voting security. Some have worked in this area for decades, others for years, and some are new to the area. But they all signed a letter about election security—right after the last presidential election. The body of the letter is also below. Today I thought […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Trust, but verify—Ronald Reagan</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/26/the-voting-paradox/group-3/" rel="attachment wp-att-18806"><img alt="" class="aligncenter size-medium wp-image-18806" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/group.png?w=200&amp;ssl=1"/></a></p>
<p>
are researchers in the area of voting security. Some have worked in this area for decades, others for years, and some are new to the area. But they all signed a letter about election security—right after the last presidential election. The body of <a href="https://electionlawblog.org/?p=118718">the letter</a> is also below.</p>
<p>
Today I thought we might look into the main paradox surrounding voting security.</p>
<p>
Peter G. Neumann and Rebecca Mercuri just published a <a href="https://cacm.acm.org/magazines/2021/6/252836-the-risks-of-election-believability-or-lack-thereof/fulltext">paper</a> in the CACM on voting security. It is titled <i>The Risks of Election Believability (or Lack Thereof)</i>. </p>
<p>They say: </p>
<blockquote><p><b> </b> <em> Trustworthiness in elections is inherently a total-system problem <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> </em>
</p></blockquote>
<p>
</p><p>
So let’s look at election security.</p>
<p>
</p><p/><h2> The Paradox </h2><p/>
<p/><p>
<i>The people who cast the votes don’t decide an election, the people who count the votes do—Joseph Stalin</i></p>
<p>
I am reluctant in citing Stalin. But I believe that he hit the mark. The key in any election, in the US or elsewhere, is how and by whom the votes are counted.  </p>
<p>
The issue with the letter, with the recent paper, and more is that the researchers seem to be caught in the middle between two points of view:</p>
<ul>
<li>
There is no evidence of voter fraud in the last presidential election. None. Zero. <p/>
</li><li>
There is no proof of voting security in the last presidential election. None. Zero.
</li></ul>
<p>
This is the problem. We would like to believe that voting is fair and secure. We would like to believe that our election—especially for president—is fair and correct. But we really have no proof that this is true. We are left with desire to make elections safer, but take the position that there is no issue with them. </p>
<p>
Good luck.</p>
<p>
</p><p/><h2> The Letter </h2><p/>
<p/><p>
Here is the letter. I think they miss saying things as bluntly as Stalin does. But they do say essentially the same.</p>
<blockquote><p><b> </b> <em> We are specialists in election security, having studied the security of voting machines, voting systems, and technology used for government elections for decades.</em></p><em>
<p>
We and other scientists have warned for many years that there are security weaknesses in voting systems and have advocated that election systems be better secured against malicious attack. As the National Academies recently concluded, “There is no realistic mechanism to fully secure vote casting and tabulation computer systems from cyber threats.” However, notwithstanding these serious concerns, we have never claimed that technical vulnerabilities have actually been exploited to alter the outcome of any US election.</p>
<p>
Anyone asserting that a US election was “rigged” is making an extraordinary claim, one that must be supported by persuasive and verifiable evidence. Merely citing the existence of technical flaws does not establish that an attack occurred, much less that it altered an election outcome. It is simply speculation.</p>
<p>
The presence of security weaknesses in election infrastructure does not by itself tell us that any election has actually been compromised. Technical, physical, and procedural safeguards complicate the task of maliciously exploiting election systems, as does monitoring of likely adversaries by law enforcement and the intelligence community. Altering an election outcome involves more than simply the existence of a technical vulnerability.</p>
<p>
We are aware of alarming assertions being made that the 2020 election was “rigged” by exploiting technical vulnerabilities. However, in every case of which we are aware, these claims either have been unsubstantiated or are technically incoherent. To our collective knowledge, no credible evidence has been put forth that supports a conclusion that the 2020 election outcome in any state has been altered through technical compromise.</p>
</em><p><em>
That said, it is imperative that the US continue working to bolster the security of elections against sophisticated adversaries. At a minimum, all states should employ election security practices and mechanisms recommended by experts to increase assurance in election outcomes, such as post-election risk-limiting audits. </em>
</p></blockquote>
<p/><p>
The letter is well written, but <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> They say in the letter: </p>
<blockquote><p><b> </b> <em> However, notwithstanding these serious concerns, we have never claimed that technical vulnerabilities have actually been exploited to alter the outcome of any US election. </em>
</p></blockquote>
<p>This seems to be a problem. How do you get people to <i>look both ways when crossing the street</i>, when you have also basically asserted: “No one has ever been hit by a car when crossing the street.”</p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/26/the-voting-paradox/cs/" rel="attachment wp-att-18795"><img alt="" class="aligncenter size-full wp-image-18795" height="183" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/cs.png?resize=276%2C183&amp;ssl=1" width="276"/></a></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We believe that election security is a real issue. Perhaps our single point is: Even if we do not believe that any fraud happen in the past, we must agree to make the voting process secure. We must use all our insights to make elections not only fair, but believed to be fair.</p>
<p>
Look both ways. </p>
<p>
Ken notes that “Trust but verify” was first the Russian proverb <i>Doveryay no proveryay</i>, but that its origins are <a href="https://www.rbth.com/lifestyle/330521-reagan-trust-but-verify-chernobyl">unclear</a> even to Russians.  He adds that besides <i>how</i> and <i>by whom</i> there is also the issue of <i>when</i> votes are counted.  The warnings he gave in our election-eve <a href="https://rjlipton.wpcomstaging.com/2020/11/03/the-election-night-time-warp/">post</a> were reflected in <a href="https://www.factcheck.org/2020/12/false-claim-about-bidens-win-probability/">claims</a> made in the early December Texas attorney general filing.</p></font></font></div>
    </content>
    <updated>2021-05-27T00:01:00Z</updated>
    <published>2021-05-27T00:01:00Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Andrew Appel"/>
    <category term="elections"/>
    <category term="Peter G. Neumann"/>
    <category term="Rebecca Mercuri"/>
    <category term="security"/>
    <category term="voting"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-06-04T15:38:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/05/26/postdoc-opportunity-in-algebraic-complexity-at-boston-college-apply-by-july-30-2021/</id>
    <link href="https://cstheory-jobs.org/2021/05/26/postdoc-opportunity-in-algebraic-complexity-at-boston-college-apply-by-july-30-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc Opportunity in Algebraic Complexity at Boston College (apply by July 30, 2021)</title>
    <summary>The Computer Science department at Boston College invites applications for a postdoctoral position in the area of algebraic complexity under the supervision of Ilya Volkovich. See attached pamphlet for more information. Please direct any inquiries to Ilya Volkovich (Email: ilya.volkovich@bc.edu) Website: https://sites.google.com/site/ilyavv/Postdoc%20add.pdf Email: ilya.volkovich@bc.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science department at Boston College invites applications for a postdoctoral position in the area of algebraic complexity under the supervision of Ilya Volkovich.</p>
<p>See attached pamphlet for more information.</p>
<p>Please direct any inquiries to Ilya Volkovich (Email: ilya.volkovich@bc.edu)</p>
<p>Website: <a href="https://sites.google.com/site/ilyavv/Postdoc%20add.pdf">https://sites.google.com/site/ilyavv/Postdoc%20add.pdf</a><br/>
Email: ilya.volkovich@bc.edu</p></div>
    </content>
    <updated>2021-05-26T21:04:52Z</updated>
    <published>2021-05-26T21:04:52Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-06-04T15:38:07Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-8805390553192485548</id>
    <link href="http://processalgebra.blogspot.com/feeds/8805390553192485548/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=8805390553192485548" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/8805390553192485548" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/8805390553192485548" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/05/course-on-ethics-and-accountability-in.html" rel="alternate" type="text/html"/>
    <title>Course on Ethics and Accountability in Computer Science</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>About one year ago, I became aware of the course "Ethics and Accountability in Computer Science" designed and taught by <a href="https://riceacademy.rice.edu/junior-fellows/dr-rodrigo-ferreira" target="_blank">Rodrigo Ferreira</a> and <a href="https://www.cs.rice.edu/~vardi/" target="_blank">Moshe Y. Vardi</a> at Rice University. The goals and high-level structure of the course, as well as its context, are described in an informative and thoughtful <a href="https://www.cs.rice.edu/~vardi/papers/sigcse21.pdf" target="_blank">paper</a> by Moshe and Rodrigo that appears in the <a href="https://dl.acm.org/doi/proceedings/10.1145/3408877" target="_blank">Proceedings of the  52nd ACM Technical Symposium on Computer Science Education (SIGCSE ’21)</a>, which I strongly recommend. You can also read a <a href="https://www.pdcnet.org/tej/content/tej_2021_0999_3_31_87" target="_blank">journal paper</a> reporting on one of their course assignments, which was designed to make students focus on practising <a href="https://medium.com/@the_jennitaur/how-to-do-nothing-57e100f59bbb" target="_blank">"deep attention"</a> in the sense of artist and academic <a href="https://www.jennyodell.com/" target="_blank">Jenny Odell</a>.</p><p>I was smitten by the underlying tenet for their course, namely that "social justice is  the single  most important issue  confronting computer science  students today." That tenet is also very much in line with a reflection on the impact that digital technology has on social justice that the <a href="https://www.gssi.it/institute/organization" target="_blank">Scientific Advisory Board of the Gran Sasso Science Institute</a> asked the Computer Science group at that institute to undertake. Therefore, after having invited Rodrigo to deliver a <a href="http://www.google.com/url?q=http%3A%2F%2Ficetcs.ru.is%2Fwebinars%2Fdeep_ethics.mp4&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNH9ux4zX7PThxWzwoNa_yyZNa-ILA" target="_blank">webinar on the course</a> at the <a href="https://sites.google.com/gssi.it/csgssi/seminars-and-events/joint-ice-tcs-csgssi-virtual-seminar" target="_blank">ICE-TCS+GSSI webinar series</a>, I decided that, as an experiment, I would offer a version of the Rice University course to master students in computer science, language technology and software engineering at Reykjavik University during our spring semester 2021. </p><p>Mine was a foolhardy decision for a variety of reasons. However, I have always believed that computer scientists should strive to be the 21st century Renaissance men and women, and bridge the gap between C. P. Snow's <a href="https://en.wikipedia.org/wiki/The_Two_Cultures" target="_blank">"two cultures"</a>. Indeed, to quote <a href="https://ptolemy.berkeley.edu/~eal/" target="_blank">Edward A. Lee</a> freely, to my mind technologists ought to be amongst the greatest humanists of our age. Despite my other commitments, offering a version of the Ferreira-Vardi course at Reykjavik University felt like the right thing to do at this time. </p><p>I taught the course over twelve weeks to a varied group of eight students, in cooperation with <a href="http://claudiopedica.com/" target="_blank">Claudio Pedica</a>. Thanks to the constant support I received from Rodrigo Ferreira, I lived to tell the tale and I hope that I managed to do some justice to the truly excellent course that Moshe and Rodrigo put together. Having a dream team of students with a variety of cultural backgrounds made the course extremely interesting and a learning experience for me. I had to refresh my memory of the philosophy I studied at high school in Italy in a previous life, learn some modern moral philosophy I had not met at school (such as the work by <a href="https://plato.stanford.edu/entries/anscombe/" target="_blank">Elisabeth Anscombe</a> and <a href="https://plato.stanford.edu/entries/philippa-foot/" target="_blank">Philippa Foot</a>, amongst others), read a substantial amount of new material (some of it fresh off the press as the course was unfolding) and broaden my horizons. I could not have asked for a better intellectual experience and the students in the course, from Denmark, France, Germany, Iceland and the Netherlands taught me well, kept me on my toes and stimulated me to keep reading material related to ethics even now that the course is over. <br/></p><p>Teaching the course reinforced my belief that a course on ethics and on the impact that our field has on social justice should be required for all students in computer science. If you plan to run such a course, I strongly recommend that you consider the Ferreira-Vardi course as a blueprint. <br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-05-26T19:04:00Z</updated>
    <published>2021-05-26T19:04:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-06-01T21:43:16Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8240675027790472002</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8240675027790472002/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/does-university-matter.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8240675027790472002" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8240675027790472002" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/does-university-matter.html" rel="alternate" type="text/html"/>
    <title>Does the university matter?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As we come out of a pandemic with online teaching and research collaborations, how much do we actually need the university?</p><p>Theoretical research in computer science, math and elsewhere it hardly slowed down with everyone hunkered down at home, and when it did it was more because of supervising kids with their on-line learning than being away from the university. Collaborating with those around the world was basically the same as collaborating with your colleagues on campus.</p><p>Many courses, especially the larger ones, worked about as well on-line as they do in person.</p><p>The pandemic is accelerating changes already in place. Before say 1960, the fastest travel for the masses was on train and boats and fast communication limited to expensive phone calls. You needed strong colleagues, a strong library and a strong support staff to be a successful academic. Traveling to meet colleagues and attend conferences was a luxury that few could do often.</p><p>The 60's gave us air travel though it wasn't until the 90's that we could readily send academic papers electronically. It's really only recently that we have the infrastructure to allow high-quality teaching and research collaboration online.</p><p>Suppose universities now just disappeared. Professors would be free agents, supported by grants and tuition from students taking their on-line courses and consulting. Students would pick and choose courses from the best instructors, their "transcript" being recorded on a blockchain-like database. PhD students would be more of an apprenticeship, a low salary in exchange for personalized mentoring from a professor. </p><p>For the experimental sciences, there would be a set of national labs that professors could join.</p><p>Startups would create apps to enable all these things, professors would just become yet another piece of the gig economy. Superstar academics can pull in large salaries, the rest would struggle to make a decent wage--not that different from actors and musicians. </p><p>Don't worry, universities aren't going anywhere soon. Or are they?</p></div>
    </content>
    <updated>2021-05-25T17:21:00Z</updated>
    <published>2021-05-25T17:21:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-06-04T12:10:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5517</id>
    <link href="https://www.scottaaronson.com/blog/?p=5517" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5517#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5517" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">In which I answer more quantum computing questions</title>
    <summary xml:lang="en-US">Yesterday, I had fun doing an open-ended Q&amp;A at the Astral Codex Ten weekly online meetup. See here for the YouTube video. The questions were mainly about quantum computing, but ranged over various other topics as well, including education policy, the Great Stagnation, and what my biggest disagreements with Scott Alexander are. In other news, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Yesterday, I had fun doing an open-ended Q&amp;A at the <a href="https://astralcodexten.substack.com/">Astral Codex Ten</a> weekly online meetup.  <a href="https://www.youtube.com/watch?v=DEYUt1tJlck">See here</a> for the YouTube video.  The questions were mainly about quantum computing, but ranged over various other topics as well, including education policy, the Great Stagnation, and what my biggest disagreements with Scott Alexander are.</p>



<p>In other news, last week I gave my talk about quantum supremacy at the (virtual, of course) <a href="https://quantumscienceseminar.com/videos/">Quantum Science Seminar</a>, organized by Sebastian Blatt and others.  <a href="https://www.youtube.com/watch?v=EeF7T9Bc1B0">See here</a> for the YouTube video.  Since I (alas) needed to leave after an hour sharp, the organizers asked if they could send me additional questions in writing and have me answer them.  I said sure, as long as I could also post the Q&amp;A on this blog!  So without further ado…</p>



<p><strong>Q:</strong> As you said, in computer science it’s difficult to prove that things are hard. From a computer science perspective, to what extent is “quantum supremacy” something absolute, and to what extent is it a shifting bar that depends on how good our classical hardware and algorithms are?</p>



<p><strong>SCOTT:</strong> It’s kind of like the question “can computers beat the best humans at chess?” The latter question also involves a non-absolute shifting bar – maybe humans are getting better! Or maybe they simply haven’t figured out how to beat the computers yet! So how can we say “computer supremacy” has been decisively achieved? Nevertheless, one can clearly see a phase transition, with Deep Blue in 1996-1997, where the burden of proof shifted massively from one side to the other, and has stayed there ever since. Even if humans are getting better, the computers have also gotten better, and at such an insane rate that humans seem to have no chance of ever again catching up.</p>



<p>From a theoretical standpoint, that’s exactly what we might expect to happen with quantum supremacy experiments – simply because they involve tasks that have polynomial scaling for quantum computers, but (as far as we know) exponential scaling for classical computers, where each additional qubit roughly doubles the classical resources required. In analyzing concrete quantum supremacy experiments, I’d say that the goal is not to pin down the exact factor by which they’re beating this or that classical simulation (the answers will change rapidly anyway, and depend on all sorts of low-level details), but simply to figure out whether or not we’ve entered that phase transition.</p>



<p><strong>Q:</strong> What do you generally think about improvements in tensor network methods as a challenge to quantum supremacy and the recent simulation of the supremacy result in Beijing?</p>



<p><strong>SCOTT:</strong> I <a href="https://www.scottaaronson.com/blog/?p=5371">blogged about this here</a>. It was a clever paper, which showed that if you focus narrowly on spoofing the linear cross-entropy benchmark used by Google, then there’s a classical algorithm to do that much faster than had been previously pointed out, by generating many samples that all share most of their bits in common (e.g., that all agree on the first 30 of the 53 bits). But many people remain unaware that, if you just changed the benchmark – for example, if you insisted that the returned samples not only pass the linear cross-entropy benchmark, but also be sufficiently different – then this classical spoofing strategy wouldn’t work and we’d be back to the previous status quo.</p>



<p><strong>Q:</strong> Why do you need a random circuit to test the device, and also why is this more interesting/useful than doing something very specific many times to test the device?</p>



<p><strong>SCOTT:</strong> The reasons to use random quantum circuits are simply that<br/>(1) they generate complicated entangled states on all the qubits nearly as rapidly as it’s possible to do so – indeed, we now have theoretical results that give a detailed understanding of this process, and<br/>(2) random circuits seem to have about as little “usable structure” (which a classical simulation might exploit) as it’s possible to have.<br/>Eventually, of course, we’d like to run actually useful circuits (say, those that arise in Shor’s factoring algorithm), which will typically have regular patterns and be extremely far from random! But then more qubits will be needed to get an advantage over classical computers. It’s not terribly surprising for the <em>first</em> advantage over classical to be via random quantum circuits, which in some sense “maximally exploit” the hardware resources available.</p>



<p><strong>Q:</strong> Have you heard of/what do you think about the <a href="https://www.nature.com/articles/s41467-021-21119-1">recent NP-verification experiment</a> using a quantum optical setup from Paris?</p>



<p><strong>SCOTT:</strong> That experiment was actually based on a protocol that Beigi, Fefferman, Drucker, Shor, and I <a href="https://arxiv.org/abs/0804.0802">proposed back in 2008</a>. It’s crucial for people to understand that there’s no claimed speedup here for <em>solving</em> any NP-complete problem. We’re talking about a more arcane task: namely, proving to someone that an NP-complete problem has a solution by sending them a small number of qubits. Even there, the protocol depends on the ability to send two quantum states that are guaranteed <em>not</em> to be entangled with each other; also, the communication savings is “only” polynomial rather than exponential (in our original protocol, roughly √n qubits where n classical bits would have been needed). Nevertheless, it’s always fun to see a real experiment implementing some version of something that you worked out on paper, even if you already knew it would work!</p>



<p><strong>Q:</strong> If the difficulty for classical simulation is related to the Hilbert space dimension, has it been formally proven that an ideal analog classical computer cannot outperform a quantum computer?</p>



<p><strong>SCOTT:</strong> This is not a question of “formal proof” but of physics. In my view, we already know deep reasons why, in our universe, analog classical computers are unlikely to be able to do anything that can’t be efficiently simulated using a standard digital computer. (In other words, why analog classical computers don’t violate the “Extended Church-Turing Thesis.”) Those reasons have to do with nonlinear dynamics chaotically amplifying even the tiniest errors in an analog device. Or, if you really want to push this discussion to the bitter end, they have to do with the breakdown in our picture of a smooth spacetime that’s expected to occur at the Planck scale, of ~10<sup>-33</sup> centimeters and ~10<sup>-43</sup> seconds, for reasons of black hole thermodynamics and quantum gravity. Crucially, neither of these issues apply to quantum computation. The former doesn’t apply because of quantum error correction  and fault-tolerance, which have the effect of “discretizing” continuous errors; while the latter doesn’t apply because as far as anyone knows today, quantum mechanics (unlike theories that assume a smooth spacetime) is exactly true.</p>



<p><strong>Q:</strong> [In the comparison between quantum and classical computation], what do we mean by “classical resources” here? Do we mean something parochial like “resources obeying non-quantum laws that are available to us humans on Earth?” Or is any classical resource fair game, no matter its size and classical equations of motion? In that case, doesn’t demonstrating quantum supremacy require demonstrating that the quantum computer exceeds the capabilities of, say, a classical computer the size of the solar system that exploits some CTC (closed timelike curve)?</p>



<p><strong>SCOTT:</strong> If CTCs were possible, that would be a revolution in physics even greater than quantum mechanics! Leaving CTCs aside, though, cosmology and quantum gravity seem to impose a limit of roughly 10<sup>122</sup> on the number of bits (and the number of operations on the bits) that any computer that fit inside the observable universe could possibly have. And if you envision that cosmological computer as a classical one, then it shouldn’t take impossibly long for us to build quantum computers that can outperform it on some tasks: indeed, a device with ~400 qubits should already be enough! But of course we’re not there yet: with 50-60 qubits, QCs right now are “merely” challenging the largest classical computers currently available on earth (again, on contrived sampling tasks), rather than the largest that could fit in the observable universe.</p>



<p><strong>Q:</strong> Why is a quantum state (superposition or entangled state) inherently more fragile than a classical state?</p>



<p><strong>SCOTT:</strong> Because when information from the state (say, whether a qubit is 0 or 1, or which path a photon takes through a beamsplitter network) “leaks out” into the environment, the information effectively becomes entangled with the environment, which damages the state in a measurable way.  Indeed, it now appears to an observer as a mixed state rather than a pure state, so that interference between the different components can no longer happen. This is a fundamental, justly-famous feature of quantum information that’s not shared by classical information.</p>



<p><strong>Q:</strong> Given that we have noisy-intermediate scale quantum devices, what do you see as the fundamental role of noise in the discussion on quantum advantage or quantum supremacy. Is it simply a question of less noise is better, or are there things that cannot be done if there is noise?</p>



<p><strong>SCOTT:</strong> There will always be <em>some</em> noise. The big question, about any given platform, is whether the noise is low enough that you can start usefully error-correcting it away, or whether the noise is so high that there’s no point (i.e., whether you’re above or below the “fault-tolerance threshold”).  Until you start doing error-correction, most experts believe there’s a severe limit to how far you can scale: probably to quantum computations involving a few hundred qubits at most. Whereas once you have error-correction, at least in principle the sky’s the limit.</p>



<p><strong>Q:</strong> You used the Wright brothers as an example where an airplane that was not practically useful itself pioneered the path to useful flight. In what sense to the sampling experiments of Google and USTC also pioneer that path for what we need for the future of quantum computing, or to what extent do you see them as niche examples of quantum supremacy?</p>



<p><strong>SCOTT:</strong> I think the majority of what Google did, in integrating 53 superconducting qubits, making them programmable, etc. – and especially in characterizing the noise in a system at that scale – will be directly useful going forward. Indeed, that’s a large part of why they did it!  Likewise, a lot of what USTC did, in integrating hundreds of beamsplitters, photon sources, and photodetectors, could be directly relevant to building a universal optical quantum computer. On the other hand, it’s true that both groups took shortcuts with the immediate goal of quantum supremacy in mind. As an example, Google’s chip uses a particular 2-qubit gate that was chosen, not because it shows up naturally in any application, but simply because it’s extra-hard to simulate using tensor network contraction algorithms, so it let them get to quantum supremacy faster than if they’d used a more conventional 2-qubit gate like the CNOT.</p>



<p><strong>Q:</strong> To what extent does the measured circuit fidelity of 0.2% in the Google experiment limit the usability of this system for other computations?</p>



<p><strong>SCOTT:</strong> Oh, we don’t know of anything particularly useful to do with Google’s Sycamore chip – that is, anything that you couldn’t do much more easily without it – other than<br/>(1) quantum supremacy demonstrations,<br/>(2) <em>possibly</em> the generation of cryptographically certified random bits, and<br/>(3) of course, calibration experiments that tell you about the behavior of integrated superconducting qubits and thereby help you iterate to the next device.<br/>But things are developing rapidly – the best circuit fidelity that was achievable in 2019 is not necessarily the best now, or the best that will be achieved in another year or two.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (May 25):</span></strong> <strong>Please, no new questions</strong>; just discussion of the existing questions and answers! I had hoped to get some work done today; I hadn’t planned on another ask-me-anything session. Thanks!</p></div>
    </content>
    <updated>2021-05-24T21:45:46Z</updated>
    <published>2021-05-24T21:45:46Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-06-01T20:00:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18770</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/05/24/acm-athena-lecturer-award/" rel="alternate" type="text/html"/>
    <title>ACM Athena Lecturer Award</title>
    <summary>It was never about winning medals or being famous—Nancy Kerrigan Award page Ayanna Howard is this year’s winner of the ACM Athena Lecturer Award. She works in robotics and is Dean of Engineering at Ohio State. She was previously Chair of the School of Interactive Computing at Georgia Tech. Today Ken and I congratulate her […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>It was never about winning medals or being famous—Nancy Kerrigan</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/05/24/acm-athena-lecturer-award/ah-2/" rel="attachment wp-att-18783"><img alt="" class="alignright wp-image-18783" height="138" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/ah-1.png?resize=154%2C138&amp;ssl=1" width="154"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Award <a href="https://awards.acm.org/about/2021-athena">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Ayanna Howard is this year’s winner of the ACM Athena Lecturer <a href="https://awards.acm.org/athena">Award</a>. She works in robotics and is <a href="https://engineering.osu.edu/about/office-dean/about-dean-ayanna-howard">Dean</a> of Engineering at Ohio State. She was previously Chair of the School of Interactive Computing at Georgia Tech.</p>
<p>
Today Ken and I congratulate her and also recognize past winners of the award.</p>
<p>
The award is not for lecturing. Here is the description:</p>
<blockquote><p><b> </b> <em> Initiated in 2006, the ACM Athena Lecturer Award celebrates women researchers who have made fundamental contributions to computer science. The award carries a cash prize of $25,000, with financial support provided by Two Sigma. The Athena Lecturer gives an invited talk at a major ACM conference of her choice. </em>
</p></blockquote>
<p>
</p><h2> Howard’s Research </h2><p/>
<p>
Ayanna Howard’s research spans three interconnected areas of robotics:</p>
<ul>
<li>
<b>Outer Space Robotics:</b> She did early work for NASA’s Jet Propulsion Laboratory. Here is a 2002 <a href="https://www.nasa.gov/vision/universe/roboticexplorers/ayanna_howard.html">article</a> that leads with her work on the “Safe Navigation Rover.” <p/>
</li><li>
<b>Inner Space Robotics:</b> This work involves interaction and assistive technologies. It includes her co-founding the company <a href="http://zyrobotics.com/about/">Zyrobotics</a>, which creates mobile therapy and educational technology tools for children. <p/>
</li><li>
<b>Inner Mindspace Robotics:</b> On what grounds can we <em>trust</em> robots? How can they be programmed to avoid biases in their interactions with human beings? Can the algorithms they use be certified for <em>fairness?</em> (We had some recent <a href="https://rjlipton.wpcomstaging.com/2021/03/10/making-algorithms-fair/">thoughts</a> of our own on the last subject.)
</li></ul>
<p>
There are technical connections that flow from a whole-situation/whole-person approach. Her NASA landing and navigation systems were not designed simply to solve a computer-vision and calculus problem of finding the flattest terrain. Instead, the design expressly maps visual and sensor readings into a knowledge and logic-based system of human reasoning. Then the system either judges or makes recommendations to human controllers. This kind of approach naturally flows into assessing the robotic judgments for blind spots and bias. Most in particular, she tries to combat a human tendency of uncritical acceptance of personal assistants. She wants to facilitate humans to challenge the robots and have the robots modulate their reasoning and behavior accordingly.</p>
<p>
She was inspired as a child by the TV show <a href="https://en.wikipedia.org/wiki/The_Bionic_Woman">The Bionic Woman</a>. Adding “-nic” after “bio” took what could have been influence to go into medicine into an avenue that channeled her childhood love of mathematics.</p>
<p>
</p><h2> A Puzzle </h2><p/>
<p>
I designed a word-search puzzle by feeding terms from the award to a web app for creating them. We wondered if it was silly to include it. But maybe the puzzle is not so silly—besides that it was created by a ‘bot—if you think in these terms:</p>
<blockquote><p><b> </b> <em> How might a robot get the most out of solving it? </em>
</p></blockquote>
<p>
Here is the grid—see if you can find the words in it. The key is at the end.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/05/24/acm-athena-lecturer-award/pu/" rel="attachment wp-att-18774"><img alt="" class="aligncenter wp-image-18774" height="400" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/pu.png?resize=400%2C400&amp;ssl=1" width="400"/></a></p>
<p>
From a human standpoint, three minutes is award level. But for a robot:</p>
<ul>
<li>
Of course, a robot could solve the puzzle in three microseconds by brute-force string matching along every row, column, and diagonal. But there is no challenge in that—it is not what we think of as human <em>reading</em>. <p/>
</li><li>
To be human fun, the puzzle should be fairly dense and have a “wordscape” where parallel rows, columns, and diagonals are used. Once we find one word, we can often find neighbors more quickly, and that feels rewarding.
</li></ul>
<p>
That’s what we’re asking: is there a way to teach robots to solve these puzzles in more human ways? OK, it’s too simple. But we like to put a peg down on a simple example, then run a line into the heart of the work, and ask: at what point does it cross the threshold of being nontrivial?</p>
<p>
</p><h2> All Athena Winners </h2><p/>
<p>
Here are links to all the winners of this award:</p>
<p>
2021 <a href="https://awards.acm.org/award_winners/howard_5736184">Ayanna Howard</a></p>
<p>
2020 <a href="https://awards.acm.org/award_winners/kraus_4452744">Sarit Kraus</a></p>
<p>
2019 <a href="https://awards.acm.org/award_winners/bertino_2269926">Elisa Bertino</a></p>
<p>
2018 <a href="https://awards.acm.org/award_winners/goldsmith_5482565">Andrea Goldsmith</a></p>
<p>
2017 <a href="https://awards.acm.org/award_winners/kavraki_3691391">Lydia Kavraki</a></p>
<p>
2016 <a href="https://awards.acm.org/award_winners/rexford_4157665">Jennifer Rexford</a></p>
<p>
2015 <a href="https://awards.acm.org/award_winners/widom_2272011">Jennifer Widom</a></p>
<p>
2014 <a href="https://awards.acm.org/award_winners/dumais_2360550">Susan Dumais</a></p>
<p>
2013 <a href="https://awards.acm.org/award_winners/yelick_3873601">Katherine Yelick</a></p>
<p>
2012 <a href="https://awards.acm.org/award_winners/lynch_2276129">Nancy Lynch</a></p>
<p>
2011 <a href="https://awards.acm.org/award_winners/olson_2607695">Judith Olson</a></p>
<p>
2010 <a href="https://awards.acm.org/award_winners/irwin_1411388">Mary Jane Irwin</a></p>
<p>
2009 <a href="https://awards.acm.org/award_winners/eggers_1922665">Susan Eggers</a></p>
<p>
2008 <a href="https://awards.acm.org/award_winners/goldwasser_8627889">Shafi Goldwasser</a></p>
<p>
2007 <a href="https://awards.acm.org/award_winners/sparck-jones_7172364">Karen Sparck-Jones</a></p>
<p>
2006 <a href="https://awards.acm.org/award_winners/estrin_1782044">Deborah Estrin</a></p>
<p>
</p><h2> Open Problems </h2><p/>
<p>
Lynch and Goldwasser are the only theory researchers. This is less than <img alt="{10 \%}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B10+%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </p>
<p>
The puzzle key:</p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/24/acm-athena-lecturer-award/key/" rel="attachment wp-att-18773"><img alt="" class="aligncenter wp-image-18773" height="425" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/key.png?resize=400%2C425&amp;ssl=1" width="400"/></a></p></font></font></div>
    </content>
    <updated>2021-05-24T16:27:05Z</updated>
    <published>2021-05-24T16:27:05Z</published>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="ACM"/>
    <category term="Algorithms"/>
    <category term="Ayanna Howard"/>
    <category term="fairness"/>
    <category term="knowledge representation"/>
    <category term="Prize"/>
    <category term="puzzle"/>
    <category term="robotics"/>
    <category term="winner"/>
    <category term="word game"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-06-04T15:38:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/072</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/072" rel="alternate" type="text/html"/>
    <title>TR21-072 |  Arithmetic Circuit Complexity of Division and Truncation | 

	Gorav Jindal, 

	Pranjal Dutta, 

	Anurag Pandey, 

	Amit Sinhababu</title>
    <summary>Given polynomials $f,g,h\,\in \mathbb{F}[x_1,\ldots,x_n]$ such that $f=g/h$, where both $g$ and $h$ are computable by arithmetic circuits of size $s$, we show that $f$ can be computed by a circuit of size  $\poly(s,\deg(h))$.  This solves a special case of division elimination for high-degree circuits (Kaltofen'87 \&amp; WACT'16). The result is an exponential improvement over Strassen's classic result (Strassen'73) when $\deg(h)$ is $\poly(s)$ and $\deg(f)$ is $\exp(s)$, since the latter gives an upper bound of $\poly(s, \deg(f))$.

Further, we show that any univariate polynomial family $(f_d)_d$, defined by the initial segment of the power series expansion of rational function $g_d(x)/h_d(x)$ up to degree $d$ (i.e.~$f_d = g_d/h_d \bmod x^{d+1}$), where circuit size of $g$ is $s_d$ and degree of $g_d$ is at most $d$, can be computed by a circuit of size $\poly(s_d,\deg(h_d),\log d)$.
We also show a hardness result when the degrees of the rational functions are high (i.e.~$\Omega (d)$), assuming hardness of the integer factorization problem.</summary>
    <updated>2021-05-24T13:20:40Z</updated>
    <published>2021-05-24T13:20:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-04T15:37:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/071</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/071" rel="alternate" type="text/html"/>
    <title>TR21-071 |  On the Algorithmic Content of Quantum Measurements | 

	Samuel Epstein</title>
    <summary>We show that given a quantum measurement, for an overwhelming majority of pure states, no meaningful information is produced. This is independent of the number of outcomes of the quantum measurement. Due to conservation inequalities, such random noise cannot be processed into coherent data.</summary>
    <updated>2021-05-23T14:21:48Z</updated>
    <published>2021-05-23T14:21:48Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-04T15:37:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5510</id>
    <link href="https://www.scottaaronson.com/blog/?p=5510" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5510#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5510" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">On turning 40 today</title>
    <summary xml:lang="en-US">Holy crap. In case you’re wondering how I spent such a milestone of a day: well, I spent hours of it at an important virtual grant review meeting with the Department of Defense. Alas, when it came time for my own big presentation at that meeting—about what my students and I had done over the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Holy crap.</p>



<p>In case you’re wondering how I spent such a milestone of a day: well, I spent hours of it at an important virtual grant review meeting with the Department of Defense.  Alas, when it came time for my own big presentation at that meeting—about what my students and I had done over the past five years to lay the theoretical foundations for the recent achievement of quantum computational supremacy—I’d uploaded the completely wrong PowerPoint file (it was something.pptx rather than something.ppt, where they <em>weren’t</em> two versions of the same presentation).  Sorting this out took about 10 minutes, destroyed my momentum, and wasted everyone’s time.  I partly blame the Microsoft Teams platform, whose limitations as conferencing software compared to Zoom necessitated emailing my presentation in the first place.  But of course, part of the blame rests with me.</p>



<p>I had to explain apologetically to the US Department of Defense that I’m no good with tech stuff—being a mere computer science PhD.  And unlike many of my colleagues (who I envy), back in my youth—for at age 40 I’m no longer young—I never had enough time to become <em>both</em> the kind of person who might earn a big grant to do quantum computing theory, <em>and</em> the kind of person who’d be minimally competent at the logistics of a review meeting for such a grant.</p>



<p/><hr/><p/>



<p>Forty years.  Seven-eighths of those years, aware of the finiteness of the speed of light and of its value.  Four-fifths of them, aware of the grislier details of the Holocaust.  Three-quarters of them, aware of what it means to write code.  Two-thirds of them, aware of polynomial versus exponential time.  More than half of them trying to understand the capabilities and limitations of quantum computers as my day job.  And then, rounding the corner, more than a third of the years writing this blog, a third of them being a professor, a quarter of them married, a fifth of them raising kids, a thirtieth of them in the midst of a global pandemic.</p>



<p>I didn’t even come <em>close </em>to achieving everything I hoped I would in my thirties.  At least a half-dozen major papers, ones I expected would’ve been finished years ago (on the mixing of coffee and cream, on complexity and firewalls and AdS/CFT, on certified random numbers from sampling-based quantum supremacy experiments, on the implications of the Raz-Tal oracle separation, …), still need to be revised or even written.  Other projects (e.g., the graphic novel about teaching math to Lily) were excitedly announced and then barely even started.  I never wrote most of my promised blog post about the continuum hypothesis, <em>or</em> the one about Stephen Wolfram’s recrudescent claims of a unified theory of physics.  And covid, which determined the world’s working conditions while we were running out the clock, turned out <em>not</em> to be a hyper-productive time for me.  That’s how you know I’m not Newton (well, it’s the not the only way you know).</p>



<p>Anyway, during the runup to it, one’s 40th birthday feels like a temporal singularity, where you have to compress more and more of what you’d hoped to achieve before age 40 as you get closer and closer to it, because what the hell is there on the other side? <em> The</em>y<em>‘re</em> over-40 and hence “old”; <em>you’re</em> under-40 and hence still “young.”</p>



<p>OK, but here I am on the other side right now, <em>the “old” side</em>, and I’m still here, still thinking and writing and feeling fairly continuous with my pre-singularity embodiment!  And so far, in 16 hours on this side, the most senile thing I’ve done has been to email the wrong file attachment and thereby ruin an important funding presenta… you know what, let’s not even go there.</p>



<p>If you feel compelled to give me a 40<sup>th</sup> birthday present, then just make it a comment on this post, as short or long as you like, about what anything I said or did meant for you.  I’m a total softie for that stuff.</p></div>
    </content>
    <updated>2021-05-21T20:44:41Z</updated>
    <published>2021-05-21T20:44:41Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Adventures in Meatspace"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Self-Referential"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-06-01T20:00:21Z</updated>
    </source>
  </entry>
</feed>
