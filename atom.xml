<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-03-06T05:22:22Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02808</id>
    <link href="http://arxiv.org/abs/2003.02808" rel="alternate" type="text/html"/>
    <title>Linear time dynamic programming for the exact path of optimal models selected from a finite set</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hocking:Toby.html">Toby Hocking</a>, Joseph Vargovich <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02808">PDF</a><br/><b>Abstract: </b>Many learning algorithms are formulated in terms of finding model parameters
which minimize a data-fitting loss function plus a regularizer. When the
regularizer involves the l0 pseudo-norm, the resulting regularization path
consists of a finite set of models. The fastest existing algorithm for
computing the breakpoints in the regularization path is quadratic in the number
of models, so it scales poorly to high dimensional problems. We provide new
formal proofs that a dynamic programming algorithm can be used to compute the
breakpoints in linear time. Empirical results on changepoint detection problems
demonstrate the improved accuracy and speed relative to grid search and the
previous quadratic time algorithm.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02801</id>
    <link href="http://arxiv.org/abs/2003.02801" rel="alternate" type="text/html"/>
    <title>Minimum bounded chains and minimum homologous chains in embedded simplicial complexes</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Borradaile:Glencora.html">Glencora Borradaile</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maxwell:William.html">William Maxwell</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nayyeri:Amir.html">Amir Nayyeri</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02801">PDF</a><br/><b>Abstract: </b>We study two optimization problems on simplicial complexes with homology over
$\mathbb{Z}_2$, the minimum bounded chain problem: given a $d$-dimensional
complex $\mathcal{K}$ embedded in $\mathbb{R}^{d+1}$ and a null-homologous
$(d-1)$-cycle $C$ in $\mathcal{K}$, find the minimum $d$-chain with boundary
$C$, and the minimum homologous chain problem: given a $(d+1)$-manifold
$\mathcal{M}$ and a $d$-chain $D$ in $\mathcal{M}$, find the minimum $d$-chain
homologous to $D$. We show strong hardness results for both problems even for
small values of $d$; $d = 2$ for the former problem, and $d=1$ for the latter
problem. We show that both problems are APX-hard, and hard to approximate
within any constant factor assuming the unique games conjecture. On the
positive side, we show that both problems are fixed parameter tractable with
respect to the size of the optimal solution. Moreover, we provide an
$O(\sqrt{\log \beta_d})$-approximation algorithm for the minimum bounded chain
problem where $\beta_d$ is the $d$th Betti number of $\mathcal{K}$. Finally, we
provide an $O(\sqrt{\log n_{d+1}})$-approximation algorithm for the minimum
homologous chain problem where $n_{d+1}$ is the number of $d$-simplices in
$\mathcal{M}$.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02725</id>
    <link href="http://arxiv.org/abs/2003.02725" rel="alternate" type="text/html"/>
    <title>Central limit theorems for additive functionals and fringe trees in tries</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Janson:Svante.html">Svante Janson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02725">PDF</a><br/><b>Abstract: </b>We give general theorems on asymptotic normality for additive functionals of
random tries generated by a sequence of independent strings. These theorems are
applied to show asymptotic normality of the distribution of random fringe trees
in a random trie. Formulas for asymptotic mean and variance are given. In
particular, the proportion of fringe trees of size $k$ (defined as number of
keys) is asymptotically, ignoring oscillations, $c/(k(k-1))$ for $k\ge2$, where
$c=1/(1+H)$ with $H$ the entropy of the digits. Another application gives
asymptotic normality of the number of $k$-protected nodes in a random trie. For
symmetric tries, it is shown that the asymptotic proportion of $k$-protected
nodes (ignoring oscillations) decreases geometrically as $k\to\infty$.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02605</id>
    <link href="http://arxiv.org/abs/2003.02605" rel="alternate" type="text/html"/>
    <title>Dynamic Approximate Maximum Independent Set of Intervals, Hypercubes and Hyperrectangles</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henzinger:Monika.html">Monika Henzinger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neumann:Stefan.html">Stefan Neumann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wiese:Andreas.html">Andreas Wiese</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02605">PDF</a><br/><b>Abstract: </b>Independent set is a fundamental problem in combinatorial optimization. While
in general graphs the problem is essentially inapproximable, for many important
graph classes there are approximation algorithms known in the offline setting.
These graph classes include interval graphs and geometric intersection graphs,
where vertices correspond to intervals/geometric objects and an edge indicates
that the two corresponding objects intersect.
</p>
<p>We present dynamic approximation algorithms for independent set of intervals,
hypercubes and hyperrectangles in $d$ dimensions. They work in the fully
dynamic model where each update inserts or deletes a geometric object. All our
algorithms are deterministic and have worst-case update times that are
polylogarithmic for constant $d$ and $\epsilon&gt; 0$, assuming that the
coordinates of all input objects are in $[0, N]^d$ and each of their edges has
length at least 1. We obtain the following results:
</p>
<p>$\bullet$ For weighted intervals, we maintain a $(1+\epsilon)$-approximate
solution.
</p>
<p>$\bullet$ For $d$-dimensional hypercubes we maintain a
$(1+\epsilon)2^{d}$-approximate solution in the unweighted case and a
$O(2^{d})$-approximate solution in the weighted case. Also, we show that for
maintaining an unweighted $(1+\epsilon)$-approximate solution one needs
polynomial update time for $d\ge2$ if the ETH holds.
</p>
<p>$\bullet$ For weighted $d$-dimensional hyperrectangles we present a dynamic
algorithm with approximation ratio $(1+\epsilon)\log^{d-1}N$.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02583</id>
    <link href="http://arxiv.org/abs/2003.02583" rel="alternate" type="text/html"/>
    <title>Maximum Clique in Disk-Like Intersection Graphs</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonnet:=Eacute=douard.html">Édouard Bonnet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grelier:Nicolas.html">Nicolas Grelier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miltzow:Tillmann.html">Tillmann Miltzow</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02583">PDF</a><br/><b>Abstract: </b>We study the complexity of Maximum Clique in intersection graphs of convex
objects in the plane. On the algorithmic side, we extend the polynomial-time
algorithm for unit disks [Clark '90, Raghavan and Spinrad '03] to translates of
any fixed convex set. We also generalize the efficient polynomial-time
approximation scheme (EPTAS) and subexponential algorithm for disks [Bonnet et
al. '18, Bonamy et al. '18] to homothets of a fixed centrally symmetric convex
set. The main open question on that topic is the complexity of Maximum Clique
in disk graphs. It is not known whether this problem is NP-hard. We observe
that, so far, all the hardness proofs for Maximum Clique in intersection graph
classes $\mathcal I$ follow the same road. They show that, for every graph $G$
of a large-enough class $\mathcal C$, the complement of an even subdivision of
$G$ belongs to the intersection class $\mathcal I$. Then they conclude invoking
the hardness of Maximum Independent Set on the class $\mathcal C$, and the fact
that the even subdivision preserves that hardness. However there is a strong
evidence that this approach cannot work for disk graphs [Bonnet et al. '18]. We
suggest a new approach, based on a problem that we dub Max Interval Permutation
Avoidance, which we prove unlikely to have a subexponential-time approximation
scheme. We transfer that hardness to Maximum Clique in intersection graphs of
objects which can be either half-planes (or unit disks) or axis-parallel
rectangles. That problem is not amenable to the previous approach. We hope that
a scaled down (merely NP-hard) variant of Max Interval Permutation Avoidance
could help making progress on the disk case, for instance by showing the
NP-hardness for (convex) pseudo-disks.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02524</id>
    <link href="http://arxiv.org/abs/2003.02524" rel="alternate" type="text/html"/>
    <title>Characterizations and approximability of hard counting classes below #P</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bakali:Eleni.html">Eleni Bakali</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chalki:Aggeliki.html">Aggeliki Chalki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pagourtzis:Aris.html">Aris Pagourtzis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02524">PDF</a><br/><b>Abstract: </b>An important objective of research in counting complexity is to understand
which counting problems are approximable. In this quest, the complexity class
TotP, a hard subclass of #P, is of key importance, as it contains
self-reducible counting problems with easy decision version, thus eligible to
be approximable. Indeed, most problems known so far to admit an fpras fall into
this class.
</p>
<p>An open question raised recently by the community of descriptive complexity
is to find a logical characterization of TotP and of robust subclasses of TotP.
In this work we define two subclasses of TotP, in terms of descriptive
complexity, both of which are robust in the sense that they have natural
complete problems, which are defined in terms of satisfiability of Boolean
formulae.
</p>
<p>We then explore the relationship between the class of approximable counting
problems and TotP.
</p>
<p>We prove that TotP $\nsubseteq$ FPRAS if and only if NP $\neq$ RP and FPRAS
$\nsubseteq$ TotP unless RP = P. To this end we introduce two ancillary classes
that can both be seen as counting versions of RP. We further show that
</p>
<p>FPRAS lies between one of these classes and a counting version of BPP.
</p>
<p>Finally, we provide a complete picture of inclusions among all the classes
defined or discussed in this paper with respect to different conjectures about
the NP vs. RP vs. P questions.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02518</id>
    <link href="http://arxiv.org/abs/2003.02518" rel="alternate" type="text/html"/>
    <title>Simple and sharp analysis of k-means||</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Václav Rozhoň <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02518">PDF</a><br/><b>Abstract: </b>We present a truly simple analysis of k-means|| (Bahmani et al., PVLDB 2012)
-- a distributed variant of the k-means++ algorithm (Arthur and Vassilvitskii,
SODA 2007) -- and improve it from $O(\log\textrm{Var} X)$, where $\textrm{Var}
X$ is the variance of the input data set, to $O(\log\textrm{Var} X /
\log\log\textrm{Var} X)$, which we show to be tight.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02513</id>
    <link href="http://arxiv.org/abs/2003.02513" rel="alternate" type="text/html"/>
    <title>Simple and Fast Algorithm for Binary Integer and Online Linear Programming</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Xiaocheng.html">Xiaocheng Li</a>, Chunlin Sun, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Ye:Yinyu.html">Yinyu Ye</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02513">PDF</a><br/><b>Abstract: </b>In this paper, we develop a simple and fast online algorithm for solving a
general class of binary integer linear programs (LPs). The algorithm requires
only one single pass through the input data and is free of doing any matrix
inversion. It can be viewed as both an approximate algorithm for solving binary
integer LPs and a fast algorithm for solving online LP problems. The algorithm
is inspired by an equivalent form of the dual problem of the relaxed LP and it
essentially performs projected stochastic subgradient descent in the dual
space. We analyze the algorithm in two different models, stochastic input model
and random permutation model, with minimal assumptions on the input of the LP.
The algorithm achieves $O\left(m^2 \sqrt{n}\right)$ expected regret under the
stochastic input model and $O\left((m^2+\log n)\sqrt{n}\right)$ expected regret
under the random permutation model, and it achieves $O(m \sqrt{n})$ expected
constraint violation under both models, where $n$ is the number of decision
variables and $m$ is the number of constraints. Furthermore, the algorithm is
generalized to a multi-dimensional LP setting which covers a wider range of
applications and features for the same performance guarantee. Numerical
experiments illustrate the general applicability and the performance of the
algorithms.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02483</id>
    <link href="http://arxiv.org/abs/2003.02483" rel="alternate" type="text/html"/>
    <title>Parameterized Algorithms for Generalizations of Directed Feedback Vertex Set</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=ouml=ke:Alexander.html">Alexander Göke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marx:D=aacute=niel.html">Dániel Marx</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mnich:Matthias.html">Matthias Mnich</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02483">PDF</a><br/><b>Abstract: </b>The Directed Feedback Vertex Set (DFVS) problem takes as input a directed
graph~$G$ and seeks a smallest vertex set~$S$ that hits all cycles in $G$. This
is one of Karp's 21 $\mathsf{NP}$-complete problems. Resolving the
parameterized complexity status of DFVS was a long-standing open problem until
Chen et al. [STOC 2008, J. ACM 2008] showed its fixed-parameter tractability
via a $4^kk! n^{\mathcal{O}(1)}$-time algorithm, where $k = |S|$.
</p>
<p>Here we show fixed-parameter tractability of two generalizations of DFVS:
</p>
<p>- Find a smallest vertex set $S$ such that every strong component of $G - S$
has size at most~$s$: we give an algorithm solving this problem in time
$4^k(ks+k+s)!\cdot n^{\mathcal{O}(1)}$. This generalizes an algorithm by Xiao
[JCSS 2017] for the undirected version of the problem.
</p>
<p>- Find a smallest vertex set $S$ such that every non-trivial strong component
of $G - S$ is 1-out-regular: we give an algorithm solving this problem in time
$2^{\mathcal{O}(k^3)}\cdot n^{\mathcal{O}(1)}$.
</p>
<p>We also solve the corresponding arc versions of these problems by
fixed-parameter algorithms.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02475</id>
    <link href="http://arxiv.org/abs/2003.02475" rel="alternate" type="text/html"/>
    <title>Optimal Discretization is Fixed-parameter Tractable</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kratsch:Stefan.html">Stefan Kratsch</a>, Tomáš Masařík, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Muzi:Irene.html">Irene Muzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pilipczuk:Marcin.html">Marcin Pilipczuk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sorge:Manuel.html">Manuel Sorge</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02475">PDF</a><br/><b>Abstract: </b>Given two disjoint sets $W_1$ and $W_2$ of points in the plane, the Optimal
Discretization problem asks for the minimum size of a family of horizontal and
vertical lines that separate $W_1$ from $W_2$, that is, in every region into
which the lines partition the plane there are either only points of $W_1$, or
only points of $W_2$, or the region is empty. Equivalently, Optimal
Discretization can be phrased as a task of discretizing continuous variables:
we would like to discretize the range of $x$-coordinates and the range of
$y$-coordinates into as few segments as possible, maintaining that no pair of
points from $W_1 \times W_2$ are projected onto the same pair of segments under
this discretization.
</p>
<p>We provide a fixed-parameter algorithm for the problem, parameterized by the
number of lines in the solution. Our algorithm works in time $2^{O(k^2 \log k)}
n^{O(1)}$, where $k$ is the bound on the number of lines to find and $n$ is the
number of points in the input.
</p>
<p>Our result answers in positive a question of Bonnet, Giannopolous, and Lampis
[IPEC 2017] and of Froese (PhD thesis, 2018) and is in contrast with the known
intractability of two closely related generalizations: the Rectangle Stabbing
problem and the generalization in which the selected lines are not required to
be axis-parallel.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02433</id>
    <link href="http://arxiv.org/abs/2003.02433" rel="alternate" type="text/html"/>
    <title>Fast Noise Removal for $k$-Means Clustering</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Im:Sungjin.html">Sungjin Im</a>, Mahshid Montazer Qaem, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moseley:Benjamin.html">Benjamin Moseley</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Xiaorui.html">Xiaorui Sun</a>, Rudy Zhou <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02433">PDF</a><br/><b>Abstract: </b>This paper considers $k$-means clustering in the presence of noise. It is
known that $k$-means clustering is highly sensitive to noise, and thus noise
should be removed to obtain a quality solution. A popular formulation of this
problem is called $k$-means clustering with outliers. The goal of $k$-means
clustering with outliers is to discard up to a specified number $z$ of points
as noise/outliers and then find a $k$-means solution on the remaining data. The
problem has received significant attention, yet current algorithms with
theoretical guarantees suffer from either high running time or inherent loss in
the solution quality. The main contribution of this paper is two-fold. Firstly,
we develop a simple greedy algorithm that has provably strong worst case
guarantees. The greedy algorithm adds a simple preprocessing step to remove
noise, which can be combined with any $k$-means clustering algorithm. This
algorithm gives the first pseudo-approximation-preserving reduction from
$k$-means with outliers to $k$-means without outliers. Secondly, we show how to
construct a coreset of size $O(k \log n)$. When combined with our greedy
algorithm, we obtain a scalable, near linear time algorithm. The theoretical
contributions are verified experimentally by demonstrating that the algorithm
quickly removes noise and obtains a high-quality clustering.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02336</id>
    <link href="http://arxiv.org/abs/2003.02336" rel="alternate" type="text/html"/>
    <title>Approximating Optimal Bidirectional Macro Schemes</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Russo:Lu=iacute=s_M=_S=.html">Luís M. S. Russo</a>, Ana D. Correia, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Francisco:Alexandre_P=.html">Alexandre P. Francisco</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02336">PDF</a><br/><b>Abstract: </b>Lempel-Ziv is an easy-to-compute member of a wide family of so-called macro
schemes; it restricts pointers to go in one direction only. Optimal
bidirectional macro schemes are NP-complete to find, but they may provide much
better compression on highly repetitive sequences. We consider the problem of
approximating optimal bidirectional macro schemes. We describe a simulated
annealing algorithm that usually converges quickly. Moreover, in some cases, we
obtain bidirectional macro schemes that are provably a 2-approximation of the
optimal. We test our algorithm on a number of artificial repetitive texts and
verify that it is efficient in practice and outperforms Lempel-Ziv, sometimes
by a wide margin.
</p></div>
    </summary>
    <updated>2020-03-06T02:30:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02323</id>
    <link href="http://arxiv.org/abs/2003.02323" rel="alternate" type="text/html"/>
    <title>Towards a Complexity-theoretic Understanding of Restarts in SAT solvers</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Chunxiao.html">Chunxiao Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fleming:Noah.html">Noah Fleming</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vinyals:Marc.html">Marc Vinyals</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pitassi:Toniann.html">Toniann Pitassi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ganesh:Vijay.html">Vijay Ganesh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02323">PDF</a><br/><b>Abstract: </b>Restarts are a widely used class of techniques integral to the efficiency of
Conflict-Driven Clause Learning (CDCL) SAT solvers. While the utility of such
policies has been well-established empirically, until now we didn't have a
complexity-theoretic understanding of why restart policies are crucial to the
power of CDCL SAT solvers.
</p>
<p>In this paper, we prove a series of theoretical results that characterize the
power of restarts for various models of Boolean SAT solvers. More precisely, we
make the following contributions. First, we prove an exponential separation
between a drunk randomized CDCL SAT solver model with restarts and the same
model without restarts using a family of satisfiable instances we call
$Ladder_n$ formulas. Second, we show that the configuration of CDCL SAT solver
with VSIDS branching and restarts (with activities erased after restarts) are
exponentially more powerful than the same configuration without restarts for a
family of unsatisfiable instances. To the best of our knowledge, these are the
first such set of separation results involving restarts in the context of SAT
solvers. Third, we show that restarts do not add any proof complexity-theoretic
power vis-a-vis a variety of models of CDCL and DPLL SAT solvers with
non-deterministic static variable selection (branching) and value selection.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02302</id>
    <link href="http://arxiv.org/abs/2003.02302" rel="alternate" type="text/html"/>
    <title>An Inverse Olympic Medal Tally Transformation for Optimal Lane-level Road Network Path Traversal</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Luxen:Dennis.html">Dennis Luxen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02302">PDF</a><br/><b>Abstract: </b>Lane-level traversal of (almost) arbitrary input paths is a common problem in
the mapping industry. This paper considers the problem of generating
\emph{feasible} and maximally convenient lane-level path traversals. The
presented approach exploits a graph transformation of an input path which is
subsequentially explored by a multi-criteria search algorithm. This approach is
able to yield paths traversals that are guaranteed to obey lane crossing rules
whenever possible, and minimize the number of legal, yet inconvenient lane
crossings along any given input path.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02291</id>
    <link href="http://arxiv.org/abs/2003.02291" rel="alternate" type="text/html"/>
    <title>Que Sera Consensus: Simple Asynchronous Agreement with Private Coins and Threshold Logical Clocks</title>
    <feedworld_mtime>1583452800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ford:Bryan.html">Bryan Ford</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jovanovic:Philipp.html">Philipp Jovanovic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Syta:Ewa.html">Ewa Syta</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02291">PDF</a><br/><b>Abstract: </b>It is commonly held that asynchronous consensus is much more complex,
difficult, and costly than partially-synchronous algorithms, especially without
using common coins. This paper challenges that conventional wisdom with que
sera consensus QSC, an approach to consensus that cleanly decomposes the
agreement problem from that of network asynchrony. QSC uses only private coins
and reaches consensus in $O(1)$ expected communication rounds. It relies on
"lock-step" synchronous broadcast, but can run atop a threshold logical clock
(TLC) algorithm to time and pace partially-reliable communication atop an
underlying asynchronous network. This combination is arguably simpler than
partially-synchronous consensus approaches like (Multi-)Paxos or Raft with
leader election, and is more robust to slow leaders or targeted network
denial-of-service attacks. The simplest formulations of QSC atop TLC incur
expected $O(n^2)$ messages and $O(n^4)$ bits per agreement, or $O(n^3)$ bits
with straightforward optimizations. An on-demand implementation, in which
clients act as "natural leaders" to execute the protocol atop stateful servers
that merely implement passive key-value stores, can achieve $O(n^2)$ expected
communication bits per client-driven agreement.
</p></div>
    </summary>
    <updated>2020-03-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4649</id>
    <link href="https://www.scottaaronson.com/blog/?p=4649" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4649#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4649" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Turn down the quantum volume</title>
    <summary xml:lang="en-US">Several people asked me to comment on the recent announcement by Honeywell that they’ll soon have what they call “the most powerful” quantum computer (see here for press release, here for Forbes article, here for paper). I’m glad that Honeywell, which many people might know as an air-conditioner manufacturer, has entered the race for trapped-ion […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Several people asked me to comment on the recent announcement by Honeywell that they’ll soon have what they call “the most powerful” quantum computer (see <a href="https://www.honeywell.com/en-us/newsroom/news/2020/03/behind-the-scenes-of-a-major-quantum-breakthrough">here for press release</a>, <a href="https://www.forbes.com/sites/moorinsights/2020/03/03/honeywell-surprisingly-announces-it-will-be-releasing-the-most-powerful-quantum-computer-in-the-world/#c271c5114b4a">here for <em>Forbes</em> article</a>, <a href="https://www.honeywell.com/content/dam/honeywell/files/Beta_10_Quantum_3_3_2020.pdf">here for paper</a>).  </p>



<p>I’m glad that Honeywell, which many people might know as an air-conditioner manufacturer, has entered the race for trapped-ion QC.  I wish them success.  I’ve known about what they were doing in part because Drew Potter, my friend and colleague in UT Austin’s physics department, took a one-year leave from UT to contribute to their effort.</p>



<p>Here I wanted to comment about one detail in Honeywell’s announcement: namely, the huge emphasis on “quantum volume” as the central metric for judging quantum computing progress, and the basis for calling their own planned device the “most powerful.”  One journalist asked me to explain why quantum volume is such an important measure.  I had to give her an honest answer: I don’t know whether it is.</p>



<p>Quantum volume was invented a few years ago by a group at IBM.  According to one of <a href="https://arxiv.org/pdf/1811.12926.pdf">their papers</a>, it can be defined roughly as 2<sup>k</sup>, where k is the largest number such that you can run a k-qubit random quantum circuit, with depth k and with any-to-any connectivity, and have at least (say) 2/3 probability of measuring an answer that passes some statistical test.  (In the paper, they use what Lijie Chen and I called <a href="https://arxiv.org/abs/1612.05903">Heavy Output Generation</a>, though Google’s Linear Cross-Entropy Benchmark is similar.)</p>



<p>I don’t know why IBM takes the “volume” to be 2<sup>k</sup> rather than k itself.  Leaving that aside, though, the idea was to invent a single “goodness measure” for quantum computers that can’t be gamed <em>either</em> by building a huge number of qubits that don’t maintain nearly enough coherence (what one might call “the D-Wave approach”), <em>or</em> by building just one perfect qubit, <em>or</em> by building qubits that behave well in isolation but don’t interact easily.  Note that the any-to-any connectivity requirement makes things harder for architectures with nearest-neighbor interactions only, like the 2D superconducting chips being built by Google, Rigetti, or IBM itself.</p>



<p>You know the notion of a researcher’s <a href="https://en.wikipedia.org/wiki/H-index">h-index</a>—defined as the largest h such that she’s published h papers that garnered h citations each?  Quantum volume is basically an h-index for quantum computers.  It’s an attempt to take several different yardsticks of experimental progress, none terribly useful in isolation, and combine them into one “consumer index.”</p>



<p>Certainly I sympathize with the goal of broadening people’s focus beyond the “but how many qubits does it have?” question—since the answer to that question is meaningless without further information about what the qubits can <em>do</em>.  From that standpoint, quantum volume seems like a clear step in the right direction.</p>



<p>Alas, <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart’s Law</a> states that “as soon as a measure becomes a target, it ceases to be a good measure.”  That happened years ago with the h-index, which now regularly pollutes academic hiring and promotion decisions, to the point where <a href="https://arxiv.org/abs/2001.09496">its inventor expressed regrets</a>.  Quantum volume is now looking to me like another example of Goodhart’s Law at work.</p>



<p>The position of Honeywell’s PR seems to be that, if they can build a device that can apply 6 layers of gates to 6 qubits, with full connectivity and good fidelity, that will then count as “the world’s most powerful quantum computer,” since it will have the largest volume.  One problem here is that such a device could be simulated by maintaining a vector of only 2<sup>6</sup>=64 amplitudes.  This is nowhere near quantum supremacy (i.e., beating classical computers at some well-defined task), which is a necessary though not sufficient condition for doing anything useful.</p>



<p>Think of a university that achieves an average faculty-to-student ratio of infinity by holding one class with zero students.  It gets the “best score” only by exploiting an obvious defect in the scoring system.</p>



<p>So what’s the alternative?  The policy that <em>I</em> prefer is simply to tell the world all your system specs, as clearly as you can, with no attempts made to bury the lede.  How many qubits do you have?  With what coherence times?  With what connectivity?  What are the 1- and 2-qubit gate fidelities?  What depth of circuit can you do?  What resources do the standard classical algorithms need to simulate your system?  Most importantly: what’s the main drawback of your system, the spec that’s the <em>least</em> good, that you most need to improve?  What prevents you from having a scalable quantum computer right now?  And are you going to tell me in your public announcement, or will you make me search in Appendix III.B of your paper, or worse yet, ask one of your competitors?</p>



<p>I confess that the answers to the above questions are harder to summarize in a single number (unless we, like, concatenated binary encodings of the answers or something).  But they <em>can</em> be ineffably combined, to produce a progress metric that one of my postdocs suggested calling “quantum scottness,” and which roughly equals the number of expressions of wide-eyed surprise minus the number of groans.</p></div>
    </content>
    <updated>2020-03-05T21:36:59Z</updated>
    <published>2020-03-05T21:36:59Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-03-05T23:54:56Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-940496487994761300</id>
    <link href="https://blog.computationalcomplexity.org/feeds/940496487994761300/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/03/a-new-college-of-computing-at-illinois.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/940496487994761300" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/940496487994761300" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/03/a-new-college-of-computing-at-illinois.html" rel="alternate" type="text/html"/>
    <title>A New College of Computing at Illinois Tech</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="separator" style="clear: both; text-align: center;">
<a href="https://1.bp.blogspot.com/-4o0elQGFp-c/XmAhjkpOpKI/AAAAAAABxio/I6qGUp1ot6UA8rJtL2GLvRDcZU3YEZhaQCLcBGAsYHQ/s1600/IllinoisTech_MiesCampus.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="266" src="https://1.bp.blogspot.com/-4o0elQGFp-c/XmAhjkpOpKI/AAAAAAABxio/I6qGUp1ot6UA8rJtL2GLvRDcZU3YEZhaQCLcBGAsYHQ/s400/IllinoisTech_MiesCampus.jpg" width="400"/></a></div>
<br/>
In 1890, Chicago South Side pastor Frank Gunsaulus gave a sermon where he said that with a million dollars he could build a school where students of all backgrounds could prepare for meaningful roles in a changing industrial society. One of the congregants, Philip Armour, came up to him after the service and told Gunsaulus that "if you give me five years of your time, I will give you the money." Thus was born the Armour Institute of Technology, the forerunner of the Illinois Institute of Technology.<br/>
<br/>
Today Illinois Tech enters a new chapter, <a href="https://www.iit.edu/news/illinois-tech-creates-college-computing-fuel-chicagos-tech-rise">announcing a College of Computing</a>, and I am honored to have been asked to serve as its inaugural dean. The college will take on a horizontal mission, to infuse computation and data science thinking throughout the curriculum in every discipline, while understanding the power, limitations and social implications of the technologies they create. We will significantly grow computing to produce the talent needed for a growing Chicago tech community. The college will develop an agile curriculum to continually reevaluate our offerings as computing technology continues to advance, and develop education as a life-long process where our alumni can always count on Illinois Tech to continually reskill to advance their careers.<br/>
<br/>
We will do it all by keeping the core principle of the original "million-dollar sermon," as important as ever, to prepare students of all backgrounds for meaningful roles in a changing technological society.<br/>
<div>
<br/></div></div>
    </content>
    <updated>2020-03-05T14:35:00Z</updated>
    <published>2020-03-05T14:35:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-03-06T04:48:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=395</id>
    <link href="https://tcsplus.wordpress.com/2020/03/04/tcs-talk-wednesday-march-11-thomas-steinke-ibm-research-almaden/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, March 11 — Thomas Steinke, IBM Research Almaden</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, March 11th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). Thomas Steinke from IBM Research Almaden will speak about “Reasoning About Generalization via Conditional Mutual Information” (abstract below). Please make sure you reserve a spot for your group […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, March 11th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong>Thomas Steinke</strong> from IBM Research Almaden will speak about “<em>Reasoning About Generalization via Conditional Mutual Information</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. In view of the recent travel restrictions and coronavirus precautions, in particular, do not hesitate to reserve a seat even for a group <i class="moz-txt-slash">of size one</i>: there should be enough room for everyone, so don’t be shy!</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We provide an information-theoretic framework for studying the generalization properties of machine learning algorithms. Our framework ties together existing approaches, including uniform convergence bounds and recent methods for adaptive data analysis. Specifically, we use Conditional Mutual Information (CMI) to quantify how well the input (i.e., the training data) can be recognized given the output (i.e., the trained model) of the learning algorithm. We show that bounds on CMI can be obtained from VC dimension, compression schemes, differential privacy, and other methods. We then show that bounded CMI implies various forms of generalization.</p>
<p>Based on joint work with Lydia Zakynthinou.</p></blockquote></div>
    </content>
    <updated>2020-03-05T01:16:38Z</updated>
    <published>2020-03-05T01:16:38Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-03-06T05:21:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02200</id>
    <link href="http://arxiv.org/abs/2003.02200" rel="alternate" type="text/html"/>
    <title>Array relocation approach for radial scanning algorithms on multi-GPU systems: total viewshed problem as a case study</title>
    <feedworld_mtime>1583366400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sanchez:A=_J=.html">A. J. Sanchez</a>, L. F. Romero, G. Bandera, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tabik:S=.html">S. Tabik</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02200">PDF</a><br/><b>Abstract: </b>In geographic information systems, Digital Elevation Models (DEMs) are
commonly processed using radial scanning based algorithms. These algorithms are
particularly popular when calculating parameters whose magnitudes decrease with
the distance squared such as those related to radio signals, sound waves, and
human eyesight. However, radial scanning algorithms imply a large number of
accesses to 2D arrays, which despite being regular, results in poor data
locality. This paper proposes a new methodology, termed sDEM, which
substantially improves the locality of memory accesses and largely increases
the inherent parallelism involved in the computation of radial scanning
algorithms. In particular, sDEM applies a data restructuring technique prior to
accessing the memory and performing the computation. In order to demonstrate
the high efficiency of sDEM, we use the problem of total viewshed computation
as a case study. Sequential, parallel, single-GPU and multi-GPU implementations
are analyzed and compared with the state-of-the-art total viewshed computation
algorithm. Experiments show that sDEM achieves an acceleration rate of up to
827.3 times for the best multi-GPU execution approach with respect to the best
multi-core implementation.
</p></div>
    </summary>
    <updated>2020-03-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02190</id>
    <link href="http://arxiv.org/abs/2003.02190" rel="alternate" type="text/html"/>
    <title>Incidences between points and curves with almost two degrees of freedom</title>
    <feedworld_mtime>1583366400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sharir:Micha.html">Micha Sharir</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zlydenko:Oleg.html">Oleg Zlydenko</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02190">PDF</a><br/><b>Abstract: </b>We study incidences between points and algebraic curves in three dimensions,
taken from a family $C$ of curves that have almost two degrees of freedom,
meaning that every pair of curves intersect in $O(1)$ points, for any pair of
points $p$, $q$, there are only $O(1)$ curves of $C$ that pass through both
points, and a pair $p$, $q$ of points admit a curve of $C$ that passes through
both of them iff $F(p,q)=0$ for some polynomial $F$.
</p>
<p>We study two specific instances, one involving unit circles in $R^3$ that
pass through some fixed point (so called anchored unit circles), and the other
involving tangencies between directed points (points and directions) and
circles in the plane; a directed point is tangent to a circle if the point lies
on the circle and the direction is the tangent direction. A lifting
transformation of Ellenberg et al. maps these tangencies to incidences between
points and curves in three dimensions. In both instances the curves in $R^3$
have almost two degrees of freedom.
</p>
<p>We show that the number of incidences between $m$ points and $n$ anchored
unit circles in $R^3$, as well as the number of tangencies between $m$ directed
points and $n$ arbitrary circles in the plane, is $O(m^{3/5}n^{3/5}+m+n)$.
</p>
<p>We derive a similar incidence bound, with a few additional terms, for more
general families of curves in $R^3$ with almost two degrees of freedom.
</p>
<p>The proofs follow standard techniques, based on polynomial partitioning, but
face a novel issue involving surfaces that are infinitely ruled by the
respective family of curves, as well as surfaces in a dual 3D space that are
infinitely ruled by the respective family of suitably defined dual curves.
</p>
<p>The general bound that we obtain is $O(m^{3/5}n^{3/5}+m+n)$ plus additional
terms that depend on how many curves or dual curves can lie on an
infinitely-ruled surface.
</p></div>
    </summary>
    <updated>2020-03-05T23:36:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02187</id>
    <link href="http://arxiv.org/abs/2003.02187" rel="alternate" type="text/html"/>
    <title>Scheduling Kernels via Configuration LP</title>
    <feedworld_mtime>1583366400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Dušan Knop, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kouteck=yacute=:Martin.html">Martin Koutecký</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02187">PDF</a><br/><b>Abstract: </b>Makespan minimization (on parallel identical or unrelated machines) is
arguably the most natural and studied scheduling problem. A common approach in
practical algorithm design is to reduce the size of a given instance by a fast
preprocessing step while being able to recover key information even after this
reduction. This notion is formally studied as kernelization (or simply, kernel)
-- a polynomial time procedure which yields an equivalent instance whose size
is bounded in terms of some given parameter. It follows from known results that
makespan minimization parameterized by the longest job processing time
$p_{\max}$ has a kernelization yielding a reduced instance whose size is
exponential in $p_{\max}$. Can this be reduced to polynomial in $p_{\max}$?
</p>
<p>We answer this affirmatively not only for makespan minimization, but also for
the (more complicated) objective of minimizing the weighted sum of completion
times, also in the setting of unrelated machines when the number of machine
kinds is a parameter. Our algorithm first solves the Configuration LP and based
on its solution constructs a solution of an intermediate problem, called huge
$N$-fold integer programming. This solution is further reduced in size by a
series of steps, until its encoding length is polynomial in the parameters.
Then, we show that huge $N$-fold IP is in NP, which implies that there is a
polynomial reduction back to our scheduling problem, yielding a kernel.
</p>
<p>Our technique is highly novel in the context of kernelization, and our
structural theorem about the Configuration LP is of independent interest.
Moreover, we show a polynomial kernel for huge $N$-fold IP conditional on
whether the so-called separation subproblem can be solved in polynomial time.
Considering that integer programming does not admit polynomial kernels except
for quite restricted cases, our "conditional kernel" provides new insight.
</p></div>
    </summary>
    <updated>2020-03-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02169</id>
    <link href="http://arxiv.org/abs/2003.02169" rel="alternate" type="text/html"/>
    <title>Pivot Selection for Median String Problem</title>
    <feedworld_mtime>1583366400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mirabal:Pedro.html">Pedro Mirabal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abreu:Jos=eacute=.html">José Abreu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pedreira:Oscar.html">Oscar Pedreira</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02169">PDF</a><br/><b>Abstract: </b>The Median String Problem is W[1]-Hard under the Levenshtein distance, thus,
approximation heuristics are used. Perturbation-based heuristics have been
proved to be very competitive as regards the ratio approximation
accuracy/convergence speed. However, the computational burden increase with the
size of the set. In this paper, we explore the idea of reducing the size of the
problem by selecting a subset of representative elements, i.e. pivots, that are
used to compute the approximate median instead of the whole set. We aim to
reduce the computation time through a reduction of the problem size while
achieving similar approximation accuracy. We explain how we find those pivots
and how to compute the median string from them. Results on commonly used test
data suggest that our approach can reduce the computational requirements
(measured in computed edit distances) by $8$\% with approximation accuracy as
good as the state of the art heuristic.
</p>
<p>This work has been supported in part by CONICYT-PCHA/Doctorado
Nacional/$2014-63140074$ through a Ph.D. Scholarship; Universidad Cat\'{o}lica
de la Sant\'{i}sima Concepci\'{o}n through the research project DIN-01/2016;
European Union's Horizon 2020 under the Marie Sk\l odowska-Curie grant
agreement $690941$; Millennium Institute for Foundational Research on Data
(IMFD); FONDECYT-CONICYT grant number $1170497$; and for O. Pedreira, Xunta de
Galicia/FEDER-UE refs. CSI ED431G/01 and GRC: ED431C 2017/58.
</p></div>
    </summary>
    <updated>2020-03-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02161</id>
    <link href="http://arxiv.org/abs/2003.02161" rel="alternate" type="text/html"/>
    <title>The Online Min-Sum Set Cover Problem</title>
    <feedworld_mtime>1583366400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fotakis:Dimitris.html">Dimitris Fotakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kavouras:Loukas.html">Loukas Kavouras</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koumoutsos:Grigorios.html">Grigorios Koumoutsos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Skoulakis:Stratis.html">Stratis Skoulakis</a>, Manolis Vardas <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02161">PDF</a><br/><b>Abstract: </b>We consider the online Min-Sum Set Cover (MSSC), a natural and intriguing
generalization of the classical list update problem. In Online MSSC, the
algorithm maintains a permutation on $n$ elements based on subsets $S_1, S_2,
\ldots$ arriving online. The algorithm serves each set $S_t$ upon arrival,
using its current permutation $\pi_{t}$, incurring an access cost equal to the
position of the first element of $S_t$ in $\pi_{t}$. Then, the algorithm may
update its permutation to $\pi_{t+1}$, incurring a moving cost equal to the
Kendall tau distance of $\pi_{t}$ to $\pi_{t+1}$. The objective is to minimize
the total access and moving cost for serving the entire sequence. We consider
the $r$-uniform version, where each $S_t$ has cardinality $r$. List update is
the special case where $r = 1$.
</p>
<p>We obtain tight bounds on the competitive ratio of deterministic online
algorithms for MSSC against a static adversary, that serves the entire sequence
by a single permutation. First, we show a lower bound of
$(r+1)(1-\frac{r}{n+1})$ on the competitive ratio. Then, we consider several
natural generalizations of successful list update algorithms and show that they
fail to achieve any interesting competitive guarantee. On the positive side, we
obtain a $O(r)$-competitive deterministic algorithm using ideas from online
learning and the multiplicative weight updates (MWU) algorithm.
</p>
<p>Furthermore, we consider efficient algorithms. We propose a memoryless online
algorithm, called Move-All-Equally, which is inspired by the Double Coverage
algorithm for the $k$-server problem. We show that its competitive ratio is
$\Omega(r^2)$ and $2^{O(\sqrt{\log n \cdot \log r})}$, and conjecture that it
is $f(r)$-competitive. We also compare Move-All-Equally against the dynamic
optimal solution and obtain (almost) tight bounds by showing that it is
$\Omega(r \sqrt{n})$ and $O(r^{3/2} \sqrt{n})$-competitive.
</p></div>
    </summary>
    <updated>2020-03-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02144</id>
    <link href="http://arxiv.org/abs/2003.02144" rel="alternate" type="text/html"/>
    <title>Online metric algorithms with untrusted predictions</title>
    <feedworld_mtime>1583366400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Antoniadis:Antonios.html">Antonios Antoniadis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coester:Christian.html">Christian Coester</a>, Marek Elias, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Polak:Adam.html">Adam Polak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Simon:Bertrand.html">Bertrand Simon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02144">PDF</a><br/><b>Abstract: </b>Machine-learned predictors, although achieving very good results for inputs
resembling training data, cannot possibly provide perfect predictions in all
situations. Still, decision-making systems that are based on such predictors
need not only to benefit from good predictions but also to achieve a decent
performance when the predictions are inadequate. In this paper, we propose a
prediction setup for arbitrary metrical task systems (MTS) (e.g., caching,
k-server and convex body chasing) and online matching on the line. We utilize
results from the theory of online algorithms to show how to make the setup
robust. Specifically for caching, we present an algorithm whose performance, as
a function of the prediction error, is exponentially better than what is
achievable for general MTS. Finally, we present an empirical evaluation of our
methods on real world datasets, which suggests practicality.
</p></div>
    </summary>
    <updated>2020-03-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.02016</id>
    <link href="http://arxiv.org/abs/2003.02016" rel="alternate" type="text/html"/>
    <title>Time-Space Tradeoffs for Finding a Long Common Substring</title>
    <feedworld_mtime>1583366400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Stav Ben Nun, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golan:Shay.html">Shay Golan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kociumaka:Tomasz.html">Tomasz Kociumaka</a>, Matan Kraus <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.02016">PDF</a><br/><b>Abstract: </b>We consider the problem of finding, given two documents of total length $n$,
a longest string occurring as a substring of both documents. This problem,
known as the Longest Common Substring (LCS) problem, has a classic $O(n)$-time
solution dating back to the discovery of suffix trees (Weiner, 1973) and their
efficient construction for integer alphabets (Farach-Colton, 1997). However,
these solutions require $\Theta(n)$ space, which is prohibitive in many
applications. To address this issue, Starikovskaya and Vildh{\o}j (CPM 2013)
showed that for $n^{2/3} \le s \le n^{1-o(1)}$, the LCS problem can be solved
in $O(s)$ space and $O(\frac{n^2}{s})$ time. Kociumaka et al. (ESA 2014)
generalized this tradeoff to $1 \leq s \leq n$, thus providing a smooth
time-space tradeoff from constant to linear space. In this paper, we obtain a
significant speed-up for instances where the length $L$ of the sought LCS is
large. For $1 \leq s \leq n$, we show that the LCS problem can be solved in
$O(s)$ space and $\tilde{O}(\frac{n^2}{L\cdot s}+n)$ time. The result is based
on techniques originating from the LCS with Mismatches problem (Flouri et al.,
2015; Charalampopoulos et al., CPM 2018), on space-efficient locally consistent
parsing (Birenzwige et al., SODA 2020), and on the structure of maximal
repetitions (runs) in the input documents.
</p></div>
    </summary>
    <updated>2020-03-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.01937</id>
    <link href="http://arxiv.org/abs/2003.01937" rel="alternate" type="text/html"/>
    <title>The discrete optimization problems with interval objective function on graphs and hypergraphs and the interval greedy algorithm</title>
    <feedworld_mtime>1583366400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prolubnikov:Alexander.html">Alexander Prolubnikov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.01937">PDF</a><br/><b>Abstract: </b>We consider the discrete optimization problems with interval objective
function on graphs and hypergraphs. For the problems, we need to find either a
strong optimal solution or a set of all possible weak solutions. A strong
solution of the problem is a solution that is optimal for all possible values
of the objective function's coefficients that belong to predefined intervals. A
weak solution is a solution that is optimal for some of the possible values of
the coefficients. We characterize the strong solutions for considered problems.
We give a generalization of the greedy algorithm for the case of interval
objective function. For the discrete optimization problems we consider, the
algorithm gives a set of all possible greedy solutions and the set of all
possible values of the objective function for the solutions. For a given
probability distribution that is defined on coefficients' intervals, we compute
probabilities of the weak solutions, expected values of the objective function
for them, etc.
</p></div>
    </summary>
    <updated>2020-03-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.01902</id>
    <link href="http://arxiv.org/abs/2003.01902" rel="alternate" type="text/html"/>
    <title>Notes on Randomized Algorithms</title>
    <feedworld_mtime>1583366400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aspnes:James.html">James Aspnes</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.01902">PDF</a><br/><b>Abstract: </b>Lecture notes for the Yale Computer Science course CPSC 469/569 Randomized
Algorithms. Suitable for use as a supplementary text for an introductory
graduate or advanced undergraduate course on randomized algorithms. Discusses
tools from probability theory, including random variables and expectations,
union bound arguments, concentration bounds, applications of martingales and
Markov chains, and the Lov\'asz Local Lemma. Algorithmic topics include
analysis of classic randomized algorithms such as Quicksort and Hoare's FIND,
randomized tree data structures, hashing, Markov chain Monte Carlo sampling,
randomized approximate counting, derandomization, quantum computing, and some
examples of randomized distributed algorithms.
</p></div>
    </summary>
    <updated>2020-03-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.01900</id>
    <link href="http://arxiv.org/abs/2003.01900" rel="alternate" type="text/html"/>
    <title>Minimum Enclosing Parallelogram with Outliers</title>
    <feedworld_mtime>1583366400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guo:Zhengyang.html">Zhengyang Guo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yi.html">Yi Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.01900">PDF</a><br/><b>Abstract: </b>We study the problem of minimum enclosing parallelogram with outliers, which
asks to find, for a given set of $n$ planar points, a parallelogram with
minimum area that encloses at least $(n-t)$ points, where the remaining points
are regarded as outliers. We present an exact algorithm with $O(k^{2}t^{4} +
n^2\log n)$ runtime and $O(kt^{2})$ space, assuming that no three points lie on
the same line. Here $k=k(n,t)$ denotes the number of points on the first
$(t+1)$ convex layers. We further propose an sampling algorithm with runtime
$O(n+\mbox{poly}(\log{n}, t, 1/\epsilon))$, which with high probability finds a
parallelogram covering at least $(1-\epsilon)(n-t)$ and at most $(n-t+1)$
points with area no more than the exact optimal value.
</p></div>
    </summary>
    <updated>2020-03-05T23:25:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.01853</id>
    <link href="http://arxiv.org/abs/2003.01853" rel="alternate" type="text/html"/>
    <title>Hypergraph Motifs: Concepts, Algorithms, and Discoveries</title>
    <feedworld_mtime>1583366400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Geon.html">Geon Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Ko:Jihoon.html">Jihoon Ko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shin:Kijung.html">Kijung Shin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.01853">PDF</a><br/><b>Abstract: </b>Hypergraphs naturally represent group interactions, which are omnipresent in
many domains: collaborations of researchers, co-purchases of items, joint
interactions of proteins, to name a few. In this work, we propose tools for
answering the following questions in a systematic manner: (Q1) what are
structural design principles of real-world hypergraphs? (Q2) how can we compare
local structures of hypergraphs of different sizes? (Q3) how can we identify
domains which hypergraphs are from? We first define hypergraph motifs
(h-motifs), which describe the connectivity patterns of three connected
hyperedges. Then, we define the significance of each h-motif in a hypergraph as
its occurrences relative to those in properly randomized hypergraphs. Lastly,
we define the characteristic profile (CP) as the vector of the normalized
significance of every h-motif. Regarding Q1, we find that h-motifs' occurrences
in 11 real-world hypergraphs from 5 domains are clearly distinguished from
those of randomized hypergraphs. In addition, we demonstrate that CPs capture
local structural patterns unique in each domain, and thus comparing CPs of
hypergraphs addresses Q2 and Q3. Our algorithmic contribution is to propose
MoCHy, a family of parallel algorithms for counting h-motifs' occurrences in a
hypergraph. We theoretically analyze their speed and accuracy, and we show
empirically that the advanced approximate version MoCHy-A+ is up to 25X more
accurate and 32X faster than the basic approximate and exact versions,
respectively.
</p></div>
    </summary>
    <updated>2020-03-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4643</id>
    <link href="https://www.scottaaronson.com/blog/?p=4643" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4643#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4643" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">A coronavirus poem</title>
    <summary xml:lang="en-US">These next few months, every time I stop myself from touching my face by force of will, Let me remind myself that the same willpower is available to diet, to exercise, to throw myself into a project, to keep calm amid screaming, to introduce myself to strangers, to decrease the fraction of my life spent […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>These next few months, every time I stop myself from touching my face by force of will,</p>



<p>Let me remind myself that the same willpower is available to diet, to exercise, to throw myself into a project, to keep calm amid screaming, to introduce myself to strangers, to decrease the fraction of my life spent getting upset that someone was mean to my ingroup on social media, or otherwise to better myself as a human specimen.</p>



<p>Yea, let all of these things be just as easy for me as it was not to touch my face.</p>



<p>Ah, but what if I forget, what if I do keep touching my face in the next few months?</p>



<p>In one plausible scenario, with at least ~0.1% probability and probably higher depending on my age, a cheap answer will be available to that question: namely, that I’ll no longer be around to ponder the implications.</p></div>
    </content>
    <updated>2020-03-04T02:13:12Z</updated>
    <published>2020-03-04T02:13:12Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Embarrassing Myself"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Self-Help"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-03-05T23:54:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4635</id>
    <link href="https://www.scottaaronson.com/blog/?p=4635" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4635#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4635" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Paperz</title>
    <summary xml:lang="en-US">Soon, all anyone will want to talk about is quarantines, food shortages, N95 masks, the suspension of universities and of scientific conferences. (As many others have pointed out, this last might actually be a boon to scientific productivity—as it was for a young Isaac Newton when Cambridge was closed for the bubonic plague, so Newton […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Soon, all anyone will want to talk about is quarantines, food shortages, N95 masks, the suspension of universities and of scientific conferences.  (As many others have pointed out, this last might actually be a boon to scientific productivity—as it was for a young Isaac Newton when Cambridge was closed for the bubonic plague, so Newton went home and invented calculus and mechanics.)</p>



<p>Anyway, before that all happens, I figured I’d get in a last post about quantum information and complexity theory progress.</p>



<p>Hsin-Yuan Huang, Richard Kueng, and John Preskill have a nice preprint entitled <a href="https://arxiv.org/abs/2002.08953">Predicting Many Properties of a Quantum System from Very Few Measurements</a>.  In it they take <a href="https://arxiv.org/abs/1711.01053">shadow tomography</a>, which I proposed a couple years ago, and try to bring it closer to practicality on near-term devices, by restricting to the special case of non-adaptive, one-shot measurements, on separate copies of the state ρ that you’re trying to learn about.  They show that this is possible using a number of copies that depends logarithmically on the number of properties you’re trying to learn (the optimal dependence), not at all on the Hilbert space dimension, and linearly on a new “shadow norm” quantity that they introduce.</p>



<p>Rahul Ilango, Bruno Loff, and Igor Oliveira announced the pretty spectacular-sounding result that the <a href="https://eccc.weizmann.ac.il/report/2020/021/">Minimum Circuit Size Problem (MCSP) is NP-complete for multi-output functions</a>—that is, for Boolean functions f with not only many input bits but many outputs.  Given the 2<sup>n</sup>-sized truth table of a Boolean function f:{0,1}<sup>n</sup>→{0,1}, the original MCSP simply asks for the size of the smallest Boolean circuit that computes f.  This problem was studied in the USSR as early as the 1950s; whether it’s NP-complete has stood for decades as one of the big open problems of complexity theory.  We’ve known that if you could quickly solve MCSP then you could also invert any one-way function, but we’ve also known technical barriers to going beyond that to a flat-out NP-hardness result, at least via known routes.  Before seeing this paper, I’d never thought about whether MCSP for many-output functions might somehow be easier to classify, but apparently it is!</p>



<p>Hamoon Mousavi, Seyed Nezhadi, and Henry Yuen have now taken the MIP*=RE breakthrough even a tiny step further, by showing that <a href="https://arxiv.org/abs/2002.10490">“zero-gap MIP*”</a> (that is, quantum multi-prover interactive proofs with an arbitrarily small gap between the completeness and soundness probabilities) takes you <strong>even beyond the halting problem</strong> (i.e., beyond Recursively Enumerable or RE), and up to the second level of the arithmetical hierarchy (i.e., to the halting problem for Turing machines with oracles for the original halting problem).  This answers a question that someone asked <a href="https://www.scottaaronson.com/blog/?p=4512#comment-1829033">in the comments section of this blog</a>.</p>



<p>Several people asked me for comment on the paper <a href="https://arxiv.org/abs/2002.07730">What limits the simulation of quantum computers?</a>, by Yiqing Zhou, Miles Stoudenmire, and Xavier Waintal.  In particular, does this paper refute or weaken Google’s quantum supremacy claim, as the paper does <em>not</em> claim to do (but, rather coyly, also does not claim <em>not</em> to do)?  Short answer: No, it doesn’t, or not now anyway.</p>



<p>Longer, more technical answer: The quoted simulation times, just a few minutes for quantum circuits with 54 qubits and depth 20, <em>assume Controlled-Z gates rather than iSWAP-like gates.</em> Using tensor network methods, the classical simulation cost with the former is roughly the square root of the simulation cost with the latter (~2<sup>k</sup> versus ~4<sup>k</sup> for some parameter k related to the depth).  As it happens, Google switched its hardware from Controlled-Z to iSWAP-like gates a couple years ago precisely because they realized this—I had a conversation about it with Sergio Boixo at the time.  Once this issue is accounted for, the quoted simulation times in the new paper seem to be roughly in line with what was previously reported by, e.g., Johnnie Gray and Google itself.</p>



<p>Oh yeah, I enjoyed <a href="https://arxiv.org/abs/2002.09524">Quantum Homeopathy Works</a>.  Cool result, and the title is actually a pretty accurate description of the contents.</p>



<p>To end with a community announcement: as many of you might know, the American Physical Society’s March Meeting, which was planned for this week in Denver, was abruptly cancelled due to the coronavirus (leaving thousands of physicists out their flights and hotel rooms—many had even already arrived there).  However, my colleague Michael Biercuk kindly alerted me to a <a href="https://virtualmarchmeeting.com/">“virtual March Meeting”</a> that’s been set up online, with recorded talks and live webinars.  Even after the pandemic passes, is this a model that we should increasingly move to?  I wouldn’t have thought so ten or fifteen years ago, but today every schlep across the continent brings me a step closer to shouting “yes”…</p></div>
    </content>
    <updated>2020-03-03T18:03:12Z</updated>
    <published>2020-03-03T18:03:12Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-03-05T23:54:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16757</id>
    <link href="https://rjlipton.wordpress.com/2020/03/03/dyson-as-a-mathematician/" rel="alternate" type="text/html"/>
    <title>Dyson as a Mathematician</title>
    <summary>With a lemma from 1947 that might be useful today? Cropped from article on his letters Freeman Dyson passed away last February 28th, one day short of the leap day, February 29th. He was one of the great physicists, one of the great writers about science, and one of the great thinkers of all time. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>With a lemma from 1947 that might be useful today?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/03/dysonyoung.jpg"><img alt="" class="alignright wp-image-16759" height="220" src="https://rjlipton.files.wordpress.com/2020/03/dysonyoung.jpg?w=148&amp;h=220" width="148"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="https://www.nytimes.com/2018/04/17/books/review/freeman-dyson-maker-of-patterns.html">article</a> on his letters</font></td>
</tr>
</tbody>
</table>
<p>
Freeman Dyson passed away last February 28th, one day short of the leap day, February 29th. He was one of the great physicists, one of the great writers about science, and one of the great thinkers of all time. He is missed.</p>
<p>
Today we wish to discuss a tiny part of Dyson’s contributions to mathematics—and ask whether it has been developed further. </p>
<p>
Dyson did so much in so many areas of science that we will leave it to others to discuss it. We covered a puzzle of his years ago <a href="https://rjlipton.wordpress.com/2014/09/09/a-challenge-from-dyson/">here</a>. He famously showed that quantum electrodynamics (QED) is a consistent theory—in particular showing that different theories connecting quantum mechanics and special relativity were the same. He published many interesting books about science in general. He speculated about aliens, about space travel, and much more.</p>
<p>
Our focus is on a beautiful result of his proved in 1947. A result about rational numbers. Nothing grandiose, nothing about infinite visions, nothing about worlds that we cannot easily imagine. </p>
<p>
</p><p/><h2> Approximations To Numbers </h2><p/>
<p/><p>
For over 2000 years we have known that <img alt="{\sqrt{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{2}}"/> is not expressible as a rational fraction. This is sometimes credited to Hippasus of <a href="https://en.wikipedia.org/wiki/Hippasus">Metapontum</a>. The obvious question, at least to math types, is how close can we make it to a rational number? The answer is interesting, with many consequences. And includes some top mathematicians such as Johann Dirichlet, Joseph Liouville, Axel Thue, Carl Siegel, Dyson, and Klaus Roth.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b>Hippasus:</b> The value <img alt="{\sqrt{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{2}}"/> is not rational.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b>Folklore:</b> The value <img alt="{\sqrt{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{2}}"/> like all algebraic numbers <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> can be algorithmically approximated by rationals. Note that <img alt="{\sqrt{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{2}}"/> is algebraic since it satisfies the equation 	</p>
<p align="center"><img alt="\displaystyle  x^{2} - 2 = 0, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7B2%7D+-+2+%3D+0%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^{2} - 2 = 0, "/></p>
<p>with <img alt="{D=2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D=2}"/>. The number <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is called the <em>degree</em> of <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/>.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b>Dirichlet:</b> The value <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> can be well approximated. That is 	</p>
<p align="center"><img alt="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{1}{q^{2}}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%7C+%5Calpha+-+%5Cfrac%7Bp%7D%7Bq%7D+%5Cright%7C+%5Cle+%5Cfrac%7B1%7D%7Bq%5E%7B2%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{1}{q^{2}}, "/></p>
<p>can be done for infinitely many <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> and <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/>.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b>Liouville:</b> The value <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> cannot be too well approximated. That is 	</p>
<p align="center"><img alt="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{c}{q^{v}}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%7C+%5Calpha+-+%5Cfrac%7Bp%7D%7Bq%7D+%5Cright%7C+%5Cle+%5Cfrac%7Bc%7D%7Bq%5E%7Bv%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{c}{q^{v}}, "/></p>
<p>cannot be done for infinitely many <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> and <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/> and some constant <img alt="{c &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c &gt; 0}"/> provided <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> is larger than <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/>.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b>Thue:</b> The value <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> cannot be too well approximated. That is 	</p>
<p align="center"><img alt="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{c}{q^{v}}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%7C+%5Calpha+-+%5Cfrac%7Bp%7D%7Bq%7D+%5Cright%7C+%5Cle+%5Cfrac%7Bc%7D%7Bq%5E%7Bv%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{c}{q^{v}}, "/></p>
<p>cannot be done for infinitely many <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> and <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/> and some constant <img alt="{c &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c &gt; 0}"/> provided <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> is larger than <img alt="{D/2+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%2F2%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D/2+1}"/>.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b>Siegel:</b> The value <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> cannot be too well approximated. That is 	</p>
<p align="center"><img alt="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{c}{q^{v}}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%7C+%5Calpha+-+%5Cfrac%7Bp%7D%7Bq%7D+%5Cright%7C+%5Cle+%5Cfrac%7Bc%7D%7Bq%5E%7Bv%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{c}{q^{v}}, "/></p>
<p>cannot be done for infinitely many <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> and <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/> for some constant <img alt="{c&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c&gt;0}"/> provided <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> is larger than <img alt="{2\sqrt{D}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Csqrt%7BD%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\sqrt{D}}"/>.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b>Dyson:</b> The value <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> cannot be too well approximated. That is 	</p>
<p align="center"><img alt="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{c}{q^{v}}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%7C+%5Calpha+-+%5Cfrac%7Bp%7D%7Bq%7D+%5Cright%7C+%5Cle+%5Cfrac%7Bc%7D%7Bq%5E%7Bv%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{c}{q^{v}}, "/></p>
<p>cannot be done for infinitely many <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> and <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/> for some constant <img alt="{c &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c &gt; 0}"/> provided <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> is larger than <img alt="{\sqrt{2D}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B2D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{2D}}"/>.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b>Roth:</b> The value <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> cannot be too well approximated. That is 	</p>
<p align="center"><img alt="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{1}{q^{v}}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%7C+%5Calpha+-+%5Cfrac%7Bp%7D%7Bq%7D+%5Cright%7C+%5Cle+%5Cfrac%7B1%7D%7Bq%5E%7Bv%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left| \alpha - \frac{p}{q} \right| \le \frac{1}{q^{v}}, "/></p>
<p>cannot be done for infinitely many <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> and <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/> provided <img alt="{D \geq 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD+%5Cgeq+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D \geq 2}"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> is larger than <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>.</p>
<p>
Note that the last result is best possible in the sense that Dirichlet achieved <img alt="{v = D = 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv+%3D+D+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v = D = 2}"/>, although slightly stronger statements than Roth can be made by using logarithms.</p>
<p>
To illustrate these formulas, Liouville proved that the following number is transcendental:</p>
<p/><p align="center"><img alt="\displaystyle  \begin{array}{rcl}  \sum_{k=1}^{\infty} \frac{1}{10^{k!}} &amp;=&amp; 0.110001000000000000000001000000000000000000000000000000000000\\  &amp;&amp; 00000000000000000000000000000000000000000000000000000000001\dots \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Csum_%7Bk%3D1%7D%5E%7B%5Cinfty%7D+%5Cfrac%7B1%7D%7B10%5E%7Bk%21%7D%7D+%26%3D%26+0.110001000000000000000001000000000000000000000000000000000000%5C%5C+%09%26%26+00000000000000000000000000000000000000000000000000000000001%5Cdots+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}  \sum_{k=1}^{\infty} \frac{1}{10^{k!}} &amp;=&amp; 0.110001000000000000000001000000000000000000000000000000000000\\  &amp;&amp; 00000000000000000000000000000000000000000000000000000000001\dots \end{array} "/></p>
<p>
Note that the exponent in Liouville’s number grows as <img alt="{k! \approx \exp(k\log k)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%21+%5Capprox+%5Cexp%28k%5Clog+k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k! \approx \exp(k\log k)}"/>. The later formulas improve this to simply exponential in <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>. For instance, </p>
<p align="center"><img alt="\displaystyle  \sum_{k=1}^{\infty}\frac{1}{10^{2^k}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bk%3D1%7D%5E%7B%5Cinfty%7D%5Cfrac%7B1%7D%7B10%5E%7B2%5Ek%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{k=1}^{\infty}\frac{1}{10^{2^k}} "/></p>
<p>is transcendental. We’ll leave it to our readers to figure out which formula gives this consequence and will give the historical answer at the end. </p>
<p/><h2> A Key Lemma </h2><p/>
<p/><p>
What we wish to highlight from Dyson’s 1947 <a href="https://projecteuclid.org/euclid.acta/1485888462">paper</a> is the main lemma for his proof. He called it the key new idea in his paper and also said it might have independent interest. We agree, and we abstract some of its statement into the following definitions. </p>
<p>
Suppose we have a polynomial <img alt="{f(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x,y)}"/>. Call a point <img alt="{(x_0,y_0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x_0%2Cy_0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(x_0,y_0)}"/> a <em>zero of order <img alt="{(p,q)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28p%2Cq%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(p,q)}"/></em> if <img alt="{0 \leq m \leq p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%5Cleq+m+%5Cleq+p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0 \leq m \leq p}"/> and <img alt="{0 \leq n \leq q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%5Cleq+n+%5Cleq+q%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0 \leq n \leq q}"/> implies </p>
<p align="center"><img alt="\displaystyle  \frac{\partial^m}{\partial x} \frac{\partial^n}{\partial y}f(x = x_0, y = y_0) = 0. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5Cpartial%5Em%7D%7B%5Cpartial+x%7D+%5Cfrac%7B%5Cpartial%5En%7D%7B%5Cpartial+y%7Df%28x+%3D+x_0%2C+y+%3D+y_0%29+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{\partial^m}{\partial x} \frac{\partial^n}{\partial y}f(x = x_0, y = y_0) = 0. "/></p>
<p>Since this includes the case <img alt="{m = n = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+n+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m = n = 0}"/>, the point <img alt="{(x_0,y_0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x_0%2Cy_0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(x_0,y_0)}"/> must be a zero of <img alt="{f(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x,y)}"/> as well as of all the partial derivatives. For example, when </p>
<p align="center"><img alt="\displaystyle  f(x,y) = xy - x - y + 1, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%2Cy%29+%3D+xy+-+x+-+y+%2B+1%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(x,y) = xy - x - y + 1, "/></p>
<p>the point <img alt="{(1,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1,1)}"/> is a zero of order <img alt="{(1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1,0)}"/> and of order <img alt="{(0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(0,1)}"/> but not of order <img alt="{(1,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1,1)}"/>. The diagonal-step nature of this example plays into the next definition. We follow Dyson in numbering from zero.</p>
<blockquote><p><b>Definition 1</b> <em> A “<img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\lambda}"/>-staircase” for <img alt="{f(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f(x,y)}"/> is given by points <img alt="{(x_0,y_0),\dots,(x_r,y_r)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x_0%2Cy_0%29%2C%5Cdots%2C%28x_r%2Cy_r%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{(x_0,y_0),\dots,(x_r,y_r)}"/> and nonnegative reals <img alt="{t_0,\dots,t_r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt_0%2C%5Cdots%2Ct_r%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{t_0,\dots,t_r}"/> such that for all <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i}"/> and <img alt="{n \leq t_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cleq+t_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n \leq t_i}"/>, </em></p><em>
<p align="center"><img alt="\displaystyle  (x_i,y_i) \quad\text{is a zero of order} \quad (\lfloor\lambda(t_i - n)\rfloor, n). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x_i%2Cy_i%29+%5Cquad%5Ctext%7Bis+a+zero+of+order%7D+%5Cquad+%28%5Clfloor%5Clambda%28t_i+-+n%29%5Crfloor%2C+n%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  (x_i,y_i) \quad\text{is a zero of order} \quad (\lfloor\lambda(t_i - n)\rfloor, n). "/></p>
</em><p><em>It is understood that the <img alt="{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x_i}"/> are distinct and the <img alt="{y_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{y_i}"/> are distinct, but they need not be distinct from each other. </em>
</p></blockquote>
<p/><p>
The parameter <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> is the steepness of the staircase. The idea is to make the staircase quite steep by taking both <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> and the degree <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> to be large. This is controlled by a parameter <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> that is inverse to <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> and chosen to meet the hypotheses of the key lemma:</p>
<blockquote><p><b>Lemma 2</b> <em><a name="key"/> Suppose <img alt="{(x_0,y_0),\dots,(x_r,y_r)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x_0%2Cy_0%29%2C%5Cdots%2C%28x_r%2Cy_r%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{(x_0,y_0),\dots,(x_r,y_r)}"/> and <img alt="{t_0,\dots,t_r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt_0%2C%5Cdots%2Ct_r%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{t_0,\dots,t_r}"/> constitute a <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\lambda}"/>-staircase for a bivariate polynomial <img alt="{f(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f(x,y)}"/> of degree <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{u}"/> in <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x}"/> and degree <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{s}"/> in <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{y}"/>, where for each <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i}"/>, <img alt="{0 \leq i \leq r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%5Cleq+i+%5Cleq+r%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{0 \leq i \leq r}"/>, </em></p><em>
<p align="center"><img alt="\displaystyle  \lfloor t_i \rfloor \leq \min\{s, \frac{u+1}{\lambda} - 1\}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clfloor+t_i+%5Crfloor+%5Cleq+%5Cmin%5C%7Bs%2C+%5Cfrac%7Bu%2B1%7D%7B%5Clambda%7D+-+1%5C%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \lfloor t_i \rfloor \leq \min\{s, \frac{u+1}{\lambda} - 1\}. "/></p>
<p>Suppose <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\delta}"/> is a positive real number such that: </p>
<p align="center"><img alt="\displaystyle  \lambda &gt; \frac{2}{\delta}, \quad \text{and}\quad \delta \geq \frac{2s}{r(u+1)}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda+%3E+%5Cfrac%7B2%7D%7B%5Cdelta%7D%2C+%5Cquad+%5Ctext%7Band%7D%5Cquad+%5Cdelta+%5Cgeq+%5Cfrac%7B2s%7D%7Br%28u%2B1%29%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \lambda &gt; \frac{2}{\delta}, \quad \text{and}\quad \delta \geq \frac{2s}{r(u+1)}. "/></p>
<p>Then <a name="bound"/></p><a name="bound">
<p align="center"><img alt="\displaystyle  \lambda \sum_{i=0}^r (1 + \lfloor t_i \rfloor)(t_i - \frac{1}{2}\lfloor t_i \rfloor) \leq (s+1)(u+1)(1 + \frac{1}{2}r(r+1)\delta). \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda+%5Csum_%7Bi%3D0%7D%5Er+%281+%2B+%5Clfloor+t_i+%5Crfloor%29%28t_i+-+%5Cfrac%7B1%7D%7B2%7D%5Clfloor+t_i+%5Crfloor%29+%5Cleq+%28s%2B1%29%28u%2B1%29%281+%2B+%5Cfrac%7B1%7D%7B2%7Dr%28r%2B1%29%5Cdelta%29.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \lambda \sum_{i=0}^r (1 + \lfloor t_i \rfloor)(t_i - \frac{1}{2}\lfloor t_i \rfloor) \leq (s+1)(u+1)(1 + \frac{1}{2}r(r+1)\delta). \ \ \ \ \ (1)"/></p>
</a></em><p><em><a name="bound"/> </em>
</p></blockquote>
<p>
</p><p/><h2> Interpretation and Application </h2><p/>
<p/><p>
Dyson supplied the following interpretation in his paper. The left-hand side of (<a href="https://rjlipton.wordpress.com/feed/#bound">1</a>) approximately counts the number of zeroes in the staircase—approximately because <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> is left as a real number. We want a good upper bound on this number</p>
<p>
A polynomial <img alt="{f(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x,y)}"/> of degree <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> in <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> in <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> may have up to <img alt="{(u+1)(s+1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28u%2B1%29%28s%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(u+1)(s+1)}"/> non-zero coefficieints. Thus <img alt="{(u+1)(s+1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28u%2B1%29%28s%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(u+1)(s+1)}"/> is intuitively the maximum number of zeroes that could be arranged “on purpose” by the choice of <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/>. </p>
<p>
We want to minimize the number af additional zeroes that could exist. Lemma <a href="https://rjlipton.wordpress.com/feed/#key">2</a> says that this is limited by the factor <img alt="{(1 + \frac{1}{2}r(r+1)\delta)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281+%2B+%5Cfrac%7B1%7D%7B2%7Dr%28r%2B1%29%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1 + \frac{1}{2}r(r+1)\delta)}"/>. We want to make this factor approach <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. We can do so by making <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> arbitrarily small. By the condition <img alt="{\lambda &gt; \frac{2}{\delta}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda+%3E+%5Cfrac%7B2%7D%7B%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda &gt; \frac{2}{\delta}}"/> this entails choosing <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> large. The other requirement to chooise <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> is that it must majorize </p>
<p align="center"><img alt="\displaystyle  \frac{2s}{r(u+1)}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2s%7D%7Br%28u%2B1%29%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{2s}{r(u+1)}. "/></p>
<p>We need <img alt="{\delta r^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+r%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta r^2}"/> to be small yet bigger than </p>
<p align="center"><img alt="\displaystyle  \frac{2sr}{u+1}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2sr%7D%7Bu%2B1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{2sr}{u+1}. "/></p>
<p>This means making <img alt="{\frac{u}{s} &gt; 2r - \frac{1}{s}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bu%7D%7Bs%7D+%3E+2r+-+%5Cfrac%7B1%7D%7Bs%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{u}{s} &gt; 2r - \frac{1}{s}}"/>. For whatever number <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> of points we are concerned with, we can meet this by making the degree <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> in <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> be larger than the degree <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> in <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> by the factor <img alt="{2r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2r}"/>. That is all we need to do for <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> and <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> to exist that will allow us to apply the lemma. </p>
<p>
Thus the upshot is that suitably “lopsided” polynomials, together with a connected region of their partial derivatives, cannot have too many more than the prescribed number of zeroes, counting multiplicity of the orders of the zeroes. The proof of the lemma works by successive applications of reasoning about determinants and linear independence of polynomials that serve as components of <img alt="{f(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x,y)}"/>. It is fairly long-winded and ends with some painstaking estimates. </p>
<p>
The application supposes for sake of contradiction that there are infinitely many fractions <img alt="{\frac{p}{q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bp%7D%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{p}{q}}"/> giving closer approximations to <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> than the theorem statement allows. It suffices to consider the case where <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> is an algebraic integer—that is, a root of a univariate polynomial <img alt="{g(z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(z)}"/> with integer coefficients. The supposition yields candidate polynomials <img alt="{f(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x,y)}"/> as differences of two polynomials, one derived from <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> and the other from the closely approximating fractions. The <img alt="{f(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x,y)}"/> have both a large staircase of zeroes and a bounded space of possible coefficients, which imposes constraints on the degrees in <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>. These elements can be manipulated, using the approximating fractions on one hand and the fact of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> (and hence its degree) being fixed on the other hand, to create the lopsided form of <img alt="{f(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x,y)}"/> in which the hypotheses of lemma <a href="https://rjlipton.wordpress.com/feed/#key">2</a> take effect. The lemma then cranks out a contradiction.</p>
<p>
At the end of his proof, Dyson deftly explains how taking <img alt="{t = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t = 1}"/>, where <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> results from the process that selects the sequence <img alt="{t_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t_i}"/> for the lemma, yields Siegel’s theorem. Thus his framework affords an extra <a href="https://rjlipton.wordpress.com/2011/08/05/give-me-a-lever/">lever</a> for manipulating ratios, in his case the ratio <img alt="{\frac{s}{t}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bs%7D%7Bt%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{s}{t}}"/>. Tending the ratio to <img alt="{\sqrt{r/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Br%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{r/2}}"/> rather than to <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> produces his theorem.</p>
<p>
Our point of interest is whether Dyson’s setup can be used to attack other questions about polynomials in complexity theory. We have not yet formed an understanding of how specific it is to the approximation application. Perhaps for complexity we would want to generalize it from polynomials in 2 to <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> variables. There could be some relation to the <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d}"/>-variable techniques used for integer multiplication as we covered <a href="https://rjlipton.wordpress.com/2019/03/29/integer-multiplication-in-nlogn-time/">here</a>, but again we’re just at the point of asking. Dyson ends his paper with the opinion that</p>
<blockquote><p><b> </b> <em> “such an investigation … would not be in any way a hopeless undertaking.” </em>
</p></blockquote>
<p>
</p><p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Have there been useful extensions of Dyson’s lemma, as opposed to improvements by Roth and others to his approximation bounds? Has Dyson’s “non-hopeless investigation” been brought to fruition? What other applications would it have? What are the closest techniques that have been used in complexity theory?</p>
<p>
The search for proving transcendental numbers has gone in other directions. These are represented in a survey <a href="http://www.its.caltech.edu/~matilde/NumberTheoryFormalLanguages.pdf">paper</a> by Jeffrey Shallit, which grew into the <a href="https://cs.uwaterloo.ca/~shallit/asas.html">book</a> <em>Automatic Sequences</em> with Jean-Paul Allouche. To answer our reader question above, in the book they trace the transcendence of <img alt="{\sum_k 10^{-2^k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_k+10%5E%7B-2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_k 10^{-2^k}}"/> to a 1916 <a href="https://www.ams.org/journals/tran/1916-017-04/S0002-9947-1916-1501054-4/S0002-9947-1916-1501054-4.pdf">paper</a> by Aubrey Kempner which uses a different technique.</p>
<p>[Edit: Fixed Roth bound on degree.]</p></font></font></div>
    </content>
    <updated>2020-03-03T17:33:10Z</updated>
    <published>2020-03-03T17:33:10Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Results"/>
    <category term="trick"/>
    <category term="algebraic numbers"/>
    <category term="approximation"/>
    <category term="Freeman Dyson"/>
    <category term="in memoriam"/>
    <category term="polynomials"/>
    <category term="transcendental"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-03-06T05:20:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=736</id>
    <link href="https://emanueleviola.wordpress.com/2020/03/02/conferences-in-an-era-of-expensive-carbon/" rel="alternate" type="text/html"/>
    <title>Conferences in an Era of Expensive Carbon</title>
    <summary>At least there’s that: I live in a world where some people care about it and publish their viewpoint in the latest CACM. Read it on your next flight. Some interesting things that won’t shock anyone: There’s a nice picture with different environmental costs based on the location of the conference. It also shows that […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>At least there’s that: I live in a world where some people care about it and publish their <a href="https://cacm.acm.org/magazines/2020/3/243024-conferences-in-an-era-of-expensive-carbon/abstract">viewpoint in the latest CACM</a>. Read it on your next flight.  Some interesting things that won’t shock anyone:</p>



<p>There’s a nice picture with different environmental costs based on the location of the conference.  It also shows that people like to go to nearby conferences, one of the reasons why “The impulse to ignore the issue is entirely understandable.”  For more perspective see some of our earlier posts for example <a href="https://emanueleviola.wordpress.com/2020/02/18/working-remotely-will-be-the-most-significant-transformation-since-agriculture/">here</a> and <a href="https://emanueleviola.wordpress.com/2020/01/01/publish-and-perish/">here</a>.</p>



<p>The viewpoint also reports on a recent switch from in-person to online program committees for flagship conferences (POPL and ICFP), following a recent trend.  For starters we continue to suggest that <a href="https://emanueleviola.wordpress.com/2017/07/28/stocfocs-pc-meetings-does-nature-of-decisions-justify-cost/">STOC and FOCS do the same, because the nature of decisions does not justify the cost.</a> The latter post also includes hard numbers on the added value of a physical meeting (with respect to accept/reject decisions — of course one can value at infinity meeting in person luminaries in your field, but that can be done in other ways and should not be tied to PC meetings).</p>



<p/></div>
    </content>
    <updated>2020-03-02T15:44:29Z</updated>
    <published>2020-03-02T15:44:29Z</published>
    <category term="Uncategorized"/>
    <category term="Utopia"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2020-03-06T05:21:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4788940302272898721</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4788940302272898721/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/03/logic-examples-for-your-discrete-math.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4788940302272898721" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4788940302272898721" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/03/logic-examples-for-your-discrete-math.html" rel="alternate" type="text/html"/>
    <title>Logic examples for your Discrete Math class</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>
<br/>
<br/>
(I injured my hand about a month ago so I have had a hard time typing. That is why<br/>
I have not blogged for a while. I'm better now but still slow. This is a post I prepared<br/>
a while back.)<br/>
<br/>
<br/>
<br/>
Here are some examples of English and logic for your discrete math class. Or for mine anyway.<br/>
<br/>
<br/></div>
<div>
1) <i>A computer programmer leaves work and heads for home. Being the good spouse that he is, he calls  his partner and asks if there's anything that needs to be picked up on the way.</i></div>
<div>
<i><br/></i></div>
<div>
<i>Yes, a gallon of milk and, oh, if they have <span class="il">eggs</span>, get a dozen.</i></div>
<div>
<i><br/></i></div>
<div>
<i>Later he arrives home and stumbles into the kitchen burdened with a dozen gallons of milk. His partner  perplexed, asks him ``why in the world did you buy 12 gallons of milk?''</i></div>
<div>
<i><br/></i></div>
<div>
<i>What did he answer?</i></div>
<div>
<br/></div>
<div>
When I told this to my class one student said that he should answer:</div>
<div>
<br/>
                                                      <i>I love you too Darling</i></div>
<div>
<br/></div>
<div>
while that is always a good thing to tell Darling, it is not the answer I had in mind.<br/>
<br/></div>
<div>
The  answer is  <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/eggs.txt">here</a>.</div>
<div>
<br/></div>
<div>
2) I saw a headline:</div>
<div>
<br/></div>
<div>
                                                 <i> Rise in faux-incest porn alarming</i></div>
<div>
<br/></div>
<div>
Give two different interpretations of this sentence. (Note- One you might agree with, the other you will likely disagree with.)<br/>
<br/>
My answer is  <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/faux.txt">here</a></div>
<div>
<br/>
3) Recently someone was describing what I work on to someone else and he said the following wonderfully ambiguous sentence<br/>
<br/>
                           <i>Bill works on puzzles and games. He also work on cake cutting, to be fair.</i><br/>
<br/>
Give two different interpretations of this sentence.  My answer is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/billcake.txt">here</a><br/>
<br/>
<br/>
4) A common saying is<br/>
<br/>
                          <i> All that glitters is not gold</i><br/>
<br/>
What does this mean literally? What did they really mean to say? My answer is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/gold.txt">here.</a><br/>
<br/>
(I had originally thought this was a quote from the Led Zeppelin song <i>Stairway to Heaven</i>;<br/>
however, an astute reader left a comment reminding me that, in that song, they actually<br/>
say that there is a lady who believes <i>All that Glitters is Gold. </i>The song implies that she is incorrect, so really<br/>
NOT(All that Glitters is Gold) which means (exists x)[x glitters but x is not gold] which actually<br/>
IS what they meant to say. Yeah!)<br/>
<br/>
5)When the chess player Bobby Fisher died I saw in one article about him the sentence<br/>
<i><br/></i>
<i>                                Bobby Fisher was a terrible anti-semite.</i><br/>
<br/>
<i/>This can be interpreted two ways. What are they? Which one did the writer probably mean? My answer is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/fisher.txt">here</a><br/>
<br/>
6) When Donald Trump broke the Nuclear Treaty with Iran he said<br/>
<br/>
                             <i> Iran is the worse enabler of terrorist in the mideast</i><br/>
<br/>
This can be interpreted two ways. What are they? Which one did Trump mean? My answer is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/iran.txt">here.</a><br/>
<br/></div>
<div>
<br/>
7) I saw the headline (see <a href="https://www.dailykos.com/stories/2019/12/31/1905544/-There-was-actually-good-news-in-the-War-on-Women-in-2019-news-we-have-to-build-on-in-2020">here</a>)<br/>
<br/>
<i>There was actually good news in the War on Women in 2019, news we have to build on in 2020</i>.<br/>
<br/>
This can be interepreted in two ways. This one I leave to you, or read the article.<br/>
<br/>
<br/>
<br/>
<br/></div></div>
    </content>
    <updated>2020-03-02T15:32:00Z</updated>
    <published>2020-03-02T15:32:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-03-06T04:48:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/03/02/prague-summer-school-on-discrete-mathematics/</id>
    <link href="https://cstheory-events.org/2020/03/02/prague-summer-school-on-discrete-mathematics/" rel="alternate" type="text/html"/>
    <title>Prague Summer School on Discrete Mathematics</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">August 24-28, 2020 Prague, Czech Republic http://pssdm.math.cas.cz/ Registration deadline: March 22, 2020 The third edition of Prague Summer School on Discrete Mathematics will feature two lecture series: Subhash Khot (New York University): Hardness of Approximation: From the PCP Theorem to the 2-to-2 Games Theorem, and Shayan Oveis Gharan (University of Washington): Polynomial Paradigm in Algorithm … <a class="more-link" href="https://cstheory-events.org/2020/03/02/prague-summer-school-on-discrete-mathematics/">Continue reading <span class="screen-reader-text">Prague Summer School on Discrete Mathematics</span></a></div>
    </summary>
    <updated>2020-03-02T14:40:44Z</updated>
    <published>2020-03-02T14:40:44Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-03-06T05:21:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=2386</id>
    <link href="https://francisbach.com/richardson-extrapolation/" rel="alternate" type="text/html"/>
    <title>On the unreasonable effectiveness of Richardson extrapolation</title>
    <summary>This month, I will follow up on last month’s blog post, and describe classical techniques from numerical analysis that aim at accelerating the convergence of a vector sequence to its limit, by only combining elements of the sequence, and without the detailed knowledge of the iterative process that has led to this sequence. Last month,...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">This month, I will follow up on <a href="https://francisbach.com/acceleration-without-pain/">last month’s blog post</a>, and describe classical techniques from numerical analysis that aim at accelerating the convergence of a vector sequence to its limit, by only combining elements of the sequence, and without the detailed knowledge of the iterative process that has led to this sequence. </p>



<p class="justify-text">Last month, I focused on sequences that converge to their limit exponentially fast (which is referred to as <em>linear</em> convergence), and I described <a href="https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process">Aitken’s \(\Delta^2\) method</a>, the <a href="https://en.wikipedia.org/wiki/Shanks_transformation">Shanks transformation</a>, Anderson acceleration and its <a href="https://arxiv.org/pdf/1606.04133">regularized version</a>. These methods are called “non-linear” acceleration techniques, because, although they combine linearly iterates as \(c_0 x_k + c_1 x_{k+1} + \cdots + c_m x_{k+m}\), the scalar weights in the linear combination depend non-linearly on \(x_k,\dots,x_{k+m}\).</p>



<p class="justify-text">In this post, I will focus on sequences that converge sublinearly, that is, with a difference to their limit that goes to zero as an inverse power of \(k\), typically in \(O(1/k)\). </p>



<h2>Richardson extrapolation</h2>



<p class="justify-text">We consider a sequence \((x_k)_{k \geq 0} \in \mathbb{R}^d\), with an asymptotic expansion of the form $$ x_k = x_\ast + \frac{1}{k}\Delta + O\Big(\frac{1}{k^2}\Big), $$ where \(x_\ast \in \mathbb{R}^d\) is the limit of \((x_k)_k\) and \(\Delta\) a vector in \(\mathbb{R}^d\).</p>



<p class="justify-text">The idea behind <a href="https://en.wikipedia.org/wiki/Richardson_extrapolation">Richardson extrapolation</a> [<a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1911.0009">1</a>] is to combine linearly two iterates taken at two different values of \(k\) so that the zero-th order term \(x_\ast\) is left unchanged, but the first order term in \(1/k\) cancels out. For \(k\) even, we can consider $$  2 x_k – x_{k/2} =  2 \Big( x_\ast + \frac{1}{k} \Delta  +O\Big(\frac{1}{k^2}\Big)  \Big) \, – \Big( x_\ast +  \frac{2}{k} \Delta  + O\Big(\frac{1}{k^2}\Big) \Big)  =  x_\ast +O\Big(\frac{1}{k^2}\Big).$$</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><a href="https://arxiv.org/pdf/1707.06386"><img alt="" class="wp-image-2421" height="179" src="https://francisbach.com/wp-content/uploads/2020/02/reg_k-1024x454.png" width="404"/></a>Illustration of Richardson extrapolation. Iterates (in black) with their first-order expansions (in red). The deviations (represented by circles) are of order \(O(1/k^2)\). Adapted from [<a href="https://arxiv.org/pdf/1707.06386">3</a>, <a href="https://arxiv.org/pdf/2002.02835">2</a>].  </figure></div>



<p class="justify-text">The key benefit of Richardson extrapolation is that we only need to know that the leading term in the asymptotic expansion is proportional to \(1/k\), <em>without the need to know the vector \(\Delta\)</em>. See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2481" height="280" src="https://francisbach.com/wp-content/uploads/2020/02/richardson_2d.gif" width="332"/>Richardson extrapolation in two dimensions. The sequence is of the form \(x_k = \frac{1}{k} \Delta_1 + \frac{(-1)^k}{k^2} \Delta_2\). The extrapolated sequence \(2 x_k – x_{k/2}\) is only plotted for \(k\) even.</figure></div>



<p class="justify-text">In this post, following [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>], I will explore situations where Richardson extrapolation can be useful within machine learning. I identified three situations where Richardson extrapolation can be useful (there are probably more):</p>



<ol class="justify-text"><li>Iterates of an optimization algorithms \((x_k)_{k \geq 0}\), and the extrapolation is \( 2x_k – x_{k/2}.\)</li><li>Extrapolation on the step-size of stochastic gradient descent, where we will combine iterates obtained from two different values of the step-size.</li><li>Extrapolation on a regularization parameter.</li></ol>



<p class="justify-text">As we will show, extrapolation techniques come with no significant loss in performance, but in several situations strong gains. It is thus “<a href="https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences">unreasonably effective</a>“.</p>



<h2>Application to optimization algorithms</h2>



<p class="justify-text">We consider an iterate \(x_k\) of an iterative optimization algorithm which is minimizing a function \(f\), thus converging to a global minimizer \(x_\ast\) of \(f\). Then so is \(x_{k/2}\), and thus also $$  x_k^{(1)} = 2x_k – x_{k/2}.$$ Therefore, performance is never significantly deteriorated (the risk is essentially to lose half of the iterations). The potential gains depend on the way \(x_k\) converges to \(x_\ast\). The existence of a convergence rate of the form \(f(x_k) -f(x_\ast) = O(1/k)\) or \(O(1/k^2)\) is not enough, as Richardson extrapolation requires a specific direction of asymptotic convergence. As illustrated below, some algorithms are oscillating around their solutions, while some converge with a specific direction. Only the latter ones can be accelerated with Richardson extrapolation, while the former ones are good candidates for <a href="https://francisbach.com/acceleration-without-pain/">Anderson acceleration</a>.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2395" height="162" src="https://francisbach.com/wp-content/uploads/2020/02/nonoscillating_oscillating-1024x350.png" width="476"/> Left: Oscillating convergence, where Richardson extrapolation does not lead to any gain. Right: non-oscillating  convergence, with a main direction \(\Delta\) (in red dotted), where Richardson extrapolation can be beneficial if the oscillations orthogonal to the direction \(\Delta\) are negligible compared to convergence along the direction \(\Delta\). </figure></div>



<p class="justify-text"><strong>Averaged gradient descent.</strong> We consider the usual gradient descent algorithm $$x_k = x_{k-1} – \gamma f'(x_{k-1}),$$ where \(\gamma &gt; 0 \) is a step-size, with Polyak-Ruppert averaging [<a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">4</a>]: $$ y_k = \frac{1}{k} \sum_{i=0}^{k-1} x_i.$$ Averaging is key to robustness to potential noise in the gradients [<a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">4</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">5</a>]. However it comes with the unintended consequence of losing the exponential forgetting of initial conditions for strongly-convex problems [<a href="https://papers.nips.cc/paper/4316-non-asymptotic-analysis-of-stochastic-approximation-algorithms-for-machine-learning.pdf">6</a>].</p>



<p class="justify-text">A common way to restore exponential convergence (up to the noise level in the stochastic case) is to consider “tail-averaging”, that is, to replace \(y_k\) by the average of only the latest \(k/2\) iterates [<a href="http://jmlr.org/papers/volume18/16-595/16-595.pdf">7</a>]. As shown below for \(k\) even, this corresponds exactly to Richardson extrapolation on the sequence \((y_k)_k\): $$ \frac{2}{k} \sum_{i=k/2}^{k-1} x_i = \frac{2}{k} \sum_{i=0}^{k-1} x_i – \frac{2}{k} \sum_{i=0}^{k/2-1} x_i = 2 y_k – y_{k/2}. $$</p>



<p class="justify-text">With basic  assumptions on \(f\), it is shown in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>] that for locally strongly-convex problems: $$y_k = x_\ast + \frac{1}{k} \Delta + O(\rho^k), $$ where  \(\displaystyle \Delta = \sum_{i=0}^\infty (x_i – x_\ast)\) and \(\rho \in (0,1)\) depends on the condition number of \(f”(x_\ast)\). This is illustrated below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2507" height="250" src="https://francisbach.com/wp-content/uploads/2020/02/averaged_gradient.png" width="342"/>Averaged gradient descent on a <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> problem in dimension \(d=400\), and with \(n=4000\) observations. For the regular averaged recursion, the line in the log-log plot has slope \(-2\). See experimental details in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>].</figure></div>



<p class="justify-text">We can make the following observations:</p>



<ul class="justify-text"><li>Before Richardson extrapolation, the asymptotic convergence rate after averaging is of order \(O(1/k^2)\), which is better than the usual \(O(1/k)\) upper-bound for the rate of gradient descent, but with a stronger assumption that in fact leads to exponential convergence before averaging.</li><li>While \(\Delta\) has a simple expression, it cannot be computed in practice (but Richardson extrapolation does not need to know it).</li><li>Richardson extrapolation leads to an exponentially convergent algorithm from an algorithm converging asymptotically in \(O(1/k^2)\).</li></ul>



<p class="justify-text"><strong>Accelerated gradient descent.</strong> Above, we considered averaged gradient descent, which is asymptotically converging as \(O(1/k^2)\), and on which Richardson extrapolation could be used with strong gains. Is it possible also for the accelerated gradient descent method [<a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=dan&amp;paperid=46009&amp;what=fullt&amp;option_lang=eng">8</a>], which has a (non-asymptotic) convergence rate of \(O(1/k^2)\) for convex functions?</p>



<p class="justify-text">It turns out that the behavior of the iterates of accelerated gradient descent is exactly of the form depicted in the left plot of the figure above: that is, the iterates \(x_k\) oscillate around the optimum [<a href="http://jmlr.org/papers/volume17/15-084/15-084.pdf">9</a>, <a href="http://proceedings.mlr.press/v40/Flammarion15.pdf">10</a>], and Richardson extrapolation is of no help, but is not degrading performance too much. See below for an illustration. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2509" height="243" src="https://francisbach.com/wp-content/uploads/2020/02/accelerated_gradient.png" width="332"/>Accelerated gradient descent on a quadratic optimization problem in dimension \(d=1000\). See experimental details in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>].  </figure></div>



<p class="justify-text"><strong>Other algorithms.</strong> It is tempting to test it on other optimization algorithms. For example, as explained in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>], Richardson extrapolation can be used to the <a href="https://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe</a> algorithm, where sometimes it helps, sometimes it doesn’t. Others could be tried.</p>



<h2>Extrapolation on the step-size of stochastic gradient descent</h2>



<p class="justify-text">While above we have focused on Richardson extrapolation applied to the number of iterations of an iterative algorithm, it is most often used in integration methods (for computing integrals or solving ordinary differential equations), and then often referred to as <a href="https://en.wikipedia.org/wiki/Romberg%27s_method">Romberg-Richardson extrapolation</a>. Within machine learning, in a similar spirit, this can be applied to the step-size of stochastic gradient descent [<a href="https://arxiv.org/pdf/1707.06386">3</a>, <a href="http://papers.nips.cc/paper/6514-stochastic-gradient-richardson-romberg-markov-chain-monte-carlo.pdf">11</a>], which I now describe.</p>



<p class="justify-text">We consider the minimization of a function \(F(x)\) defined on \(\mathbb{R}^d\), which can be written as an expectation as $$F(x) = \mathbb{E}_{z} f(x,z).$$ We assume that we have access to \(n\) independent and identically distributed observations (i.i.d.) \(z_1,\dots,z_n\). This is a typical scenario in machine learning, where \(f(x,z)\) represents the loss for the predictor parameterized by \(x\) on the observation \(z\). </p>



<p class="justify-text">The stochastic gradient method is particularly well adapted, and we consider here a single pass, as $$x_i= x_{i-1} – \gamma f'(x_{i-1},z_i),$$ where the gradient is taken with respect to the first variable, for \(i = 1,\dots,n\). It is known that with a constant step-size, when \(n\) tends to infinity, \(x_n\) will <em>not</em> converge to the minimizer \(x_\ast\) of \(F\), as the algorithm always moves [<a href="https://epubs.siam.org/doi/pdf/10.1137/0324039">16</a>], as illustrated below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2488" height="279" src="https://francisbach.com/wp-content/uploads/2020/02/logistic_2d-1.gif" width="403"/>Stochastic gradient descent on a logistic regression problem: (blue) without averaging, (red) with averaging.</figure></div>



<p class="justify-text">One way to damp the oscillations is to consider averaging, that is, $$ y_n = \frac{1}{n+1} \sum_{i=0}^{n} x_i$$ (we consider uniform averaging for simplicity). For least-squares regression, this leads to a converging algorithm [<a href="https://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">12</a>] with attractive properties for ill-conditioned problems (see also <a href="https://francisbach.com/the-sum-of-a-geometric-series-is-all-you-need/">January’s blog post</a>). However, for general loss functions, it is shown in [<a href="https://arxiv.org/pdf/1707.06386">3</a>] that \(y_n\) converges to some \(y^{(\gamma)} \neq x_\ast\). There is a bias due to a step-size \(\gamma\) that does not go to zero. In order to apply Richardson extrapolation, together with Aymeric Dieuleveut and Alain Durmus [<a href="https://arxiv.org/pdf/1707.06386">3</a>], we showed that $$ y^{(\gamma)} = x_\ast + \gamma \Delta + O(\gamma^2),$$ for some \(\Delta \in \mathbb{R}^d\) with some complex expression. Thus, we have $$2 y^{(\gamma)} – y^{(2\gamma)} = x_\ast + O(\gamma^2),$$ thus gaining one order. If we consider the iterate \(y_n^{(\gamma)}\) and \(y_n^{(2 \gamma)}\) associated to the two step-sizes \(\gamma\) and \(2 \gamma\), the linear combination $$2 y_n^{(\gamma)} – y_n^{(2\gamma)} $$ has an improved behavior as it tends to \(2 y^{(\gamma)} – y^{(2\gamma)} = x_\ast + O(\gamma^2)\): it remains not convergent, but get to way smaller values. See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2525" height="274" src="https://francisbach.com/wp-content/uploads/2020/02/SGD_logistic-1.png" width="359"/>Averaged stochastic gradient descent on a logistic regression problem in dimension 20.</figure></div>



<p class="justify-text"><strong>Higher-order extrapolation.</strong> If we can accelerate a sequence by extrapolation, why not extrapolate the extrapolated sequence? This is possible if we have an higher-order expansion of the form $$ y^{(\gamma)} = \theta_\ast + \gamma \Delta_1 + \gamma^2 \Delta_2 + O(\gamma^3),$$ for some (typically unknown) vectors \(\Delta_1\) and \(\Delta_2\). Then, the sharp reader can check that $$3 y_n^{(\gamma)} – 3 y_n^{(2\gamma)} +  y_n^{(3\gamma)}, $$ will lead to cancellation of the first two orders \(\gamma\) and \(\gamma^2\). This is illustrated above for SGD.</p>



<p class="justify-text">Then, why not extrapolate the extrapolation of the extrapolated sequence? One can check that $$4 y_n^{(\gamma)} – 6 y_n^{(2\gamma)} + 4  y_n^{(3\gamma)}  -y_n^{(4\gamma)}, $$ will lead to cancellation of the first three orders of an expansion of \(y^{(\gamma)}\). The <a href="https://en.wikipedia.org/wiki/Binomial_coefficient">binomial coefficient</a> aficionados have already noticed the pattern there, and checked that $$ \sum_{i=1}^{m+1} (-1)^{i-1} { m+1 \choose i} y_n^{(i\gamma)}$$ will lead to cancellations of the first \(m\) orders.</p>



<p class="justify-text">Then, why not go on forever? First because \(m+1\) recursions have to be run in parallel, and second, because the constant in front of the term in \(\gamma^{m+1}\) typically explodes, a phenomenon common to many expansion methods.</p>



<h2>Extrapolation on a regularization parameter</h2>



<p class="justify-text">We now explore the application of Richardson extrapolation to regularization methods. In a nutshell, regularization allows to make an estimation problem more stable (less subject to variations for statistical problems) or the algorithm faster (for optimization problems). However, regularization adds a bias that needs to be removed. In this section, we apply Richardson extrapolation to the regularization parameter to reduce this bias. I will only present an application to smoothing for non-smooth optimization (see an application to  ridge regression in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>]).</p>



<p class="justify-text"><strong>Non-smooth optimization problems</strong>. We consider the minimization of a convex function of the form \(f = h + g\), where \(h\) is smooth and \(g\) is non-smooth. These optimization problems are ubiquitous in machine learning and signal processing, where the lack of smoothness can come from (a) non-smooth losses such as max-margin losses used in support vector machines and more generally structured output classification [<a href="https://icml.cc/Conferences/2005/proceedings/papers/113_StructuredPrediction_TaskarEtAl.pdf">13</a>], and (b) sparsity-inducing regularizers (see, e.g., [<a href="https://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">14</a>] and references therein). While many algorithms can be used to deal with this non-smoothness, we consider a classical smoothing technique below.</p>



<p class="justify-text"><strong>Nesterov smoothing</strong>. In this section, we consider the smoothing approach of Nesterov [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">15</a>] where the non-smooth term is “smoothed” into \(g_\lambda\), where \(\lambda\) is a regularization parameter, and accelerated gradient descent is used to minimize \(h+g_\lambda\). </p>



<p class="justify-text">A typical way of smoothing the function \(g\) is to add \(\lambda\) times a strongly convex regularizer (such as the squared Euclidean norm) to the Fenchel conjugate of \(g\); this leads to a function \(g_\lambda\) which has a smoothness constant (defined as the maximum of the largest eigenvalues of all Hessians) proportional to \(1/\lambda\), with a uniform error of \(O(\lambda)\) between \(g\) and \(g_\lambda\). Given that accelerated gradient descent leads to an iterate with excess function values proportional to \(1/(\lambda k^2)\) after \(k\) iterations, with the choice of \(\lambda \propto 1/k\), this leads to an excess in function values proportional to \(1/k\), which improves on the subgradient method which converges in \(O(1/\sqrt{k})\). Note that the amount of regularization depends on the number of iterations, so that this smoothing method is not “anytime”.</p>



<p class="justify-text"><strong>Richardson extrapolation.</strong> If we denote by \(x_\lambda\) the minimizer of \(h+g_\lambda\) and \( x_\ast\) the global minimizer of \( f=h+g\), if we can show that \( x_\lambda = x_\ast + \lambda \Delta + O(\lambda^2)\), then \( x^{(1)}_\lambda = 2 x_\lambda – x_{2\lambda} = O(\lambda^2)\) and we can expand \( f(x_\lambda^{(1)})  = f(x_\ast)  + O(\lambda^2)\), which is better than the \(O(\lambda)\) approximation without extrapolation. </p>



<p class="justify-text">Then, given a number of iterations \(k\), with \( \lambda \propto k^{-2/3}\), to balance the two terms \( 1/(\lambda k^2)\) and \( \lambda^2\),  we get an overall convergence rate for the non-smooth problem of \( k^{-4/3}\). </p>



<p class="justify-text"><strong>\(m\)-step Richardson extrapolation</strong>. Like above for the step-size, we can also consider \(m\)-step Richardson extrapolation \(x_{\lambda}^{(m)}\), which leads to a bias proportional to \(\lambda^{m+1}\). Thus, if we consider \(\lambda \propto 1/k^{2/(m+2)}\), to balance the terms \(1/(\lambda k^2)\) and \(\lambda^{m+1}\), we get an error for the non-smooth problem of \(1/k^{2(m+1)/(m+2)}\), which can get arbitrarily close to \(1/k^2\) when \(m\) gets large. The downsides (like for the extrapolation on the step-size above) are that (a) the constants in front of the asymptotic equivalent may blow up (a classical problem in high-order expansions), and (b) \(m\)-step extrapolation requires running the algorithm \(m\) times (this can be down in parallel). In the experiment below, 3-step extrapolation already brings in most of the benefits.</p>



<p class="justify-text">In order to experimentally study the benefits of extrapolation, for the <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Lasso</a> optimization problem, and for a series of regularization parameters equal to \(2^{i}\) for \(i\) between \(-18\) and \(1\) (sampled every \(1/5\)), we run accelerated gradient descent on \(h+g_\lambda\) and we plot the value of \(f(x)-f(x_\ast)\) for the various estimates, where for each number of iterations, we minimize over the regularization parameter. This is an oracle version of varying \(\lambda\) as a function of the number of iterations. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2530" height="255" src="https://francisbach.com/wp-content/uploads/2020/02/smoothing.png" width="351"/>Excess function values as a function of the number of iterations, <em>taking into account that \(m\)-step Richardson extrapolation requires \(m\)-times more iterations</em>. There is indeed a strong improvement approaching the rate \(1/k^2\).</figure></div>



<h2>Conclusion</h2>



<p class="justify-text">These last two blog posts were dedicated to acceleration techniques coming from numerical analysis. They are cheap to implement, typically do not interfere with the underlying algorithm, and when used in the appropriate situation, can bring in significant speed-ups.</p>



<p class="justify-text">Next month, I will most probably host an invited post by my colleague <a href="https://www.di.ens.fr/~ataylor/">Adrien Taylor</a>, who will explain how machines can <s>replace</s> help researchers that prove bounds on optimization algorithms.</p>



<h2>References</h2>



<p class="justify-text">[1] Lewis Fry Richardson. <a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1911.0009">The approximate arithmetical solution by finite differences of physical problems involving differential equations, with an application to the stresses in a masonry dam</a>. <em>Philosophical Transactions of the Royal Society of London, Series A</em>, 210(459-470):307–357, 1911.<br/>[2] Francis Bach. <a href="https://arxiv.org/pdf/2002.02835">On the Effectiveness of Richardson Extrapolation in Machine Learning</a>. Technical report, arXiv:2002.02835, 2020.<br/>[3] Aymeric Dieuleveut, Alain Durmus, Francis Bach. <a href="https://arxiv.org/pdf/1707.06386">Bridging the Gap between Constant Step Size Stochastic Gradient Descent and Markov Chains</a>. To appear in <em>The Annals of Statistics</em>, 2019.<br/>[4] Boris T. Polyak,  Anatoli B. Juditsky. <a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">Acceleration of stochastic approximation by averaging</a>. <em>SIAM journal on control and optimization</em> 30(4):838-855, 1992.<br/>[5] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, Alexander Shapiro<em>. </em><a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">Robust stochastic approximation approach to stochastic programming</a>. <em>SIAM Journal on optimization</em>, 19(4):1574-1609, 2009.<br/>[6] Francis Bach, Eric Moulines. <a href="https://papers.nips.cc/paper/4316-non-asymptotic-analysis-of-stochastic-approximation-algorithms-for-machine-learning.pdf">Non-asymptotic analysis of stochastic approximation algorithms for machine learning</a>. <em>Advances in Neural Information Processing Systems</em>, 2011.<br/>[7] Prateek Jain, Praneeth Netrapalli, Sham Kakade, Rahul Kidambi, Aaron Sidford. <a href="http://jmlr.org/papers/volume18/16-595/16-595.pdf">Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification</a>. <em>The Journal of Machine Learning Research</em>, 18(1), 8258-8299, 2017.<br/>[8] Yurii E. Nesterov. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=dan&amp;paperid=46009&amp;what=fullt&amp;option_lang=eng">A method of solving a convex programming problem with convergence rate \(O(1/k^2)\)</a>, <em>Doklady Akademii Nauk SSSR</em>, 269(3):543–547, 1983.<br/>[9] Weijie Su, Stephen Boyd, and Emmanuel J. Candes. <a href="http://jmlr.org/papers/volume17/15-084/15-084.pdf">A differential equation for modeling Nesterov’s accelerated gradient method: theory and insights</a>. <em>Journal of Machine Learning Research</em>, 17(1):5312-5354, 2016.<br/>[10] Nicolas Flammarion, and Francis Bach. <a href="http://proceedings.mlr.press/v40/Flammarion15.pdf">From Averaging to Acceleration, There is Only a Step-size</a>. <em>Proceedings of the International Conference on Learning Theory (COLT)</em>, 2015. <br/>[11] Alain Durmus, Umut Simsekli, Eric Moulines, Roland Badeau, and Gaël Richard. <a href="http://papers.nips.cc/paper/6514-stochastic-gradient-richardson-romberg-markov-chain-monte-carlo.pdf">Stochastic gradient Richardson-Romberg Markov chain Monte Carlo</a>. In <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2016.<br/>[12] Francis Bach and Eric Moulines. <a href="https://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate \(O(1/n)\)</a>. <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2013.<br/>[13] Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. <a href="https://icml.cc/Conferences/2005/proceedings/papers/113_StructuredPrediction_TaskarEtAl.pdf">Learning structured prediction models: A large margin approach</a>. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 2005.<br/>[14] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. <a href="https://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">Optimization with sparsity-inducing penalties</a>. Foundations and Trends in Machine Learning, 4(1):1–106, 2012<br/>[15] Yurii Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions</a>. Mathematical Programming , 103(1):127–152, 2005.<br/>[16] Georg Ch. Pflug. <a href="https://epubs.siam.org/doi/pdf/10.1137/0324039">Stochastic minimization with constant step-size: asymptotic laws</a>. <em>SIAM Journal on Control and Optimization</em>, (24)4:655-666, 1986.</p>



<p/></div>
    </content>
    <updated>2020-03-01T12:41:33Z</updated>
    <published>2020-03-01T12:41:33Z</published>
    <category term="Machine learning"/>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-03-06T05:22:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/03/01/postdoc-at-uc-san-diego-apply-by-march-1-2020/</id>
    <link href="https://cstheory-jobs.org/2020/03/01/postdoc-at-uc-san-diego-apply-by-march-1-2020/" rel="alternate" type="text/html"/>
    <title>postdoc at UC San Diego (apply by March 1, 2020)</title>
    <summary>UCSD has several Postdoctoral Fellowships, aimed at preparing outstanding researchers for academic and leadership careers in Data Science. Areas of interest include both theoretical and practical aspects of machine learning, statistics, algorithms, and their applications. Applications will be reviewed until positions are filled. Website: http://dsfellows.ucsd.edu/ Email: shachar.lovett@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>UCSD has several Postdoctoral Fellowships, aimed at preparing outstanding researchers for academic and leadership careers in Data Science. Areas of interest include both theoretical and practical aspects of machine learning, statistics, algorithms, and their applications. Applications will be reviewed until positions are filled.</p>
<p>Website: <a href="http://dsfellows.ucsd.edu/">http://dsfellows.ucsd.edu/</a><br/>
Email: shachar.lovett@gmail.com</p></div>
    </content>
    <updated>2020-03-01T01:04:27Z</updated>
    <published>2020-03-01T01:04:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-03-06T05:21:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/02/29/leap-day-linkage</id>
    <link href="https://11011110.github.io/blog/2020/02/29/leap-day-linkage.html" rel="alternate" type="text/html"/>
    <title>Leap day linkage</title>
    <summary>Jeff Erickson links one of the first non-trivial straight skeletons (as a roof model), from Kotirte Ebenen (Kotirte Projektionen) und deren Anwendung by Gustav A. von Peschka (1877).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://mathstodon.xyz/@jeffgerickson/103671508553971700">Jeff Erickson links</a> one of the first non-trivial <a href="https://en.wikipedia.org/wiki/Straight_skeleton">straight skeletons</a> (as a roof model), from <em>Kotirte Ebenen (Kotirte Projektionen) und deren Anwendung</em> by Gustav A. von Peschka (1877).</p>
  </li>
  <li>
    <p><a href="https://math.unice.fr/~indira/Mygifs.html">Indira Lara Chatterji has some nice open-licensed animated gifs of concepts in low-dimensional geometry and topology</a> (<a href="https://mathstodon.xyz/@11011110/103684262226520136"/>).</p>
  </li>
  <li>
    <p><a href="https://harvardmagazine.com/2012/11/absolutely-beautiful">Geometric art of Morton C. Bradley, Jr.</a> (<a href="https://mathstodon.xyz/@11011110/103695425477296068"/>, <a href="https://www.georgehart.com/rp/MortonBradley/Morton-Bradley.html">see also</a>). As <a href="https://joelcooper.wordpress.com/2012/06/21/geometric-art-of-morton-c-bradley-jr/">Joel Cooper writes</a>, “I was fascinated first by the complex yet harmonious geometry of these forms, but it is really the use of color that makes these sculptures so transcendent. These sculptures are really exercises in color theory in three dimensions.”</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wordpress.com/2020/02/01/subliminal-graph-duals/">Subliminal graph duals</a> and <a href="https://rjlipton.wordpress.com/2020/02/11/using-negative-nodes-to-count/">using negative nodes to count</a> (<a href="https://mathstodon.xyz/@11011110/103701104439329493"/>). These posts abstract a graph to a 2-polymatroid, the function that maps a subset of edges to the number of vertices it touches. This resembles matroid rank, but a single edge has rank 2. This leads to a notion of a dual, which usually does not come from a graph, with an interesting operation of “exploding” edges dual to edge deletion.</p>
  </li>
  <li>
    <p>The latest <em>Notices</em> has profiles of mathematicians <a href="http://www.ams.org/journals/notices/202003/rnoti-p327.pdf">Fan Chung</a>, <a href="http://www.ams.org/journals/notices/202003/rnoti-p345.pdf">Olga Taussky-Todd</a>, and <a href="http://www.ams.org/journals/notices/202003/rnoti-p368.pdf">Dorothy Hoover</a> (<a href="https://mathstodon.xyz/@mathcination/103690730215482537"/>).</p>
  </li>
  <li>
    <p>My graph algorithm lectures on stable matching have shifted their terminology and metaphors to stable matching from stable marriage (outdated, sexist, heteronormative, potentially offensive, and not a good fit for the applications) but so far Wikipedia hasn’t. Maybe the ngram below explains why: there’s a base level of colloquial usage of “stable marriage” masking the popularity of “matching” in technical usage. Google Scholar shows a clearer picture: 3940 hits for “stable matching” since 2016, 2710 for “stable marriage”. (<a href="https://mathstodon.xyz/@11011110/103717212351898100"/>).</p>

    <p style="text-align: center;"><img alt="Stable matching vs stable marriage Google ngram" src="https://11011110.github.io/blog/assets/2020/stable-marriage-vs-matching.png"/></p>
  </li>
  <li>
    <p><a href="https://divisbyzero.com/2019/05/01/braidtiles/">Braid tiles</a> (<a href="https://mathstodon.xyz/@11011110/103723728540456919"/>). Print your own onto cardstock, cut out, and rearrange, for all your hands-on braid-group experimental-mathematics needs.</p>
  </li>
  <li>
    <p><a href="https://gendergapinscience.files.wordpress.com/2020/02/final_report_20200204-1.pdf">Final report of the international project “A Global Approach to the Gender Gap in Mathematical, Computing, and Natural Sciences: How to Measure It, How to Reduce It?”</a> (<a href="https://mathstodon.xyz/@11011110/103729947676143732"/>, <a href="https://euro-math-soc.eu/news/20/02/26/preliminary-report-project-global-approach-gender-gap-mathematical-computing-and">via</a>), including recommendations for instructors and organizations.</p>
  </li>
  <li>
    <p><a href="https://www.vice.com/en_us/article/4agamm/the-worlds-second-largest-wikipedia-is-written-almost-entirely-by-one-bot">The world’s second largest Wikipedia is written almost entirely by one bot</a> (<a href="https://mathstodon.xyz/@11011110/103735065395757430"/>, <a href="https://news.ycombinator.com/item?id=22403626">via</a>). The language is Cebuano, from the Philippines. Consensus on the English Wikipedia is that automatic translation and automatic content generation are not good enough, but the different languages of Wikipedia are run independently and have different standards from each other.</p>
  </li>
  <li>
    <p>You can encode complex numbers as pairs of real numbers, their real and imaginary parts, and perform complex arithmetic using only real-number operations. But can you go the other way, and encode real numbers somehow using complex arithmetic? <a href="http://jdh.hamkins.org/the-real-numbers-are-not-interpretable-in-the-complex-field/">Joel David Hamkins gives a simple proof of the folklore result that the answer is no</a> (<a href="https://mathstodon.xyz/@11011110/103738094179310308"/>). The short reason why not: there are too few equivalence classes of k-tuples of them to use as representatives of the reals.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/351743/440">Polyhedra that can pack 3-space only in a non-vertex-to-vertex fashion</a> (<a href="https://mathstodon.xyz/@11011110/103744463137230104"/>). The tiling by congruent convex polyhedra in Michael Korn’s answer is very pretty, but a proof that it is the only possible tiling by these polyhedra is still lacking (although it appears very likely to be true).</p>
  </li>
</ul></div>
    </content>
    <updated>2020-02-29T14:46:00Z</updated>
    <published>2020-02-29T14:46:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-02-29T22:47:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4626</id>
    <link href="https://www.scottaaronson.com/blog/?p=4626" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4626#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4626" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Freeman Dyson and Boris Tsirelson</title>
    <summary xml:lang="en-US">Today, as the world braces for the possibility of losing millions of lives to the new coronavirus—to the hunger for pangolin meat, of all things (combined with the evisceration of competent public health agencies like the CDC)—we also mourn the loss of two incredibly special lives, those of Freeman Dyson (age 96) and Boris Tsirelson […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Today, as the world braces for the possibility of losing millions of lives to the new coronavirus—to the <em>hunger for pangolin meat</em>, of all things (combined with the evisceration of competent public health agencies like the CDC)—we also mourn the loss of two incredibly special lives, those of Freeman Dyson (age 96) and Boris Tsirelson (age 69).</p>



<p>Freeman Dyson was sufficiently legendary, both within and beyond the worlds of math and physics, that there’s very little I can add to <a href="https://www.nytimes.com/2020/02/28/science/freeman-dyson-dead.html">what’s been said</a>.  It seemed like he was immortal, although I’d heard from mutual friends that his health was failing over the past year.  When I spent a year as a postdoc at the Institute for Advanced Study, in 2004-5, I often sat across from Dyson in the common room, while he drank tea and read the news.  That I never once struck up a conversation with him is a regret that I’ll now carry with me forever.</p>



<p>My only exchange with Dyson came when he gave a lecture at UC Berkeley, about how life might persist infinitely far into the future, even after the last stars had burnt out, by feeding off steadily dimishing negentropy flows in the nearly-thermal radiation.  During the Q&amp;A, I challenged Dyson that his proposal seemed to assume an analog model of computation.  But, I asked, once we took on board the quantum-gravity insights of Jacob Bekenstein and others, suggesting that nature behaves like a (quantum) digital computer at the Planck scale, with at most ~10<sup>43</sup> operations per second and ~10<sup>69</sup> qubits per square meter and so forth, wasn’t this sort of proposal ruled out?  “I’m not going to argue with you,” was Dyson’s response.  Yes, he’d assumed an analog computational model; if computation was digital then that surely changed the picture.</p>



<p>Sometimes—and not just with his climate skepticism, but also (e.g.) with his idea that general relativity and quantum mechanics <em>didn’t need to be reconciled</em>, that it was totally fine for the deepest layer of reality to be a patchwork of inconsistent theories—Dyson’s views struck me as not merely contrarian but as a high-level form of trolling.  Even so, Dyson’s book <em><a href="https://www.amazon.com/Disturbing-Universe-Sloan-Foundation-Science/dp/0465016774">Disturbing the Universe</a></em> had had a major impact on me as a teenager, for the sparkling prose as much as for the ideas.</p>



<p>With Dyson’s passing, the scientific world has lost one of its last direct links to a heroic era, of Einstein and Oppenheimer and von Neumann and a young Richard Feynman, when theoretical physics stood at the helm of civilization like never before or since.  Dyson, who apparently remained not only lucid but <em>mathematically powerful</em> (!) well into his last year, clearly remembered when the Golden Age of science fiction looked like simply sober forecasting; when the smartest young people, rather than denouncing each other on Twitter, dreamed of scouting the solar system in thermonuclear-explosion-powered spacecraft and seriously worked to make that happen.</p>



<p>Boris Tsirelson (<a href="http://www.math.tau.ac.il/~tsirel/">homepage</a>, <a href="https://en.wikipedia.org/wiki/Boris_Tsirelson">Wikipedia</a>), who emigrated from the Soviet Union and then worked at Tel Aviv University (where my wife Dana attended his math lectures), wasn’t nearly as well known as Dyson to the wider world, but was equally beloved within the quantum computing and information community.  <a href="https://en.wikipedia.org/wiki/Tsirelson%27s_bound">Tsirelson’s bound</a>, which he proved in the 1980s, showed that even quantum mechanics could only violate the Bell inequality by so much and by no more, could only let Alice and Bob win the <a href="https://www.cs.cmu.edu/~odonnell/quantum18/lecture07.pdf">CHSH game</a> with probability cos<sup>2</sup>(π/8).  This seminal result anticipated many of the questions that would only be asked decades later with the rise of quantum information.  Tsirelson’s investigations of quantum nonlocality also led him to pose the famous <a href="https://arxiv.org/abs/0812.4305">Tsirelson’s problem</a>: loosely speaking, can all sets of quantum correlations that can arise from an infinite amount of entanglement, be arbitrarily well approximated using <em>finite</em> amounts of entanglement?  The spectacular answer—no—was only announced one month ago, as a corollary of the <a href="https://www.scottaaronson.com/blog/?p=4512">MIP*=RE breakthrough</a>, something that Tsirelson happily lived to see although I don’t know what his reaction was (<font color="red"><strong>update:</strong></font> I’m told that he indeed learned of it in his final weeks, and was happy about it).  Sadly, for some reason, I never met Tsirelson in person, although I did have lively email exchanges with him 10-15 years ago about his problem and other topics.  This <a href="https://www.iqoqi-vienna.at/en/blog/article/boris-tsirelson/?fbclid=IwAR1PrVvK0u5XmnFLLoPMzMN3x9rY1WIdp1wrYZ_yYPlqSGRpXDkollYTCR0">amusing interview</a> with Tsirelson gives some sense for his personality (hat tip to Gil Kalai, who knew Tsirelson well).</p>



<p>Please share any memories of Dyson or Tsirelson in the comments section.</p></div>
    </content>
    <updated>2020-02-29T13:53:37Z</updated>
    <published>2020-02-29T13:53:37Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-03-05T23:54:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4315</id>
    <link href="https://lucatrevisan.wordpress.com/2020/02/29/lies-damn-lies-and-covid-19/" rel="alternate" type="text/html"/>
    <title>Lies, damn lies, and covid-19</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In the past two weeks, in Italy, we have been drowning in information about the novel coronavirus infection, but the statistics that have been circulating were lacking proper context and interpretation. Is covid-19 just a stronger form of the flu … <a href="https://lucatrevisan.wordpress.com/2020/02/29/lies-damn-lies-and-covid-19/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the past two weeks, in Italy, we have been drowning in information about the novel coronavirus infection, but the statistics that have been circulating were lacking proper context and interpretation. Is covid-19 just a stronger form of the flu or is it a threat to the world economy? Yes.</p>
<p>Now that the first community transmissions are happening in my adopted home in the San Francisco Bay Area, I would like to relay to my American readers what I learned from the Italian experience.</p>
<p><span id="more-4315"/></p>
<p>The most quoted statistics concerned the mortality rate, which has been around 2-2.5% worldwide but only about 0.8% outside of Wuhan, while the flu has a mortality rate around 0.1% and measles around 0.2%. Furthermore, there are no reports of children and infants having died (<a href="http://weekly.chinacdc.cn/en/article/id/e53946e2-c6c4-41e9-9a9b-fea8db1a8f51">source</a>) or even having been in critical conditions. This means that it is an infection only somewhat more serious than the flu and that, in particular, parents should not be concerned about their small children.</p>
<p>The other important statistics, however, is the number of infected people that require intensive care. Some source give it at 5%, while the number circulating in Italy is 10%. </p>
<p>Now, developed countries have of the order of an ICU bed per 10,000 people: the European average is one bed per 9,000 people, Italy has one per 8,000, Germany has one per 3,400 (<a href="https://link.springer.com/article/10.1007/s00134-012-2627-8">source</a>) and the United States has one per 4,000 (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4351597/">source</a>). </p>
<p>ICU beds tend to have high occupancy, and they are used for victims of stroke and heart attacks, patients recovering from difficult surgeries and so on.</p>
<p>This means that if a person in 1,000 is sick with covid-19 at a given time, the critical cases might overwhelm the capacity of intensive care units. So far, this has happened only in Wuhan, at which point the Chinese government started implementing increasingly strong measures, leading to the lockdown of Wuhan and other places in Hubei province. A ratio of 0.1% of infected people at a regional level has not occurred anywhere outside Hubei province. Even in Lombardy we have “only” about 500 cases out of 10 million people. Two nights ago, however, in the town of Lodi (which has about 50,000 residents) 17 people where brought to the hospital in critical condition from the nearby “red zone”, and many of them had to be routed to other cities because Lodi could not deal with them.</p>
<p>It has been estimated that a covid-19 infected person infects on average another 2-2.5 other people (the “R0” parameter of the disease), typically over a week or less, leading to a rather fast exponential growth. On the Diamond Princess, about 20% of passengers and crew were infected. The only way to reduce the R0 below 1, which would make the epidemic die down, or at least to a value not much bigger than 1 (which would make it grow more slowly) is to reduce person-to-person contact. This is why clusters are locked down, and in places too big to lock down there are measures to reduce such contact, such as closing schools, shutting down sports events, conferences, fairs, concerts etc., and encouraging people to work from home.</p>
<p>The latter “lockdown-lite” measures have stopped the growth of infections in cities like Shanghai, Hong Kong and Singapore, where there were relatively large clusters of cases (much more in Shanghai than in Hong Kong and Singapore). We haven’t seen the number of cases in Korea and Italy leveling off, but this is because testing tends to discover infections that happened one or two weeks prior to the test, so the effect of any measure is only seen in the test numbers a couple of weeks or more after they are implemented.</p>
<p>Meanwhile, Italy is building field hospitals in parking lots, using technology tested during earthquakes, to add hospital bed capacity to the system.</p>
<p>Finally, the measures introduced a week ago are starting to show dramatic economic consequences. The government first emphasized the gravity of the situation, in order to be seen as taking strong actions in a grave moment, and then realized it had caused a PR disaster and now is trying to sound a more optimistic note and to walk back some of the measures that are having the most negative economic consequences. Other governments are unlikely to repeat the same errors, so one should expect official government communication to be biased on the side of not creating alarm.</p>
<p>So here are the lessons for America, in brief:</p>
<ul>
<li>Don’t worry about your young children
</li>
<li>If the government follows public health best practices, prepare to see school closures and cancelations of big events wherever there are signs of community transmission</li>
<li>With containment efforts similar to Hong Kong’s and Singapore’s (and Italy’s, Japan’s and Korea’s), we might see as little as one infection per 10,000-100,000 people, meaning nearly zero risk on an individual basis and no major strain on hospitals, but with significant effects on daily life and on the economy
</li><li>There will be political pressure to send out optimistic messages (see how the White House is asking to pre-approve all communication out of the CDC and the NIH) and to avoid measures that could hurt the economy</li>
<li>If the containment efforts are too light, and infections reach even a person in 1,000 on a regional level, the individual risk is still extremely small, but the health care system will be unable to deal with the critically ill. Note that <em>this hasn’t happened anywhere outside of Wuhan</em> and whatever directives come from the top, it’s hard to imagine that local officials would let it happen in their jurisdiction</li>
<li>Try not to have a stroke, a heart attack, or serious surgery in the next few months</li>
</ul></div>
    </content>
    <updated>2020-02-29T12:38:13Z</updated>
    <published>2020-02-29T12:38:13Z</published>
    <category term="lies damn lies and ..."/>
    <category term="covid-19"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-03-06T05:20:15Z</updated>
    </source>
  </entry>
</feed>
