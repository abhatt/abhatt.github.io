<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-03-04T03:22:17Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/</id>
    <link href="https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/" rel="alternate" type="text/html"/>
    <title>2-round BFT SMR with n=4, f=1</title>
    <summary>Guest post by Zhuolun Xiang In the previous post, we presented a summary of our good-case latency results for Byzantine broadcast and Byzantine fault tolerant state machine replication (BFT SMR), where the good case measures the latency to commit given that the leader/broadcaster is honest. In this post, we describe...</summary>
    <updated>2021-03-03T11:37:00Z</updated>
    <published>2021-03-03T11:37:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2021-03-03T22:46:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.01872</id>
    <link href="http://arxiv.org/abs/2103.01872" rel="alternate" type="text/html"/>
    <title>Block Elimination Distance</title>
    <feedworld_mtime>1614729600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Öznur Yaşar Diner, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giannopoulou:Archontia_C=.html">Archontia C. Giannopoulou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stamoulis:Giannos.html">Giannos Stamoulis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thilikos:Dimitrios_M=.html">Dimitrios M. Thilikos</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.01872">PDF</a><br/><b>Abstract: </b>We introduce the block elimination distance as a measure of how close a graph
is to some particular graph class. Formally, given a graph class ${\cal G}$,
the class ${\cal B}({\cal G})$ contains all graphs whose blocks belong to
${\cal G}$ and the class ${\cal A}({\cal G})$ contains all graphs where the
removal of a vertex creates a graph in ${\cal G}$. Given a hereditary graph
class ${\cal G}$, we recursively define ${\cal G}^{(k)}$ so that ${\cal
G}^{(0)}={\cal B}({\cal G})$ and, if $k\geq 1$, ${\cal G}^{(k)}={\cal B}({\cal
A}({\cal G}^{(k-1)}))$. The block elimination distance of a graph $G$ to a
graph class ${\cal G}$ is the minimum $k$ such that $G\in{\cal G}^{(k)}$ and
can be seen as an analog of the elimination distance parameter, with the
difference that connectivity is now replaced by biconnectivity. We show that,
for every non-trivial hereditary class ${\cal G}$, the problem of deciding
whether $G\in{\cal G}^{(k)}$ is NP-complete. We focus on the case where ${\cal
G}$ is minor-closed and we study the minor obstruction set of ${\cal G}^{(k)}$.
We prove that the size of the obstructions of ${\cal G}^{(k)}$ is upper bounded
by some explicit function of $k$ and the maximum size of a minor obstruction of
${\cal G}$. This implies that the problem of deciding whether $G\in{\cal
G}^{(k)}$ is constructively fixed parameter tractable, when parameterized by
$k$. Our results are based on a structural characterization of the obstructions
of ${\cal B}({\cal G})$, relatively to the obstructions of ${\cal G}$. We give
two graph operations that generate members of ${\cal G}^{(k)}$ from members of
${\cal G}^{(k-1)}$ and we prove that this set of operations is complete for the
class ${\cal O}$ of outerplanar graphs. This yields the identification of all
members ${\cal O}\cap{\cal G}^{(k)}$, for every $k\in\mathbb{N}$ and every
non-trivial minor-closed graph class ${\cal G}$.
</p></div>
    </summary>
    <updated>2021-03-03T22:37:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.01660</id>
    <link href="http://arxiv.org/abs/2103.01660" rel="alternate" type="text/html"/>
    <title>On Optimal $w$-gons in Convex Polygons</title>
    <feedworld_mtime>1614729600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Keikha:Vahideh.html">Vahideh Keikha</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.01660">PDF</a><br/><b>Abstract: </b>Let $P$ be a set of $n$ points in $\mathbb{R}^2$. For a given positive
integer $w&lt;n$, our objective is to find a set $C \subset P$ of points, such
that $CH(P\setminus C)$ has the smallest number of vertices and $C$ has at most
$n-w$ points. We discuss the $O(wn^3)$ time dynamic programming algorithm for
monotone decomposable functions (MDF) introduced for finding a class of optimal
convex $w$-gons, with vertices chosen from $P$, and improve it to $O(n^3 \log
w)$ time, which gives an improvement to the existing algorithm for MDFs if
their input is a convex polygon.
</p></div>
    </summary>
    <updated>2021-03-03T22:43:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-03-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.01640</id>
    <link href="http://arxiv.org/abs/2103.01640" rel="alternate" type="text/html"/>
    <title>Double Coverage with Machine-Learned Advice</title>
    <feedworld_mtime>1614729600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lindermayr:Alexander.html">Alexander Lindermayr</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Megow:Nicole.html">Nicole Megow</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Simon:Bertrand.html">Bertrand Simon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.01640">PDF</a><br/><b>Abstract: </b>We study the fundamental online $k$-server problem in a learning-augmented
setting. While in the traditional online model, an algorithm has no information
about the request sequence, we assume that there is given some advice (e.g.
machine-learned predictions) on an algorithm's decision. There is, however, no
guarantee on the quality of the prediction and it might be far from being
correct.
</p>
<p>Our main result is a learning-augmented variation of the well-known Double
Coverage algorithm for k-server on the line (Chrobak et al., SIDMA 1991) in
which we integrate predictions as well as our trust into their quality. We give
an error-dependent competitive ratio, which is a function of a user-defined
trustiness parameter, and which interpolates smoothly between an optimal
consistency, the performance in case that all predictions are correct, and the
best-possible robustness regardless of the prediction quality. When given good
predictions, we improve upon known lower bounds for online algorithms without
advice. We further show that our algorithm achieves for any k an almost optimal
consistency-robustness tradeoff, within a class of deterministic algorithms
respecting local and memoryless properties. Our algorithm outperforms a
previously proposed (more general) learning-augmented algorithm. It is
remarkable that the previous algorithm heavily exploits memory, whereas our
algorithm is memoryless. Finally, we demonstrate in experiments the
practicability and the superior performance of our algorithm on real-world
data.
</p></div>
    </summary>
    <updated>2021-03-03T22:41:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.01398</id>
    <link href="http://arxiv.org/abs/2103.01398" rel="alternate" type="text/html"/>
    <title>Approximation Algorithms for Orthogonal Non-negative Matrix Factorization</title>
    <feedworld_mtime>1614729600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Charikar:Moses.html">Moses Charikar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hu:Lunjia.html">Lunjia Hu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.01398">PDF</a><br/><b>Abstract: </b>In the non-negative matrix factorization (NMF) problem, the input is an
$m\times n$ matrix $M$ with non-negative entries and the goal is to factorize
it as $M\approx AW$. The $m\times k$ matrix $A$ and the $k\times n$ matrix $W$
are both constrained to have non-negative entries. This is in contrast to
singular value decomposition, where the matrices $A$ and $W$ can have negative
entries but must satisfy the orthogonality constraint: the columns of $A$ are
orthogonal and the rows of $W$ are also orthogonal. The orthogonal non-negative
matrix factorization (ONMF) problem imposes both the non-negativity and the
orthogonality constraints, and previous work showed that it leads to better
performances than NMF on many clustering tasks. We give the first
constant-factor approximation algorithm for ONMF when one or both of $A$ and
$W$ are subject to the orthogonality constraint. We also show an interesting
connection to the correlation clustering problem on bipartite graphs. Our
experiments on synthetic and real-world data show that our algorithm achieves
similar or smaller errors compared to previous ONMF algorithms while ensuring
perfect orthogonality (many previous algorithms do not satisfy the hard
orthogonality constraint).
</p></div>
    </summary>
    <updated>2021-03-03T22:41:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.01294</id>
    <link href="http://arxiv.org/abs/2103.01294" rel="alternate" type="text/html"/>
    <title>Wide Network Learning with Differential Privacy</title>
    <feedworld_mtime>1614729600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Huanyu.html">Huanyu Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mironov:Ilya.html">Ilya Mironov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hejazinia:Meisam.html">Meisam Hejazinia</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.01294">PDF</a><br/><b>Abstract: </b>Despite intense interest and considerable effort, the current generation of
neural networks suffers a significant loss of accuracy under most practically
relevant privacy training regimes. One particularly challenging class of neural
networks are the wide ones, such as those deployed for NLP typeahead prediction
or recommender systems.
</p>
<p>Observing that these models share something in common--an embedding layer
that reduces the dimensionality of the input--we focus on developing a general
approach towards training these models that takes advantage of the sparsity of
the gradients.
</p>
<p>More abstractly, we address the problem of differentially private Empirical
Risk Minimization (ERM) for models that admit sparse gradients.
</p>
<p>We demonstrate that for non-convex ERM problems, the loss is logarithmically
dependent on the number of parameters, in contrast with polynomial dependence
for the general case. Following the same intuition, we propose a novel
algorithm for privately training neural networks. Finally, we provide an
empirical study of a DP wide neural network on a real-world dataset, which has
been rarely explored in the previous work.
</p></div>
    </summary>
    <updated>2021-03-03T22:40:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/030</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/030" rel="alternate" type="text/html"/>
    <title>TR21-030 |  Hardness of Constant-round Communication Complexity | 

	Rahul Ilango, 

	Shuichi Hirahara, 

	Bruno Loff</title>
    <summary>How difficult is it to compute the communication complexity of a two-argument total Boolean function $f:[N]\times[N]\to\{0,1\}$, when it is given as an $N\times N$ binary matrix? In 2009, Kushilevitz and Weinreb showed that this problem is cryptographically hard, but it is still open whether it is NP-hard. 

In this work, we show that it is NP-hard to approximate the size (number of leaves) of the smallest constant-round protocol for a two-argument total Boolean function $f:[N]\times[N]\to\{0,1\}$, when it is given as an $N\times N$ binary matrix. Along the way to proving this, we show a new *deterministic* variant of the round elimination lemma, which may be of independent interest.</summary>
    <updated>2021-03-02T21:31:00Z</updated>
    <published>2021-03-02T21:31:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-04T03:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/</id>
    <link href="https://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/" rel="alternate" type="text/html"/>
    <title>Faculty at Universidad Católica de Chile (apply by April 10, 2021)</title>
    <summary>The Institute for Mathematical and Computational Engineering at Universidad Católica de Chile offers one or more full-time positions. We invite applications from candidates in the areas of Data Science, Machine Learning, Optimization, Statistics and Stochastic, although other areas from Computational Science and Engineering, Optimization and Applied Mathematics will also be considered. Website: http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc Email: pbarcelo@uc.cl</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Institute for Mathematical and Computational Engineering at Universidad Católica de Chile offers one or more full-time positions. We invite applications from candidates in the areas of Data Science, Machine Learning, Optimization, Statistics and Stochastic, although other areas from Computational Science and Engineering, Optimization and Applied Mathematics will also be considered.</p>
<p>Website: <a href="http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc">http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc</a><br/>
Email: pbarcelo@uc.cl</p></div>
    </content>
    <updated>2021-03-02T15:50:54Z</updated>
    <published>2021-03-02T15:50:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-04T03:20:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1842</id>
    <link href="https://theorydish.blog/2021/03/02/automated-design-of-error-correcting-codes-part-1/" rel="alternate" type="text/html"/>
    <title>Automated Design of Error-Correcting Codes, Part 1</title>
    <summary>Introduction. For nearly a century, error-correcting codes (ECCs) have been used for allowing communication even when the used communication channel is corrupted by noise. Beyond communication, error-correcting codes have found a variety of other uses, from multiclass learning to even showing hardness of approximation. As such, understanding the rich world of error-correcting codes is essential for progress in all of these domains. In our setting, imagine Alice wants to send a message to Bob of length , but the channel between them is corrupted by noise. To overcome this, Alice uses an encoder to turn her message into a longer, redundant string of length . Then, Bob receives this transmission and uses a decoder to (hopefully) recover the original message of length . Two important properties are the rate of the code (essentially what fraction of the transmission is “information”) and the bit error rate (BER) which is the (expected) number of decoding errors divided by . Desirable properties are to make the rate as large as possible and the BER as small as possible. Basic ECC paradigm. Since the days of Claude Shannon, many error-correcting codes have been discovered, such as Reed-Solomon codes and BCH codes. Each error-correcting code [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Introduction. </strong>For nearly a century, error-correcting codes (ECCs) have been used for allowing communication even when the used communication channel is corrupted by noise. Beyond communication, error-correcting codes have found a variety of other uses, from<a href="https://en.wikipedia.org/wiki/Multiclass_classification"> multiclass learning</a> to even <a href="https://arxiv.org/pdf/1002.3864.pdf">showing hardness of approximation</a>. As such, understanding the rich world of error-correcting codes is essential for progress in all of these domains.</p>



<p>In our setting, imagine Alice wants to send a message to Bob of length <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="k"/>, but the channel between them is corrupted by noise. To overcome this, Alice uses an <em>encoder</em> to turn her message into a longer, redundant string of length <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="n"/>. Then, Bob receives this transmission and uses a <em>decoder</em> to (hopefully) recover the original message of length <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="k"/>. Two important properties are the <em>rate</em> <img alt="k/n" class="latex" src="https://s0.wp.com/latex.php?latex=k%2Fn&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="k/n"/> of the code (essentially what fraction of the transmission is “information”) and the <em>bit error rate (BER)</em> which is the (expected) number of decoding errors divided by <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="k"/>. Desirable properties are to make the rate as large as possible and the BER as small as possible.</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img alt="" height="353" src="https://lh3.googleusercontent.com/9y96GkfFf6FBCUKoufmsqUGfXt7VmrDqAuCQI1IaOfy4DB-VmJWEvSwL9c1mj9QP9gVYq49haRBI96eNMx5qPpr3BFhhGuWvTv4wixNbLLuTuSnI3xv36xaVa64D3DvshMs_XwmT" width="526"/></figure></div>



<p class="has-text-align-center">Basic ECC paradigm.</p>



<p>Since the days of <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a>, many error-correcting codes have been discovered, such as <a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction">Reed-Solomon codes</a> and <a href="https://en.wikipedia.org/wiki/BCH_code">BCH codes</a>. Each error-correcting code has its own tradeoffs (e.g., some have higher rate, some are more resistant to special kinds of channel corruptions, etc.). With the large number of ECCs which have been discovered, it can sometimes be overwhelming what the proper error correcting code is for a given application. Further, if the application is sufficiently specialized there may be <em>no </em>known ECC which meets your needs. Such concerns motivate the <em>automation</em> <em>of error correcting codes</em>, which is the main topic of this blog post. </p>



<p>I’m using the word “automation” to cover a variety of tasks which various computational methods could assist with in the study of ECCs:</p>



<ol><li>Existence — Does the code I want even exist?</li><li>Encoding — What is the “best” way to convert my messages into a code?</li><li>Decoding — How do I recover from noisy transmissions?</li><li>Verification — Is the proposed ECC design provably correct?</li><li>Selection — Which ECC from a given class should I use for a given application?</li></ol>



<p>Each of these facets of the automation of ECCs is a whole field of research! In this and the subsequent post, I will discuss at a high level two types of techniques which have been used to approach these questions: “Formal Methods” and “Machine Learning.” We’ll cover formal methods in this post, and in the next post we will cover machine learning methods.</p>



<p><strong>Formal Methods. </strong>The field of Formal Methods strives to give <em>provable guarantees</em> for various computational questions by reducing them to formal logic. Although formal methods are mostly used for software and hardware verification (that is, making sure they are “bug free”), such tools are also used by mathematicians to show the validity of mathematical statements that would be difficult to prove by hand. For example, the <a href="https://en.wikipedia.org/wiki/Kepler%27s_conjecture">Kepler conjecture</a>, a question of what is the best way to pack spheres in three dimensions–essentially finding an optimal error-correcting code in Euclidean space–was only firmly proved by <a href="https://github.com/flyspeck/flyspeck">Thomas Hales and his team</a> through the use of automated theorem-proving tools.</p>



<p>A line of work for using formal methods to directly construct practical ECCs was initiated by <a href="https://ieeexplore.ieee.org/abstract/document/5699220">Shamshiri and Cheng</a> in 2010. In their work, they are motivated by designing error-correcting codes for static random-access memory (SRAM), the kind that is often used for CPU caches. When using SRAM (pictured below), a worry is that cosmic rays could hit some of the bits, causing them to flip. Further, it is not uncommon for a group of consecutive bits to flip. As such, it is desirable to esure that the error correcting code can correct either <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="g"/> <em>global </em>errors or <img alt="l" class="latex" src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="l"/> <em>local</em> errors. Correcting global errors is a common property of error correcting codes, such as BCH codes or the <a href="https://en.wikipedia.org/wiki/Binary_Golay_code">Golay code</a>. However, local error correction is a much less common property to guarantee. Thus, the authors use a <em>SAT solver</em> to construct error correcting codes with the properties they desire.</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img alt="" height="318" src="https://lh4.googleusercontent.com/qQRzeu2F2XzrjAl6DyPPWAnBVOKOSODIBX4RPBuz0aAMqErYxC2ZyAbtWy3z8ORU4rvMT9UEMI8-C7boHeKEijFwrKY2pRaneEm1lsAoKBUHpQQ1rcMpaLBZs8zAwr2yviMGJPyG" width="529"/></figure></div>



<p class="has-text-align-center">Static random-access memory (source: <a href="https://en.wikipedia.org/wiki/File:Hyundai_RAM_HY6116AP-10.jpg" rel="prettyphoto">Wikipedia</a>)</p>



<p>Assuming that the code to be construction is linear (the encoding map is a linear function over the field <img alt="mathbb F_2" class="latex" src="https://s0.wp.com/latex.php?latex=mathbb+F_2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="mathbb F_2"/>), then the error correcting code can be described by a <a href="https://en.wikipedia.org/wiki/Parity-check_matrix">parity check matrix</a> M in <img alt="{0,1}^{(n-k) times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2C1%7D%5E%7B%28n-k%29+times+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{0,1}^{(n-k) times n}"/>. The key observation the authors make is that for M to be a proper error correcting code, every error pattern <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="p"/> (i.e., vectors with hamming weight at most <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="g"/> or consecutive errors in a block of length <img alt="l" class="latex" src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="l"/>) must have <img alt="Mp" class="latex" src="https://s0.wp.com/latex.php?latex=Mp&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="Mp"/> be a distinct vector. For instance  <img alt="M(0, 1, 0, 0, 1) neq M(1,1,1,0,0)" class="latex" src="https://s0.wp.com/latex.php?latex=M%280%2C+1%2C+0%2C+0%2C+1%29+neq+M%281%2C1%2C1%2C0%2C0%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="M(0, 1, 0, 0, 1) neq M(1,1,1,0,0)"/> if <img alt="g = 2" class="latex" src="https://s0.wp.com/latex.php?latex=g+%3D+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="g = 2"/> and <img alt="l = 3" class="latex" src="https://s0.wp.com/latex.php?latex=l+%3D+3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="l = 3"/>.</p>



<p>These Boolean constraints can be expressed in conjunctive normal form, i.e., a SAT instance. As such a SAT-solver can be used to determine if there exists a matrix M with the given properties for a given k and n. For instance, they are able to find an error correcting code with parameters <img alt="k = 16, n = 26, g = 2" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3D+16%2C+n+%3D+26%2C+g+%3D+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="k = 16, n = 26, g = 2"/> and <img alt="l = 4" class="latex" src="https://s0.wp.com/latex.php?latex=l+%3D+4&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="l = 4"/>. In their follow-up work [<a href="https://ieeexplore.ieee.org/abstract/document/6139156">Shamshi, Ghofrani, Cheng, 2011]</a>, they use this error-correcting code for modeling an “on-chip network” between CPU cores in a multi-core processor<strong>.</strong></p>



<p>Another line of work led by Ben Curtis (see the <a href="https://cs.uwaterloo.ca/~cbright/reports/cacm-preprint.pdf">survey by Curtis, Kotsireas, and Ganesh</a>) has been seeking to construct ECC-like combinatorial objects. An example of such an object is a Hadamard matrix: a square matrix with <img alt="\pm 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpm+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\pm 1"/> entries such that every row and column is orthogonal in <img alt="\mathbb R^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\mathbb R^n"/>. In fact, the authors search for a special type of Hadamard matrix made up of a quartet Williamson matrices which have an intricate algebraic structure. They find these objects by using an algorithm which goes back-and-forth between a SAT solver with a CAS (computer algebraic system) to help narrow the search space. </p>



<p>Formal Methods have further applications in error-correcting codes for <a href="https://ieeexplore.ieee.org/abstract/document/6649704">distributed cloud storage</a> and <a href="https://arxiv.org/pdf/1804.02317.pdf">value-deviation-bounded codes</a>.</p>



<p>This concludes our first post. In the next post, we will cover machine learning methods. </p>



<p>Are you aware of other examples or applications of automation to error-correcting codes? If so, please leave a comment.</p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Aviad Rubinstein, Moses Charikar, and Mary Wootters for valuable feedback. </p></div>
    </content>
    <updated>2021-03-02T15:00:00Z</updated>
    <published>2021-03-02T15:00:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>jbrakensiek</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-03-04T03:21:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/" rel="alternate" type="text/html"/>
    <title>Associate professor at KTH Royal Institute of Technology (apply by April 15, 2021)</title>
    <summary>KTH Royal Institute of Technology, School of Electrical Engineering and Computer Science has a vacancy for one Associate Professor with specialization in Foundations of Data Science. The position will be permanent and full time, to start as soon as possible. Website: https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/ Email: tenuretrack@eecs.kth.se</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>KTH Royal Institute of Technology, School of Electrical Engineering and Computer Science has a vacancy for one Associate Professor with specialization in Foundations of Data Science. The position will be permanent and full time, to start as soon as possible.</p>
<p>Website: <a href="https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/">https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/</a><br/>
Email: tenuretrack@eecs.kth.se</p></div>
    </content>
    <updated>2021-03-01T21:54:01Z</updated>
    <published>2021-03-01T21:54:01Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-04T03:20:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/029</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/029" rel="alternate" type="text/html"/>
    <title>TR21-029 |  Public-Coin Statistical Zero-Knowledge Batch Verification against Malicious Verifiers | 

	Inbar Kaslasi, 

	Ron Rothblum, 

	Prashant Nalini Vasudevan</title>
    <summary>Suppose that a problem $\Pi$ has a statistical zero-knowledge (SZK) proof with communication complexity $m$. The question of batch verification for SZK asks whether one can prove that $k$ instances $x_1,\ldots,x_k$ all belong to $\Pi$ with a statistical zero-knowledge proof whose communication complexity is better than $k \cdot m$ (which is the complexity of the trivial solution of executing the original protocol independently on each input).

In a recent work, Kaslasi et al. (TCC, 2020) constructed such a batch verification protocol for any problem having a non-interactive SZK (NISZK) proof-system. Two drawbacks of their result are that their protocol is private-coin and is only zero-knowledge with respect to the honest verifier.

In this work, we eliminate these two drawbacks by constructing a public-coin malicious-verifier SZK protocol for batch verification of NISZK. Similarly to the aforementioned prior work, the communication complexity of our protocol is $\big(k+poly(m) \big) \cdot polylog(k,m)$</summary>
    <updated>2021-03-01T16:38:36Z</updated>
    <published>2021-03-01T16:38:36Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-04T03:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2021/03/01/beyondlogconcave2/</id>
    <link href="http://offconvex.github.io/2021/03/01/beyondlogconcave2/" rel="alternate" type="text/html"/>
    <title>Beyond log-concave sampling (Part 2)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In our previous <a href="http://www.offconvex.org/2020/09/19/beyondlogconvavesampling">blog post</a>, we introduced the challenges of sampling distributions beyond log-concavity. 
We first introduced the problem of sampling from a distibution $p(x) \propto e^{-f(x)}$ given value or gradient oracle access to $f$, as an analogous problem to black-box optimization with oracle access. We introduced the natural algorithm for sampling in this setup: Langevin Monte Carlo, a Markov Chain reminiscent of noisy gradient descent,</p>

\[x_{t+\eta} = x_t - \eta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I).\]

<p>Finally, we laid out the challenges when $f$ is not convex; in particular, LMC can suffer from slow mixing.</p>

<p>In this and the coming post, we describe two of our recent works tackling this problem. We identify two kinds of structure beyond log-concavity under which we can design provably efficient algorithms:  <em>multi-modality</em> and <em>manifold structure in the level sets</em>. These structures commonly occur in practice, especially in problems involving statistical inference and posterior sampling in generative models.</p>

<p>In this post, we will focus on multimodality, covered by the paper <a href="https://arxiv.org/abs/1812.00793">Simulated tempering Langevin Monte Carlo</a> by Rong Ge, Holden Lee, and Andrej Risteski.</p>

<h1 id="sampling-multimodal-distributions-with-simulated-tempering">Sampling multimodal distributions with simulated tempering</h1>

<p>The classical scenario in which Langevin takes exponentially long to mix is when $p$ is a mixture of two well-separated gaussians. In broadest generality, this was considered by <a href="http://www.ems-ph.org/journals/show_abstract.php?issn=1435-9855%20&amp;vol=6&amp;iss=4&amp;rank=1">Bovier et al. 2004</a> who used tools from metastable processes to show that transitioning from one peak to another can take exponential time. Roughly speaking, they show the transition time is proportional to the “energy barrier” a particle has to cross. If the gaussians have unit variance and means at distance $2r$, then the probability density at a point midway in between is $\propto e^{-r^2/2}$, and this energy barrier is $\propto e^{r^2/2}$. Thus, the mixing time is exponential. Qualitatively, the intuition for this phenomenon is simple to describe: if started at point A, the drift (i.e. gradient) term will push the walk towards A, so long as it’s close to the basin around A; hence, to transition from A to B (through C) the Gaussian noise must persistenly counteract the gradient term.</p>

<center>
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_bovier.gif" width="500"/>
</center>

<p>Hence Langevin on its own will not work even in very simple multimodal settings.</p>

<p>In <a href="https://arxiv.org/abs/1812.00793">our paper</a>, we show that combining Langevin Monte Carlo with a temperature-based heuristic called <em>simulated tempering</em> can significantly speed up mixing for multimodal distributions, where the number of modes is not too large, and the modes “look similar.”</p>

<p>More precisely, we show:</p>

<blockquote>
  <p><strong>Theorem (Ge, Lee, Risteski ‘18, informal)</strong>: If $p(x)$ is a mixture of $k$ shifts of a strongly log-concave distribution in $d$ dimensions (e.g. Gaussian), an algorithm based on simulated tempering and Langevin Monte Carlo that runs in time poly($d,k, 1/\varepsilon$) produces samples from a distribution $\varepsilon$-close to $p$ in total variation distance.</p>
</blockquote>

<p>The main idea is to create a meta-Markov chain (the simulated tempering chain) which has two types of moves: change the current “temperature” of the sample, or move “within” a temperature. The main intuition behind this is that at higher temperatures, the distribution is flatter, so the chain explores the landscape faster (see the figure below).</p>

<center> 
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_tempering.gif"/>
</center>

<p>More formally, the distribution at inverse temperature $\beta$ is given by $p_\beta(x) \propto e^{-\beta f(x)}$. The Langevin chain which corresponds to $\beta$ is given by</p>

\[x_{t+\eta} = x_t - \eta \beta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I).\]

<p>As in the figure above, a high temperature (low $\beta&lt;1$) flattens out the distribution and causes the chain to mix faster (top distribution in figure). However, we can’t merely run Langevin at a higher temperature, because the stationary distribution of the high-temperature chain is wrong: it’s $p_\beta(x)$. The idea behind simulated tempering is to run Langevin chains at different temperatures, sometimes swapping to another temperature to help lower-temperature chains explore. To maintain the right stationary distributions at each temperature, we use a Metropolis-Hastings filtering step.</p>

<p>More formally, choosing a suitable sequence $0&lt; \beta_1&lt; \cdots &lt;\beta_L=1$, we define the simulated tempering chain as follows.</p>

<p><img src="http://holdenlee.github.io/pics/stl.png" style="float: right;" width="300"/></p>

<ul>
  <li>The <em>state space</em> is a pair of a temperature and location in space $(i, x), i \in [L], x \in \mathbb{R}^d$.<br/>
<!--$L$ copies of the state space (in our case $\mathbb R^d$), one copy for each temperature.--></li>
  <li>The <em>transitions</em> are defined as follows.
    <ul>
      <li>If the current point is $(i,x)$, then <em>evolve</em> $x$ according to Langevin diffusion with inverse temperature $\beta_i$.</li>
      <li>Propose swaps with some rate $\lambda &gt;0$. Proposing a swap means attempting to move to a neighboring chain, i.e. change $i$ to $i’=i\pm 1$. With probability $\min{p_{i’}(x)/p_i(x), 1}$, the transition is accepted. Otherwise, stay at the same point. This is a <em>Metropolis-Hastings step</em>; its purpose is to preserve the stationary distribution.</li>
    </ul>
  </li>
</ul>

<p>Finally, it’s not too hard to see that at the stationary distribution, the samples at the $L$th level ($\beta_L=1$) are the desired samples.</p>

<h2 id="proof-idea-decomposition-theorem">Proof idea: decomposition theorem</h2>

<p>The main strategy is inspired by Madras and Randall’s <a href="https://www.jstor.org/stable/2699896">Markov chain decomposition theorem</a>, which gives a criterion for a Markov chain to mix rapidly: partition the state space into sets, and show that</p>

<ol>
  <li>The Markov chain mixes rapidly when restricted to each set of the partition.</li>
  <li>The <em>projected</em> Markov Chain, which we define momentarily, mixes rapidly. If there are $m$ sets, the projected chain $\overline M$ is defined on the state space ${1,\ldots, m}$, and transition probabilities are given by average probability flows between the corresponding sets.</li>
</ol>

<p>To implement this strategy, we first have to specify the partition. In fact, we roughly show that there is a partition of $[L] \times \mathbb{R}^d$ in which:</p>

<ol>
  <li>The simulated tempering Langevin chain mixes fast within each of the sets.</li>
  <li>The “volume” of the sets (under the stationary distribution of the tempering chain) is not too small.
<!-- [HL: alt.] There is no set at high temperature that has much larger volume at low temperature.
 --></li>
</ol>

<p>In applying the Madras-Randall framework with this partition, it’s clear that point (1) above satisfies requirement (1) for the framework; point (2) ensures that the projected Markov chain has no “bottlenecks” and hence that it mixes rapidly (requirement (2)). More precisely, we can show rapid mixing either through the method of canonical paths or Cheeger’s inequality. To do this, we exhibit a “good-probability” path between any two sets in the partition, going through the highest temperature.</p>

<p>The intuition for why this path works is illustrated in the figure below: when transitioning from the set corresponding to the left mode at level $L$ to the right mode at level $L$, each of the steps up/down the temperatures are accepted with good probability if the neighboring temperatures are not too different; at the highest temperature, the chain mixes fast by point (1), and since each of the sets are not too small by point (2), there is a reasonable probability to end at the right mode at the highest temperature.</p>

<center>
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_conductance.gif"/>
</center>

<!--(rework this picture?) This is a Markov chain with a small state space, so its spectral gap is easy to lower-bound (e.g., with Cheeger's inequality). The one thing we need to check is that there is no "bottleneck," i.e., one set in the partition that has low probability at high temperature and high probability at low temperature. -->

<p>Intuitively, the partition should track the “modes” of the distribution, but a technical hurdle in implementing this plan is in defining the partition when the modes overlap. One can either do this spectrally (i.e. showing that the Langevin chain has a spectral gap, and use theorems about <a href="https://arxiv.org/abs/1309.3223">spectral graph partitioning</a>, as we did in the <a href="https://arxiv.org/abs/1710.02736">first version</a> of the paper), or use a functional “soft decomposition theorem” which is a more flexible version of the classical decomposition theorem, which we use in a <a href="https://arxiv.org/abs/1812.00793">later version</a> of the paper.</p>

<!-- ![](http://holdenlee.github.io/pics/proj_chain.png)--></div>
    </summary>
    <updated>2021-03-01T14:00:00Z</updated>
    <published>2021-03-01T14:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2021-03-03T22:44:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/</id>
    <link href="https://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/" rel="alternate" type="text/html"/>
    <title>PhD. Thesis at LAMSADE (Paris Dauphine) (apply by April 30, 2021)</title>
    <summary>PhD. Thesis offer in Paris Dauphine University “Algorithmic aspects of intersection graphs” Website: https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf Email: florian.sikora@dauphine.fr</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>PhD. Thesis offer in Paris Dauphine University “Algorithmic aspects of intersection graphs”</p>
<p>Website: <a href="https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf">https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf</a><br/>
Email: florian.sikora@dauphine.fr</p></div>
    </content>
    <updated>2021-03-01T10:35:30Z</updated>
    <published>2021-03-01T10:35:30Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-04T03:20:52Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/</id>
    <link href="https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/" rel="alternate" type="text/html"/>
    <title>Good-case Latency of Byzantine Broadcast: a Complete Categorization</title>
    <summary>Guest post by Zhuolun Xiang State Machine Replication and Broadcast Many existing permission blockchains are built using Byzantine fault-tolerant state machine replication (BFT SMR), which ensures all honest replicas agree on the same sequence of client inputs. Most of the practical solutions for BFT SMR are based on the Primary-Backup...</summary>
    <updated>2021-02-28T18:07:00Z</updated>
    <published>2021-02-28T18:07:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2021-03-03T22:46:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5663884325046890461</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5663884325046890461/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/using-number-of-phds-as-measure-of.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5663884325046890461" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5663884325046890461" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/using-number-of-phds-as-measure-of.html" rel="alternate" type="text/html"/>
    <title>Using number-of-PhD's as a measure of smartness is stupid.</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In <i>Thor:Ragnorak</i> Bruce Banner mentions that he has 7 PhDs. Gee, I wonder how he managed to slip that into a conversation casually.  Later in the movie:</p><p><br/></p><p>Bruce: I don't know how to fly one of those (it an Alien Spacecraft)</p><p>Thor: You're a scientist. Use one of your PhD's </p><p>Bruce: None of them are for flying alien spaceships.</p><p><br/></p><p>On the episode <i>Double Date </i>of Archer (Season 11, Episode 6) Gabrielle notes that she has 2 PhD's whereas Lana only has 1 PhD. </p><p><br/></p><p>I am sure there are other examples of a work of fiction using <i>number of PhDs </i>as a way to say that someone is smart. In reality the number of PhD's one has is... not really a thing. </p><p>In reality if a scientist wants to do work in another field they... do work in that field.</p><p>Godel did research in Physics in the 1950's, but it would have been silly to go back and get a PhD in it.</p><p>Fortnow did research in Economics, but it would have been silly to go back and get a PhD in it. </p><p>Amy Farrah Fowler worked in neurobiology and then in Physics. Her Nobel prize in physics (with Sheldon Cooper) is impressive, getting a PhD in Physics would be ... odd. Imagine someone looking at here resume: <i>She has a Nobel Prize in Physics, but does she have a PhD? Did she pass her qualifying</i> <i>exams?</i>  This is the flip side of what I mentioned in a prior post about PhD's: <i>Not only does Dr. Doom want to take over the world, but his PhD is from The University of Latveria, which is not accredited. </i></p><p>There are other examples.</p><p>There ARE some people who get two PhDs for reasons of job market or other such things. That's absolutely fine of course. However, I wonder if in the real world they brag about it. I doubt it. </p><p>Is there anyone who has 3 PhDs? I would assume yes, but again, I wonder if they brag about it. Or should. </p><p>WHY do TV and movies use number-of-PhDs as a sign of genius? I do not know- especially since there are BETTER ways say someone is a genius in a way the audience can understand:  number-of-Nobel-prizes, number-of-times-mentioned-in-complexityblog,  number of Dundie's (see <a href="https://theoffice.fandom.com/wiki/Dundie">here</a>), etc. </p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-02-28T17:42:00Z</updated>
    <published>2021-02-28T17:42:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-03-04T02:39:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/02/28/linkage</id>
    <link href="https://11011110.github.io/blog/2021/02/28/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>STOC 2021 accepted papers (\(\mathbb{M}\)).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="http://acm-stoc.org/stoc2021/accepted-papers.html">STOC 2021 accepted papers</a> (<a href="https://mathstodon.xyz/@11011110/105748480557219533">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://randomascii.wordpress.com/2021/02/16/arranging-invisible-icons-in-quadratic-time/">Arranging invisible icons in quadratic time</a> (<a href="https://mathstodon.xyz/@11011110/105756917532905626">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26152335">via</a>). Yet another instance where using a too-slow algorithm causes a UI hang, with the twist that the better solution would not be to replace it with a faster algorithm, but instead to not do the useless thing that the bad algorithm does at all.</p>
  </li>
  <li>
    <p><a href="https://joshdata.me/iceberger.html">Fun with shapes: draw an iceberg and see which way up and how deep it would float</a> (<a href="https://mathstodon.xyz/@11011110/105768276511155377">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26201160">via</a>, <a href="https://www.metafilter.com/190533/Iceberger">via2</a>, <a href="https://boingboing.net/2021/02/20/make-your-own-iceberg-with-iceberger.html">via3</a>). Inspired by <a href="https://mobile.twitter.com/GlacialMeg/status/1362557149147058178">a twitter thread by Megan Thompson-Munson</a> pointing out that many supposed photos or illustrations of icebergs are fake and wrong.</p>
  </li>
  <li>
    <p>Draw an infinite subgraph of the 3d integer lattice in which each vertex has four co-planar neighbors, in a perpendicular plane to each of its neighbors (<a href="https://mathstodon.xyz/@11011110/105771494222747316">\(\mathbb{M}\)</a>). This completely determines the subgraph, which is 4-regular and highly symmetric. It is the graph of adjacencies of the cubes in the <a href="https://en.wikipedia.org/wiki/Tetrastix">tetrastix structure</a>. Does this graph have a name and history?</p>
  </li>
  <li>
    <p><a href="https://cacm.acm.org/magazines/2021/3/250708-gender-trends-in-computer-science-authorship">Gender trends in computer science authorship</a> (<a href="https://mathstodon.xyz/@11011110/105781841287243050">\(\mathbb{M}\)</a>). Takeaways for me (mostly from the barely-readable Fig. 4) are:</p>

    <ul>
      <li>
        <p>Roughly one in four coauthors of CS research publications are currently female, up from a big dip of one in seven in the 1970s to 1990s.</p>
      </li>
      <li>
        <p>Mathematics started lower and is currently more or less the same.</p>
      </li>
      <li>
        <p>We are not on track to gender parity.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>I’m sad that the only way to find a viewable version of the 1991 short film <em><a href="https://en.wikipedia.org/wiki/Not_Knot">Not Knot</a></em> (on the hyperbolic geometry of knot complements) seems to be through pirate copies (<a href="https://mathstodon.xyz/@11011110/105785401264334824">\(\mathbb{M}\)</a>). Or you could pay $45 to Amazon for a copy on DVD. Do most people still have DVD players? At least they’re not still trying to sell it on VHS only.</p>
  </li>
  <li>
    <p><a href="https://cscresearchblog.wordpress.com/2018/11/16/karp-sipser-heuristic-and-reductions/">On the slow spread of knowledge of nice theorems</a> (<a href="https://mathstodon.xyz/@11011110/105793165233864617">\(\mathbb{M}\)</a>), an amusing cartoon at the end of a longer blog post on fast graph matching heuristics.</p>
  </li>
  <li>
    <p>Today’s LaTeX formatting tip (<a href="https://mathstodon.xyz/@11011110/105796107362586793">\(\mathbb{M}\)</a>): You know that bug where amsthm + hyperref, with one numbering for theorems and lemmas and corollaries and whatever, causes <code class="language-plaintext highlighter-rouge">\autoref</code> to call them theorems even when they’re really lemmas and corollaries and whatever? If you don’t, you’re lucky. Anyway, there’s a very simple workaround: after loading amsthm and hyperref, add one more package:</p>

    <p><code class="language-plaintext highlighter-rouge">\usepackage[capitalize,nameinlink]{cleveref}</code></p>

    <p>Then, just use <code class="language-plaintext highlighter-rouge">\cref</code> everywhere you were using <code class="language-plaintext highlighter-rouge">\autoref</code>. Problem solved!</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm">Lloyd’s algorithm</a> animated for 3d points (<a href="https://mathstodon.xyz/@tpfto/105553548210257285">\(\mathbb{M}\)</a>). See also <a href="https://mathstodon.xyz/@tpfto/105803635782297523">the spherical version</a>.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wordpress.com/2021/02/27/new-old-ancient-results/">Applications of the no-3-in-line problem and cap-sets to complexity theory</a> (<a href="https://mathstodon.xyz/@11011110/105807834096788492">\(\mathbb{M}\)</a>). “What is most curious to us is that for matrix multiplication, the cap-set related technique frustrates a better complexity upper bound, whereas [for linear algebraic circuits] it frustrates a better lower bound.”</p>
  </li>
  <li>
    <p><a href="https://www.bldgblog.com/2013/08/tensioned-suspension/">Tensioned suspension</a> (<a href="https://mathstodon.xyz/@11011110/105811049795181041">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=9093187">via</a>): sculptures by Dan Grayber in which the weight of mechanical linkages causes them to push out against the sides of their glass enclosures, seemingly causing them to hang suspended in air. More at <a href="http://www.dangrayber.com/">Grayber’s web site</a>.</p>
  </li>
</ul></div>
    </content>
    <updated>2021-02-28T16:24:00Z</updated>
    <published>2021-02-28T16:24:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-03-01T01:04:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18225</id>
    <link href="https://rjlipton.wordpress.com/2021/02/27/new-old-ancient-results/" rel="alternate" type="text/html"/>
    <title>New, Old, Ancient Results</title>
    <summary>Nonexistence theorems and attempts at lower bounds Cropped from src Joshua Grochow is an assistant professor in Computer Science and Mathematics at the University of Colorado at Boulder. He was a student of Ketan Mulmuley and Lance Fortnow at Chicago; his dissertation and some subsequent papers did much to widen the horizons of the “Geometric […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Nonexistence theorems and attempts at lower bounds</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/02/jgcropped.jpg"><img alt="" class="alignright wp-image-18228" height="168" src="https://rjlipton.files.wordpress.com/2021/02/jgcropped.jpg?w=141&amp;h=168" width="141"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="https://home.cs.colorado.edu/~jgrochow/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Joshua Grochow is an assistant professor in Computer Science and Mathematics at the University of Colorado at Boulder. He was a student of Ketan Mulmuley and Lance Fortnow at Chicago; his <a href="https://home.cs.colorado.edu/~jgrochow/grochow-thesis.pdf">dissertation</a> and <a href="https://arxiv.org/pdf/1304.6333.pdf">some</a> <a href="https://arxiv.org/pdf/1112.2012.pdf">subsequent</a> <a href="https://arxiv.org/pdf/1605.02815.pdf">papers</a> did much to widen the horizons of the “Geometric Complexity Theory” (GCT) program. He is also a gifted expositor.</p>
<p>
Today we will highlight some of his work and some of his exposition of new and old theorems.</p>
<p>
An ancient one is Stephen Mahaney’s famous theorem on the nonexistence of sparse <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{NP}}"/>-complete sets (unless <img alt="{\mathsf{NP = P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP+%3D+P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{NP = P}}"/>). Grochow <a href="https://arxiv.org/pdf/1610.05825.pdf">discusses</a> a simpler proof of the theorem by Manindra Agrawal and gives some further impacts on GCT. </p>
<p>
A recent <a href="https://drops.dagstuhl.de/opus/volltexte/2021/13570/pdf/LIPIcs-ITCS-2021-31.pdf">one</a> with Youming Qiao is on an old topic and is in the 2021 Innovations in Theoretical Computer Science conference. It is titled, “On the Complexity of Isomorphism Problems for Tensors, Groups, and Polynomials I: Tensor Isomorphism-Completeness,” and grows out of a 2019 <a href="https://arxiv.org/abs/1907.00309">paper</a> by the same authors. </p>
<p>
This came to my attention through communications with Grochow’s <a href="https://michaellevet.github.io">student</a>, Michael Levet. Indeed, Levet is the reason for my putting this all together. He raised through email some questions about an ancient result of mine on group isomorphism. I reported <a href="https://rjlipton.wordpress.com/2013/05/11/advances-on-group-isomorphism/">previously</a>:</p>
<blockquote><p><b> </b> <em> Long ago Bob Tarjan and Zeke Zalcstein and I made a simple observation: Group isomorphism could be done in time 	</em></p><em>
<p align="center"><img alt="\displaystyle  n^{\log_{2} n + O(1)}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++n%5E%7B%5Clog_%7B2%7D+n+%2B+O%281%29%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  n^{\log_{2} n + O(1)}. "/></p>
</em><p><em>This relies on the easy-to-prove fact that every group has at most <img alt="{\log_{2} n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog_%7B2%7D+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\log_{2} n}"/> generators. We have discussed this idea earlier <a href="https://rjlipton.wordpress.com/2011/10/08/an-annoying-open-problem/">here</a>. </em>
</p></blockquote>
<p>
Levet raised an issue about related observations of mine—ones that were misleading at best. I think he has a good point and we are still trying to unravel exactly what I meant back then. I applaud him for reading ancient stuff, for trying to extend it, and for working on such problems. I wish him well.</p>
<p>
</p><h2> No Three In a Row </h2><p/>
<p>While Levet and I work that out and think about Grochow’s paper on isomorphism problems with Qiao, Ken and I want to highlight a different expository <a href="https://www.ams.org/journals/bull/2019-56-01/S0273-0979-2018-01648-0/S0273-0979-2018-01648-0.pdf">paper</a> by Grochow on news from 2016 that we <a href="https://rjlipton.wordpress.com/2016/06/15/polynomial-prestidigitation/">covered</a> then. Grochow’s paper appeared in the <em>AMS Bulletin</em> and is titled, “New Applications Of The Polynomial Method: The Cap Set Conjecture And Beyond.” </p>
<p>
To lead in to the subject, here is a <a href="https://archive.org/stream/amusementsinmath00dude#page/94/mode/2up">problem</a> from 1917 by the English puzzlemaster Henry Dudeney titled, “A Puzzle With Pawns”:</p>
<blockquote><p><b> </b> <em> Place two pawns in the middle of the chess- board, one at Q 4 and the other at K 5. Now, place the remaining fourteen pawns (sixteen in all) so that no three shall be in a straight line in any possible direction. Note that I purposely do not say queens, because by the words ” any possible direction ” I go beyond attacks on diagonals. The pawns must be regarded as mere points in space — at the centres of the squares. </em>
</p></blockquote>
<p>
Sixteen is obviously the maximum possible for a standard <img alt="{8 \times 8}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{8 \times 8}"/> chessboard because a seventeenth pawn would make three in some row and some column. For an <img alt="{r \times r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Ctimes+r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r \times r}"/> board, the limit is <img alt="{2r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2r}"/> by similar reasoning—this is an example of the <em>pigeonhole principle</em> which we just <a href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/">mentioned</a>. </p>
<p>
It is possible to achieve the maximum for all <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r}"/> up to <img alt="{46}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B46%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{46}"/> and then the only other cases known are <img alt="{48}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B48%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{48}"/>, <img alt="{50}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B50%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{50}"/>, and <img alt="{52}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{52}"/>. That’s it. Here are solutions for <img alt="{r = 10}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r = 10}"/> and <img alt="{r = 52}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r = 52}"/>. The latter was found by Achim Flammenkamp, whose <a href="http://wwwhomes.uni-bielefeld.de/achim/no3in/readme.html">page</a> has encyclopedic information. On the former, the pieces are positioned on gridpoints like stones in Go, which seems a better context for this problem than chess. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/02/3inarow20and52.jpg"><img alt="" class="aligncenter wp-image-18230" height="257" src="https://rjlipton.files.wordpress.com/2021/02/3inarow20and52.jpg?w=550&amp;h=257" width="550"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://en.wikipedia.org/wiki/No-three-in-line_problem">src1</a>, <a href="https://mathworld.wolfram.com/No-Three-in-a-Line-Problem.html">src2</a></font>
</td>
</tr>
</tbody></table>
<p>
The conjecture is not only that a <img alt="{2n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2n}"/>-size solution exists for only finitely many <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/>, but also that the maximum size for all sufficiently large <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r}"/> is bounded by <img alt="{cr}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bcr%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{cr}"/> with <img alt="{c &lt; 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{c &lt; 2}"/>, indeed, with <img alt="{c &lt; 1.815}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+1.815%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{c &lt; 1.815}"/>. It is known that <img alt="{(1.5-\epsilon_r)r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281.5-%5Cepsilon_r%29r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(1.5-\epsilon_r)r}"/> stones can always be placed with no three collinear, where the <img alt="{\epsilon_r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\epsilon_r}"/> depends on the closeness of a prime to <img alt="{\frac{r}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Br%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{r}{2}}"/>. </p>
<p>
The problem can be taken to dimensions <img alt="{n \ge 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cge+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n \ge 3}"/> that are beyond the plane. We can also extend what is meant by a “line” via various notions of wrapping-around. Then the question is how close the maximum size can stay to being linear in the size of the space—as <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/> and/or the size <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r}"/> of an individual dimension increase.</p>
<p>
</p><h2> Not As Easy As Tic-Tac-Toe </h2><p/>
<p>
The theme of the no-three-in-a-line problem is fundamental to combinatorics. There are tons of problems of the form: </p>
<blockquote><p><b> </b> <em> How many objects can one place, so that no pattern of some certain type exists? </em>
</p></blockquote>
<p>
In <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/>-dimensional space the smallest <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r}"/> of interest is <img alt="{r = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r = 3}"/>. This means playing on higher-dimensional versions of the <img alt="{3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{3 \times 3}"/> grid and <img alt="{3 \times 3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{3 \times 3 \times 3}"/> cube. Then the only Euclidean lines are the kind we know from tic-tac-toe: straight across or down, or diagonal. </p>
<p>
For dimension <img alt="{n \geq 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n \geq 3}"/> there are other kinds of diagonals, such as within a face or through the center of the cube, but they all win at <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/>-dimensional tic-tac-toe. So the problem becomes: what is the maximum number of moves you can make by yourself without creating a win at tic-tac-toe? The <em>cap-set problem</em> adds a twist by extending the notion of what is a <em>line</em>. It is like playing tic-tac-toe on a floor of <img alt="{3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{3 \times 3}"/> tiles where a play in one tile is replicated in all of them. Then you can make a line by playing in a corner and in the middle of the two opposite edges, as shown at left in the following diagram (original drawing).</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2021/02/lineandcapset3x3.png"><img alt="" class="aligncenter wp-image-18231" height="222" src="https://rjlipton.files.wordpress.com/2021/02/lineandcapset3x3.png?w=550&amp;h=222" width="550"/></a></p>
<p>
The four orange O’s at right have no 3-in-a-line even with this extended notion of line. Note that the four blank cells in the top two rows also avoid putting 3 in a line. Four is the maximum, however: it is not possible to have a drawn game in extended <img alt="{3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{3 \times 3}"/> tic-tac-toe. </p>
<p>
The theorem <a href="https://arxiv.org/pdf/1605.09223v1.pdf">proved</a> by Jordan Ellenberg and Dion Gijswijt in 2016 is that the upper bound is not only a vanishing fraction of the size <img alt="{3^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{3^n}"/> of the space as <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/> grows, it is bounded by <img alt="{c^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{c^n}"/> where <img alt="{c &lt; 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{c &lt; 3}"/>. Namely:</p>
<blockquote><p><b>Theorem 1</b> <em> Every cap set in the <img alt="{3^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{3^n}"/>-cube has size at most <img alt="{2.756^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2.756%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2.756^n}"/>. </em>
</p></blockquote>
<p>
</p><h2> Using Polynomials </h2><p/>
<p>There is a simple way to express the extended notion of “line” that works for all dimensions <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/>: Number the coordinates of each dimension <img alt="{0,1,2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2C1%2C2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{0,1,2}"/>. Make the space <img alt="{\{0,1,2\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%2C2%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\{0,1,2\}^n}"/> with addition modulo <img alt="{q = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{q = 3}"/>, that is, make it <img alt="{\mathbb{Z}_3^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_3%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathbb{Z}_3^n}"/>. Then the condition for three points <img alt="{A,B,C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A,B,C}"/> to be in a line is simply </p>
<p align="center"><img alt="\displaystyle  A + B = 2C. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A+%2B+B+%3D+2C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  A + B = 2C. "/></p>
<p>It is easy to write polynomial equations over the field <img alt="{\mathbb{F}_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathbb{F}_3}"/> to express the property of a set having such a line. What was unexpected, until Ernie Croot, Vsevolod Lev, and Péter Pál Pach solved a related problem with <img alt="{q = 4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq+%3D+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{q = 4}"/>, was that there would be</p>
<blockquote><p><b> </b> <em> “an ingeniously simple way to split the polynomial[s] into pieces with smaller exponents, which led to a bound on the size of collections with no [lines].” </em>
</p></blockquote>
<p>
The quotation comes from an <a href="https://www.quantamagazine.org/set-proof-stuns-mathematicians-20160531">article</a> by Erica Klarreich for <em>Quanta</em> right then in 2016. A 2016 AMS Feature <a href="http://www.ams.org/publicoutreach/feature-column/fc-2016-08">column</a> by David Austin covers how to make this say a set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{S}"/> of points is a cap set modulo 3: </p>
<p align="center"><img alt="\displaystyle  S \uplus S \cap 2S = \emptyset, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%5Cuplus+S+%5Ccap+2S+%3D+%5Cemptyset%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  S \uplus S \cap 2S = \emptyset, "/></p>
<p>where we (not Austin) write <img alt="{S \uplus S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Cuplus+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{S \uplus S}"/> to mean the set of sums <img alt="{a + b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a + b}"/> where <img alt="{a,b \in S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb+%5Cin+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a,b \in S}"/> and <img alt="{b \neq a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb+%5Cneq+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{b \neq a}"/>. If there is an element <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r}"/> in the intersection then <img alt="{a + b = r = 2c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b+%3D+r+%3D+2c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a + b = r = 2c}"/>, and since <img alt="{3c = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3c+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{3c = 0}"/>, we get <img alt="{a + b + c = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b+%2B+c+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a + b + c = 0}"/> with <img alt="{a,b,c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a,b,c}"/> in <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{S}"/> and all distinct, a contradiction. (If <img alt="{c = b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{c = b}"/> then <img alt="{a + 2b = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+2b+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a + 2b = 0}"/>, so <img alt="{a = b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a = b}"/>.) Let <img alt="{m_d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{m_d}"/> stand for the number of monomials of degree at most <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d}"/> in the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/> variables. The key first insight is:</p>
<blockquote><p><b>Lemma 2</b> <em> If a polynomial <img alt="{p(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p(x_1,\dots,x_n)}"/> of degree <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d}"/> vanishes on <img alt="{S\uplus S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5Cuplus+S%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{S\uplus S}"/>, then <img alt="{p(x) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%28x%29+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p(x) = 0}"/> for all but at most <img alt="{2m_{d/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2m_%7Bd%2F2%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2m_{d/2}}"/> points of <img alt="{2S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2S%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2S}"/>. </em>
</p></blockquote>
<p>
One could first try to interpret this as saying that <img alt="{S \uplus S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Cuplus+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{S \uplus S}"/> “looks like” <img alt="{2S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2S}"/> to polynomials <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p}"/> of “low” degree <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d}"/>. However, if <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d}"/> stays low relative to <img alt="{|S|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{|S|}"/> then the “if” part would hold vacuously, opposing the goal of bounding <img alt="{|S|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{|S|}"/> and making the whole idea self-defeating. In fact, the important tension comes when <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d}"/> is intermediate: <img alt="{d = (q-1)n/3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd+%3D+%28q-1%29n%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d = (q-1)n/3}"/>, which for <img alt="{q = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{q = 3}"/> makes <img alt="{d/2 = n/3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%2F2+%3D+n%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d/2 = n/3}"/> and <img alt="{d = 2n/3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd+%3D+2n%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d = 2n/3}"/> neatly occupy the middle of the range <img alt="{\{1,\dots,n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cdots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\{1,\dots,n\}}"/>.</p>
<p>
The proof also uses the trick that if a product of two monomials has degree <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d}"/> then one of them must have degree at most <img alt="{\lfloor d/2\rfloor}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clfloor+d%2F2%5Crfloor%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\lfloor d/2\rfloor}"/>. As I (Ken writing these sections) <a href="https://rjlipton.wordpress.com/2016/06/15/polynomial-prestidigitation/">wrote</a> about it back in 2016, this reminds of Roman Smolensky’s degree-halving <a href="https://rjlipton.wordpress.com/2012/03/11/a-note-on-distributions-and-approximation/">trick</a> in his celebrated 1987 <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.883&amp;rep=rep1&amp;type=pdf">theorem</a> on lower bounds for mod-<img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p}"/> versus mod-<img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{q}"/>. This trick, however, runs from <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/> to <img alt="{n/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n/2}"/> for all moduli. </p>
<p>
In any event, the 2016 papers were a new form of the polynomial method that led to striking new results. What Grochow’s survey does for us now is bring out wider implications of this ingenuity.</p>
<p>
</p><h2> Applications in Complexity </h2><p/>
<p>Grochow’s four application areas in section 4 of his survey are:</p>
<ol>
<li>
Progress on various forms of `sunflower’ conjectures. <p/>
</li><li>
Barriers to attempts to show that the exponent of matrix multiplication is <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2}"/>. <p/>
</li><li>
Removing edges to make graphs triangle-free. <p/>
</li><li>
Matrix rigidity and lower bounds.
</li></ol>
<p>
We say a little more about the last of these. For any <img alt="{N \times N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN+%5Ctimes+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{N \times N}"/> matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A}"/> and <img alt="{r \leq N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Cleq+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r \leq N}"/> define the <em>rigidity</em> <img alt="{R_A(r)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_A%28r%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{R_A(r)}"/> to be the minimum number of entries in which <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A}"/> differs from some matrix of rank (at most) <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r}"/>. The highest possible rigidity for rank <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r}"/> is <img alt="{(N - r)^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28N+-+r%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(N - r)^2}"/>, since zeroing out an <img alt="{(n-r)\times(n-r)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28n-r%29%5Ctimes%28n-r%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(n-r)\times(n-r)}"/> block leaves a matrix of rank at most <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r}"/>. Sufficiently random matrices meet this upper bound with high probability, but the best lower bounds for explicit families of matrices are <img alt="{\Omega(\frac{N^2}{r}\log\frac{N}{r})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28%5Cfrac%7BN%5E2%7D%7Br%7D%5Clog%5Cfrac%7BN%7D%7Br%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\Omega(\frac{N^2}{r}\log\frac{N}{r})}"/>, which is only quasi-linear when <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r}"/> is close to <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{N}"/>. The question is whether we can inch this up to <img alt="{\Omega(n^{1+\epsilon})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5E%7B1%2B%5Cepsilon%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\Omega(n^{1+\epsilon})}"/> for some <img alt="{\epsilon &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\epsilon &gt; 0}"/>.</p>
<blockquote><p><b>Definition 3</b> <em> A family of matrices <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A_N}"/> is <em>significantly rigid</em> if there is an <img alt="{\epsilon &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\epsilon &gt; 0}"/> such that taking <img alt="{r = \frac{N}{\log\log N}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+%5Cfrac%7BN%7D%7B%5Clog%5Clog+N%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r = \frac{N}{\log\log N}}"/> makes <img alt="{R_{A_N}(r) = \Omega(N^{1+\epsilon})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_%7BA_N%7D%28r%29+%3D+%5COmega%28N%5E%7B1%2B%5Cepsilon%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{R_{A_N}(r) = \Omega(N^{1+\epsilon})}"/>. </em>
</p></blockquote>
<p>
The interest in this definition comes from a lack of lower bounds on linear algebraic circuits computing natural families <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A_N}"/> of linear transformations that seems even more extreme than our lack of super-linear lower bounds on Boolean circuits, nor better than <img alt="{\Omega(N\log N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28N%5Clog+N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\Omega(N\log N)}"/> for general algebraic circuits computing polynomials in <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{N}"/> variables of degree <img alt="{B^{O(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{B^{O(1)}}"/>. It is still consistent with our knowledge that every natural family <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A_N}"/> can be computed by linear algebraic circuits of <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{O(N)}"/> size <b>and</b> <img alt="{O(\log N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{O(\log N)}"/> depth. Leslie Valiant in 1977 proved the following sufficient condition to improve this state of affairs.</p>
<blockquote><p><b>Theorem 4</b> <em> Every significantly rigid family <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A_N}"/> cannot be computed by linear algebraic circuits of linear size and logarithmic depth. </em>
</p></blockquote>
<p>
So for coming on half a century the question has been:</p>
<blockquote><p><b> </b> <em> Can we construct a natural explicit family of significantly rigid matrices? </em>
</p></blockquote>
<p>
Beliefs that the Hadamard matrices provided such a family were <a href="https://arxiv.org/pdf/1611.05558.pdf">refuted</a> by Josh Alman and Ryan Williams at STOC 2017, and <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.3100">known</a> <a href="https://core.ac.uk/download/pdf/82556808.pdf">results</a> for Vandermonde matrices do not have <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r}"/> close enough to <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/>. </p>
<p>
One hope had been to derive such matrices from explicit functions <img alt="{f_n(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_n%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f_n(x_1,\dots,x_n)}"/> over <img alt="{\mathbb{Z}_p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathbb{Z}_p}"/> for <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p}"/> prime by taking <img alt="{N = p^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+p%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{N = p^n}"/> and defining </p>
<p align="center"><img alt="\displaystyle  A_{f_n}[\vec{x},\vec{y}] = f(x_1 + y_1,\dots,x_n + y_n). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A_%7Bf_n%7D%5B%5Cvec%7Bx%7D%2C%5Cvec%7By%7D%5D+%3D+f%28x_1+%2B+y_1%2C%5Cdots%2Cx_n+%2B+y_n%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  A_{f_n}[\vec{x},\vec{y}] = f(x_1 + y_1,\dots,x_n + y_n). "/></p>
<p>Unfortunately, the polynomial method for cap sets shows that no such attempt can work. Zeev Dvir and Benjamin Edelman <a href="https://theoryofcomputing.org/articles/v015a008/">proved</a> that no matter how <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f}"/> and <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\epsilon}"/> are chosen, there is <img alt="{\delta &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\delta &gt; 0}"/> such that for all large enough <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/>, </p>
<p align="center"><img alt="\displaystyle  R_{A_{f_n}}(N^{1-\delta}) &lt; n^{1+\epsilon}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R_%7BA_%7Bf_n%7D%7D%28N%5E%7B1-%5Cdelta%7D%29+%3C+n%5E%7B1%2B%5Cepsilon%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  R_{A_{f_n}}(N^{1-\delta}) &lt; n^{1+\epsilon}. "/></p>
<p>This means we cannot get <img alt="{R_{A_{f_n}}(r) = \Omega(n^{1+\Omega(1)})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_%7BA_%7Bf_n%7D%7D%28r%29+%3D+%5COmega%28n%5E%7B1%2B%5COmega%281%29%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{R_{A_{f_n}}(r) = \Omega(n^{1+\Omega(1)})}"/> for <img alt="{r = \frac{N}{\log\log N}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+%5Cfrac%7BN%7D%7B%5Clog%5Clog+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{r = \frac{N}{\log\log N}}"/>, indeed, far from it. What is most curious to us is that for matrix multiplication, the cap-set related technique frustrates a better complexity upper bound, whereas here it frustrates a better lower bound.</p>
<p>
</p><h2> Open Problems </h2><p/>
<p>What further applications can we find for the polynomial method?</p>
<p/></font></font></div>
    </content>
    <updated>2021-02-27T20:10:57Z</updated>
    <published>2021-02-27T20:10:57Z</published>
    <category term="algorithms"/>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="Teaching"/>
    <category term="trick"/>
    <category term="algebraic complexity"/>
    <category term="cap sets"/>
    <category term="circuits"/>
    <category term="complexity"/>
    <category term="Dion Gijswijt"/>
    <category term="Ernie Croot"/>
    <category term="Henry Dudeney"/>
    <category term="Jordan Ellenberg"/>
    <category term="Joshua Grochow"/>
    <category term="Ketan Mulmuley"/>
    <category term="Leslie Valiant"/>
    <category term="lower bounds"/>
    <category term="no-three-in-row problem"/>
    <category term="Peter Pach"/>
    <category term="polynomial method"/>
    <category term="Stephen Mahaney"/>
    <category term="Vsevolod Lev"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-03-04T03:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/028</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/028" rel="alternate" type="text/html"/>
    <title>TR21-028 |  Branching Programs with Bounded Repetitions and $\mathrm{Flow}$ Formulas | 

	Anastasia Sofronova, 

	Dmitry Sokolov</title>
    <summary>Restricted branching programs capture various complexity measures like space in Turing machines or length of proofs in proof systems. In this paper, we focus on the application in the proof complexity that was discovered by Lovasz et al. '95 who showed the equivalence between regular Resolution and read-once branching programs for ``unsatisfied clause search problem'' ($\mathrm{Search}_{\varphi}$). This connection is widely used, in particular, in the recent breakthrough result about the Clique problem in regular Resolution by Atserias et al. '18.

We study the branching programs with bounded repetitions, so-called $(1, +k)$-BPs (Sieling '96) in application to the $\mathrm{Search}_{\varphi}$ problem. On the one hand, it is a natural generalization of read-once branching programs. On the other hand, this model gives a powerful proof system that can efficiently certify the unsatisfiability of a wide class of formulas that is hard for Resolution (Knop '17).


We deal with $\mathrm{Search}_{\varphi}$ that is ``relatively easy'' compared to all known hard examples for the $(1, +k)$-BPs. We introduce the first technique for proving exponential lower bounds for the $(1, +k)$-BPs on $\mathrm{Search}_{\varphi}$. To do it we combine a well-known technique for proving lower bounds on the size of branching programs (Sieling '96; Sieling, Wegener '94; Jukna, Razborov '98) with the modification of the ``closure'' technique (Alekhnovich et al. 04; Alekhnovich, Razborov '03). In contrast with the most Resolution lower bounds, our technique uses not only ``local'' properties of the formula, but also a ``global'' structure. Our hard examples are based on the $\mathrm{Flow}$ formulas introduced in (Alekhnovich, Razborov '03).</summary>
    <updated>2021-02-27T19:40:44Z</updated>
    <published>2021-02-27T19:40:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-04T03:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/26/phd-position-at-university-of-amsterdam-apply-by-march-18-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/26/phd-position-at-university-of-amsterdam-apply-by-march-18-2021/" rel="alternate" type="text/html"/>
    <title>PhD position at University of Amsterdam (apply by March 18, 2021)</title>
    <summary>The University of Amsterdam encourages applications for an open PhD position in the theory of quantum computing and quantum networks. Potential research topics include multi-party quantum computation, secure positioning, and multi-party communication complexity. Website: https://www.uva.nl/shared-content/uva/en/vacancies/2021/02/21-069-phd-position-on-the-theory-of-quantum-networks.html Email: f.speelman@uva.nl</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The University of Amsterdam encourages applications for an open PhD position in the theory of quantum computing and quantum networks. Potential research topics include multi-party quantum computation, secure positioning, and multi-party communication complexity.</p>
<p>Website: <a href="https://www.uva.nl/shared-content/uva/en/vacancies/2021/02/21-069-phd-position-on-the-theory-of-quantum-networks.html">https://www.uva.nl/shared-content/uva/en/vacancies/2021/02/21-069-phd-position-on-the-theory-of-quantum-networks.html</a><br/>
Email: f.speelman@uva.nl</p></div>
    </content>
    <updated>2021-02-26T10:38:43Z</updated>
    <published>2021-02-26T10:38:43Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-04T03:20:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/26/faculty-at-krea-university-india-apply-by-may-1-2021-2/</id>
    <link href="https://cstheory-jobs.org/2021/02/26/faculty-at-krea-university-india-apply-by-may-1-2021-2/" rel="alternate" type="text/html"/>
    <title>Faculty at KREA University, India (apply by May 1, 2021)</title>
    <summary>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science. Open House, 27th Feb 2021 9:00 AM [IST]: https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf Website: https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d Email: sias.chair_sciences@krea.edu.in</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science.</p>
<p>Open House, 27th Feb 2021 9:00 AM [IST]:<br/>
<a href="https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf">https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf</a></p>
<p>Website: <a href="https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d">https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d</a><br/>
Email: sias.chair_sciences@krea.edu.in</p></div>
    </content>
    <updated>2021-02-26T08:49:00Z</updated>
    <published>2021-02-26T08:49:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-04T03:20:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/25/tenure-track-open-rank-at-university-of-illinois-urbana-champaign-apply-by-june-1-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/25/tenure-track-open-rank-at-university-of-illinois-urbana-champaign-apply-by-june-1-2021/" rel="alternate" type="text/html"/>
    <title>Tenure Track (Open Rank) at University of Illinois, Urbana-Champaign (apply by June 1, 2021)</title>
    <summary>The Department of Computer Science at the University of Illinois Urbana-Champaign invites applications for full-time tenure-track faculty positions at all levels (Assistant Professor, Associate Professor, Full Professor). We particularly encourage applications in quantum computing, but also welcome applications from exceptional candidates in other areas. Website: https://cs.illinois.edu/about/positions/faculty-positions Email: chekuri@illinois.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Illinois Urbana-Champaign invites applications for full-time tenure-track faculty positions at all levels (Assistant Professor, Associate Professor, Full Professor). We particularly encourage applications in quantum computing, but also welcome applications from exceptional candidates in other areas.</p>
<p>Website: <a href="https://cs.illinois.edu/about/positions/faculty-positions">https://cs.illinois.edu/about/positions/faculty-positions</a><br/>
Email: chekuri@illinois.edu</p></div>
    </content>
    <updated>2021-02-25T22:59:03Z</updated>
    <published>2021-02-25T22:59:03Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-04T03:20:52Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2202248828009562800</id>
    <link href="https://blog.computationalcomplexity.org/feeds/2202248828009562800/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/complexity-is-enemy-of-speed.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2202248828009562800" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2202248828009562800" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/complexity-is-enemy-of-speed.html" rel="alternate" type="text/html"/>
    <title>Complexity is the Enemy of Speed</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The title of this post came from an <a href="https://www.wsj.com/articles/connecticuts-covid-vaccine-lesson-11614124012">opinion piece</a> in the Wall Street Journal yesterday on vaccine distribution. Many attempts to get the vaccines to the right groups first have slowed down distribution and sometime even caused <a href="https://www.nbcnews.com/news/us-news/thousands-covid-19-vaccines-wind-garbage-because-fed-state-guidelines-n1254364">vaccines to go to waste</a>. Rules to help spread vaccines across minority groups often backfire. Often when some rules lead to inequity, we try to fix it with more rules when we need less much less. Attempts to distribute vaccines to multiple medical and pharmacy sites have made it difficult to get appointments even if you are eligible.</p><p>Randomness is the simplest way to fairness. The movie Contagion got it right, just choose birthdays by picking balls from a bin to distribute the vaccine. Then people can just show up at a few chosen sites with proof of birthday. No need to sign up.</p><p>You could argue to add back conditions like age, medical conditions, jobs but that just leads you down the same problematic path. The fastest way to get past this pandemic is to get vaccines into arms. Trust the randomness.</p></div>
    </content>
    <updated>2021-02-25T14:19:00Z</updated>
    <published>2021-02-25T14:19:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-03-04T02:39:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/25/faculty-at-krea-university-india-apply-by-may-1-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/25/faculty-at-krea-university-india-apply-by-may-1-2021/" rel="alternate" type="text/html"/>
    <title>Faculty at KREA University, India (apply by May 1, 2021)</title>
    <summary>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science. Open House, 27th Feb 2021 9:00 AM [IST]: https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf Website: https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d Email: sias.chair_sciences@krea.edu.in</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science.</p>
<p>Open House, 27th Feb 2021 9:00 AM [IST]:<br/>
<a href="https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf">https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf</a></p>
<p>Website: <a href="https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d">https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d</a><br/>
Email: sias.chair_sciences@krea.edu.in</p></div>
    </content>
    <updated>2021-02-25T09:47:05Z</updated>
    <published>2021-02-25T09:47:05Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-04T03:20:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=532</id>
    <link href="https://tcsplus.wordpress.com/2021/02/24/tcs-talk-wednesday-march-3-steve-hanneke-ttic/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, March 3 — Steve Hanneke, TTIC</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, March 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Steve Hanneke from TTIC will speak about “A Theory of Universal Learning” (abstract below). You can reserve a spot as an individual or a group to join us […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, March 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Steve Hanneke</strong> from TTIC will speak about “<em>A Theory of Universal Learning</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> aftwerwards, so people who did not sign up will still be able to watch the talk.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its “learning curve”, that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the PAC model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free PAC model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically fixed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy.</p>
<p>In this work, we study an alternative learning model that better captures such practical aspects of machine learning, but still gives rise to a complete theory of the learnable in the spirit of the PAC model. More precisely, we consider the problem of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring uniformity over the distribution. The main result of this work is a remarkable trichotomy: there are only three possible rates of universal learning. More precisely, we show that the learning curves of any given concept class decay either at an exponential, linear, or arbitrarily slow rates. Moreover, each of these cases is completely characterized by appropriate combinatorial parameters, and we exhibit optimal learning algorithms that achieve the best possible rate in each case.</p>
<p>Joint work with Olivier Bousquet, Shay Moran, Ramon van Handel, and Amir Yehudayoff.</p></blockquote></div>
    </content>
    <updated>2021-02-25T02:03:43Z</updated>
    <published>2021-02-25T02:03:43Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-03-04T03:21:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8008</id>
    <link href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/" rel="alternate" type="text/html"/>
    <title>Unsupervised Learning and generative models</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Scribe notes by Richard Xu Previous post: What do neural networks learn and when do they learn it Next post: TBD. See also all seminar posts and course webpage. lecture slides (pdf) – lecture slides (Powerpoint with animation and annotation) – video In this lecture, we move from the world of supervised learning to unsupervised … <a class="more-link" href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">Continue reading <span class="screen-reader-text">Unsupervised Learning and generative models</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Scribe notes by <a href="https://github.com/rxu18">Richard Xu</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/">What do neural networks learn and when do they learn it</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/?order=asc">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_3.pdf">lecture slides (pdf)</a> – <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_3.pptx">lecture slides (Powerpoint with animation and annotation)</a> – <a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=70cafab0-bdea-412b-a353-acc90173fd61">video</a></p>



<p>In this lecture, we move from the world of supervised learning to unsupervised learning, with a focus on generative models. We will</p>



<ul><li>Introduce unsupervised learning and the relevant notations.</li><li>Discuss various approaches for generative models, such as PCA, VAE, Flow Models, and GAN.</li><li>Discuss theoretical and practical results we currently have for these approaches.</li></ul>



<h2>Setup for Unsupervised Learning</h2>



<p>In <em>supervised learning</em>, we have data <img alt="x_i\sim p\subset \mathbb R^d" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%5Csim+p%5Csubset+%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i\sim p\subset \mathbb R^d"/> and we want to understand the distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>. For example,</p>



<ol><li><em>Probability estimation:</em> Given <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, can we compute/approximate <img alt="p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)"/> (the probability that <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is output under <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>)?</li><li><em>Generation:</em> Can we sample from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>, or from a “nearby” distribution?</li><li><em>Encoding:</em> Can we find a representation <img alt="E:\mathrm{Support}(p) \rightarrow \mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathrm%7BSupport%7D%28p%29+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathrm{Support}(p) \rightarrow \mathbb{R}^r"/> such that for <img alt="x \sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x \sim p"/>, <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> makes it easy to answer semantic questions on <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>? And such that <img alt="\langle E(x) , E(x') \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+E%28x%29+%2C+E%28x%27%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle E(x) , E(x') \rangle"/> corresponds to “semantic similarity” of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> and <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x'"/>?</li><li><em>Prediction:</em> We would like to be able to predict (for example) the second half of <img alt="x \sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x \sim p"/> from the first half. More generally, we want to solve the <em>conditional generation</em> task, where given some function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> (e.g., the projection to the first half) and some value <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/>, we can sample from the conditional probability distribution <img alt="p|f(x)=y" class="latex" src="https://s0.wp.com/latex.php?latex=p%7Cf%28x%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p|f(x)=y"/>.</li></ol>



<p>Our “dream” is to solve all of those by the following setup:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/EPyXsSW.png"/></figure>



<p>There is an “encoder” <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> that maps <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> into a representation <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> in the latent space, and then a “decoder” <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> that can transform such a representation back into <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. We would like it to be the case that:</p>



<ol><li><em>Generation:</em> For <img alt="x\sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim p"/>, the induced distribution <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> is “nice” and efficiently sampleable (e.g., the standard normal <img alt="N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,I)"/> over <img alt="\mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{R}^r"/>) such that we can (approximately) sample from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> by sampling <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> and outputting <img alt="D(z)" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)"/>.</li><li><em>Density estimation:</em> We would like to be able to evaluate the probability that <img alt="D(z)=x" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)=x"/>. For example, if <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> is the inverse of <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/>, and <img alt="z \sim N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Csim+N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z \sim N(0,I)"/> we could do so by computing <img alt="| E(x) |" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+E%28x%29+%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| E(x) |"/>.</li><li><em>Semantic representation:</em> We would like the latent representation <img alt="E(z)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(z)"/> to map <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> into meaningful latent space. Ideally, linear directions in this space will correspond to semantic attributes.</li><li><em>Conditional sampling:</em> We would like to be able to do conditional generation, and in particular for some functions <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> and values <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/>, be able to sample from the set of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/>‘s such that <img alt="f(E(z))=y" class="latex" src="https://s0.wp.com/latex.php?latex=f%28E%28z%29%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(E(z))=y"/></li></ol>



<p>Ideally, if we could map images to the latent variables used to generate them and vice versa (as in the cartoon from the last lecture), then we could achieve these goals:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/3X4aqfl.png"/></figure>



<p>At the moment, we do not have a single system that can solve all these problems for a natural domain such as images or language, but we have several approaches that achieve part of the dream.</p>



<p><strong>Digressions.</strong> Before discussing concrete models, we make three digressions. One will be non-technical, and the other three technical. The three technical digressions are the following:</p>



<ol><li>If we have multiple objectives, we want a way to interpolate between them.</li><li>To measure how good our models are, we have to measure distances between statistical distributions.</li><li>Once we come up with generating models, we would <em>metrics</em> for measuring how good they are.</li></ol>



<h2>Non-technical digression: Is deep learning a cargo cult science? (spoiler: no)</h2>



<p>In an <a href="https://calteches.library.caltech.edu/51/2/CargoCult.htm">influential essay</a>, Richard Feynman coined the term “cargo cult science” for the activities that have superficial similarities to science but do not follow the scientific method. Some of the tools we use in machine learning look suspiciously close to “cargo cult science.” We use the tools of classical learning, but in a setting in which they were not designed to work in and on which we have no guarantees that they will work. For example, we run (stochastic) gradient descent – an algorithm designed to minimize a convex function – to minimize convex loss. We also write use <em>empirical risk minimization</em> – minimizing loss on our training set – in a setting where we have no guarantee that it will not lead to “overfitting.”</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/tBw6UsX.png"/></figure>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/s5G6xfj.png"/></figure>



<p>And yet, unlike the original cargo cults, in deep learning, “the planes do land”, or at least they often do. When we use a tool <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> in a situation <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X"/> that it was not designed to work in, it can play out in one (or mixture) of the following scenarios:</p>



<ul><li><strong>Murphy’s Law:</strong> “Anything that can go wrong will go wrong.” As computer scientists, we are used to this scenario. The natural state of our systems is that they have bugs and errors. There is a reason why software engineering talks about “contracts”, “invariants”, preconditions” and “postconditions”: typically, if we try to use a component <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> in a situation that it wasn’t designed for, it will not turn out well. This is doubly the case in security and cryptography, where people have learned the hard way time and again that Murphy’s law holds sway.</li><li><strong>“Marley’s Law”:</strong> “Every little thing gonna be alright”. In machine learning, we sometimes see the opposite phenomenon- we use algorithms outside the conditions under which they have been analysed or designed to work in, but they still produce good results. Part of it could be because ML algorithms are already robust to certain errors in their inputs, and their output was only guaranteed to be approximately correct in the first place.</li></ul>



<p>Murphy’s law does occasionally pop up, even in machine learning. We will see examples of both phenomena in this lecture.</p>



<h2>Technical digression 1: Optimization with Multiple Objectives</h2>



<p>During machine learning, we often have multiple objectives to optimize. For example, we may want both an efficient encoder and an effective decoder, but there is a tradeoff between them.</p>



<p>Suppose we have 2 loss functions <img alt="L_1(w)" class="latex" src="https://s0.wp.com/latex.php?latex=L_1%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1(w)"/> and <img alt="L_2(w)" class="latex" src="https://s0.wp.com/latex.php?latex=L_2%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_2(w)"/>, but there can be a trade off between them. The <em>pareto curve</em> is the set <img alt="P={(a,b): \forall w\in W, L_1(w)\ge a\vee L_2(w)\ge b.}" class="latex" src="https://s0.wp.com/latex.php?latex=P%3D%7B%28a%2Cb%29%3A+%5Cforall+w%5Cin+W%2C+L_1%28w%29%5Cge+a%5Cvee+L_2%28w%29%5Cge+b.%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P={(a,b): \forall w\in W, L_1(w)\ge a\vee L_2(w)\ge b.}"/></p>



<figure class="wp-block-image"><img alt="Pareto curve for 2 loss functions" src="https://i.imgur.com/QbPRQtR.jpg"/></figure>



<p>If a model is above the curve, it is not optimal. If it is below the curve, the model is infeasible.</p>



<p>When the set <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> is convex, we can reach any point on the curve <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> by minimizing <img alt="L_1(w)+\lambda L_2(w)" class="latex" src="https://s0.wp.com/latex.php?latex=L_1%28w%29%2B%5Clambda+L_2%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1(w)+\lambda L_2(w)"/>. The proof is by the picture above: for any point <img alt="(a_0,b_0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(a_0,b_0)"/> on the curve, there is a tangent line at <img alt="(a_0,b_0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(a_0,b_0)"/> that is strictly below the curve. If <img alt="a+\lambda b" class="latex" src="https://s0.wp.com/latex.php?latex=a%2B%5Clambda+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="a+\lambda b"/> is the normal vector for this line, then the global minimum of <img alt="a+\lambda b" class="latex" src="https://s0.wp.com/latex.php?latex=a%2B%5Clambda+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="a+\lambda b"/> on the feasible set will be <img alt="(a_0,b_0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(a_0,b_0)"/>.<br/>This motivates the common practice of minimizing two introducing a hyperparameter <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda"/> to aggregate two objectives into one.</p>



<p>When <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> is not convex, it may well be that:</p>



<ul><li>Some points on <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> are not minima of <img alt="L_1 + \lambda L_2" class="latex" src="https://s0.wp.com/latex.php?latex=L_1+%2B+%5Clambda+L_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1 + \lambda L_2"/></li><li><img alt="L_1 + \lambda L_2" class="latex" src="https://s0.wp.com/latex.php?latex=L_1+%2B+%5Clambda+L_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1 + \lambda L_2"/> might have multiple minima</li><li>Depending on the path one takes, it is possible to get “stuck” in a point that is <em>not</em> a global minima</li></ul>



<p>The following figure demonstrates all three possibilities</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/Rjg4iZU.png"/></figure>



<p>Par for the course, this does not stop people in machine learning from using this approach to minimize different objectives, and often “Marley’s Law” holds, and this works fine. But this is not always the case. A <a href="https://engraved.ghost.io/why-machine-learning-algorithms-are-hard-to-tune/">nice blog post by Degrave and Kurshonova</a> discusses this issue and why sometimes we do in fact, see “Murphy’s law” when we combine objectives. They also detail some other approaches for combining objectives, but there is no single way that will work in all cases.</p>



<p>Figure from Degrave-Kurshonova demonstrating where the algorithm could reach in the non-convex case depending on initialization and <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda"/>:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/4VZZaRR.gif"/></figure>



<h2>Technical digression 2: Distances between probability measures</h2>



<p>Suppose we have two distributions <img alt="p,q" class="latex" src="https://s0.wp.com/latex.php?latex=p%2Cq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p,q"/> over <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/>. There are two common ways of measuring the distances between them.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/5JTzg6D.png"/></figure>



<p>The <em>Total Variance (TV)</em> (also known as statistical distance) between <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is equal to</p>



<p><img alt="\Delta_{TV}(p,q)=\frac12 \sum_{x\in D}|p(x)-q(x)| = \max_{f:D\to {0,1}} | \mathbb{E}_p(f)-\mathbb{E}_q(f)|." class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29%3D%5Cfrac12+%5Csum_%7Bx%5Cin+D%7D%7Cp%28x%29-q%28x%29%7C+%3D+%5Cmax_%7Bf%3AD%5Cto+%7B0%2C1%7D%7D+%7C+%5Cmathbb%7BE%7D_p%28f%29-%5Cmathbb%7BE%7D_q%28f%29%7C.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q)=\frac12 \sum_{x\in D}|p(x)-q(x)| = \max_{f:D\to {0,1}} | \mathbb{E}_p(f)-\mathbb{E}_q(f)|."/></p>



<p>The second equality can be proved by constructing <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> that outputs 1 on <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> where <img alt="p(x)-q(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29-q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)-q(x)"/> and vice versa. The <img alt="\max_f" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\max_f"/> definition has a crypto-flavored interpretation: For any adversary <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>, the TV measures the advantage they can have over half of determining whether <img alt="x\sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim p"/> or <img alt="x\sim q" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim q"/>.</p>



<p>Second, the <em>Kullback–Leibler (KL) Divergence</em> between <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is equal to</p>



<p><img alt="\Delta_{KL}(p||q)=\mathbb{E}_{x\sim p}(\log p(x)/q(x))." class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%3D%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+p%28x%29%2Fq%28x%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)=\mathbb{E}_{x\sim p}(\log p(x)/q(x))."/></p>



<p>(The total variation distance is symmetric, in the sense that <img alt="\Delta_{TV}(p,q)=\Delta_{TV}(q,p)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29%3D%5CDelta_%7BTV%7D%28q%2Cp%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q)=\Delta_{TV}(q,p)"/>, but the KL divergence is not. Both have the property that they are non-negative and equal to zero if and only if <img alt="p=q" class="latex" src="https://s0.wp.com/latex.php?latex=p%3Dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=q"/>.)</p>



<p>Unlike the total variation distance, which is bounded between <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> and <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/>, the KL divergence can be arbitrarily large and even infinite (though it can be shown using the concavity of log that it is always non-negative). To interpret the KL divergence, it is helpful to separate between the case that <img alt="\Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)"/> is close to zero and the case where it is a large number. If <img alt="\Delta_{KL}(p||q) \approx \delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx \delta"/> for some <img alt="\delta \ll 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cll+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta \ll 1"/>, then we would need about <img alt="1/\delta" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\delta"/> samples to distinguish between samples of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and samples of <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>. In particular, suppose that we get <img alt="x_1,\ldots,x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_n"/> and we want to distinguish between the case that we they were independently sampled from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and the case that they were independently sampled from <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>. A natural (and as it turns out, optimal) approach is to use a <em>likelihood ratio test</em> where we decide the samples came from <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T"/> if <img alt="\Pr_p[x_1,\ldots,x_n]/\Pr_q[x_1,\ldots,x_n]&gt;T" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr_p%5Bx_1%2C%5Cldots%2Cx_n%5D%2F%5CPr_q%5Bx_1%2C%5Cldots%2Cx_n%5D%3ET&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Pr_p[x_1,\ldots,x_n]/\Pr_q[x_1,\ldots,x_n]&gt;T"/>. For example, if we set <img alt="T=20" class="latex" src="https://s0.wp.com/latex.php?latex=T%3D20&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T=20"/> then this approach will guarantee that our “false positive rate” (announcing that samples came from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> when they really came from <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>) will be most <img alt="1/20=5\%" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F20%3D5%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/20=5\%"/>. Taking logs and using the fact that the probability of these independent samples is the product of probabilities, this amounts to testing whether <img alt="\sum_{i=1}^n \log \left(\tfrac{p(x_i)}{q(x_i)}\right) \geq \log T" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5En+%5Clog+%5Cleft%28%5Ctfrac%7Bp%28x_i%29%7D%7Bq%28x_i%29%7D%5Cright%29+%5Cgeq+%5Clog+T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_{i=1}^n \log \left(\tfrac{p(x_i)}{q(x_i)}\right) \geq \log T"/>. When samples come from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>, the expectation of the righthand side is <img alt="n\cdot \Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ccdot+%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n\cdot \Delta_{KL}(p||q)"/>, so we see that to ensure <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T"/> is larger than <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/> we need the number samples to be at least <img alt="1/\Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\Delta_{KL}(p||q)"/> (and as it turns out, this will do).</p>



<p>When the <img alt="KL" class="latex" src="https://s0.wp.com/latex.php?latex=KL&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="KL"/> divergence is a large number <img alt="k&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=k%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k&gt;1"/>, we can think of it as the number of bits of “surprise” in <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> as opposed to <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>. For example, in the common case where <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is obtained by conditioning <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> on some event <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/>, <img alt="\Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)"/> will typically be <img alt="\log 1/\Pr[A]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+1%2F%5CPr%5BA%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log 1/\Pr[A]"/> (some fine print applies). In general, if <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is obtained from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> by revealing <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k"/> bits of information (i.e., by conditioning on a random variable whose mutual information with <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> is <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k"/>) then <img alt="\Delta_{KL}(p||q)=k" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%3Dk&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)=k"/>.</p>



<p><strong>Generalizations:</strong> The total variation distance is a special case of metrics of the form <img alt="\Delta(p,q) = \max_{f \in \mathcal{F}} |\mathbb{E}_{x\sim p} f(x) - \mathbb{E}_{x \sim q} f(x)|" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta%28p%2Cq%29+%3D+%5Cmax_%7Bf+%5Cin+%5Cmathcal%7BF%7D%7D+%7C%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D+f%28x%29+-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+f%28x%29%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta(p,q) = \max_{f \in \mathcal{F}} |\mathbb{E}_{x\sim p} f(x) - \mathbb{E}_{x \sim q} f(x)|"/>. These are known as <a href="https://arxiv.org/abs/0901.2698">integral probability metrics</a> and include examples such as the Wasserstein distance, Dudley metric, and Maximum Mean Discrepancy. KL divergence is a special case of divergence measures known as <a href="https://en.wikipedia.org/wiki/F-divergence"><img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>-divergence</a>, which are measures of the form <img alt="\Delta_f(p||q)= \mathbb{E}_{x \sim q} f\left(\tfrac{p(x)}{q(x)}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_f%28p%7C%7Cq%29%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+f%5Cleft%28%5Ctfrac%7Bp%28x%29%7D%7Bq%28x%29%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_f(p||q)= \mathbb{E}_{x \sim q} f\left(\tfrac{p(x)}{q(x)}\right)"/>. The KL divergence is obtained by setting <img alt="f(t) = t \log t" class="latex" src="https://s0.wp.com/latex.php?latex=f%28t%29+%3D+t+%5Clog+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(t) = t \log t"/>. (In fact even the TV distance is a special case of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> divergence by setting <img alt="f(t)=|t-1|/2" class="latex" src="https://s0.wp.com/latex.php?latex=f%28t%29%3D%7Ct-1%7C%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(t)=|t-1|/2"/>.)</p>



<p><strong>Normal distributions:</strong> It is a useful exercise to calculate the TV and KL distances for normal random variables. If <img alt="p=N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3DN%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=N(0,1)"/> and <img alt="q=N(-\epsilon,1)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%28-%5Cepsilon%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(-\epsilon,1)"/>, then since most probability mass in the regime where <img alt="p(x) \approx (1\pm \epsilon) q(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Capprox+%281%5Cpm+%5Cepsilon%29+q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x) \approx (1\pm \epsilon) q(x)"/>, <img alt="\Delta_{TV}(p,q) \approx \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29+%5Capprox+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q) \approx \epsilon"/> (i.e., up to some multiplicative constant). For KL divergence, if we selected <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> from a normal between <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> then with probability about half we’ll have <img alt="p(x) \approx q(x)(1+\epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Capprox+q%28x%29%281%2B%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x) \approx q(x)(1+\epsilon)"/> and with probability about half we will have <img alt="p(q) \approx q(x)(1-\epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28q%29+%5Capprox+q%28x%29%281-%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(q) \approx q(x)(1-\epsilon)"/>. By selecting <img alt="x\sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim p"/>, we increase probability of the former to <img alt="\approx 1/2+\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+1%2F2%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\approx 1/2+\epsilon"/> and the decrease the probability of the latter to <img alt="\approx 1/2 - \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+1%2F2+-+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\approx 1/2 - \epsilon"/>. So we have <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\epsilon"/> bias towards <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>‘s where <img alt="p(x)/q(x) \approx 1+\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29%2Fq%28x%29+%5Capprox+1%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)/q(x) \approx 1+\epsilon"/>, or <img alt="\log p(x)/q(x) \approx \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+p%28x%29%2Fq%28x%29+%5Capprox+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log p(x)/q(x) \approx \epsilon"/>. Hence <img alt="\Delta_{KL}(p||q) \approx \epsilon^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx \epsilon^2"/>. The above generalizes to higher dimensions. If <img alt="p= N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3D+N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p= N(0,I)"/> is a <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/>-variate normal, and <img alt="q=N(\mu,I)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%28%5Cmu%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(\mu,I)"/> for <img alt="\mu \in \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mu \in \mathbb{R}^d"/>, then (for small <img alt="|\mu|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cmu%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="|\mu|"/>) <img alt="\Delta_{TV}(p,q) \approx |\mu|" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29+%5Capprox+%7C%5Cmu%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q) \approx |\mu|"/> while <img alt="\Delta_{KL}(p||q)\approx |\mu|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%5Capprox+%7C%5Cmu%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)\approx |\mu|^2"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/w3aXr99.png"/></figure>



<p>If <img alt="p=N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3DN%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=N(0,1)"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is a “narrow normal” of the form <img alt="q=N(0,\epsilon^2)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%280%2C%5Cepsilon%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(0,\epsilon^2)"/> then their TV distance is close to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/> while <img alt="\Delta_{KL}(p||q) \approx 1/\epsilon^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+1%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx 1/\epsilon^2"/>. In the <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> dimensional case, if <img alt="p=N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=N(0,I)"/> and <img alt="q=N(0,V)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%280%2CV%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(0,V)"/> for some covariance matrix <img alt="V" class="latex" src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="V"/>, then <img alt="\Delta_{KL}(p||q) \approx \mathrm{Tr}(V^{-1}) - d + \ln \det V" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cmathrm%7BTr%7D%28V%5E%7B-1%7D%29+-+d+%2B+%5Cln+%5Cdet+V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx \mathrm{Tr}(V^{-1}) - d + \ln \det V"/>. The two last terms are often less significant. For example if <img alt="V = \epsilon^2 I" class="latex" src="https://s0.wp.com/latex.php?latex=V+%3D+%5Cepsilon%5E2+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="V = \epsilon^2 I"/> then <img alt="\delta_{KL}(p||q) \approx d/\epsilon^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+d%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta_{KL}(p||q) \approx d/\epsilon^2"/>.</p>



<h2>Technical digression 3: benchmarking generative models</h2>



<p>Given a distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> of natural data and a purported generative model <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/>, how do we measure the quality of <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/>?</p>



<p>A natural measure is the KL divergence <img alt="\Delta_{KL}(p||g)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||g)"/> but it can be hard to evaluate, since it involves the term <img alt="p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)"/> which we cannot evaluate. However, we can rewrite the KL divergence as <img alt="\mathbb{E}_{x\sim p}(\log p(x)) - \mathbb{E}_{x\sim p}(\log q(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+p%28x%29%29+-+%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+q%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_{x\sim p}(\log p(x)) - \mathbb{E}_{x\sim p}(\log q(x))"/>. The term <img alt="\mathbb{E}{x\sim p} \log p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%7Bx%5Csim+p%7D+%5Clog+p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}{x\sim p} \log p(x)"/> is equal to <img alt="-H(p)" class="latex" src="https://s0.wp.com/latex.php?latex=-H%28p%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-H(p)"/> where <img alt="H(p)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p)"/> is the <em>entropy</em> of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>. The term <img alt="-\mathbb{E}_{x \sim p} \log q(x)" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+%5Clog+q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\mathbb{E}_{x \sim p} \log q(x)"/> is known as the <em>cross entropy </em>of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>. Note that the cross-entropy of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/> is simply the expectation of the negative log likelihood of <img alt="g(x)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g(x)"/> for <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> sampled from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>.</p>



<p>When <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> is fixed, minimizing <img alt="\Delta_{KL}(p||g)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||g)"/> corresponds to minimizing the cross entropy <img alt="H(p,g)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%2Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p,g)"/> or equivalently, maximizing the log likelihood. This is useful since often is the case that we can sample elements from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> (e.g., natural images) but can only evaluate the probability function for <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/>. Hence a common metric in such cases is minimizing the cross-entropy / negative log likelihood <img alt="H(p,g)= -\mathbb{E}_{x sim p} \log g(x) = \mathbb{E}_{x \sim p} \log (1/g(x))" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%2Cg%29%3D+-%5Cmathbb%7BE%7D_%7Bx+sim+p%7D+%5Clog+g%28x%29+%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+%5Clog+%281%2Fg%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p,g)= -\mathbb{E}_{x sim p} \log g(x) = \mathbb{E}_{x \sim p} \log (1/g(x))"/>. For images, a common metric is “bits per pixel” which simply equals <img alt="H(p,q)/d" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%2Cq%29%2Fd&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p,q)/d"/> where <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> is the length of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. Another metric (often used in natural language processing) is perplexity, which interchanges the expectation and the logarithm. The logarithm of the perplexity of <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/> is <img alt="- \tfrac{1}{d}\log \mathbb{E}_{x \sim p} g(x)" class="latex" src="https://s0.wp.com/latex.php?latex=-+%5Ctfrac%7B1%7D%7Bd%7D%5Clog+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="- \tfrac{1}{d}\log \mathbb{E}_{x \sim p} g(x)"/> where <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> is the length of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> (e.g., in tokens). Another way to write this is that log of the perplexity is the average of <img alt="\log g(x_i|x{&lt;i})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+g%28x_i%7Cx%7B%3Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log g(x_i|x{&lt;i})"/> where <img alt="g(x_i|x_{&lt;i})" class="latex" src="https://s0.wp.com/latex.php?latex=g%28x_i%7Cx_%7B%3Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g(x_i|x_{&lt;i})"/> is the probability of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/> under <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/> conditioned on the first <img alt="i-1" class="latex" src="https://s0.wp.com/latex.php?latex=i-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i-1"/> parts of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>.</p>



<p><strong>Memorization for log-likelihood.</strong> The issue of “overfitting” is even more problematic for generative models than for classifiers. Given samples <img alt="x_1,\ldots,x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_n"/> and enough parameters, we can easily come up with a model corresponding to the uniform distribution <img alt="{ x_1,\ldots, x_n }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+x_1%2C%5Cldots%2C+x_n+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ x_1,\ldots, x_n }"/>. This is obviously a useless model that will never generate new examples. However, this model will not only get a large log likelihood value on the training set, in fact, it will get <em>even better log likelihood</em> than the true distribution! For example, any reasonable natural distribution on images would have at least tens of millions, if not billions or trillions of potential images. In contrast, a typical training set might have fewer than 1M samples. Hence, unlike in the classification setting, for generation, the “overfitting” model will not only match but can, in fact, beat the ground truth. (This is reminiscent of the following quote from <a href="https://etc.usf.edu/lit2go/86/peter-pan/1602/chapter-12-the-children-are-carried-off/">Peter and Wendy</a>: <em>“Not a sound is to be heard, save when they give vent to a wonderful imitation of the lonely call of the coyote. The cry is answered by other braves; and some of them do it even better than the coyotes, who are not very good at it.”</em>)</p>



<p>If we cannot compute the density function, then benchmarking becomes more difficult. What often happens in practice is an “I know it when I see it” approach. The paper includes a few pictures generated by the model, and if the pictures look realistic, we think it is a good model. However, this can be deceiving. After all, we are feeding in good pictures into the model, so generating a good photo may not be particularly hard (e.g. the model might memorize some good pictures and use those as outputs).</p>



<p>There is another metric called the <em>inception score</em>, which loosely corresponds to how similar the “inception” neural network finds the GAN model to ImageNet (in the sense that inception thinks it covers many of the ImageNet classes and that produces images on which inception has high confidence)  but it too has its problems. <a href="https://arxiv.org/pdf/1905.10887.pdf">Ravuri-Vinyalis 2019</a> used a GAN model with a good inception score used its outputs to train a different model on ImageNet. Despite the high inception score (which should have indicated that the GANs output are as good as ImageNets) the accuracy when training on the GAN output dropped from the original value of <img alt="74\%" class="latex" src="https://s0.wp.com/latex.php?latex=74%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="74\%"/> to as low as <img alt="5\%" class="latex" src="https://s0.wp.com/latex.php?latex=5%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="5\%"/>!  (Even in the best case, accuracy dropped by at least 30 points.) Compare this with the  11-14% drop when we train on ImageNet and test on <a href="https://arxiv.org/abs/1902.10811">ImageNet v2</a>.</p>



<p>This figure from <a href="https://arxiv.org/abs/1701.00160">Goodfellow’s tutorial</a> describes generative models where we know and don’t know how to compute the density function:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/3hVJBPl.png"/></figure>



<h1>Auto Encoder / Decoder</h1>



<p>We now shift our attention to the encoder/decoder architecture mentioned above.</p>



<p>Recall that we want to understand <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>, generate new elements <img alt="x^*" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^*"/>, and find a good representation of the elements. Our dream is to solve all of the issues with auto encoder/decoder, whose setup is as follows:</p>



<figure class="wp-block-image"><img alt="Setup for Auto Encoder/Decoder" src="https://i.imgur.com/udIY089.png"/></figure>



<p>That is, we want <img alt="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathbb{R}^d \rightarrow \mathbb{R}^r"/>, <img alt="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathbb%7BR%7D%5Er+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D:\mathbb{R}^r \rightarrow \mathbb{R}^d"/> such that</p>



<ul><li><img alt="D(E(x)) \approx x" class="latex" src="https://s0.wp.com/latex.php?latex=D%28E%28x%29%29+%5Capprox+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(E(x)) \approx x"/></li><li>The representation <img alt="E,D" class="latex" src="https://s0.wp.com/latex.php?latex=E%2CD&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E,D"/> enables us to solve tasks such as generation, classification, etc..</li></ul>



<p>To each the first point, we can aim to minimize <img alt="\sum_i ||x_i - D(E(x_i))||^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_i+%7C%7Cx_i+-+D%28E%28x_i%29%29%7C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_i ||x_i - D(E(x_i))||^2"/>. However, we can of course, make this loss zero by letting <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> and <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> be the identity function. Much of the framework of generative models can be considered as placing some restrictions on the “communication channel” that rule out this trivial approach, with the hope that would require the encoder and decoder to “intelligently” correspond to the structure of the natural data.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/XW6cDE7.png"/></figure>



<h2>Auto Encoders: noiseless short <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/></h2>



<p>A natural idea is to simply restrict the dimension of the latent space to be small (<img alt="r \ll d" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cll+d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r \ll d"/>). In principle, the optimal compression scheme for a probability distribution will require knowing the distribution. Moreover, the optimal compression will maximize the entropy of the latent data <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/>. Since the maximum entropy distribution is uniform (in the discrete case), we could easily sample from it. (In the continuous setting, the standard normal distribution plays the role of the uniform distribution.)</p>



<p>For starter, consider the case of picking <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> to be small and minimizing <img alt="\sum ||x_i - D(E(x_i))||^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum+%7C%7Cx_i+-+D%28E%28x_i%29%29%7C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum ||x_i - D(E(x_i))||^2"/> for <em>linear</em> <img alt="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathbb{R}^d \rightarrow \mathbb{R}^r"/>, <img alt="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathbb%7BR%7D%5Er+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D:\mathbb{R}^r \rightarrow \mathbb{R}^d"/>. Since <img alt="DE" class="latex" src="https://s0.wp.com/latex.php?latex=DE&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="DE"/> is a rank <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> matrix, we can write this as finding a rank <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> matrix <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L"/> that minimizes <img alt="| (I-L)X|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+%28I-L%29X%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| (I-L)X|^2"/> where <img alt="X = (x_1,\ldots,x_n)" class="latex" src="https://s0.wp.com/latex.php?latex=X+%3D+%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X = (x_1,\ldots,x_n)"/> is our input data. It can be shown that <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L"/> that would minimize this will be the projection to the top <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> eigenvectors of <img alt="XX^\top" class="latex" src="https://s0.wp.com/latex.php?latex=XX%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="XX^\top"/> which exactly corresponds to <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis (PCA)</a>.</p>



<p>In the nonlinear case, we can obtain better compression. However, we do not achieve our other goals:</p>



<ul><li>It is not the case that we can generate realistic data by sampling uniform/normal <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> and output <img alt="D(z)" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)"/></li><li>It is not the case that semantic similarity between <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> and <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x'"/> corresponds to large dot product between <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> and <img alt="E(x')" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x')"/>.</li></ul>



<p>It seems that model just rediscovers a compression algorithm like JPEG. We do not expect the JPEG encoding of an image to be semantically informative, and JPEG decoding of a random file will not be a good way to generate realistic images. It turns out that sometimes “Murphy’s law” does hold and if it’s possible to minimize the loss in a not very useful way then that will indeed be the case.</p>



<h2>Variational Auto Encoder (VAE)</h2>



<p>We now discuss <em>variational auto encoders</em> (VAEs). We can think of these as generalization auto-encoders to the case where the channel has some Gaussian noise. We will describe VAEs in two nearly equivalent ways:</p>



<ul><li>We can think of VAEs as trying to optimize two objectives: both the auto-encoder objective of minimizing <img alt="| D(E(x))-x|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+D%28E%28x%29%29-x%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| D(E(x))-x|^2"/> and another objective of minimizing the KL divergence between <img alt="D(x)" class="latex" src="https://s0.wp.com/latex.php?latex=D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(x)"/> and the standard normal distribution <img alt="N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,I)"/>.</li><li>We can think of VAEs as trying to maximize a proxy for the log-likelihood. This proxy is a quantity known as the “Evidence Lower Bound (ELBO)” which we can evaluate using <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> and <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> and is always smaller or equal to the log-likelihood.</li></ul>



<p>We start with the first description. One view of VAEs is that we search for a pair <img alt="E,D" class="latex" src="https://s0.wp.com/latex.php?latex=E%2CD&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E,D"/> of encoder and decoder that are aimed at minimizing the following two objectives:</p>



<ul><li><img alt="| x - D(E(x))|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+x+-+D%28E%28x%29%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| x - D(E(x))|^2"/> (standard AE objective)</li><li><img alt="\Delta_{KL}( E(x) || N(0,I) )" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28+E%28x%29+%7C%7C+N%280%2CI%29+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}( E(x) || N(0,I) )"/> (distance of latent from the standard normal)</li></ul>



<p>To make the second term a function of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, we consider <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> as a probability distribution with respect to a <em>fixed</em> <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. To ensure this makes sense, we need to make <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> <em>randomized</em>. A randomized Neural network has “sampling neurons” that take no input, have parameters <img alt="\mu,\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%2C%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mu,\sigma"/> and produce an element <img alt="v \sim N(\mu,\sigma^2)" class="latex" src="https://s0.wp.com/latex.php?latex=v+%5Csim+N%28%5Cmu%2C%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v \sim N(\mu,\sigma^2)"/>. We can train such a network by fixing a random <img alt="t \sim N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Csim+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="t \sim N(0,1)"/> and defining the neuron to simply output <img alt="\mu + \sigma t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%2B+%5Csigma+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mu + \sigma t"/>.</p>



<p><strong>ELBO derivation:</strong> Another view of VAEs is that they aim at maximizing a term known as the evidence lower bound or ELBO. We start by deriving this bound. Let <img alt="Z=N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z=N(0,I)"/> be the standard normal distribution over the latent space. Define <img alt="p_x" class="latex" src="https://s0.wp.com/latex.php?latex=p_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p_x"/> to be the distribution of <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z"/> conditioned on <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> decoding to <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> (i.e., <img alt="Z= z\sim Z|D(z)=x" class="latex" src="https://s0.wp.com/latex.php?latex=Z%3D+z%5Csim+Z%7CD%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z= z\sim Z|D(z)=x"/>, and define <img alt="q_x" class="latex" src="https://s0.wp.com/latex.php?latex=q_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q_x"/> be the distribution <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/>. Since <img alt="\Delta_{KL}(q_x||p_x) \geq 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28q_x%7C%7Cp_x%29+%5Cgeq+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(q_x||p_x) \geq 0"/>, we know that</p>



<p><img alt="0 \leq -H(q_x)- \mathbb{E}_{z \sim q_x} \log p_x(z)" class="latex" src="https://s0.wp.com/latex.php?latex=0+%5Cleq+-H%28q_x%29-+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+p_x%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0 \leq -H(q_x)- \mathbb{E}_{z \sim q_x} \log p_x(z)"/></p>



<p>By the definition of <img alt="p_x" class="latex" src="https://s0.wp.com/latex.php?latex=p_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p_x"/>, <img alt="p_x(z) = \Pr[ Z=z \;\wedge\; D(z)=x ] / \Pr[D(Z)=x]" class="latex" src="https://s0.wp.com/latex.php?latex=p_x%28z%29+%3D+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2F+%5CPr%5BD%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p_x(z) = \Pr[ Z=z \;\wedge\; D(z)=x ] / \Pr[D(Z)=x]"/>. Hence we can derive that</p>



<p><img alt="0 \leq -H(q_x) - \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + \log \Pr[ D(Z)=x]" class="latex" src="https://s0.wp.com/latex.php?latex=0+%5Cleq+-H%28q_x%29+-+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2B+%5Clog+%5CPr%5B+D%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0 \leq -H(q_x) - \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + \log \Pr[ D(Z)=x]"/><br/>(since <img alt="\Pr[ D(Z)=x]" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B+D%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Pr[ D(Z)=x]"/> depends only on <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, given that <img alt="Z=N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z=N(0,I)"/>.)</p>



<p>Rearranging, we see that</p>



<p><img alt="\log Pr[ D(Z)=x] \geq \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + H(q_x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+Pr%5B+D%28Z%29%3Dx%5D+%5Cgeq+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2B+H%28q_x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log Pr[ D(Z)=x] \geq \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + H(q_x)"/></p>



<p>or in other words, we have the following theorem:</p>



<p><strong>Theorem (ELBO):</strong> For every (possibly randomized) maps <img alt="E:\mathcal{X} \rightarrow \mathcal{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathcal%7BZ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathcal{X} \rightarrow \mathcal{Z}"/> and <img alt="D:\mathcal{Z} \rightarrow \mathcal{X}" class="latex" src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathcal%7BZ%7D+%5Crightarrow+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D:\mathcal{Z} \rightarrow \mathcal{X}"/>, distribution <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z"/> over <img alt="\mathcal{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BZ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{Z}"/> and <img alt="x\in \mathcal{X}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\in \mathcal{X}"/>,</p>



<p><img alt="\log \Pr[ D(Z)=x] \geq \Pr_{z \sim E(x), z'  \sim Z}[ D(z) = x \wedge z=z' ] + H(E(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+%5CPr%5B+D%28Z%29%3Dx%5D+%5Cgeq+%5CPr_%7Bz+%5Csim+E%28x%29%2C+z%27++%5Csim+Z%7D%5B+D%28z%29+%3D+x+%5Cwedge+z%3Dz%27+%5D+%2B+H%28E%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log \Pr[ D(Z)=x] \geq \Pr_{z \sim E(x), z'  \sim Z}[ D(z) = x \wedge z=z' ] + H(E(x))"/></p>



<p>The left-hand side of this inequality is simply the log-likelihood of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. The right-hand side (which, as the inequality shows, is always smaller or equal to it) is known as the <em>evidence lower bound</em> or ELBO. We can think of VAEs as trying to maximize the ELBO.</p>



<p>The reason that the two views are roughly equivalent is the follows:</p>



<ul><li>The first term of the ELBO, known as the <em>reconstruction term</em>, is <img alt="\mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ]"/> if we assume some normal noise, then the probabiility taht <img alt="D(z)=x" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)=x"/> will be proportional to <img alt="\exp(-|x-D(z)|^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%7Cx-D%28z%29%7C%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\exp(-|x-D(z)|^2)"/> since for <img alt="q_x" class="latex" src="https://s0.wp.com/latex.php?latex=q_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q_x"/>, <img alt="z=E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=z%3DE%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z=E(x)"/> we get that <img alt="\log Pr[ Z=z \;\wedge\; D(z)=x ] \approx -| x- D(E(x))|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+Pr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%5Capprox+-%7C+x-+D%28E%28x%29%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log Pr[ Z=z \;\wedge\; D(z)=x ] \approx -| x- D(E(x))|^2"/> and hence maximizing this term corresponds to minimizing the square distance.</li><li>The second term of the ELBO, known as the <em>divergence term</em>, is <img alt="H(q_x)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28q_x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(q_x)"/> which is roughly equal to <img alt="r -\Delta_{KL}(q_x||N(0,I))" class="latex" src="https://s0.wp.com/latex.php?latex=r+-%5CDelta_%7BKL%7D%28q_x%7C%7CN%280%2CI%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r -\Delta_{KL}(q_x||N(0,I))"/>, where <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> is the dimension of the latent space. Hence maximizing this term corresponds to minimizing the KL divergence between <img alt="q_x=E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=q_x%3DE%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q_x=E(x)"/> and the standard normal distribution.</li></ul>



<p>How well does VAE work? First of all, we can actually generate images using them. We also find that similar inputs will have similar encodings, which is good. However, sometimes VAEs can still “cheat” (as in auto encoders). There is a risk that the learned model will split <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z"/> to two parts of the form <img alt="(N(0,I), JPEG(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%28N%280%2CI%29%2C+JPEG%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(N(0,I), JPEG(x))"/>. The first part of the data is there to minimize divergence, while the second part is there for reconstruction. Such a model is similarly uninformative.</p>



<p>However, VAEs have found practical success. For example, <a href="https://arxiv.org/pdf/1610.00291.pdf">Hou et. al 2016</a> used VAE to create an encoding where two dimensions seem to correspond to “sunglasses” and “blondness”, as illustrated below. We do note that “sunglasses” and “blondness” are somewhere between “semantic” and “syntactic” attributes. They do correspond to relatively local changes in “pixel space”.</p>



<figure class="wp-block-image"><img alt="VAE Example 1" src="https://i.imgur.com/O48nnWB.jpg"/></figure>



<p>The picture can be blurry because of the noise we injected to make <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> random. However, recent models have used new techniques (e.g. <a href="https://arxiv.org/abs/1906.00446">vector quantized VAE</a> and <a href="https://arxiv.org/abs/2007.03898">hierarchical VAE</a>) to resolve the blurriness and significantly improve on state of art.</p>



<h2>Flow Models</h2>



<p>In a flow model, we flip the order of <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> and <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> and set <img alt="E=D^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=E%3DD%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E=D^{-1}"/> (so <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> must be invertible). The input <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> to <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> will come from the standard normal distribution <img alt="N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,I)"/>. The idea is that we obtain <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> by a composition of simple invertible functions. We use the fact that if we can compute the density function of a distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> over <img alt="\mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{R}^d"/> and <img alt="f:\mathbb{R}^d \rightarrow \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f:\mathbb{R}^d \rightarrow \mathbb{R}^d"/> is invertible and differentiable, then we can compute the density function of <img alt="f\circ p" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f\circ p"/> (i.e., the distribution obtained by sampling <img alt="w \sim p" class="latex" src="https://s0.wp.com/latex.php?latex=w+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w \sim p"/> and outputting <img alt="f(w)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(w)"/>). To see why this is the case, consider the setting when <img alt="d=2" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d=2"/> and a small <img alt="\delta \times \delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Ctimes+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta \times \delta"/> rectangle <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/>. If <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/> is small enough, <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> will be roughly linear and hence will map <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> into a parallelogram <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B"/>. Shifting the <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> coordinate by <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/> corresponds to shifting the output of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> by the vector <img alt="\delta (\tfrac{d f_x}{dx}, \tfrac{d f_y}{dx})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%28%5Ctfrac%7Bd+f_x%7D%7Bdx%7D%2C+%5Ctfrac%7Bd+f_y%7D%7Bdx%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta (\tfrac{d f_x}{dx}, \tfrac{d f_y}{dx})"/> and shifting the <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/> coordinate by <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/> corresponds to shifting the output of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> by the vector <img alt="\delta (\tfrac{d f_x}{dy}, \tfrac{d f_y}{dy})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%28%5Ctfrac%7Bd+f_x%7D%7Bdy%7D%2C+%5Ctfrac%7Bd+f_y%7D%7Bdy%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta (\tfrac{d f_x}{dy}, \tfrac{d f_y}{dy})"/>. For every <img alt="z \in B" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Cin+B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z \in B"/>, the density of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> under <img alt="f\circ p" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f\circ p"/> will be proportional to the density of <img alt="f^{-1}(z)" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7B-1%7D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f^{-1}(z)"/> with the proportionality fector being <img alt="vol(A)/vol(B)" class="latex" src="https://s0.wp.com/latex.php?latex=vol%28A%29%2Fvol%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="vol(A)/vol(B)"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/pet8tBU.png"/></figure>



<p>Overall we the density of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> under <img alt="f \circ p" class="latex" src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f \circ p"/> will equal <img alt="p(f^{-1}(z))" class="latex" src="https://s0.wp.com/latex.php?latex=p%28f%5E%7B-1%7D%28z%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(f^{-1}(z))"/> times the inverse determinant of the <em>Jacobian</em> of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> at the point <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/></p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/ItOPqSX.png"/></figure>



<p>There are different ways to compose together simple reversible functions to compute a complex one. Indeed, this issue also arises in cryptography and quantum computing (e.g., the <a href="https://en.wikipedia.org/wiki/Feistel_cipher">Fiestel cipher</a>). Using similar ideas, it is not hard to show that any probability distribution can be approximated by a (sufficiently big) combination of simple reversible functions.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/AlYrWJx.png"/></figure>



<p>In practice, we have some recent succcessful flow models. A few examples of these models are in the lecture slides.</p>



<h1>Giving up on the dream</h1>



<p>In section 2, we had a dream of doing both representation and generation at once. So far, we have not been able to find success with these models. What if we do each goal separately?</p>



<p>The tasks of representation becomes self-supervised learning with approaches such SIMCLR. The task of generation can be solved by GANs. Both areas have had recent success.</p>



<figure class="wp-block-image"><img alt="Model after we separate E and D" src="https://i.imgur.com/D0CpobJ.jpg"/></figure>



<p>Open-AI <a href="https://openai.com/blog/clip">CLIP</a> and <a href="https://openai.com/blog/dall-e/">DALL-E</a> is a pair of models that perform each part of these tasks well, and suggest an approach to merge them.<br/>CLIP does representation for both texts and images where the two encoders are aligned, i.e. <img alt="\langle E(\text{'cat'}), E(\text{img of cat)}\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+E%28%5Ctext%7B%27cat%27%7D%29%2C+E%28%5Ctext%7Bimg+of+cat%29%7D%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle E(\text{'cat'}), E(\text{img of cat)}\rangle"/> is large. DALL-E, given some text, generates an image corresponding to the text. Below are images generated by DALL-E when asked for an armchair in the shape of an avocado.</p>



<figure class="wp-block-image"><img alt="DALL-E Example" src="https://i.imgur.com/ZcsHXKE.png"/></figure>



<h2>Contrastive learning</h2>



<p>The general approach used in CLIP is called contrastive learning.</p>



<p>Suppose we have some representation function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> and inputs <img alt="u_i,v_i" class="latex" src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u_i,v_i"/> which represent similar objects. Let <img alt="M_{i,j}=f(u_i\cdot v_j)" class="latex" src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Cj%7D%3Df%28u_i%5Ccdot+v_j%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M_{i,j}=f(u_i\cdot v_j)"/>, then we want <img alt="M_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M_{i,j}"/> to be large when <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/>, but small when <img alt="i\neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\neq j"/>. So, let the loss function be <img alt="L(M)=\sum M_{i,i} / \sum_{i\neq j} M_{i,j}." class="latex" src="https://s0.wp.com/latex.php?latex=L%28M%29%3D%5Csum+M_%7Bi%2Ci%7D+%2F+%5Csum_%7Bi%5Cneq+j%7D+M_%7Bi%2Cj%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(M)=\sum M_{i,i} / \sum_{i\neq j} M_{i,j}."/> How do we create similar <img alt="u_i,v_i" class="latex" src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u_i,v_i"/>? In SIMCLR, <img alt="u_i,v_i" class="latex" src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u_i,v_i"/> are augmentations of the same image <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/>. In CLIP, <img alt="(u_i,v_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%28u_i%2Cv_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(u_i,v_i)"/> is an image and a text that describes it.</p>



<p>CLIPs representation space does seem to have nice properties such as correspondence between semantic attributes and linear directions, which enables doing some “semantic linear algebra” on representations: (see this based on <a href="https://github.com/haltakov/natural-language-image-search">Vladimir Hatlakov’s code</a> – in the snippet below <code>tenc</code> maps text to its encoding/representation and <code>get_img</code> finds nearest image to representation in a the unsplash dataset):</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/02/image-6.png"><img alt="" class="wp-image-8017" src="https://windowsontheory.files.wordpress.com/2021/02/image-6.png?w=854"/></a></figure>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/02/image-7.png"><img alt="" class="wp-image-8019" src="https://windowsontheory.files.wordpress.com/2021/02/image-7.png?w=444"/></a></figure>



<h2>GANs</h2>



<p>The theory of GANs is currently not well-developed. As an objective, we want images that “look real” (which is not well defined), and we have no posterior distribution. If we just define the distribution based on real images, our GAN might memorize the photos to beat us.</p>



<p>However, we know that Neural Networks are good at discriminating real vs. fake images. So, we add in a discriminator <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> and define the loss function <img alt="L(D) = \max_{f:\mathbb R^d\to \mathbb R} |\mathbb{E}_{\hat x\sim D(z)}f(\hat x)-\mathbb{E}_{x\sim p}f(x)|." class="latex" src="https://s0.wp.com/latex.php?latex=L%28D%29+%3D+%5Cmax_%7Bf%3A%5Cmathbb+R%5Ed%5Cto+%5Cmathbb+R%7D+%7C%5Cmathbb%7BE%7D_%7B%5Chat+x%5Csim+D%28z%29%7Df%28%5Chat+x%29-%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7Df%28x%29%7C.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(D) = \max_{f:\mathbb R^d\to \mathbb R} |\mathbb{E}_{\hat x\sim D(z)}f(\hat x)-\mathbb{E}_{x\sim p}f(x)|."/></p>



<p>The generator model and discriminator model form a 2-player game, which are often harder to train and very delicate. We typically train by changing a player’s action to the best response. However, we need to be careful if the two players have very different skill levels. They may be stuck in a setting where no change of strategies will make much difference, since the stronger player always dominates the weaker one. In particular in GANs we need to ensure that the generator is not cheating by using a degenerate distribution that still succeeds with respect to the discriminator.</p>



<p>If a 2-player model makes training more difficult, why do we use it? If we fix the discriminator, then the generator can find a picture that the discriminator thinks is real and only output that one, obtaining low loss. As a result, the discriminator needs to update along with the generator. This example also highlights that the discriminator’s job is often harder. To fix this, we have to somehow require the generator to give us good entropy.</p>



<p>Finally, how good are GANs in practice? Recently, we have had GANs that make great images as well as audios. For example, modern deepfake techniques often use GANs in their architecture. However, it is still unclear how rich the images are.</p></div>
    </content>
    <updated>2021-02-24T23:21:58Z</updated>
    <published>2021-02-24T23:21:58Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-03-04T03:21:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18187</id>
    <link href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/" rel="alternate" type="text/html"/>
    <title>A Quiz of Quotes</title>
    <summary>Everything that can be invented has been invented—Charles Duell, Commissioner, U.S. Office of Patents, 1899 MathQuotes src George Cantor has been featured here and here and here before on GLL. Of course, he invented modern set theory and changed math forever. His birthday is soon, so we thought we would talk about him now—he was […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Everything that can be invented has been invented—Charles Duell, Commissioner, U.S. Office of Patents, 1899</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/cantormathquote/" rel="attachment wp-att-18201"><img alt="" class="alignright wp-image-18201" height="150" src="https://rjlipton.files.wordpress.com/2021/02/cantormathquote.jpg?w=150&amp;h=150" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">MathQuotes <a href="https://www.facebook.com/mathsqoutes/posts/the-mathematician-does-not-study-pure-mathematics-because-it-is-useful-he-studie/144012037239958/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
George Cantor has been featured <a href="https://rjlipton.wordpress.com/2014/07/31/the-cantor-bernstein-schroder-theorem/">here</a> and <a href="https://rjlipton.wordpress.com/2009/04/18/cantors-non-diagonal-proof/">here</a> and <a href="https://rjlipton.wordpress.com/2012/09/04/thinking-out-of-the-notation-box/">here</a> before on GLL. Of course, he invented modern set theory and changed math forever. His birthday is soon, so we thought we would talk about him now—he was born on March 3rd in 1845.</p>
<p>
Today we thought it might be fun to have a quiz on math quotes.<br/>
<span id="more-18187"/></p>
<p>
Wait. Cantor did not invent quotation marks, nor is he known for many quotes. He does of course have many famous results, and they will live forever. But his results were subject to immediate horrible criticism and therefore memorable quotes. </p>
<p>Leopold Kronecker was a particular source of barbs.  For example: “What good is your beautiful proof on the transcendence of <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\pi}"/>? Why investigate such problems, given that irrational numbers do not even exist?” </p>
<p>
As a complexity theorist I must say that Kronecker has a point when he also said: </p>
<blockquote><p><b> </b> <em> “Definitions must contain the means of reaching a decision in a finite number of steps, and existence proofs must be conducted so that the quantity in question can be calculated with any degree of accuracy.” </em>
</p></blockquote>
<p>David Hilbert defended Cantor and said: “No one shall expel us from the paradise that Cantor has created.”</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/cantor/" rel="attachment wp-att-18190"><img alt="" class="aligncenter wp-image-18190" height="205" src="https://rjlipton.files.wordpress.com/2021/02/cantor.png?w=600&amp;h=205" width="600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">BBVA Open Mind <a href="https://www.bbvaopenmind.com/en/science/mathematics/georg-cantor-the-man-who-discovered-different-infinities/">src</a>
</font></td>
</tr>
</tbody></table>
<p/><h2> Quotes Quiz </h2><p/>
<p/><p>
On to the quiz. Each quote is followed by two possible authors in alphabetical order. You should pick the one you think is correct. The players are: </p>
<blockquote><p><b> </b> <em> 1. Douglas Adams  2. Bernard Baruch  3. Eric Temple Bell  4. Raoul Bott<br/>
5. Paul Erdős  6. Richard Hamming  7. Godfrey Hardy  8. David Hilbert<br/>
9. Admiral Grace Hooper  10. Alan Kay  11. Donald Knuth  12. John von Neumann<br/>
13. Alan Perlis  14. Henri Poincaré  15. Srinivasa Ramanujan  16. Marcus du Sautoy<br/>
17. Raymond Smullyan  18. Alan Turing  19. Moshe Vardi  20. Andrew Wiles<br/>
</em>
</p></blockquote>
<p>
</p><ol>
<p/><li>
Those who can imagine anything, can create the impossible.<br/>
—Kay <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Turing<p/>
<p/></li><li>
I really didn’t foresee the Internet. But then, neither did the computer industry. Not that that tells us very much of course–the computer industry didn’t even foresee that the century was going to end.<br/>
— Adams <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Knuth<p/>
<p/></li><li>
One man’s constant is another man’s variable.<br/>
—Perlis <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> du Sautoy<p/>
<p/></li><li>
The most damaging phrase in the language is: “It’s always been done that way.”<br/>
—-Hopper <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Perlis<p/>
<p/></li><li>
The best way to predict the future is to invent it.<br/>
—Kay <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Turing<p/>
<p/></li><li>
The purpose of computing is insight, not numbers.<br/>
Adams <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Hamming<p/>
<p/></li><li>
Beware of bugs in the above code; I have only proved it correct, not tried it.<br/>
—Knuth <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Vardi<p/>
<p/></li><li>
No, it is a very interesting number, it is the smallest number expressible as a sum of two cubes in two different ways.<br/>
—Bell <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Ramanujan <p/>
<p/></li><li>
Beauty is the first test: there is no permanent place in the world for ugly mathematics. <br/>
—Erdős <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Hardy<p/>
<p/></li><li>
Mathematics is the art of giving the same name to different things. <br/>
—Hooper <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Poincaré <p/>
<p/></li><li>
There’s no sense in being precise when you don’t even know what you’re talking about.<br/>
—Bott <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> von Neumann<p/>
<p/></li><li>
I hope we’ll be able to solve these problems before we leave. <br/>
—Erdős <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Perlis<p/>
<p/></li><li>
Some people are always critical of vague statements. I tend rather to be critical of precise statements; they are the only ones which can correctly be labeled ‘wrong’. <br/>
—Knuth <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Smullyan<p/>
<p/></li><li>
Everything that humans can do a machine can do. <br/>
—Perlis <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Vardi<p/>
<p/></li><li>
“Obvious” is the most dangerous word in mathematics.<br/>
— Bell <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Hooper<p/>
<p/></li><li>
Just because we can’t find a solution, it doesn’t mean there isn’t one.<br/>
— Adams <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Wiles<p/>
<p/></li><li>
Mathematics is a place where you can do things which you can’t do in the real world.<br/>
— du Sautoy <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Turing<p/>
<p/></li><li>
Millions saw the apple fall, but Newton asked why.<br/>
— Baruch <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Hopper<p/>
<p/></li><li>
The definition of a good mathematical problem is the mathematics it generates rather than the problem itself.<br/>
— Hilbert <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Wiles<p/>
<p/></li><li>
There are two ways to do great mathematics. The first is to be smarter than everybody else. The second way is to be stupider than everybody else – but persistent.<br/>
— Bott <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Knuth<p/>
</li></ol>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
“I always have a quotation for everything—it saves original thinking.”<br/>
—Dorothy Sayers</p>
<p>
Here are the answers:</p>
<p><a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/ans/" rel="attachment wp-att-18194"><img alt="" class="alignright size-full wp-image-18194" height="702" src="https://rjlipton.files.wordpress.com/2021/02/ans.png?w=600&amp;h=702" width="600"/></a></p></font></font></div>
    </content>
    <updated>2021-02-24T17:44:38Z</updated>
    <published>2021-02-24T17:44:38Z</published>
    <category term="History"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Cantor"/>
    <category term="fun"/>
    <category term="quiz"/>
    <category term="quote"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-03-04T03:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/027</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/027" rel="alternate" type="text/html"/>
    <title>TR21-027 |  Almost Optimal Super-Constant-Pass Streaming Lower Bounds for Reachability | 

	Lijie Chen, 

	Gillat Kol, 

	Dmitry Paramonov, 

	Raghuvansh Saxena, 

	Zhao Song, 

	Huacheng Yu</title>
    <summary>We give an almost quadratic $n^{2-o(1)}$ lower bound on the space consumption of any $o(\sqrt{\log n})$-pass streaming algorithm solving the (directed) $s$-$t$ reachability problem. This means that any such algorithm must essentially store the entire graph. As corollaries, we obtain almost quadratic space lower bounds for additional fundamental problems, including maximum matching, shortest path, matrix rank, and linear programming.

Our main technical contribution is the definition and construction of set hiding graphs, that may be of independent interest: we give a general way of encoding a set $S \subseteq [k]$ as a directed graph with $n = k^{ 1 + o( 1 ) }$ vertices, such that deciding whether $i \in S$ boils down to deciding if $t_i$ is reachable from $s_i$, for a specific pair of vertices $(s_i,t_i)$ in the graph. Furthermore, we prove that our graph ``hides'' $S$, in the sense that no low-space streaming algorithm with a small number of passes can learn (almost) anything about $S$.</summary>
    <updated>2021-02-24T02:18:04Z</updated>
    <published>2021-02-24T02:18:04Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-04T03:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5350</id>
    <link href="https://www.scottaaronson.com/blog/?p=5350" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5350#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5350" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Stop emailing my utexas address</title>
    <summary xml:lang="en-US">A month ago, UT Austin changed its email policies—banning auto-forwarding from university accounts to Gmail accounts, apparently as a way to force the faculty and other employees to separate their work email from their personal email, and thereby comply with various government regulations. Ever since that change, the email part of my life has been […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>A month ago, UT Austin changed its email policies—banning auto-forwarding from university accounts to Gmail accounts, apparently as a way to force the faculty and other employees to separate their work email from their personal email, and thereby comply with various government regulations.  Ever since that change, the email part of my life has been a <em>total, unmitigated disaster</em>.  I’ve missed (or been late to see) dozens of important work emails, with the only silver lining being that that’s arguably UT’s problem more than it is mine!</p>



<p>And yes, I’ve already gone to technical support; the only answer I’ve gotten is that (in so many words) there <em>is</em> no answer.  Other UT faculty are somehow able to deal with this because they are them; I am unable to deal with it because I am me.  As a mere PhD in computer science, I’m utterly unqualified to set up a technical fix for this sort of problem.</p>



<p>So the bottom line is: <strong>from now on, if you want me to see an email, send it to scott@scottaaronson.com</strong>.  Really.  If you try sending it to aaronson@cs.utexas.edu, it will land in a separate inbox that I can access only with great inconvenience.  And if, God forbid, you try sending it to aaronson@utexas.edu, the email will bounce and I’ll never see it at all.  Indeed, a central purpose of this post is just to have a place to point the people who contact me every day, shocked that their emails to me bounced.</p>



<p>This whole episode has given me <em>immense</em> sympathy for Hillary Clinton, and for the factors that led her to set up clintonemail.com from her house.  It’s not merely that her private email server was a laughably trivial reason to end the United States’ 240-year run of democratic government.  Rather it’s that, even on the narrow question of emails, I now feel certain that <em>Hillary was 100% right</em>.  Bureaucracy that impedes communication is a cancer on human civilization.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> Thanks so much to commenter Avraham and to my colleague Etienne Vouga, who quickly gave me the crucial information that tech support would not, and thereby let me solve this problem.  I can once again easily read emails sent to aaronson@cs.utexas.edu … well, at least for now!  I’m now checking about aaronson@utexas.edu.  Again, though, <strong>scott@scottaaronson.com to be safe</strong>.</p></div>
    </content>
    <updated>2021-02-23T21:00:37Z</updated>
    <published>2021-02-23T21:00:37Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-02-24T23:14:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/026</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/026" rel="alternate" type="text/html"/>
    <title>TR21-026 |  Conditional Dichotomy of Boolean Ordered Promise CSPs | 

	Joshua Brakensiek, 

	Venkatesan Guruswami, 

	Sai Sandeep</title>
    <summary>Promise Constraint Satisfaction Problems (PCSPs) are a generalization of Constraint Satisfaction Problems (CSPs) where each predicate has a strong and a weak form and given a CSP instance, the objective is to distinguish if the strong form can be satisfied vs. even the weak form cannot be satisfied. Since their formal introduction by Austrin, Guruswami, and Håstad, there has been a flurry of works on PCSPs, including recent breakthroughs in approximate graph coloring. The key tool in studying PCSPs is the algebraic framework developed in the context of CSPs where the closure properties of the satisfying solutions known as *polymorphisms* are analyzed. 
    
    The polymorphisms of PCSPs are significantly richer than CSPs---this is illustrated by the fact that even in the Boolean case, we still do not know if there exists a dichotomy result for PCSPs analogous to Schaefer's dichotomy result for CSPs. In this paper, we study a special case of Boolean PCSPs, namely Boolean *Ordered* PCSPs where the Boolean PCSPs have the predicate $x \leq y$. In the algebraic framework, this is the special case of Boolean PCSPs when the polymorphisms are *monotone* functions. We prove that Boolean Ordered PCSPs exhibit a computational dichotomy assuming the Rich $2$-to-$1$ Conjecture due to Braverman, Khot, and Minzer, which is a perfect completeness surrogate of the Unique Games Conjecture. 
    
    In particular, assuming the Rich $2$-to-$1$ Conjecture, we prove that a Boolean Ordered PCSP can be solved in polynomial time if for every $\epsilon &gt;0$, it has polymorphisms where each coordinate has *Shapley value* at most $\epsilon$, else it is NP-hard. The algorithmic part of our dichotomy result is based on a structural lemma showing that Boolean monotone functions with each coordinate having low Shapley value have arbitrarily large threshold functions as minors. The hardness part proceeds by showing that the Shapley value is consistent under a uniformly random $2$-to-$1$ minor. As a structural result of independent interest, we construct an example to show that the Shapley value can be inconsistent under an adversarial $2$-to-$1$ minor.</summary>
    <updated>2021-02-23T17:37:03Z</updated>
    <published>2021-02-23T17:37:03Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-04T03:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6398537358110172858</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6398537358110172858/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/good-names-and-bad-names-of-game-shows.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6398537358110172858" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6398537358110172858" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/good-names-and-bad-names-of-game-shows.html" rel="alternate" type="text/html"/>
    <title>Good Names and Bad Names of Game Shows and theorems</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> In my post on Alex Trebek, see <a href="https://blog.computationalcomplexity.org/2020/11/alex-trebekwhat-is-todays-post-about.html">here</a>, I noted that <i>Jeopardy!</i> is not a good name for the game show since it doesn't tell you much about the show. Perhaps <i>Answers and Questions </i>is a better name.</p><p>The following game shows have names that tell you something about the game and hence have better names: </p><p>Wheel of Fortune, The Price is Right, Lets make a Deal, Beautiful women have suitcases full of money (the original name for Deal-No Deal), Win Ben Stein's Money, Beat the Geeks. </p><p>In Math we often name a concept  after a person. While this may be a good way to honor someone, the name does not tell us much about the concept and it leads to statements like:</p><p><br/></p><p><i>A Calabi-Yau manifold is a compact complex Kahler manifold with a trivial first Chern class. </i></p><p><i>A Kahler manifold is a Hermitian manifold for which the Hermitian form is closed.</i></p><p><i>A Hermitian manifold is the complex analog of the Riemann manifold. </i></p><p>(These examples are from an article I will point to later---I do not understand <i>any </i>of these terms, though I once knew what a <i>Riemann manifold</i> was. I heard the term <i>Kahler Manifold </i>in the song <a href="https://www.youtube.com/watch?v=2rjbtsX7twc">Bohemian Gravity</a>.  It's at about the 4 minute 30 second place.) </p><p>While I am amused by the name <i>Victoria Delfino Problems</i> (probably the only realtor who has problems in math named after her, see my post <a href="https://blog.computationalcomplexity.org/2021/02/the-victoria-delfino-problems-example.html">here</a>) it's not a descriptive way to name open problems in descriptive set theory. </p><p><br/></p><p>Sometimes  a name becomes SO connected to a concept that it IS descriptive, e.g.:</p><p><i>The first proof of VDW's theorem yields ACKERMAN-LIKE bounds. </i></p><p>but you cannot count on that happening AND it is only descriptive to people already somewhat in the field. </p><p><br/></p><p>What to do? <a href="http://nautil.us/issue/89/the-dark-side/why-mathematicians-should-stop-naming-things-after-each-other">This</a> article makes the  ballian point that we should   STOP DOING THIS and that the person who first proves the theorem should name it in a way that tells you something about the concept. I would agree. But this can still be hard to really do.</p><p><br/></p><p>In my book on Muffin Mathematics (see <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215170">here</a>) I have a sequence of methods called</p><p>Floor Ceiling, Half, Mid, Interval, Easy-Buddy-Match, Hard-Buddy-Match, Gap, Train. </p><p>There was one more method that I didn't quite name, but I used the phrase `Scott Muffin Problem' to honors Scott Huddleton who came up with the method, in my description of it. </p><p>All but the last concept were given ballian names.  Even so, you would need to read the book to see why the names make sense. Still, that would be easier than trying to figure out what a Calabi-Yau manifold is. </p><p><br/></p><p/><br/></div>
    </content>
    <updated>2021-02-23T05:36:00Z</updated>
    <published>2021-02-23T05:36:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-03-04T02:39:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18152</id>
    <link href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/" rel="alternate" type="text/html"/>
    <title>Riemann Hypothesis—Why So Hard?</title>
    <summary>“If I were to awaken after having slept a thousand years, my first question would be: has the Riemann Hypothesis been proven?” — David Hilbert Steklov Institute memorial page Sergei Voronin was an expert in number theory, who studied the Riemann zeta function, but who sadly died young over twenty years ago. We discussed his […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>“If I were to awaken after having slept a thousand years, my first question would be: has the Riemann Hypothesis been proven?” — David Hilbert</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/voroninsteklov/" rel="attachment wp-att-18177"><img alt="" class="alignright wp-image-18177" height="190" src="https://rjlipton.files.wordpress.com/2021/02/voroninsteklov.jpg?w=142&amp;h=190" width="142"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Steklov Institute memorial <a href="http://www.mi-ras.ru/index.php?c=inmemoria&amp;l=1">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Sergei Voronin was an expert in number theory, who studied the Riemann zeta function, but who sadly died young over twenty years ago. We discussed his amazing 1975 result about the Riemann zeta function <a href="https://rjlipton.wordpress.com/2012/12/04/the-amazing-zeta-code/">here</a>. Others call the result the amazing theorem. I (Dick) am getting old—I almost forgot that we did a <a href="https://rjlipton.wordpress.com/2016/12/20/hunting-complexity-in-zeta/">post</a> on his theorem again over four years ago. </p>
<p>
Today I thought we would recall his theorem, sketch why the theorem is true, and then discuss some extensions.<br/>
<span id="more-18152"/></p>
<p>
Starting with Alan Turing we have been interested in universal objects. Turing famously <a href="https://en.wikipedia.org/wiki/Universal_Turing_machine">proved</a> that there are universal machines: these can simulate any other machine on any input. Martin Davis has an entire <a href="https://www.amazon.com/Universal-Computer-Road-Leibniz-Turing/dp/0393047857">book</a> on this subject. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/davis/" rel="attachment wp-att-18156"><img alt="" class="aligncenter wp-image-18156" height="375" src="https://rjlipton.files.wordpress.com/2021/02/davis.jpg?w=235&amp;h=375" width="235"/></a>
</td>
</tr>
</tbody></table>
<p>
Universal objects are basic to complexity theory. Besides Turing’s notion, a universal property is key to the definition of NP-complete. A set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{S}"/> in NP is NP-complete provided all other sets in NP can be reduced to <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{S}"/> in polynomial time. Michael Nielsen once began a <a href="https://www.quantamagazine.org/the-physical-origin-of-universal-computing-20151027/">discussion</a> of universality in this amusing fashion:</p>
<blockquote><p><b> </b> <em> Imagine you’re shopping for a new car, and the salesperson says, “Did you know, this car doesn’t just drive on the road.” “Oh?” you reply. “Yeah, you can also use it to do other things. For instance, it folds up to make a pretty good bicycle. And it folds out to make a first-rate airplane. Oh, and when submerged it works as a submarine. And it’s a spaceship too!” </em>
</p></blockquote>
<p/><h2> Voronin’s Insight </h2><p/>
<p>
In 1975 Voronin had the brilliant insight that the Riemann zeta <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> function has an interesting universality property.  Roughly speaking, it says that a wide class of analytic functions can be approximated by shifts <img alt="{\zeta(s+it)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%2Bit%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s+it)}"/> with real <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{t}"/>. Recall 	</p>
<p align="center"><img alt="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Czeta%28s%29+%3D+%5Csum_%7Bn%3D1%7D%5E%5Cinfty+%5Cfrac%7B1%7D%7Bn%5Es%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} "/></p>
<p>for <img alt="{\Re(s) &gt;1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\Re(s) &gt;1}"/>, and it has an analytic extension for all other values but <img alt="{s=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{s=1}"/>.</p>
<p>
The intense interest in the <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> function started in 1859 with Bernhard Riemann’s breakthrough <a href="https://en.wikipedia.org/wiki/On_the_Number_of_Primes_Less_Than_a_Given_Magnitude">article</a>. This was the first statement of what we call the Riemann Hypothesis (RH).</p>
<p>
In over a century of research on RH before Voronin’s theorem, many identities, many results, many theorems were proved about the zeta function. But none saw that the <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> function was universal before Voronin. Given the zeta function’s importance in understanding the structure of prime numbers this seems to be surprising. </p>
<p>
Before we define the universal property I thought it might be useful to state a related <a href="https://arxiv.org/pdf/1305.3933.pdf">property</a> that the <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> function has:</p>
<blockquote><p><b>Theorem 1</b> <em> Suppose that <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{P}"/> is a polynomial so that for all <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{s}"/>, 	</em></p><em>
<p align="center"><img alt="\displaystyle  P\left(\zeta(s), \zeta{'}(s),\dots,\zeta^{(m)}(s) \right) = 0. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%5Cleft%28%5Czeta%28s%29%2C+%5Czeta%7B%27%7D%28s%29%2C%5Cdots%2C%5Czeta%5E%7B%28m%29%7D%28s%29+%5Cright%29+%3D+0.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  P\left(\zeta(s), \zeta{'}(s),\dots,\zeta^{(m)}(s) \right) = 0. "/></p>
</em><p><em>Then <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{P}"/> is identically zero. </em>
</p></blockquote>
<p>Since <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{s}"/> is a single variable, this says that <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> and its derivatives <img alt="{\zeta'(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%27%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta'(s)}"/> and <img alt="{\zeta''(s) \dots }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%27%27%28s%29+%5Cdots+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta''(s) \dots }"/> do not satisfy any polynomial relationship. This means intuitively that <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> must be <a href="https://en.wikipedia.org/wiki/Hypertranscendental_function">hypertranscendental</a>. Let’s now make this formal.</p>
<p/><h2> Voronin’s Theorem </h2><p/>
<p>
Here is his <a href="https://en.wikipedia.org/wiki/Zeta_function_universality">theorem</a>: </p>
<blockquote><p><b>Theorem 2</b> <em> Let <img alt="{0&lt;r&lt;1/4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%3Cr%3C1%2F4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{0&lt;r&lt;1/4}"/>. Let <img alt="{f(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f(s)}"/> be an analytic function that never is zero for <img alt="{|s| \le r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cs%7C+%5Cle+r%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{|s| \le r}"/>. Then for any <img alt="{\epsilon&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\epsilon&gt;0}"/> there is a real <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{t}"/> so that 	</em></p><em>
<p align="center"><img alt="\displaystyle  \max_{\left | s \right | \leq r} \left | \zeta(s + \frac{3}{4} + i t) - f(s) \right | &lt; \epsilon. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmax_%7B%5Cleft+%7C+s+%5Cright+%7C+%5Cleq+r%7D+%5Cleft+%7C+%5Czeta%28s+%2B+%5Cfrac%7B3%7D%7B4%7D+%2B+i+t%29+-+f%28s%29+%5Cright+%7C+%3C+%5Cepsilon.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \max_{\left | s \right | \leq r} \left | \zeta(s + \frac{3}{4} + i t) - f(s) \right | &lt; \epsilon. "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
See the <a href="https://www.researchgate.net/profile/Renata-Macaitiene/publication/321139128_Zeros_of_the_Riemann_zeta-function_and_its_universality/links/5c7f7b5092851c695058d6fe/Zeros-of-the-Riemann-zeta-function-and-its-universality.pdf">paper</a> “Zeroes of the Riemann zeta-function and its universality,” by Ramunas Garunkstis, Antanas Laurincikas, and Renata Macaitiene, for a detailed modern discussion of his theorem. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/thm/" rel="attachment wp-att-18158"><img alt="" class="aligncenter wp-image-18158" height="275" src="https://rjlipton.files.wordpress.com/2021/02/thm.png?w=320&amp;h=275" width="320"/></a>
</td>
</tr>
</tbody></table>
<p/><p><br/>
Note that the theorem is not constructive. However, the values of <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{t}"/> that work have a positive density—there are lots of them. Also note the restriction that <img alt="{f(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f(s)}"/> is never zero is critical. Otherwise one would be able to show that the Riemann Hypothesis is false. In 2003, Garunkstis et al. did prove a constructive version, in a <a href="https://www.jstor.org/stable/43736941?seq=1">paper</a> titled, “Effective Uniform Approximation By The Riemann Zeta-Function.”</p>
<p/><h2> Voronin’s Proof </h2><p/>
<p>
The key insight is to combine two properties of the zeta <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> function: The usual definition with the Euler product. Recall the Riemann zeta-function has an Euler product expression 	</p>
<p align="center"><img alt="\displaystyle  \zeta(s) = \prod_p \frac{1}{1-p^{-s}}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Czeta%28s%29+%3D+%5Cprod_p+%5Cfrac%7B1%7D%7B1-p%5E%7B-s%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \zeta(s) = \prod_p \frac{1}{1-p^{-s}}. "/></p>
<p>where <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p}"/> runs over prime numbers. This is valid only in the region <img alt="{\Re(s) &gt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\Re(s) &gt; 1}"/>, but it makes sense in a approximate sense in the critical strip: 	</p>
<p align="center"><img alt="\displaystyle  1/2 &lt; \Re(s) &lt; 1. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2F2+%3C+%5CRe%28s%29+%3C+1.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  1/2 &lt; \Re(s) &lt; 1. "/></p>
<p>Then take logarithms and since <img alt="{\log(p)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28p%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\log(p)}"/> are linearly independent over <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Q}"/>, we can apply the Kronecker approximation theorem to obtain that any target function <img alt="{f(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f(s)}"/> can be approximated by the above finite truncation. This is the basic structure of the <a href="https://en.wikipedia.org/wiki/Zeta_function_universality">proof</a>.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Voronin’s insight was immediately interesting to number theorists. Many found new methods for proving universality and for extending it to other functions. Some methods work for all zeta-functions defined by Euler products. See this <a href="https://arxiv.org/pdf/1407.4216.pdf">survey</a> by Kohji Matsumoto and a recent <a href="https://www.semanticscholar.org/paper/Quantized-Number-Theory%2C-Fractal-Strings-and-the-to-Herichi-Lapidus/37dbcf9c28c316b8cfcfe74394f3a1f2d709235d">paper</a><br/>
by Hafedh Herichi and Michel Lapidus, the latter titled “Quantized Number Theory, Fractal Strings and the Riemann Hypothesis: From Spectral Operators to Phase Transitions and Universality.”</p>
<p>
Perhaps the most interesting question is: </p>
<p><i>Can universality be used to finally unravel the RH?</i> </p>
<p>See Paul Gauthier’s 2014 IAS <a href="http://www.math.kent.edu/~zvavitch/informal/Informal_Analysis_Seminar/Slides,_April_2014_files/IAS2014_Gauthier_1.pdf">talk</a>, “Universality and the Riemann Hypothesis,” for some ideas.</p>
<p>
[fixed missing line at end]</p></font></font></div>
    </content>
    <updated>2021-02-21T23:39:00Z</updated>
    <published>2021-02-21T23:39:00Z</published>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="amazing"/>
    <category term="theorem"/>
    <category term="universal"/>
    <category term="Voronin"/>
    <category term="zeta"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-03-04T03:20:48Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-1990979806491986425</id>
    <link href="http://processalgebra.blogspot.com/feeds/1990979806491986425/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=1990979806491986425" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/1990979806491986425" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/1990979806491986425" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/02/article-by-sergey-kitaev-and-anthony.html" rel="alternate" type="text/html"/>
    <title>Article by Sergey Kitaev and Anthony Mendes in Jeff Remmel's memory</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://www.strath.ac.uk/staff/kitaevsergeydr/" target="_blank">Sergey Kitaev</a> just shared with me <a href="http://ecajournal.haifa.ac.il/Volume2021/ECA2021_S1H2.pdf" target="_blank">an article</a> he wrote with Anthony Mendes in <a href="https://senate.universityofcalifornia.edu/in-memoriam/files/jeffrey-b-remmel.html" target="_blank">Jeff Remmel's memory</a>. Jeff Remmel was a distinguished mathematician with a very successful career in both logic and combinatorics. </p><p>The short biography at the start of the article paints a vivid picture of Jeff Remmel's  personality, and will be of interest and inspiration to many readers. His hiring as "an Assistant Professor in the Department of Mathematics at UC San Diego at age 25, without officially finishing his Ph.D. and without having published a single paper" was, in Jeff Remmel's own words, a "fluke that will never happen again."<br/></p><p>I had the pleasure of making Jeff Remmel's acquaintance when he visited Sergey in Reykjavik and thoroughly enjoyed talking to him about a variety of subjects. He was truly a larger-than-life academic. <br/></p></div>
    </content>
    <updated>2021-02-21T11:47:00Z</updated>
    <published>2021-02-21T11:47:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-02-21T11:47:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/025</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/025" rel="alternate" type="text/html"/>
    <title>TR21-025 |  Improved Maximally Recoverable LRCs using Skew Polynomials | 

	Sivakanth Gopi, 

	Venkatesan Guruswami</title>
    <summary>An $(n,r,h,a,q)$-Local Reconstruction Code is a linear code over $\mathbb{F}_q$ of length $n$, whose codeword symbols are partitioned into $n/r$ local groups each of size $r$. Each local group satisfies `$a$' local parity checks to recover from `$a$' erasures in that local group and there are further $h$ global parity checks to provide fault tolerance from more global erasure patterns. Such an LRC is Maximally Recoverable (MR), if it offers the best blend of locality and global erasure resilience---namely it can correct all erasure patterns whose recovery is information-theoretically feasible given the locality structure (these are precisely patterns with up to `$a$' erasures in each local group and an additional $h$ erasures anywhere in the codeword).

Random constructions can easily show the existence of MR LRCs over very large fields, but a major algebraic challenge is to construct MR LRCs, or even show their existence, over smaller fields, as well as understand inherent lower bounds on their field size. We give an explicit construction of $(n,r,h,a,q)$-MR LRCs with field size $q$ bounded by $\left(O\left(\max\{r,n/r\}\right)\right)^{\min\{h,r-a\}}$. This improves upon known constructions in many relevant parameter ranges. Moreover, it matches the lower bound from Gopi et al. (2020) in an interesting range of parameters where $r=\Theta(\sqrt{n})$, $r-a=\Theta(\sqrt{n})$ and $h$ is a fixed constant with $h\le a+2$, achieving the optimal field size of $\Theta_{h}(n^{h/2}).$

Our construction is based on the theory of skew polynomials.  We believe skew polynomials should have further applications in coding and complexity theory; as a small illustration we show how to capture algebraic results underlying list decoding folded Reed-Solomon and multiplicity codes in a unified way within this theory.</summary>
    <updated>2021-02-21T10:21:05Z</updated>
    <published>2021-02-21T10:21:05Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-04T03:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/024</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/024" rel="alternate" type="text/html"/>
    <title>TR21-024 |  A Majority Lemma for Randomised Query Complexity | 

	Mika Göös, 

	Gilbert Maystre</title>
    <summary>We show that computing the majority of $n$ copies of a boolean function $g$ has randomised query complexity $\mathrm{R}(\mathrm{Maj} \circ g^n) = \Theta(n\cdot \bar{\mathrm{R}}_{1/n}(g))$. In fact, we show that to obtain a similar result for any composed function $f\circ g^n$, it suffices to prove a sufficiently strong form of the result only in the special case $g=\mathrm{GapOr}$.</summary>
    <updated>2021-02-21T10:18:33Z</updated>
    <published>2021-02-21T10:18:33Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-04T03:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/023</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/023" rel="alternate" type="text/html"/>
    <title>TR21-023 |  $3.1n - o(n)$ Circuit Lower Bounds for Explicit Functions | 

	Tianqi Yang, 

	Jiatu Li</title>
    <summary>Proving circuit lower bounds has been an important but extremely hard problem for decades. Although one may show that almost every function $f:\mathbb{F}_2^n\to\mathbb{F}_2$ requires circuit of size $\Omega(2^n/n)$ by a simple counting argument, it remains unknown whether there is an explicit function (for example, a function in $NP$) not computable by circuits of size $10n$. In fact, a $3n-o(n)$ explicit lower bound by Blum (TCS, 1984) was unbeaten for over 30 years until a recent breakthrough by Find et al. (FOCS, 2016), which proved a $(3+\frac{1}{86})n-o(n)$ lower bound for affine dispersers, a class of functions known to be constructible in $P$.

In this paper, we prove a stronger lower bound $3.1n - o(n)$ for affine dispersers. To get this result, we strengthen the gate elimination approach for $(3+\frac{1}{86})n$ lower bound, by a more sophisticated case analysis that significantly decreases the number of bottleneck structures introduced during the elimination procedure. Intuitively, our improvement relies on three observations: adjacent bottleneck structures becomes less troubled; the gates eliminated are usually connected; and the hardest cases during gate elimination have nice local properties to prevent the introduction of new bottleneck structures.</summary>
    <updated>2021-02-21T06:59:55Z</updated>
    <published>2021-02-21T06:59:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-04T03:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/022</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/022" rel="alternate" type="text/html"/>
    <title>TR21-022 |  Depth lower bounds in Stabbing Planes for combinatorial principles | 

	Stefan Dantchev, 

	Nicola  Galesi, 

	Abdul Ghani, 

	Barnaby Martin</title>
    <summary>We prove logarithmic depth lower bounds in Stabbing Planes for the classes of  combinatorial principles known as  the Pigeonhole principle and the Tseitin contradictions. The depth lower bounds are new, obtained by giving almost linear length lower bounds which do not depend on the bit-size of the inequalities and in the case of  the Pigeonhole principle are tight. 

The technique known so far to prove depth lower bounds for Stabbing Planes is a generalization of that used for the Cutting Planes proof system.  In this work  we  introduce two  new approaches to prove length/depth lower bounds in Stabbing Planes: one relying on Sperner's Theorem which works for the Pigeonhole principle and Tseitin contradictions over the complete graph; a second proving the lower bound for Tseitin contradictions over a grid graph, which uses a result on essential  coverings of the boolean cube by linear polynomials, which in turn relies on Alon's combinatorial Nullenstellensatz</summary>
    <updated>2021-02-20T20:20:01Z</updated>
    <published>2021-02-20T20:20:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-04T03:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/021</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/021" rel="alternate" type="text/html"/>
    <title>TR21-021 |  Average-Case Perfect Matching Lower Bounds from Hardness of Tseitin Formulas | 

	Kilian Risse, 

	Per Austrin</title>
    <summary>We study the complexity of proving that a sparse random regular graph on an odd number of vertices does not have a perfect matching, and related problems involving each vertex being matched some pre-specified number of times. We show that this requires proofs of degree $\Omega(n/\log n)$ in the Polynomial Calculus (over fields of characteristic $\ne 2$) and Sum-of-Squares proof systems, and exponential size in the bounded-depth Frege proof system. This resolves a question by Razborov asking whether the Lovász-Schrijver proof system requires $n^\delta$ rounds to refute these formulas for some $\delta &gt; 0$. The results are obtained by a worst-case to average-case reduction of these formulas relying on a topological embedding theorem which may be of independent interest.</summary>
    <updated>2021-02-20T18:26:59Z</updated>
    <published>2021-02-20T18:26:59Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-04T03:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings</id>
    <link href="https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings.html" rel="alternate" type="text/html"/>
    <title>Loops, degrees, and matchings</title>
    <summary>A student in my graph algorithms class asked how self-loops in undirected graphs affect the vertex degrees and matchings of a graph. The standard answer is that a self-loop adds two to the degree (because each edge has two endpoints) and that they are useless in matching because matchings should have at most one incidence to each vertex, not two. But that’s just a convention; one could reasonably declare that the contribution of a self-loop to the degree is one, and I’m pretty sure I’ve seen sources that do just that. With that alternative convention, it should be possible to include a self-loop in a matching, and use it to match only a single vertex.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A student in my graph algorithms class asked how <a href="https://11011110.github.io/blog/2021/02/19/Loop (graph theory)">self-loops</a> in undirected graphs affect the vertex degrees and matchings of a graph. The standard answer is that a self-loop adds two to the degree (because each edge has two endpoints) and that they are useless in matching because matchings should have at most one incidence to each vertex, not two. But that’s just a convention; one could reasonably declare that the contribution of a self-loop to the degree is one, and I’m pretty sure I’ve seen sources that do just that. With that alternative convention, it should be possible to include a self-loop in a matching, and use it to match only a single vertex.</p>

<p>However, this turns out not to make much difference to many matching problems, because the following simple transformation turns a problem with self-loops (allowed in matchings in this way) into a problem with no self-loops (so it doesn’t matter whether they are allowed or not). Simply form a <a href="https://en.wikipedia.org/wiki/Covering_graph">double cover</a>\(^*\) of the given graph (let’s call it the “loopless double cover”) by making two copies of the graph and replacing all corresponding pairs of loops by simple edges from one copy to the other. In weighted matching problems, give the replacement edges for the loops the sum of the weights of the two loops they replace; all other edges keep their original weights.</p>

<p style="text-align: center;"><img alt="The loopless double cover of a graph and of one of its loopy matchings" src="https://11011110.github.io/blog/assets/2021/loopless-double-cover.svg"/></p>

<p>Then (unlike the <a href="https://en.wikipedia.org/wiki/Bipartite_double_cover">bipartite double cover</a>, which also eliminates loops) the cardinality or optimal weight of a matching in the loopy graph can be read off from the corresponding solution in its loopless double cover. Any matching of the original loopy graph can be translated into a matching of the loopless cover by applying the same loopless cover translation to the matching instead of to the whole graph; this doubles the total weight of the matching and the total number of matched vertices. And among matchings on the loopless cover, when trying to optimize weight or matched vertices, it is never helpful to match the two copies differently, so there is an optimal solution that can be translated back to the original graph without changing its optimality.</p>

<p>This doesn’t quite work for the problem of finding a matching that maximizes the total number of matched edges, rather than the total number of matched vertices. These two problems are the same in simple graphs, but different in loopy graphs. However, in a loopy graph, if you are trying to maximize matched edges, you might as well include all loops in the matching, and then search for a maximum matching of the simple graph induced by the remaining unmatched vertices. Again, in this case, you don’t get a problem that requires any new algorithms to solve it.</p>

<p>In the case of my student, I only provided the conventional answer, because really all they wanted to know was whether these issues affected how they answered one of the homework questions, and the answer was that the question didn’t involve and didn’t need loops. However it seems that the more-complicated answer is that even if you allow loops to count only one unit towards degree, and to be included in matchings, they don’t change the matching problem much.</p>

<p>\(^*\) This is only actually a covering graph under the convention that the degree of a loop is one. For the usual degree-2 convention for loops, you would need to replace each loop by a pair of parallel edges, forming a multigraph, to preserve the degrees of the vertices.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105762400402127534">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-02-19T18:57:00Z</updated>
    <published>2021-02-19T18:57:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-03-01T01:04:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21277</id>
    <link href="https://gilkalai.wordpress.com/2021/02/19/nostalgia-corner-john-riordans-referee-report-of-my-first-paper/" rel="alternate" type="text/html"/>
    <title>Nostalgia corner: John Riordan’s referee report of my first paper</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In 1971/1972 academic year, I was an undergraduate student at the Hebrew University of Jerusalem and toward the end of the year I wrote a paper about Abel’s sums. I sent it to John Riordan the author of the books  … <a href="https://gilkalai.wordpress.com/2021/02/19/nostalgia-corner-john-riordans-referee-report-of-my-first-paper/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In 1971/1972 academic year, I was an undergraduate student at the Hebrew University of Jerusalem and toward the end of the year I wrote a paper about Abel’s sums. I sent it to John Riordan the author of the books  “Combinatorial Identities” and “Combinatorial Analysis”.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan1.png"><img alt="" class="alignnone size-full wp-image-21279" height="834" src="https://gilkalai.files.wordpress.com/2021/02/riordan1.png?w=640&amp;h=834" width="640"/></a></p>
<p>I received this letter shortly after my 17th birthday, and I was surely very happy to read the sentence <span style="color: #0000ff;">“I think you have had a splendid idea”</span>.  Here is part of Riordan’s remarks. The full report is <a href="https://gilkalai.files.wordpress.com/2021/02/riordan-rep1.pdf">here</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan3.png"><img alt="" class="alignnone size-full wp-image-21280" height="827" src="https://gilkalai.files.wordpress.com/2021/02/riordan3.png?w=640&amp;h=827" width="640"/></a></p>
<p>It took me some time to revise the paper and get it printed. And here is the report for the second version.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan2.png"><img alt="" class="alignnone size-full wp-image-21281" height="839" src="https://gilkalai.files.wordpress.com/2021/02/riordan2.png?w=640&amp;h=839" width="640"/></a></p>
<p>And here is part of Riordan’s second round of remarks. The full report is <a href="https://gilkalai.files.wordpress.com/2021/02/riordan-rep2.pdf">here</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan4.png"><img alt="" class="alignnone size-full wp-image-21282" height="843" src="https://gilkalai.files.wordpress.com/2021/02/riordan4.png?w=640&amp;h=843" width="640"/></a></p>
<p>I was certainly happy to read the following sentence: <span style="color: #0000ff;">“I would remark that the result for  <em>p = -1 </em> is new and perhaps <span style="color: #ff0000;">the simplest derivation of Abel’s result</span>.”</span></p>
<p>In 1978 I actually visited John Riordan in his office at Rockefeller University, NYC. I remember him as very cheerful and he told me that when his first book appeared he was working at Bell Labs and his managers wanted to talk to him. He was a bit worried that they would not approve of him spending time and effort to write a book in pure mathematics. But actually, they gave him a salary raise!</p>
<p>(If you have a picture of John Riordan, please send me.)</p>
<p>In 1979 the paper <a href="https://gilkalai.files.wordpress.com/2021/02/1-s2.0-0097316579900475-main-1.pdf">appeared</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan5.png"><img alt="" class="alignnone size-full wp-image-21286" src="https://gilkalai.files.wordpress.com/2021/02/riordan5.png?w=640"/></a></p></div>
    </content>
    <updated>2021-02-19T09:23:53Z</updated>
    <published>2021-02-19T09:23:53Z</published>
    <category term="Combinatorics"/>
    <category term="personal"/>
    <category term="Abel sums"/>
    <category term="John Riordan"/>
    <category term="Niels Henrik Abel"/>
    <category term="refereeing"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-03-04T03:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18125</id>
    <link href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/" rel="alternate" type="text/html"/>
    <title>Computing’s Role In The Pandemic</title>
    <summary>How can we help? Joe Biden is the 46th president of the USA. Note is called a centered triangular number. These numbers obey the formula: and start with The previous one, the , was Herbert Hoover, hmmm. Biden has promised to make controlling the Covid-19 pandemic one of his top priorities. Today I thought we […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>How can we help?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/unknown-145/" rel="attachment wp-att-18132"><img alt="" class="alignright size-full wp-image-18132" src="https://rjlipton.files.wordpress.com/2021/02/unknown.jpeg?w=600"/></a></p>
<p>
Joe Biden is the 46th president of the USA. Note <img alt="{46}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B46%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{46}"/> is called a <a href="https://en.wikipedia.org/wiki/Centered_triangular_number">centered triangular number</a>. These numbers obey the formula: 	</p>
<p align="center"><img alt="\displaystyle  \frac{3n^2 + 3n + 2}{2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B3n%5E2+%2B+3n+%2B+2%7D%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \frac{3n^2 + 3n + 2}{2} "/></p>
<p>and start with <img alt="{1,4,10,19,31,46,\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2C4%2C10%2C19%2C31%2C46%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1,4,10,19,31,46,\dots}"/> The previous one, the <img alt="{31^{st}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B31%5E%7Bst%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{31^{st}}"/>, was Herbert Hoover, hmmm. Biden has promised to make controlling the Covid-19 <a href="https://en.wikipedia.org/wiki/COVID-19_pandemic">pandemic</a> one of his top priorities. </p>
<p>
Today I thought we would discuss how he might use computer technology to help get the virus under control.<br/>
<span id="more-18125"/></p>
<p>
First, we thank the drug companies since we now have <a href="https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States#Vaccines">vaccines</a> that work against the virus. Without these we would have little chance to bring the pandemic under control at all. </p>
<p>
Second, we must state that we are worried that the virus is mutating and this may render the current vaccines less useful, if not useless. We hope this is not happening, or that the drug companies will be able to respond with vaccine boosters. Today there seems to be <a href="https://www.usnews.com/news/health-news/articles/2021-02-18/pfizer-coronavirus-vaccine-protects-against-uk-south-africa-variants-study-shows">good news</a> and <a href="https://www.reuters.com/article/us-health-coronavirus-vaccines-variants/pfizer-says-south-african-variant-could-significantly-reduce-protective-antibodies-idUSKBN2AH2VG">bad news</a>.  </p>
<p>
Results will fluctuate, but in any case, vaccines will definitely play a key role in defeating the pandemic. We want to ask the same about computing technology.</p>
<p>
</p><p/><h2> Computing’s Role—I </h2><p/>
<p/><p>
There are many web sites that discuss how computing technology can play a role in defeating the pandemic. Here are some of the main points:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Tracking People:</i> Many places are interested in tracking who are sick. Tracking can by itself help stop the spreading of the virus, and thus help save lives. For example, <a href="https://www.computer.org/publications/tech-news/five-ways-tech-is-being-used-to-fight-covid-19">IEEE</a> says: </p>
<blockquote><p><b> </b> <em> “We believe software can help combat this global pandemic, and that’s why we’re launching the Code Against COVID-19 initiative…,” said Weiting Liu, founder and CEO of Arc. “From tracking outbreaks and reducing the spread to scaling testing and supporting healthcare, teams around the world are using software to flatten the curve. The eMask app (real-time mask inventory in Taiwan) and TraceTogether (contact tracing in Singapore) are just two of the many examples.” </em>
</p></blockquote>
<p/><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Changing Behavior:</i> A powerful idea is to avoid human to human contact and thus stop the spread of the virus. For example, here are <a href="https://www.weforum.org/agenda/2020/04/10-technology-trends-coronavirus-covid19-pandemic-robotics-telehealth">examples</a> from a longer list of ideas: </p>
<ul>
<li>
Robot Deliveries; <p/>
</li><li>
Digital and Contactless Payments; <p/>
</li><li>
Remote Work and Remote Learning and more.
</li></ul>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Changing Health Delivery:</i> An important idea is how can we reduce the risk of health delivery. A paradox is that health care may need to be avoided, since traditional delivery requires human contact. There are many examples of ways to make health care online, and therefore safer. Shwetak Patel won the 2018 ACM Prize in Computing for contributions to creative and practical sensing systems for sustainability and health. He outlined here <a href="https://cccblog.org/2020/09/24/what-role-can-computing-play-in-battling-the-covid-19-pandemic/">CCC blog</a> how health care could be made more online.</p>
<p>
</p><p/><h2> Computing’s Role—II </h2><p/>
<p/><p>
The above ideas are fine but I believe the real role for computing is simple: </p>
<blockquote><p><b> </b> <em> <i>Make signing up and obtaining an appointment for a vaccine easier, fairer, and sooner.</i> </em>
</p></blockquote>
<p/><p>
In the US each state is in charge of running web sites that allow people to try and get an appointment for a vaccine shot. <i>Try</i> is the key word. Almost all sites require an appointment to get a shot—walk-ins are mostly not allowed. </p>
<p>
I cannot speak for all states and all web sites, but my direct experience is that the sites are terrible. Signing up for a vaccination shot is a disaster. The web sites that I have seen are poorly written, clumsy, and difficult to use. They are some of the worst sites I have ever needed to use, for anything. Some of the top issues: </p>
<ol>
<li>
The sites require you to sign in each time from scratch. <p/>
</li><li>
The sites require you to sign in each time from scratch. <p/>
</li><li>
The sites require you to sign in each time from scratch. <p/>
</li><li>
The sites rules are confusing and unclear. <p/>
</li><li>
You may need to search for particular vaccine locations, rather than for any locations. <p/>
</li><li>
And more <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\dots}"/>
</li></ol>
<p>Repeating (1,2,3) is a poor joke, but one that reflects reality. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
If Amazon, Google, Apple had sites that worked this way, they would be out of business quickly. Perhaps this is the key: <i>Can our top companies help build the state sites?</i> Is it too late to help? See <a href="https://www.nytimes.com/2021/01/12/technology/the-problem-with-vaccine-websites.html">here</a> for a New York Times article on this issue: </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/game/" rel="attachment wp-att-18129"><img alt="" class="aligncenter wp-image-18129" src="https://rjlipton.files.wordpress.com/2021/02/game.png?w=300" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<blockquote><p><b> </b> <em> When you start to pull your hair out because you can’t register for a vaccine on a local website, remember that it’s not (only) the fault of a bad tech company or misguided choices by government leaders today. It’s a systematic failure years in the making. </em>
</p></blockquote>
<p/><p>
Also is the issue of <a href="https://medium.com/berkeleyischool/fairness-in-the-age-of-algorithms-feb11c56a709">algorithmic fairness</a> relevant here? We know that it is unfortunately easy to have web sites that are unfair—that assign vaccine sign up dates unfairly, that favor one class of people over another. </p>
<p/></font></font></div>
    </content>
    <updated>2021-02-19T03:03:24Z</updated>
    <published>2021-02-19T03:03:24Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="19"/>
    <category term="covid"/>
    <category term="states"/>
    <category term="vaccine"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-03-04T03:20:48Z</updated>
    </source>
  </entry>
</feed>
