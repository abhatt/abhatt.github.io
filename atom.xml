<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-05-27T20:21:53Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.12844</id>
    <link href="http://arxiv.org/abs/2005.12844" rel="alternate" type="text/html"/>
    <title>Approximation Schemes for ReLU Regression</title>
    <feedworld_mtime>1590537600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diakonikolas:Ilias.html">Ilias Diakonikolas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goel:Surbhi.html">Surbhi Goel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karmalkar:Sushrut.html">Sushrut Karmalkar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klivans:Adam_R=.html">Adam R. Klivans</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soltanolkotabi:Mahdi.html">Mahdi Soltanolkotabi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.12844">PDF</a><br/><b>Abstract: </b>We consider the fundamental problem of ReLU regression, where the goal is to
output the best fitting ReLU with respect to square loss given access to draws
from some unknown distribution. We give the first efficient, constant-factor
approximation algorithm for this problem assuming the underlying distribution
satisfies some weak concentration and anti-concentration conditions (and
includes, for example, all log-concave distributions). This solves the main
open problem of Goel et al., who proved hardness results for any exact
algorithm for ReLU regression (up to an additive $\epsilon$). Using more
sophisticated techniques, we can improve our results and obtain a
polynomial-time approximation scheme for any subgaussian distribution. Given
the aforementioned hardness results, these guarantees can not be substantially
improved.
</p>
<p>Our main insight is a new characterization of surrogate losses for nonconvex
activations. While prior work had established the existence of convex
surrogates for monotone activations, we show that properties of the underlying
distribution actually induce strong convexity for the loss, allowing us to
relate the global minimum to the activation's Chow parameters.
</p></div>
    </summary>
    <updated>2020-05-27T01:20:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.12648</id>
    <link href="http://arxiv.org/abs/2005.12648" rel="alternate" type="text/html"/>
    <title>On the improvement of the in-place merge algorithm parallelization</title>
    <feedworld_mtime>1590537600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Berenger Bramas, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bramas:Quentin.html">Quentin Bramas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.12648">PDF</a><br/><b>Abstract: </b>In this paper, we present several improvements in the parallelization of the
in-place merge algorithm, which merges two contiguous sorted arrays into one
with an O(T) space complexity (where T is the number of threads). The approach
divides the two arrays into as many pairs of partitions as there are threads
available; such that each thread can later merge a pair of partitions
independently of the others. We extend the existing method by proposing a new
algorithm to find the median of two partitions. Additionally, we provide a new
strategy to divide the input arrays where we minimize the data movement, but at
the cost of making this stage sequential. Finally, we provide the so-called
linear shifting algorithm that swaps two partitions in-place with contiguous
data access. We emphasize that our approach is straightforward to implement and
that it can also be used for external (out of place) merging. The results
demonstrate that it provides a significant speedup compared to sequential
executions, when the size of the arrays is greater than a thousand elements.
</p></div>
    </summary>
    <updated>2020-05-27T01:23:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.12568</id>
    <link href="http://arxiv.org/abs/2005.12568" rel="alternate" type="text/html"/>
    <title>Topological Drawings meet Classical Theorems from Convex Geometry</title>
    <feedworld_mtime>1590537600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bergold:Helena.html">Helena Bergold</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Felsner:Stefan.html">Stefan Felsner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scheucher:Manfred.html">Manfred Scheucher</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schr=ouml=der:Felix.html">Felix Schröder</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Steiner:Raphael.html">Raphael Steiner</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.12568">PDF</a><br/><b>Abstract: </b>In this article we discuss classical theorems from Convex Geometry in the
context of topological drawings. In a simple topological drawing of the
complete graph $K_n$, any two edges share at most one point: either a common
vertex or a point where they cross. Triangles of simple topological drawings
can be viewed as convex sets, this gives a link to convex geometry.
</p>
<p>We present a generalization of Kirchberger's Theorem, a family of simple
topological drawings with arbitrarily large Helly number, and a new proof of a
topological generalization of Carath\'{e}odory's Theorem in the plane. We also
discuss further classical theorems from Convex Geometry in the context of
simple topological drawings.
</p>
<p>We introduce "generalized signotopes" as a generalization of topological
drawings. As indicated by the name they are a generalization of signotopes, a
structure studied in the context of encodings for arrangements of pseudolines.
</p></div>
    </summary>
    <updated>2020-05-27T01:20:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.12414</id>
    <link href="http://arxiv.org/abs/2005.12414" rel="alternate" type="text/html"/>
    <title>On Optimal Partitioning For Sparse Matrices In Variable Block Row Format</title>
    <feedworld_mtime>1590537600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahrens:Peter.html">Peter Ahrens</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boman:Erik_G=.html">Erik G. Boman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.12414">PDF</a><br/><b>Abstract: </b>The Variable Block Row (VBR) format is an influential blocked sparse matrix
format designed to represent shared sparsity structure between adjacent rows
and columns. VBR consists of groups of adjacent rows and columns, storing the
resulting blocks that contain nonzeros in a dense format. This reduces the
memory footprint and enables optimizations such as register blocking and
instruction-level parallelism. Existing approaches use heuristics to determine
which rows and columns should be grouped together. We adapt and optimize a
dynamic programming algorithm for sequential hypergraph partitioning to produce
a linear time algorithm which can determine the optimal partition of rows under
an expressive cost model, assuming the column partition remains fixed.
Furthermore, we show that the problem of determining an optimal partition for
the rows and columns simultaneously is NP-Hard under a simple linear cost
model.
</p>
<p>To evaluate our algorithm empirically against existing heuristics, we
introduce the 1D-VBR format, a specialization of VBR format where columns are
left ungrouped. We evaluate our algorithms on all 1626 real-valued matrices in
the SuiteSparse Matrix Collection. When asked to minimize an empirically
derived cost model for a sparse matrix-vector multiplication kernel, our
algorithm produced partitions whose 1D-VBR realizations achieve a speedup of at
least 1.18 over an unblocked kernel on 25% of the matrices, and a speedup of at
least 1.59 on 12.5% of the matrices. The 1D-VBR representation produced by our
algorithm had faster SpMVs than the 1D-VBR representations produced by any
existing heuristics on 87.8% of the test matrices.
</p></div>
    </summary>
    <updated>2020-05-27T01:20:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11736</id>
    <link href="http://arxiv.org/abs/2005.11736" rel="alternate" type="text/html"/>
    <title>Efficient Intervention Design for Causal Discovery with Latents</title>
    <feedworld_mtime>1590537600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Addanki:Raghavendra.html">Raghavendra Addanki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kasiviswanathan:Shiva_Prasad.html">Shiva Prasad Kasiviswanathan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McGregor:Andrew.html">Andrew McGregor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musco:Cameron.html">Cameron Musco</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11736">PDF</a><br/><b>Abstract: </b>We consider recovering a causal graph in presence of latent variables, where
we seek to minimize the cost of interventions used in the recovery process. We
consider two intervention cost models: (1) a linear cost model where the cost
of an intervention on a subset of variables has a linear form, and (2) an
identity cost model where the cost of an intervention is the same, regardless
of what variables it is on, i.e., the goal is just to minimize the number of
interventions. Under the linear cost model, we give an algorithm to identify
the ancestral relations of the underlying causal graph, achieving within a
$2$-factor of the optimal intervention cost. This approximation factor can be
improved to $1+\epsilon$ for any $\epsilon &gt; 0$ under some mild restrictions.
Under the identity cost model, we bound the number of interventions needed to
recover the entire causal graph, including the latent variables, using a
parameterization of the causal graph through a special type of colliders. In
particular, we introduce the notion of $p$-colliders, that are colliders
between pair of nodes arising from a specific type of conditioning in the
causal graph, and provide an upper bound on the number of interventions as a
function of the maximum number of $p$-colliders between any two nodes in the
causal graph.
</p></div>
    </summary>
    <updated>2020-05-27T01:22:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=749</id>
    <link href="https://emanueleviola.wordpress.com/2020/05/26/fast-stuff/" rel="alternate" type="text/html"/>
    <title>Fast stuff</title>
    <summary>5-second UFC knockout Bullet chess</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>5-second UFC <a href="https://www.youtube.com/watch?v=_rCTmFxQ4pM">knockout</a></p>



<p><a href="https://www.youtube.com/watch?v=gqyAc4VnObc">Bullet chess</a></p></div>
    </content>
    <updated>2020-05-26T12:37:20Z</updated>
    <published>2020-05-26T12:37:20Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2020-05-27T20:21:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4816</id>
    <link href="https://www.scottaaronson.com/blog/?p=4816" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4816#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4816" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Collapsing Leviathan</title>
    <summary xml:lang="en-US">I was seriously depressed for the last week, by noticeably more than my baseline amount for the new pandemic-ravaged world. The depression seems to have been triggered by two pieces of news: The US Food and Drug Administration—yes, the same FDA whose failure to approve covid tests in February infamously set the stage for the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>I was seriously depressed for the last week, by noticeably more than my baseline amount for the new pandemic-ravaged world.  The depression seems to have been triggered by two pieces of news:</p>



<ol><li>The US Food and Drug Administration—yes, the same FDA whose failure to approve covid tests in February infamously set the stage for the deaths of 100,000 Americans—has now <em>also</em> <a href="https://www.nytimes.com/2020/05/15/us/coronavirus-testing-seattle-bill-gates.html">banned</a> the Gates Foundation’s program for at-home covid testing.  This, it seems to me, is not the sort of thing that could happen in a still-functioning society, one where people valued their own and their neighbors’ physical survival, and viewed rules and regulations as merely instruments to that end.  It’s the sort of thing that one imagines in the waning years of a doomed empire, when no one pretends anymore that they can fix or improve the Leviathan; they’re all just scurrying to flee the Leviathan as it collapses with a thud.  More broadly, I <em>still</em> don’t think that the depth of America’s humiliation and downfall has sunk in to most Americans.  For me, it starts and ends with a single observation: <strong>where fifty years ago we landed humans on the moon, today we can no longer make or distribute paper masks, even when hundreds of thousands of lives depend on it.</strong>  Look, there are many countries, like Taiwan and New Zealand, that managed to protect both their economies <em>and</em> their vulnerable citizens’ lives, by crushing the virus early.  Then there are countries that waited, until they faced an excruciating choice between the two.  But here in the US, we’ve somehow achieved the worst of both worlds—triggering a second Great Depression while <em>also</em> utterly failing to control the virus.  Can we abandon the charade of treating this as a legible “policy choice,” to be debated in earnest thinkpieces?  To me, it just feels like the death-spasm of a collapsing Leviathan.</li><li>Something that, at first glance, might seem trivial by comparison, but isn’t: the University of California system—ignoring the <a href="https://senate.universityofcalifornia.edu/_files/reports/kkb-jn-standardized-testing.pdf">advice</a> of its own Academic Senate, and at the apparent insistence of its chancellor Janet Napolitano—will now <a href="https://timesofsandiego.com/education/2020/05/24/the-story-behind-university-of-californias-landmark-decision-to-ditch-the-sat/">permanently end</a> the use of the SAT and ACT in undergraduate admissions.  This is widely expected, probably correctly, to trigger a chain reaction, whereby one US university after the next will abandon standardized tests.  As a result, admissions to the top US universities—and hence, most chances for social advancement in the US—will henceforth be based <em>entirely</em> on shifting and nebulous criteria that rich, well-connected kids and their parents spend most of their lives figuring out, rather than merely <em>mostly</em> based on such criteria.  The last side door for smart noncomformist kids is now being slammed shut.  From now on, in the US, the <em>only</em> paths to success that clearly delineate their rules will be sports, gambling, reality TV, and the like.  In case it matters to anyone reading this, I feel certain that a 15-year-old me wouldn’t stand a chance in the emerging regime—any more than nerdy Jewish kids did in the USSR of the 1970s, or the US of the 1920s.  (As I’ve <a href="https://www.scottaaronson.com/blog/?p=2003">previously recounted</a> on this blog, the US’s “holistic” college admissions system, with its baffling-to-foreigners emphasis on “character,” “leadership,” “well-roundedness,” etc. rather than test scores, originated in a successful push a century ago by the presidents of Harvard, Princeton, and Yale to keep Jewish enrollments down.  Today the system fulfills precisely the same function, except against Asian-Americans rather than Jews.)  Ironically but predictably, the death of the SAT—i.e., of one of the most fearsome weapons against entrenched wealth and power ever devised—is being <em>celebrated</em> by the self-described champions of the underdog.  I have one question for those champions: do you not understand what your system will <em>actually</em> do to society’s underdogs?  Or do you understand perfectly well, and approve?</li></ol>



<p>To put it bluntly—since events like these leave no room for euphemism—a hundred thousand Americans are now dead from covid, and hundreds of thousands more are poised to die, because smart people are no longer in charge.  And the death of the SAT will help ensure that smart people will never be <em>back</em> in charge.  Obama might be remembered by history as America’s last smart-person-in-charge, its last competent technocrat—but one man couldn’t stop a tidal wave of stupid.</p>



<p>I know from experience what many will readers will say to all this: “instead of wallowing in gloom, Scott, why don’t you just make falsifiable predictions about the bad outcomes you expect from these developments, and then score yourself later?”</p>



<p>So here’s the thing about that.</p>



<p>Shortly after Trump was elected, I changed this blog’s background to black, as a small way to mourn the United States that I’d grown up thinking that I lived in, the one that had at least some ideals.  Today, with four years of hindsight, my thinking then feels <em>overly optimistic</em>: why plain black?  Why not, like, images of rotting corpses in a pit?</p>



<p>And yet, were I foolish enough to register predictions in 2016, I would’ve said that within one year, Trump’s staggering incompetence would <em>surely</em> cause some catastrophe or other to grip the country—a really obvious one, with mass death and even Trump’s beloved stock market cratering.</p>



<p>And then after a year, commenters would ridicule me, because none of that had happened.  After two years, they’d ridicule me again because it <em>still</em> hadn’t happened, and after three years they’d ridicule me a third time.</p>



<p>Now it’s happened.</p>



<p>America, we now know, is like the cartoon character who runs off a cliff: it dangled in midair for three years, defying physics, before it finally looked down.</p>



<p>Look, I’m a theoretical computer scientist.  By training, I deal in asymptotics, not in constant factors.  I don’t often make predictions with deadlines; when I do, I often regret it.  It’s a good thing that I became an academic rather than an investor!  For I’ve learned that the only “oracular power” I have is to make statements like:</p>



<blockquote class="wp-block-quote"><p>My eyes, my brain, and the pit of my stomach are all blaring at me that the asymptotics of this situation just took a sharp turn for the worse.  Sure, for an unknown length of time, noise and constant factors could mask the effects.  But eventually, either (1) society will need to reverse what it just did, or else (2) terrible effects will spring from it, or else (3) the entire universe no longer makes sense.</p></blockquote>



<p>When I’ve felt this way in the past, option (3) rarely turned out to be the right answer.</p>



<p>So, what can anyone say that will make me less depressed?  Thanks in advance!</p></div>
    </content>
    <updated>2020-05-26T12:12:41Z</updated>
    <published>2020-05-26T12:12:41Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Self-Referential"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-05-27T13:17:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.12169</id>
    <link href="http://arxiv.org/abs/2005.12169" rel="alternate" type="text/html"/>
    <title>Depth-2 QAC circuits cannot simulate quantum parity</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pad=eacute=:Daniel.html">Daniel Padé</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fenner:Stephen.html">Stephen Fenner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grier:Daniel.html">Daniel Grier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thierauf:Thomas.html">Thomas Thierauf</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.12169">PDF</a><br/><b>Abstract: </b>We show that the quantum parity gate on $n &gt; 3$ qubits cannot be cleanly
simulated by a quantum circuit with two layers of arbitrary C-SIGN gates of any
arity and arbitrary 1-qubit unitary gates, regardless of the number of allowed
ancilla qubits. This is the best known and first nontrivial separation between
the parity gate and circuits of this form. The same bounds also apply to the
quantum fanout gate. Our results are incomparable with those of Fang et al.
[3], which apply to any constant depth but require a sublinear number of
ancilla qubits on the simulating circuit.
</p></div>
    </summary>
    <updated>2020-05-26T22:24:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.12065</id>
    <link href="http://arxiv.org/abs/2005.12065" rel="alternate" type="text/html"/>
    <title>On the Problem of $p_1^{-1}$ in Locality-Sensitive Hashing</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahle:Thomas_Dybdahl.html">Thomas Dybdahl Ahle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.12065">PDF</a><br/><b>Abstract: </b>A Locality-Sensitive Hash (LSH) function is called
$(r,cr,p_1,p_2)$-sensitive, if two data-points with a distance less than $r$
collide with probability at least $p_1$ while data points with a distance
greater than $cr$ collide with probability at most $p_2$. These functions form
the basis of the successful Indyk-Motwani algorithm (STOC 1998) for nearest
neighbour problems. In particular one may build a $c$-approximate nearest
neighbour data structure with query time $\tilde O(n^\rho/p_1)$ where
$\rho=\frac{\log1/p_1}{\log1/p_2}\in(0,1)$. That is, sub-linear time, as long
as $p_1$ is not too small. This is significant since most high dimensional
nearest neighbour problems suffer from the curse of dimensionality, and can't
be solved exact, faster than a brute force linear-time scan of the database.
</p>
<p>Unfortunately, the best LSH functions tend to have very low collision
probabilities, $p_1$ and $p_2$. Including the best functions for Cosine and
Jaccard Similarity. This means that the $n^\rho/p_1$ query time of LSH is often
not sub-linear after all, even for approximate nearest neighbours!
</p>
<p>In this paper, we improve the general Indyk-Motwani algorithm to reduce the
query time of LSH to $\tilde O(n^\rho/p_1^{1-\rho})$ (and the space usage
correspondingly.) Since $n^\rho p_1^{\rho-1} &lt; n \Leftrightarrow p_1 &gt; n^{-1}$,
our algorithm always obtains sublinear query time, for any collision
probabilities at least $1/n$. For $p_1$ and $p_2$ small enough, our improvement
over all previous methods can be \emph{up to a factor $n$} in both query time
and space.
</p>
<p>The improvement comes from a simple change to the Indyk-Motwani algorithm,
which can easily be implemented in existing software packages.
</p></div>
    </summary>
    <updated>2020-05-26T22:44:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11985</id>
    <link href="http://arxiv.org/abs/2005.11985" rel="alternate" type="text/html"/>
    <title>Stratified Formal Deformations and Intersection Homology of Data Point Clouds</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Banagl:Markus.html">Markus Banagl</a>, Tim Mäder, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sadlo:Filip.html">Filip Sadlo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11985">PDF</a><br/><b>Abstract: </b>Intersection homology is a topological invariant which detects finer
information in a space than ordinary homology. Using ideas from classical
simple homotopy theory, we construct local combinatorial transformations on
simplicial complexes under which intersection homology remains invariant. In
particular, we obtain the notions of stratified formal deformations and
stratified spines of a complex, leading to reductions of complexes prior to
computation of intersection homology. We implemented the algorithmic execution
of such transformations, as well as the calculation of intersection homology,
and apply these algorithms to investigate the intersection homology of
stratified spines in Vietoris-Rips type complexes associated to point sets
sampled near given, possibly singular, spaces.
</p></div>
    </summary>
    <updated>2020-05-26T23:11:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11912</id>
    <link href="http://arxiv.org/abs/2005.11912" rel="alternate" type="text/html"/>
    <title>Symmetric Linear Programming Formulations for Minimum Cut with Applications to TSP</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carr:Robert_D=.html">Robert D. Carr</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iglesias:Jennifer.html">Jennifer Iglesias</a>, Giuseppe Lanciac, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moseley:Benjamin.html">Benjamin Moseley</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11912">PDF</a><br/><b>Abstract: </b>We introduce multiple symmetric LP relaxations for minimum cut problems. The
relaxations give optimal and approximate solutions when the input is a
Hamiltonian cycle. We show that this leads to one of two interesting results.
In one case, these LPs always give optimal and near optimal solutions, and then
they would be the smallest known symmetric LPs for the problems considered.
Otherwise, these LP formulations give strictly better LP relaxations for the
traveling salesperson problem than the subtour relaxation. We have the smallest
known LP formulation that is a 9/8-approximation or better for min-cut. In
addition, the LP relaxation of min-cut investigated in this paper has
interesting constraints; the LP contains only a single typical min-cut
constraint and all other constraints are typically only used for max-cut
relaxations.
</p></div>
    </summary>
    <updated>2020-05-26T22:44:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11818</id>
    <link href="http://arxiv.org/abs/2005.11818" rel="alternate" type="text/html"/>
    <title>Proper Learning, Helly Number, and an Optimal SVM Bound</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bousquet:Olivier.html">Olivier Bousquet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanneke:Steve.html">Steve Hanneke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moran:Shay.html">Shay Moran</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhivotovskiy:Nikita.html">Nikita Zhivotovskiy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11818">PDF</a><br/><b>Abstract: </b>The classical PAC sample complexity bounds are stated for any Empirical Risk
Minimizer (ERM) and contain an extra logarithmic factor $\log(1/{\epsilon})$
which is known to be necessary for ERM in general. It has been recently shown
by Hanneke (2016) that the optimal sample complexity of PAC learning for any VC
class C is achieved by a particular improper learning algorithm, which outputs
a specific majority-vote of hypotheses in C. This leaves the question of when
this bound can be achieved by proper learning algorithms, which are restricted
to always output a hypothesis from C.
</p>
<p>In this paper we aim to characterize the classes for which the optimal sample
complexity can be achieved by a proper learning algorithm. We identify that
these classes can be characterized by the dual Helly number, which is a
combinatorial parameter that arises in discrete geometry and abstract
convexity. In particular, under general conditions on C, we show that the dual
Helly number is bounded if and only if there is a proper learner that obtains
the optimal joint dependence on $\epsilon$ and $\delta$.
</p>
<p>As further implications of our techniques we resolve a long-standing open
problem posed by Vapnik and Chervonenkis (1974) on the performance of the
Support Vector Machine by proving that the sample complexity of SVM in the
realizable case is $\Theta((n/{\epsilon})+(1/{\epsilon})\log(1/{\delta}))$,
where $n$ is the dimension. This gives the first optimal PAC bound for
Halfspaces achieved by a proper learning algorithm, and moreover is
computationally efficient.
</p></div>
    </summary>
    <updated>2020-05-26T22:45:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11796</id>
    <link href="http://arxiv.org/abs/2005.11796" rel="alternate" type="text/html"/>
    <title>Walrasian Equilibria in Markets with Small Demands</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deligkas:Argyrios.html">Argyrios Deligkas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Melissourgos:Themistoklis.html">Themistoklis Melissourgos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Spirakis:Paul_G=.html">Paul G. Spirakis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11796">PDF</a><br/><b>Abstract: </b>We study the complexity of finding a Walrasian equilibrium in markets where
the agents have $k$-demand valuations, where $k$ is a constant. This means that
the maximum value of every agent comes from a bundle of size at most $k$. Our
results are threefold. For unit-demand agents, where the existence of a
Walrasian equilibrium is guaranteed, we show that the problem is in quasi-NC.
Put differently, we give the first parallel algorithm that finds a Walrasian
equilibrium in polylogarithmic time. This comes in striking contrast to all
existing algorithms that are highly sequential. For $k=2$, we show that it is
NP-hard to decide if a Walrasian equilibrium exists even if the valuations are
submodular, while for $k=3$ the hardness carries over to budget-additive
valuations. In addition, we give a polynomial-time algorithm for markets with
2-demand single-minded valuations, or unit-demand valuations. Our last set of
results consists of polynomial-time algorithms for $k$-demand valuations in
unbalanced markets; markets where the number of items is significantly larger
than the number of agents, or vice versa.
</p></div>
    </summary>
    <updated>2020-05-26T22:30:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11766</id>
    <link href="http://arxiv.org/abs/2005.11766" rel="alternate" type="text/html"/>
    <title>The Weisfeiler-Leman dimension of distance-hereditary graphs</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gavrilyuk:Alexander_L=.html">Alexander L. Gavrilyuk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nedela:Roman.html">Roman Nedela</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Ponomarenko:Ilia.html">Ilia Ponomarenko</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11766">PDF</a><br/><b>Abstract: </b>A graph is said to be distance-hereditary if the distance function in every
connected induced subgraph is the same as in the graph itself. We prove that
the ordinary Weisfeiler-Leman algorithm correctly tests the isomorphism of any
two graphs if one of them is distance-hereditary; more precisely, the
Weisfeiler-Leman dimension of the class of finite distance-hereditary graphs is
equal to $2$. The previously best known upper bound for the dimension was $7$.
</p></div>
    </summary>
    <updated>2020-05-26T22:26:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11758</id>
    <link href="http://arxiv.org/abs/2005.11758" rel="alternate" type="text/html"/>
    <title>On the impact of treewidth in the computational complexity of freezing dynamics</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goles:Eric.html">Eric Goles</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Montealegre:Pedro.html">Pedro Montealegre</a>, Martín Ríos-Wilson, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Theyssier:Guillaume.html">Guillaume Theyssier</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11758">PDF</a><br/><b>Abstract: </b>An automata network is a network of entities, each holding a state from a
finite set and evolving according to a local update rule which depends only on
its neighbors in the network's graph. It is freezing if there is an order on
states such that the state evolution of any node is non-decreasing in any
orbit. They are commonly used to model epidemic propagation, diffusion
phenomena like bootstrap percolation or cristal growth. In this paper we
establish how treewidth and maximum degree of the underlying graph are key
parameters which influence the overall computational complexity of finite
freezing automata networks. First, we define a general model checking formalism
that captures many classical decision problems: prediction, nilpotency,
predecessor, asynchronous reachability. Then, on one hand, we present an
efficient parallel algorithm that solves the general model checking problem in
NC for any graph with bounded degree and bounded treewidth. On the other hand,
we show that these problems are hard in their respective classes when
restricted to families of graph with polynomially growing treewidth. For
prediction, predecessor and asynchronous reachability, we establish the
hardness result with a fixed set-defiend update rule that is universally hard
on any input graph of such families.
</p></div>
    </summary>
    <updated>2020-05-26T22:22:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11668</id>
    <link href="http://arxiv.org/abs/2005.11668" rel="alternate" type="text/html"/>
    <title>Quadratic Sieve Factorization Quantum Algorithm and its Simulation</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhatia:Amandeep_Singh.html">Amandeep Singh Bhatia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Ajay.html">Ajay Kumar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11668">PDF</a><br/><b>Abstract: </b>Quantum computing is a winsome field that concerns with the behaviour and
nature of energy at the quantum level to improve the efficiency of
computations. In recent years, quantum computation is receiving much attention
for its capability to solve difficult problems efficiently in contrast to
classical computers. Specifically, some well-known public-key cryptosystems
depend on the difficulty of factoring large numbers, which takes a very long
time. It is expected that the emergence of a quantum computer has the potential
to break such cryptosystems by 2020 due to the discovery of powerful quantum
algorithms (Shor's factoring, Grover's search algorithm and many more). In this
paper, we have designed a quantum variant of the second fastest classical
factorization algorithm named "Quadratic Sieve". We have constructed the
simulation framework of quantized quadratic sieve algorithm using high-level
programming language Mathematica. Further, the simulation results are performed
on a classical computer to get a feel of the quantum system and proved that it
is more efficient than its classical variants from computational complexity
point of view.
</p></div>
    </summary>
    <updated>2020-05-26T22:43:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11654</id>
    <link href="http://arxiv.org/abs/2005.11654" rel="alternate" type="text/html"/>
    <title>A Note on the Concrete Hardness of the Shortest Independent Vectors Problem in Lattices</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aggarwal:Divesh.html">Divesh Aggarwal</a>, Eldon Chung <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11654">PDF</a><br/><b>Abstract: </b>Bl\"omer and Seifert showed that $\mathsf{SIVP}_2$ is NP-hard to approximate
by giving a reduction from $\mathsf{CVP}_2$ to $\mathsf{SIVP}_2$ for constant
approximation factors as long as the $\mathsf{CVP}$ instance has a certain
property. In order to formally define this requirement on the $\mathsf{CVP}$
instance, we introduce a new computational problem called the Gap Closest
Vector Problem with Bounded Minima. We adapt the proof of Bl\"omer and Seifert
to show a reduction from the Gap Closest Vector Problem with Bounded Minima to
$\mathsf{SIVP}$ for any $\ell_p$ norm for some constant approximation factor
greater than $1$.
</p>
<p>In a recent result, Bennett, Golovnev and Stephens-Davidowitz showed that
under Gap-ETH, there is no $2^{o(n)}$-time algorithm for approximating
$\mathsf{CVP}_p$ up to some constant factor $\gamma \geq 1$ for any $1 \leq p
\leq \infty$. We observe that the reduction in their paper can be viewed as a
reduction from $\mathsf{Gap3SAT}$ to the Gap Closest Vector Problem with
Bounded Minima. This, together with the above mentioned reduction, implies
that, under Gap-ETH, there is no $2^{o(n)}$-time algorithm for approximating
$\mathsf{SIVP}_p$ up to some constant factor $\gamma \geq 1$ for any $1 \leq p
\leq \infty$.
</p></div>
    </summary>
    <updated>2020-05-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11622</id>
    <link href="http://arxiv.org/abs/2005.11622" rel="alternate" type="text/html"/>
    <title>Unsupervised Geometric Disentanglement for Surfaces via CFAN-VAE</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>N. Joseph Tatro, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schonsheck:Stefan_C=.html">Stefan C. Schonsheck</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lai:Rongjie.html">Rongjie Lai</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11622">PDF</a><br/><b>Abstract: </b>For non-Euclidean data such as meshes of humans, a prominent task for
generative models is geometric disentanglement; the separation of latent codes
for intrinsic (i.e. identity) and extrinsic (i.e. pose) geometry. This work
introduces a novel mesh feature, the conformal factor and normal feature
(CFAN), for use in mesh convolutional autoencoders. We further propose
CFAN-VAE, a novel architecture that disentangles identity and pose using the
CFAN feature and parallel transport convolution. CFAN-VAE achieves this
geometric disentanglement in an unsupervised way, as it does not require label
information on the identity or pose during training. Our comprehensive
experiments, including reconstruction, interpolation, generation, and canonical
correlation analysis, validate the effectiveness of the unsupervised geometric
disentanglement. We also successfully detect and recover geometric
disentanglement in mesh convolutional autoencoders that encode xyz-coordinates
directly by registering its latent space to that of CFAN-VAE.
</p></div>
    </summary>
    <updated>2020-05-26T22:52:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11621</id>
    <link href="http://arxiv.org/abs/2005.11621" rel="alternate" type="text/html"/>
    <title>ManifoldPlus: A Robust and Scalable Watertight Manifold Surface Generation Method for Triangle Soups</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Jingwei.html">Jingwei Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Yichao.html">Yichao Zhou</a>, Leonidas Guibas <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11621">PDF</a><br/><b>Abstract: </b>We present ManifoldPlus, a method for robust and scalable conversion of
triangle soups to watertight manifolds. While many algorithms in computer
graphics require the input mesh to be a watertight manifold, in practice many
meshes designed by artists are often for visualization purposes, and thus have
non-manifold structures such as incorrect connectivity, ambiguous face
orientation, double surfaces, open boundaries, self-intersections, etc.
Existing methods suffer from problems in the inputs with face orientation and
zero-volume structures. Additionally most methods do not scale to meshes of
high complexity. In this paper, we propose a method that extracts exterior
faces between occupied voxels and empty voxels, and uses a projection-based
optimization method to accurately recover a watertight manifold that resembles
the reference mesh. Compared to previous methods, our methodology is simpler.
It does not rely on face normals of the input triangle soups and can accurately
recover zero-volume structures. Our algorithm is scalable, because it employs
an adaptive Gauss-Seidel method for shape optimization, in which each step is
an easy-to-solve convex problem. We test ManifoldPlus on ModelNet10 and
AccuCity datasets to verify that our methods can generate watertight meshes
ranging from object-level shapes to city-level models. Furthermore, through our
experimental evaluations, we show that our method is more robust, efficient and
accurate than the state-of-the-art. Our implementation is publicly available.
</p></div>
    </summary>
    <updated>2020-05-26T22:52:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11617</id>
    <link href="http://arxiv.org/abs/2005.11617" rel="alternate" type="text/html"/>
    <title>MeshODE: A Robust and Scalable Framework for Mesh Deformation</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Jingwei.html">Jingwei Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Chiyu_Max.html">Chiyu Max Jiang</a>, Baiqiang Leng, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Bin.html">Bin Wang</a>, Leonidas Guibas <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11617">PDF</a><br/><b>Abstract: </b>We present MeshODE, a scalable and robust framework for pairwise CAD model
deformation without prespecified correspondences. Given a pair of shapes, our
framework provides a novel shape feature-preserving mapping function that
continuously deforms one model to the other by minimizing fitting and rigidity
losses based on the non-rigid iterative-closest-point (ICP) algorithm. We
address two challenges in this problem, namely the design of a powerful
deformation function and obtaining a feature-preserving CAD deformation. While
traditional deformation directly optimizes for the coordinates of the mesh
vertices or the vertices of a control cage, we introduce a deep bijective
mapping that utilizes a flow model parameterized as a neural network. Our
function has the capacity to handle complex deformations, produces deformations
that are guaranteed free of self-intersections, and requires low rigidity
constraining for geometry preservation, which leads to a better fitting quality
compared with existing methods. It additionally enables continuous deformation
between two arbitrary shapes without supervision for intermediate shapes.
Furthermore, we propose a robust preprocessing pipeline for raw CAD meshes
using feature-aware subdivision and a uniform graph template representation to
address artifacts in raw CAD models including self-intersections, irregular
triangles, topologically disconnected components, non-manifold edges, and
nonuniformly distributed vertices. This facilitates a fast deformation
optimization process that preserves global and local details. Our code is
publicly available.
</p></div>
    </summary>
    <updated>2020-05-26T22:53:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11614</id>
    <link href="http://arxiv.org/abs/2005.11614" rel="alternate" type="text/html"/>
    <title>Optimized Quantum Circuit Partitioning</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Omid Daei, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navi:Keivan.html">Keivan Navi</a>, Mariam Zomorodi-Moghadam <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11614">PDF</a><br/><b>Abstract: </b>The main objective of this paper is to improve the communication costs in
distributed quantum circuits. To this end, we present a method for generating
distributed quantum circuits from monolithic quantum circuits in such a way
that communication between partitions of a distributed quantum circuit is
minimized. Thus, the communication between distributed components is performed
at a lower cost. Compared to existing works, our approach can effectively map a
quantum circuit into an appropriate number of distributed components. Since
teleportation is usually the protocol used to connect components in a
distributed quantum circuit, our approach ultimately reduces the number of
teleportations. The results of applying our approach to the benchmark quantum
circuits determine its effectiveness and show that partitioning is a necessary
step in constructing distributed quantum circuit.
</p></div>
    </summary>
    <updated>2020-05-26T22:51:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11589</id>
    <link href="http://arxiv.org/abs/2005.11589" rel="alternate" type="text/html"/>
    <title>MaxSAT Resolution and Subcube Sums</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filmus:Yuval.html">Yuval Filmus</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahajan:Meena.html">Meena Mahajan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sood:Gaurav.html">Gaurav Sood</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vinyals:Marc.html">Marc Vinyals</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11589">PDF</a><br/><b>Abstract: </b>We study the MaxRes rule in the context of certifying unsatisfiability. We
show that it can be exponentially more powerful than tree-like resolution, and
when augmented with weakening (the system MaxResW), p-simulates tree-like
resolution. In devising a lower bound technique specific to MaxRes (and not
merely inheriting lower bounds from Res), we define a new semialgebraic proof
system called the SubCubeSums proof system. This system, which p-simulates
MaxResW, is a special case of the Sherali-Adams proof system. In expressivity,
it is the integral restriction of conical juntas studied in the contexts of
communication complexity and extension complexity. We show that it is not
simulated by Res. Using a proof technique qualitatively different from the
lower bounds that MaxResW inherits from Res, we show that Tseitin
contradictions on expander graphs are hard to refute in SubCubeSums. We also
establish a lower bound technique via lifting: for formulas requiring large
degree in SubCubeSums, their XOR-ification requires large size in SubCubeSums.
</p></div>
    </summary>
    <updated>2020-05-26T22:24:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11547</id>
    <link href="http://arxiv.org/abs/2005.11547" rel="alternate" type="text/html"/>
    <title>DartMinHash: Fast Sketching for Weighted Sets</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Christiani:Tobias.html">Tobias Christiani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11547">PDF</a><br/><b>Abstract: </b>Weighted minwise hashing is a standard dimensionality reduction technique
with applications to similarity search and large-scale kernel machines. We
introduce a simple algorithm that takes a weighted set $x \in \mathbb{R}_{\geq
0}^{d}$ and computes $k$ independent minhashes in expected time $O(k \log k +
\Vert x \Vert_{0}\log( \Vert x \Vert_1 + 1/\Vert x \Vert_1))$, improving upon
the state-of-the-art BagMinHash algorithm (KDD '18) and representing the
fastest weighted minhash algorithm for sparse data. Our experiments show
running times that scale better with $k$ and $\Vert x \Vert_0$ compared to ICWS
(ICDM '10) and BagMinhash, obtaining $10$x speedups in common use cases. Our
approach also gives rise to a technique for computing fully independent
locality-sensitive hash values for $(L, K)$-parameterized approximate near
neighbor search under weighted Jaccard similarity in optimal expected time
$O(LK + \Vert x \Vert_0)$, improving on prior work even in the case of
unweighted sets.
</p></div>
    </summary>
    <updated>2020-05-26T22:46:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11541</id>
    <link href="http://arxiv.org/abs/2005.11541" rel="alternate" type="text/html"/>
    <title>Finding Small Satisfying Assignments Faster Than Brute Force: A Fine-grained Perspective into Boolean Constraint Satisfaction</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=uuml=nnemann:Marvin.html">Marvin Künnemann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marx:D=aacute=niel.html">Dániel Marx</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11541">PDF</a><br/><b>Abstract: </b>To study the question under which circumstances small solutions can be found
faster than by exhaustive search (and by how much), we study the fine-grained
complexity of Boolean constraint satisfaction with size constraint exactly $k$.
More precisely, we aim to determine, for any finite constraint family, the
optimal running time $f(k)n^{g(k)}$ required to find satisfying assignments
that set precisely $k$ of the $n$ variables to $1$.
</p>
<p>Under central hardness assumptions on detecting cliques in graphs and
3-uniform hypergraphs, we give an almost tight characterization of $g(k)$ into
four regimes: (1) Brute force is essentially best-possible, i.e., $g(k) = (1\pm
o(1))k$, (2) the best algorithms are as fast as current $k$-clique algorithms,
i.e., $g(k)=(\omega/3\pm o(1))k$, (3) the exponent has sublinear dependence on
$k$ with $g(k) \in [\Omega(\sqrt[3]{k}), O(\sqrt{k})]$, or (4) the problem is
fixed-parameter tractable, i.e., $g(k) = O(1)$.
</p>
<p>This yields a more fine-grained perspective than a previous FPT/W[1]-hardness
dichotomy (Marx, Computational Complexity 2005). Our most interesting technical
contribution is a $f(k)n^{4\sqrt{k}}$-time algorithm for SubsetSum with
precedence constraints parameterized by the target $k$ -- particularly the
approach, based on generalizing a bound on the Frobenius coin problem to a
setting with precedence constraints, might be of independent interest.
</p></div>
    </summary>
    <updated>2020-05-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1908.04263</id>
    <link href="http://arxiv.org/abs/1908.04263" rel="alternate" type="text/html"/>
    <title>Computation of Jacobi sums of order l^2 and 2l^2 with prime l</title>
    <feedworld_mtime>1590451200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahmed:Md=_Helal.html">Md. Helal Ahmed</a>, Dr. Jagmohan Tanti, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pushp:Sumant.html">Sumant Pushp</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1908.04263">PDF</a><br/><b>Abstract: </b>In this paper, we present the fast computational algorithms for the Jacobi
sums of orders $l^2$ and $2l^{2}$ with odd prime $l$ by formulating them in
terms of the minimum number of cyclotomic numbers of the corresponding orders.
We also implement two additional algorithms to validate these formulae, which
are also useful for the demonstration of the minimality of cyclotomic numbers
required.
</p></div>
    </summary>
    <updated>2020-05-26T22:46:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3492</id>
    <link href="https://agtb.wordpress.com/2020/05/25/guide-to-virtual-ec-2020/" rel="alternate" type="text/html"/>
    <title>Guide to Virtual EC 2020</title>
    <summary>From the Virtual Transition Team for EC 2020: Due to concerns regarding the novel coronavirus COVID-19, the 2020 ACM Conference on Economics and Computation (EC 2020) will be held virtually.  This change of format offers exciting new opportunities. This guide can also be found on the EC 2020 webpage. Overview June 15 – 19: Mentoring […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="font-weight: 400;"><i>From the Virtual Transition Team for EC 2020:</i> Due to concerns regarding the novel coronavirus COVID-19, the 2020 ACM Conference on Economics and Computation (EC 2020) will be held virtually.  This change of format offers exciting new opportunities. This guide can also be found on the </span><a href="http://ec20.sigecom.org/participation/covid/"><span style="font-weight: 400;">EC 2020 webpage</span></a><span style="font-weight: 400;">.</span></p>
<h2><span style="font-weight: 400;">Overview</span></h2>
<p><b>June 15 – 19:</b><span style="font-weight: 400;"> Mentoring Workshop and Live Tutorial Pre-recording Sessions.</span><br/>
<b>June 22 – July 3:</b><span style="font-weight: 400;"> Live EC Paper Pre-recording Plenary Sessions.</span><br/>
<b>July 13:</b><span style="font-weight: 400;"> Tutorial Watch Parties, Business Meeting and Poster Session</span><br/>
<b>July 14 – 16:</b><span style="font-weight: 400;"> EC Conference (Paper Watch Parties, Paper Poster Sessions, and Plenaries).</span><br/>
<b>July 17 – 22:</b><span style="font-weight: 400;"> Workshops.</span></p>
<h2><span style="font-weight: 400;">Philosophy</span></h2>
<p><span style="font-weight: 400;">The planning committee has been hard at work revisioning EC 2020 as a virtual event.  The event aims to emphasize opportunities afforded by the virtual format with activities that are intractable in the physical format, but not to recreate aspects of the physical conference experience that are difficult virtually.  Some aspects of the event will look similar to a physical conference, while some will be quite different.  The desiderata for the selected format are:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Maximize exposure for EC papers.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Maximize interaction between community members.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Allow broad global accessibility.</span></li>
</ul>
<p><span style="font-weight: 400;">The organizational team studied the results of many virtual conferences that have already taken place (</span><a href="https://aamas2020.conference.auckland.ac.nz/"><span style="font-weight: 400;">AAMAS</span></a><span style="font-weight: 400;">, </span><a href="https://cacm.acm.org/blogs/blog-cacm/243882-the-asplos-2020-online-conference-experience/fulltext"><span style="font-weight: 400;">ASPLOS</span></a><span style="font-weight: 400;">, </span><a href="https://www.eurosys2020.org/"><span style="font-weight: 400;">EuroSys</span></a><span style="font-weight: 400;">, </span><a href="https://iccp2020.engr.wustl.edu/vsetup.html?fbclid=IwAR3ONLEftxPsTmW4b1oTyCjvAFWWOxWRWS4bV6kTox1yP8AUpgSAbkL76Qc"><span style="font-weight: 400;">ICCP</span></a><span style="font-weight: 400;">, </span><a href="https://medium.com/@iclr_conf/gone-virtual-lessons-from-iclr2020-1743ce6164a3"><span style="font-weight: 400;">ICLR</span></a><span style="font-weight: 400;">, </span><a href="https://www.daniellitt.com/blog/2020/4/20/wagon-lessons-learned"><span style="font-weight: 400;">WAGON</span></a><span style="font-weight: 400;">, among others, as well as the </span><a href="https://people.clarkson.edu/~jmatthew/acm/VirtualConferences_GuideToBestPractices_CURRENT.pdf"><span style="font-weight: 400;">ACM guidelines</span></a><span style="font-weight: 400;">).  Some general rules of thumb are (a) prerecording talks, (b) emphasizing posters, (c) expecting about four hours a day of participation, (d) prime time is 11am-3pm ET.  </span></p>
<p><span style="font-weight: 400;">The planning committee adopted a strategy that allows the minimal commitment level for authors and attendees of presenting and viewing 18 minute talks and attending activities only during the main EC week of July 13-17.  However, official EC events will run June 15 to July 24 and the planning committee highly encourages members of the community to enjoy a more relaxed pace where all programming is plenary.</span></p>
<h2><span style="font-weight: 400;">Registration</span></h2>
<p><span style="font-weight: 400;">Registration will be mandatory but complementary with ACM SIGecom membership ($10 Professional / $5 Student) which can easily be completed online.  Registration will open on June 1.</span></p>
<h2><span style="font-weight: 400;">Mentoring Workshop and Live Tutorial Pre-recording Sessions</span></h2>
<p><b>June 15-19:</b><span style="font-weight: 400;"> To give junior researchers plenty of time to prepare for the EC conference, the mentoring workshops and tutorial live pre-recording sessions will be held the week of June 15.  In addition to the usual activities, junior students will be paired with senior students who will share their EC itinerary and be available for discussions of papers and events in online chat throughout the duration of the EC events.  There will be three tutorials, each broken into four 45 minute segments and recorded over several days and with live audiences of EC participants.  Students will be able to work together on exercises in between sessions.</span></p>
<h2><span style="font-weight: 400;">Live Paper Pre-recording Plenary Sessions</span></h2>
<p><b>June 22 – July 3:</b><span style="font-weight: 400;">  EC papers will keep the usual 18-minute format.  The planning committee highly encourages authors to choose to pre-record their EC talks in live pre-recording sessions.  Each session comprises three papers and is followed by a virtual coffee break for discussion between the speakers, coauthors, and attendees.  Speakers and attendees are recommended to schedule for 2 hours.  These sessions are all plenary and will be scheduled for synergies in topic and preferred timing of the speakers.  These sessions begin at regular times:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Eastern/China 9:00, 13:00, 17:00, 21:00, 1:00, 5:00.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Central/Israel  8:00, 12:00, 16:00, 20:00, 24:00, 4:00.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Mountain/Europe 7:00, 11:00, 15:00, 19:00, 23:00, 3:00.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Pacific/London  6:00, 10:00, 14:00, 18:00, 22:00, 2:00.</span></li>
</ul>
<p><span style="font-weight: 400;">We expect about three sessions a day over the ten weekdays between June 22 and July 3.  (Pre-recording sessions are optional but highly encouraged for authors of EC papers.)</span></p>
<h2><span style="font-weight: 400;">EC Conference</span></h2>
<p><b>July 13-16: </b><span style="font-weight: 400;">July 13 is tutorial day.  It will include the EC business meeting and a contributed poster session for breaking results and results from other venues.  The main conference runs July 14-July 16 with programming from 9am-5pm Eastern.  This programming includes pre-recorded talk watch parties, poster discussion sessions for the papers, live plenary talks, and live highlights beyond EC.  The paper and poster discussion sessions will run in two-hour blocks as follows:</span></p>
<p><span style="font-weight: 400;">Hour 1: 3-5 parallel tracks of watch parties (3 papers each), with realtime chat with the authors.</span></p>
<p><span style="font-weight: 400;">Hour 2: plenary 1-minute lightning talks for all papers, breakout room for discussion for each paper with poster (a poster is 1-4 slides in PDF). (The lightning talks and poster discussion rooms are optional but highly encouraged for authors of the session’s papers.)</span></p>
<p><span style="font-weight: 400;">As these watch parties will be in parallel sessions and some may be at times that are hard for some members of the community to attend, the planning committee highly encourages participation in the live pre-recording sessions (June 22-July 3) and all talks will be available for individual viewing in advance of the conference. </span></p>
<h2><span style="font-weight: 400;">EC Workshops</span></h2>
<p><b>July 17-22:</b><span style="font-weight: 400;"> Three workshops will take place on the Friday of EC and the following week.  Details for these events are still being worked out by the organizers.</span></p>
<h2><span style="font-weight: 400;">Best Presentation by Student or Postdoctoral Research Award.</span></h2>
<p><span style="font-weight: 400;">The</span><a href="http://www.sigecom.org/award-presentation.html"><span style="font-weight: 400;"> Best Presentation by a Student or Postdoctoral Researcher Award</span></a><span style="font-weight: 400;"> is designed to encourage and acknowledge excellence in oral presentations by students and recent graduates. In addition to an honorarium, this year’s winner will be invited to present in a special session at WINE 2020 with expenses covered by SIGecom. </span><span style="font-weight: 400;">To be considered for the award, the presenter must participate in a live pre-recording session. </span><span style="font-weight: 400;"> This year will feature several “fun” categories for video presentations. Audience nominations are requested for creative and fun videos in the categories of: Best Video, Best Cameo by a Child or Pet, Best Special Effects, Best Soundtrack, or Best [Insert Your Nomination Here]. Submit nominations </span><a href="https://forms.gle/bfWEqDwseGHnciUP6"><span style="font-weight: 400;">here</span></a><span style="font-weight: 400;">.</span></p>
<h2><span style="font-weight: 400;">Virtual Transition Team</span></h2>
<p><span style="font-weight: 400;">Virtual aspects of the conference are being coordinated by a Virtual Transition Team appointed by the SIGecom Executive Committee, that in addition to the </span><a href="http://sigecom.org/officers.html"><span style="font-weight: 400;">Executive Committee</span></a><span style="font-weight: 400;"> and the </span><a href="http://ec20.sigecom.org/committees-acm/organizing-committee/"><span style="font-weight: 400;">EC 2020 organizing committee</span></a><span style="font-weight: 400;"> includes the following officers:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Virtual General Chair: Jason Hartline</span></li>
</ul>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Virtual Local Chair: Yannai Gonczarowski</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Virtual Global Outreach Chairs: Rediet Abebe and Eric Sodomka</span></li>
</ul>
<p><span style="font-weight: 400;">The team is looking forward to interacting with everyone in the community at an outstanding conference!</span></p></div>
    </content>
    <updated>2020-05-25T21:37:35Z</updated>
    <published>2020-05-25T21:37:35Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Jason Hartline</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2020-05-27T20:20:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1531592307894359428</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1531592307894359428/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/oldest-living-baseball-players-can-you.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1531592307894359428" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1531592307894359428" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/oldest-living-baseball-players-can-you.html" rel="alternate" type="text/html"/>
    <title>Oldest Living Baseball Players- can you estimate...</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">(The Baseball season is delayed or cancelled, so I post about baseball instead.)<br/>
<br/>
This post is going to ask a question that you could look up on the web. But what fun with that be?<br/>
<br/>
The following statements are true<br/>
<br/>
1) Don Larsen, a professional baseball player who played from 1953 to 1967, is still alive. He is 90 years old (or perhaps 90 years young---I don't know the state of his health).  He was born Aug 7, 1929. He is best know for pitching a perfect game in the World Series in 1956, pitching for the Yankees. He played for several other teams as well, via trades (this was before free agency).<br/>
(CORRECTION- I wrote this post a while back, and Don Larsen has died since then.)<br/>
<br/>
<br/>
2) White Ford, a professional baseball player who played from 1950 to 1967, is still alive. He is 91 years old (or perhaps 91 years young---I don't know the state of his health).  He was born Oct 21,  1928. He had many great seasons and is in the hall of fame. He played for the New York Yankees and no other team.<br/>
<br/>
3) From 1900 (or so) until 1962 there were 16 professional baseball teams which had 25 people each. From 1962 until 1969 there were 20 teams which had 25 people each. There were also many minor league teams.<br/>
<br/>
4) The youngest ballplayers are usually around 20. The oldest around 35. These are not exact numbers<br/>
<br/>
SO here is my question: Try to estimate<br/>
<br/>
1) How many LIVING  retired major league baseball players are there now who are older than Don Larsen?<br/>
<br/>
2) How many LIVING retired major league baseball players are of an age between Don and Whitey?<br/>
<br/>
3) How  many LIVING retired major league baseball players are older than Whitey Ford?<br/>
<br/>
Give your REASONING for your answer.<br/>
<br/></div>
    </content>
    <updated>2020-05-25T16:34:00Z</updated>
    <published>2020-05-25T16:34:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-05-25T16:34:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17076</id>
    <link href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/" rel="alternate" type="text/html"/>
    <title>Proof of the Diagonal Lemma in Logic</title>
    <summary>Why is the proof so short yet so difficult? Saeed Salehi is a logician at the University of Tabriz in Iran. Three years ago he gave a presentation at a Moscow workshop on proofs of the diagonal lemma. Today I thought I would discuss the famous diagonal lemma. The lemma is related to Georg Cantor’s […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Why is the proof so short yet so difficult?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/saeed_salehi4/" rel="attachment wp-att-17078"><img alt="" class="alignright  wp-image-17078" src="https://rjlipton.files.wordpress.com/2020/05/saeed_salehi4.jpg?w=200" width="200"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Saeed Salehi is a logician at the University of Tabriz in Iran. Three years ago he gave a <a href="http://wrm17.mi.ras.ru/slides/Salehi.pdf">presentation</a> at a Moscow workshop on proofs of the diagonal lemma.</p>
<p>
Today I thought I would discuss the famous <a href="https://en.wikipedia.org/wiki/Diagonal_lemma">diagonal lemma</a>. </p>
<p>
The lemma is related to Georg Cantor’s famous diagonal argument yet is different. The logical version imposes requirements on when the argument applies, and requires that it be expressible within a formal system. </p>
<p>
The lemma underpins Kurt Gödel’s famous 1931 <a href="http://www.w-k-essler.de/pdfs/goedel.pdf">proof</a> that arithmetic is incomplete. However, Gödel did not state it as a lemma or proposition or theorem or anything else. Instead, he focused his attention on what we now call Gödel numbering. We consider this today as “obvious” but his paper’s title ended with “Part I”. And he had readied a “Part II” with over 100 pages of calculations should people question that his numbering scheme was expressible within the logic. </p>
<p>
Only after his proof was understood did people realize that one part, perhaps the trickiest part, could be abstracted into a powerful lemma. The tricky part is <em>not</em> the Gödel numbering. People granted that it can be brought within the logic once they saw enough of Gödel’s evidence, and so we may write <img alt="{\ulcorner \phi \urcorner}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Culcorner+%5Cphi+%5Curcorner%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ulcorner \phi \urcorner}"/> for the function giving the Gödel number of any formula <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> and use that in other formulas. The hard part is what one <em>does</em> with such expressions. </p>
<p>
This is what we will try to motivate.</p>
<p>
</p><p/><h2> Tracing the Lemma </h2><p/>
<p/><p>
Rudolf Carnap is often credited with the first formal statement, in 1934, for instance by Eliott Mendelson in his famous <a href="https://www.goodreads.com/book/show/250868.Introduction_to_Mathematical_Logic">textbook</a> on logic. Carnap was a <a href="https://en.wikipedia.org/wiki/Rudolf_Carnap">member</a> of the <a href="https://en.wikipedia.org/wiki/Vienna_Circle">Vienna Circle</a>, which Gödel frequented, and Carnap is <a href="http://texts.cdlib.org/view?docId=hb6h4nb3q7&amp;doc.view=frames&amp;chunk.id=div00004&amp;toc.depth=1&amp;toc.id=">considered</a> a giant among twentieth-century philosophers. He worked on sweeping grand problems of philosophy, including logical positivism and analysis of human language via syntax before semantics. Yet it strikes us with irony that his work on the lemma may be the best remembered.</p>
<p>
Who did the lemma first? Let’s leave that for others and move on to the mystery of how to prove the lemma once it is stated. I must say the lemma is easy to state, easy to remember, and has a short proof. But I believe that the proof is not easy to remember or even follow. </p>
<p>
Salehi’s <a href="http://wrm17.mi.ras.ru/slides/Salehi.pdf">presentation</a> quotes others’ opinions about the proof:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Sam Buss: “Its proof [is] quite simple but rather tricky and difficult to conceptualize.”</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> György Serény (we jump to Serény’s <a href="https://arxiv.org/pdf/math/0606425.pdf">paper</a>): “The proof of the lemma as it is presented in textbooks on logic is not self-evident to say the least.”</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Wayne Wasserman: “It is `Pulling a Rabbit Out of the Hat’—Typical Diagonal Lemma Proofs Beg the Question.”</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/hat/" rel="attachment wp-att-17079"><img alt="" class="aligncenter size-full wp-image-17079" src="https://rjlipton.files.wordpress.com/2020/05/hat.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
So I am not alone, and I thought it might be useful to try and unravel its proof. This exercise helped me and maybe it will help you.</p>
<p>
Here goes. </p>
<p>
</p><p/><h2> Stating the Lemma </h2><p/>
<p/><p>
Let <img alt="{S(w)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28w%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S(w)}"/> be a formula in Peano Arithmetic (<img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{PA}"/>). We claim that there is some sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++PA+%5Cvdash+%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). "/></p>
<p>Formally, </p>
<blockquote><p><b>Lemma 1</b> <em> Suppose that <img alt="{S(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S(x)}"/> is some formula in <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/>. Then there is a sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\phi}"/> so that 	</em></p><em>
<p align="center"><img alt="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++PA+%5Cvdash+%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
The beauty of this lemma is that it was used by Gödel and others to prove various powerful theorems. For example, the lemma quickly proves this result of Alfred Tarski:</p>
<blockquote><p><b>Theorem 2</b> <em> Suppose that <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/> is consistent. Then <i>truth</i> cannot be defined in <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/>. That is there is <b>no</b> formula <img alt="{Tr(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BTr%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{Tr(x)}"/> so that for all sentences <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\phi}"/> <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/> proves 	</em></p><em>
<p align="center"><img alt="\displaystyle  \phi \iff Tr(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff Tr(\ulcorner \phi \urcorner). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
The proof is this. Assume there is such a formula <img alt="{Tr(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BTr%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Tr(x)}"/>. Then use the diagonal lemma and get 	</p>
<p align="center"><img alt="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+%5Cneg+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner)."/></p>
<p>This shows that 	</p>
<p align="center"><img alt="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner) \iff Tr(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+%5Cneg+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29+%5Ciff+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner) \iff Tr(\ulcorner \phi \urcorner). "/></p>
<p>This is a contradiction. A short proof. </p>
<p>
</p><p/><h2> The Proof </h2><p/>
<p/><p>
The key is to define the function <img alt="{F(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(n)}"/> as follows: Suppose that <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is the Gödel number of a formula of the form <img alt="{A(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A(x)}"/> for some variable <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> then 	</p>
<p align="center"><img alt="\displaystyle  F(n) = \ulcorner A(\ulcorner A(x) \urcorner) \urcorner. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28n%29+%3D+%5Culcorner+A%28%5Culcorner+A%28x%29+%5Curcorner%29+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(n) = \ulcorner A(\ulcorner A(x) \urcorner) \urcorner. "/></p>
<p>If <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is not of this form then define <img alt="{F(n)=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(n)=0}"/>. This is a strange function, a clever function, but a perfectly fine function, It certainly maps numbers to numbers. It is certainly recursive, actually it is clearly computable in polynomial time for any reasonable Gödel numbering. Note: the function <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> does depend on the choice of the variable <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Thus, 	</p>
<p align="center"><img alt="\displaystyle  F(\ulcorner y=0 \urcorner) = \ulcorner (\ulcorner y=0 \urcorner)=0 \urcorner, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+y%3D0+%5Curcorner%29+%3D+%5Culcorner+%28%5Culcorner+y%3D0+%5Curcorner%29%3D0+%5Curcorner%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(\ulcorner y=0 \urcorner) = \ulcorner (\ulcorner y=0 \urcorner)=0 \urcorner, "/></p>
<p>and 	</p>
<p align="center"><img alt="\displaystyle  F(\ulcorner x=0 \urcorner) = \ulcorner (\ulcorner x=0 \urcorner)=0 \urcorner. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+x%3D0+%5Curcorner%29+%3D+%5Culcorner+%28%5Culcorner+x%3D0+%5Curcorner%29%3D0+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(\ulcorner x=0 \urcorner) = \ulcorner (\ulcorner x=0 \urcorner)=0 \urcorner. "/></p>
<p>	          Now we make two definitions:</p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++g%28w%29+%26%5Cequiv%26+S%28F%28w%29%29+%5C%5C++++++++%5Cphi+%26%5Cequiv%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} "/></p>
<p>
Now we compute just using the definitions of <img alt="{F, g, \phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%2C+g%2C+%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F, g, \phi}"/>:</p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)) \\           &amp;=&amp; S(\ulcorner g(\ulcorner g(x) \urcorner) \urcorner) \\               &amp;=&amp; S(\ulcorner \phi \urcorner). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++%5Cphi+%26%3D%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5C%5C+++++++++++++++++%26%3D%26+S%28F%28%5Culcorner+g%28x%29+%5Curcorner%29%29+%5C%5C+++++++++++%26%3D%26+S%28%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner%29+%5C%5C+++++++++++++++%26%3D%26+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)) \\           &amp;=&amp; S(\ulcorner g(\ulcorner g(x) \urcorner) \urcorner) \\               &amp;=&amp; S(\ulcorner \phi \urcorner). \end{array} "/></p>
<p>We are done.</p>
<p>
</p><p/><h2> But … </h2><p/>
<p/><p>
Where did this proof come from? Suppose that you forgot the proof but remember the statement of the lemma. I claim that we can then reconstruct the proof. </p>
<p>
First let’s ask: Where did the definition of the function <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> come from? Let’s see. Imagine we defined </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++g%28w%29+%26%5Cequiv%26+S%28F%28w%29%29+%5C%5C++++++++%5Cphi+%26%5Cequiv%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} "/></p>
<p>But left <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> undefined for now. Then</p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++%5Cphi+%26%3D%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5C%5C+++++++++++++++++%26%3D%26+S%28F%28%5Culcorner+g%28x%29+%5Curcorner%29%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)). \end{array} "/></p>
<p>But we want <img alt="{\phi = S(\ulcorner \phi \urcorner)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3D+S%28%5Culcorner+%5Cphi+%5Curcorner%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi = S(\ulcorner \phi \urcorner)}"/> that happens provided:</p>
<p align="center"><img alt="\displaystyle  \ulcorner g(\ulcorner g(x) \urcorner) \urcorner) = F(\ulcorner g(x) \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner%29+%3D+F%28%5Culcorner+g%28x%29+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \ulcorner g(\ulcorner g(x) \urcorner) \urcorner) = F(\ulcorner g(x) \urcorner). "/></p>
<p>This essentially gives the definition of the function <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. Pretty neat.</p>
<p>
</p><p/><h2> But but … </h2><p/>
<p/><p>
Okay where did the definition of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> and <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> come from? It is reasonable to define 	</p>
<p align="center"><img alt="\displaystyle  g(w) \equiv S(F(w)), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28w%29+%5Cequiv+S%28F%28w%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g(w) \equiv S(F(w)), "/></p>
<p>for some <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. We cannot change <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> but we can control the input to the formula <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, so let’s put a function there. Hence the definition for <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> is not unreasonable. </p>
<p>
Okay how about the definition of <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/>? Well we could argue that this is the magic step. If we are given this definition then <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> follows, by the above. I would argue that <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> is not completely surprising. The name of the lemma is after all the “diagonal” lemma. So defining <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> as the application of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> to itself is plausible.</p>
<p>
</p><p/><h2> Taking an Exam </h2><p/>
<p/><p>
Another way to think about the diagonal lemma is imagine you are taking an exam in logic. The first question is: </p>
<blockquote><p><b> </b> <em> Prove in <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/> that for any <img alt="{S(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S(x)}"/> there is a sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\phi}"/> so that 	</em></p><em>
<p align="center"><img alt="\displaystyle  \phi \iff S(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff S(\ulcorner \phi \urcorner). "/></p>
</em><p><em/>
</p></blockquote>
<p>You read the question again and think: “I wish I had studied harder, I should have not have checked Facebook last night. And then went out and <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/>” But you think let’s not panic, let’s think.</p>
<p>
Here is what you do. You say let me define 	</p>
<p align="center"><img alt="\displaystyle  g(x) = S(F(x)), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%29+%3D+S%28F%28x%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g(x) = S(F(x)), "/></p>
<p>for some <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. You recall there was a function that depends on <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, and changing the input from <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> to <img alt="{F(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(x)}"/> seems to be safe. Okay you say, now what? I need the definition of <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. Hmmm let me wait on that. I recall vaguely that <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> had a strange definition. I cannot recall it, so let me leave it for now.</p>
<p>
But you think: I need a sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/>. A sentence cannot have an unbound variable. So <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> cannot be <img alt="{g(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(x)}"/>. It could be <img alt="{g(m)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28m%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(m)}"/> for some <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/>. But what could <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> be? How about <img alt="{\ulcorner \phi \urcorner}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Culcorner+%5Cphi+%5Curcorner%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ulcorner \phi \urcorner}"/>. This makes 	</p>
<p align="center"><img alt="\displaystyle  \phi = g(\ulcorner g \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%3D+g%28%5Culcorner+g+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \phi = g(\ulcorner g \urcorner). "/></p>
<p>It is after all the diagonal lemma. Hmmm does this work. Let’s see if this works. Wait as above I get that <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> is now forced to satisfy 	</p>
<p align="center"><img alt="\displaystyle  F(\ulcorner g(x) \urcorner) = \ulcorner g(\ulcorner g(x) \urcorner) \urcorner. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+g%28x%29+%5Curcorner%29+%3D+%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(\ulcorner g(x) \urcorner) = \ulcorner g(\ulcorner g(x) \urcorner) \urcorner. "/></p>
<p>Great this works. I think this is the proof. Wonderful. Got the first question. </p>
<p>
Let’s look at the next exam question. Oh no <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Does this help? Does this unravel the mystery of the proof? Or is it still magic?</p>
<p/><p><br/>
[Fixed equation formatting]</p></font></font></div>
    </content>
    <updated>2020-05-25T15:55:22Z</updated>
    <published>2020-05-25T15:55:22Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="diagonal lemma"/>
    <category term="diagonalization"/>
    <category term="Godel"/>
    <category term="numbering"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-05-27T20:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/benchmarks/</id>
    <link href="https://gradientscience.org/benchmarks/" rel="alternate" type="text/html"/>
    <title>From ImageNet to Image Classification</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/2005.11295" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/ImageNetMultiLabel" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Data
</a>
<br/></p>

<p><i>In our <a href="https://arxiv.org/abs/2005.11295">new paper</a>, we explore how closely
the ImageNet benchmark aligns with the object recognition task it serves as a
proxy for. We find pervasive and systematic deviations of ImageNet annotations
from the ground truth, which can often be attributed to specific design choices
in the data collection pipeline. These issues indicate that ImageNet accuracy
alone might be insufficient to effectively gauge real model performance.  </i></p>

<h2 id="contextualizing-progress-on-benchmarks">Contextualizing Progress on Benchmarks</h2>

<p>Large-scale benchmarks are central to machine learning—they serve both
as concrete targets for model development, and as proxies for assessing model
performance on real-world tasks we actually care about. However, few benchmarks
are perfect, and so as our models get increasingly better at them, we must also
ask ourselves: <i>to what extent is performance on existing benchmarks
indicative of progress on the real-world tasks that motivate them?</i></p>

<p>In this post, we will explore this question of <i>benchmark-task alignment</i>
in the context of the popular
<a href="http://image-net.org/challenges/LSVRC/2012/">ImageNet object recognition dataset</a>.
Specifically, our goal is to understand how well the underlying ground truth is
captured by the dataset itself—this dataset is, after all, what we consider
to be the gold standard during model training and evaluation.</p>

<h3 id="a-sneak-peak-into-imagenet">A sneak peak into ImageNet</h3>
<p>The ImageNet dataset contains over a million images of objects from a thousand,
quite diverse classes. Like many other benchmarks of that scale, ImageNet was
not carefully curated by experts, but instead created via crowd-sourcing,
without perfect quality control. So what does ImageNet data look like? Here are
a few image-label pairs from the dataset:</p>

<div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/0.jpg"/>
      <div class="fake_label">missile</div>
      <div class="true_label">projectile</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/1.jpg"/>
      <div class="fake_label">stage</div>
      <div class="true_label">acoustic guitar</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/2.jpg"/>
      <div class="fake_label">monastery</div>
      <div class="true_label">church</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/3.jpg"/>
      <div class="fake_label">Norwich terrier</div>
      <div class="true_label">Norfolk terrier</div>
    </div>
</div>
<p><br/></p>

<p>These samples appear pretty reasonable…but are they? Actually, while these
are indeed <i>images</i> from the dataset, the labels shown above are
<i>not</i> their actual ImageNet labels!
<a href="https://gradientscience.org/" id="reveal">[Click to see the actual ImageNet labels]</a>
Still, even though not “correct” from the point of view of the ImageNet
dataset, these labels <i>do</i> correspond to actual ImageNet classes, and
appear plausible when you see them in isolation. This shows that for ImageNet
images, which capture objects in diverse real-world conditions, the ImageNet
label may not properly reflect the ground truth.</p>

<p>In our work, we dive into examining how this label misalignment actually
impacts ImageNet: how often do ImageNet labels deviate from the ground truth?
And how do shortcomings in these labels impact ImageNet-trained models?</p>

<h3 id="revisiting-the-imagenet-collection-pipeline">Revisiting the ImageNet collection pipeline</h3>
<p>Before going further, let’s take a look at how ImageNet was created. To build
such a large dataset, the creators of ImageNet had to leverage scalable methods
like automated data collection and crowd-sourcing. That is, they first selected
a set of object classes (using the <a href="https://wordnet.princeton.edu">WordNet</a>
hierarchy), and queried various search engines to obtain a pool of candidate
images. These candidate images were then verified by annotators on <a href="https://www.mturk.com/">Mechanical
Turk (MTurk)</a> using (what we will refer to as) the
<span class="sc">Contains</span> task: annotators were shown images retrieved
for a specific class
label, and were subsequently asked to select the ones that actually contain an
object of this class. Only images that multiple annotators validated ended up
in the final dataset.</p>

<div>
  <div class="stages block">
      <div class="stage rbutton block clicked" id="selection">Class Selection</div>
      <div class="stage rbutton block" id="retrieval">Image Retrieval</div>
      <div class="stage rbutton block" id="filtering">Label Validation</div>
  </div>
  <img id="stage_img" src="https://gradientscience.org/assets/multilabel/pipeline/selection.jpg"/>
</div>
<div class="footnote"> <strong>Imagenet collection pipeline:</strong> Click on a
stage at the top for an illustration. </div>

<p>While this is a natural approach to scalably annotate data (and, in fact, is
commonly used to create large-scale benchmarks—e.g.,
<a href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL VOC</a>,
<a href="http://cocodataset.org/#home">COCO</a>, <a href="http://places.csail.mit.edu/">places</a>),
it has an important caveat. Namely, this process has an inherent bias: the
annotation task itself is phrased as a leading question. ImageNet annotators
were not asked to provide an image label, but instead only to verify if a
<i>specific</i> label (predetermined by the image retrieval process) was
<i>contained</i> in an image. Annotators had no knowledge of what the other
classes in the dataset even were, or the granularity at which they were
required to make distinctions. In fact, they were explicitly instructed to
ignore clutter and obstructions.</p>

<p>Looking back at the ImageNet samples shown above, one can see how this setup
could lead to imperfect annotations. For instance, it is unclear if the average
annotator knows the differences between a “Norwich terrier” and a “Norfolk
terrier”, especially if they don’t even know that both of these (as well as 22
other terrier breeds) are valid ImageNet classes. Also, the
<span class="sc">Contains</span> task itself might be ill-suited for annotating
multi-object images—the answer to the <span class="sc">Contains</span> question would be yes for any
object in the image that corresponds to an ImageNet class. It is not unthinkable
that the same images could have made it into ImageNet under the labels “stage”
and “Norwich terrier” had they come up in the search results for those classes
instead.</p>

<p>Overall, this suggests that the labeling issues in ImageNet may go beyond just
occasional annotator mistakes—the <i>design</i> of the data collection
pipeline itself could have caused these labels to systematically deviate from
the ground truth.</p>

<h3 id="diagnosing-benchmark-task-misalignment">Diagnosing benchmark-task misalignment</h3>

<p>To characterize how wide-spread these deviations are, we first need to get a
better grasp of the ground truth for ImageNet data. In order to do this at
scale, we still need to rely on crowd-sourcing. However, in contrast to the
original label validation setup, we design a new annotation task based directly
on <i>image classification</i>. Namely, we present annotators with a set of
possible labels for a single image <i>simultaneously</i>. We then ask them to
assign one label to every object in the image, and identify what they believe
to be the main object. (Note that we intentionally ask for such fine-grained
image annotations since, as we saw before, a single label might be inherently
insufficient to capture the ground truth.)</p>

<p>Of course, we need to ensure that annotators can meaningfully perform this
task. To this end we  devise a way to narrow down the label choices they are
presented with (all thousand ImageNet classes would be nearly impossible for a
worker to choose between!). Specifically, for each image, we identify the most
relevant labels by pooling together the top-5 predictions of a diverse set of
ImageNet models and filtering them via the <span class="sc">Contains</span>
task. Note that, by doing so, we are effectively bootstrapping the existing
ImageNet labels by first using them to train models and then using model
predictions to get better annotation candidates.</p>

<p>This is what our resulting annotation task looks like:</p>

<p><img src="https://gradientscience.org/assets/multilabel/main_task.jpg"/></p>

<p>We aggregate the responses from multiple annotators to get per-image estimates
of the number of objects in the image (along with their corresponding labels),
as well as which object humans tend to view as the main one.</p>

<p>We collect such <a href="https://github.com/MadryLab/ImageNetMultiLabel">annotations for 10k images from the ImageNet validation
set</a>. With these more
fine-grained and accurate annotations in hand, we now examine where the
original ImageNet labels may fall short.</p>

<h3 id="multi-object-images">Multi-object images</h3>
<p>The simplest way in which ImageNet labels could deviate from the ground truth
is if the image contains multiple objects. So, the first thing we want to
understand is: how many ImageNet images contain objects from more than one
valid class?</p>

<p>It turns out: quite a few! Indeed, more than <b>20%</b> of the images contain
more than one ImageNet object. Examples:</p>

<div class="widget">
  <div class="choices_one">
    <span class="widgetheading">Inspect Image</span>
  </div>
  <div class="choices_img">
    <span class="widgetheading">Image</span>
  </div>
  <div class="choices_info block">
    <span class="widgetheading">Annotation</span>
  </div>
  <div class="choices_one" id="multi"> </div>
  <div class="choices_img widgetheading"> <img id="multi1"/> </div>
  <div class="choices_info block">
    <div class="choices_info_text" id="multiclass"> </div>
  </div>
</div>
<div style="clear: both;"/>
<div class="footnote"> <strong>Multi-object images:</strong> Choose an image on
the left to see the annotations we obtain for it.  </div>

<p>Looking at some of these images, it is clear that the problem is not just
natural image clutter but also the fact that certain objects are quite likely
to co-occur in the real-world—e.g., “table lamp” and “lamp shade”. This means
that choosing classes which in principle correspond to distinct objects (e.g.,
using WordNet) is not enough to guarantee that the corresponding images have
unambiguous labels. For example, see if you can guess the ImageNet label for
the samples below:</p>

<div class="widget">
  <span class="widgetheading" id="coclass">Chosen class pair</span>
  <div class="choices_one_full" id="co">
  <div class="show_labels rbutton block" id="cooc">Show Labels</div>
  </div>
  <div id="coimages" style="border-right: 10px white solid;"> </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Co-occurring objects:</strong> Select a class pair on the top and see if
you can identify which images correspond to each class (click on "Show Labels"
to reveal the answers).
</div>

<h4 id="model-performance-on-multi-object-images">Model performance on multi-object images</h4>

<p>So, how do models deal with images that contain multiple objects? To understand
this, we evaluate a number of models (from AlexNet to EfficientNet-B7), and
measure their accuracy (w.r.t. the ImageNet labels) on such images. We plot
these accuracies below (as a function of their full test accuracy):</p>

<canvas height="200" id="multi_drop" width="400"/>
<div class="footnote">
<strong>Performance on multi-object images:</strong>
Model accuracy variation as a function of the number of objects in the image.
Hover over each data point to see the corresponding model name.
</div>

<p>Across the board, in comparison to their performance on single-object images,
models suffer around a 10% accuracy drop on multi-object ones. At the same
time, this drop more-or-less disappears if we consider a model prediction to be
correct if it matches the label of <i>any</i> valid object in the image (see
the <a href="https://arxiv.org/abs/2005.11295">paper</a> for specifics).</p>

<div class="footnote">
    <strong>Aside:</strong> The original motivation of top-5 accuracy was
    exactly to accommodate such multi-label images. However, we find that, while
    it does largely account for these multi-object confusions, it also seems to
    overestimate accuracy on single-object images.
</div>

<p>Still, even though models seem to struggle with multi-object images, they
perform much better than chance (i.e., better than what one would get if they
were picking the label of an object in the image at random). This makes sense
when the image has a single prominent object that also matches the ImageNet
label. However, for a third of all multi-object images the ImageNet label does
not even match what annotators deem to be the main object in the image. Yet,
even in these cases, models still successfully predict the ImageNet label
(instead of what humans consider to be the right label for the image)!</p>

<canvas height="200" id="overfitting" width="400"/>
<div class="footnote">
<strong>Performance on images with annotator-label disagreement:</strong>
Model accuracy on images where annotators do not consider the ImageNet label to
be the main object. Baseline: accuracy of picking the label of a random
prominent object in the image.
</div>

<p>Here, models seem to base their predictions on biases in the dataset which
humans do not find salient. For instance, models get high accuracy on the class
“pickelhaube”, even though, pickelhaubes are usually present in images with
other, more salient objects, such as “military uniforms”, suggesting that
ImageNet models may be overly sensitive to the presence of distinctive objects
in the image. While exploiting such biases would improve ImageNet accuracy,
this strategy might not translate to improved performance on object recognition
in the wild. Here are a few examples that seem to exhibit a similar mismatch:</p>

<div class="widget">
  <div class="choices_one">
    <span class="widgetheading">Inspect Image</span>
  </div>
  <div class="choices_img">
    <span class="widgetheading">Image</span>
  </div>
  <div class="choices_info block">
    <span class="widgetheading">Annotation</span>
  </div>
  <div class="choices_one" id="main"> </div>
  <div class="choices_img widgetheading"> <img id="main1"/> </div>
  <div class="choices_info block">
    <div class="choices_info_text" id="mainclass"> </div>
  </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Annotator-label disagreement:</strong> Select an image on the left to
see the annotations we obtain for it. Notice that the ImageNet label is
different from the (annotator-selected) "main label".
</div>

<h3 id="biases-in-label-validation">Biases in label validation</h3>

<p>Let us now turn our attention to the ImageNet data filtering process. Recall
that each class in ImageNet was constructed by automatically retrieving many
images and filtering them (via the <span class="sc">Contains</span> task described above). How likely
were annotators to filter out mislabeled images under this setup?</p>

<p>To understand this, we replicate the original filtering process on the existing
ImageNet images. But this time, instead of only asking annotators to check if
the image is valid with respect to its ImageNet label (i.e., the search query),
we also try several other labels (each in isolation, with different sets of
annotators).</p>

<p>We find that annotators frequently deem an image to be valid for <i>many</i>
different labels—even when only one object is present. Typically, this occurs
when the image is ambiguous and lacks enough context (e.g. “seashore” or
“lakeshore”), or annotators are likely confused between different semantically
similar labels (e.g., “assault rifle” vs. “rifle”, dog breeds). It turns out
that this confusion, at least partly, stems from the one-sidedness of the
<span class="sc">Contains</span> task—i.e., asking annotators to ascertain the validity of a specific
label without them knowing about any other options. If instead we present
annotators with all the relevant labels simultaneously and ask them to choose
one (as we did in our annotation setup), this kind of label confusion is
alleviated: annotators select significantly fewer labels in total (see our
<a href="https://arxiv.org/abs/2005.11295">paper</a> for details). So, even putting
annotator’s expertise aside, the specific annotation task setup itself
drastically affects the quality of the resulting dataset labels.</p>

<div>
  <div class="dropdown">
    <div class="rates block" id="dropdownMenuButton">
      Choose annotator threshold
    </div>
    <div class="rates">
      <div class="block rbutton clicked">10%</div>
      <div class="block rbutton">30%</div>
      <div class="block rbutton">50%</div>
    </div>
  </div>
  <img id="filtering_plot" src="https://gradientscience.org/assets/multilabel/filtering/filtering_0.1.jpg"/>
</div>
<div class="footnote">
<strong>Number of valid labels:</strong> Distribution of labels annotators deem
valid (based on an agreement threshold) for single-object images in the <span class="sc">Contains</span>
task. Click the percentages on the top to change the annotator agreement
threshold.
</div>

<p>Going back to ImageNet, our findings give us reason to believe that annotators
may have had a rather limited ability to correct errors in labeling. Thus, in
certain cases, ImageNet labels were largely determined by the automated image
retrieval process—propagating any biases or mixups this process might
introduce to the final dataset.</p>

<p>In fact, we can actually see direct evidence of that in the ImageNet
dataset—there are pairs of classes that appear to be <i>inherently
ambiguous</i> (e.g., “laptop computer” and “notebook computer”) and neither
human annotators, nor models, can tell the corresponding images apart (see
below). If such class pairs actually overlap in terms of their ImageNet images,
it is unclear how models can learn to separate them without memorizing specific
validation examples.</p>

<div class="widget">
  <span class="widgetheading" id="ambclass">Chosen class pair</span>
  <div class="choices_one_full" id="am">
  <div class="show_labels rbutton block" id="amb">Show Labels</div>
  </div>
  <div id="ambimages" style="border-right: 10px white solid;"> </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Ambiguous class pairs:</strong> Select a class pair on the top and see if
you can identify which images correspond to each class (click on "Show Labels"
to reveal the answers).
</div>

<h3 id="beyond-test-accuracy-human-centric-model-evaluation">Beyond test accuracy: human-centric model evaluation</h3>

<p>Performance of ImageNet-trained models is typically judged based on their
ability to predict the dataset labels—yet, as we saw above, these labels may
not fully capture the ground truth. Hence, ImageNet accuracy may not reflect
properly model performance—for instance, measuring accuracy alone could
unfairly penalize models for certain correct predictions on  multi-object
images. So, how can we better assess model performance?</p>

<p>One approach is to measure model-human alignment directly—we present model
predictions to annotators and ask them to gauge their validity:</p>

<canvas height="200" id="limit" width="400"/>
<div class="footnote">
<strong>Human-based model evaluation:</strong> Fraction of annotators that
select a label as valid in the <span class="sc">Contains</span> task (i.e.,
selection frequency) for ImageNet labels and model predictions. Baseline:
(number of correct predictions) × (average selection frequency of the
ImageNet label).
</div>

<p>Surprisingly, we find that for state-of-the-art models, annotators actually
deem the prediction that models make to be valid about as often as the ImageNet
label (even when the two <i> do not</i> match). Thus, recent models may be
better at predicting the ground truth than their top-1 accuracy (w.r.t. the
ImageNet label) would indicate.</p>

<p>However, this does not imply that improving ImageNet accuracy is meaningless.
For instance, non-expert annotators may not be able to tell apart certain
fine-grained class differences (e.g., dog breeds) and for some of these images
the ImageNet label may actually match the ground truth. What it does indicate,
though, is that we are at a point where it may be hard to gauge if better
performance on ImageNet corresponds to actual progress or merely to exploiting
idiosyncrasies of the dataset.</p>

<p>For further experimental details and additional results (e.g., human confusion
matrices), take a look at <a href="https://arxiv.org/abs/2005.11295">our paper</a>!</p>

<h3 id="conclusions">Conclusions</h3>

<p>We took a closer look at how well ImageNet aligns with the real-world object
recognition task—even though ImageNet is used extensively, we rarely question
whether its labels actually reflect the ground truth. We saw that oftentimes
ImageNet labels do not fully capture image content—e.g., many images have
multiple (ImageNet) objects and there are classes that are inherently
ambiguous. As a result, models trained using these labels as ground truth end
up learning unintended biases and confusions.</p>

<p>Our analysis indicates that when creating datasets we must be aware of (and try
to mitigate) ways in which scalable data collection practices can skew the
corresponding annotations (see our
<a href="https://gradientscience.org/data_rep_bias">previous post</a> for another
example of such a skew). Finally, given that such imperfections in our datasets
could be inevitable, we also need to think about how to reliably assess model
performance in their presence.</p>





















<b/><br/><hr/><b/><br/><br/><br/><b/><br/><br/><div class="cooc_img block"><div class="image_label label_cooc"><br/></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg"/></div><div class="amb_img block"><div class="image_label label_amb"><br/></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg"/></div><div class="bias_img block"><div class="image_label label_bias"><br/></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg"/></div></div>
    </summary>
    <updated>2020-05-25T00:00:00Z</updated>
    <published>2020-05-25T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2020-05-26T23:12:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Vienna, TU Vienna, IST Austria, WU Vienna (apply by June 15, 2020)</title>
    <summary>The Vienna Graduate School on Computational Optimization (VGSCO) is a research and training program funded by the Austrian Science Funds (FWF). The VGSCO offers lecture series given by international experts in optimization related fields, organizes research seminars, retreats, soft skills courses, scientific workshops, and social events, provides travel grants, and supports research stays abroad. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Vienna Graduate School on Computational Optimization (VGSCO) is a research and training program funded by the Austrian Science Funds (FWF). The VGSCO offers lecture series given by international experts in optimization related fields, organizes research seminars, retreats, soft skills courses, scientific workshops, and social events, provides travel grants, and supports research stays abroad.</p>
<p>Website: <a href="http://vgsco.univie.ac.at/positions">http://vgsco.univie.ac.at/positions</a><br/>
Email: vgsco@univie.ac.at</p></div>
    </content>
    <updated>2020-05-24T08:48:14Z</updated>
    <published>2020-05-24T08:48:14Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-27T20:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/082</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/082" rel="alternate" type="text/html"/>
    <title>TR20-082 |  MaxSAT Resolution and Subcube Sums | 

	Yuval Filmus, 

	Meena Mahajan, 

	Gaurav Sood, 

	Marc Vinyals</title>
    <summary>We study the MaxRes rule in the context of certifying unsatisfiability. We show that it can be exponentially more powerful than tree-like resolution, and when augmented with weakening (the system MaxResW), p-simulates tree-like resolution. In devising a lower bound technique specific to MaxRes (and not merely inheriting lower bounds from Res), we define a new semialgebraic proof system called the SubCubeSums proof system. This system, which p-simulates MaxResW, is a special case of the Sherali-Adams proof system. In expressivity, it is the integral restriction of conical juntas studied in the contexts of communication complexity and extension complexity. We show that it is not simulated by Res. Using a proof technique qualitatively different from the lower bounds that MaxResW inherits from Res, we show that Tseitin contradictions on expander graphs are hard to refute in SubCubeSums. We also establish a lower bound technique via lifting: for formulas requiring large degree in SubCubeSums, their XOR-ification requires large size in SubCubeSums.</summary>
    <updated>2020-05-23T19:58:01Z</updated>
    <published>2020-05-23T19:58:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-27T20:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=437</id>
    <link href="https://tcsplus.wordpress.com/2020/05/22/tcs-talk-wednesday-may-27-rahul-ilango-mit/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 27 — Rahul Ilango, MIT</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 27th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Rahul Ilango from MIT will speak about “Is it (NP) hard to distinguish order from chaos?” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 27th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Rahul Ilango</strong> from MIT will speak about “<em>Is it (NP) hard to distinguish order from chaos?</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br/>
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: The Minimum Circuit Size Problem (MCSP) roughly asks what the “complexity” of a given string is. Informally, one can think of this as determining the degree of “computational order” a string has.</p>
<p>In the past several years, there has been a resurgence of interest in MCSP. A series of exciting results have begun unraveling what looks to be a fascinating story. This story already reveals deep connections between MCSP and a growing list of fields, including cryptography, learning theory, structural complexity theory, average-case complexity, and circuit complexity. As an example, Santhanam recently proved a conditional equivalence between the complexity of MCSP and the existence of one-way functions.</p>
<p>This talk is split into two parts. The first part is a broad introduction to MCSP, answering the following questions: What is this problem? Why is it interesting? What do we know so far, and where might the story go next? The second part discusses recent joint work with Bruno Loff and Igor Oliveira showing that the “multi-output version” of MCSP is NP-hard.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2020-05-22T22:37:13Z</updated>
    <published>2020-05-22T22:37:13Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-05-27T20:21:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17063</id>
    <link href="https://rjlipton.wordpress.com/2020/05/22/math-tells/" rel="alternate" type="text/html"/>
    <title>Math Tells</title>
    <summary>How to tell what part of math you are from Gerolamo Cardano is often credited with introducing the notion of complex numbers. In 1545, he wrote a book titled Ars Magna. He introduced us to numbers like in his quest to understand solutions to equations. Cardano was often short of money and gambled and played […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>How to tell what part of math you are from</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/unknown-141/" rel="attachment wp-att-17065"><img alt="" class="alignright size-medium wp-image-17065" height="160" src="https://rjlipton.files.wordpress.com/2020/05/unknown-1.jpeg?w=300&amp;h=160" width="300"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Gerolamo Cardano is often credited with introducing the notion of complex numbers. In 1545, he wrote a book titled <i>Ars Magna</i>. He introduced us to numbers like <img alt="{\sqrt{-5}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B-5%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{-5}}"/> in his quest to understand solutions to equations. Cardano was often short of money and gambled and played a certain board game to make money—see the second paragraph <a href="https://en.wikipedia.org/wiki/Gerolamo_Cardano#Mathematics">here</a>. </p>
<p>
Today, for amusement, Ken and I thought we’d talk about tells.</p>
<p>
What are tells? Wikipedia <a href="https://en.wikipedia.org/wiki/Tell_(poker)">says</a>: </p>
<blockquote><p><b> </b> <em> A tell in poker is a change in a player’s behavior or demeanor that is claimed by some to give clues to that player’s assessment of their hand. </em>
</p></blockquote>
<p/><p>
<a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/tell/" rel="attachment wp-att-17068"><img alt="" class="aligncenter size-medium wp-image-17068" height="300" src="https://rjlipton.files.wordpress.com/2020/05/tell.jpg?w=201&amp;h=300" width="201"/></a></p>
<p>
</p><p/><h2> Other Tells </h2><p/>
<p/><p>
Ken and I have been thinking of tells in a wider sense—when and whether one can declare inferences amid uncertain information. Historians face this all the time. So do biographers, at least when their subjects are no longer living. We would also like to make inferences in our current world, such as about the pandemic. The stakes can be higher than in poker. In poker, if your “tell” inference is wrong and you lose, you can play another hand—unless you went all in. With science and other academic areas the attitude must be that you’re all-in all the time.</p>
<p>
Cardano furnishes several instances. Wikipedia—which we regard as an equilibrium of opinions—says that Cardano </p>
<blockquote><p><b> </b> <em> acknowledged the existence of imaginary numbers … [but] did not understand their properties, [which were] described for the first time by his Italian contemporary Rafael Bombelli. </em>
</p></blockquote>
<p/><p>
This is a negative inference from how one of Cardano’s books stops short of treating imaginary numbers as objects that follow rules. </p>
<p>
There are also questions about whether Cardano can be considered “the father of probability” ahead of Blaise Pascal and Pierre de Fermat. Part of the problem is that Cardano’s own writings late in life recounted his first erroneous reasonings as well as final understanding in a Hamlet-like fashion. Wikipedia doubts whether he really knew the rule of multiplying probabilities of independent events, whereas the <a href="http://www.columbia.edu/~pg2113/index_files/Gorroochurn-Some Laws.pdf">essay</a> by Prakash Gorroochurn cited there convinces us that he did. Similar doubt extends to how much Cardano knew about the natural sciences, as correct inferences (such as mountains with seashell fossils having once been underwater) are mixed in with what we today regard as howlers.</p>
<p>
Every staging of Shakespeare’s <i>Hamlet</i> shows a book by Cardano—or does it? In Act II, scene 2, Polonius asks, “What do you read, my lord?”; to which Hamlet first replies “Words words words.” Pressed on the book’s topic, Hamlet perhaps references the section “Misery of Old Age” in Cardano’s 1543 book <i>De Consolatione</i> but what he <a href="https://www.sparknotes.com/nofear/shakespeare/hamlet/page_102/">says</a> is so elliptical it is hard to tell. The book also includes particular allusions between sleep and death that go into Hamlet’s soliloquy opening Act III. The book had been published in England in 1573 as <i>Cardan’s Comfort</i> under the aegis of the Earl of Oxford so it was well-known. Yet the writer Italo Calvino <a href="https://books.google.com/books?id=pQabBQAAQBAJ&amp;pg=PA77&amp;lpg=PA77&amp;dq=Hamlet+words+words+words+Cardano&amp;source=bl&amp;ots=mu1gcM6b8E&amp;sig=ACfU3U3W7te91qlDaIIC8EeHLM1GxpR2Dw&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwikkNjX1MfpAhWChHIEHQaWDZsQ6AEwCXoECAoQAQ#v=onepage&amp;q=Hamlet words words words Cardano&amp;f=false">held back</a> from the inference:</p>
<blockquote><p><b> </b> <em> To conclude from this that the book read by Hamlet is definitely Cardano, as is held by some scholars of Shakespeare’s sources, is perhaps unjustified. </em>
</p></blockquote>
<p/><p>
To be sure, there are some who believe Shakespeare’s main source was Oxford, in manuscripts if not flesh and blood. One reason we do not go there is that we do not see the wider community as having been able to establish reliable principles for judging what kinds of inferences are probably valid. We wonder if one could do an experiment of taking resolved cases, removing most of the information to take them down to the level of unresolved cases, and seeing what kinds of inferences from partial information would have worked.  That’s not our expertise, but within our expertise in math and CS, we wonder if a little experiment will be helpful.</p>
<p>
To set the idea, note that imaginary numbers are also called complex numbers. Yet the term complex numbers can mean other things. Besides numbers like <img alt="{2 + 3i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+%2B+3i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 + 3i}"/> it also can mean how hard it is to <a href="https://en.wikipedia.org/wiki/Integer_complexity">construct</a> a number. </p>
<blockquote><p><b> </b> <em> In number theory, the integer complexity of an integer is the smallest number of ones that can be used to represent it using ones and any number of additions, multiplications, and parentheses. It is always within a constant factor of the logarithm of the given integer. </em>
</p></blockquote>
<p/><p>
How easy is it to tell what kind of “complex” is meant if you only have partial information?  We don’t only mean scope-of-terminology issues; often a well-defined math object is used in multiple areas.  Let’s try an experiment.</p>
<p>
</p><p/><h2> Math Tells </h2><p/>
<p/><p>
Suppose you <s> walk in</s> log-in to a talk without any idea of the topic. If the speaker uses one of these terms can you tell what her talk might be about? Several have multiple meanings. What are some of them? A passing score is <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/></p>
<ol>
<p/><li>
She says let <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> be a <i>c.e.</i> set.<p/>
<p/></li><li>
She says let <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> be in <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega}"/>.<p/>
<p/></li><li>
She says by the König principle.<p/>
<p/></li><li>
She says <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> is a <i>prime</i>.<p/>
<p/></li><li>
She says <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is a <i>prime</i>.<p/>
<p/></li><li>
She says <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is <i>solvable</i>.<p/>
<p/></li><li>
She says let its <i>degree</i> be <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>.<p/>
<p/></li><li>
She says there is a <i>run</i>.<p/>
<p/></li><li>
She says it is <i>reducible</i>.<p/>
<p/></li><li>
She says it is <i>satisfiable</i>.<p/>
</li></ol>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What are your answers? Do you have some tells of your own?</p>
<p/></font></font></div>
    </content>
    <updated>2020-05-22T16:12:56Z</updated>
    <published>2020-05-22T16:12:56Z</published>
    <category term="History"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="poker"/>
    <category term="tells"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-05-27T20:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/081</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/081" rel="alternate" type="text/html"/>
    <title>TR20-081 |  Algebraic Hardness versus Randomness in Low Characteristic | 

	Robert Andrews</title>
    <summary>We show that lower bounds for explicit constant-variate polynomials over fields of characteristic $p &gt; 0$ are sufficient to derandomize polynomial identity testing over fields of characteristic $p$. In this setting, existing work on hardness-randomness tradeoffs for polynomial identity testing requires either the characteristic to be sufficiently large or the notion of hardness to be stronger than the standard syntactic notion of hardness used in algebraic complexity. Our results make no restriction on the characteristic of the field and use standard notions of hardness.

We do this by combining the Kabanets-Impagliazzo generator with a white-box procedure to take $p$-th roots of circuits computing a $p$-th power over fields of characteristic $p$. When the number of variables appearing in the circuit is bounded by some constant, this procedure turns out to be efficient, which allows us to bypass difficulties related to factoring circuits in characteristic $p$.

We also combine the Kabanets-Impagliazzo generator with recent ``bootstrapping'' results in polynomial identity testing to show that a sufficiently-hard family of explicit constant-variate polynomials yields a near-complete derandomization of polynomial identity testing. This result holds over fields of both zero and positive characteristic and complements a recent work of Guo, Kumar, Saptharishi, and Solomon, who obtained a slightly stronger statement over fields of characteristic zero.</summary>
    <updated>2020-05-21T23:45:11Z</updated>
    <published>2020-05-21T23:45:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-27T20:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/080</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/080" rel="alternate" type="text/html"/>
    <title>TR20-080 |  Continuous LWE | 

	Joan Bruna, 

	Oded Regev, 

	Min Jae Song, 

	Yi Tang</title>
    <summary>We introduce a continuous analogue of the Learning with Errors (LWE) problem, which we name CLWE. We give a polynomial-time quantum reduction from worst-case lattice problems to CLWE, showing that CLWE enjoys similar hardness guarantees to those of LWE. Alternatively, our result can also be seen as opening new avenues of (quantum) attacks on lattice problems. Our work resolves an open problem regarding the computational complexity of learning mixtures of Gaussians without separability assumptions (Diakonikolas 2016, Moitra 2018). As an additional motivation, (a slight variant of) CLWE was considered in the context of robust machine learning (Diakonikolas et al.~FOCS 2017), where hardness in the statistical query (SQ) model was shown; our work addresses the open question regarding its computational hardness (Bubeck et al.~ICML 2019).</summary>
    <updated>2020-05-21T14:39:43Z</updated>
    <published>2020-05-21T14:39:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-27T20:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/079</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/079" rel="alternate" type="text/html"/>
    <title>TR20-079 |  On Minimizing Regular Expressions Without Kleene Star | 

	Hermann Gruber , 

	Markus Holzer, 

	Simon Wolfsteiner</title>
    <summary>Finite languages lie at the heart of literally every regular expression. Therefore, we investigate the approximation complexity of minimizing regular expressions without Kleene star, or, equivalently, regular expressions describing finite languages. On the side of approximation hardness, given such an expression of size~$s$, we prove that it is impossible to approximate the minimum size required by an equivalent regular expression within a factor of  $O\left(\frac{s}{(\log s)^{2+\delta}}\right)$ if the running time is bounded by a quasipolynomial function depending on~$\delta$, for every $\delta&gt;0$, unless the exponential time hypothesis (ETH) fails. For approximation ratio~$O(s^{1-\delta})$, we prove an exponential time lower bound depending on~$\delta$, assuming ETH. The lower bounds apply for alphabets of constant size. On the algorithmic side, we show that the problem can be approximated in polynomial time within~$O(\frac{s\log\log s}{\log s})$, with~$s$ being the size of the given regular expression. For constant alphabet size, the bound improves to~$O(\frac{s}{\log s})$. Finally, we devise a familiy of superpolynomial approximation algorithms that attain the performance ratios of the lower bounds, while their running times are only slightly above those excluded by the ETH.</summary>
    <updated>2020-05-21T14:33:24Z</updated>
    <published>2020-05-21T14:33:24Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-27T20:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/078</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/078" rel="alternate" type="text/html"/>
    <title>TR20-078 |  The New Complexity Landscape around Circuit Minimization | 

	Eric Allender</title>
    <summary>We survey recent developments related to the Minimum Circuit Size Problem</summary>
    <updated>2020-05-21T14:15:55Z</updated>
    <published>2020-05-21T14:15:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-27T20:20:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4805</id>
    <link href="https://www.scottaaronson.com/blog/?p=4805" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4805#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4805" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Quantum Computing Lecture Notes 2.0</title>
    <summary xml:lang="en-US">Two years ago, I posted detailed lecture notes on this blog for my Intro to Quantum Information Science undergrad course at UT Austin. Today, with enormous thanks to UT PhD student Corey Ostrove, we’ve gotten the notes into a much better shape (for starters, they’re now in LaTeX). You can see the results here (7MB)—it’s […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Two years ago, I posted detailed <a href="https://www.scottaaronson.com/blog/?p=3943">lecture notes</a> on this blog for my Intro to Quantum Information Science undergrad course at UT Austin.  Today, with enormous thanks to UT PhD student Corey Ostrove, we’ve gotten the notes into a much better shape (for starters, they’re now in LaTeX).  You can <a href="https://www.scottaaronson.com/qclec.pdf">see the results here</a> (7MB)—it’s basically a 260-page introductory quantum computing textbook in beta form, covering similar material as many other introductory quantum computing textbooks, but in my style for those who like that.  It’s missing exercises, as well as material on quantum supremacy experiments, recent progress in hardware, etc., but that will be added in the next version if there’s enough interest.  Enjoy!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Unrelated Announcement:</span></strong> Bjorn Poonen at MIT pointed me to <a href="https://researchseminars.org/">researchseminars.org</a>, a great resource for finding out about technical talks that are being held online in the era of covid.  The developers recently added CS as a category, but so far there are very few CS talks listed.  Please help fix that!</p></div>
    </content>
    <updated>2020-05-20T21:14:56Z</updated>
    <published>2020-05-20T21:14:56Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-05-27T13:17:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1701</id>
    <link href="https://theorydish.blog/2020/05/19/incentive-compatible-sensitive-surveys/" rel="alternate" type="text/html"/>
    <title>Incentive Compatible Sensitive Surveys</title>
    <summary>Crucial decisions are increasingly being made by automated machine learning algorithms. These algorithms rely on data, and without high quality data, the resulting decisions may be inaccurate and/or unfair. In some cases, data is readily available: for example, location data passively collected by smartphones. In other cases, data may be difficult to obtain by automated means, and it is necessary to directly survey the population. However, individuals are not always motivated to take surveys if they receive no benefit. Offering a monetary reward may incentivize some individuals to participate, but there is a problem with this approach: what if an individual’s data is correlated with their willingness to take the survey? For concreteness, imagine that you are a health administrator trying to estimate the average weight in a population. This is a sensitive attribute that individuals may be reluctant to disclose, especially if their weight is not considered healthy. A generic survey may yield disproportionately more respondents with “healthy” weights, and thus may result an an inaccurate estimate (see, e.g, Shields et al., 2011). In this post, we discuss three papers which propose solutions to this problem through the lens of mechanism design. The idea is to carefully design payments [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1703" height="303" src="https://theorydish.files.wordpress.com/2020/05/survey.jpg?w=764" width="580"/></figure></div>

<p>Crucial decisions are increasingly being made by automated machine learning algorithms. These algorithms rely on data, and without high quality data, the resulting decisions may be inaccurate and/or unfair. In some cases, data is readily available: for example, location data passively collected by smartphones. In other cases, data may be difficult to obtain by automated means, and it is necessary to directly survey the population.</p>

<p>However, individuals are not always motivated to take surveys if they receive no benefit. Offering a monetary reward may incentivize some individuals to participate, but there is a problem with this approach: what if an individual’s data is correlated with their willingness to take the survey?</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1708" height="184" src="https://theorydish.files.wordpress.com/2020/05/correlation.png?w=459" width="457"/></figure></div>

<p>For concreteness, imagine that you are a health administrator trying to estimate the average weight in a population. This is a sensitive attribute that individuals may be reluctant to disclose, especially if their weight is not considered healthy. A generic survey may yield disproportionately more respondents with “healthy” weights, and thus may result an an inaccurate estimate (see, e.g, Shields et al., 2011).</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1707" height="234" src="https://theorydish.files.wordpress.com/2020/05/scale.jpeg?w=1024" width="408"/></figure></div>

<p>In this post, we discuss three papers which propose solutions to this problem through the lens of <em>mechanism design</em>. The idea is to carefully design payments so that we received an unbiased sample, leading to a hopefully accurate estimate.</p>

<h2><strong>Model</strong></h2>

<p>We use <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/> to denote agent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/>‘s data (e.g., her weight). We assume that each agent also has a personal cost <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/>, representing her level of reluctance to reveal her data. Agent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/> is willing to reveal <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/> if and only if she receives a payment of at least <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/>. Our goal is to allocate higher payments to agents with higher <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/>‘s, in order to get an unbiased sample. However, we also must obey an budget constraint: we cannot spend more than <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="B"/> total. The solution is to transact non-deterministically: with some probability, offer to purchase an agent’s data. Agents with higher costs will receive higher payments, but lower transaction probabilities.</p>

<p>We assume that agents are drawn at random independently from some distribution. Our crucial assumption is that we known the marginal distribution of agent costs, which we denote <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal{F}"/> (we will explore later what happens when this assumption is removed). However, we do not know the distribution of <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>, and that distribution can be arbitrarily correlated with <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal{F}"/>. As mentioned above, one might expect agents with less “desirable” <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>‘s may have higher costs, but one can imagine more complex correlations as well.</p>

<p>Our mechanisms consist of two parts: an <em>allocation rule</em> <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A"/>, and a <em>payment rule</em> <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P"/>. Given <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A"/> and <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P"/>, the mechanism works as follows:</p>

<ol><li>Ask each agent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/> to report <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/>. Let <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c"/> denote the actual reported cost.</li><li>With probability <img alt="A(c)" class="latex" src="https://s0.wp.com/latex.php?latex=A%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A(c)"/>, we purchase the agent’s data and pay her <img alt="P(c)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P(c)"/>. With probability <img alt="1 - A(c)" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+A%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="1 - A(c)"/>, we do not buy the data, and no payment is made.</li><li>At the end, use the data we learned to form an estimate of the population average of <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>. Let <img alt="\bar{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbar%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\bar{z}"/> denote our estimate.</li></ol>

<p>In this model, agent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/>‘s expected utility for reporting <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c"/> is <img alt="u_i(c) = A(c) (P(c) - c_i)" class="latex" src="https://s0.wp.com/latex.php?latex=u_i%28c%29+%3D+A%28c%29+%28P%28c%29+-+c_i%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="u_i(c) = A(c) (P(c) - c_i)"/>.</p>

<p>We have four main requirements:</p>

<ol><li><strong>Truthfulness. </strong>It should be in each agent’s best interest to truthfully report <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/>.</li><li><strong>Individual rationality. </strong>Agents should not receive negative utility if they are honest, i.e., we should have <img alt="P(c_i) \ge c_i" class="latex" src="https://s0.wp.com/latex.php?latex=P%28c_i%29+%5Cge+c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P(c_i) \ge c_i"/> for all <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/>.</li><li><strong>Budget constrained. </strong>Our total expected payment should not exceed <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="B"/>, i.e., <img alt="\mathbb{E}[\sum_i A(c_i) P(c_i)] \le B" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Csum_i+A%28c_i%29+P%28c_i%29%5D+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathbb{E}[\sum_i A(c_i) P(c_i)] \le B"/>.</li><li><strong>Unbiased. </strong>Our estimate isn’t consistently too high or too low. Specifically, the expected value of our estimate should be equal to the true average, i.e., <img alt="\mathbb{E}[\bar{z}] = \mathbb{E}[z_i]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Cbar%7Bz%7D%5D+%3D+%5Cmathbb%7BE%7D%5Bz_i%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathbb{E}[\bar{z}] = \mathbb{E}[z_i]"/>.</li></ol>

<p>Lack of bias doesn’t mean that our estimate is accurate, however. To this end, our primary goal is to <strong>minimize the variance</strong>, subject to the mechanism obeying the four above criteria. We evaluate variance via a worst-case framework: given a mechanism, we wish minimize the variance with respect to the worst-case distribution of agents for that mechanism. The idea is that the distribution of <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>‘s is not known to the mechanism, so we require it to perform well for all distributions.</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1729" height="203" src="https://theorydish.files.wordpress.com/2020/05/goal.jpg?w=970" width="438"/>Goal.</figure></div>

<p>When we refer to the “optimal” mechanism, we mean minimum variance, subject to being truthful, individually rational, budget constrained, and unbiased (henceforth TIBU).</p>

<p><strong>The Horvitz Thomspon Estimator</strong></p>

<p>Once we have learned the <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>‘s, how do we actual form an estimate of the mean? Luckily for us, this question has a simple answer. If we restrict ourselves to linear unbiased estimators, there is a unique way to do this, known as the <em>Horvitz-Thompson estimator</em>:</p>

<p><img alt="\bar{z} = \displaystyle\sum\limits_i \cfrac{z_i}{A(c_i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbar%7Bz%7D+%3D+%5Cdisplaystyle%5Csum%5Climits_i+%5Ccfrac%7Bz_i%7D%7BA%28c_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\bar{z} = \displaystyle\sum\limits_i \cfrac{z_i}{A(c_i)}"/></p>

<p>Thus our task is simply to choose <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A"/> and <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P"/>.</p>

<h2>Approach #1</h2>

<p>This model was first considered by Roth and Schoenebeck (2012). They are able to characterize a mechanism which is TIBU and has variance at most <img alt="1/n" class="latex" src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="1/n"/> more than the optimal variance, where <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n"/> is the number of agents. However, they do make the strong assumption that <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/> is either 0 or 1.</p>

<p>Their approach relies on <em>Take-It-Or-Leave-It</em> mechanisms. Such a mechanism is defined by a distribution $G$ over the positive real numbers, and works as follows:</p>

<ol><li>Each agent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/> reports a cost <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c"/>.</li><li>Sample a payment <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="p"/> from <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="G"/>.</li><li>If <img alt="p \ge c" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cge+c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="p \ge c"/>, buy the agent’s data with payment <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="p"/>. If <img alt="p &lt; c" class="latex" src="https://s0.wp.com/latex.php?latex=p+%3C+c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="p &lt; c"/>, do not buy the agent’s data.</li></ol>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" height="212" src="https://pbs.twimg.com/media/EDzLmHAWwAAEPkl.jpg" width="293"/></figure></div>

<p>This amounts to an allocation rule <img alt="A(c) = 1 - \text{Pr}[p\ge c]" class="latex" src="https://s0.wp.com/latex.php?latex=A%28c%29+%3D+1+-+%5Ctext%7BPr%7D%5Bp%5Cge+c%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A(c) = 1 - \text{Pr}[p\ge c]"/>, and a payment rule <img alt="P(c)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P(c)"/> equal to the distribution <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="G"/> conditioned on being at least <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c"/>. The authors show that these mechanisms are fully general, i.e., any allocation and payment rule can be implemented by a Take-It-Or-Leave-It mechanism.</p>

<p>The proof of their main result is primarily based on using the calculus of variations to optimize over the space of distributions <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="G"/>. The paper contains some additional results, for example regarding an alternate model where we wish to minimize the budget, but that is outside the scope of this blog post.</p>

<h2>Approach #2</h2>

<p>Although the above result is a great step, it leaves room for improvement. First of all, the assumption that <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/> is binary is quite strong, and does not apply to our running example of body weight. Secondly, their mechanism does not quite achieve the optimal variance. Chen et al. (2018) remedy both of these concerns. That is, they allow <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/> to be any real number, and they characterize the TIBU mechanism with optimal variance. Their result also generalizes to more complex statistical estimates, not just the average <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>, and it holds for both continuous and discrete agent distributions.</p>

<p>The approach of Chen et al. (2018) is based on two primary ideas. First, they show that any monotone allocation rule (i.e., we are always less likely to purchase data from an agent with higher cost) can be implemented in a TIBU fashion by a unique payment rule. Thus we only need to identify the optimal allocation rule. (This is similar to the standard result from auction theory about implementable monotone allocation rules (Myerson 1981).)</p>

<p>The second idea is to view the problem as a zero-sum game between ourselves (the mechanism designer) and an adversary who chooses the distribution of agents. Given a distribution, we choose an allocation rule to minimize the variance, and given an allocation rule, the adversary chooses a distribution to maximize the variance.</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" height="297" src="https://i2.wp.com/marketbusinessnews.com/wp-content/uploads/2016/08/Zero-Sum-Game.jpg?fit=511%2C521&amp;ssl=1&amp;resize=1200%2C1223.4833659491" width="292"/></figure></div>

<p>The authors are able to solve for the equilibrium of this game and thus identify the TIBU mechanism with minimum possible variance.</p>

<h2>Approach #3</h2>

<p>Approach #2 gave us our desired result: a minimum variance mechanism subject to our four desired properties (TIBU), for any distribution of <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>‘s. But we are still making a very strong assumption: that we know the distribution of agent costs.</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1766" height="230" src="https://theorydish.files.wordpress.com/2020/05/prior.png?w=500" width="427"/></figure></div>

<p>Chen and Zheng (2019) do away with this assumption in a follow-up paper. They consider a model where the mechanism has no prior information on the distribution of costs (or on the distribution of data), and $n$ agents arrive one-by-one in a uniformly random order. Each agent reports a cost, and we decide whether to buy her data, and what to pay her. In order to price well, we need to learn the cost distribution, but we must do this while simultaneously making irrevocable purchasing decisions. The main result is a TIBU mechanism with variance at most a constant factor worse than optimal.</p>

<p>The authors note that after each step <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/>, the reported costs up to that point induce an empirical cost distribution <img alt="\mathcal{F}_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal{F}_i"/>. Using the results of Chen et al. (2018), we can determine the optimal mechanism for <img alt="\mathcal{F}_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal{F}_i"/>. The basic idea is to use that mechanism for the current step, learn a new agent cost <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/> (note that the agent reports <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/> regardless of whether we purchase her data) and then update our empirical distribution accordingly. (The authors actually end up using an approximately optimal allocation rule, but the idea is the same.) The mechanism also uses more budget in the earlier rounds, to make up for the pricing being less accurate.</p>

<h2>Discussion</h2>

<p>In this post, we considered the problem of surveying a sensitive attribute where an agent’s data may be correlated with their willingness to participate. We discussed three different approaches, all of which rely of giving higher payments to agents with higher costs, in order to incentivize them to participate and to obtain an unbiased estimate. The final approach was able to give a truthful, individually rational, budget feasible, and unbiased mechanism with approximately optimal variance, without making any prior assumptions on the distribution of agents.</p>

<p>However, all three of the approaches assume that agents cannot lie about the data. This is reasonable for some attributes, such as a body weight, where an agent can be asked to step onto a physical scale. However, requiring participants come in person to a particular location will certainly lead to less engagement. Furthermore, for other sensitive attributes, there may not be a verifiable way to obtain the data. Future work could investigate alternative models where this assumption is not necessary. For example, perhaps agents do not maliciously lie, but rather are simply inaccurate at reporting their own attributes. For example, research has demonstrated that people consistently over-report height and under-report weight (e.g., Gorber et al., 2007). Could a mechanism learn the pattern of inaccuracy and compensate for that to still obtain an unbiased estimate?</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1775" height="243" src="https://theorydish.files.wordpress.com/2020/05/bias.png?w=1024" width="317"/></figure></div>

<h2><strong>References</strong></h2>

<ol><li>Yiling Chen, Nicole Immorlica, Brendan Lucier, Vasilis Syrgkanis, and Juba Ziani. “Optimal data acquisition for statistical estimation.” In <em>Proceedings of the 2018 ACM Conference on Economics and Computation</em>. 2018.</li><li>Yiling Chen and Shuran Zheng. “Prior-free data acquisition for accurate statistical estimation.” <em>Proceedings of the 2019 ACM Conference on Economics and Computation</em>. 2019.</li><li>Sarah Connor Gorber, Mark S. Tremplay, David Moher, and B. Gorber (2007). A comparison of direct vs. self‐report measures for assessing height, weight and body mass index: a systematic review. <em>Obesity reviews</em>, <em>8</em>(4), 307-326.</li><li>Roger Myerson. Optimal auction design. Mathematics of Operations Research, 6(1):58–73. 1981.</li><li>Aaron Roth and Grant Schoenebeck. “Conducting truthful surveys, cheaply.” <em>Proceedings of the 2012 ACM Conference on Electronic Commerce</em>. 2012.</li><li>Margot Shields, Sarah Connor Gorber, Ian Janssen, and Mark S. Tremblay. (2011). Bias in self-reported estimates of obesity in Canadian health surveys: an update on correction equations for adults. <em>Health Reports</em>, <em>22</em>(3), 35.</li></ol>

<p> </p></div>
    </content>
    <updated>2020-05-19T20:40:34Z</updated>
    <published>2020-05-19T20:40:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>bplaut</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-05-27T20:21:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1781</id>
    <link href="https://theorydish.blog/2020/05/19/nominations-for-tcs-women-rising-star-talks-at-stoc/" rel="alternate" type="text/html"/>
    <title>Nominations for TCS Women Rising Star talks at STOC</title>
    <summary>Directly from the organizers: ———– Dear colleagues, We invite you to nominate speakers for TCS Women Rising Star talks at STOC 2019, which are planned as part of our virtual TCS Women Spotlight Workshop. To be eligible, your nominee has to be a female or a minority researcher working in theoretical computer science (all topics represented at STOC are welcome) and has to be a graduating PhD student or a postdoc. You can make your nomination by filling this form by May 28th: https://forms.gle/R9nmit62ESA6V9vv6 STOC 2020 workshops will happen between June 23 and 25, with exact day/time TBD. You can see the list of speakers from last year here: https://sigact.org/tcswomen/2nd-tcs-women-meeting/tcs-women-2019/ Looking forward to your nominations and to seeing you at the our TCS Women Spotlight Workshop, Barna Saha, Virginia Vassilevska Williams, and Sofya Raskhodnikova</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>Directly from the organizers:</div>
<div/>
<div>———–</div>
<div/>
<div/>
<div>Dear colleagues,</div>
<div/>
<div>We invite you to nominate speakers for TCS Women Rising Star talks at STOC 2019, which are planned as part of our virtual TCS Women Spotlight Workshop. To be eligible, your nominee has to be a female or a minority researcher working in theoretical computer science (all topics represented at STOC are welcome) and has to be a graduating PhD student or a postdoc. You can make your nomination by filling this form by May 28th:</div>
<div/>
<div><a href="https://forms.gle/R9nmit62ESA6V9vv6" rel="noopener" target="_blank">https://forms.gle/R9nmit62ESA6V9vv6</a></div>
<div/>
<div>STOC 2020 workshops will happen between June 23 and 25, with exact day/time TBD.</div>
<div/>
<div>You can see the list of speakers from last year here:</div>
<div><a href="https://sigact.org/tcswomen/2nd-tcs-women-meeting/tcs-women-2019/" rel="noopener" target="_blank">https://sigact.org/tcswomen/2nd-tcs-women-meeting/tcs-women-2019/</a></div>
<div/>
<div>Looking forward to your nominations and to seeing you at the our TCS Women Spotlight Workshop,</div>
<div>Barna Saha, Virginia Vassilevska Williams, and Sofya Raskhodnikova</div></div>
    </content>
    <updated>2020-05-19T20:39:28Z</updated>
    <published>2020-05-19T20:39:28Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-05-27T20:21:26Z</updated>
    </source>
  </entry>
</feed>
