<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-05-06T15:41:24Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6832816500979478899</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6832816500979478899/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/negotiations.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6832816500979478899" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6832816500979478899" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/negotiations.html" rel="alternate" type="text/html"/>
    <title>Negotiations</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>So you got an offer to be an assistant professor in the computer science department at Prestigious U. Congratulations! </p><p>Time to negotiate your offer with the chair. Don't be nervous. This shouldn't be adversarial. Both of you have the same goal in mind--for you to come to Prestigious and be successful. </p><p>Let's discuss the different aspects of each package.</p><p><b>Research </b></p><p>Funds for supporting your research such as equipment, graduate student support, travel and postdocs. Here you should explain what you need to be successful. This will vary by subdiscipline, a systems researcher will need more equipment and students than a theorist. Keep in mind the university is giving you funds for 2-4 years to start your research, after which you are expected to fund your own research via grants.</p><p>I don't recommend taking on a postdoc right at the start of your first academic appointment. Postdocs require good mentoring while you need to spend the first year getting your research up and running. If you do ask for postdoc money, ask to have a flexible start time.</p><p>Many departments give course reductions to get your research going. I'd suggest asking to spend your first semester teaching a graduate topics course based on your thesis research to pick up some PhD students followed by a semester with no classes to get you research program going.</p><p><b>Salary</b></p><p>This includes actual salary, which is also the base for future raises, and summer salary in the first couple of years. Feel free to ask for more salary, but often these numbers are fixed for new assistant professors. There is more give if you take an academic job later in your career. You could also say something like, "Well if you can't give me more salary maybe you could give me another semester of grad student support?"</p><p><b>Partner</b></p><p>It seems 80% of the time, a job candidate has a partner that needs accommodating. Don't wait until the end of negotiations, bring it up early. The more time we have, the better we can help. Doesn't matter what job they want--we know people and we know people who know people.</p><p><b>Thesis</b></p><p>Many schools won't hire you as an assistant professor if you haven't finished your thesis. Has to do with college rankings work. Don't worry--they will generally give you some other role with the same package until you finish. This might delay your tenure clock though.</p><p><b>Delayed start time</b></p><p>A January start is usually fine with good reason but if you weren't planning to start until the fall of 2022 why are you on the market this year? If you do get the department to hold a position for you, remember you are also making a commitment--this is not an opportunity to try again for something better.</p><p><b>Overall</b></p><p>You may not get all that you want after a negotiation--don't take it personally. You shouldn't necessarily choose the place that gives you the biggest package. It's far more important in the long run that you pick a place where you can best succeed both professionally and personally, and the package is just a small piece of that puzzle.</p></div>
    </content>
    <updated>2021-05-06T13:07:00Z</updated>
    <published>2021-05-06T13:07:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-06T14:29:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=1628</id>
    <link href="https://toc4fairness.org/self-fulfilling-and-self-negating-predictions-a-short-tale-of-performativity-in-machine-learning/" rel="alternate" type="text/html"/>
    <title>Self-fulfilling and self-negating predictions: a short tale of performativity in machine learning</title>
    <summary>This post is based on results and discussions from a series of joint works with Moritz Hardt, Celestine Mendler-Dünner, John Miller, and Juan C. Perdomo. In 1998, Michel Callon wrote ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><blockquote class="wp-block-quote has-text-align-center"><p><em>This post is based on results and discussions from a series of joint works with Moritz Hardt, Celestine Mendler-Dünner, John Miller, and Juan C. Perdomo.</em></p></blockquote>



<p>In 1998, Michel Callon wrote what would be the first in an ongoing series of controversial publications in economic sociology [1]. He was the first to propose the idea that “the economy is not embedded in society but in economics”. With this, he challenged the conventional view that economic theories and models passively observe markets and infer their behavior, just like laws of physics passively describe the principles governing natural phenomena. Instead, Callon argued that economic theories are <em>performative:</em> they induce the economy, creating the phenomena they aim to describe.</p>



<p>One example that is often cited in support of Callon’s claims is the impact of the celebrated Black-Scholes-Merton options pricing model [2, 3]. MacKenzie and Millo [4] investigated the role of this model in the economy and found that it “made itself true”. In their words,</p>



<p class="has-text-align-center"><em>“Black, Scholes, and Merton’s model did not describe an already existing world: when first formulated, its assumptions were quite unrealistic, and empirical prices differed systematically from the model. Gradually, though, the financial markets changed in a way that fitted the model”.</em></p>



<p>Indeed, participants in the market started making decisions assuming the market obeys the mathematical laws implied by the Black-Scholes-Merton model. As MacKenzie and Millo put it, “pricing models came to shape the very way participants thought and talked about options”.</p>



<p>This phenomenon — whereby models and predictions inform decision-making and thus alter the target of prediction itself — is by no means special to economic forecasts.</p>



<p>Predictive policing, for example, develops algorithms that use historical data to estimate the likelihood of crime at a given location. Those locations where criminal behavior is deemed likely by the system typically get more police patrols and surveillance in general. In a kind of self-fulfilling prophecy [5], these actions resulting from prediction might further increase the <em>perceived</em> crime rate at the patrolled locations, thus biasing the data used for future decisions.</p>



<p>A similar feedback loop arises in traffic predictions, when drivers decide which route to take based on the estimated time of arrival (ETA) calculated by a traffic prediction system. If the predictive system estimates low ETA for a given route, many drivers take the route, potentially leading to an overflow of traffic and making the ETA prediction inaccurate as a result. Contrary to the previous example, traffic predictions arguably exhibit a self-negating prophecy: low ETA might imply a longer travel time, and vice versa.</p>



<p>While the previous examples deal with qualitatively different feedback mechanisms, the interplay of predictions and decision-making is similar. First, one uses historical data to build a predictive model. Then, the predictions of the model feed into and inform consequential decisions. Finally, these decisions trigger changes in the environment, making future observations differ from those in the initial dataset.</p>



<p class="has-text-align-center"><img height="81" src="https://lh3.googleusercontent.com/C3qD-SK6AD4gDpkJznllugJS8OZaJwGvXSNi96qYKPu2ALr5vSxPMhteCScLq-CCdKOvh8bZ8KB-gvTRVkbZALVEuML0WhSV0pX8YM1MHupTLPjZ8PKEydiNu_h3iUdlVAYUcFDT" width="624"/></p>



<p>We refer to prediction problems that exhibit this feedback-loop behavior as <em>performative prediction</em> problems.</p>



<p>In the language of machine learning, such a change in patterns would often be called <em>distribution shift</em>. Notably, however, performative distribution shift is not due to external factors independent of the model, such as, say, when traffic patterns change due to seasonal effects. Rather, the distribution shift is triggered directly by the choice of predictive model. (Of course, distribution shifts can also be caused by a combination of external factors and model choice.)</p>



<p>To formalize performative prediction mathematically, it is instructive to contrast performative prediction problems with supervised learning problems. In supervised learning, the decision-maker observes pairs of features and outcomes <img height="15" src="https://lh5.googleusercontent.com/2U92MhGBO6DJqe607HL2T4uWicPXKFgrU2PoSaeymNLxbOteL6r_dhkuuFo91W2AXoAzrhxt8Ndg1jfhf8KVqZMbID1koi01cpGcXz-CTACfH_b54DohHMqpO2hJ5bEtfdE6v6Ag" width="72"/> drawn from a <em>fixed</em> distribution <img height="13" src="https://lh6.googleusercontent.com/Rj1cE6eMb1304KzXnhY0e_dxrgXvPNd9q-IzgZLuKKxFYfn3q82FIiQUwb0_DO8G-xyuGdp8T5Y-EN5bKYPPxSd5QaMMO1irMwSeI6O5sy08Ubriy2cFliObESx_XOoFzPwZy6Wi" width="14"/>. The key difference in performative prediction is that there is no longer an unknown static distribution generating observations; rather, data is drawn from a <em>model-dependent distribution</em> <img height="17" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/>, where <img height="12" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/> is a parameter vector specifying the deployed model. For example, <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/> could be the weights of a neural network, or a vector of linear regression coefficients. For a given choice of parameters <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/>, <img height="17" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/> should be thought of as the distribution over features and outcomes that results from making decisions according to the model specified by <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/>. In the context of the traffic prediction example, <img height="17" src="https://lh4.googleusercontent.com/HDm0VWvPNAUIdPfMSLc0ptEDS7Tph1p00WbC6TlSbQ8oYjK2nJSuSMiLuKN8IAA89L2H8rDLRPf9GWwc37Zb_0_qLxN-UuVVsz_hkBB43hQcTOtV4HUg30BtsHel_hpCFHfYJa6E" width="32"/> could be a distribution over traffic conditions and travel times, given that drivers make routing decisions in response to ETA forecasts by model <img height="13" src="https://lh3.googleusercontent.com/TkPLvHJ9t0soCQkx43reFppYsx4b0sJrNUBWd_Yx_rarIU4nS92GpBFBn-D_jhvDNAU04Aee4NMsVzHEozOLxXN11DMl4h1O2TvHfNih9IqHi0R3wM1otXYEp1MbkZlbI0faSV1i" width="8"/>.</p>



<p>In supervised learning, the quality of a model <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/> is typically measured by its <em>risk</em>, namely, the expected loss of the model on instances from distribution <img height="13" src="https://lh6.googleusercontent.com/Rj1cE6eMb1304KzXnhY0e_dxrgXvPNd9q-IzgZLuKKxFYfn3q82FIiQUwb0_DO8G-xyuGdp8T5Y-EN5bKYPPxSd5QaMMO1irMwSeI6O5sy08Ubriy2cFliObESx_XOoFzPwZy6Wi" width="14"/> as measured via a loss function <img height="16" src="https://lh5.googleusercontent.com/iLwCF0E2qkddKEmiLhziUoOB171Na3z6aLjqWSYTPykfGKIx5PBtNT9qsZXIMPLc3hVXkkupbpqILbC4Fx9p_h5G8a1GmlYVL3rcx787SVO-24HKrL39LxAwPGaQ2i8wuPURBoyS" width="9"/>:</p>



<p class="has-text-align-center"><img height="26" src="https://lh5.googleusercontent.com/vP7Ll-u4VopJmW24L2lMvqwf8N9bOrKaoLFDDlmcZgmML4CM8bM53_lMEG--5Om2mTYOQ5uNqUBkzlp2Hgd9i3EO8Y8bzC8kvkFgQKSfqOgHoWiMpK2MVasGJmdFTdERwIUg0RzK" width="153"/></p>



<p>Since performative prediction does not admit one true data-generating distribution, but rather a family of distributions <img height="18" src="https://lh6.googleusercontent.com/un2s0qA36MFKtNGHoDb7sQdPfToUoZjYy45nbwmSy2jQhSDJbiouNjATQWsSwDlpUF-PfOk1ElFPDb8Rsr_TAAB0GUemVV5Y_bG1EISEUD7M9N5RGxug1gGdvpXa-D70ATQSafzT" width="58"/>, evaluating a model <img height="13" src="https://lh4.googleusercontent.com/zJsapVnRoEAmwRciJFyPDFM3pc3YOuB_6gM71GJX1Aa_9Tm09RLD4mj_DOEkH0CzKHFPV93LesWVf1AUH9QlwQoU0x07xusBHiT8-EeAApf2qMsufNc8Vuzc77LWu4bxEuYwgxXp" width="8"/> calls for a new risk concept. Arguably the most natural counterpart of the risk in supervised learning is the expected loss on the distribution that arises once the model is deployed and feeds into consequential decisions. This leads to the notion of <em>performative risk</em>, defined as:</p>



<p class="has-text-align-center"><img height="30" src="https://lh5.googleusercontent.com/zWCmnf2uG6j8LZG7w68p8y4I2wHcKc82ZULfh_MUmGc3YI6UByQP8dB3nBDugFmg8t0VybMz6RV4tzviUsIyHmVY1uUH1Xjv2XKTSEQGpFW3-o0e50-rxKeouEula7UNdqC6HYHr" width="181"/></p>



<p>Adopting the performative risk as the single overarching measure of quality, a model would be optimal if it minimizes the performative risk. While an appealing solution concept, performative optimality is difficult to achieve, seeing the double dependence of the risk function on <img height="13" src="https://lh3.googleusercontent.com/ywU1sy8RfVug9ZI2C0ETvBvXg7_gqi_Xx9a1-nILxAhSafCu1OvR5095fDFr3PJN9bmBa6qoPoLzz0bFrjPtTEw5-BX3vvKHqTUyvDFaDwt7mc0B41XPpPgZZ_T4gb72PAFMDwAK" width="8"/>. One of the main computational difficulties is the fact that, even if the loss <img height="16" src="https://lh5.googleusercontent.com/WZWkgjmj0CvaBREmhCbwBw1i5JfL1Tviq96JbXG1SePOG-BXJ8uv_fUUE2KKJCw2qvd5zpA_GUhHZ9azV6AUENm0qVsKnTPMg6WzLvT5QGGLNwcLyhRU0iOhYtRy6Fylx66uMGRS" width="9"/> is convex in <img height="13" src="https://lh3.googleusercontent.com/ywU1sy8RfVug9ZI2C0ETvBvXg7_gqi_Xx9a1-nILxAhSafCu1OvR5095fDFr3PJN9bmBa6qoPoLzz0bFrjPtTEw5-BX3vvKHqTUyvDFaDwt7mc0B41XPpPgZZ_T4gb72PAFMDwAK" width="8"/>, <img height="17" src="https://lh4.googleusercontent.com/-syiEeOCPlb5lZOfPAK063xD0s6EraAKMtHoPUR8cIfpnP0AdkrnINBFw6Ejb8nr1jF3G7QO7v5K7o28nSnnI74SPxaJH7apiBu7Bpd7DiWnZ6Hx79Yi4v3iwJQR8j0eUJh2Qzfk" width="43"/> need not be convex. Prior work on strategic classification [6] implies a set sufficient conditions for <img height="17" src="https://lh4.googleusercontent.com/-syiEeOCPlb5lZOfPAK063xD0s6EraAKMtHoPUR8cIfpnP0AdkrnINBFw6Ejb8nr1jF3G7QO7v5K7o28nSnnI74SPxaJH7apiBu7Bpd7DiWnZ6Hx79Yi4v3iwJQR8j0eUJh2Qzfk" width="43"/> to be convex in a binary classification context, and in recent work [7] we identified a complementary set of conditions when the family of distributions <img height="18" src="https://lh4.googleusercontent.com/Ti5C4ibszTKUqTM5evbcuaHojWHAN_Rq20XcnDlunJNG1M4lBKsjB7nJOKEN4lTSdWNmgKkcxqAwPPRahQcZ4uL8kCZHn_AuaB_Z9T35eztMk2CZzMcK50GMGnH69Jdjd0euwDtI" width="58"/> forms an appropriate location-scale family. That said, in many practical settings convexity might be an unrealistic and unnecessarily strong guarantee to aim for. As we know from present-day machine learning, even non-convex problems can sometimes be amenable to simple optimization algorithms. Understanding the optimization landscape of the performative risk beyond convex settings is a fruitful direction going forward.</p>



<p>A seemingly less ambitious target is to find a model that is <em>locally</em> optimal in some appropriate sense. For example, one could optimize for models that are optimal on the distribution that they induce:</p>



<p class="has-text-align-center"><img height="22" src="https://lh6.googleusercontent.com/ENuLNHEI7mNEDONR7rUA2FzwbkvqbINkwHJfO7jiiODrZ3_Ko9NZmJAt4Qv0cPNF4-_C68yI3CE4UMcFjzP8v5UHdu_LgBkdrCnUj4A-E6uccXlclWWZef2onVzsd5X2sHq5tTlh" width="233"/></p>



<p>We call a model that satisfies the fixed-point equation above <em>performatively stable</em>. Performative stability arises naturally when the decision-maker applies the heuristic of myopically updating the model based on the distribution resulting from the previous deployment:</p>



<p class="has-text-align-center"><img height="20" src="https://lh4.googleusercontent.com/RlZ0ZmF3E0VHyj_ktWb3QtAxsXTY_WjoGS29Jss3v_5cXnDVsWbeEvaTAA4l4tUYa5ccYvJPWcPXy5oLa3BnBT0Yqws89mNind-2UT8IA2ip3_pt3PbFhiNVcgIhgMS1omjfgzI1" width="248"/></p>



<p>If this retraining strategy converges, then it necessarily converges to a performatively stable solution. This is an appealing property, since it says that stability eliminates the need for retraining. Several existing works [8, 9, 10] have identified necessary and sufficient conditions for the above retraining heuristic, and some of its efficient approximations, to converge to a stable point. Roughly speaking, retraining converges to a stable solution if the loss is well-behaved and the performative feedback effects are not too strong. If either of those two conditions is violated, there is no guarantee of convergence.</p>



<p>In the language of game theory, one can think of performative prediction as a two-player game between a decision-maker, who decides which predictive model to deploy, and the model’s environment, which generates observations according to <img height="16" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/>. If <img height="16" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/> is thought of as the “best response” (according to some underlying utility) of the model’s environment to the deployment of model <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/>, then a performatively stable solution corresponds to a <em>Nash</em> equilibrium, while a performatively optimal solution corresponds to a <em>Stackelberg</em> equilibrium with the decision-maker acting as the leader.</p>



<p>Only in special cases, such as in well-behaved zero-sum games, it is known that Nash equilibria coincide with Stackelberg equilibria. Therefore, whenever performative prediction is a well-behaved zero-sum game, all stable solutions are also performatively optimal. However, <em>performative prediction is typically not a zero-sum game</em>. For example, if the decision-maker’s loss simply measures predictive accuracy, it seems odd that the environment’s primary objective is to hurt the model’s accuracy. Indeed, a typical performative prediction problem is a general-sum game without much structure. This implies that stable solutions and performative optima can be <em>very different</em>. And, since naive retraining strategies only converge to stability, this means that such myopic updates can be an inadequate method of overcoming performative distribution shifts and achieving low performative risk. This observation further motivates understanding the optimization landscape of the performative risk, as well as developing efficient algorithms for optimizing it. Recent work has explored several algorithmic solutions [11, 7], appropriate in convex settings.</p>



<p>Performative prediction relates to many other areas beyond game theory, including bandits, reinforcement learning, control theory. These frameworks are flexible enough to capture performative prediction as a special case, however performativity arises via distinctive feedback mechanisms and as such deserves its own specialized analysis. There is a long way to go in understanding the properties of performative distribution shifts, how they connect to feedback mechanisms in other disciplines, and how to tackle these shifts in practice. Furthermore, it is unclear whether a single distribution <img height="16" src="https://lh6.googleusercontent.com/a-1tKaIQT9pJwi6WQ4eV4ZO3xcFy1e_joEh0tz1QdG-cj7x-60nDwfRyyztRSzjEaL-0xbImihmV5LDB8njPUhSlsIeyJABtowIBOktz79sRW7hAY8B75CePuDxp47xEFOZ68gSA" width="32"/> is expressive enough to describe the observations after model deployment; in practice there are different kinds of memory effects [12] and self-reinforcing loops that make the data distribution evolve with time, even when the model is kept fixed. Finally, to make the existing theoretical insights actionable, going forward we need to think about what is the right solution concept — both statistically and ethically — to optimize for in performative settings.</p>



<p><br/>[1] M. Callon. Introduction: the embeddedness of economic markets in economics. <em>The Sociological Review</em>, 1998<br/>[2] F. Black, M. Scholes. The pricing of options and corporate liabilities. <em>The Journal of Political Economy</em>, 1973<br/>[3] R. C. Merton. Theory of rational option pricing. <em>The Bell Journal of Economics and Management Science</em>, 1973<br/>[4] D. MacKenzie, Y. Millo. Constructing a market, performing theory: The historical sociology of a financial derivatives exchange. <em>American Journal of Sociology</em>, 2003<br/>[5] D. Ensign, S. A. Friedler, S. Neville, C. Scheidegger, S. Venkatasubramanian. Runaway feedback loops in predictive policing. <em>ACM Conference on Fairness, Accountability and Transparency</em>, 2018<br/>[6] J. Dong, A. Roth, Z. Schutzman, B. Waggoner, Z. S. Wu. Strategic classification from revealed preferences. <em>ACM Conference on Economics and Computation</em>, 2018<br/>[7] J. Miller, J. C. Perdomo, T. Zrnic. Outside the echo chamber: Optimizing the performative risk. <em>arXiv preprint</em>, 2021<br/>[8] J. C. Perdomo, T. Zrnic, C. Mendler-Dünner, M. Hardt. Performative prediction. <em>International Conference on Machine Learning</em>, 2020<br/>[9] C. Mendler-Dünner, J. C. Perdomo, T. Zrnic, M. Hardt. Stochastic optimization for performative prediction. <em>Conference on Neural Information Processing Systems</em>, 2020<br/>[10] D. Drusvyatskiy, L. Xiao. Stochastic optimization with decision-dependent distributions. <em>arXiv preprint</em>, 2020<br/>[11] Z. Izzo, L. Ying, J. Zou. How to learn when data reacts to your model: Performative gradient descent. <em>arXiv preprint</em>, 2021<br/>[12] G. Brown, S. Hod, I. Kalemaj. Performative prediction in a stateful world. <em>arXiv preprint</em>, 2020</p></div>
    </content>
    <updated>2021-05-06T12:42:52Z</updated>
    <published>2021-05-06T12:42:52Z</published>
    <category term="Blog"/>
    <author>
      <name>tijanazrnic</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i1.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2021-05-06T15:41:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/066</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/066" rel="alternate" type="text/html"/>
    <title>TR21-066 |  Dimension-free Bounds and Structural Results in Communication Complexity | 

	Lianna Hambardzumyan, 

	Hamed Hatami, 

	Pooya Hatami</title>
    <summary>The purpose of this article is to initiate a systematic study of dimension-free relations between basic communication and query complexity measures  and various  matrix norms.  In other words, our goal is to obtain    inequalities that bound a parameter   solely as a function of another parameter. This is in contrast to perhaps the more common framework in communication complexity  where  poly-logarithmic dependencies on the number of input bits are   tolerated. 


Dimension-free bounds are also closely related to structural results, where one seeks to describe the structure of Boolean matrices and functions that have low complexity.  We prove such  theorems for several communication and query complexity measures as well as various matrix and operator norms. In several other cases we show that such bounds do not exist. 


We propose several conjectures, and establish that, in addition to applications in complexity theory, these   problems are central to  characterization of the idempotents of the algebra of Schur multipliers, and could lead to new extensions of  Cohen's celebrated idempotent theorem regarding the Fourier algebra.</summary>
    <updated>2021-05-05T18:59:23Z</updated>
    <published>2021-05-05T18:59:23Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-06T15:37:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/tpdp21-cfp/</id>
    <link href="https://differentialprivacy.org/tpdp21-cfp/" rel="alternate" type="text/html"/>
    <title>Call for Papers - Workshop on the Theory and Practice of Differential Privacy (TPDP 2021)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Work on differential privacy spans a number of different research communities, including theoretical computer science, machine learning, statistics, security, law, databases, cryptography, programming languages, social sciences, and more.
Each of these communities may choose to publish their work in their own community’s venues, which could result in small groups of differential privacy researchers becoming isolated.
To alleviate these issues, we have the Workshop on the <a href="https://tpdp.journalprivacyconfidentiality.org/">Theory and Practice of Differential Privacy</a> (TPDP), which is intended to bring these subcommunities together under one roof (well, a virtual one at least for 2020 and 2021).</p>

<p>We have just posted the <a href="https://tpdp.journalprivacyconfidentiality.org/2021/TPDP2021CfP.pdf">Call for Papers</a> for <a href="https://tpdp.journalprivacyconfidentiality.org/2021/">TPDP 2021</a>, which will be a workshop affiliated with <a href="https://icml.cc/Conferences/2021/">ICML 2021</a>.
The submission deadline is Friday, May 28, 2021, Anywhere on Earth (conveniently, two days after the deadline for NeurIPS 2021).
Submissions are extended abstracts of up to four pages in length, and will undergo a lightweight review process, based mostly on relevance and interest to the differential privacy community.
The workshop is non-archival, so feel free to submit recent work at any stage of publication.
Submissions will be on <a href="https://openreview.net/group?id=ICML.cc/2021/Workshop/TPDP">OpenReview</a>, but since submitted work may be preliminary, the process will be “closed” similar to traditional review processes.
One goal of the workshop is to be inclusive and welcoming to newcomers to the differential privacy community, so please consider participating even if you are new to the field.</p>

<p>Most papers will be presented as posters at a (virtual) poster session, while a few papers will be selected for spotlight talks.
There will also be plenary talks by <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a> (Hebrew University of Jerusalem) and <a href="https://www2.math.upenn.edu/~ryrogers/">Ryan Rogers</a> (LinkedIn).
The program co-chairs are <a href="https://sites.gatech.edu/rachel-cummings/">Rachel Cummings</a> and <a href="http://www.gautamkamath.com/">myself</a>.
Please submit your best work on differential privacy, and hope to see you there!</p>
<p align="center">
  <img src="https://differentialprivacy.org/images/Ligett.png"/>
  <img src="https://differentialprivacy.org/images/Rogers.png"/> <br/>
    <i>Invited speakers Katrina Ligett and Ryan Rogers</i>
</p></div>
    </summary>
    <updated>2021-05-05T14:00:00Z</updated>
    <published>2021-05-05T14:00:00Z</published>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-05-05T22:58:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18701</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/05/05/how-to-teach-math/" rel="alternate" type="text/html"/>
    <title>How to Teach Math?</title>
    <summary>The only way to learn mathematics is to do mathematics—Paul Halmos Angie Hodge is an Associate Professor of mathematics at Northern Arizona University. Her interests as stated on her website are focused mainly on education, mentoring, and equity in the STEM disciplines. Today I thought we would discuss the issue of teaching and learning math […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>The only way to learn mathematics is to do mathematics—Paul Halmos</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/05/how-to-teach-math/an/" rel="attachment wp-att-18704"><img alt="" class="alignright size-thumbnail wp-image-18704" height="150" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/an-150x150.jpg?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
Angie Hodge is an Associate Professor of mathematics at Northern Arizona University. Her interests as stated on her <a href="https://directory.nau.edu/person/amh952">website</a> are focused mainly on education, mentoring, and equity in the STEM disciplines. </p>
<p>
Today I thought we would discuss the issue of teaching and learning math and complexity theory.</p>
<p>
I am now retired from Georgia Tech. But I continue to be interested in how we can teach math. Dually how we can learn math. I still try to learn math—not for formal classes, nor in formal classes—but to help advance my own research. Even now I find that I need to learn some topics to help write a post or to solve a problem. </p>
<p>
On this blog with Ken I am also interested in still helping others learn. Some call that teaching. This is therefore the topic for today.</p>
<p>
</p><p/><h2> First An Issue </h2><p/>
<p/><p>
One thing that drives me nuts about learning new math is what I will call the </p>
<blockquote><p><b> </b> <em> <i>“It is too obvious to state” principle.</i> </em>
</p></blockquote>
<p>What I mean is that when you start to learn material from some corner of math you get the key definitions and the main results. What I do not always get is some totally obvious ideas. Does this make any sense?</p>
<p>
Here is an example. Consider the class of sequences that are defined by linear <a href="https://en.wikipedia.org/wiki/Recurrence_relation">recurrences</a>. That is: a recursion that defines a sequence as a linear combination of earlier terms. One of the most famous is the Fibonacci numbers: 	</p>
<p align="center"><img alt="\displaystyle  F_n = F_{n-1} + F_{n-2}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F_n+%3D+F_%7Bn-1%7D+%2B+F_%7Bn-2%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>And <img alt="{F_0=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF_0%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{F_1=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF_1%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
The property we are interested in is: </p>
<blockquote><p><b> </b> <em> <i>Is a product of two such sequences also of the same form?</i> </em>
</p></blockquote>
<p>This is a basic question, but it is not trivial to find the <a href="https://reader.elsevier.com/reader/sd/pii/0012365X79901869?token=C0E36B1EE6735F9AB6140E86924EB1A19F236C9507028E02CB912D21952F2438487C78DDBB68CC1541D2E9CF5754B8C1&amp;originRegion=us-east-1&amp;originCreation=20210415000540">answer</a>. </p>
<p>
</p><p/><h2> How To Teach? </h2><p/>
<p/><p>
This is an ancient question that we all probably have thought about from time to time. It is complicated by the recent issue of most classes being done via online. Hodge has some nice stuff on teaching, especially on Inquiry-Based Learning (IBL). </p>
<ul>
<li>
See her <a href="https://www.artofmathematics.org/blogs/cvonrenesse/guest-blog-by-angie-hodge">blog</a>. <p/>
</li><li>
See <a href="http://math.uchicago.edu/~boller/IBL/">this</a> for her comments on <em>Inquiry-Based Learning</em>. <p/>
</li><li>
See her thoughts on <a href="https://www.maa.org/sites/default/files/Programs/MathFest2018_IPS_Hodge2.pdf">IBL</a>.
</li></ul>
<p>
</p><p/><h2> Ken’s Similar Question </h2><p/>
<p/><p>
Ken writes: I am interested in manipulating logistic curves. Such curves not only undergird the chess rating system and the theory of standardized tests, they relate directly to the design of chess programs which I use to take my data. This decade’s neural chess-playing programs, following on from AlphaZero, express values directly in terms of the likelihood of winning, as numbers between 0 and 1, rather than traditional evaluation schemes built on counting 1.00 for a pawn, 3–3.5 for a knight or bishop, and so on. </p>
<p>
The new likelihood numbers follow a logistic curve. I especially want to convert from the evaluation numbers to them. After doing this conversion for various major chess programs, I would like to average their values as input to my predictive model. This involves taking averages of logistic curves. The curves can be generalized to the <a href="https://en.wikipedia.org/wiki/Generalised_logistic_function">form</a> named for Francis Richards: </p>
<p align="center"><img alt="\displaystyle  f(x) = A + \frac{K-A}{(C + Qe^{-Bx})^{1/\nu}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+A+%2B+%5Cfrac%7BK-A%7D%7B%28C+%2B+Qe%5E%7B-Bx%7D%29%5E%7B1%2F%5Cnu%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>for constant parameters <img alt="{A,K,B,C,Q,\nu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CK%2CB%2CC%2CQ%2C%5Cnu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The standard family has <img alt="{A=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{K=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{C=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{\nu=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnu%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{Q=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, with <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> the central parameter determining the slope of the curve <img alt="{f(t) = \frac{1}{1+e^{-Bx}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28t%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-Bx%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> at <img alt="{x=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We can ask the basic question about intermediate families between the standard family and the most general kind:</p>
<blockquote><p><b> </b> <em> When does a linear combination of logistic curves belong to the same family? And if it doesn’t belong, how close to a member of the family does it come? </em>
</p></blockquote>
<p/><p>
My point is that I have not been able to find an easy source for answers. This strikes me as exactly the kind of question for which sites like MathOverflow and StackExchange exist. But it is also a nice instructional exercise for training students in both the grit and adventure of mathematical research. </p>
<p>
One can also pose literally the same as Dick’s question above: how about a product of two curves <img alt="{f_1(x),f_2(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%28x%29%2Cf_2%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from the family? The product still has values that run from <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> goes from <img alt="{-\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to <img alt="{+\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. If we want the curves all to have value <img alt="{f(0) = 0.5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%280%29+%3D+0.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then we can compose the product with a square root, viz. <img alt="{f(x) = \sqrt{f_1(x) f_2(x)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%3D+%5Csqrt%7Bf_1%28x%29+f_2%28x%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, thus taking a geometric rather than arithmetic mean. I mentioned other nuts-and-bolts problems about logistic curves in this <a href="https://rjlipton.wpcomstaging.com/2018/08/25/do-you-want-to-know-a-secret/">post</a> and its longer <a href="https://rjlipton.wpcomstaging.com/2018/09/07/sliding-scale-problems/">followup</a>.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
The following video shows how not to teach math: <a href="https://www.youtube.com/watch?v=vU5LoCLGMdQ&amp;t=109s">The Kettles</a> do math.</p>
<p/><p><br/>
[some format and word tweaks]</p></font></font></div>
    </content>
    <updated>2021-05-05T13:56:56Z</updated>
    <published>2021-05-05T13:56:56Z</published>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Angie Hodge"/>
    <category term="Fibonacci sequence"/>
    <category term="IBL"/>
    <category term="learn"/>
    <category term="logistic curves"/>
    <category term="teach"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-05-06T15:38:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/065</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/065" rel="alternate" type="text/html"/>
    <title>TR21-065 |  One-way communication complexity and non-adaptive decision trees | 

	Nikhil Mande, 

	Swagato Sanyal</title>
    <summary>We study the relationship between various one-way communication complexity measures of a composed function with the analogous decision tree complexity of the outer function. We consider two gadgets: the AND function on 2 inputs, and the Inner Product on a constant number of inputs. Let $IP$ denote Inner Product on $2b$ bits.

1) If $f$ is a total Boolean function that depends on all of its inputs, the bounded-error one-way quantum communication complexity of $f \circ IP$ equals $\Omega(n(b-1))$.

2) If $f$ is a partial Boolean function, the deterministic one-way communication complexity of $f \circ IP$ is at least $\Omega(b \cdot D_{dt}^{\rightarrow}(f))$, where $D_{dt}^{\rightarrow}(f)$ denotes the non-adaptive decision tree complexity of $f$.

For our quantum lower bound, we show a lower bound on the VC-dimension of $f \circ IP$, and then appeal to a result of Klauck [STOC'00].
Our deterministic lower bound relies on a combinatorial result due to Frankl and Tokushige [Comb.'99].

It is known due to a result of Montanaro and Osborne [arXiv'09] that the deterministic one-way communication complexity of $f \circ XOR_2$ equals the non-adaptive parity decision tree complexity of $f$.
In contrast, we show the following with the gadget $AND_2$.

1) There exists a function for which even the randomized non-adaptive AND decision tree complexity of $f$ is exponentially large in the deterministic one-way communication complexity of $f \circ AND_2$.

2) For symmetric functions $f$, the non-adaptive AND decision tree complexity of $f$ is at most quadratic in the (even two-way) communication complexity of $f \circ AND_2$.

In view of the first point, a lower bound on non-adaptive AND decision tree complexity of $f$ does not lift to a lower bound on one-way communication complexity of $f \circ AND_2$.
The proof of the first point above uses the well-studied Odd-Max-Bit function.
For the second bullet, we first observe a connection between the one-way communication complexity of $f$ and the M\"obius sparsity of $f$, and then use a known lower bound on the M\"obius sparsity of symmetric functions. An upper bound on the non-adaptive AND decision tree complexity of symmetric functions follows implicitly from prior work on combinatorial group testing; for the sake of completeness, we include a proof of this result.</summary>
    <updated>2021-05-05T13:16:19Z</updated>
    <published>2021-05-05T13:16:19Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-06T15:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3524</id>
    <link href="https://agtb.wordpress.com/2021/05/05/netecon-2021/" rel="alternate" type="text/html"/>
    <title>NetEcon 2021</title>
    <summary>NetEcon’21, the 16th Workshop on the Economics of Networks, Systems and Computation, will take place on July 23, 2021. NetEcon’21 is a workshop of EC’21, the 22nd ACM Conference on Economics and Computation, which will be held on July 19-23, 2021, co-located with the 6th World Congress of the Game Theory Society. The aim of NetEcon […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>NetEcon’21,</strong> the 16th Workshop on the Economics of Networks, Systems and Computation, will take place on July 23, 2021. NetEcon’21 is a workshop of EC’21, the 22nd ACM Conference on Economics and Computation, which will be held on July 19-23, 2021, co-located with the 6th World Congress of the Game Theory Society. The aim of NetEcon is to foster discussions on the application of economic and game-theoretic models and principles to address challenges in the development of networks and network-based applications and services.</p>



<p>Details regarding submission rules and dates can be found at <a href="https://netecon21.gametheory.online/" rel="noreferrer noopener" target="_blank">https://netecon21.gametheory.online/</a>. A novelty compared with prior editions of the workshop is that papers that were already formatted for and submitted to EC’21 or SIGMETRICS’21 may retain this format (for submission) if submitted together with all the reviews (see submission guidelines for details).</p></div>
    </content>
    <updated>2021-05-05T01:12:21Z</updated>
    <published>2021-05-05T01:12:21Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Kevin Leyton-Brown</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2021-05-06T15:37:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01615</id>
    <link href="http://arxiv.org/abs/2105.01615" rel="alternate" type="text/html"/>
    <title>Deterministic Rounding of Dynamic Fractional Matchings</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhattacharya:Sayan.html">Sayan Bhattacharya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kiss:Peter.html">Peter Kiss</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01615">PDF</a><br/><b>Abstract: </b>We present a framework for deterministically rounding a dynamic fractional
matching. Applying our framework in a black-box manner on top of existing
fractional matching algorithms, we derive the following new results: (1) The
first deterministic algorithm for maintaining a $(2-\delta)$-approximate
maximum matching in a fully dynamic bipartite graph, in arbitrarily small
polynomial update time. (2) The first deterministic algorithm for maintaining a
$(1+\delta)$-approximate maximum matching in a decremental bipartite graph, in
polylogarithmic update time. (3) The first deterministic algorithm for
maintaining a $(2+\delta)$-approximate maximum matching in a fully dynamic
general graph, in small polylogarithmic (specifically, $O(\log^4 n)$) update
time. These results are respectively obtained by applying our framework on top
of the fractional matching algorithms of Bhattacharya et al. [STOC'16],
Bernstein et al. [FOCS'20], and Bhattacharya and Kulkarni [SODA'19].
</p>
<p>Prior to our work, there were two known general-purpose rounding schemes for
dynamic fractional matchings. Both these schemes, by Arar et al. [ICALP'18] and
Wajc [STOC'20], were randomized.
</p>
<p>Our rounding scheme works by maintaining a good {\em matching-sparsifier}
with bounded arboricity, and then applying the algorithm of Peleg and Solomon
[SODA'16] to maintain a near-optimal matching in this low arboricity graph. To
the best of our knowledge, this is the first dynamic matching algorithm that
works on general graphs by using an algorithm for low-arboricity graphs as a
black-box subroutine. This feature of our rounding scheme might be of
independent interest.
</p></div>
    </summary>
    <updated>2021-05-05T22:54:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01582</id>
    <link href="http://arxiv.org/abs/2105.01582" rel="alternate" type="text/html"/>
    <title>FPT algorithms for packing $k$-safe spanning rooted sub(di)graphs</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bessy:St=eacute=phane.html">Stéphane Bessy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/H=ouml=rsch:Florian.html">Florian Hörsch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maia:Ana_Karolinna.html">Ana Karolinna Maia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rautenbach:Dieter.html">Dieter Rautenbach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sau:Ignasi.html">Ignasi Sau</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01582">PDF</a><br/><b>Abstract: </b>We study three problems introduced by Bang-Jensen and Yeo [Theor. Comput.
Sci. 2015] and by Bang-Jensen, Havet, and Yeo [Discret. Appl. Math. 2016] about
finding disjoint "balanced" spanning rooted substructures in graphs and
digraphs, which generalize classic packing problems. Namely, given a positive
integer $k$, a digraph $D=(V,A)$, and a root $r \in V$, we consider the problem
of finding two arc-disjoint $k$-safe spanning $r$-arborescences and the problem
of finding two arc-disjoint $(r,k)$-flow branchings. We show that both these
problems are FPT with parameter $k$, improving on existing XP algorithms. The
latter of these results answers a question of Bang-Jensen, Havet, and Yeo
[Discret. Appl. Math. 2016]. Further, given an integer $k$, a graph $G=(V,E)$,
and $r \in V$, we consider the problem of finding two arc-disjoint $(r,k)$-safe
spanning trees. We show that this problem is also FPT with parameter $k$, again
improving on a previous XP algorithm. Our main technical contribution is to
prove that the existence of such spanning substructures is equivalent to the
existence of substructures with size and maximum (out-)degree both bounded by a
(linear or quadratic) function of $k$, which may be of independent interest.
</p></div>
    </summary>
    <updated>2021-05-05T22:48:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01485</id>
    <link href="http://arxiv.org/abs/2105.01485" rel="alternate" type="text/html"/>
    <title>Hadamard matrices in $\{0,1\}$ presentation and an algorithm for generating them</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sharipov:Ruslan.html">Ruslan Sharipov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01485">PDF</a><br/><b>Abstract: </b>Hadamard matrices are square $n\times n$ matrices whose entries are ones and
minus ones and whose rows are orthogonal to each other with respect to the
standard scalar product in $\Bbb R^n$. Each Hadamard matrix can be transformed
to a matrix whose entries are zeros and ones. This presentation of Hadamard
matrices is investigated in the paper and based on it an algorithm for
generating them is designed.
</p></div>
    </summary>
    <updated>2021-05-05T22:54:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01469</id>
    <link href="http://arxiv.org/abs/2105.01469" rel="alternate" type="text/html"/>
    <title>Counting vertices of integer polytopes defined by facets</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guo:Heng.html">Heng Guo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jerrum:Mark.html">Mark Jerrum</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01469">PDF</a><br/><b>Abstract: </b>We present a number of complexity results concerning the problem of counting
vertices of an integral polytope defined by a system of linear inequalities.
The focus is on polytopes with small integer vertices, particularly 0/1
polytopes and half-integral polytopes.
</p></div>
    </summary>
    <updated>2021-05-05T22:37:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01465</id>
    <link href="http://arxiv.org/abs/2105.01465" rel="alternate" type="text/html"/>
    <title>Isolation schemes for problems on decomposable graphs</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nederlof:Jesper.html">Jesper Nederlof</a>, Michał Pilipczuk, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Swennenhuis:C=eacute=line_M=_F=.html">Céline M. F. Swennenhuis</a>, Karol Węgrzycki <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01465">PDF</a><br/><b>Abstract: </b>The Isolation Lemma of Mulmuley, Vazirani and Vazirani [Combinatorica'87]
provides a self-reduction scheme that allows one to assume that a given
instance of a problem has a unique solution, provided a solution exists at all.
Since its introduction, much effort has been dedicated towards derandomization
of the Isolation Lemma for specific classes of problems. So far, the focus was
mainly on problems solvable in polynomial time.
</p>
<p>In this paper, we study a setting that is more typical for
$\mathsf{NP}$-complete problems, and obtain partial derandomizations in the
form of significantly decreasing the number of required random bits. In
particular, motivated by the advances in parameterized algorithms, we focus on
problems on decomposable graphs. For example, for the problem of detecting a
Hamiltonian cycle, we build upon the rank-based approach from [Bodlaender et
al., Inf. Comput.'15] and design isolation schemes that use
</p>
<p>- $O(t\log n + \log^2{n})$ random bits on graphs of treewidth at most $t$;
</p>
<p>- $O(\sqrt{n})$ random bits on planar or $H$-minor free graphs; and
</p>
<p>- $O(n)$-random bits on general graphs.
</p>
<p>In all these schemes, the weights are bounded exponentially in the number of
random bits used. As a corollary, for every fixed $H$ we obtain an algorithm
for detecting a Hamiltonian cycle in an $H$-minor-free graph that runs in
deterministic time $2^{O(\sqrt{n})}$ and uses polynomial space; this is the
first algorithm to achieve such complexity guarantees. For problems of more
local nature, such as finding an independent set of maximum size, we obtain
isolation schemes on graphs of treedepth at most $d$ that use $O(d)$ random
bits and assign polynomially-bounded weights.
</p>
<p>We also complement our findings with several unconditional and conditional
lower bounds, which show that many of the results cannot be significantly
improved.
</p></div>
    </summary>
    <updated>2021-05-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01459</id>
    <link href="http://arxiv.org/abs/2105.01459" rel="alternate" type="text/html"/>
    <title>Inaccessible Entropy II: IE Functions and Universal One-Way Hashing</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haitner:Iftach.html">Iftach Haitner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Holenstein:Thomas.html">Thomas Holenstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reingold:Omer.html">Omer Reingold</a>, Salil Vadhan, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wee:Hoeteck.html">Hoeteck Wee</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01459">PDF</a><br/><b>Abstract: </b>This paper uses a variant of the notion of \emph{inaccessible entropy}
(Haitner, Reingold, Vadhan and Wee, STOC 2009), to give an alternative
construction and proof for the fundamental result, first proved by Rompel (STOC
1990), that \emph{Universal One-Way Hash Functions (UOWHFs)} can be based on
any one-way functions. We observe that a small tweak of any one-way function
$f$ is already a weak form of a UOWHF: consider the function $F(x,i)$ that
returns the $i$-bit-long prefix of $f(x)$. If $F$ were a UOWHF then given a
random $x$ and $i$ it would be hard to come up with $x'\neq x$ such that
$F(x,i)=F(x',i)$. While this may not be the case, we show (rather easily) that
it is hard to sample $x'$ with almost full entropy among all the possible such
values of $x'$. The rest of our construction simply amplifies and exploits this
basic property.Combined with other recent work, the construction of three
fundamental cryptographic primitives (Pseudorandom Generators, Statistically
Hiding Commitments and UOWHFs) out of one-way functions is now to a large
extent unified. In particular, all three constructions rely on and manipulate
computational notions of entropy in similar ways. Pseudorandom Generators rely
on the well-established notion of pseudoentropy, whereas Statistically Hiding
Commitments and UOWHFs rely on the newer notion of inaccessible entropy.
</p></div>
    </summary>
    <updated>2021-05-05T22:38:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01427</id>
    <link href="http://arxiv.org/abs/2105.01427" rel="alternate" type="text/html"/>
    <title>Codes for the Z-channel</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Polyanskii:Nikita.html">Nikita Polyanskii</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Yihan.html">Yihan Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01427">PDF</a><br/><b>Abstract: </b>This paper is a collection of results on combinatorial properties of codes
for the Z-channel. A Z-channel with error fraction $\tau$ takes as input a
length-$n$ binary codeword and injects in an adversarial manner $n\tau$
asymmetric errors, i.e., errors that only zero out bits but do not flip $0$'s
to $1$'s. It is known that the largest $(L-1)$-list-decodable code for the
Z-channel with error fraction $\tau$ has exponential (in $n$) size if $\tau$ is
less than a critical value that we call the Plotkin point and has constant size
if $\tau$ is larger than the threshold. The $(L-1)$-list-decoding Plotkin point
is known to be $ L^{-\frac{1}{L-1}} - L^{-\frac{L}{L-1}} $, which equals $1/4$
for unique-decoding with $ L-1=1 $. In this paper, we derive various results
for the size of the largest codes above and below the list-decoding Plotkin
point. In particular, we show that the largest $(L-1)$-list-decodable code
$\epsilon$-above the Plotkin point has size $\Theta_L(\epsilon^{-3/2})$ for any
$L-1\ge1$. We also devise upper and lower bounds on the exponential size of
codes below the list-decoding Plotkin point.
</p></div>
    </summary>
    <updated>2021-05-05T22:41:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01413</id>
    <link href="http://arxiv.org/abs/2105.01413" rel="alternate" type="text/html"/>
    <title>Classes of intersection digraphs with good algorithmic properties</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaffke:Lars.html">Lars Jaffke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kwon:O=joung.html">O-joung Kwon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Telle:Jan_Arne.html">Jan Arne Telle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01413">PDF</a><br/><b>Abstract: </b>An intersection digraph is a digraph where every vertex $v$ is represented by
an ordered pair $(S_v, T_v)$ of sets such that there is an edge from $v$ to $w$
if and only if $S_v$ and $T_w$ intersect. An intersection digraph is reflexive
if $S_v\cap T_v\neq \emptyset$ for every vertex $v$. Compared to well-known
undirected intersection graphs like interval graphs and permutation graphs, not
many algorithmic applications on intersection digraphs have been developed.
Motivated by the successful story on algorithmic applications of intersection
graphs using a graph width parameter called mim-width, we introduce its
directed analogue called `bi-mim-width' and prove that various classes of
reflexive intersection digraphs have bounded bi-mim-width. In particular, we
show that as a natural extension of $H$-graphs, reflexive $H$-digraphs have
linear bi-mim-width at most $12|E(H)|$, which extends a bound on the linear
mim-width of $H$-graphs [On the Tractability of Optimization Problems on
$H$-Graphs. Algorithmica 2020]. For applications, we introduce a novel
framework of directed versions of locally checkable problems, that streamlines
the definitions and the study of many problems in the literature and
facilitates their common algorithmic treatment. We obtain unified
polynomial-time algorithms for these problems on digraphs of bounded
bi-mim-width, when a branch decomposition is given. Locally checkable problems
include Kernel, Dominating Set, and Directed $H$-Homomorphism.
</p></div>
    </summary>
    <updated>2021-05-05T22:53:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01390</id>
    <link href="http://arxiv.org/abs/2105.01390" rel="alternate" type="text/html"/>
    <title>Optimal Algorithms for Range Searching over Multi-Armed Bandits</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Barman:Siddharth.html">Siddharth Barman</a>, Ramakrishnan Krishnamurthy, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rahul:Saladi.html">Saladi Rahul</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01390">PDF</a><br/><b>Abstract: </b>This paper studies a multi-armed bandit (MAB) version of the range-searching
problem. In its basic form, range searching considers as input a set of points
(on the real line) and a collection of (real) intervals. Here, with each
specified point, we have an associated weight, and the problem objective is to
find a maximum-weight point within every given interval.
</p>
<p>The current work addresses range searching with stochastic weights: each
point corresponds to an arm (that admits sample access) and the point's weight
is the (unknown) mean of the underlying distribution. In this MAB setup, we
develop sample-efficient algorithms that find, with high probability,
near-optimal arms within the given intervals, i.e., we obtain PAC (probably
approximately correct) guarantees. We also provide an algorithm for a
generalization wherein the weight of each point is a multi-dimensional vector.
The sample complexities of our algorithms depend, in particular, on the size of
the optimal hitting set of the given intervals.
</p>
<p>Finally, we establish lower bounds proving that the obtained sample
complexities are essentially tight. Our results highlight the significance of
geometric constructs -- specifically, hitting sets -- in our MAB setting.
</p></div>
    </summary>
    <updated>2021-05-05T22:55:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01265</id>
    <link href="http://arxiv.org/abs/2105.01265" rel="alternate" type="text/html"/>
    <title>Finding Triangles or Independent Sets</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dumitrescu:Adrian.html">Adrian Dumitrescu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01265">PDF</a><br/><b>Abstract: </b>(I) We revisit the algorithmic problem of finding all triangles in a graph
</p>
<p>$G=(V,E)$ with $n$ vertices and $m$ edges. According to a result of Chiba and
Nishizeki (1985), this task can be achieved by a combinatorial algorithm
running in $O(m^{3/2})$ time. We derive this worst-case bound from first
principles and with a simple proof. We then provide a combinatorial algorithm
for finding all triangles in a graph and show that is amenable to the same
running time analysis. We also show that the running time of such an algorithm
cannot be improved in the worst-case and for the entire range of parameters $m$
and $n$. Our arguments are extended to the problem of finding all small
complete subgraphs of a given fixed size.
</p>
<p>(II) Given a graph $G=(V,E)$ with $n$ vertices and $m$ edges, we present a
randomized algorithm that computes a $(1 \pm \varepsilon)$-approximation of the
number of triangles in $G$ and finds a witness with high probability in
$O\left( n^{\omega(1-\delta)} \right)$ or $O \left( \left( m n^{-2\delta}
\right)^{\frac{2\omega}{\omega+1}} \right)$ expected time, provided $G$ has a
suitable superlinear number of edges and triangles (where $\omega &lt; 2.376$ is
the exponent of matrix multiplication, and $\varepsilon \leq 0.5$, $\delta \leq
0.25$ are positive constants). This limits the range of a conjecture of
P\v{a}tra\c{s}cu (2010) regarding the triangle detection problem.
</p>
<p>(III) We present an algorithm which given a graph $G=(V,E)$ performs one of
the following tasks in $O(m+n)$ (i.e., linear) time: (i) compute a
$\Omega(1/\sqrt{n})$-approximation of a maximum independent set in $G$ (a
well-known NP-hard problem), or (ii) find a triangle in $G$. The run-time is
faster than that for any previous method for each of these tasks. A series of
trade-off results is also available.
</p></div>
    </summary>
    <updated>2021-05-05T22:46:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01201</id>
    <link href="http://arxiv.org/abs/2105.01201" rel="alternate" type="text/html"/>
    <title>Spectral independence, coupling with the stationary distribution, and the spectral gap of the Glauber dynamics</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jain:Vishesh.html">Vishesh Jain</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pham:Huy_Tuan.html">Huy Tuan Pham</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vuong:Thuy_Duong.html">Thuy Duong Vuong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01201">PDF</a><br/><b>Abstract: </b>We present a new lower bound on the spectral gap of the Glauber dynamics for
the Gibbs distribution of a spectrally independent $q$-spin system on a graph
$G = (V,E)$ with maximum degree $\Delta$. Notably, for several interesting
examples, our bound covers the entire regime of $\Delta$ excluded by arguments
based on coupling with the stationary distribution. As concrete applications,
by combining our new lower bound with known spectral independence computations
and known coupling arguments:
</p>
<p>(1) We show that for a triangle-free graph $G = (V,E)$ with maximum degree
$\Delta \geq 3$, the Glauber dynamics for the uniform distribution on proper
$k$-colorings with $k \geq (1.763\dots + \delta)\Delta$ colors has spectral gap
$\tilde{\Omega}_{\delta}(|V|^{-1})$. Previously, such a result was known either
if the girth of $G$ is at least $5$ [Dyer et.~al, FOCS 2004], or under
restrictions on $\Delta$ [Chen et.~al, STOC 2021; Hayes-Vigoda, FOCS 2003].
</p>
<p>(2) We show that for a regular graph $G = (V,E)$ with degree $\Delta \geq 3$
and girth at least $6$, and for any $\varepsilon, \delta &gt; 0$, the partition
function of the hardcore model with fugacity $\lambda \leq
(1-\delta)\lambda_{c}(\Delta)$ may be approximated within a
$(1+\varepsilon)$-multiplicative factor in time
$\tilde{O}_{\delta}(n^{2}\varepsilon^{-2})$. Previously, such a result was
known if the girth is at least $7$ [Efthymiou et.~al, SICOMP 2019].
</p>
<p>(3) We show for the binomial random graph $G(n,d/n)$ with $d = O(1)$, with
high probability, an approximately uniformly random matching may be sampled in
time $O_{d}(n^{2+o(1)})$. This improves the corresponding running time of
$\tilde{O}_{d}(n^{3})$ due to [Jerrum-Sinclair, SICOMP 1989; Jerrum, 2003].
</p></div>
    </summary>
    <updated>2021-05-05T22:54:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01193</id>
    <link href="http://arxiv.org/abs/2105.01193" rel="alternate" type="text/html"/>
    <title>Improved approximation algorithms for bounded-degree local Hamiltonians</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anshu:Anurag.html">Anurag Anshu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gosset:David.html">David Gosset</a>, Karen J. Morenz Korol, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soleimanifar:Mehdi.html">Mehdi Soleimanifar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01193">PDF</a><br/><b>Abstract: </b>We consider the task of approximating the ground state energy of two-local
quantum Hamiltonians on bounded-degree graphs. Most existing algorithms
optimize the energy over the set of product states. Here we describe a family
of shallow quantum circuits that can be used to improve the approximation ratio
achieved by a given product state. The algorithm takes as input an $n$-qubit
product state $|v\rangle$ with mean energy $e_0=\langle v|H|v\rangle$ and
variance $\mathrm{Var}=\langle v|(H-e_0)^2|v\rangle$, and outputs a state with
an energy that is lower than $e_0$ by an amount proportional to
$\mathrm{Var}^2/n$. In a typical case, we have $\mathrm{Var}=\Omega(n)$ and the
energy improvement is proportional to the number of edges in the graph. When
applied to an initial random product state, we recover and generalize the
performance guarantees of known algorithms for bounded-occurrence classical
constraint satisfaction problems. We extend our results to $k$-local
Hamiltonians and entangled initial states.
</p></div>
    </summary>
    <updated>2021-05-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01161</id>
    <link href="http://arxiv.org/abs/2105.01161" rel="alternate" type="text/html"/>
    <title>Approximability of all finite CSPs in the dynamic streaming setting</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chou:Chi=Ning.html">Chi-Ning Chou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovnev:Alexander.html">Alexander Golovnev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sudan:Madhu.html">Madhu Sudan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Velusamy:Santhoshini.html">Santhoshini Velusamy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01161">PDF</a><br/><b>Abstract: </b>A constraint satisfaction problem (CSP), Max-CSP$({\cal F})$, is specified by
a finite set of constraints ${\cal F} \subseteq \{[q]^k \to \{0,1\}\}$ for
positive integers $q$ and $k$. An instance of the problem on $n$ variables is
given by $m$ applications of constraints from ${\cal F}$ to subsequences of the
$n$ variables, and the goal is to find an assignment to the variables that
satisfies the maximum number of constraints. In the
$(\gamma,\beta)$-approximation version of the problem for parameters $0 \leq
\beta &lt; \gamma \leq 1$, the goal is to distinguish instances where at least
$\gamma$ fraction of the constraints can be satisfied from instances where at
most $\beta$ fraction of the constraints can be satisfied.
</p>
<p>In this work we consider the approximability of this problem in the context
of streaming algorithms and give a dichotomy result in the dynamic setting,
where constraints can be inserted or deleted. Specifically, for every family
${\cal F}$ and every $\beta &lt; \gamma$, we show that either the approximation
problem is solvable with polylogarithmic space in the dynamic setting, or not
solvable with $o(\sqrt{n})$ space. We also establish tight inapproximability
results for a broad subclass in the streaming insertion-only setting. Our work
builds on, and significantly extends previous work by the authors who consider
the special case of Boolean variables ($q=2$), singleton families ($|{\cal F}|
= 1$) and where constraints may be placed on variables or their negations. Our
framework extends non-trivially the previous work allowing us to appeal to
richer norm estimation algorithms to get our algorithmic results. For our
negative results we introduce new variants of the communication problems
studied in the previous work, build new reductions for these problems, and
extend the technical parts of previous works.
</p></div>
    </summary>
    <updated>2021-05-05T22:42:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01149</id>
    <link href="http://arxiv.org/abs/2105.01149" rel="alternate" type="text/html"/>
    <title>Near-Optimal Cayley Expanders for Abelian Groups</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jalan:Akhil.html">Akhil Jalan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moshkovitz:Dana.html">Dana Moshkovitz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01149">PDF</a><br/><b>Abstract: </b>We give an efficient deterministic algorithm that outputs an expanding
generating set for any finite abelian group. The size of the generating set is
close to the randomized construction of Alon and Roichman (1994), improving
upon various deterministic constructions in both the dependence on the
dimension and the spectral gap. By obtaining optimal dependence on the
dimension we resolve a conjecture of Azar, Motwani, and Naor (1998) in the
affirmative. Our technique is an extension of the bias amplification technique
of Ta-Shma (2017), who used random walks on expanders to obtain expanding
generating sets over the additive group of n-bit strings. As a consequence, we
obtain (i) randomness-efficient constructions of almost k-wise independent
variables, (ii) a faster deterministic algorithm for the Remote Point Problem,
(iii) randomness-efficient low-degree tests, and (iv) randomness-efficient
verification of matrix multiplication.
</p></div>
    </summary>
    <updated>2021-05-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.01138</id>
    <link href="http://arxiv.org/abs/2105.01138" rel="alternate" type="text/html"/>
    <title>Fault Tolerant Max-Cut</title>
    <feedworld_mtime>1620172800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Censor=Hillel:Keren.html">Keren Censor-Hillel</a>, Noa Marelly, Roy Schwartz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tonoyan:Tigran.html">Tigran Tonoyan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.01138">PDF</a><br/><b>Abstract: </b>In this work, we initiate the study of fault tolerant Max Cut, where given an
edge-weighted undirected graph $G=(V,E)$, the goal is to find a cut $S\subseteq
V$ that maximizes the total weight of edges that cross $S$ even after an
adversary removes $k$ vertices from $G$. We consider two types of adversaries:
an adaptive adversary that sees the outcome of the random coin tosses used by
the algorithm, and an oblivious adversary that does not. For any constant
number of failures $k$ we present an approximation of $(0.878-\epsilon)$
against an adaptive adversary and of $\alpha_{GW}\approx 0.8786$ against an
oblivious adversary (here $\alpha_{GW}$ is the approximation achieved by the
random hyperplane algorithm of [Goemans-Williamson J. ACM `95]). Additionally,
we present a hardness of approximation of $\alpha_{GW}$ against both types of
adversaries, rendering our results (virtually) tight.
</p>
<p>The non-linear nature of the fault tolerant objective makes the design and
analysis of algorithms harder when compared to the classic Max Cut. Hence, we
employ approaches ranging from multi-objective optimization to LP duality and
the ellipsoid algorithm to obtain our results.
</p></div>
    </summary>
    <updated>2021-05-05T22:45:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/064</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/064" rel="alternate" type="text/html"/>
    <title>TR21-064 |  Streaming approximation resistance of every ordering CSP | 

	Santhoshini Velusamy, 

	Noah Singer, 

	Madhu Sudan</title>
    <summary>An ordering constraint satisfaction problem (OCSP) is given by a positive integer $k$ and a constraint predicate $\Pi$ mapping permutations on $\{1,\ldots,k\}$ to $\{0,1\}$. Given an instance of OCSP$(\Pi)$ on $n$ variables and $m$ constraints, the goal is to find an ordering of the $n$ variables that maximizes the number of constraints that are satisfied, where a constraint specifies a sequence of $k$ distinct variables and the constraint is satisfied by an ordering on the $n$ variables if the ordering induced on the $k$ variables in the constraint satisfies $\Pi$. Ordering constraint satisfaction problems capture natural problems including ''Maximum acyclic subgraph (MAS)'' and ''Betweenness''. 

In this work we consider the task of approximating the maximum number of satisfiable constraints in the (single-pass) streaming setting, where an instance is presented as a stream of constraints. We show that for every $\Pi$, OCSP$(\Pi)$ is approximation-resistant to $o(\sqrt{n})$-space streaming algorithms, i.e., algorithms using $o(\sqrt{n})$ space cannot distinguish streams where almost every constraint is satisfiable from streams where no ordering beats the random ordering by a noticeable amount. In the case of MAS our result shows that for every $\epsilon&gt;0$, MAS is not $1/2+\epsilon$-approximable. The previous best inapproximability result only ruled out a $3/4$ approximation.

Our results build on a recent work of Chou, Golovnev, Sudan, and Velusamy who show tight inapproximability results for some constraint satisfaction problems over arbitrary (finite) alphabets. We show that the hard instances from this earlier work have the following ''small-set expansion'' property: in every partition of the hypergraph formed by the constraints into small blocks, most of the hyperedges are incident on vertices from distinct blocks. By exploiting this combinatorial property, in combination with a natural reduction from CSPs over large finite alphabets to OCSPs, we give optimal inapproximability results for all OCSPs.</summary>
    <updated>2021-05-04T21:39:50Z</updated>
    <published>2021-05-04T21:39:50Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-06T15:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.let-all.com/blog/?p=55</id>
    <link href="https://www.let-all.com/blog/2021/05/04/alt-highlights-an-interview-with-the-pc-chairs-of-alt-2021/" rel="alternate" type="text/html"/>
    <title>ALT Highlights – An Interview with the PC Chairs of ALT 2021</title>
    <summary>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference ALT 2021, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by the <a href="https://www.let-all.com/">Learning Theory Alliance</a>, and overseen by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Learning Theory Alliance blog</a>.</p>



<p>This is the fourth post in the series, an interview with ALT 2021 PC Chairs <a href="http://vtaly.net/">Vitaly Feldman</a> and <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a>, written by <a href="https://www.comp.nus.edu.sg/~sutanu/">Sutanu Gayen</a> and <a href="https://sites.google.com/view/michal-moshkovitz">Michal Moshkovitz</a>.</p>



<hr class="wp-block-separator"/>



<p>We had the great opportunity to attend <em>The 32nd International Conference on Algorithmic Learning Theory</em>, held online between March 16-19, 2021, and co-chaired by Vitaly Feldman and Katrina Ligett. Vitaly is a research scientist at Apple AI Research and has done foundational works in machine learning and privacy-preserving data analysis. Katrina is an Associate Professor of Computer Science at the Hebrew University of Jerusalem and has done pivotal works in data privacy, algorithmic fairness, algorithmic game theory, and online algorithms. We asked for an interview with them about their experiences and perspectives as co-chairs, to which they kindly agreed. We are happy to share with the readers the excerpts of this interview.</p>



<p class="has-text-align-center"><img src="https://lh4.googleusercontent.com/7RUOfC-v06amphwIhfCYsQNH4jUg82AVsePZcbWO0D40DhSRk-Cf_wnXx9y5RI-qVYz6D-IRAxRzsQ9lWBpraofuggrTYC_sG40GehOCvSrQyZfj1khlMTVqK23NKOZueGcbK_xa" style="width: 225px;"/>           <img height="252" src="https://lh4.googleusercontent.com/lUMmpb6Bcfq3bPljS7jYaF2TFaXRks64--IeslcdR3WzqnCtUETu-ymoOSxm7ys_6eyRFb2mGmd1mCe1boNHdxii0d1_UCthAEJy_eTsWsGQsFKpsV5snZe3Nm9g-QDy0JTnzPKx" width="194"/></p>



<p><strong>How it started</strong></p>



<p>How are chairs and program committees chosen? </p>



<p class="has-text-color" style="color: #0000ff;">Katrina: The chairs are selected by the Association for Algorithmic Learning Theory (AALT) Steering Committee (<a href="http://algorithmiclearningtheory.org/alt-steering-committee/">http://algorithmiclearningtheory.org/alt-steering-committee/</a>), and the chairs then select the program committee members. Vitaly and I brainstormed potential PC member names, solicited additional suggestions, and also considered the lists of people who have served on recent ALT and COLT PCs. In building the PC, we had many considerations in mind, including coverage of research areas, and various metrics of diversity.</p>



<p><strong>The chairs’ role</strong></p>



<p>What are the different tasks a chair has? What is the most difficult task?</p>



<p class="has-text-color" style="color: #0000ff;">Katrina: At a high level, the roles of the PC chairs are to build the PC, oversee the reviewing process and create the conference program. Practically, though, there are a lot of decisions that need to be discussed, emails to be sent, and a lot of organizational aspects to tend to—configuring the reviewing platform, sending reminders, chasing down late reviews, and so on. Vitaly and I have a very, very long joint “to do” list—and luckily, it’s now almost all crossed off! We also had additional responsibilities this year because of the move to the virtual conference format, including selecting the technologies, overseeing the pre-recording process, and much more.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I think the hardest and probably the most time-consuming is making the accept/reject decisions on papers.  For a large fraction of the papers arriving at the decision requires getting a sense of the results; understanding the main points in reviews, author responses and discussion (while calibrating them to the PC members and reviewers); ensuring that each paper is properly discussed by chasing reviewers, asking questions and often soliciting additional opinions. We also needed to come up with a set of criteria for deciding on borderline cases and make sure that these criteria are applied as consistently as possible. At the end it is a rather long and iterative process that luckily for us has converged to a program we are happy with.</p>



<p>How much time do you spend doing chair tasks? How do you balance chairing a conference (a massive amount of work) with all your other commitments? Do you turn down other service items you would generally accept, etc.?</p>



<p class="has-text-color" style="color: #0000ff;">Katrina: It’s difficult to estimate the number of hours, but I think we have been meeting regularly since early June 2020, and we’re only wrapping up our work now, in late March 2021. It’s a much longer-timeframe commitment than serving as a PC member. I actually am chairing a second conference this year, FORC, and together it makes for a pretty serious load. As a result, I have been declining all other conference-related service. I also have a couple of other pretty substantial service commitments, as well, so I just don’t have bandwidth this year for additional PC and Area Chair-type roles.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that it’s hard to tell how much time we spent in total. My rough estimate is that it’s about a month of full-time work. I also had to decline most other service commitments during that period some of which I’d normally accept. Naturally, it also slows down other work so I definitely had to lean more on my collaborators in some of the ongoing projects <img alt="&#x1F642;" class="wp-smiley" src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f642.png" style="height: 1em;"/></p>



<p>Can chairs bring their own personality into the conference? How? </p>



<p class="has-text-color" style="color: #0000ff;">Katrina: One area where the chairs enjoy freedom is in selecting the keynote and tutorial speakers. I’m biased, of course, but I think we chose very well, and all of these speakers (Joelle Pineau, Shay Moran, and Costis Daskalakis) gave excellent talks (they were recorded—check them out if you missed them)! We also were fortunate to be able to work with amazing partners who organized the mentoring workshop (Surbhi Goel, Nika Hagtalab, and Ellen Vitercik) and the Women in ML Theory event (Tosca Lechner and Ruth Urner). These aspects of the conference beyond the papers are a way for the chairs to express their priorities.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: The chairs have a lot of freedom in choosing how to run the review process, design the conference program and who else will be involved. So, inevitably, the chairs’ personalities and tastes end up being reflected in the final results. </p>



<p>Does the online conference impact the chair job? How? </p>



<p class="has-text-color" style="color: #0000ff;">Katrina: Typically, the PC chairs build the program, and then many details of organizing and running the conference get handed off to local organization chairs. But this year, since ALT was held virtually, there were many unusual tasks that fell to the PC chairs—not just the obvious ones like choosing the technologies and format and negotiating those contracts, but smaller things like chasing down authors who failed to upload their recordings, and developing instructions for people in various roles to interact with the conference platform.</p>



<p class="has-text-color" style="color: #0000ff;">In addition, COVID times placed strains on many people, which made it more challenging to recruit PC members, and resulted in a higher than usual rate of late reviews and PC drop-outs, which of course left us scrambling.</p>



<p>What motivates you to spend time on a conference service?</p>



<p class="has-text-color" style="color: #0000ff;">Katrina: We all rely on the conference system for our personal professional advancement, and for the advancement of our field as a whole. So we all owe that system our service, of course, according to our abilities, availability, and seniority. Also, it’s fun to get this different perspective on the conference review process and on the field. And it’s an honor to be entrusted with shepherding a conference for a year and hopefully nurturing its growth.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that it’s a mix of (1) contribution to the community I’m a member of and, perhaps, an opportunity to improve some of its processes (2) a learning experience that gives one a higher-level view of the research that is happening and people who do it (3) honor and recognition that come with the job.</p>



<p><strong>Awards </strong></p>



<p>How do you decide which papers were chosen as awards? </p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: A necessary condition for a paper to receive an award is that at least one of the PC members/reviewers assigned to the paper is excited about the results.  So we start by looking at papers with the highest scores (typically all papers that received at least one “strong accept”) and reading their reviews. This allowed us to narrow down the list to a set of 5-6 candidates. From those we selected the winners by learning more about the results and selecting those, we found the most significant and interesting for the community.</p>



<p><strong>The review process</strong></p>



<p>What are your thoughts about the current peer review process in ALT? What are the downsides and advantages? Do you have suggestions for improvement?</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: It is common that confidence in correctness of the results in a conference submission is based on higher-level sanity checks and general intuition of the reviewers. Naturally, the more interesting and important result the more likely it is to be scrutinized. At this ALT we did not run into a situation where the authors’ reputation affected our confidence in the correctness of the results. In case of concerns about correctness of an interesting result we would ask either an expert on the PC or an external expert to try to verify the result.</p>



<p class="has-luminous-vivid-orange-color has-text-color">ALT currently relies on a traditional theory conference model of reviewing and for a typical submission has several PC members who are experts in the subarea. The reviewing load is also relatively light (8 papers per PC member). So I think that the overall reviewing quality is pretty much as good as it gets in ML (and is similar to COLT). Naturally, the model is not perfect and there is still variation in the quality of individual reviews. This year many more reviewers and PC members were under unusual time pressure due to the pandemic so perhaps the variation was higher than usual.</p>



<p><strong>The future</strong></p>



<p>What are your suggestions for the next chair? </p>



<p class="has-text-color" style="color: #0000ff;">Katrina: Make sure you have a good co-chair. <img alt="&#x1F642;" class="wp-smiley" src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f642.png" style="height: 1em;"/> Vitaly has been a great partner for this process—fun to work with, reliable, always willing to pitch in even on the less-fun tasks, and I have great respect for his technical perspective.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that diversity of perspectives and expertise is useful in several ways. Most notably, it gives the chairs a wider network of people to select the PC from. But I completely agree with Katrina, that the most important thing is the ability of co-chairs to work well together: after all, it’s a lot of work and complicated decisions that need to be made jointly. Here, I couldn’t have asked for more: Katrina is amazing both professionally and personally. Working with her was definitely the highlight of being the ALT co-chair and learned a lot from her in the process as well.</p></div>
    </content>
    <updated>2021-05-04T16:40:08Z</updated>
    <published>2021-05-04T16:40:08Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://www.let-all.com/blog</id>
      <logo>https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/04/logo.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://www.let-all.com/blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://www.let-all.com/blog" rel="alternate" type="text/html"/>
      <title>The Learning Theory Alliance Blog</title>
      <updated>2021-05-06T15:41:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=559</id>
    <link href="https://tcsplus.wordpress.com/2021/05/04/tcs-talk-wednesday-may-12-santhoshini-velusamy-harvard-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 12 — Santhoshini Velusamy, Harvard University</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Santhoshini Velusamy from Harvard University will speak about “Classification of the approximability of all finite Max-CSPs in the dynamic streaming setting” (abstract below). You can reserve a spot […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://scholar.harvard.edu/santhoshiniv/home"><strong>Santhoshini Velusamy</strong></a> from Harvard University will speak about “<em>Classification of the approximability of all finite Max-CSPs in the dynamic streaming setting</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk)</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: A maximum constraint satisfaction problem, Max-CSP(F), is specified by a finite family of constraints F, where each constraint is of arity k. An instance of the problem on n variables is given by m applications of constraints from F to length-k subsequences of the n variables, and the goal is to find an assignment to the n variables that satisfies the maximum number of constraints. The class of Max-CSP(F) includes optimization problems such as Max-CUT, Max-DICUT, Max-3SAT, Max-q-Coloring, Unique Games, etc.</p>
<p>In this talk, I will present our recent dichotomy theorem on the approximability of Max-CSP(F) for every finite family F, in the single-pass dynamic streaming setting. In this setting, at each time step, a constraint is either added to or deleted from the stream. In the end, the streaming algorithm must estimate the maximum number of constraints that can be satisfied using space that is only polylogarithmic in n. No background in streaming algorithms or constraint satisfaction problems will be needed to enjoy this talk!</p>
<p>The talk will be based on <a href="https://eccc.weizmann.ac.il/report/2021/011/">this paper</a>, and <a href="https://eccc.weizmann.ac.il/report/2021/063/">this paper</a> with Chi-Ning Chou, Alexander Golovnev, and Madhu Sudan.</p></blockquote></div>
    </content>
    <updated>2021-05-04T06:48:59Z</updated>
    <published>2021-05-04T06:48:59Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-05-06T15:40:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/05/03/postdoc-at-uc-irvine-apply-by-june-4-2021/</id>
    <link href="https://cstheory-jobs.org/2021/05/03/postdoc-at-uc-irvine-apply-by-june-4-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at UC Irvine (apply by June 4, 2021)</title>
    <summary>One Post-doctoral position is available under the guidance of Ioannis Panageas. The appointment is for one year and may be renewable if funding permits. Requirement is a Ph.D. in TCS/Theory of ML, or related field. Expertise can be demonstrated by 3 top-tier publications in venues like ICML, NeurIPS, AISTATS, STOC, FOCS, SODA, ICALP, EC. Anticipated […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>One Post-doctoral position is available under the guidance of Ioannis Panageas. The appointment is for one year and may be renewable if funding permits. Requirement is a Ph.D. in TCS/Theory of ML, or related field. Expertise can be demonstrated by 3 top-tier publications in venues like ICML, NeurIPS, AISTATS, STOC, FOCS, SODA, ICALP, EC. Anticipated starting date is October 1 2021 (negotiable).</p>
<p>Website: <a href="https://recruit.ap.uci.edu/JPF06615">https://recruit.ap.uci.edu/JPF06615</a><br/>
Email: ipanagea@ics.uci.edu</p></div>
    </content>
    <updated>2021-05-03T22:59:14Z</updated>
    <published>2021-05-03T22:59:14Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-05-06T15:38:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/063</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/063" rel="alternate" type="text/html"/>
    <title>TR21-063 |  Approximability of all finite CSPs in the dynamic streaming setting | 

	Chi-Ning  Chou, 

	Alexander Golovnev, 

	Madhu Sudan, 

	Santhoshini Velusamy</title>
    <summary>A constraint satisfaction problem (CSP), Max-CSP$({\cal F})$, is specified by a finite set of constraints ${\cal F} \subseteq \{[q]^k \to \{0,1\}\}$ for positive integers $q$ and $k$. An instance of the problem on $n$ variables is given by $m$ applications of constraints from ${\cal F}$ to subsequences of the $n$ variables, and the goal is to find an assignment to the variables that satisfies the maximum number of constraints. In the $(\gamma,\beta)$-approximation version of the problem for parameters $0 \leq \beta &lt; \gamma \leq 1$, the goal is to distinguish instances where at least $\gamma$ fraction of the constraints can be satisfied from instances where at most $\beta$ fraction of the constraints can be satisfied. 

In this work we consider the approximability of this problem in the context of streaming algorithms and give a dichotomy result in the dynamic setting, where constraints can be inserted or deleted. Specifically,  for every family ${\cal F}$ and every $\beta &lt; \gamma$,  we show that either the approximation problem is solvable with polylogarithmic space in the dynamic setting, or not solvable with $o(\sqrt{n})$ space. We also establish tight inapproximability results for a broad subclass in the streaming insertion-only setting. Our work builds on, and significantly extends previous work by the authors who consider the special case of Boolean variables ($q=2$), singleton families ($|{\cal F}| = 1$) and where constraints may be placed on variables or their negations. Our framework extends non-trivially the previous work allowing us to appeal to richer norm estimation algorithms to get our algorithmic results. For our negative results we introduce new variants of the communication problems studied in the previous work, build new reductions for these problems, and extend the technical parts of previous works. In particular, previous works used Fourier analysis over the Boolean cube to prove their results and the results seemed particularly tailored to functions on Boolean literals (i.e., with negations). Our techniques surprisingly allow us to get to general $q$-ary CSPs without negations by appealing to the same Fourier analytic starting point over Boolean hypercubes.</summary>
    <updated>2021-05-03T20:10:43Z</updated>
    <published>2021-05-03T20:10:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-06T15:37:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7474459772601125760</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7474459772601125760/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/the-mythical-man-month-hen-day-and-cat.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7474459772601125760" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7474459772601125760" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/the-mythical-man-month-hen-day-and-cat.html" rel="alternate" type="text/html"/>
    <title>The Mythical Man-Month, Hen-Day, and Cat-Minute (Fred Brooks Turned 90)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><i> The Mythical Man-Month </i>is a great book which talks about the (obvious in retrospect) fact that putting more people on a project may slow it down. It was by Fred Brooks who turned 90 in April (he is still alive). It's a good read. I actually read it many years ago when I exchanged books with a Software Engineer I was dating- She lent me <i>The Mythical Man Month </i>which I found interesting, and I lent her <i>What is the name of this book by Smullyan </i>which she found amusing. Did this exchange of books help our relationship? We have now been married for many years, though its not clear if we can trace this to the exchange of books OR to the fact that she had KNUTH Volumes 1 and 3, and I had KNUTH Volume 2. </p><p> Fred Brooks: You have my thanks and of course Happy Birthday!</p><p>When I read The Mythical Man-Month  I was reminded of a math problem I heard as a kid: </p><p>If a hen-and-half lays an egg-and-a-half in a day-and-a-half then how many eggs can seven hen lay in seven days? </p><p>My answer: if (3/2) hens lay (3/2) eggs in (3/2) days then that's 2/3 of an egg per hen-day, so the answer is </p><p>49* 2/3 = 32 and 2/3 eggs.</p><p>It did not bother me one whit that (1) you can't have 2/3 of an egg, and (2) Just like adding more people might slow down a project, adding more hens might end up being a bad idea-- especially if they are all crowded into the same chicken-coop and hence don't feel much like laying eggs.</p><p>Who was the first person to note that adding <i>more</i> people or hens might be a bad idea? I do not know, but here is an amusing, yet realistic, article by Mark Twain on what I would call <i>The mythical</i> <i>cat-minute. </i>My advisor Harry Lewis send it to me in the midst of an email exchange about <i>The Mythical</i> <i>Man-Month.</i> He got it from a student of his, Larry Denenberg. Here it is: </p><p><br/></p><p>CATS AND RATS</p><pre>The following piece first appeared in ``The Monthly Packet'' of February
1880 and is reprinted in _The_Magic_of_Lewis_Carroll_, edited by John
Fisher, Bramhall House, 1973.


   If 6 cats kill 6 rats in 6 minutes, how many will be needed to kill
   100 rats in 50 minutes?

   This is a good example of a phenomenon that often occurs in working
   problems in double proportion; the answer looks all right at first, but,
   when we come to test it, we find that, owing to peculiar circumstances in
   the case, the solution is either impossible or else indefinite, and needing
   further data.  The 'peculiar circumstance' here is that fractional cats or
   rats are excluded from consideration, and in consequence of this the
   solution is, as we shall see, indefinite.

   The solution, by the ordinary rules of Double Proportion, is 12 cats.
   [Steps of Carroll's solution, in the notation of his time, omitted.]

   But when we come to trace the history of this sanguinary scene through all
   its horrid details, we find that at the end of 48 minutes 96 rats are dead,
   and that there remain 4 live rats and 2 minutes to kill them in: the
   question is, can this be done?

   Now there are at least *four* different ways in which the original feat,
   of 6 cats killing 6 rats in 6 minutes, may be achieved.  For the sake of
   clearness let us tabulate them:
      A.  All 6 cats are needed to kill a rat; and this they do in one minute,
          the other rats standing meekly by, waiting for their turn.
      B.  3 cats are needed to kill a rat, and they do it in 2 minutes.
      C.  2 cats are needed, and do it in 3 minutes.
      D.  Each cat kills a rat all by itself, and takes 6 minutes to do it.

   In cases A and B it is clear that the 12 cats (who are assumed to come
   quite fresh from their 48 minutes of slaughter) can finish the affair in
   the required time; but, in case C, it can only be done by supposing that 2
   cats could kill two-thirds of a rat in 2 minutes; and in case D, by
   supposing that a cat could kill one-third of a rat in two minutes.  Neither
   supposition is warranted by the data; nor could the fractional rats (even
   if endowed with equal vitality) be fairly assigned to the different cats.
   For my part, if I were a cat in case D, and did not find my claws in good
   working order, I should certainly prefer to have my one-third-rat cut off
   from the tail end.

   In cases C and D, then, it is clear that we must provide extra cat-power.
   In case C *less* than 2 extra cats would be of no use.  If 2 were supplied,
   and if they began killing their 4 rats at the beginning of the time, they
   would finish them in 12 minutes, and have 36 minutes to spare, during which
   they might weep, like Alexander, because there were not 12 more rats to
   kill.  In case D, one extra cat would suffice; it would kill its 4 rats in
   24 minutes, and have 26 minutes to spare, during which it could have killed
   another 4.  But in neither case could any use be made of the last 2
   minutes, except to half-kill rats---a barbarity we need not take into
   consideration.

   To sum up our results.  If the 6 cats kill the 6 rats by method A or B,
   the answer is 12; if by method C, 14; if by method D, 13.

   This, then, is an instance of a solution made `indefinite' by the
   circumstances of the case.  If an instance of the `impossible' be desired,
   take the following: `If a cat can kill a rat in a minute, how many would be
   needed to kill it in the thousandth part of a second?'  The *mathematical*
   answer, of course, is `60,000,' and no doubt less than this would *not*
   suffice; but would 60,000 suffice?  I doubt it very much.  I fancy that at
   least 50,000 of the cats would never even see the rat, or have any idea of
   what was going on.

   Or take this: `If a cat can kill a rat in a minute, how long would it be
   killing 60,000 rats?'  Ah, how long, indeed!  My private opinion is that
   the rats would kill the cat.
</pre><div><br/></div><p><br/></p></div>
    </content>
    <updated>2021-05-02T19:33:00Z</updated>
    <published>2021-05-02T19:33:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-06T14:29:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/062</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/062" rel="alternate" type="text/html"/>
    <title>TR21-062 |  Improved Hitting Set for Orbit of ROABPs | 

	Vishwas Bhargava, 

	Sumanta Ghosh</title>
    <summary>The orbit of an $n$-variate polynomial $f(\mathbf{x})$ over a field $\mathbb{F}$ is the set $\{f(A \mathbf{x} +  b)\,\mid\, A\in \mathrm{GL}({n,\mathbb{F}})\mbox{ and }\mathbf{b} \in \mathbb{F}^n\}$, and the orbit of a polynomial class is the union of orbits of all the polynomials in it. In this paper, we give improved constructions of hitting-sets for the orbit of read-once oblivious algebraic branching programs (ROABPs) and a related model. Over field with characteristic zero or greater than $d$, we construct a hitting set of size  $(ndw)^{O(w^2\log n\cdot \min\{w^2, d\log w\})}$  for the orbit of ROABPs in unknown variable order where $d$ is the individual degree and $w$ is the width of ROABPs. We also give hitting set of size $(ndw)^{O(\min\{w^2,d\log w\})}$ for the orbit of polynomials  computed by $w$-width ROABPs in any variable order. Our hitting sets improve upon the results of Saha and Thankey \cite{Saha-Thankey'21} who gave an $(ndw)^{O(d\log w)}$ size hitting set for the orbit of commutative ROABPs (a subclass of \textit{any-order} ROABPs) and $(nw)^{O(w^6\log n)}$ size hitting set for the orbit of multilinear ROABPs. Designing better hitting sets in large individual degree regime, for instance $d&gt;n$, was asked as an open problem by \cite{Saha-Thankey'21} and this work solves it in  small width setting. 

We prove some new rank concentration results by establishing \emph{low-cone concentration} for the polynomials over vector spaces, and they strengthen some previously known \emph{low-support} based rank concentrations shown in \cite{FSS14}. These new low-cone concentration results are crucial in our hitting set construction, and may be of independent interest. To the best of our knowledge, this is the first time when low-cone rank concentration has been used for designing hitting sets.</summary>
    <updated>2021-05-02T07:04:05Z</updated>
    <published>2021-05-02T07:04:05Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-06T15:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18674</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/" rel="alternate" type="text/html"/>
    <title>Test of Time</title>
    <summary>Time is the ultimate critic. What future generations think of us and our work ultimately determines our standing or lack of it— Stewart Stafford Bobby Kleinberg just reached out to those of us who post from time to time. He wanted some help in announcing a new STOC Test of Time Award. So today, Ken […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Time is the ultimate critic. What future generations think of us and our work ultimately determines our standing or lack of it— Stewart Stafford</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/bk/" rel="attachment wp-att-18676"><img alt="" class="alignright size-thumbnail wp-image-18676" height="150" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/bk-150x150.png?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
Bobby Kleinberg just reached out to those of us who post from time to time. He wanted some help in announcing a new STOC Test of Time Award. </p>
<p>
So today, Ken and I put this together. </p>
<p>Bobby said: </p>
<blockquote><p><b> </b> <em> As with FOCS, three awards will be given: one for papers from approximately 10 years ago, one for approximately 20 years ago, and one for approximately 30 years ago. The selection committee for this year’s award will be Joe Halpern, Mihalis Yannakakis, and Salil Vadhan. </em>
</p></blockquote>
<p/><p>
I would suggest that one of Bobby’s papers could fit this award: </p>
<p>Group-theoretic algorithms for matrix multiplication <br/>
Henry Cohn, Robert Kleinberg, Balazs Szegedy, Christopher Umans </p>
<p>
Well, I can say that without being out of order <em>here</em> because that paper was in FOCS, not STOC.</p>
<p>
</p><p/><h2> Criteria </h2><p/>
<p>
</p><li>
<i>Area</i>: Opening up a new area of research <p/>
</li><li>
<i>Proof</i>: Introducing new proof techniques <p/>
</li><li>
<i>Use</i>: Solving a problem of lasting importance <p/>
</li><li>
<i>Else</i>: Stimulating advances in other areas of computer science or in other disciplines. <p/>
<p>
Go here for details on how to <a href="https://sigact.org/prizes/stoc_tot.html">nominate</a>. By the way: Another test of Time is that the nominations are due relatively soon—May 24. So if you wish to nominate some paper please act soon. </p>
<p>
Ken notes that the first criterion could also be called <i>Leadership</i>, the second always comes with an element of <i>Surprise</i>, and the last two have aspects of <i>Applicability</i> and <i>Practicality</i>.  Adding those to my terms makes a double acronym <i>APPLAUSE</i>.</p>
<p>
</p><p/><h2> Early Early Years </h2><p/>
<p/><p>
I have been around long enough to fit the a 30++ years category, and Ken almost <a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/">ditto</a>. Here are some opinions on the early days. Those papers with<br/>
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img alt="" class="alignleft  wp-image-18682" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1"/></a><br/>
are an absolute must include—I hope you agree.</p>
<p>
<a href="https://dblp.org/db/conf/stoc/stoc81.html">1981</a>:</p>
<p>
Space-Bounded Probabilistic Turing Machine Complexity Classes Are Closed under Complement <br/>
<i>Use</i>.</p>
<p>
<a href="https://dblp.org/db/conf/stoc/stoc82.html">1982</a>:</p>
<p>
Shafi Goldwasser, Silvio Micali <br/>
Probabilistic Encryption and How to Play Mental Poker Keeping Secret All Partial Information <br/>
<i>Area</i>.</p>
<p>
<a href="https://dl.acm.org/doi/proceedings/10.1145/800061">1983</a>:</p>
<p>
Miklós Ajtai, Janos Komlós, and Endre Szemerédi <br/>
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img alt="" class="alignleft  wp-image-18682" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1"/></a>An <img alt="{0(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> sorting network <br/>
<i>Use</i> and <i>Proof</i>.</p>
<p>
Larry Stockmeyer <br/>
The complexity of approximate counting <br/>
<i>Use</i>.</p>
<p>
<a href="https://dl.acm.org/doi/proceedings/10.1145/800057">1984</a>:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img alt="" class="alignleft  wp-image-18682" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1"/></a>Les Valiant <br/>
A theory of the learnable <br/>
<i>Area</i> and <i>Else</i>.</p>
<p>
Narendra Karmarkar <br/>
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img alt="" class="alignleft  wp-image-18682" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1"/></a>A new polynomial-time algorithm for linear programming <br/>
<i>Use</i> and <i>Proof</i>.</p>
<p>
Of course, these are my own opinions (with concurrence from Ken) and do not reflect those of organizations we belong to.</p>
<p/><h2> Open Problems </h2><p/>
<p/><p>
Ken thinks that one way not to be asked here about my own papers is to mention one, so here goes:</p>
<p>
<a href="https://dblp.org/db/conf/stoc/stoc80.html">1980</a></p>
<p>
Ravindran Kannan, Richard Lipton <br/>
The Orbit Problem is Decidable <br/>
<i>Proof</i>.  Ken adds: Could also be <i>Surprise</i> because we not only showed decidable but in polynomial time.  But the real test of time here may be whether (despite some technical limitations) it proves useful to the great recent interest in adjacent kinds of orbit problems in <a href="https://rjlipton.wpcomstaging.com/2018/06/06/princeton-is-invariant/">invariant</a> and <a href="https://rjlipton.wpcomstaging.com/2015/07/12/the-long-reach-of-reachability/">reachability</a> theory.</p></li></font></font></div>
    </content>
    <updated>2021-04-30T22:53:16Z</updated>
    <published>2021-04-30T22:53:16Z</published>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="People"/>
    <category term="award"/>
    <category term="Bobby Kleinberg"/>
    <category term="Prize"/>
    <category term="STOC"/>
    <category term="Test of Time"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-05-06T15:38:18Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-3713824207379370168</id>
    <link href="http://processalgebra.blogspot.com/feeds/3713824207379370168/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=3713824207379370168" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/3713824207379370168" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/3713824207379370168" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/04/polyconc-online-collaboration-to.html" rel="alternate" type="text/html"/>
    <title>PolyConc: Online collaboration to improve on a result on the equational theory of CCS modulo bisimilarity</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The aim of this post is to try and start an online collaboration to improve the solution to a problem in the equational logic of processes that I posed in a <a href="https://www.brics.dk/NS/03/2/BRICS-NS-03-2.pdf" target="_blank">survey paper in 2003</a>, namely    </p><blockquote>  Can one obtain a finite axiomatisation of the parallel composition operator in bisimulation semantics by adding only one binary operator to the signature of (recursion, restriction, and relabelling free) CCS? </blockquote><p>     Valentina Castiglioni, Wan Fokkink, Anna Ingólfsdóttir, Bas Luttik and I published a partial, negative answer to the above question in a <a href="https://doi.org/10.4230/LIPIcs.CSL.2021.8" target="_blank">paper at CSL 2021</a>. (See the <a href="https://arxiv.org/pdf/2010.01943.pdf" target="_blank">arXiv version</a> for details and for the historical context for the above question.) Our solution is based on three simplifying assumptions that are described in detail in Section 3 of the <a href="https://arxiv.org/pdf/2010.01943.pdf" target="_blank">above-mentioned paper</a>. We'd be very interested in hearing whether any member of the research community in process algebra, universal algebra and equational logic can relax or remove any of our simplifying assumptions. In particular, one can start with assumptions 3 and 2. </p><p>We would also welcome any comments and suggestions on whether some version of that problem can be solved using existing results from equational logic and universal algebra. In particular, are there any general results guaranteeing that, under certain conditions, the reduct of a finitely based algebra is also finitely based? Or, conversely, that if some algebra is not finitely based, then so its expansion with a new operator?</p><p>To start with, add any contributions you might have as comments to this post. If ever we make substantial enough progress on the above question, anyone who has played a positive role in extending our results will be a co-author of the resulting paper. </p><p>Let PolyConc begin!<br/></p><p/></div>
    </content>
    <updated>2021-04-30T20:41:00Z</updated>
    <published>2021-04-30T20:41:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-05-06T15:25:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/04/30/linkage</id>
    <link href="https://11011110.github.io/blog/2021/04/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>The EFF on FLoC (\(\mathbb{M}\)), Google’s plan for browsers to aggregate your browsing habits and make them public for ad-personalization. Short summary: it’s a bad idea and if you care about privacy you should switch to a non-Chrome browser. Technical summary: it’s based on k-anonymity, known as inadequate at protecting individual privacy in social networks. If you use Chrome, assume all bad guys on the web can see all your browsing.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.eff.org/deeplinks/2021/03/googles-floc-terrible-idea">The EFF on FLoC</a> (<a href="https://mathstodon.xyz/@11011110/106079326968515725">\(\mathbb{M}\)</a>), Google’s plan for browsers to aggregate your browsing habits and make them public for ad-personalization. Short summary: it’s a bad idea and if you care about privacy you should switch to a non-Chrome browser. Technical summary: it’s based on <a href="https://en.wikipedia.org/wiki/K-anonymity">k-anonymity</a>, known as <a href="https://doi.org/10.1109/CASoN.2010.139">inadequate at protecting individual privacy in social networks</a>. If you use Chrome, assume all bad guys on the web can see all your browsing.</p>
  </li>
  <li>
    <p>Relevant to my recent post on Pick’s theorem: <a href="https://www.youtube.com/watch?v=osF2JhrVHxc">Chris Staecker on the dot planimeter</a> (<a href="https://mathstodon.xyz/@11011110/106090366361151274">\(\mathbb{M}\)</a>), a device for <a href="https://en.wikipedia.org/wiki/Dot_planimeter">approximating area by counting grid points</a>.</p>
  </li>
  <li>
    <p><a href="https://www.peeta.net/">Anamorphic street art by Peeta transforms building shapes into 3d geometric abstractions</a> (<a href="https://mathstodon.xyz/@11011110/106100758800338449">\(\mathbb{M}\)</a>, <a href="https://weburbanist.com/2019/07/12/anamorphic-street-art-new-abstract-murals-by-peeta-pop-off-the-wall/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://lore.kernel.org/linux-nfs/YH+zwQgBBGUJdiVK@unreal/">Students of University of Minnesota assistant professor Kangjie Lu caught allegedly deliberately sending buggy patches to Linux kernel as some kind of breaching experiment</a>, resulting in <a href="https://lore.kernel.org/linux-nfs/YH%2FfM%2FTsbmcZzwnX@kroah.com/">the whole university being banned from Linux kernel development</a> (<a href="https://mathstodon.xyz/@11011110/106104208447478044">\(\mathbb{M}\)</a>, <a href="https://lobste.rs/s/3qgyzp/they_introduce_kernel_bugs_on_purpose">via</a>). They claim to have been declared IRB-exempt but this appears to be a mistake by the IRB. See also <a href="https://cse.umn.edu/cs/statement-cse-linux-kernel-research-april-21-2021">department reaction</a> and <a href="https://www.metafilter.com/191207/How-to-get-your-University-banned-in-1-easy-step">metafilter discussion</a>.</p>
  </li>
  <li>
    <p><a href="https://mastodon.social/@joshmillard/106109652170356976">Josh Millard plays with algorithmically-generated pen-plotter art</a>; <a href="https://www.patreon.com/posts/50677186">more</a>.</p>
  </li>
  <li>
    <p><a href="https://www.flyingcoloursmaths.co.uk/a-pretty-puzzle/">You could prove that the number of integer solutions to \(x^2+xy+y^2=a\) is a multiple of six for positive \(a\) by finding a hidden group structure</a> (<a href="https://mathstodon.xyz/@11011110/106113101236984677">\(\mathbb{M}\)</a>). Or, you could recognize that it’s the norm of the Eisenstein integers under a small change of basis from the usual one and that they have six-fold rotational symmetry.</p>
  </li>
  <li>
    <p><a href="https://www.origamitessellations.com/2018/03/paper-engineering-from-the-bauhaus-josef-albers-to-the-modern-day/">Paper engineering from the Bauhaus</a> and <a href="https://www.origamitessellations.com/2018/04/reverse-engineering-bauhaus-paper-designs-part-two/">reverse-engineering Bauhaus paper designs</a> (<a href="https://mathstodon.xyz/@11011110/106118829181433597">\(\mathbb{M}\)</a>). These designs are more curved kirigami than origami, producing smooth-looking 3d shapes from cut sheets of flat paper.</p>
  </li>
  <li>
    <p><a href="https://www.scientificamerican.com/article/the-art-of-mathematics-in-chalk/">The art of mathematics in chalk</a> (<a href="https://mathstodon.xyz/@11011110/106124866673951925">\(\mathbb{M}\)</a>, <a href="https://whatsonmyblackboard.wordpress.com/">see also</a>). Teaser for the forthcoming book <em>Do Not Erase: Mathematicians and Their Chalkboards</em>, featuring several photographic spreads of chalkboard illustrations and formulas and their explanations. They appear to be mostly set-ups rather than captured from active research, but still pretty and interesting. <a href="https://11011110.github.io/blog/2019/09/30/linkage.html">I linked an earlier post on this in 2019</a> but with fewer photos and no explanations.</p>
  </li>
  <li>
    <p><a href="https://coq.discourse.group/t/renaming-coq/1264">The Coq theorem prover brainstorms a name change</a> (<a href="https://mathstodon.xyz/@11011110/106127971601789143">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/191240/Not-every-woman-is-offended-by-this-name-but-enough-people-are">via</a>), after too many women get harrassed for saying they work on Coq.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Tetrad_(geometry_puzzle)">Tetrad puzzle</a> (<a href="https://mathstodon.xyz/@11011110/106136227486814000">\(\mathbb{M}\)</a>). It’s possible to arrange four congruent hexagons so they tile a disk with each pair sharing a length of boundary, but the known pentagons with four pairwise-touching copies leave a hole in the region they tile. Is the hole necessary?</p>
  </li>
  <li>
    <p><a href="https://github.andrewt.net/mercator-rotator/">Mercator Rotator</a> (<a href="https://mastodon.social/@andrewt/105950778379233419">\(\mathbb{M}\)</a>), a tool for drawing Mercator-projection world maps with different viewpoints than the usual one. Set the pole on a place you don’t like to see the map of a world without it.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/138752/440">Tetrahedra passing through a hole</a> (<a href="https://mathstodon.xyz/@11011110/106152970915208525">\(\mathbb{M}\)</a>). This is from eight years ago, but was active again recently. The question is: what’s the smallest-area hole in a plane through which you can push a unit tetrahedron? DPKR has a very pretty animated answer, but sadly it’s not optimal: there’s a triangular hole with smaller area \(1/\sqrt{8}\), known <a href="https://doi.org/10.1016/j.comgeo.2011.07.004">minimal for translational motion</a>. The problem for more general motion seems to be still open.</p>
  </li>
  <li>
    <p><a href="http://arxiv.org/abs/1112.4205v2">Lagarias’s survey on the Takagi Function</a> and <a href="https://www.jstor.org/stable/2324028">Mallows’ survey on Conway’s $10,000 sequence</a> (<a href="https://mathstodon.xyz/@11011110/106153079186577081">\(\mathbb{M}\)</a>, <a href="https://mathstodon.xyz/@esoterica/106152848486538030">via</a>, <a href="https://en.wikipedia.org/wiki/Blancmange_curve">see also</a>) have very similar-looking figures, but little or no overlap in references. Maybe someone knows of an explanation for the similarity?</p>
  </li>
</ul></div>
    </content>
    <updated>2021-04-30T17:00:00Z</updated>
    <published>2021-04-30T17:00:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-01T00:37:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8108</id>
    <link href="https://windowsontheory.org/2021/04/30/alt-highlights-equilibrium-computation-and-the-foundations-of-deep-learning/" rel="alternate" type="text/html"/>
    <title>ALT Highlights – Equilibrium Computation and the Foundations of Deep Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">[Guest post by Kush Bhatia and Cyrus Rashtchian, foreword by Gautam Kamath] Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference ALT 2021, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs … <a class="more-link" href="https://windowsontheory.org/2021/04/30/alt-highlights-equilibrium-computation-and-the-foundations-of-deep-learning/">Continue reading <span class="screen-reader-text">ALT Highlights – Equilibrium Computation and the Foundations of Deep Learning</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>[Guest post by Kush Bhatia and Cyrus Rashtchian, foreword by Gautam Kamath]</p>



<p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. Boaz has kindly agreed to host a post in this series. This initiative is organized by the <a href="https://www.let-all.com/">Learning Theory Alliance</a>, and overseen by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Learning Theory Alliance blog</a>.</p>



<p>This is the third post in the series, an interview with <a href="http://people.csail.mit.edu/costis/">Constantinos Daskalakis</a> and coverage of his ALT 2021 <a href="https://www.youtube.com/watch?v=GpaCWKlOMig">keynote talk</a>, written by <a href="https://people.eecs.berkeley.edu/~kush/">Kush Bhatia</a> and <a href="https://sites.google.com/site/cyrusrashtchian/">Cyrus Rashtchian</a>.</p>



<hr class="wp-block-separator"/>



<p>To make a decision for ourselves, we need to think about the impact of our actions to our objectives. But, when our actions affect other people and their actions affect our objectives, we also need to consider their incentives, and choose our actions in anticipation of theirs. This increase in complexity also occurs in situations involving multiple decision-making machines (e.g., self-driving cars), automated systems (e.g., algorithmic stock trading), or living organisms (e.g., groups of cells).</p>



<p>Studying decision-making in so-called multi-agent environments has been a question of interest to mathematicians and economists for centuries. In the 1830s, Antoine Augustin Cournot developed a theory of competition to model oligopolies, inspired by observing competition in a spring water duopoly. Jumping forward to the 20th century, researchers converged that a fruitful approach to analyzing multi-agent systems is studying them at “equilibrium”, that is, in situations where the system is stable in the sense that all parties are satisfied with their actions. A fundamental concept of equilibrium, studied by John von Neumann, and later by von Neumann and Oskar Morgenstern is a collection of actions, one per agent, such that no agent has an incentive to deviate from their choice given the actions of the other agents. While a nice proposal, they could only show that such equilibrium is guaranteed to exist in situations conforming to what is called a “two-player zero-sum game”. In such games, two agents are in exact competition with each other; whatever one player wins the other loses.<sup><a href="https://windowsontheory.org/feed/#fn1">1</a></sup> In 1950, John F. Nash showed that this notion of equilibrium, named Nash equilibrium in his honor, indeed exists for most naturally occurring multi-agent problems.<sup><a href="https://windowsontheory.org/feed/#fn2">2</a></sup></p>



<p>While Nash established the existence of equilibrium, one question bothered economists and computer scientists alike: “<em>Is it possible to efficiently find the Nash equilibrium of a game</em>?”</p>



<p>Throughout the rest of the 20th century, many people proposed algorithms for computing Nash equilibrium, but none of them succeeded in showing that it can be done efficiently (i.e., in polynomial time in the size of the game). During his early graduate school days at UC Berkeley, Constantinos (a.k.a. Costis) Daskalakis obsessed over this question. Then, in 2006, Costis, along with co-authors Paul Goldberg and Christos Papadimitriou, who was also his PhD advisor, showed a surprising result: finding a Nash equilibrium is computationally intractable!</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-311" src="https://kamathematics.files.wordpress.com/2021/04/fig1-1.png?w=643"/><strong>Figure 1.</strong> Utility functions encode the value an agent gets from a particular decision. (a) In classical Economics and Game Theory literature, these trade off benefits and costs of actions. For e.g., consider an agent deciding on the quantity <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> of basic material to procure while the other agent sets the price <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> per unit of this material. If we let <img alt="g(x)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28x%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> denote the revenue from selling the product produced using <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> units of basic material, where <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a concave function of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> (capturing diminishing returns), then the overall utility of the agent choosing <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> is <img alt="f(x,y) = g(x) - x \cdot y" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%2Cy%29+%3D+g%28x%29+-+x+%5Ccdot+y&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, an overall concave function. (b) Modern multi-agent problems involve agents choosing parameters of deep neural networks. This quickly leads to non-concave agent utilities.</figure>



<p>A central concept in analyzing multi-agent systems is the <em>utility function</em> of each interacting agent. This is  a function that captures the value that the agent derives as a function of their own action as well as those of the other agents. A common assumption is that the utility function of each agent is a concave function of their own action for any collection of actions committed by the others. Concavity often arises when agents trade off benefits and costs from their actions taking into account properties like diminishing returns and risk-aversion (see Figure 1 for an illustration). It is also crucial in guaranteeing that equilibria exist.</p>



<h3>From Game Theory and Economics to Deep Learning</h3>



<p>Recently, Costis has shifted his attention to the more general setting where the underlying utilities can be arbitrary non-concave functions of the agents’ actions. “Earlier, I was interested in the problem of equilibrium computation for its fundamental applications in Game Theory and Economics and its intimate connections to duality theory, topology and complexity theory. As Machine Learning is now moving towards multi-agent learning, studying more general setups arising from nonconcave agent utilities becomes increasingly relevant,” says Costis. He elaborates that the recent success of deep learning methods has largely been in single-agent setups and that the next frontier is to replicate this success in multi-agent settings. It is at this intersection of deep learning and multi-agent learning that non-concave utility functions arise — when actions correspond to setting the parameters of deep neural networks, agent utilities quickly become non-concave in the space of these parameters(see Figure 1).</p>



<p>The focus of Costis’ <a href="https://www.youtube.com/watch?v=GpaCWKlOMig" rel="noreferrer noopener" target="_blank">keynote talk</a>, on joint work with Stratis Skoulakis and Manolis Zampetakis [<a href="https://windowsontheory.org/feed/#DSZ21">DSZ21</a>] was on the simplest multi-agent problem: two-player zero-sum games. In these games, two players, the <em>min</em> and the <em>max</em> player, choose actions <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> respectively, which are constrained to lie in some compact and convex set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, i.e., <img alt="(x,y) \in S" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Cin+S&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The agents share some objective function <img alt="f(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%2Cy%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> that <em>min</em> wants to minimize and <em>max</em> wants to maximize. Classical studies, going back to von Neumann’s celebrated work [<a href="https://windowsontheory.org/feed/#vN28">vN28</a>] focus on when this objective is a convex function of the <em>min</em> player’s action and a concave function of the <em>max</em> player’s action, the so-called “convex-concave setting”. For any function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> which is convex-concave, there exists a Nash equilibrium, i.e., a point <img alt="(x^*, y^*)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%5E%2A%2C+y%5E%2A%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> satisfying</p>



<p class="has-text-align-center" id="eq-nash"><img alt="f(x^*, y) \leq f(x^*, y^*) \leq f(x, y^*) \quad \text{for all}\; x, y \text{ such that } (x, y^*), (x^*, y) \in S.\qquad (1)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%5E%2A%2C+y%29+%5Cleq+f%28x%5E%2A%2C+y%5E%2A%29+%5Cleq+f%28x%2C+y%5E%2A%29+%5Cquad+%5Ctext%7Bfor+all%7D%5C%3B+x%2C+y+%5Ctext%7B+such+that+%7D+%28x%2C+y%5E%2A%29%2C+%28x%5E%2A%2C+y%29+%5Cin+S.%5Cqquad+%281%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Thus the <em>min</em> (<em>max</em>) player has no incentive to deviate from <img alt="x^* (y^*)" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%2A+%28y%5E%2A%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> as long as the other player remains fixed. The existence of such <img alt="(x^*, y^*)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%5E%2A%2C+y%5E%2A%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> follows from von Neumann’s minimax theorem and Rosen’s generalization of this theorem to the case that agents actions are jointly constrained [<a href="https://windowsontheory.org/feed/#Ros65">Ros65</a>]. However, when <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> is not convex-concave, the minimax theorem fails, and we lose the existence of Nash equilibrium. For a simple example, consider <img alt="f(x,y) = (x-y)^2" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%2Cy%29+%3D+%28x-y%29%5E2&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> over the space <img alt="S=[0,1]^2" class="latex" src="https://s0.wp.com/latex.php?latex=S%3D%5B0%2C1%5D%5E2&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Given any decision choice <img alt="(x, y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> if <img alt="x\neq y" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cneq+y&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the <em>min</em> player should move <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> towards <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Otherwise, if <img alt="x = y" class="latex" src="https://s0.wp.com/latex.php?latex=x+%3D+y&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the <em>max</em> player wants to move away from <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Thus, no pair <img alt="(x^*, y^*)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%5E%2A%2C+y%5E%2A%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> satisfies equation <a href="https://windowsontheory.org/feed/#eq-nash">(1)</a>.</p>



<p>Nonconvex-nonconcave utility functions naturally arise in adversarial training applications, such as Generative Adversarial Network (GAN) training, where the goal is to learn how to generate new data, such as images, from the same distribution that generated a collection of given data. Specifically, GANs are trained by trying to identify the equilibrium of a two-player zero-sum game between a generator model (the <em>min</em> player) and a discriminator model (the <em>max</em> player). Each of these models are viewed as agents, choosing parameters in deep neural networks, and the objective, capturing how close the generated distribution is to the target distribution, amounts to a nonconvex-nonconcave function of the underlying network parameters, which the <em>min</em> player aims to minimize and the <em>max</em> player aims to maximize.</p>



<p>Given that Nash equilibria may not exist when the objective is not convex-concave, what type of solutions should we target when studying two player zero-sum games with such objectives? “One property that we would like our target solutions to possess is that they are universal, i.e. they are guaranteed to exist for any objective function. We can take them to a practitioner and tell them that they are always plausible targets for their computations,” says Costis. With this in mind, Costis and his co-authors consider a relaxed equilibrium concept, called <img alt="(\epsilon, \delta)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cepsilon%2C+%5Cdelta%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>-Nash equilibrium. This is a pair <img alt="(x^*, y^*)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%5E%2A%2C+y%5E%2A%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> satisfying</p>



<p class="has-text-align-center"><img alt="f(x^*, y^*) &lt; f(x, y^*) + \epsilon \quad \text{for all}\; x \text{ such that } \|x - x^*\| \leq \delta \text{ and } (x,y^*)\in S;" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%5E%2A%2C+y%5E%2A%29+%3C+f%28x%2C+y%5E%2A%29+%2B+%5Cepsilon+%5Cquad+%5Ctext%7Bfor+all%7D%5C%3B+x+%5Ctext%7B+such+that+%7D+%5C%7Cx+-+x%5E%2A%5C%7C+%5Cleq+%5Cdelta+%5Ctext%7B+and+%7D+%28x%2Cy%5E%2A%29%5Cin+S%3B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/><img alt="f(x^*, y^*) &gt; f(x^*, y) - \epsilon \quad \text{for all}\; y \text{ such that } \|y - y^*\| \leq \delta \text{ and } (x^*,y)\in S." class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%5E%2A%2C+y%5E%2A%29+%3E+f%28x%5E%2A%2C+y%29+-+%5Cepsilon+%5Cquad+%5Ctext%7Bfor+all%7D%5C%3B+y+%5Ctext%7B+such+that+%7D+%5C%7Cy+-+y%5E%2A%5C%7C+%5Cleq+%5Cdelta+%5Ctext%7B+and+%7D+%28x%5E%2A%2Cy%29%5Cin+S.&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This relaxes Nash equilibrium: given strategy <img alt="y^*" class="latex" src="https://s0.wp.com/latex.php?latex=y%5E%2A&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> for the <em>max</em> player, the <em>min</em> player can improve by at most <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> by changing their action in a ball of radius <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> around <img alt="x^*" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and a symmetric condition holds for the <em>max</em> player, given strategy <img alt="x^*" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> for the <em>min</em> player. One of the main insights of their paper is that such local Nash equilibria <em>are guaranteed to</em> exist as long as <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a small enough function of <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>’s smoothness, namely whenever <img alt="\delta \le \sqrt{2\epsilon \over L}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cle+%5Csqrt%7B2%5Cepsilon+%5Cover+L%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> is <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>’s smoothness. This non-trivial result is established via an application of Brouwer’s fixed point theorem.</p>



<h3>Can algorithms find a local Nash equilibrium?</h3>



<p>The next question, pertaining to computational complexity, is to determine whether finding an <img alt="(\epsilon, \delta)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cepsilon%2C+%5Cdelta%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>-Nash equilibrium is algorithmically tractable in the regime of parameters (small enough <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>) where it is guaranteed to exist. As a first step, Costis and his co-authors focus on first-order algorithms, which have access to the gradient of the objective function. Examples include gradient descent and variants thereof, which have been the main computational engine behind the success of deep learning in single-agent problems. A classical result known for these methods in minimization settings is that they are efficient in computing <img alt="(\epsilon,\delta)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cepsilon%2C%5Cdelta%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>-minima of non-convex, smooth objectives <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>. These are points <img alt="x^*" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> such that <img alt="f(x^*) &lt; f(x) + \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%5E%2A%29+%3C+f%28x%29+%2B+%5Cepsilon&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> for all feasible <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> such that <img alt="\|x - x^*\| \leq \delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Cx+-+x%5E%2A%5C%7C+%5Cleq+%5Cdelta&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Namely, given query access to the gradient <img alt="\nabla f" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+f&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> of some <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>-smooth objective <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> with values normalized to <img alt="[0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, it is possible to compute <img alt="(\epsilon,\delta)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cepsilon%2C%5Cdelta%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>-minima in polynomially many, in <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="1/\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Cepsilon&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, steps and queries to <img alt="\nabla f" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+f&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, as long as <img alt="\delta \le \sqrt{2\epsilon \over L}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cle+%5Csqrt%7B2%5Cepsilon+%5Cover+L%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In contrast to minimization, a main contribution of Costis’ work is to establish an intractability result for min-maximization, showing that the number of gradient queries for any first-order algorithm to compute <img alt="(\epsilon,\delta)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cepsilon%2C%5Cdelta%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>-Nash equilibria must be exponential in at least one of <img alt="1/\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Cepsilon&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the dimension <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, or the smoothness <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> of the objective.</p>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<blockquote class="wp-block-quote"><p>Theorem 1 (informal). First-order methods need a number of queries to <img alt="\nabla f" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+f&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> that is exponential in at least one of <img alt="1 /\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1+%2F%5Cepsilon&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, or the dimension to find <img alt="(\epsilon,\delta)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cepsilon%2C%5Cdelta%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>-Nash equilibria, even when <img alt="\delta \le \sqrt{2\epsilon \over L}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cle+%5Csqrt%7B2%5Cepsilon+%5Cover+L%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, i.e. in the regime in which they are guaranteed to exist.</p></blockquote>



<p>This theorem tells us that there exist objective functions for which the min-maximization problem can be computationally intractable for any first-order algorithm. Requiring many queries is one way to say that the problem is hard in practice. Indeed, practitioners have found it notoriously hard to get the discriminator-generator neural networks to converge to good solutions for generative modelling problems using gradient-based methods.</p>



<p>From a technical perspective, this work represents a new approach for proving lower bounds in optimization. Classical lower bounds in the optimization literature, going back to Nemirovsky and Yudin [<a href="https://windowsontheory.org/feed/#NY83">NY83</a>] target the black-box setting: an algorithm is given access to an oracle which outputs some desired information about a function when presented with a query. For example, a first-order oracle outputs the gradient of a function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> at a given input. Costis shared that he and his co-authors first tried to construct a black-box lower bound for local Nash equilibria directly. However, they were unsuccessful. Any direct construction they tried ended up introducing spurious local Nash equilibria, which first-order algorithms might find in polynomial time. Their direct attempts at a lower bound failed to capture the computational hardness of the problem. They quickly realized that they needed a deeper understanding of the problem at hand, better insight on what made it harder than minimization.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-310" height="331" src="https://kamathematics.files.wordpress.com/2021/04/fig2.png?w=1011" width="581"/><strong>Figure 2. </strong>(a) Black-box models work with an oracle which an algorithm queries to obtain information. White-box models provide algorithms access to functions which can compute the desired information. (b) Architecture of black-box intractability results (focus of the optimization literature): first show computational hardness results in the white-box model (focus of computational complexity); then compose those with black-box lower bounds for any problem in the class for which hardness results were established.</figure></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p>That insight came when they switched to studying the complexity of the white-box version of the problem, wherein the optimization algorithm can look inside the oracle that computes <img alt="\nabla f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> and possibly <img alt="f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> as well. One might wonder why one would want to consider such white-box models over black-box ones if their goal is to prove intractability results for methods that have limited access to <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Indeed, proving an intractability result in the white-box model is much harder because the set of algorithms that use white-box access to the objective is strictly bigger than those using only black-box access. However, the key difference is that we are not looking for the same kind of hardness in the two models. In the black-box model, we are looking for unconditional computational hardness, that is, showing that any algorithm will require exponentially many queries to the gradient oracle. On the other hand, in the white-box model, we would like to show complexity-theoretic hardness, i.e., show that solving the problem at hand is at least as hard (or exactly as hard) as solving the hardest problems in some complexity class. Such a complexity-theoretic hardness result is conditional; it says that solving this problem will be computationally intractable as long as some computational complexity conjecture, such as P <img alt="\neq" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cneq&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> NP, holds. Importantly, showing hardness (or completeness) of a problem in some complexity class typically entails a fine-grained understanding of the nature of the problem and how that enables it to encode other problems in the target complexity class.</p>
</div></div>



<p>In the white-box model, the authors show that the problem of computing local Nash equilibria is PPAD-complete. In other words, computing this local equilibrium concept in zero-sum games with nonconvex-nonconcave objectives is exactly as hard as computing Nash equilibria in general-sum games with concave agent utilities.<sup><a href="https://windowsontheory.org/feed/#fn3">3</a></sup> This result is established by exhibiting a reduction from a variant of the Sperner coloring problem,<sup><a href="https://windowsontheory.org/feed/#fn4">4</a></sup> which is a PPAD-complete problem, to a discrete nonconvex-nonconcave min-maximization problem, where the two agents choose points on a hypergrid.</p>



<h3>Unexpected challenges in high dimensions</h3>



<p>Having established this result, Costis and his coauthors presumed that the hardest part of the problem was behind them. However, another challenge awaited them. They still had to construct a continuous interpolation of their discrete function to satisfy the desired Lipschitz and smoothness properties in a computationally efficient manner. To understand the challenge with this, consider a simple two-dimensional example with two actions per agent. Suppose we are given prescribed values for <img alt="f(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%2Cy%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> on all four vertices of <img alt="\{0,1\}^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5E2&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> and our goal is to construct a continuous and smooth function on <img alt="[0,1]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D%5E2&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which matches the prescribed function values at the corners. A simple approach is to define <img alt="f(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%2Cy%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> at any point <img alt="(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> using a smooth interpolation of all four corners of <img alt="\{0,1\}^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5E2&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This works, but does not scale to high dimensions. An approach that would scale computationally in high dimensions is to first triangulate <img alt="[0,1]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D%5E2&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> by chopping it along its main diagonal and then interpolate the function on each triangle separately. However, this simple approach fails since the gradient of the interpolated function can be discontinuous when crossing the diagonal. “This part turned out to be more technically challenging than we had thought in high dimensions,” says Costis. He and his coauthors overcame the issue by proposing a new <em>smooth and computationally efficient interpolation</em> scheme, which they expect will have more applications in transferring hardness results from discrete problems to continuous problems.</p>



<p>To obtain Theorem 1, the authors show that one can translate their complexity-theoretic hardness in the white-box model to an unconditional intractability result in the black-box model. This follows immediately from their reduction from Sperner to local Nash equilibrium. Indeed, finding local Nash equilibria in the min-max instance at the output of their reduction provides solutions to the Sperner instance at its input. Moreover, a single query (function value or gradient value) to the min-max objective requires <img alt="O(d)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28d%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> queries to the Sperner coloring circuit in order to be computed. Finally, it is known that, with black-box queries to the Sperner coloring circuit, exponentially many queries are necessary to compute a solution [<a href="https://windowsontheory.org/feed/#HPV89">HPV89</a>, <a href="https://windowsontheory.org/feed/#Pap94">Pap94</a>]. An exponential black-box lower bound for local Nash equilibrium thus follows.</p>



<p>This proof architecture can be used more generally to prove intractability results for optimization problems involving smooth objectives. First, ignore the black-box model and focus on identifying the complexity class that captures the complexity of the problem in the white-box model. Then, focus on obtaining a hardness result for a discrete version of the problem. Once this is established, one can use the techniques presented in this work to lift this intractability from the discrete to the continuous problem. If there are black-box lower bounds for any problem residing in the complexity class for which the white-box version of the problem is hard, then these lower bounds can be composed with hardness reductions to establish lower bounds for the black-box version of the problem. As an aside, Costis mentions that it would be interesting if one could establish the lower bound of Theorem 1 in the black-box model directly, i.e., without going through the PPAD machinery.</p>



<h3>Looking forward</h3>



<p>Costis ends on an optimistic note: “While this might appear as a negative result, it really is a positive one.” Explaining further, he says that a philosophical consequence of his intractability results is that the multi-agent future of deep learning is going to have a lot of interesting “texture” — it will involve a large breadth of communities and motivate a plethora of problems at the interface of theoretical computer science, game theory, economics and machine learning. Costis envisions a change in balance in the multi-agent world: while recent successes of deep learning in single-agent problems capitalize on access to large data, unprecedented computational power, and effective inductive biases, multi-agent problems will demand much stronger inductive biases, invoking domain expertise in order to develop effective models and useful learning targets, as well as to discover algorithms that attain those targets.</p>



<p>Indeed, domain expertise has been crucial in some recent high-profile machine learning achievements in multi-agent settings: the AlphaGo agent for playing the game of Go and the Libratus agent for playing Texas Hold’em. For these, a game-theoretic understanding has been infused into the structure and training of the learning algorithm. In addition to using deep neural networks, AlphaGo uses a Monte Carlo tree search procedure to determine the best next move as well as to collect data for the training of the neural networks during self play, while Libratus uses counterfactual regret minimization to approximate the equilibrium of the game. The success of both algorithms required combining machine learning expertise with game-theoretic expertise about how to solve the games at hand.</p>



<p>More broadly, Costis urges young researchers to move beyond the classical statistical paradigm which assumes independent and identically distributed observations, and embrace learning challenges that are motivated from learning problems with state, incomplete or biased data, data with dependencies, and multi-agent learning applications. In particular, he would like to see more activity in obtaining new models and algorithms for reinforcement and multi-agent learning, better tools for high-dimensional learning problems with data bias and dependencies, as well as deeper connections to causal inference and econometrics. <em>“There is a lot of beautiful mathematics to be done and new continents to explore motivated by these challenges.”</em></p>



<h4>Acknowledgements </h4>



<p>We are thankful to Margalit Glasgow, Gautam Kamath, Praneeth Netrapalli, Arun Sai Suggala, and Manolis Zampetakis for providing valuable feedback on the blog. We would like to especially thank Costis Daskalakis for helpful conversations related to the technical and philosophical aspects of this work, and valuable comments throughout the writing of this article.</p>
</div></div>



<p><strong>Notes</strong></p>



<p id="fn1"><sup>1 </sup> As we discuss later, this existence only holds when the gains of each player are a concave function of their own actions.</p>



<p id="fn2"><sup>2</sup> This led to Nash winning the Nobel prize in Economics in 1994 with John Harsanyi and Reinhard Selten.</p>



<p id="fn3"><sup>3</sup> PPAD is the complexity class that exactly captures the complexity of computing Nash equilibria in general-sum games, computing fixed points of Lipschitz functions in convex and compact domains, and many other equilibrium and fixed point computation problems.</p>



<p id="fn4"><sup>4</sup> In Sperner, we are given white-box access to a circuit that computes colors for the vertices of some canonical simplicization of the <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/>-dimensional simplex. Each vertex receives one of the colors in <img alt="\{1,\ldots,d+1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C%5Cldots%2Cd%2B1%5C%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> and each color <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> does not appear on any vertex of the triangulation lying on facet <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002"/> of the simplex. The goal is to find a simplex of the triangulation with all vertices colored differently.</p>



<p><strong>Bibliography</strong></p>



<p id="DSZ21">[DSZ21] Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained min-max optimization. Symposium on Theory of Computing, 2021</p>



<p id="HPV89">[HPV89] Michael D Hirsch, Christos H Papadimitriou, and Stephen A Vavasis. Exponential lower bounds for finding brouwer fixed points. Journal of Complexity, 1989.</p>



<p id="NY83">[NY83] Arkadi S. Nemirovsky and David B. Yudin. Problem complexity and method efficiency in optimization. Wiley, 1983.</p>



<p id="Pap94">[Pap94] Christos H. Papadimitriou. On the complexity of the parity argument and other inefficient proofs of existence. Journal of Computer and System Sciences, 1994.</p>



<p id="Ros65">[Ros65] J. Ben Rosen. Existence and uniqueness of equilibrium points for concave n-person games. Econometrica, 1965.</p>



<p id="vN28">[vN28] John von Neumann. Zur Theorie der Gesellschaftsspiele. In Mathematische annalen, 1928.</p></div>
    </content>
    <updated>2021-04-30T14:05:03Z</updated>
    <published>2021-04-30T14:05:03Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Gautam</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-05-06T15:39:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=94</id>
    <link href="https://dstheory.wordpress.com/2021/04/29/thursday-may-6th-hamed-hassani-from-university-of-pennsylvania/" rel="alternate" type="text/html"/>
    <title>Thursday May 6th — Hamed Hassani  from University of Pennsylvania</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Thursday, May 6th at 11:00 AM Pacific Time (14:00 Eastern Time, 19:00 Central European Time, 18:00 UTC).  Hamed Hassani from Univeristy of Pennsylvania will speak about “Learning Robust Models: How does the Geometry of Perturbations Play a Role?” Please register here to join the<a class="more-link" href="https://dstheory.wordpress.com/2021/04/29/thursday-may-6th-hamed-hassani-from-university-of-pennsylvania/">Continue reading <span class="screen-reader-text">"Thursday May 6th — Hamed Hassani  from University of Pennsylvania"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="has-text-align-justify">The next <a href="https://sites.google.com/view/dstheory/home" rel="noreferrer noopener" target="_blank">Foundations of Data Science</a> virtual talk will take place on <strong>Thursday, May 6</strong>th at <strong>11:00 AM Pacific Time</strong> (14:00 Eastern Time, 19:00 Central European Time, 18:00 UTC).  <strong><a href="https://www.seas.upenn.edu/~hassani/index.html" rel="noreferrer noopener" target="_blank">Hamed Hassani</a></strong> from<strong> Univeristy of Pennsylvania</strong> will speak about “Learning Robust Models: How does the Geometry of Perturbations Play a Role?”</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: In this talk, we will focus on the emerging field of (adversarially) robust machine learning. The talk will be self-contained and no particular background on robust learning will be needed. Recent progress in this field has been accelerated by the observation that despite unprecedented performance on clean data, modern learning models remain fragile to seemingly innocuous changes such as small, norm-bounded additive perturbations.  Moreover, recent work in this field has looked beyond norm-bounded perturbations and has revealed that various other types of distributional shifts in the data can significantly degrade performance.  However, in general our understanding of such shifts is in its infancy and several key questions remain unaddressed.</p>



<p class="has-text-align-justify">The goal of this talk is to explain why robust learning paradigms have to be designed — and sometimes rethought — based on the geometry of the input perturbations.  We will cover a wide range of perturbation geometries from simple norm-bounded perturbations, to sparse, natural, and more general distribution shifts.  As we will show, the geometry of the perturbations necessitates fundamental modifications to the learning procedure as well as the architecture in order to ensure robustness. In the first part of the talk, we will discuss our recent theoretical results on robust learning with respect to various geometries, along with fundamental tradeoffs between robustness and accuracy, phase transitions, etc.  The remaining portion of the talk will be about developing practical robust training algorithms and evaluating the resulting (robust) deep networks against state-of-the-art methods on naturally-varying, real-world datasets.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2021-04-29T21:14:05Z</updated>
    <published>2021-04-29T21:14:05Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2021-05-06T15:41:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/061</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/061" rel="alternate" type="text/html"/>
    <title>TR21-061 |  Reflections on Proof Complexity and Counting Principles | 

	Noah Fleming, 

	Toniann Pitassi</title>
    <summary>This paper surveys the development of propositional proof complexity and the seminal contributions of Alasdair Urquhart. We focus on the central role of counting principles, and in particular Tseitin's graph tautologies, to most of the key advances in lower bounds in proof complexity. We reflect on a couple of key ideas that Urquhart pioneered: (i) graph expansion as a tool for distinguishing between easy and hard principles, and (ii) ``reductive" lower bound arguments, proving via a simulation theorem that an optimal proof cannot bypass the obvious (inefficient) one.</summary>
    <updated>2021-04-29T20:10:17Z</updated>
    <published>2021-04-29T20:10:17Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-06T15:37:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/04/28/how-good-greed</id>
    <link href="https://11011110.github.io/blog/2021/04/28/how-good-greed.html" rel="alternate" type="text/html"/>
    <title>How good is greed for the no-three-in-line problem?</title>
    <summary>The 37th European Workshop on Computational Geometry (EuroCG 2021) was earlier this month, but its book of abstracts remains online. This has an odd position in the world of academic publishing: the “abstracts” are really short papers, so it looks a lot like a published conference proceedings. However, it declares that you should really pretend that it’s not a proceedings, in order to allow the same work to go on to another conference with a published proceedings, getting around the usual prohibitions on double publication. Instead, its papers “should be considered a preprint rather than a formally reviewed paper”. But I think that doesn’t preclude citing them, with care, just as you might occasionally cite arXiv preprints. The workshop’s lack of peer review and selectivity is actually a useful feature, allowing it to act as an outlet for works that are too small or preliminary for publication elsewhere. In North America, the Canadian Conference on Computational Geometry performs much the same role, but does publish a proceedings; its submission deadline is rapidly approaching.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="http://eurocg21.spbu.ru/">37th European Workshop on Computational Geometry (EuroCG 2021)</a> was earlier this month, but its <a href="http://eurocg21.spbu.ru/wp-content/uploads/2021/04/proceedings.pdf">book of abstracts</a> remains online. This has an odd position in the world of academic publishing: the “abstracts” are really short papers, so it looks a lot like a published conference proceedings. However, it declares that you should really pretend that it’s not a proceedings, in order to allow the same work to go on to another conference with a published proceedings, getting around the usual prohibitions on double publication. Instead, its papers “should be considered a preprint rather than a formally reviewed paper”. But I think that doesn’t preclude citing them, with care, just as you might occasionally cite arXiv preprints. The workshop’s lack of peer review and selectivity is actually a useful feature, allowing it to act as an outlet for works that are too small or preliminary for publication elsewhere. In North America, the <a href="http://cccg.ca/">Canadian Conference on Computational Geometry</a> performs much the same role, but does publish a proceedings; its <a href="https://projects.cs.dal.ca/cccg2021/the-call-for-papers-is-out/">submission deadline</a> is rapidly approaching.</p>

<p>Anyway, one of the EuroCG not-really-a-published-paper things is mine: “Geometric dominating sets – A minimum version of the no-three-in-line problem”, with Oswin Aichholzer and Eva-Maria Hainzl. As the title suggests, it’s related to the <a href="https://en.wikipedia.org/wiki/No-three-in-line_problem">no-three-in-line problem</a>, in which one must place as many points as possible in a grid so that no three are collinear. I’ve written about the same problem here <a href="https://11011110.github.io/blog/2018/11/10/random-no-three.html">several</a> <a href="https://11011110.github.io/blog/2018/11/12/gurobi-vs-no.html">times</a> <a href="https://11011110.github.io/blog/2018/12/08/general-position-hypercube.html">already</a>. On an \(n\times n\) grid, there’s an easy upper bound of \(2n\) on the number of points, but it’s widely conjectured that the actual number is a smaller linear function of \(n\). It was a big step forward when Erdős showed that \(n\bigl(1-o(1)\bigr)\) points can be placed, and this was later improved to \(\tfrac{3}{2}n\bigl(1-o(1)\bigr)\).</p>

<p>These big no-three-in-line sets are constructed algebraically, but what if we try something simpler, a greedy algorithm that just adds points one by one (in a random or systematic order) until getting stuck? This question was already asked in the 1970s by Martin Gardner, and studied by several other authors since. But it is, if anything, even more frustratingly unknown than the no-three-in-line problem itself. We don’t know whether, in general, it’s possible to get stuck with fewer points than the maximum solution to the no-three-in-line problem, or even whether it’s possible to get stuck with fewer than \(2n\) points for infinitely many values of \(n\). For some values of \(n\) we do know smaller stuck solutions, though: for instance, here’s one with \(28\) points on a \(36\times 36\) grid.</p>

<p style="text-align: center;"><img alt="A 28-point greedy solution to the no-three-in-line problem on a 36x36 grid" src="https://11011110.github.io/blog/assets/2021/greedy-no3-36x36.svg"/></p>

<p>It was known that greedy solutions always have \(\Omega(\sqrt{n})\) points, and one of our main results is to improve this bound to \(\Omega(n^{2/3})\). The known \(\Omega(\sqrt{n})\) lower bound is easy to see: A single line through two selected points can cover at most \(n\) other grid points, so you need \(n\) lines to cover the whole grid, and you need \(\Omega(\sqrt{n})\) points to determine this many lines. With fewer points, there won’t be enough lines through your points to cover the whole grid, and your greedy solution won’t be stuck. Our new \(\Omega(n^{2/3})\) bound looks more carefully at the tradeoff between numbers of lines and numbers of points per line. It can be divided into two cases:</p>

<ul>
  <li>
    <p>Suppose, first, that the selected point set has the property that, for any selected point \(p\), the lines through \(p\) cover fewer than \(n^{4/3}\) grid points. Because each selected point covers few grid points, we need to select many points to cover the whole grid: at least \(\Omega(n^{2/3})\) points.</p>
  </li>
  <li>
    <p>Suppose on the other hand that the lines through some point \(p\) cover at least  \(n^{4/3}\) grid points. Parameterize these lines by the \(L_\infty\) distance to the closest grid point (regardless of whether that point is one of the selected ones). Then there are \(O(k)\) lines with parameter \(k\), each of which covers \(O(n/k)\) grid points. Summing over small values of \(k\) shows that, even if we use lines that cover as many grid points as possible, we need  \(\Omega(n^{2/3})\) lines through \(p\) to cover this many grid points. Each of these lines is determined by another selected point, so we need \(\Omega(n^{2/3})\) selected points.</p>
  </li>
</ul>

<p>The actual proof in the paper takes into account that not all the grid points near \(p\) are the nearest on their line, and does the summation over small values of \(k\) more carefully, to get more precise constant factors in the bounds. Our paper also includes another variation of the problem in which we allow our selected points to be collinear but require the lines through them to cover all unselected points. There, we can make a little progress: we show that \(n\) points, or in some cases slightly fewer than \(n\) points, are sufficient. The same \(\Omega(n^{2/3})\) lower bound is still valid for this case, but there’s still a big gap between the lower bound and the upper bound.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106146843522019241">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-04-28T18:23:00Z</updated>
    <published>2021-04-28T18:23:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-01T00:37:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18647</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/04/28/congrats-to-the-new-members/" rel="alternate" type="text/html"/>
    <title>Congrats to the New Members</title>
    <summary>If you know you are on the right track, if you have this inner knowledge, then nobody can turn you off no matter what they say—Barbara McClintock David Oxtoby is the president of AAAS. He just announced the 2021 new members. See the press release. Today I thought I would explain which choices are correct […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>If you know you are on the right track, if you have this inner knowledge, then nobody can turn you off <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> no matter what they say—Barbara McClintock</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/28/congrats-to-the-new-members/oxtobycropped/" rel="attachment wp-att-18671"><img alt="" class="alignright size-full wp-image-18671" height="150" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/oxtobyCropped.png?resize=121%2C150&amp;ssl=1" width="121"/></a></p>
<p>
David Oxtoby is the president of <a href="https://www.amacad.org/new-members-2021">AAAS</a>. He just announced the 2021 new members. See the <a href="https://www.amacad.org/news/2021-member-announcement">press release</a>.</p>
<p>
Today I thought I would explain which choices are correct and which are <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/>
<span id="more-18647"/></p>
<p>
Just kidding. Of course they all are terrific choices. Here are some choices in previous years—going back a little while: </p>
<blockquote><p><b> </b> <em> The new class joins Academy members elected before them, including Benjamin Franklin (elected 1781) and Alexander Hamilton (1791) in the eighteenth century; Ralph Waldo Emerson (1864), Maria Mitchell (1848), and Charles Darwin (1874) in the nineteenth; Albert Einstein (1924), Robert Frost (1931), Margaret Mead (1948), Groucho Marx (1951), Milton Friedman (1959), Martin Luther King, Jr. (1966), and Anthony Fauci (1991) in the twentieth. </em>
</p></blockquote>
<p>
</p><p/><h2> Computer Science </h2><p/>
<p/><p>
Here they are with some fun facts.</p>
<p>
<a href="https://en.wikipedia.org/wiki/Demis_Hassabis">Demis Hassabis</a> (International Honorary Member), DeepMind <br/>
Child chess prodigy: at the age of 13 had an Elo rating of 2300.</p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/28/congrats-to-the-new-members/dh/" rel="attachment wp-att-18657"><img alt="" class="aligncenter size-thumbnail wp-image-18657" height="150" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/dh-150x150.png?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
<a href="https://en.wikipedia.org/wiki/Charles_Lee_Isbell_Jr.">Charles Isbell</a>, Georgia Institute of Technology <br/>
Started the Black History Database. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/28/congrats-to-the-new-members/ci/" rel="attachment wp-att-18659"><img alt="" class="aligncenter size-thumbnail wp-image-18659" height="150" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ci-150x150.jpg?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
<a href="https://en.wikipedia.org/wiki/Fei-Fei_Li">Fei-Fei Li</a>, Stanford University <br/>
Helped create ImageNet. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/28/congrats-to-the-new-members/fl/" rel="attachment wp-att-18660"><img alt="" class="aligncenter size-thumbnail wp-image-18660" height="150" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/fl-150x150.png?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
<a href="https://www.linkedin.com/in/murielmedard/">Muriel Medard</a>, Massachusetts Institute of Technology <br/>
Co-founded multiple companies. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/28/congrats-to-the-new-members/mm-3/" rel="attachment wp-att-18661"><img alt="" class="aligncenter size-thumbnail wp-image-18661" height="150" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/mm-1-150x150.png?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
<a href="https://en.wikipedia.org/wiki/Stefan_Savage">Stefan Savage</a>, University of California, San Diego <br/>
Uses cool names for projects: Jetset, Trufflehunter, <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/28/congrats-to-the-new-members/ss/" rel="attachment wp-att-18666"><img alt="" class="aligncenter size-thumbnail wp-image-18666" height="150" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ss-150x150.png?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
<a href="https://en.wikipedia.org/wiki/Margo_Seltzer">Margo Seltzer</a>, University of British Columbia <br/>
Very busy: see her <a href="https://www.seltzer.com/margo/calendar/">calendar</a> </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/28/congrats-to-the-new-members/ms-2/" rel="attachment wp-att-18663"><img alt="" class="aligncenter size-thumbnail wp-image-18663" height="150" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ms-150x150.png?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
<a href="https://en.wikipedia.org/wiki/Daniel_Spielman">Daniel Spielman</a>, Yale University <br/>
Once held a professorship named for “Hank the Deuce” or “HF2”. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/28/congrats-to-the-new-members/ds/" rel="attachment wp-att-18664"><img alt="" class="aligncenter size-thumbnail wp-image-18664" height="150" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ds-150x150.png?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
[re-aligned photo at top]</p>
<p/><h2> Mathematics </h2><p/>
<p/><p>
<a href="https://en.wikipedia.org/wiki/Yakov_Eliashberg">Yakov Eliashberg</a>, Stanford University <br/>
<a href="https://en.wikipedia.org/wiki/Benson_Farb">Benson Farb</a>, University of Chicago <br/>
<a href="https://www.math.nyu.edu/faculty/masmoudi/">Nader Masmoudi</a>, New York University <br/>
<a href="https://en.wikipedia.org/wiki/Kavita_Ramanan">Kavita Ramanan</a>, Brown University <br/>
<a href="https://en.wikipedia.org/wiki/Scott_D._Sheffield">Scott Sheffield</a>, Massachusetts Institute of Technology <br/>
<a href="https://en.wikipedia.org/wiki/Karen_E._Smith">Karen Smith</a>, University of Michigan <br/>
<a href="https://en.wikipedia.org/wiki/Amie_Wilkinson">Amie Wilkinson</a>, University of Chicago </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Do you have better fun facts on the winners? Let us know if you do.</p>
<p/></font></font></div>
    </content>
    <updated>2021-04-28T16:20:07Z</updated>
    <published>2021-04-28T16:20:07Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="AAAS"/>
    <category term="Awards"/>
    <category term="congrats"/>
    <category term="new members"/>
    <category term="who"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-05-06T15:38:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8105</id>
    <link href="https://windowsontheory.org/2021/04/27/google-research-workshop-on-deep-learning-theory/" rel="alternate" type="text/html"/>
    <title>Google Research Workshop on Deep Learning Theory</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">[Guest post from Pranjal Awasthi and Rina Panigrahy – workshop looks great! –Boaz] Please join us for a virtual Google workshop on “Conceptual Understanding of Deep Learning”  When: May 17th 9am-4pm. Where: Live over Youtube, Goal: How does the Brain/Mind (perhaps even an artificial one) work at an algorithmic level? While deep learning has produced tremendous technological … <a class="more-link" href="https://windowsontheory.org/2021/04/27/google-research-workshop-on-deep-learning-theory/">Continue reading <span class="screen-reader-text">Google Research Workshop on Deep Learning Theory</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post from Pranjal Awasthi and Rina Panigrahy – workshop looks great! –Boaz]</em><br/></p>



<p>Please join us for a virtual Google workshop on “<a href="https://sites.google.com/view/conceptualdlworkshop/home" rel="noreferrer noopener" target="_blank">Conceptual Understanding of Deep Learning</a>” </p>



<p>When: May 17th 9am-4pm. Where: <a href="https://www.youtube.com/watch?v=g5DGBWjiULQ" rel="noreferrer noopener" target="_blank">Live over Youtube</a>,</p>



<p><strong>Goal: </strong>How does the Brain/Mind (perhaps even an artificial one) work at an algorithmic level? While deep learning has produced tremendous technological strides in recent decades, there is an unsettling feeling of a lack of “conceptual” understanding of why it works and to what extent it will work in the current form. The goal of the workshop is to bring together theorists and practitioners to develop an understanding of the right algorithmic view of deep learning, characterizing the class of functions that can be learned, coming up with the right learning architecture that may (provably) learn multiple functions, concepts and remember them over time as humans do, theoretical understanding of language, logic, RL, meta learning and lifelong learning.</p>



<p>The speakers and panelists include Turing award winners Geoffrey Hinton, Leslie Valiant, and Godel Prize winner Christos Papadimitriou (<a href="https://sites.google.com/corp/view/conceptualdlworkshop/home" rel="noreferrer noopener" target="_blank">full-details</a>).   </p>



<p><strong>Panel Discussion: </strong>There will also be a panel discussion on the fundamental question of “Is there a mathematical model for the Mind?”. We will explore basic questions such as “Is there a provable algorithm that captures the essential capabilities of the mind?”, “How do we remember complex phenomena?”, “How is a knowledge graph created automatically?”, “How do we learn new concepts, function and action hierarchies over time?” and “Why do human decisions seem so interpretable?”<br/><br/>Twitter:<a href="https://twitter.com/search?q=%23ConceptualDLWorkshop&amp;src=recent_search_click" rel="noreferrer noopener" target="_blank"> #ConceptualDLWorkshop</a>. Please help advertise on mailing-lists/blog-posts and <a href="https://twitter.com/rinapy/status/1384311169519788032" rel="noreferrer noopener" target="_blank">Retweet</a>.<br/>Hope to see you there!</p>



<p>Rina Panigrahy<br/>(<a href="http://theory.stanford.edu/~rinap" rel="noreferrer noopener" target="_blank">http://theory.stanford.edu/~rinap</a>)</p></div>
    </content>
    <updated>2021-04-27T18:55:31Z</updated>
    <published>2021-04-27T18:55:31Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-05-06T15:39:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/alt-highlights/</id>
    <link href="https://differentialprivacy.org/alt-highlights/" rel="alternate" type="text/html"/>
    <title>ALT Highlights - An Equivalence between Private Learning and Online Learning (ALT '21 Tutorial)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! 
To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. 
Given the topic of this post, we felt <a href="https://differentialprivacy.org/">DifferentialPrivacy.org</a> was a great fit.
This initiative is organized by the <a href="https://www.let-all.com/">Learning Theory Alliance</a>, and overseen by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. 
All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Learning Theory Alliance blog</a>.</p>

<p>The second post is coverage of <a href="http://www.cs.technion.ac.il/~shaymrn">Shay Moran</a>’s <a href="https://www.youtube.com/watch?v=wk910Aj559A">tutorial</a>, by <a href="https://people.eecs.berkeley.edu/~kush/">Kush Bhatia</a> and <a href="https://web.stanford.edu/~mglasgow/">Margalit Glasgow</a>.</p>

<hr/>

<p>The tutorial at ALT was given by <a href="http://www.cs.technion.ac.il/~shaymrn/">Shay Moran</a>, an assistant professor at the Technion in Haifa.
His <a href="https://www.youtube.com/watch?v=wk910Aj559A">talk</a> focused on recent results showing a deep connection between two important areas in learning theory: online learning and differentially private learning. 
While online learning is a well-established area that has been studied since the invention of the Perceptron algorithm in 1954, differential privacy (DP) - introduced in the seminal work of Dwork, McSherry, Nissim, and Smith in 2006 <a href="https://journalprivacyconfidentiality.org/index.php/jpc/article/view/405"><strong>[DMNS06]</strong></a> - has received increasing attention in recent years from both theoretical and applied research communities along with industry and the government. 
This recent interest in differential privacy comes from a need to protect the privacy rights of the individuals, while still allowing one to derive useful conclusions from the dataset.
Shay, in a sequence of papers with co-authors Noga Alon, Mark Bun, Roi Livni, and Maryanthe Malliaris, revealed a surprising qualitative connection between these two models of learning: a concept class can be learned in an online fashion if and only if this concept class can be learned in an offline fashion by a differentially private algorithm.</p>

<p>The main objectives of this tutorial were to give an in-depth foray into this recent work, and present an opportunity to young researchers to identify interesting research problems at the intersection of these two fields. This line of work (<a href="https://arxiv.org/abs/1806.00949"><strong>[ALMM19]</strong></a>, <a href="https://arxiv.org/abs/2003.00563"><strong>[BLM20]</strong></a>) - which has been primarily featured in general CS Theory conferences, including recently winning a best paper award at FOCS -  introduced several new techniques which could be of use to the machine learning theory community. The first part of this article focuses on the technical challenges of characterizing DP learnability and the solutions used in Shay’s work, which originate from combinatorics and model theory. In the rest of this article, we highlight some of the exciting open directions Shay sees more broadly in learning theory.</p>

<h2 id="background-pac-learning-online-learning-and-dp-pac-learning">Background: PAC Learning, Online Learning, and DP PAC Learning</h2>

<p>We begin by reviewing the classical setting of PAC learning. 
The goal of PAC learning is to learn some function from a <em>concept class</em> \(\mathcal{H} \), a set of functions from some domain \( \mathcal{X} \) to \( \{0, 1\} \). 
<img align="right" src="https://differentialprivacy.org/images/PACLearning.png" style="width: 300px; height: 300px;"/> 
In the realizable setting of PAC learning, which we will focus on here, the learner is presented with \( n \) labeled training samples \( \{(x_i, y_i)\}_{i=1}^{n} \) where \( x_i \in \mathcal{X} \) and \( y_i = h(x_i) \) for some function \( h \in \mathcal{H} \). While the learner knows the concept class \( \mathcal{H} \), the function \( h \) is unknown, and after seeing the \( n \) samples, the learner algorithm \( A \) must output some function \( \hat{h}: \mathcal{X} \rightarrow \{0, 1\} \). A concept class is <em>PAC-learnable</em> if there exists an algorithm \( A \) such that for any distribution over samples, as the number of labeled training samples goes to infinity, the probability that \( \hat{h} \) incorrectly labels a new random sample goes to 0. We will say that \( A \) is a <em>proper</em> learner if \( A \) outputs a function in \(\mathcal{H}\), while \( A \) is <em>improper</em> if it may output a function outside of \( \mathcal{H} \). More quantitative measures of learnability concern the exact number of samples \( n \) needed for the learner to correctly predict future labels with nontrivial probability.</p>

<p>DP PAC learning imposes an additional restriction on PAC learning: the learner must output a function which does not reveal too much about any one sample in the input. Formally, we say an algorithm \( A \) is \( (\varepsilon, \delta) \)-DP if for any two neighboring inputs \( X = (X_1, \cdots,  X_i, \cdots, X_n) \) and \( X’ = (X_1, \cdots, X_i’,\cdots, X_n) \) which differ at exactly one sample, for any set \( S \), \( \Pr[A(X) \in S] \leq e^\varepsilon Pr[A(X’) \in S] + \delta \) . A concept class is <em>DP PAC learnable</em> if it can be PAC-learned by a \( (0.1, o(1/n)) \)-DP algorithm \( A \). That is, changing any one training sample \( (x_i, y_i) \) should not affect the distribution over concepts output by \( A \) by too much.</p>

<p>Online learning considers a setting where the samples arrive one-by-one, and the learner must make predictions as this process unfolds. <img align="right" src="https://differentialprivacy.org/images/OnlineLearning.png" style="width: 300px; height: 300px;"/>Formally, in realizable online learning, at each round \( i = 1,\dots,T \), the learner is presented with a sample \( x_i\). The learner must predict the label, and then the true label \( y_i \) is revealed to the learner. The sequence of examples \( x_i \) and the labels \( y_i \) may be chosen adversarially, but they must be consistent with some function \( h \in \mathcal{H} \). The goal of the learner is to minimize the total number of mistakes made by round \( T \), also called the <em>mistake bound</em>. If there is a learner such that at \( T \rightarrow \infty \), the number of mistakes is \( o(T) \), we say that the class of functions \( \mathcal{H} \) is <em>online learnable</em>. Because of the adversarial nature of the examples, online learning is well known to be much harder than PAC learning, and is possible precisely when the <em>Littlestone Dimension</em> of the concept class is finite. Figure 1 below illustrates the definition of the Littlestone Dimension. One important PAC-learnable concept class which is not online learnable is the infinite class of thresholds on \( \mathbb{R} \): The set of functions \(\{h_t\}_{t \in \mathbb{R}} \) where \( h_t(x) = \textbf{1}(x &gt; t) \).</p>

<p><img alt="Figure 1: Littlestone Dimension" src="https://differentialprivacy.org/images/LD.png" title="Figure 1: Littlestone Dimension"/></p>

<p><strong>Figure 1</strong>: The Littlestone dimension is the maximum depth of a tree shattered by \( \mathcal{H} \), where each node may contain a unique element from \(\mathcal{X}\). The tree is shattered if each leaf can be labeled by a function in \(\mathcal{H} \) that labels each element on the path to the root according to the value of the edge entering it. In this figure, we show how the set of thresholds on \( [0, 1] \) shatters this tree of depth 3.</p>

<p>It’s worth taking a moment to understand intuitively why learning thresholds might be hard for both an online learner and a DP learner. In an online setting, suppose the adversary chooses the next example to be any value of \( x \) in between all previously 0-labeled examples and all 1-labeled examples. Then no matter what label \( A \) chooses, the adversary can say \( A \) was incorrect. In a DP setting, a simple proper learner that outputs a threshold that makes no errors on the training data will reveal too much information about the samples at the boundary of 0-labeled samples and 1-labeled samples.</p>

<h2 id="a-challenging-question">A Challenging Question</h2>

<p>The journey to characterizing DP PAC learnability started soon after the introduction of DP, in <a href="https://arxiv.org/abs/0803.0924"><strong>[KLNRS08]</strong></a>, where the concept of DP PAC learning was introduced. This work primarily considered the more stringent <em>pure</em> DP-learning, where \( \delta = 0\). This work established that any finite concept class could be learned privately by applying the <em>exponential mechanism</em> (a standard technique in DP) to the empirical error of each candidate concept. Using the exponential mechanism, the learner could output each function \( h \in \mathcal{H}\) with probability proportional to \( \exp(- \varepsilon(\# \text{ of errors h on the training data})) \). This was sufficiently private because the empirical error is not too sensitive to a change of one training sample. Later works <a href="https://arxiv.org/abs/1402.2224"><strong>[BNS14]</strong></a> combinatorially characterized the complexity of pure DP PAC learning in terms of a measure called <em>representation dimension</em>, showing that in some cases where PAC learning was possible, pure DP PAC learning was not. <a href="https://arxiv.org/abs/1504.07553"><strong>[BNSV15]</strong></a> further yielded lower bounds on the limits of proper DP PAC learning.<sup id="fnref:1"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:1" rel="footnote">1</a></sup><br/>
Both pure and proper DP learning though are significantly more stringent than improper DP learning, and proving lower bounds against an improper DP learner posed a serious challenge. Even the following simple-sounding question was unsolved:</p>

<blockquote>
  <p>Can an improper DP algorithm learn the infinite class of thresholds over [0, 1]? (*)</p>
</blockquote>

<p>This question stands at the center of Shay’s work, which unfolded while Shay was residing at the Institute for Advanced Study (IAS) at Princeton from 2017 to 2019.<sup id="fnref:2"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:2" rel="footnote">2</a></sup> At the time, Shay was working on understanding the expressivity of limited mutual-information algorithms, that is, algorithms which expose little information about the total input. “If we were to have directly worked on this problem, I believe we wouldn’t have solved it,” Shay says. Instead, they came from the angle of mutual-information, a concept qualitatively similar to DP, but armed with a rich toolkit from 70 years of information theory. One of Shay’s prior works with Raef Bassily, Ido Nachum, Jonathan Shafer, and Amir Yehudayof established lower bounds on the mutual information of any proper algorithm that learns thresholds <a href="https://arxiv.org/abs/1710.05233"><strong>[BMNSY18]</strong></a>, though this didn’t yet address the challenge presented by improper learners.</p>

<p>Unlike most lower bounds in theoretical computer science, proving hardness of learning the infinite class of thresholds on the line against an improper DP algorithm would require coming up with algorithm-specific distributions over samples. That is, instead of showing that one distribution over samples would be impossible to learn for all algorithms — in the way the a uniform distribution over the set in \(\mathcal{X}\) shattered by the concept class is hard to PAC-learn for any algorithm — they would have to come up with a distribution on \(\mathcal{X}\) specific to each candidate learning algorithm. Indeed, if the distribution was known to the learner, it was possible to devise a DP algorithm using the exponential mechanism (again!) which could learn any PAC-learnable concept class. Similar to the case of finite concept classes, here we can apply the exponential mechanism to some finite set of representative functions forming a cover of the concept class.</p>

<h2 id="uncovering-a-solution">Uncovering a Solution</h2>

<p>At the IAS, Shay met his initial team to tackle this obstacle: Noga Alon, a combinatorialist, Roi Livni, a learning theorist, and Maryanthe Malliaris, a model theorist. Shay and Maryanthe would walk together from IAS to their combinatorics class at Princeton taught by Noga and discuss mathematics. While Marayathe studied the abstract mathematical field of model theory, Shay and Maryanthe soon noticed a connection between model theory and machine learning: the Littlestone dimension. “There is applied math, then pure math, and then model theory is way over there,” Shay elaborates. “If theoretical machine learning is between applied math and pure math, model theory is on the other extreme.” This surprising interdisciplinary connection led them to use a result from model theory: a concept class had finite Littlestone Dimension precisely when the concept class had finite threshold dimension (formally, the maximum number of threshold functions that could be embedded in the class). This meant that answering (*) negatively was enough to show that DP PAC learning was as hard as online learning.</p>

<p>The idea for showing a lower bound for improper DP learning of thresholds came from Ramsey Theory, a famous area in combinatorics. Ramsey theory guarantees the existence of structured subsets among large, but arbitrary, combinatorial objects. A toy example of Ramsey Theory is that in any graph on 6 or more nodes, there must be a group of 3 nodes which form either a clique or an independent set. In the case of DP learning thresholds, the learning algorithm \( A \) is the arbitrary (and unstructured) combinatorial object. Ramsey Theory guarantees that for any PAC learner \( A \), there exists some large subset \( \mathcal{X}’ \) of the domain \(\mathcal{X}=[0, 1]\) on which \( A \) is close to behaving “normally”. The next step is to show that behaving normally contradicts differentially privacy. We’ll dig more technically into this argument in the next couple paragraphs. Note that we invent a couple definitions for the sake of exposition (“proper-normal” and “improper-normal”) that don’t appear in <a href="https://arxiv.org/abs/1504.07553"><strong>[BNSV15]</strong></a> or <a href="https://arxiv.org/abs/1806.00949"><strong>[ALMM19]</strong></a>.</p>

<p>Let’s start by seeing how a simpler version of this argument from <a href="https://arxiv.org/abs/1504.07553"><strong>[BNSV15]</strong></a> works to show that proper DP algorithms cannot learn thresholds. Recall that in a proper algorithm, after seeing \( n \) labeled samples, \( A \) must output a threshold. To be a PAC learner, if exactly half of the \( n \) samples are labeled \( 1 \) (which we will call a <em>balanced</em> sample), \( A \) must output a threshold between the smallest and largest sample with constant probability. Indeed, otherwise the empirical error of \( A \) will be too large to even hope to generalize. This implies that for a balanced list of samples \( S = [(x_1, 0), \ldots ,(x_{n/2}, 0), (x_{n/2+1}, 1), \ldots ,(x_n, 1)] \) (ordered by \(x\)-value), there must exist an integer \( k \in [n] \) for which with probability \( \Omega(1/n) \), \(A \) outputs a threshold in between 
\( x_k \) and \( x_{k+1} \). We’ll say that \( A \) is <em>\(k\)-proper-normal</em> on a set \( \mathcal{X}’ \) if for any balanced sample \( S \) of \( n \) points in \( \mathcal{X}’ \), \( A \) outputs a threshold in between the \(k\)th and \(k+1\)th ordered samples with probability \( \Omega(1/n) \). For example, the naive algorithm that always outputs a threshold between the 0-labeled samples and 1-labeled samples is \(n/2\)-normal on the entire domain \([0, 1]\). Ramsey theory guarantees that there is an arbitrary large subset of the domain \([0, 1]\) on which \(A \) is \(k\)-proper-normal for some \(k\). (See Figure 2).</p>

<p><img alt="Figure 2: Ramsey's Theorem" src="https://differentialprivacy.org/images/RamseyTheorem.png" title="Figure 2: Ramsey's Theorem"/>
<strong>Figure 2</strong>: Ramsey’s Theorem guarantees a large subset \( \mathcal{X}’ \) on which \( A \) is \( k \)-proper-normal for some \( k \).</p>

<p>The second part of the argument shows that being \( k \)-proper-normal on a large set is in direct conflict with differential privacy. We show this argument in Figure 3 by constructing a set of \( n \) samples \( S^* \) on which \( A \) must output a threshold in many distinct regions with substantial probability.</p>

<p><img alt="Figure 3: A Hard Distribution" src="https://differentialprivacy.org/images/DP_Construction.png" title="Figure 2: Ramsey's Theorem"/></p>

<p><strong>Figure 3:</strong> A distribution showing the conflict between \(A \) being DP and \(k\)-proper normal. By DP, the behaviour of \(A\) on \(S^* \) must be similar to its behaviour on \(S_i\) for \(i = 1 \ldots \Omega(n).\) Namely, since \(S^* \) and \( S_i \) differ by at most two points, \( A(S^*) \) must output a threshold in \( I_i \) with probability \( p \geq (q - 2\delta)e^{-(2\varepsilon)} \) for each \( i \), where \( q \) is a lower bound on the probability that \( A(S_i) \) outputs a threshold in \( I_i \). Because \( A \) is \(k\)-proper-normal, \( q &gt; c/n \), so \( p = \Omega(1/n) \). This yields a contradiction for \( m &gt;1/p \) because \( A \) cannot simultaneously output a threshold in two of the intervals \( I_i \).</p>

<p>For the case of improper algorithms, Shay and his coauthors considered an alternative notion of normality, which we will term <em>improper-normal</em>. Recall that in this case the output \( A(S) \) of the learner on a sample \( S \) is <em>any function</em> from \( [0, 1] \) to \( \{0, 1\} \) and not necessarly a threshold. We’ll say that \( A \) is \(k\)-improper-normal on a set \( \mathcal{X}’ \) if for any balanced sample \( S = [(x_1, 0), \ldots, (x_{n/2}, 0), (x_{n/2 +1}, 1), \ldots,  (x_n, 1)] \) of \( n \) points in \( \mathcal{X}’, \Pr[A(S)(w) = 1] - \Pr[A(S)(v) = 1] &gt; \Omega(1/n) \) for any \( w \in (x_{k - 1}, x_k) \) and \( v \in  (x_k, x_{k + 1}) \) in \( \mathcal{X}’ \setminus \{x_1, … x_n\} \). To PAC learn, A must be \(k\)-improper-normal for some \(k\) on any set of \(n + 1\) points. Applying the same Ramsey Theorem to a graph with colored hyperedges of size \(n + 1\) shows that there must exist an arbitrarily large set \( \mathcal{X}’ \) on which \( A \) is \(k\)-improper normal for some \( k \). A similar (but more nuanced) argument as before shows that a learner cannot be simultaneously private and \(k\)-improper-normal on some distribution over \( \mathcal{X}’ \).</p>

<p>For the last piece of the puzzle, showing the upper bound converse, Mark Bun, an expert in differential privacy at Princeton at the time, joined in. Beginning in the fall of 2019, Mark, Shay, and Roi worked out an upper bound that showed that any class with finite Littlestone dimension could be learned privately. Their technique introduced a new notion of stability, called <em>global stability</em>, which is a property of algorithms that frequently output the same hypothesis. Given a globally stable PAC learner \( A \), to obtain a DP PAC learner, one can run \( A \) many times and produce a histogram of the output hypotheses, add noise to this histogram for the sake of privacy, and then select the most frequent output hypothesis. The construction for the globally stable learner uses the Standard Optimal Algorithm for online learning as a black box - though the reduction is very computationally intensive and results in a sample complexity depending exponentially on the Littlestone Dimension. This reduction was improved in <a href="https://arxiv.org/abs/2012.03893"><strong>[GGKM20]</strong></a>, where the authors gave a reduction requiring only polynomially many samples in the Littlestone dimension.</p>

<h2 id="outlook">Outlook</h2>

<p>Shay mentions that these results are only a first step towards understanding differentially private PAC learning. This work establishes a deep connection between online learning and DP learning, qualitatively showing that these two problems have similar underlying complexity. Recent works <a href="https://arxiv.org/abs/1905.11311"><strong>[GHM19]</strong></a> have gone a step further and established polynomial time reductions from DP learning to online learning under certain conditions. At the same time, Bun <a href="https://arxiv.org/abs/2007.05665"><strong>[B20]</strong></a> demonstrates a computational gap between the two problems: they exhibit a concept class which is DP PAC learnable in polynomial time, but no algorithm can learn it online in polynomial time and sample complexity.</p>

<p>An interesting question here is whether a polynomial time online learning algorithm implies a polynomial time DP learning algorithm? With regards to sample complexity, tighter quantitative bounds relating the sample complexity of DP learning to the Threshold dimension and the Littlestone Dimension, along with constructive reductions from DP learning to online learning are wide open. An interesting conjecture, also highlighted in the talk, is whether DP PAC learning and PAC learning are actually equivalent up to a \(log*\) factor of the Littlestone dimension? Solving this would mean that for most natural function classes, one need not pay a very high price for private learning as compared to PAC learning.</p>

<p>From a more practical perspective, studying such qualitative equivalences can lay the groundwork allowing one to use the vast existing knowledge in the field of online learning to design better algorithms for DP learning, and vice versa. Despite its abstractness, Shay believes this result still has significance for engineers: “If they want to do something differentially privately, if they already have a good online learning algorithm for this, maybe they can modify it,” Shay says. “It gives some kind of inspiration.”</p>

<p>From a broader perspective, Shay believes that discovering clean, beautiful mathematical models that are more realistic than the PAC learning model and understanding the price one pays for privacy in those models are important directions for future research. One model, highlighted in the tutorial by Shay is that of <em>Universal Learning</em>, introduced in a recent work <a href="https://arxiv.org/abs/2011.04483"><strong>[BHMVY21]</strong></a> with Olivier Bousquet, Steve Hanneke, Ramon van Handel and Amir Yehudayoff. This model considers a learning task with a fixed distribution of samples and studies the hardness of the problem as the number of samples \( n \to \infty \). This setup better captures the practical aspects of modern machine learning and overcomes the limitations of the PAC model, which studies worst-case distributions for each sample size.</p>

<p>And how should one go about identifying such better mathematical models? “Pick the simplest problem that you don’t know how to solve and see where that leads you”, says Shay. One should begin by trying to understand the deficiencies of existing learning models, by identifying simple examples which go beyond these existing models. For example, Livni and Moran <a href="https://arxiv.org/abs/2006.13508"><strong>[LM20]</strong></a> exhibit the limitations of PAC-Bayes framework through a simple 1D linear classification problem. Fixing such limitations can often lead one to discover better learning models.</p>

<hr/>
<p><em>Thanks to Gautam Kamath, Shay Moran, and Keziah Naggita for helpful conversations and comments.</em></p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For a more complete background on the progress in DP PAC learning, we refer the reader to the excellent survey blog post <a href="https://differentialprivacy.org/private-pac/">here</a>. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>At the time, Shay was additionally affiliated with Princeton University and Google Brain. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:2">↩</a></p>
    </li>
  </ol>
</div></div>
    </summary>
    <updated>2021-04-26T16:00:00Z</updated>
    <published>2021-04-26T16:00:00Z</published>
    <author>
      <name>Margalit Glasgow</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-05-05T22:58:13Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-9152128988613149804</id>
    <link href="https://blog.computationalcomplexity.org/feeds/9152128988613149804/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/ferrers-diagrams-can-be-used-to-prove-x.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/9152128988613149804" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/9152128988613149804" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/ferrers-diagrams-can-be-used-to-prove-x.html" rel="alternate" type="text/html"/>
    <title>Ferrer's Diagrams can be used to prove X theorems about partitions. What is X?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>1978: I took an excellent  ugrad course in combinatorics from James C Frauenthal (he sometimes wrote his name as the biniomial cofficient (J choose F))  and he covered Ferrer's diagrams. They are a nice way to prove equalities about types of partitions.   See <a href="https://www.britannica.com/science/combinatorics/The-Ferrer-diagram">here</a> for a definition and a few examples. I have this (possibly false) memory that there were LOTS of partition theorems proven nicely with Ferrer's diagrams.</p><p>Fast forward to 2021:</p><p>2021: My TA Emily  needs a topic to cover in Honors Discrete Math. I have this memory that there were LOTS of theorems about partitions proven with Ferrer's diagrams. We look at many websites on Ferrer diagrams  and  find only TWO examples:</p><p>The numb of partitions of n into k parts is the numb of partitions of n into parts the largest of which is k.</p><p><br/></p><p>The numb of partitions of n into \le k parts is the numb of partitions of n into parts the largest of which is \le k</p><p>We DO find many theorems about partitions such as this corollary to the Rogers-Ramanujan theorem:</p><p>The numb of partitions of n such that adjacent parts differ by at least 2 is the numb of partitions of n such that each partition is either \equiv 1 mod 5 or \equiv 4 mod 5.</p><p>This is a HARD theorem and there is no Ferrer-diagram or other elementary proof. </p><p>SO, I have one MEMORY but the reality seems different. Possibilities:</p><p>1) My memory is wrong. There really are only 2 examples (or some very small number).</p><p>2) There are other examples but I can't find them on the web. I HOPE this is true--- if someone knows of other ways to use Ferrer diagrams to get partition results, please comment. </p><p><br/></p></div>
    </content>
    <updated>2021-04-26T02:01:00Z</updated>
    <published>2021-04-26T02:01:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-06T14:29:25Z</updated>
    </source>
  </entry>
</feed>
