<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-07-06T21:21:51Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02513</id>
    <link href="http://arxiv.org/abs/1907.02513" rel="alternate" type="text/html"/>
    <title>Locally Private k-Means Clustering</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stemmer:Uri.html">Uri Stemmer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02513">PDF</a><br/><b>Abstract: </b>We design a new algorithm for the Euclidean $k$-means problem that operates
in the local model of differential privacy. Unlike in the non-private
literature, differentially private algorithms for the $k$-means incur both
additive and multiplicative errors. Our algorithm significantly reduces the
additive error while keeping the multiplicative error the same as in previous
state-of-the-art results. Specifically, on a database of size $n$, our
algorithm guarantees $O(1)$ multiplicative error and $\approx n^{1/2+a}$
additive error for an arbitrarily small constant $a$, whereas all previous
algorithms in the local model on had additive error $\approx n^{2/3+a}$.
</p>
<p>We give a simple lower bound showing that additive error of $\approx\sqrt{n}$
is necessary for $k$-means algorithms in the local model (at least for
algorithms with a constant number of interaction rounds, which is the setting
we consider in this paper).
</p></div>
    </summary>
    <updated>2019-07-05T23:28:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02369</id>
    <link href="http://arxiv.org/abs/1907.02369" rel="alternate" type="text/html"/>
    <title>Expansion Testing using Quantum Fast-Forwarding and Seed Sets</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Apers:Simon.html">Simon Apers</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02369">PDF</a><br/><b>Abstract: </b>Expansion testing aims to decide whether an $n$-node graph has expansion at
least $\Phi$, or is far from any such graph. We propose a quantum expansion
tester with complexity $\widetilde{O}(n^{1/3}\Phi^{-1})$. This accelerates the
$\widetilde{O}(n^{1/2}\Phi^{-2})$ classical tester by Goldreich and Ron
[Algorithmica '02], and combines the $\widetilde{O}(n^{1/3}\Phi^{-2})$ and
$\widetilde{O}(n^{1/2}\Phi^{-1})$ quantum speedups by Ambainis, Childs and Liu
[RANDOM '11] and Apers and Sarlette [QIC '19], respectively. The latter
approach builds on a quantum fast-forwarding scheme, which we improve upon by
initially growing a seed set in the graph. To grow this seed set we borrow a
so-called evolving set process from the graph clustering literature, which
allows to grow an appropriately local seed set.
</p></div>
    </summary>
    <updated>2019-07-05T23:25:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02353</id>
    <link href="http://arxiv.org/abs/1907.02353" rel="alternate" type="text/html"/>
    <title>Fixed-parameter tractability of counting small minimum $(S,T)$-cuts</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berg=eacute=:Pierre.html">Pierre Bergé</a>, Benjamin Mouscadet, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rimmel:Arpad.html">Arpad Rimmel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tomasik:Joanna.html">Joanna Tomasik</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02353">PDF</a><br/><b>Abstract: </b>The parameterized complexity of counting minimum cuts stands as a natural
question because Ball and Provan showed its #P-completeness. For any undirected
graph $G=(V,E)$ and two disjoint sets of its vertices $S,T$, we design a
fixed-parameter tractable algorithm which counts minimum edge $(S,T)$-cuts
parameterized by their size $p$. Our algorithm operates on a transformed graph
instance. This transformation, called drainage, reveals a collection of at most
$n=\left| V \right|$ successive minimum $(S,T)$-cuts $Z_i$. We prove that any
minimum $(S,T)$-cut $X$ contains edges of at least one cut $Z_i$. This
observation, together with Menger's theorem, allows us to build the algorithm
counting all minimum $(S,T)$-cuts with running time $2^{O(p^2)}n^{O(1)}$.
Initially dedicated to counting minimum cuts, it can be modified to obtain an
FPT sampling of minimum edge $(S,T)$-cuts.
</p></div>
    </summary>
    <updated>2019-07-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02320</id>
    <link href="http://arxiv.org/abs/1907.02320" rel="alternate" type="text/html"/>
    <title>Optimal transport on large networks a practitioner guide</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Charpentier:Arthur.html">Arthur Charpentier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Galichon:Alfred.html">Alfred Galichon</a>, Lucas Vernet <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02320">PDF</a><br/><b>Abstract: </b>This article presents a set of tools for the modeling of a spatial allocation
problem in a large geographic market and gives examples of applications. In our
settings, the market is described by a network that maps the cost of travel
between each pair of adjacent locations. Two types of agents are located at the
nodes of this network. The buyers choose the most competitive sellers depending
on their prices and the cost to reach them. Their utility is assumed additive
in both these quantities. Each seller, taking as given other sellers prices,
sets her own price to have a demand equal to the one we observed. We give a
linear programming formulation for the equilibrium conditions. After formally
introducing our model we apply it on two examples: prices offered by petrol
stations and quality of services provided by maternity wards. These examples
illustrate the applicability of our model to aggregate demand, rank prices and
estimate cost structure over the network. We insist on the possibility of
applications to large scale data sets using modern linear programming solvers
such as Gurobi. In addition to this paper we released a R toolbox to implement
our results and an online tutorial (<a href="http://optimalnetwork.github.io">this http URL</a>)
</p></div>
    </summary>
    <updated>2019-07-05T23:30:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02319</id>
    <link href="http://arxiv.org/abs/1907.02319" rel="alternate" type="text/html"/>
    <title>The Complexity of Approximately Counting Retractions to Square-Free Graphs</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Focke:Jacob.html">Jacob Focke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldberg:Leslie_Ann.html">Leslie Ann Goldberg</a>, Stanislav Živný <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02319">PDF</a><br/><b>Abstract: </b>A retraction is a homomorphism from a graph $G$ to an induced subgraph $H$ of
$G$ that is the identity on $H$. In a long line of research, retractions have
been studied under various algorithmic settings. Recently, the problem of
approximately counting retractions was considered. We give a complete
trichotomy for the complexity of approximately counting retractions to all
square-free graphs (graphs that do not contain a cycle of length $4$). It turns
out there is a rich and interesting class of graphs for which this problem is
complete in the class $\#\mathrm{BIS}$. As retractions generalise
homomorphisms, our easiness results extend to the important problem of
approximately counting homomorphisms. By giving new $\#\mathrm{BIS}$-easiness
results we now settle the complexity of approximately counting homomorphisms
for a whole class of non-trivial graphs which were previously unresolved.
</p></div>
    </summary>
    <updated>2019-07-05T23:23:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02308</id>
    <link href="http://arxiv.org/abs/1907.02308" rel="alternate" type="text/html"/>
    <title>The Alternating BWT: an algorithmic perspective</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giancarlo:Raffaele.html">Raffaele Giancarlo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manzini:Giovanni.html">Giovanni Manzini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Restivo:Antonio.html">Antonio Restivo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosone:Giovanna.html">Giovanna Rosone</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sciortino:Marinella.html">Marinella Sciortino</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02308">PDF</a><br/><b>Abstract: </b>The Burrows-Wheeler Transform (BWT) is a word transformation introduced in
1994 for Data Compression. It has become a fundamental tool for designing
self-indexing data structures, with important applications in several area in
science and engineering. The Alternating Burrows-Wheeler Transform (ABWT) is
another transformation recently introduced in [Gessel et al. 2012] and studied
in the field of Combinatorics on Words. It is analogous to the BWT, except that
it uses an alternating lexicographical order instead of the usual one. Building
on results in [Giancarlo et al. 2018], where we have shown that BWT and ABWT
are part of a larger class of reversible transformations, here we provide a
combinatorial and algorithmic study of the novel transform ABWT. We establish a
deep analogy between BWT and ABWT by proving they are the only ones in the
above mentioned class to be rank-invertible, a novel notion guaranteeing
efficient invertibility. In addition, we show that the backward-search
procedure can be efficiently generalized to the ABWT; this result implies that
also the ABWT can be used as a basis for efficient compressed full text
indices. Finally, we prove that the ABWT can be efficiently computed by using a
combination of the Difference Cover suffix sorting algorithm
[K\"{a}rkk\"{a}inen et al., 2006] with a linear time algorithm for finding the
minimal cyclic rotation of a word with respect to the alternating
lexicographical order.
</p></div>
    </summary>
    <updated>2019-07-05T23:25:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02274</id>
    <link href="http://arxiv.org/abs/1907.02274" rel="alternate" type="text/html"/>
    <title>Min-Cost Flow in Unit-Capacity Planar Graphs</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karczmarz:Adam.html">Adam Karczmarz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sankowski:Piotr.html">Piotr Sankowski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02274">PDF</a><br/><b>Abstract: </b>In this paper we give an $\widetilde{O}((nm)^{2/3}\log C)$ time algorithm for
computing min-cost flow (or min-cost circulation) in unit capacity planar
multigraphs where edge costs are integers bounded by $C$. For planar
multigraphs, this improves upon the best known algorithms for general graphs:
the $\widetilde{O}(m^{10/7}\log C)$ time algorithm of Cohen et al. [SODA 2017],
the $O(m^{3/2}\log(nC))$ time algorithm of Gabow and Tarjan [SIAM J. Comput.
1989] and the $\widetilde{O}(\sqrt{n}m \log C)$ time algorithm of Lee and
Sidford [FOCS 2014]. In particular, our result constitutes the first known
fully combinatorial algorithm that breaks the $\widetilde{O}(m^{3/2})$ time
barrier for min-cost flow problem in planar graphs.
</p>
<p>To obtain our result we first give a very simple successive shortest paths
based scaling algorithm for unit-capacity min-cost flow problem that does not
explicitly operate on dual variables. This algorithm also runs in
$\widetilde{O}(m^{3/2}\log{C})$ time for general graphs, and, to the best of
our knowledge, it has not been described before. We subsequently show how to
implement this algorithm faster on planar graphs using well-established tools:
$r$-divisions and efficient algorithms for computing (shortest) paths in
so-called dense distance graphs.
</p></div>
    </summary>
    <updated>2019-07-05T23:27:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02266</id>
    <link href="http://arxiv.org/abs/1907.02266" rel="alternate" type="text/html"/>
    <title>Reliable Hubs for Partially-Dynamic All-Pairs Shortest Paths in Directed Graphs</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karczmarz:Adam.html">Adam Karczmarz</a>, Jakub Łącki <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02266">PDF</a><br/><b>Abstract: </b>We give new partially-dynamic algorithms for the all-pairs shortest paths
problem in weighted directed graphs. Most importantly, we give a new
deterministic incremental algorithm for the problem that handles updates in
$\widetilde{O}(mn^{4/3}\log{W}/\epsilon)$ total time (where the edge weights
are from $[1,W]$) and explicitly maintains a $(1+\epsilon)$-approximate
distance matrix. For a fixed $\epsilon&gt;0$, this is the first deterministic
partially dynamic algorithm for all-pairs shortest paths in directed graphs,
whose update time is $o(n^2)$ regardless of the number of edges. Furthermore,
we also show how to improve the state-of-the-art partially dynamic randomized
algorithms for all-pairs shortest paths [Baswana et al. STOC'02, Bernstein
STOC'13] from Monte Carlo randomized to Las Vegas randomized without increasing
the running time bounds (with respect to the $\widetilde{O}(\cdot)$ notation).
</p>
<p>Our results are obtained by giving new algorithms for the problem of
dynamically maintaining hubs, that is a set of $\widetilde{O}(n/d)$ vertices
which hit a shortest path between each pair of vertices, provided it has
hop-length $\Omega(d)$. We give new subquadratic deterministic and Las Vegas
algorithms for maintenance of hubs under either edge insertions or deletions.
</p></div>
    </summary>
    <updated>2019-07-05T23:27:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02251</id>
    <link href="http://arxiv.org/abs/1907.02251" rel="alternate" type="text/html"/>
    <title>Hardness of Bichromatic Closest Pair with Jaccard Similarity</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pagh:Rasmus.html">Rasmus Pagh</a>, Nina Stausholm, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thorup:Mikkel.html">Mikkel Thorup</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02251">PDF</a><br/><b>Abstract: </b>Consider collections $\mathcal{A}$ and $\mathcal{B}$ of red and blue sets,
respectively. Bichromatic Closest Pair is the problem of finding a pair from
$\mathcal{A}\times \mathcal{B}$ that has similarity higher than a given
threshold according to some similarity measure. Our focus here is the classic
Jaccard similarity $|\textbf{a}\cap \textbf{b}|/|\textbf{a}\cup \textbf{b}|$
for $(\textbf{a},\textbf{b})\in \mathcal{A}\times \mathcal{B}$.
</p>
<p>We consider the approximate version of the problem where we are given
thresholds $j_1&gt;j_2$ and wish to return a pair from $\mathcal{A}\times
\mathcal{B}$ that has Jaccard similarity higher than $j_2$ if there exists a
pair in $\mathcal{A}\times \mathcal{B}$ with Jaccard similarity at least $j_1$.
The classic locality sensitive hashing (LSH) algorithm of Indyk and Motwani
(STOC '98), instantiated with the MinHash LSH function of Broder et al., solves
this problem in $\tilde O(n^{2-\delta})$ time if $j_1\ge j_2^{1-\delta}$. In
particular, for $\delta=\Omega(1)$, the approximation ratio
$j_1/j_2=1/j_2^{\delta}$ increases polynomially in $1/j_2$.
</p>
<p>In this paper we give a corresponding hardness result. Assuming the
Orthogonal Vectors Conjecture (OVC), we show that there cannot be a general
solution that solves the Bichromatic Closest Pair problem in
$O(n^{2-\Omega(1)})$ time for $j_1/j_2=1/j_2^{o(1)}$. Specifically, assuming
OVC, we prove that for any $\delta&gt;0$ there exists an $\varepsilon&gt;0$ such that
Bichromatic Closest Pair with Jaccard similarity requires time
$\Omega(n^{2-\delta})$ for any choice of thresholds $j_2&lt;j_1&lt;1-\delta$, that
satisfy $j_1\le j_2^{1-\varepsilon}$.
</p></div>
    </summary>
    <updated>2019-07-05T23:20:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02245</id>
    <link href="http://arxiv.org/abs/1907.02245" rel="alternate" type="text/html"/>
    <title>Optimizing micro-tiles in micro-structures as a design paradigm</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Antolin:Pablo.html">Pablo Antolin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Buffa:Annalisa.html">Annalisa Buffa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Elaine.html">Elaine Cohen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dannenhoffer:John_F=.html">John F. Dannenhoffer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Elber:Gershon.html">Gershon Elber</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Elgeti:Stefanie.html">Stefanie Elgeti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haimes:Robert.html">Robert Haimes</a>, Richard Riesenfeld <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02245">PDF</a><br/><b>Abstract: </b>In recent years, new methods have been developed to synthesize complex porous
and micro-structured geometry in a variety of ways. In this work, we take these
approaches one step further and present these methods as an efficacious design
paradigm. Specifically, complex micro-structure geometry can be synthesized
while optimizing certain properties such as maximal heat exchange in heat
exchangers, or minimal weight under stress specifications. By being able to
adjust the geometry, the topology and/or the material properties of individual
tiles in the micro-structure, possibly in a gradual way, a porous object can be
synthesized that is optimal with respect to the design specifications. As part
of this work, we exemplify this paradigm on a variety of diverse applications.
</p></div>
    </summary>
    <updated>2019-07-05T23:31:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02218</id>
    <link href="http://arxiv.org/abs/1907.02218" rel="alternate" type="text/html"/>
    <title>Sampling Sketches for Concave Sublinear Functions of Frequencies</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Edith.html">Edith Cohen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Geri:Ofir.html">Ofir Geri</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02218">PDF</a><br/><b>Abstract: </b>We consider massive distributed datasets that consist of elements modeled as
key-value pairs and the task of computing statistics or aggregates where the
contribution of each key is weighted by a function of its frequency (sum of
values of its elements). This fundamental problem has a wealth of applications
in data analytics and machine learning, in particular, with concave sublinear
functions of the frequencies that mitigate the disproportionate effect of keys
with high frequency. The family of concave sublinear functions includes low
frequency moments ($p \leq 1$), capping, logarithms, and their compositions. A
common approach is to sample keys, ideally, proportionally to their
contributions and estimate statistics from the sample. A simple but costly way
to do this is by aggregating the data to produce a table of keys and their
frequencies, apply our function to the frequency values, and then apply a
weighted sampling scheme. Our main contribution is the design of composable
sampling sketches that can be tailored to any concave sublinear function of the
frequencies. Our sketch structure size is very close to the desired sample size
and our samples provide statistical guarantees on the estimation quality that
are very close to that of an ideal sample of the same size computed over
aggregated data. Finally, we demonstrate experimentally the simplicity and
effectiveness of our methods.
</p></div>
    </summary>
    <updated>2019-07-05T23:28:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02211</id>
    <link href="http://arxiv.org/abs/1907.02211" rel="alternate" type="text/html"/>
    <title>Optimal Decision Trees for the Algorithm Selection Problem: Integer Programming Based Approaches</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Matheus Guedes Vilas Boas, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Santos:Haroldo_Gambini.html">Haroldo Gambini Santos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Merschmann:Luiz_Henrique_de_Campos.html">Luiz Henrique de Campos Merschmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berghe:Greet_Vanden.html">Greet Vanden Berghe</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02211">PDF</a><br/><b>Abstract: </b>Even though it is well known that for most relevant computational problems
different algorithms may perform better on different classes of problem
instances, most computational experiments still focus on determining a single
best algorithm configuration based on aggregate results such as the average. In
this paper, we propose Integer Programming based approaches to build decision
trees for the Algorithm Selection Problem. These techniques allow to
automatically: (i) find the most important problem features to determine
problem classes; (ii) group the problems into classes and (iii) select the best
algorithm configuration for each class. To evaluate this new approach,
extensive computational experiments were executed using the linear programming
algorithms implemented in the COIN-OR Branch &amp; Cut solver in a comprehensive
set of instances, including all MIPLIB benchmark instances. The results
exceeded our initial expectations. While the single best parameter setting
discovered decreased the total running time by 22%, our approach decreased the
total running time by 40% in average in 10-fold cross validation experiments.
These results indicate that our method generalizes quite well and does not
overfit.
</p></div>
    </summary>
    <updated>2019-07-05T23:29:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02171</id>
    <link href="http://arxiv.org/abs/1907.02171" rel="alternate" type="text/html"/>
    <title>Sketched MinDist</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Phillips:Jeff_M=.html">Jeff M. Phillips</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Pingfan.html">Pingfan Tang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02171">PDF</a><br/><b>Abstract: </b>We consider sketch vectors of geometric objects $J$ through the \mindist
function \[ v_i(J) = \inf_{p \in J} \|p-q_i\| \] for $q_i \in Q$ from a point
set $Q$. Collecting the vector of these sketch values induces a simple,
effective, and powerful distance: the Euclidean distance between these sketched
vectors. This paper shows how large this set $Q$ needs to be under a variety of
shapes and scenarios. For hyperplanes we provide direct connection to the
sensitivity sample framework, so relative error can be preserved in $d$
dimensions using $Q = O(d/\varepsilon^2)$. However, for other shapes, we show
we need to enforce a minimum distance parameter $\rho$, and a domain size $L$.
For $d=2$ the sample size $Q$ then can be $\tilde{O}((L/\rho) \cdot
1/\varepsilon^2)$. For objects (e.g., trajectories) with at most $k$ pieces
this can provide stronger \emph{for all} approximations with
$\tilde{O}((L/\rho)\cdot k^3 / \varepsilon^2)$ points. Moreover, with similar
size bounds and restrictions, such trajectories can be reconstructed exactly
using only these sketch vectors.
</p></div>
    </summary>
    <updated>2019-07-05T23:30:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02145</id>
    <link href="http://arxiv.org/abs/1907.02145" rel="alternate" type="text/html"/>
    <title>Linear Size Sparsifier and the Geometry of the Operator Norm Ball</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Victor Reis, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rothvoss:Thomas.html">Thomas Rothvoss</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02145">PDF</a><br/><b>Abstract: </b>The Matrix Spencer Conjecture asks whether given $n$ symmetric matrices in
$\mathbb{R}^{n \times n}$ with eigenvalues in $[-1,1]$ one can always find
signs so that their signed sum has singular values bounded by $O(\sqrt{n})$.
The standard approach in discrepancy requires proving that the convex body of
all good fractional signings is large enough. However, this question has
remained wide open due to the lack of tools to certify measure lower bounds for
rather small non-polyhedral convex sets.
</p>
<p>A seminal result by Batson, Spielman and Srivastava from 2008 shows that any
undirected graph admits a linear size spectral sparsifier. Again, one can
define a convex body of all good fractional signings. We can indeed prove that
this body is close to most of the Gaussian measure. This implies that a
discrepancy algorithm by the second author can be used to sample a linear size
sparsifer. In contrast to previous methods, we require only a logarithmic
number of sampling phases.
</p></div>
    </summary>
    <updated>2019-07-05T23:25:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.02120</id>
    <link href="http://arxiv.org/abs/1907.02120" rel="alternate" type="text/html"/>
    <title>Towards improving Christofides algorithm for half-integer TSP</title>
    <feedworld_mtime>1562284800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haddadan:Arash.html">Arash Haddadan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Newman:Alantha.html">Alantha Newman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.02120">PDF</a><br/><b>Abstract: </b>We study the traveling salesman problem (TSP) in the case when the objective
function of the subtour linear programming relaxation is minimized by a
half-cycle point: $x_e \in \{ 0 ,1/2 , 1 \}$ where the half-edges form a
2-factor and the 1-edges form a perfect matching. Such points are sufficient to
resolve half-integer TSP in general and they have been conjectured to
demonstrate the largest integrality gap for the subtour relaxation.
</p>
<p>For half-cycle points, the best-known approximation guarantee is $3/2$ due to
Christofides famous algorithm. Proving an integrality gap of $\alpha$ for the
subtour relaxation is equivalent to showing that $\alpha x$ can be written as a
convex combination of tours, where $x$ is any feasible solution for this
relaxation. To beat Christofides bound, our goal is to show that
$(2-\epsilon)x$ can be written as a convex combination of tours for some
positive constant $\epsilon$. Let $y_e = 2-\epsilon$ when $x_e=1$ and $y_e=
3/4$ when $x_e = 1/2$. As a first step towards this goal, our main result is to
show that $y$ can be written as a convex combination of tours. In other words,
we show that we can save on 1-edges, which has several applications. Among
them, it gives an alternative algorithm for the recently studied uniform cover
problem. Our main new technique is a procedure to glue tours over proper 3-edge
cuts that are tight with respect to $x$ , thus reducing the problem to a base
case in which such cuts do not occur.
</p></div>
    </summary>
    <updated>2019-07-05T23:29:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4233</id>
    <link href="https://www.scottaaronson.com/blog/?p=4233" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4233#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4233" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">John Wright joins UT Austin</title>
    <summary xml:lang="en-US">I’m delighted to announce that quantum computing theorist John Wright will be joining the computer science faculty at UT Austin in Fall 2020, after he finishes a one-year postdoc at Caltech. John made an appearance on this blog a few months ago, when I wrote about the new breakthrough by him and Anand Natarajan: namely, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-image"><figure class="aligncenter"><img alt="" class="wp-image-4244" src="https://www.scottaaronson.com/blog/wp-content/uploads/2019/07/image-1.png"/></figure></div>



<p>I’m delighted to announce that quantum computing theorist <a href="http://www.mit.edu/~jswright/">John Wright</a> will be joining the computer science faculty at UT Austin in Fall 2020, after he finishes a one-year postdoc at Caltech. </p>



<p>John made an appearance on this blog a few months ago, when I <a href="https://www.scottaaronson.com/blog/?p=4172">wrote about</a> the <a href="https://arxiv.org/abs/1904.05870">new breakthrough</a> by him and <a href="http://www.its.caltech.edu/~anataraj/">Anand Natarajan</a>: namely, that MIP* (multi-prover interactive proofs with entangled provers) contains NEEXP (nondeterministic double-exponential time).  Previously, MIP* had only been known to contain NEXP (nondeterministic <em>single</em> exponential time).  So, this is an exponential expansion in the power of entangled provers over what was previously known and believed, and the first proof that entanglement actually <em>increases</em> the power of multi-prover protocols, rather than decreasing it (as it could’ve done a priori).  Even more strikingly, there seems to be no natural stopping point: MIP* might soon swallow up arbitrary towers of exponentials or even the halting problem (!).  For more, see for example <a href="https://www.quantamagazine.org/computer-scientists-expand-the-frontier-of-verifiable-knowledge-20190523/">this <em>Quanta</em> article</a>, or <a href="https://mycqstate.wordpress.com/2019/04/14/randomness-and-interaction-entanglement-ups-the-game/">this post by Thomas Vidick</a>, or <a href="http://www.henryyuen.net/post/alice-and-bob-visit/">this short story [sic] by Henry Yuen</a>.</p>



<p>John grew up in Texas, so he’s no stranger to BBQ brisket or scorching weather.  He did his undergrad in computer science at UT Austin—my colleagues remember him as a star—and then completed his PhD with Ryan O’Donnell at Carnegie Mellon, followed by a postdoc at MIT.  Besides the work on MIP*, John is also well-known for his <a href="https://arxiv.org/abs/1508.01907">2015 work with O’Donnell</a> pinning down the sample complexity of quantum state tomography.  Their important result, a version of which was independently obtained by <a href="https://arxiv.org/abs/1508.01797">Haah et al.</a>, says that if you want to learn an unknown d-dimensional quantum mixed state ρ to a reasonable precision, then ~d<sup>2</sup> copies of ρ are both necessary and sufficient.  This solved a problem that had personally interested me, and already plays a role in, e.g., my work on <a href="https://arxiv.org/abs/1711.01053">shadow tomography</a> and <a href="https://www.scottaaronson.com/papers/dpgentle.pdf">gentle measurements</a>.</p>



<p>Our little <a href="https://www.cs.utexas.edu/~qic/">quantum information center</a> at UT Austin is growing rapidly.  <a href="http://sites.utexas.edu/shyamshankar/">Shyam Shankar</a>, a superconducting qubits guy who previously worked in Rob Schoelkopf’s lab at Yale, will also be joining UT’s Electrical and Computer Engineering department this fall.  I’ll have two new postdocs—<a href="https://twitter.com/a_rocchetto?lang=en">Andrea Rocchetto</a> and <a href="http://www.cs.huji.ac.il/~yosiat/">Yosi Atia</a>—as well as new PhD students.  We’ll continue recruiting this coming year, with potential opportunities for students, postdocs, faculty, and research scientists across the CS, physics, and ECE departments as well as the Texas Advanced Computing Center (TACC).  I hope you’ll consider applying to join us.</p>



<p>With no evaluative judgment attached, I can honestly say that this is an unprecedented time for quantum computing as a field.  Where once faculty applicants struggled to make a case for quantum computing (physics departments: “but isn’t this really CS?” / CS departments: “isn’t it really physics?” / everyone: “couldn’t this whole QC thing, like, all blow over in a year?”), today departments are vying with each other and with industry players and startups to recruit talented people.  In such an environment, we’re fortunate to be doing as well as we are.  We hope to continue to expand.</p>



<p>Meanwhile, this was an <a href="https://www.nytimes.com/2019/01/24/technology/computer-science-courses-college.html">unprecedented year for CS hiring at UT Austin</a> more generally.  John Wright is one of at least four new faculty (probably more) who will be joining us.  It’s a good time to be in CS.</p>



<p>A huge welcome to John, and hook ’em Hadamards!</p>



<p>(And for US readers: have a great 4<sup>th</sup>!  Though how could any fireworks match the proof of the Sensitivity Conjecture?)</p></div>
    </content>
    <updated>2019-07-04T01:08:54Z</updated>
    <published>2019-07-04T01:08:54Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-07-05T04:22:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/07/03/postdoc-at-national-university-of-singapore-apply-by-november-30-2019/</id>
    <link href="https://cstheory-jobs.org/2019/07/03/postdoc-at-national-university-of-singapore-apply-by-november-30-2019/" rel="alternate" type="text/html"/>
    <title>Postdoc at National University of Singapore (apply by November 30, 2019)</title>
    <summary>Multiple post-doctoral research positions available in the project on “Provably Verified and Explainable Probabilistic Reasoning,” led by the Principle Investigator, Kuldeep S. Meel. The project broadly aims to develop of formal methods for AI techniques and employ advances in AI techniques for the development of formal methods. Website: https://meelgroup.github.io/files/postdoc.html Email: meel+postdoc@comp.nus.edu.sg</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Multiple post-doctoral research positions available in the project on “Provably Verified and Explainable Probabilistic Reasoning,” led by the Principle Investigator, Kuldeep S. Meel.</p>
<p>The project broadly aims to develop of formal methods for AI techniques and employ advances in AI techniques for the development of formal methods.</p>
<p>Website: <a href="https://meelgroup.github.io/files/postdoc.html">https://meelgroup.github.io/files/postdoc.html</a><br/>
Email: meel+postdoc@comp.nus.edu.sg</p></div>
    </content>
    <updated>2019-07-03T19:39:18Z</updated>
    <published>2019-07-03T19:39:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-07-06T21:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16070</id>
    <link href="https://rjlipton.wordpress.com/2019/07/03/mathematics-of-gerrymandering/" rel="alternate" type="text/html"/>
    <title>Mathematics of Gerrymandering</title>
    <summary>Can theory help? source; art by Bill Hennessy John Roberts is the Chief Justice of the United States. Today I will discuss the recent Supreme Court decision on gerrymandering. The 5-4 decision in Rucho v. Common Cause takes the courts out of deciding if redistricting was done fairly. Roberts, penning the majority argument, felt that […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Can theory help?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/07/03/mathematics-of-gerrymandering/unknown-124/" rel="attachment wp-att-16073"><img alt="" class="alignright size-full wp-image-16073" src="https://rjlipton.files.wordpress.com/2019/07/unknown.jpeg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://nonperele.com/john-biskupic-how-john-roberts-controls-the-us-supreme-court/">source</a>; art by Bill Hennessy</font></td>
</tr>
</tbody>
</table>
<p>
John Roberts is the Chief Justice of the United States.</p>
<p>
Today I will discuss the recent Supreme Court <a href="https://www.scotusblog.com/case-files/cases/rucho-v-common-cause-2/">decision</a> on gerrymandering.</p>
<p>
The 5-4 decision in Rucho v. Common Cause takes the courts out of deciding if redistricting was done fairly. Roberts, penning the majority argument, felt that it was hard, if not impossible, for courts to determine whether districts were reasonably drawn. That is, whether partisan motives dominated when they were created.</p>
<p>
I will explain what <a href="https://en.wikipedia.org/wiki/Gerrymandering">gerrymandering</a> is, and how computational methods may play a role. I must add a takeaway: </p>
<blockquote><p><b> </b> <em> <i>The current view of computational methods to avoid gerrymandering may be based on incorrect assumptions.</i> </em>
</p></blockquote>
<p>More on this later.</p>
<p>
</p><p/><h2> How It Works </h2><p/>
<p/><p>
You probably know that gerrymandering is used, negatively, to describe creating voting districts that do not reflect the voters will. The term was coined as part of an attack on the then Governor of Massachusetts in 1812. The shape of one particularly contrived district looked like a salamander. Since his name was Elbridge Gerry, it became a <i>gerrymander</i>. He was a Democratic-Republican and was not re-elected. </p>
<p>
<a href="https://rjlipton.wordpress.com/2019/07/03/mathematics-of-gerrymandering/gerry/" rel="attachment wp-att-16071"><img alt="" class="aligncenter size-full wp-image-16071" src="https://rjlipton.files.wordpress.com/2019/07/gerry.png?w=600"/></a></p>
<p>
Here is a figure from our friends at Wikipedia that presents examples of gerrymandering. </p>
<p>
<a href="https://rjlipton.wordpress.com/2019/07/03/mathematics-of-gerrymandering/divide/" rel="attachment wp-att-16072"><img alt="" class="aligncenter  wp-image-16072" src="https://rjlipton.files.wordpress.com/2019/07/divide.png?w=270" width="270"/></a></p>
<p>
By redistricting in the above, one can achieve anything from districts all won by the majority, to most won by the majority. These examples, show how the party who controls the districts can control the outcome.</p>
<p>
Well that is an overstatement. They can control the believed leanings of the voters in the districts. In the above example, they can control how yellow a district is. Real life is complicated by other factors: </p>
<ol>
<li>
A candidate may win because they are just more popular. That is a green candidate could still win in a yellow district. <p/>
</li><li>
A candidate may win because of random fluctuations. If the voter margin is small enough, random fluctuations could change the “expected” outcome.
</li></ol>
<p>
The latter is a danger for gerrymanderers. This <a href="https://www.brennancenter.org/blog/what-is-extreme-gerrymandering">site</a> on gerrymandering says: </p>
<blockquote><p><b> </b> <em> The trick is not to spread your voters out so much that districts become vulnerable to flipping to the other party in the normal give and take of electoral politics. </em>
</p></blockquote>
<p>
</p><p/><h2> How We Model It </h2><p/>
<p/><p>
Since Roberts is not our intended audience we will use mathematical definitions. I am sure Roberts and the rest of the Supreme Court justices are smart, but we have our own methods. We do not use legal jargon such as “prima facie” and suspect they do not use math jargon like “prime” numbers.</p>
<p>
So let the yellow party have <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> fraction of the voters. Suppose the voters have to be divided into <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d}"/> districts of the same size. A division is just a vector <img alt="{y=(y_{1},\dots,y_{d})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%3D%28y_%7B1%7D%2C%5Cdots%2Cy_%7Bd%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y=(y_{1},\dots,y_{d})}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  y_{1} + \cdots + y_{d} = 1, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7B1%7D+%2B+%5Ccdots+%2B+y_%7Bd%7D+%3D+1%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{1} + \cdots + y_{d} = 1, "/></p>
<p>and each <img alt="{y_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_{i}}"/> is non-negative. For such a vector <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> the number that yellow <i>wins</i> is the number of indices <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  y_{k} \ge \frac{1}{2d}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bk%7D+%5Cge+%5Cfrac%7B1%7D%7B2d%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{k} \ge \frac{1}{2d}. "/></p>
<p>Note, we assume that <img alt="{1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/2}"/> of the voters makes a district a win. We can change this to strictly larger than <img alt="{1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/2}"/>, but will leave it for now.</p>
<blockquote><p><b>Definition 1</b> <em> Define <img alt="{{\rm MaxWin}(d, \alpha)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+MaxWin%7D%28d%2C+%5Calpha%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{{\rm MaxWin}(d, \alpha)}"/> to the maximum over all <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{y}"/> of the number of wins; and define <img alt="{{\rm MinWin}(d, \alpha)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+MinWin%7D%28d%2C+%5Calpha%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{{\rm MinWin}(d, \alpha)}"/> to be the minimum number of wins. </em>
</p></blockquote>
<p>Note, we do not care who is doing the redistricting. Nor do we care about the geometry. For example 	</p>
<p align="center"><img alt="\displaystyle  {\rm MaxWin}(5, 0.60) = 5 \text{ and } {\rm MinWin}(5, 0.60) = 1. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+MaxWin%7D%285%2C+0.60%29+%3D+5+%5Ctext%7B+and+%7D+%7B%5Crm+MinWin%7D%285%2C+0.60%29+%3D+1.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm MaxWin}(5, 0.60) = 5 \text{ and } {\rm MinWin}(5, 0.60) = 1. "/></p>
<p>What can we say about these functions? Here are some simple observations.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> If <img alt="{\alpha \ge 0.5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha+%5Cge+0.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha \ge 0.5}"/>, then <img alt="{{\rm MaxWin}(d, \alpha) = d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+MaxWin%7D%28d%2C+%5Calpha%29+%3D+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\rm MaxWin}(d, \alpha) = d}"/>. Just place <img alt="{\alpha }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha }"/> fraction of the voters in each district.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> If <img alt="{\alpha &gt; 0.5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha+%3E+0.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha &gt; 0.5}"/>, then <img alt="{{\rm MinWin}(d, \alpha) \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+MinWin%7D%28d%2C+%5Calpha%29+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\rm MinWin}(d, \alpha) \ge 1}"/>. No matter how the districts are drawn there must be at least one where yellow has a majority. </p>
<p>
An advantage of these functions is that now we can discuss growth rates, not just present examples. A strength of theory is that we have replaced statements like “this algorithm is fast” by formulas for their running time. Another advantage is that these functions are <i>independent of geometry</i>. Previously I thought the dominating issue was how regions looked. Now I believe the issue is how close one gets to the best and worst case: <img alt="{\rm MaxWin}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crm+MaxWin%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rm MaxWin}"/> and <img alt="{\rm MinWin}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crm+MinWin%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rm MinWin}"/>. </p>
<blockquote><p><b>Lemma 2</b> <em> Suppose that <img alt="{\alpha &gt; 1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha+%3E+1%2F2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\alpha &gt; 1/2}"/>. Then 	</em></p><em>
<p align="center"><img alt="\displaystyle  {\rm MinWin}(d, \alpha) = d-\lfloor 2(1-\alpha) d \rfloor " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+MinWin%7D%28d%2C+%5Calpha%29+%3D+d-%5Clfloor+2%281-%5Calpha%29+d+%5Crfloor+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm MinWin}(d, \alpha) = d-\lfloor 2(1-\alpha) d \rfloor "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
<em>Proof:</em>  Let <img alt="{\beta = 1-\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+%3D+1-%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta = 1-\alpha}"/>. Let’s create the arrangement that makes green win as many districts as possible. If <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> regions are mostly green then that takes <img alt="{\frac{g}{2d}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bg%7D%7B2d%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{g}{2d}}"/> of the <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> green voters. This implies that 	</p>
<p align="center"><img alt="\displaystyle  \frac{g}{2d} \le \beta. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7Bg%7D%7B2d%7D+%5Cle+%5Cbeta.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{g}{2d} \le \beta. "/></p>
<p>So it follows 	</p>
<p align="center"><img alt="\displaystyle  g \le 2\beta d. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g+%5Cle+2%5Cbeta+d.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g \le 2\beta d. "/></p>
<p>This proves that <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> is equal to <img alt="{\lfloor 2\beta d \rfloor}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clfloor+2%5Cbeta+d+%5Crfloor%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lfloor 2\beta d \rfloor}"/>. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
It may help to set <img alt="{\alpha = 1/2 + \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha+%3D+1%2F2+%2B+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha = 1/2 + \epsilon}"/> where <img alt="{\epsilon&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon&gt;0}"/>. Let’s agree to ignore the rounding off and delete the floor and ceiling functions. Then 	</p>
<p align="center"><img alt="\displaystyle  {\rm MinWin}(d, \alpha) = d-2(1-\alpha) d " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+MinWin%7D%28d%2C+%5Calpha%29+%3D+d-2%281-%5Calpha%29+d+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm MinWin}(d, \alpha) = d-2(1-\alpha) d "/></p>
<p>and so 	</p>
<p align="center"><img alt="\displaystyle  {\rm MinWin}(d, \alpha) = 2\epsilon d. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+MinWin%7D%28d%2C+%5Calpha%29+%3D+2%5Cepsilon+d.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm MinWin}(d, \alpha) = 2\epsilon d. "/></p>
<p>Thus for <img alt="{\epsilon = 1/10}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+1%2F10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon = 1/10}"/> we get that 	</p>
<p align="center"><img alt="\displaystyle  {\rm MaxWin}(d, \alpha) = d \text{ and } {\rm MinWin}(d, \alpha) = 0.2d. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+MaxWin%7D%28d%2C+%5Calpha%29+%3D+d+%5Ctext%7B+and+%7D+%7B%5Crm+MinWin%7D%28d%2C+%5Calpha%29+%3D+0.2d.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm MaxWin}(d, \alpha) = d \text{ and } {\rm MinWin}(d, \alpha) = 0.2d. "/></p>
<p>This says that independent of any geometry if yellow has a ten percent majority, then best case if they set the districts they could win all, and if green sets the districts the worst they can get is twenty percent of the districts.</p>
<p>
</p><p/><h2> How Algorithms Can Help </h2><p/>
<p/><p>
There is long-term and continuing interest in algorithms that automate redistricting. The hope is that automated systems will be able to create districts that are fair. A trouble with this research is that there is no universal notion of what makes districts fair. The mantra is: </p>
<blockquote><p><b> </b> <em> <i>A redistricting is fair if and only if the districts collectively satisfy some geometric criterion</i>. </em>
</p></blockquote>
<p>An explicit statement of such a criterion, from a <a href="https://bdistricting.com/about.html">site</a> on such algorithms, is: </p>
<blockquote><p><b> </b> <em> <i>The best district map is the one where people have the lowest average distance to the center of their district</i>. </em>
</p></blockquote>
<p>Of course the name “gerrymandering” came from how districts looked. Somehow this enshrined the notion that districts must look right. I feel this could be wrong.</p>
<p>
Here is a quote from a recent <a href="http://district.cs.brown.edu">paper</a> on an algorithm for redistricting. </p>
<blockquote><p><b> </b> <em> We propose a method for redistricting, decomposing a geographical area into subareas, called districts, so that the populations of the districts are as close as possible and the districts are compact and contiguous. Each district is the intersection of a polygon with the geographical area. The polygons are convex and the average number of sides per polygon is less than six. </em>
</p></blockquote>
<p>The authors are Philip Klein and Neal Young, who are well known researchers on various aspects of algorithms. They do interesting work, their paper is interesting, but the assumption that geometry is the key I do not get.</p>
<p>
</p><p/><h2> How To Do Better? </h2><p/>
<p/><p>
I think that we need to go beyond geometry to understand and avoid gerrymandering. The connection between geometry and fairness is driven—I believe—by tradition. Voters in the same district probably want to be near each other. In the past being near each other was probably important, since travel was so difficult. Perhaps today location is less of an issue then it was before cars, phones, cell phones, internet access, and email. Perhaps districts can be fair and yet do not look good. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
An analogy to <a href="https://en.wikipedia.org/wiki/Fair_cake-cutting">cake cutting</a> may occur to you. Recall in the cake cutting problem success is <i>not</i> measured in how the pieces of the cake look. It is only measured by whether the parties cutting the cake are happy. Is there some way to push this analogy? I just came across a <a href="https://arxiv.org/pdf/1710.08781.pdf">paper</a> using the cake cutting method: <i>A Partisan Districting Protocol With Provably Nonpartisan Outcomes</i> by Wesley Pegden, Ariel Procaccia, and Dingli Yu. More in the future.</p>
<p>
Can algorithmic methods help? Is geometry the fundamental issue? </p>
<p/><p><br/>
[sourced photo, other word edits]</p></font></font></div>
    </content>
    <updated>2019-07-03T13:51:13Z</updated>
    <published>2019-07-03T13:51:13Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="cake-cutting"/>
    <category term="districts"/>
    <category term="fair"/>
    <category term="geometry"/>
    <category term="gerrymander"/>
    <category term="gerrymandering"/>
    <category term="reasonable"/>
    <category term="voters"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-07-06T21:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1136</id>
    <link href="https://ptreview.sublinear.info/?p=1136" rel="alternate" type="text/html"/>
    <title>News for June 2019</title>
    <summary>We’ve got four papers this month. A mix of distribution testing, matrix problems, and a different paper on the power of sampling. On the Complexity of Estimating the Effective Support Size, by Oded Goldreich (ECCC). In distribution testing, a classic problem is that of approximating the support size. By a (now) classic result of Valiant-Valiant, […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We’ve got four papers this month. A mix of distribution testing, matrix problems, and a different paper on the power of sampling.</p>



<p><strong>On the Complexity of Estimating the Effective Support Size</strong>, by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2019/088/">ECCC</a>). In distribution testing, a classic problem is that of approximating the support size. By a (now) classic result of <a href="http://theory.stanford.edu/~valiant/papers/VV_stoc11.pdf">Valiant-Valiant</a>, the complexity of this problem is \(\Theta(n/\log n)\). This paper raises the question of approximating the “effective” support, and that too with more than just samples from the distribution. The \(\epsilon\)-support of discrete distribution \(\mathcal{D}\) is the smallest support among any distribution \(\mathcal{D}’\) such that \(\|\mathcal{D} – \mathcal{D}’\|_1 \leq \epsilon\) (or TV-distance). Denote this as \(supp_\epsilon(\mathcal{D})\). One can also consider a bicriteria version. Given approximation parameter \(f\) and thresholds \(\epsilon_1 &lt; \epsilon_2\), we need to provide a number in the range \([supp_{\epsilon_2}(\mathcal{D}), f\cdot supp_{\epsilon_1}(\mathcal{D})]\). The primary model studied allows for random samples and evaluation queries (where one gets the probability of any known element of the domain). In this model, for arbitrary \(\epsilon_1, \epsilon_2\), there is a continuum of algorithms, trading off query complexity with approximation. At one end, for \(f = O(\log n)\), the query complexity is \(\widetilde{O}(1/\epsilon_1)\). At the other end, for \(f=1\), the query complexity is \(O(\log^* n)\). (Here, \(n = supp_{\epsilon_1}(\mathcal{D})\).) There are lower bounds showing the necessity of evaluation queries for subpolynomial query complexities.</p>



<p><strong>Communication and Memory Efficient Testing of Discrete Distributions</strong>, by Ilias Diakonikolas, Themis Gouleakis, Daniel M. Kane, and Sankeerth Rao (<a href="https://arxiv.org/abs/1906.04709">arXiv</a>). This paper adds additional computational constraints to the distribution testing problem. In the streaming setting, the algorithm is only allowed a single pass over the random samples. It has \(m\) bits of storage, much smaller than \(n\), the distribution size. The aim is to minimize the number of samples required for uniformity (and closeness) testing. For uniformity testing in the streaming setting, the paper gives an algorithm that uses \(\widetilde{O}(m + n/m)\) samples. The standard collision algorithm requires \(\Theta(\sqrt{n})\) storage (to store the samples), while this result gives a non-trivial bound for \(m \ll \sqrt{n}\). There are lower bounds showing that these bounds are basically tight. In the distributed distribution testing problem, there are a number of processors, each holding \(\ell\) samples. A referee asks a question to the processor, whose answers are broadcast to everyone. The aim is to minimize communication cost. For uniformity testing in this setting, there is a protocol using \(O(\sqrt{n\log n/\ell})\) bits of communication. As a sanity check, note that for \(\ell = \sqrt{n}\), one processor could simply run the collision algorithm locally to report the result. </p>



<p><strong>Querying a Matrix through Matrix-Vector Products</strong> by Xiaoming Sun, David P. Woodruff, Guang Yang, and Jialin Zhang (<a href="https://arxiv.org/pdf/1906.05736.pdf">arXiv</a>). Consider \(n \times d\) matrix \(M\) over some field \(\mathbb{F}\). One gets access to this matrix through matrix-vector products, and wishes to test some matrix property. This is a natural model in many settings, and generalizes the classic setting of query access. A subtle point is that one can only right multiply with “query” vectors, and there are problems where left multiplication can change the complexity. (A nice example in the paper is testing if a square matrix is symmetric. With both left and right multiplications, this is easy, since we can directly access rows and columns. By only accessing columns, this is non-trivial.) This paper studies a number of problems, broadly classified as linear algebra problems, statistics problems, and graph problems. Some highlights are: testing symmetry can be done in \(O(1)\) queries, and the maximum eigenvalue can be approximated in \(O(\log n)\) queries adaptively (but there is an \(\Omega(n)\) non-adaptive lower bound). For graph problems, here’s an interesting discovery. If \(M\) is the adjacency matrix, connectivity requires \(\Omega(n/\log n)\) queries. But if \(M\) is the signed edge-vertex matrix, then this can be done in \(poly(\log n)\) queries. This paper provides a number of interesting directions and problems to study.</p>



<p><strong>The Adversarial Robustness of Sampling</strong> by Omri Ben-Eliezer and Eylon Yogev (arXiv). This isn’t a property testing paper, but how can one ignore a paper on understanding the power of sampling? It’s convenient to think of the following setting. An algorithm gets a stream of \(n\) numbers, and has to answer some questions about the stream (say, the median or other quantiles). The simplest strategy is to take a small random sample of the stream. But what if an adversary was generating the stream, depending on the current sample? Under what circumstances is the sample still “representative” of the stream? The specific results of this paper require getting into set systems and VC dimension, which I’ll leave for the sake of simplicity. Let’s go back to the median question. To get an approximate median, a constant number of samples suffice. A consequence of the main result is that if one takes \(O(\log n)\) samples, then this is robust to adversarial streams. On the other hand, lower bounds show that constant sized samples can be fooled by an adversary. The paper is a really interesting read, and is a nice take on “standard facts” that we take for granted.</p>



<p/></div>
    </content>
    <updated>2019-07-03T00:54:36Z</updated>
    <published>2019-07-03T00:54:36Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2019-07-05T23:34:57Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8952301722851173857</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8952301722851173857/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/local-kid-makes-history.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8952301722851173857" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8952301722851173857" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/local-kid-makes-history.html" rel="alternate" type="text/html"/>
    <title>Local Kid Makes History</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="separator" style="clear: both; text-align: center;">
</div>
<a href="https://1.bp.blogspot.com/-OpUgtDyqMf0/XRuOhVHWTFI/AAAAAAABplw/Tz0jTV3EQCo9w0aZ1bsI9xsJiec3OfytgCLcBGAs/s1600/Huang.jpg" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" height="200" src="https://1.bp.blogspot.com/-OpUgtDyqMf0/XRuOhVHWTFI/AAAAAAABplw/Tz0jTV3EQCo9w0aZ1bsI9xsJiec3OfytgCLcBGAs/s200/Huang.jpg" width="163"/></a>The <a href="https://www.scottaaronson.com/blog/?p=4229">blogosphere</a> is <a href="https://gilkalai.wordpress.com/2019/07/02/amazing-hao-huang-proved-the-sensitivity-conjecture/">blowing</a> <a href="https://windowsontheory.org/2019/07/02/sensitivity-conjecture-proved/">up</a> over Hao Huang's just <a href="https://arxiv.org/abs/1907.00847">posted proof</a> of the sensitivity conjecture, what was one of the more <a href="https://blog.computationalcomplexity.org/2017/12/razors-edge.html">frustrating open questions</a> in complexity.<br/>
<br/>
Huang, an assistant professor in the math department at Emory, settled an open question about the hypercube. The hypercube is a graph on N=2<sup>n</sup> vertices where each vertex corresponds to an n-bit string and their are edges between vertices corresponding to strings that differ in a single bit. Think of the set of the strings of odd parity, N/2 vertices with no edges between them. Add any other vertex and it would have n neighbors. Huang showed that no matter how you placed those N/2+1 vertices in the hypercube, some vertex will have at least n<sup>1/2</sup> neighbors. By an <a href="https://doi.org/10.1016/0097-3165(92)90060-8">old result</a> of Gotsman and Linial, Huang's theorem implies the sensitivity conjecture.<br/>
<br/>
I won't go through the shockingly simple proof, the <a href="https://arxiv.org/abs/1907.00847">paper</a> is well written, or you can read the blogs I linked to above or even just Ryan O'Donnell's <a href="https://twitter.com/BooleanAnalysis/status/1145837576487612416">tweet</a>.<br/>
<br/>
I have nothing more to say than wow, just wow.</div>
    </content>
    <updated>2019-07-02T17:05:00Z</updated>
    <published>2019-07-02T17:05:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-07-06T08:23:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7530</id>
    <link href="https://windowsontheory.org/2019/07/02/sensitivity-conjecture-proved/" rel="alternate" type="text/html"/>
    <title>Sensitivity conjecture proved!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-jetpack-markdown"><p>In a recent breakthrough, <a href="http://www.mathcs.emory.edu/~hhuan30/">Hao Huang</a> gave a <a href="https://arxiv.org/abs/1907.00847">6 page paper</a> proving the longstanding sensitivity conjecture. (Hat tip, <a href="https://www.scottaaronson.com/blog/?p=4229">Scott Aaronson</a> and <a href="https://gilkalai.wordpress.com/2019/07/02/amazing-hao-huang-proved-the-sensitivity-conjecture/">Gil Kalai</a>. See this <a href="https://cstheory.stackexchange.com/questions/27714/sensitivity-block-sensitivity-conjecture-implications">stackexchange post</a> and <a href="https://eccc.weizmann.ac.il/report/2016/062/">this paper of Avishai</a> for some links to the literature on this.)</p>
<p>The proof is beautiful and simple. I will write a few words here, but it is probably easier for you to just read the <a href="https://arxiv.org/abs/1907.00847">paper</a>. The sensitivity conjecture <a href="https://www.sciencedirect.com/science/article/pii/0097316592900608">was known</a> to follow from the following statement: let <img alt="G_n" class="latex" src="https://s0.wp.com/latex.php?latex=G_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_n"/> be the <em>Boolean Cube</em> which is the degree <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> graph on <img alt="N=2^n" class="latex" src="https://s0.wp.com/latex.php?latex=N%3D2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N=2^n"/> vertices identified with <img alt="\{0,1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{0,1\}^n"/> such that for every <img alt="x,y\in \{0,1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy%5Cin+%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y\in \{0,1\}^n"/>, <img alt="x \sim y" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Csim+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \sim y"/> if their Hamming distance is one. Then, the maximum degree of every subgraph <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> of <img alt="G_n" class="latex" src="https://s0.wp.com/latex.php?latex=G_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_n"/> of size <img alt="&gt;N/2" class="latex" src="https://s0.wp.com/latex.php?latex=%3EN%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="&gt;N/2"/> is at least <img alt="\sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sqrt{n}"/>.</p>
<p>Hao proves the above statement by showing that there is a <em>signing</em> of the <img alt="N\times N" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Ctimes+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\times N"/> adjacency matrix of <img alt="G_n" class="latex" src="https://s0.wp.com/latex.php?latex=G_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_n"/> that turns it into a matrix with <img alt="N/2" class="latex" src="https://s0.wp.com/latex.php?latex=N%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N/2"/> eigenvalues equaling <img alt="+\sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%2B%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="+\sqrt{n}"/> and <img alt="N/2" class="latex" src="https://s0.wp.com/latex.php?latex=N%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N/2"/> eigenvalues equaling <img alt="-\sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="-\sqrt{n}"/>. That is, he shows (using a simple but clever inductive argument, see the 5 line proof of his Lemma 2.2) that there is an <img alt="N\times N" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Ctimes+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\times N"/> matrix <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> with entries in <img alt="\{ 0, \pm 1 \}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B+0%2C+%5Cpm+1+%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{ 0, \pm 1 \}"/> whose nonzero entries correspond to the edges of the Boolean cube, and such that all the <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/> eigenvalues of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> are <img alt="\pm \sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpm+%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\pm \sqrt{n}"/> and they sum up to zero. (Note that this makes sense since <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> should have the same <em>Frobenius norm</em> as  the adjacency matrix of <img alt="G_n" class="latex" src="https://s0.wp.com/latex.php?latex=G_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_n"/>. The Frobenius norm squared is both the sum of squares of entries, which is <img alt="N\cdot n" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Ccdot+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\cdot n"/> for <img alt="G_n" class="latex" src="https://s0.wp.com/latex.php?latex=G_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_n"/> which is a degree <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> graph, and also equal to the sum of squares of the eigenvalues, which is <img alt="N\cdot n" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Ccdot+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\cdot n"/> if all eigenvalues are <img alt="\pm \sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpm+%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\pm \sqrt{n}"/>.)</p>
<p>Once you have such a signing, the result follows from <a href="https://en.wikipedia.org/wiki/Min-max_theorem#Cauchy_interlacing_theorem">Cauchy’s Interlace Theorem</a> that says that for every <img alt="N\times N" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Ctimes+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\times N"/> matrix <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> and any <img alt="M\times M" class="latex" src="https://s0.wp.com/latex.php?latex=M%5Ctimes+M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M\times M"/> matrix <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> that is a principle sub-matrix of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/>,</p>
<p><img alt="\lambda_{1+N-M}(A) \leq \lambda_1(B) \leq \lambda_1(A)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_%7B1%2BN-M%7D%28A%29+%5Cleq+%5Clambda_1%28B%29+%5Cleq+%5Clambda_1%28A%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_{1+N-M}(A) \leq \lambda_1(B) \leq \lambda_1(A)"/></p>
<p>where <img alt="\lambda_1(A) \geq \lambda_2(A) \cdots \geq \lambda_N(A)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_1%28A%29+%5Cgeq+%5Clambda_2%28A%29+%5Ccdots+%5Cgeq+%5Clambda_N%28A%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_1(A) \geq \lambda_2(A) \cdots \geq \lambda_N(A)"/> are the eigenvalues of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> and <img alt="\lambda_1(B)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_1%28B%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_1(B)"/> is the maximum eigenvalue of <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/>. A corollary of this (which is the only fact we need) is that if <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> has its top eigenvalue <img alt="\lambda_1(A)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_1%28A%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_1(A)"/> with multiplicity <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> (i.e., <img alt="\lambda_1(A)=\lambda_K(A)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_1%28A%29%3D%5Clambda_K%28A%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_1(A)=\lambda_K(A)"/>), then every principle sub-matrix <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> of order larger than <img alt="N-K" class="latex" src="https://s0.wp.com/latex.php?latex=N-K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N-K"/> will satisfy <img alt="\lambda_1(B) = \lambda_1(A)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_1%28B%29+%3D+%5Clambda_1%28A%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_1(B) = \lambda_1(A)"/>. (In fact, we only need <img alt="\lambda_1(B) \geq \lambda_1(A)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_1%28B%29+%5Cgeq+%5Clambda_1%28A%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_1(B) \geq \lambda_1(A)"/>.)</p>
<p>Indeed, suppose that <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> is a subgraph of the Boolean cube of size <img alt="M&gt;N/2" class="latex" src="https://s0.wp.com/latex.php?latex=M%3EN%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M&gt;N/2"/>. Then the principle submatrix <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> corresponding to the vertices of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> satisfies <img alt="\lambda_1(B) \geq \lambda_{1+N-M}(A) = \sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_1%28B%29+%5Cgeq+%5Clambda_%7B1%2BN-M%7D%28A%29+%3D+%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_1(B) \geq \lambda_{1+N-M}(A) = \sqrt{n}"/> (since <img alt="M&gt;N/2" class="latex" src="https://s0.wp.com/latex.php?latex=M%3EN%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M&gt;N/2"/> and the first <img alt="N/2" class="latex" src="https://s0.wp.com/latex.php?latex=N%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N/2"/> eigenvalues of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> are <img alt="+\sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%2B%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="+\sqrt{n}"/>).
But it’s easy to show that for every matrix <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> the maximum eigenvalue of <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> is upper bounded by the maximum <img alt="\ell_1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ell_1"/> norm of its rows, which in our case is the maximum degree of the graph <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/>.</p>
</div></div>
    </content>
    <updated>2019-07-02T16:18:34Z</updated>
    <published>2019-07-02T16:18:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-07-06T21:20:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3401</id>
    <link href="https://agtb.wordpress.com/2019/07/02/papafest-september-6-8-columbia/" rel="alternate" type="text/html"/>
    <title>PapaFest (September 6-8 @ Columbia)</title>
    <summary>Announcing a conference celebrating Christos Papadimitriou, September 6-8, 2019, at Columbia University.  Registration is free.  Confirmed invited speakers include: Sanjeev Arora, Michael Collins, Michael I. Jordan, Anna Karlin,  Jon Kleinberg, Elias Koutsoupias, Adi Livnat, Noam Nisan, Prabhakar Raghavan, Scott Shenker, Eva Tardos, John Tsitsiklis, Leslie Valiant, Umesh Vazirani, and Santosh Vempala. More details here. Advertisements</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Announcing a conference celebrating Christos Papadimitriou, September 6-8, 2019, at Columbia University.  Registration is free.  Confirmed invited speakers include: Sanjeev Arora, Michael Collins, Michael I. Jordan, Anna Karlin,  Jon Kleinberg, Elias Koutsoupias, Adi Livnat, Noam Nisan, Prabhakar Raghavan, Scott Shenker, Eva Tardos, John Tsitsiklis, Leslie Valiant, Umesh Vazirani, and Santosh Vempala.</p>
<p>More details <a href="http://papafest.cs.columbia.edu/">here.</a></p></div>
    </content>
    <updated>2019-07-02T12:47:57Z</updated>
    <published>2019-07-02T12:47:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>timroughgarden</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-07-06T21:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/07/02/the-autumn-school-on-machine-learning/</id>
    <link href="https://cstheory-events.org/2019/07/02/the-autumn-school-on-machine-learning/" rel="alternate" type="text/html"/>
    <title>The Autumn school on Machine Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">October 3-11, 2019 Tbilisi, Georgia https://cte.ibsu.edu.ge/autumn/ Registration deadline: October 3, 2019 The school will be organized by the International Black Sea University with the support of Shota Rustaveli National Science Foundation of Georgia (SRNSFG). The intended audience of the autumn school includes BSc, MSc and PhD students, researchers as well as industry professionals from the … <a class="more-link" href="https://cstheory-events.org/2019/07/02/the-autumn-school-on-machine-learning/">Continue reading <span class="screen-reader-text">The Autumn school on Machine Learning</span></a></div>
    </summary>
    <updated>2019-07-02T09:09:23Z</updated>
    <published>2019-07-02T09:09:23Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-07-06T21:21:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-6337479962158243575</id>
    <link href="http://processalgebra.blogspot.com/feeds/6337479962158243575/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=6337479962158243575" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6337479962158243575" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6337479962158243575" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2019/07/phd-position-at-tu-wirn-formal-methods.html" rel="alternate" type="text/html"/>
    <title>PhD position at TU Wirn: Formal methods applied to the specification and monitoring of large-scale, spatially-distributed, stochastic systems</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="_5pbx userContent _3576" id="js_3b"><i>Interesting PhD position at TU Wien with <a class="profileLink" href="http://www.eziobartocci.com/" title="Ezio Bartocci">Ezio Bartocci</a> and <a href="https://lauranenzi.github.io/">Laura Nenzi</a>. Spread the news! </i><br/><br/> The <a href="http://ti.tuwien.ac.at/">Institute of Computer Engineering</a> at the Technische Universität Wien (TU Wien) is seeking a candidate for a  research assistant position (PhD student, 4 years). The position is  available from September (a starting date until Fall 2019 is intended)  and the successful candidate will be a PhD student of the <a href="https://logic-cs.at/phd/">LogiCS Doctoral Program</a> and she/he will be supervised by Prof. Ezio Bartocci &lt;<a href="https://l.facebook.com/l.php?u=http%3A%2F%2Fwww.eziobartocci.com%2F%3Ffbclid%3DIwAR0DbRlQ3WbZCS1HzoZXYIRYXBsRDz2tCxE2G_6APz6K76RRDmLVMIkgBOg&amp;h=AT3xOIDKhxhBw2_daqAKot9Y3qtCNlITUpSxTEttEVYsbYoofL5LKQ7MD72mx8Nf3f0zPGNE5AY-FvqTQsTVZMllhO6DR94ug3vsjG3H84L4h8RDLdPWJxht8otBFNix1De-G1NEeihv6M6l-3cp" rel="noopener nofollow" target="_blank">http://www.eziobartocci.com/</a>&gt; and co-supervised by Dr. Laura Nenzi &lt;<a href="https://l.facebook.com/l.php?u=https%3A%2F%2Flauranenzi.github.io%2F%3Ffbclid%3DIwAR0O-FIXoCWcyMBVUqn5K-Ejd0vxQQJFYQ7ni1qBAdy9Ez01d6CDolZcb6U&amp;h=AT1RfdebNNlmUThAy2WWm6AclixjQ6sGfMnjrc5C-e4EbEglPtsVHAvYqJHIPfOZfHtYPGxz__so98GKEhDQZkGKQWB2z2fBom5MdlaCvrImUyjEZ5wotHeR5-w6sF4BkAIL-PGzihR9ZVsmSasc" rel="noopener nofollow" target="_blank">https://lauranenzi.github.io/</a>&gt;<br/> The successful applicant will carry out his/her PhD in the research  area of formal methods applied to the specification and monitoring of  large-scale, spatially-distributed, stochastic systems.<br/> The position in funded by the ÖAW and the FWF &lt;<a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fm.fwf.ac.at%2Fen%2Fresearch-funding%2Fpersonnel-costs%2F%3Ffbclid%3DIwAR10rZDtpakXdwSo_WXE4A9GuFDbfkJoVYoCTgpZuBDmCMPVOmC8ZAo7Cyg&amp;h=AT13vFq0Svw4kLAe0VMNoonNMZyo8o6gdfK_-O0_LFJNhJavAH7Zt2YCyKMEKHuymeDiRrW6PggMAbBH0FyhcksLJJ0py3YWXi5vgwUwD0r_i3SFPvATFXhyZukxJL770ABLvhkdvgFQyiFvm5I8" rel="noopener nofollow" target="_blank">https://m.fwf.ac.at/en/research-funding/personnel-costs/</a>&gt;  for the recently acquired YIRG grant: “High-dimensional statistical  learning: new methods to advance economic and sustainability policies”,  an interdisciplinary project to be led for the TU part by PhD Laura  Nenzi.<br/> TASK DESCRIPTION (leader Laura Nenzi):   <br/> Formal  methods provide precise formal specification languages that can be  easily interpreted by humans and verification algorithms that can check  in an automatic way the value of satisfaction of interesting properties.  One of the main problems of such techniques is the curse of  dimensionality. A possibility to treat the problem is to use approximate  methods such as statistical model checking. However, even these  methodologies can be unfeasible for very-large-scale stochastic systems.  A new research line consists of exploiting machine learning techniques  and Bayesian inference to identify relevant data and decrease the  computational cost, permitting the application of such powerful formal  analysis on very complex systems. An important aspect that will be  covered in the study is the spatial configuration of such systems, a key  feature in several real case studies that are considering in the  project. The methodology will be principally applied to tackle questions  related to sustainable urban mobility and thus responsible consumption.<br/> APPLICATION<br/> Please apply your application following  the instructions in the DK LogiCS admission portal. <a href="https://l.facebook.com/l.php?u=https%3A%2F%2Flogic-cs.at%2Fphd%2Fadmission%2F%3Ffbclid%3DIwAR3oZZ1EmwtB7RHLErRrlbfY-HBeprkDbr8y-WyttxBelPGVrFY46hZuy3o&amp;h=AT2erabItYmdcemt5VHuXftBV82JOmliZPV-SIiYfhedKPgLyH9PR_pf1dJ3EHa-SQgsBTWJqgjsgzUJPauzRI4h_2t1t8Ci_NW6DannlCzLkriOSguTjs3IfU4p0eBWTTEvJmljKFMYpbZ36tKn" rel="noopener nofollow" target="_blank">https://logic-cs.at/phd/admission/</a> &lt;<a href="https://logic-cs.at/phd/admission/?fbclid=IwAR1rL9Zavpr2JH9oOonuC0OZ0wPUsoFxWi0usVFZPXQJzSJeSQr97rk490c" rel="noopener nofollow" target="_blank">https://logic-cs.at/phd/admission/</a>&gt;,  by indicating in the application form: Prof. Ezio Bartocci and Dr.  Laura Nenzi as supervisors. While it is not necessary to have the Master  degree at the moment of the application, it is instead mandatory to  complete it before starting the PhD.<br/> CONTACT DETAILS<br/> For  further information and inquiries about this post please contact Laura  Nenzi, e-mail: laura.nenzi@gmail.com  .</div></div>
    </content>
    <updated>2019-07-02T08:47:00Z</updated>
    <published>2019-07-02T08:47:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2019-07-02T12:00:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/07/02/papafest-for-christos-70th-birthday/</id>
    <link href="https://cstheory-events.org/2019/07/02/papafest-for-christos-70th-birthday/" rel="alternate" type="text/html"/>
    <title>PapaFest for Christos’ 70th birthday</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">September 6-8, 2019 Columbia University http://papafest.cs.columbia.edu/ We are happy to invite you to Columbia University to celebrate Christos Papadimitriou’s contributions to science on the occasion of his 70th birthday, through a mix of talks, panels, and fun activities. One of world’s leading computer scientists, Christos is best known for his work in computational complexity, helping … <a class="more-link" href="https://cstheory-events.org/2019/07/02/papafest-for-christos-70th-birthday/">Continue reading <span class="screen-reader-text">PapaFest for Christos’ 70th birthday</span></a></div>
    </summary>
    <updated>2019-07-02T08:32:49Z</updated>
    <published>2019-07-02T08:32:49Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-07-06T21:21:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4229</id>
    <link href="https://www.scottaaronson.com/blog/?p=4229" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4229#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4229" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Sensitivity Conjecture resolved</title>
    <summary xml:lang="en-US">The Sensitivity Conjecture, which I blogged about here, says that, for every Boolean function f:{0,1}n→{0,1}, the sensitivity of f—that is, the maximum, over all 2n input strings x∈{0,1}n, of the number of input bits such that flipping them changes the value of f—is at most polynomially smaller than a bunch of other complexity measures of […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Sensitivity Conjecture, which I blogged about <a href="https://www.scottaaronson.com/blog/?p=453">here</a>, says that, for every Boolean function f:{0,1}<sup>n</sup>→{0,1}, the <em>sensitivity</em> of f—that is, the maximum, over all 2<sup>n</sup> input strings x∈{0,1}<sup>n</sup>, of the number of input bits such that flipping them changes the value of f—is at most polynomially smaller than a bunch of other complexity measures of f, including f’s block sensitivity, degree as a real polynomial, and classical and quantum query complexities.  (For more, see for example <a href="http://www.cs.columbia.edu/~rocco/Teaching/S12/Readings/BdW.pdf">this survey</a> by Buhrman and de Wolf.  Or for quick definitions of the relevant concepts, <a href="https://cstheory.stackexchange.com/questions/19902/boolean-functions-where-sensitivity-equals-block-sensitivity">see here</a>.)</p>



<p>Ever since it was posed by Nisan and Szegedy in 1989, this conjecture has stood as one of the most frustrating and embarrassing open problems in all of combinatorics and theoretical computer science.  It seemed so easy, and so similar to other statements that had 5-line proofs.  But a lot of the best people in the field sank months into trying to prove it.  For whatever it’s worth, I also sank … well, at least weeks into it.</p>



<p>Now <a href="http://www.mathcs.emory.edu/~hhuan30/">Hao Huang</a>, a mathematician at Emory University, has posted a <a href="http://www.mathcs.emory.edu/~hhuan30/papers/sensitivity_1.pdf">6-page preprint</a> on his homepage that finally proves the Sensitivity Conjecture, in the form s(f)≥√deg(f).  (I thank Ryan O’Donnell for tipping me off to this.)  Within the preprint, the proof itself is about a page and a half.</p>



<p>Whenever there’s an announcement like this, ~99% of the time either the proof is wrong, or at any rate it’s way too complicated for outsiders to evaluate it quickly.  This is one of the remaining 1% of cases.  I’m rather confident that the proof is right.  Why?  Because I read and understood it.  It took me about half an hour.  If you’re comfortable with concepts like <em>induced subgraph</em> and <em>eigenvalue</em>, you can do the same.</p>



<p>From pioneering work by Gotsman and Linial in 1992, it was known that to prove the Sensitivity Conjecture, it suffices to prove the following even simpler combinatorial conjecture:</p>



<blockquote class="wp-block-quote"><p>Let S be any subset of the n-dimensional Boolean hypercube, {0,1}<sup>n</sup>, which has size 2<sup>n-1</sup>+1.  Then there must be a point in S with at least ~n<sup>c</sup> neighbors in S.</p></blockquote>



<p>Here c&gt;0 is some constant (say 1/2), and two points in S are “neighbors” if and only they differ in a single coordinate.  Note that if S had size 2<sup>n-1</sup>, then the above statement would be false—as witnessed, for example, by the set of all n-bit strings with an even number of 1’s.</p>



<p>Huang proceeds by proving the Gotsman-Linial Conjecture.  And the way he proves Gotsman-Linial is … well, at this point maybe I should just let you <a href="http://www.mathcs.emory.edu/~hhuan30/papers/sensitivity_1.pdf">read the damn preprint</a> yourself.  I can’t say it more simply than he does.</p>



<p>If I had to try anyway, I’d say: Huang constructs a 2<sup>n</sup>×2<sup>n</sup> matrix, called A<sub>n</sub>, that has 0’s where there are no edges between the corresponding vertices of the Boolean hypercube, and either 1’s or -1’s where there <em>are</em> edges—with a simple, weird pattern of 1’s and -1’s that magically makes everything work.  He then lets H be an induced subgraph of the Boolean hypercube of size 2<sup>n-1</sup>+1.  He lower-bounds the maximum degree of H by the largest eigenvalue of the corresponding (2<sup>n-1</sup>+1)×(2<sup>n-1</sup>+1) submatrix of A<sub>n</sub>.  Finally, he lower-bounds that largest eigenvalue by … no, I don’t want to spoil it!  Read it yourself!</p>



<p>Paul Erdös famously spoke of a book, maintained by God, in which was written the simplest, most beautiful proof of each theorem.  The highest compliment Erdös could give a proof was that it “came straight from the book.”  In this case, I find it hard to imagine that even God knows how to prove the Sensitivity Conjecture in any simpler way than this.</p>



<p>Indeed, the question is: how could such an elementary 1.5-page argument have been overlooked for 30 years?  I don’t have a compelling answer to that, besides noting that “short” and “elementary” often have little to do with “obvious.”  Once you start looking at the spectral properties of this matrix A<sub>n</sub>, the pieces snap together in precisely the right way—but how would you know to look at that?</p>



<p>By coincidence, earlier today I finished reading my first PG Wodehouse novel (<em><a href="http://www.gutenberg.org/files/10554/10554-h/10554-h.htm">Right Ho, Jeeves!</a></em>), on the gushing recommendation of a friend.  I don’t know how I’d missed Wodehouse for 38 years.  His defining talent is his ability to tie together five or six plot threads in a way that feels perfect and inevitable even though you didn’t see it coming.  This produces a form of pleasure that’s nearly indistinguishable from the pleasure one feels in reading a “proof from the book.”  So my pleasure centers are pretty overloaded today—but in such depressing times for the world, I’ll take pleasure wherever I can get it.</p>



<p>Huge congratulations to Hao!</p>



<p><strong>Added thought:</strong> What this really is, is one of the purest illustrations I’ve seen in my career of the power and glory of the P≠NP phenomenon.  We talk all the time about how proofs are easier to verify than to find.  In practice, though, it can be far from obvious that that’s true.  Consider your typical STOC/FOCS paper: writing it probably took the authors several months, while fully understanding the thing from scratch would probably take … <em>also</em> several months!  If there’s a gap, it’s only by a factor of 4 or 5 or something.  Whereas in this case, I don’t know how long Huang spent searching for the proof, but the combined search efforts of the community add up to years or decades.  The ratio of the difficulty of finding to the difficulty of completely grasping is in the hundreds of thousands or millions.</p>



<p><strong>Another added thought:</strong> Because Hao actually proves a stronger statement than the original Sensitivity Conjecture, it has additional implications, a few of which Hao mentions in his preprint.  Here’s one he didn’t mention: any randomized algorithm to guess the parity of an n-bit string, which succeeds with probability at least 2/3 on the majority of strings, must make at least ~√n queries to the string, while any such quantum algorithm must make at least ~n<sup>1/4</sup> queries.  For more, see the paper <a href="https://arxiv.org/pdf/1312.0036.pdf">Weak Parity</a> by me, Ambainis, Balodis, and Bavarian (Section 6).</p>



<p><strong>Important Update:</strong> Hao Huang himself has graciously <a href="https://www.scottaaronson.com/blog/?p=4229#comment-1813116">visited the comment section</a> to satisfy readers’ curiosity by providing a detailed timeline of his work on the Sensitivity Conjecture.  (tl;dr: he was introduced to the problem by Mike Saks in 2012, and had been attacking it on and off since then, until he finally had the key insight this past month while writing a grant proposal.  Who knew that grant proposals could ever be useful for anything?!?)</p>



<p><strong>Another Update:</strong> In the comments section, my former student Shalev Ben-David points out a <a href="https://www.scottaaronson.com/blog/?p=4229#comment-1813084">simplification</a> of Huang’s argument, which no longer uses Cauchy’s interlacing theorem.  I thought there was no way this proof could possibly be made any simpler, and I was wrong!</p></div>
    </content>
    <updated>2019-07-02T05:15:43Z</updated>
    <published>2019-07-02T05:15:43Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-07-05T04:22:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17499</id>
    <link href="https://gilkalai.wordpress.com/2019/07/02/amazing-hao-huang-proved-the-sensitivity-conjecture/" rel="alternate" type="text/html"/>
    <title>Amazing: Hao Huang Proved the Sensitivity Conjecture!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Today’s arXived amazing paper by Hao Huang’s Induced subgraphs of hypercubes and a proof of the Sensitivity Conjecture Contains an amazingly short and beautiful proof of a famous open problem from the theory of computing – the sensitivity conjecture posed … <a href="https://gilkalai.wordpress.com/2019/07/02/amazing-hao-huang-proved-the-sensitivity-conjecture/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Today’s arXived amazing paper by Hao Huang’s</p>
<p><a href="https://arxiv.org/abs/1907.00847">Induced subgraphs of hypercubes and a proof of the Sensitivity Conjecture</a></p>
<p>Contains an amazingly short and beautiful proof of a famous open problem from the theory of computing – the sensitivity conjecture posed by Noam Nisan and Mario Szegedi</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/07/hao-huang-chicago.jpg"><img alt="" class="alignnone size-medium wp-image-17505" height="225" src="https://gilkalai.files.wordpress.com/2019/07/hao-huang-chicago.jpg?w=300&amp;h=225" width="300"/></a></p>
<p><span style="color: #ff0000;"><strong>Hao Huang</strong></span></p>
<p><strong>Abstract: </strong>In this paper, we show that every <img alt="2^{n-1}+1" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7Bn-1%7D%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{n-1}+1"/>-vertex induced subgraph of the <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>-dimensional cube graph has maximum degree at least <img alt="\sqrt n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sqrt n"/>. This result is best possible, and improves a logarithmic lower bound shown by Chung, Füredi, Graham and Seymour in 1988. As a direct consequence, we prove that the sensitivity and degree of a boolean function are polynomially related, solving an outstanding foundational problem in theoretical computer science, the Sensitivity Conjecture of Nisan and Szegedy.</p>
<p>The proof relies on important relation between the two problems  by Gotsman and Linial. It uses beautifully Cauchy’s interlace theorem (for eigenvalues).</p>
<p>Thanks to Noga Alon for telling me about the breakthrough. For more on the conjecture and a polymath project devoted to solve it see <a href="https://www.scottaaronson.com/blog/?p=453">this post</a> on Aaronson’s Shtetl Optimized (SO).  See also <a href="https://rjlipton.wordpress.com/2016/10/05/congratulations-noam/">this post on GLL.</a> Updates: See this <a href="https://www.scottaaronson.com/blog/?p=4229">new lovely post on SO</a> (and don’t miss the excellent comment thread); and <a href="https://windowsontheory.org/2019/07/02/sensitivity-conjecture-proved/">this post by Boaz Barak </a>on WOT (Window on Theory) explains the main ingredient of Huang’s proof. Here is <a href="https://blog.computationalcomplexity.org/2019/07/local-kid-makes-history.html">a post by Lance Fortnow</a> on CC (Computational Complexity) mentioning a <a href="https://twitter.com/BooleanAnalysis/status/1145837576487612416">remarkable tweet</a> by Ryan O’Donnell (see below the fold). Here is <a href="https://www.scottaaronson.com/blog/?p=4229#comment-1813116">a comment by Hao</a> on SO on the history of his own work on the problem since he heard it from Mike Saks in 2012.</p>
<p>Also today on the arXive a paper by <a href="https://arxiv.org/search/math?searchtype=author&amp;query=Pat%C3%A1kov%C3%A1%2C+Z">Zuzana (Zuzka) Patáková</a> and me: <a href="https://arxiv.org/abs/1907.00885">Intersection Patterns of Planar Sets</a>. Like one of my very first papers “Intersection patterns of convex sets” the new paper deals with face numbers of nerves of geometric sets.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/07/kp1.png"><img alt="" class="alignnone size-full wp-image-17510" src="https://gilkalai.files.wordpress.com/2019/07/kp1.png?w=640"/></a><span id="more-17499"/></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/07/kp2.png"><img alt="" class="alignnone size-full wp-image-17511" src="https://gilkalai.files.wordpress.com/2019/07/kp2.png?w=640"/></a></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/07/ro.png"><img alt="" class="alignnone size-full wp-image-17515" height="675" src="https://gilkalai.files.wordpress.com/2019/07/ro.png?w=640&amp;h=675" width="640"/></a></p></div>
    </content>
    <updated>2019-07-02T04:07:15Z</updated>
    <published>2019-07-02T04:07:15Z</published>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Hao Huang"/>
    <category term="sensitivity conjecture"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-06T21:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4220</id>
    <link href="https://www.scottaaronson.com/blog/?p=4220" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4220#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4220" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Quantum Sabinacy</title>
    <summary xml:lang="en-US">Sabine Hossenfelder—well-known to readers of Shtetl-Optimized for opposing the building of a higher-energy collider, and various other things—has weighed in on “quantum supremacy” in this blog post and this video. Sabine consulted with me by phone before doing the video and post, and despite what some might see as her negative stance, I agree with […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Sabine Hossenfelder—well-known to readers of <em>Shtetl-Optimized</em> for <a href="https://www.scottaaronson.com/blog/?p=4122">opposing the building</a> of a higher-energy collider, and various other things—has weighed in on “quantum supremacy” in <a href="http://backreaction.blogspot.com/2019/06/quantum-supremacy-what-is-it-and-what.html?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed:+Backreaction+(Backreaction)&amp;m=1">this blog post</a> and <a href="https://www.youtube.com/watch?time_continue=1&amp;v=GKnfVA1v5ow">this video</a>.  Sabine consulted with me by phone before doing the video and post, and despite what some might see as her negative stance, I agree with what she has to say substantially more than I disagree.</p>



<p>I do, however, have a few quibbles:</p>



<p> 1. We don’t know that millions of physical qubits will be needed for useful simulations of quantum chemistry.  It all depends on how much error correction is needed and how good the error-correcting codes and simulation algorithms become.  Like, sure, you can generate pessimistic forecasts by plugging numbers in to the best known codes and algorithms.  But “the best known” is a rapidly moving target—one where there have already been orders-of-magnitude improvements in the last decade.</p>



<p>2. To my mind, there’s a big conceptual difference between a single molecule that you can’t efficiently simulate classically, and a programmable computer that you can’t efficiently simulate classically.  The difference, in short, is that only for the computer, and not for the molecule, would it ever make sense to say it had given you a <strong>wrong</strong> answer!  In other words, a physical system becomes a “computer” when, and only when, you have sufficient understanding of, and control over, its state space and time evolution that you can ask the system to simulate something <em>other than itself</em>, and then judge whether it succeeded or failed at that goal.</p>



<p>3. The entire point of my recent work, on certified randomness generation (see for example <a href="https://www.scottaaronson.com/talks/certrand2.ppt">here</a> or <a href="https://www.quantamagazine.org/how-to-turn-a-quantum-computer-into-the-ultimate-randomness-generator-20190619/">here</a>), is that sampling random bits with a NISQ-era device <em>could</em> have a practical application.  That application is … I hope you’re sitting down for this … sampling random bits!  And then, more importantly and nontrivially, <strong>proving</strong> to a faraway skeptic that the bits really were randomly generated.</p>



<p>4. While I was involved in some of the first proposals for NISQ quantum supremacy experiments (such as <a href="https://en.wikipedia.org/wiki/Boson_sampling">BosonSampling</a>), I certainly can’t take sole credit for the idea of quantum supremacy!  The term, incidentally, <a href="https://arxiv.org/abs/1203.5813">was coined by John Preskill</a>.</p>



<p>5. The term “NISQ” (Noisy Intermediate Scale Quantum) was also <a href="https://arxiv.org/abs/1801.00862">coined by John Preskill</a>.  He had no intention of misleading investors—he just needed a term to discuss the quantum computers that will plausibly be available in the near future.  As readers of this blog know, there certainly <em>has</em> been some misleading of investors (and journalists, and the public…) about the applications of near-term QCs.  But I don’t think you can lay it at the feet of the term “NISQ.”</p></div>
    </content>
    <updated>2019-07-01T18:30:10Z</updated>
    <published>2019-07-01T18:30:10Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-07-05T04:22:46Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7054757178293557182</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7054757178293557182/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/a-proof-that-227-pi-0-and-more.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7054757178293557182" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7054757178293557182" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/a-proof-that-227-pi-0-and-more.html" rel="alternate" type="text/html"/>
    <title>A proof that 22/7 - pi &gt; 0 and more</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
My father was a High School English teacher who did not know much math. As I was going off to college, intending to major in math, he gave me the following sage advice:<br/>
<br/>
1) <i>Take Physics as well as Math since Physics and Math go well together.</i> This was good advice. I took the first year of Physics for Physics Majors, and I later took a senior course in Mechanics since that was my favorite part of the first year course. Kudos to Dad!<br/>
<br/>
2) π <i>is exactly </i>22/7. I knew this was not true, but I also keow that I had no easy way to show him this. In fact, I wonder if I could have proven it myself back then.<br/>
<br/>
I had not thought about this in many years when I came across the following:<br/>
<br/>
Problem A-1 on the 1968 Putnam exam:<br/>
<br/>
Prove 22/7 - π = ∫<sub>0</sub><sup>1</sup> (x<sup>4</sup>(1-x)<sup>4</sup>)/(1+ x<sup>2</sup> )dx<br/>
<br/>
(I can easily do his by partial fractions and remembering that ∫ 1/(1+x^2) dx = tan<sup>-1</sup>x  which is tan inverse, not 1/tan. See <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/pi.pdf">here</a>.)<br/>
<br/>
(ADDED LATER---I have added conjectures on getting integrals of the form above except with 4 replaced by any natural number. Be the first on your block to solve my conjectures! It has to be easier than the Sensitivity Conjecture!)<br/>
<br/>
<br/>
<br/>
Let n &amp;in N which we will choose later. By looking at the circle that is inscribed in a regular n-polygon (n even) one finds that <br/>
<br/>
<br/>
n tan(π/n) &gt;  π <br/>
<br/>
<br/>
So we seek an even  value of n such that<br/>
<br/>
<br/>
n tan(π/n) &lt; 22/7<br/>
<br/>
<br/>
Using Wolfram alpha the smallest such n is 92.<br/>
<br/>
Would that convince Dad? Would he understand it? Probably not. Oh well.<br/>
<br/>
Some misc points.<br/>
<br/>
<br/>
1)  While working on this post I originally wanted to find tan(π/2<sup>7</sup>) by using the half-angle formula many times, and get an exact answer in terms of radicals,  rather than using Wolfram Alpha. <br/>
<br/>
a) While I have lots of combinatorics books, theory of comp books, and more Ramsey Theory books than one person should own in my house, I didn't have a SINGLE book with any trig in it.<br/>
<br/>
b) I easily found it on the web: <br/>
<br/>
tan(x/2) = sqrt( (1-cos x)/(1+cos x) ) = sin x/(1+cos x) = (1-cos x)/(sin x).<br/>
<br/>
None of these seems like it would get me a nice expression for tan(π/2<sup>7</sup>). But I don't know. Is there a nice expression for tan(π/2<sup>k</sup>) ? If you know of one then leave a polite comment.<br/>
<br/>
2) I assumed that there was a more clever and faster way to do the integral. I could not find old Putnam exams and their solutions  the web (I'm sure they are there someplace! --- if you know then comment politely with a pointer). So I got a book out of the library <i>The William Lowell Putnam Mathematical Competition Problems and Solutions 1965--1984</i> by Alexanderson, Klosinski, and Larson. Here is the clever solution:<br/>
<br/>
<i>The standard approach from Elementary Calculus applies.<br/>
<br/>
</i><br/>
<br/>
Not as clever as I as hoping for.<br/>
<br/>
3) I also looked at the integral with 4 replaced by 1,2,3,4,...,16. The results are in the writeup I pointed to before. It looks like I can use this sequence to get  upper and lower bound on i, ln(2), pi+2ln(2), and pi-2ln(2). I have not proven any of this. But take a look! And as noted above I have conjectures!<br/>
<br/>
<br/>
4) When I looked up INSCRIBING a circle in a regular n-polygon, Google kept giving me CIRCUMSCRIBING. Why? I do not know but I can speculate. Archimedes had a very nice way of using circumscribed circles to approximate pi. Its on youtube <a href="https://www.youtube.com/watch?v=_rJdkhlWZVQ">here</a>.  Hence people are used to using circumscribed rather than inscribed circles.<br/>
<br/></div>
    </content>
    <updated>2019-07-01T02:34:00Z</updated>
    <published>2019-07-01T02:34:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-07-06T08:23:39Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/06/30/linkage</id>
    <link href="https://11011110.github.io/blog/2019/06/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Ok, some of these are not so much links as mini-reports from SPAA/STOC/FCRC. For an actual conference report, see Lance’s post.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Ok, some of these are not so much links as mini-reports from SPAA/STOC/FCRC. For an actual conference report, see <a href="https://blog.computationalcomplexity.org/2019/06/fcrc-2019.html">Lance’s post</a>.</p>

<ul>
  <li>
    <p><a href="https://uhills.org/the-university-hills-section-marker-a-history-of-maps-markers-and-monuments-that-eventually-created-university-hills/">Squaring the spherical earth</a> (<a href="https://mathstodon.xyz/@11011110/102285188426196734"/>). For surveying purposes, Orange County is divided into “sections”, typically one square mile (not axis-aligned!) with small brass markers at their corners. One corner lands in the UCI faculty housing development where I live, and the housing association took the opportunity to make a larger decorative marker for it.</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2019/03/22/danny-nguyen-and-igor-pak-presburger-arithmetic-problem-solved/">Integer linear programming, change-making, and Presburger arithmetic</a> (<a href="https://mathstodon.xyz/@11011110/102291138538406550"/>). Integer arithmetic problems with a constant number of variables and one level of quantifiers (example: given a constant number of coin types, find the largest amount of money for which you cannot make change) have long been known to be polynomially solvable, but <a href="http://www.math.ucla.edu/~pak/papers/hard_presburger3.pdf">in FOCS 2017 Nguyen and Pak proved that only two levels of quantifiers make the problem hard</a>.</p>
  </li>
  <li>
    <p><a href="https://tomas.rokicki.com/dottri.html">Dots and triangles</a> (<a href="https://mathstodon.xyz/@christianp/102297036146535876"/>). Online variant of dots and boxes by Tomas Rikicki.</p>
  </li>
  <li>
    <p><a href="https://cameroncounts.wordpress.com/2019/06/19/combinatorics-at-strathclyde/">University of Strathclyde proposes to axe combinatorics and their three strong combinatorics faculty members</a> (<a href="https://mathstodon.xyz/@11011110/102301038310925223"/>, <a href="https://gowers.wordpress.com/2019/06/19/the-fate-of-combinatorics-at-strathclyde/">via</a>). This comes despite the group being both strong in research and important in undergraduate education. The apparent cause is Strathclyde’s placement of combinatorics in computer science rather than in mathematics and in their use of standards aimed more at computer science than mathematics (like bringing in large grants). There’s an <a href="https://britishcombinatorial.wordpress.com/2019/06/20/combinatorics-at-strathclyde-2/">online petition against the cuts</a> closing very soon: 5pm British time, July 1.</p>
  </li>
  <li>
    <p>Catherine Greenhill is setting up a new network for women in combinatorics, meaning “anyone who identifies as a woman, is non-binary, two-spirit, or gender diverse” (<a href="https://mathstodon.xyz/@11011110/102311965002907932"/>). <a href="https://womenincombinatorics.com/">Their website</a> currently only has a sign-up form, but expect more to come.</p>
  </li>
  <li>
    <p>Slides from three of my recent conference talks (<a href="https://mathstodon.xyz/@11011110/102319479687501682"/>):
<a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-SoCG-19-many-lines-slides.pdf">Cubic planar graphs that cannot be drawn on few lines</a>; <a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-SoCG-19-counting-slides.pdf">Counting polygon triangulations is hard</a>; <a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-SPAA-19-slides.pdf">NC algorithms for perfect matching and maximum flow in one-crossing-minor-free graphs</a>.</p>
  </li>
  <li>
    <p>Certain conference speakers need to be told that using sans-serif ∑ for one central notation and sans-serif bold ∑ for a different central notation is a bad idea. That decorating both of them by the same subscripts and the same hats doesn’t help. And that when someone asks for clarification of the notation, answering with “We should move on…this is a thing you can compute on your own” rather than actually explaining is rude (<a href="https://mathstodon.xyz/@11011110/102323855724493135"/>).</p>
  </li>
  <li>
    <p>The STOC Wikipedia edit-a-thon was called off because the convention center was locked up and participants couldn’t get into the room it was scheduled for (<a href="https://mathstodon.xyz/@11011110/102330434291269046"/>). But <a href="https://thmatters.wordpress.com/2019/06/26/edit-a-thon-update/">it was successfully rescheduled for the next day</a>, unfortunately too late in the conference for me to participate.</p>

    <p>In other news from STOC, spammy journal publishers have found a new way to spam us: fund student authors with travel awards (laudable and non-spammy!) but then require the student presenters to display a whole slide of advertising for the journal by way of acknowledgements (spammy!).</p>
  </li>
  <li>
    <p><a href="https://xkcd.com/2168/">xkcd on reading Wikipedia in the original Greek</a> (<a href="https://mathstodon.xyz/@11011110/102339055555301837"/>).</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1906.10668">Discrete logarithms in quasi-polynomial time in finite fields of fixed characteristic</a> (<a href="https://mathstodon.xyz/@erou/102337004608271854"/>). New preprint by Kleinjung and Weselowski.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=OerQAsqEOLc">Short video on interleaving multiple copies of the infinite Laves graph</a> (<a href="https://mathstodon.xyz/@11011110/102353592823020968"/>). Sound not necessary.</p>
  </li>
  <li>
    <p><a href="https://arstechnica.com/science/2019/06/two-new-papers-explore-the-complicated-physics-behind-bubbles-and-foams/">Two new papers explore the complicated physics behind bubbles and foams</a> (<a href="https://mathstodon.xyz/@11011110/102356874963380843"/>).
Juanes et al find <a href="http://dx.doi.org/10.1073/pnas.1819744116">universality in pinching off uniformly sized bubbles</a> from a tube much like drops from a dripping faucet. And Yanagisawa and Kurita discover <a href="http://dx.doi.org/10.1038/s41598-019-41486-6">two mechanisms for breaking bubbles to propagate through a foam</a>. As it contracts, the breaking bubble can hit other bubbles, and it can also scatter off droplets which hit other bubbles.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1902.07622">Centrality analysis of Wikipedia links between mathematicians</a> (<a href="https://mathstodon.xyz/@11011110/102361585031049847"/>, <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2019-06-30/Recent_research">via</a>). None of the names listed are surprising, but the ordering might be a little. Noether makes the top 10; Bourbaki, Grothendieck, and Turing are farther down. Martin Gardner makes the cut (barely), at #35.</p>
  </li>
</ul></div>
    </content>
    <updated>2019-06-30T16:16:00Z</updated>
    <published>2019-06-30T16:16:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-06-30T23:47:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16046</id>
    <link href="https://rjlipton.wordpress.com/2019/06/29/a-prime-breakthrough/" rel="alternate" type="text/html"/>
    <title>A Prime Breakthrough</title>
    <summary>A breakthrough on the Riemann Hypothesis [ Composite of various sources ] Michael Griffin, Ken Ono, Larry Rolen, and Don Zagier (GORZ) have recently published a paper on an old approach to the famous Riemann Hypothesis (RH). Today we will discuss their work and its connection to P=NP. Their paper is titled, “Jensen polynomials for […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A breakthrough on the Riemann Hypothesis</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/29/a-prime-breakthrough/gorz/" rel="attachment wp-att-16067"><img alt="" class="alignright size-medium wp-image-16067" height="300" src="https://rjlipton.files.wordpress.com/2019/06/gorz.png?w=261&amp;h=300" width="261"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Composite of various sources ]</font></td>
</tr>
</tbody>
</table>
<p>
Michael Griffin, Ken Ono, Larry Rolen, and Don Zagier (GORZ) have recently published a paper on an old approach to the famous Riemann Hypothesis (RH).</p>
<p>
Today we will discuss their work and its connection to P=NP.</p>
<p>
Their <a href="https://arxiv.org/pdf/1902.07321.pdf">paper</a> is titled, “Jensen polynomials for the Riemann zeta function and other sequences.” The final <a href="https://doi.org/10.1073/pnas.1902572116">version</a> is in the Proceedings of the National Academy of Sciences (PNAS).</p>
<p>
The RH is still open. We are not aware of any update on the status of Michael Atiyah’s claim to have solved it since this <a href="https://blogs.ams.org/blogonmathblogs/2018/10/01/on-michael-atiyah-and-the-riemann-hypothesis/">note</a> on an AMS blog and our own <a href="https://rjlipton.wordpress.com/2018/09/26/reading-into-atiyahs-proof/">discussion</a> of his papers’ contents.</p>
<p>
Recall the RH is a central conjecture in number theory, and it has the following properties: </p>
<ul>
<li>
If true, it would greatly enhance our understanding of the structure of the primes: <img alt="{2,3,5,7,\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%2C3%2C5%2C7%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2,3,5,7,\dots}"/> <p/>
</li><li>
It has resisted attacks for 160 years and counting. <p/>
</li><li>
There are an immense number of statements equivalent to the RH.
</li></ul>
<p>See, for example, the survey <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.66.3584&amp;rep=rep1&amp;type=pdf">paper</a>, “The Riemann Hypothesis,” by Brian Conrey. </p>
<p>
Complexity theory has the P=NP question; number theory has the RH. Both seem to be beyond reach, and both are fundamental questions. The recent work of GORZ has created buzz among number theorists—perhaps they are on the verge of a breakthrough? Is there hope that we might see progress on P=NP? Or must we wait 160 years? </p>
<p>
The buzz is reflected in a May 28 <a href="https://www.livescience.com/65577-riemann-hypothesis-big-step-math.html">story</a> in the online journal <i>LiveScience</i>. It quotes Enrico Bombieri, who wrote the official Clay Prize <a href="https://www.claymath.org/sites/default/files/official_problem_description.pdf">description</a> of RH, as saying:</p>
<blockquote><p><b> </b> <em> “Although this remains far away from proving the Riemann hypothesis, it is a big step forward. There is no doubt that this paper will inspire further fundamental work in other areas of number theory as well as in mathematical physics.” </em>
</p></blockquote>
<p/><p>
Bombieri wrote an accompanying <a href="https://www.pnas.org/content/early/2019/05/22/1906804116">paper</a> in PNAS, in which the above quoted sentences also appear. We will try to explain what the excitement is about.</p>
<p>
</p><p/><h2> The Approach and Results </h2><p/>
<p/><p>
RH is equivalent to the hyperbolicity of Jensen polynomials for the Riemann zeta function. Note: A real, polynomial <img alt="{p(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p(x)}"/> is <i>hyperbolic</i> if all its roots are real—a fancy name for a simple concept.</p>
<p>
There is an analytic function <img alt="{E(z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E(z)}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  RH \iff E(z) \text{ has all real roots}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++RH+%5Ciff+E%28z%29+%5Ctext%7B+has+all+real+roots%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  RH \iff E(z) \text{ has all real roots}. "/></p>
<p>Almost a hundred years ago, Johan Jensen and George Pólya created an approach to the RH based on <img alt="{E(z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E(z)}"/>. Rather than prove <img alt="{E(z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E(z)}"/> has only real roots, they showed it is enough to show that a family of polynomials <img alt="{J(n, d)(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%28n%2C+d%29%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J(n, d)(x)}"/> all are hyperbolic. The point is that polynomials are “simpler” than analytic functions—at least that is the hope.</p>
<p>
What GORZ prove is this: </p>
<blockquote><p><b>Theorem 1</b> <em> For each <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{d}"/> and almost all <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/>, the polynomial <img alt="{J(n, d)(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%28n%2C+d%29%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{J(n, d)(x)}"/> is hyperbolic. </em>
</p></blockquote>
<p>Of course, “almost all” means that there at most a finite number of polynomials that are not hyperbolic. They also show that for degree <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d}"/> fixed, 	</p>
<p align="center"><img alt="\displaystyle  \lim_{n \rightarrow \infty} J(n, d)(x) = H_{d}(x). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clim_%7Bn+%5Crightarrow+%5Cinfty%7D+J%28n%2C+d%29%28x%29+%3D+H_%7Bd%7D%28x%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \lim_{n \rightarrow \infty} J(n, d)(x) = H_{d}(x). "/></p>
<p>Where the polynomials <img alt="{H_{d}(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH_%7Bd%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H_{d}(x)}"/> have only real roots.</p>
<p>
This is explained further in a <a href="http://people.oregonstate.edu/~petschec/ONTD/Talk1.pdf">talk</a> by Ono, which has this table showing how the polynomials converge:</p>
<p><a href="https://rjlipton.wordpress.com/2019/06/29/a-prime-breakthrough/hyper/" rel="attachment wp-att-16049"><img alt="" class="aligncenter size-medium wp-image-16049" height="187" src="https://rjlipton.files.wordpress.com/2019/06/hyper.png?w=300&amp;h=187" width="300"/></a></p>
<p>Note: they use different notation for the polynomials.</p>
<p>
</p><p/><h2> The Approximation Property </h2><p/>
<p>There are many equivalent formulations of the RH.  While all are equivalent, some are more equivalent than others.  Some seem to be a more plausible path toward a resolution of the RH. Of course to date none have worked. </p>
<p>There is a way to distinguish equivalent formulations of the RH. Suppose that <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> depends on some parameter <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta"/>. Suppose that as <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta"/> increases the statement <img alt="S(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=S%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S(\beta)"/> becomes a weaker version of the RH.<br/>
Then let us informally say that the formulation <img alt="S(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=S%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S(\beta)"/> has the “Approximation Property” (AP). The point is that progress in proving caes of <img alt="S(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=S%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S(\beta)"/>—even as partial progress—is exciting. But if the equivalence only holds for <img alt="\beta=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta=0"/>, with no connections for higher <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta"/>, then the partial progress could be seen as morally useful—but it is not really mathematically useful.</p>
<p>There are equivalences of the RH that have AP and some that seem not to have it. The approximation for the RH is natural. The RH states that there is no zero <img alt="\sigma + it" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma+%2B+it&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma + it"/> of the Riemann zeta function with <img alt="\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma"/> above <img alt="1/2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1/2"/>. The weaker statement: If <img alt="\sigma + it" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma+%2B+it&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma + it"/> is a zero, then <img alt="\sigma &gt;  \alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma+%3E++%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma &gt;  \alpha"/> is unknown for any <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/> in the open interval <img alt="(1/2,1)" class="latex" src="https://s0.wp.com/latex.php?latex=%281%2F2%2C1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(1/2,1)"/>. This can be used to get a property with the AP.</p>
<p>For example, this 2003 <a href="http://\href{https://arxiv.org/abs/math/0307215}">paper</a> by Luis Baez-Duarte, titled “A new necessary and sufficient condition for the Riemann hypothesis,” has the AP. He notes, in our terminology, that his property is an AP.</p>
<p/><h2> Open Problems </h2><p/>
<p/><p>
Does the approach of GORZ have the AP? The issue is that while we get a universal statement in terms of <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>, the “all but finitely many” condition on <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> works against its being an approximation—what if the finite sets of exceptions grow in terms of <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> in ways that offset the purpose behind the equivalence?</p>
<p/></font></font></div>
    </content>
    <updated>2019-06-29T14:50:42Z</updated>
    <published>2019-06-29T14:50:42Z</published>
    <category term="History"/>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="hyperbolic"/>
    <category term="PNAS"/>
    <category term="polynomial"/>
    <category term="Proof"/>
    <category term="real roots"/>
    <category term="RH"/>
    <category term="Riemann Hypothesis"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-07-06T21:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17489</id>
    <link href="https://gilkalai.wordpress.com/2019/06/28/another-sensation-annika-heckel-non-concentration-of-the-chromatic-number-of-a-random-graph/" rel="alternate" type="text/html"/>
    <title>Another sensation – Annika Heckel: Non-concentration of the chromatic number of a random graph</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Annika Heckel Sorry for the long period of non blogging. There are a lot of things to report and  various other plans for posts and I hope to come back to it soon. But it is nice to break the … <a href="https://gilkalai.wordpress.com/2019/06/28/another-sensation-annika-heckel-non-concentration-of-the-chromatic-number-of-a-random-graph/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2019/06/annika-1.png"><img alt="" class="alignnone size-full wp-image-17490" src="https://gilkalai.files.wordpress.com/2019/06/annika-1.png?w=640"/></a></p>
<p><span style="color: #ff0000;"><strong>Annika Heckel</strong></span></p>
<p>Sorry for the long period of non blogging. There are a lot of things to report and  various other plans for posts and I hope to come back to it soon. But it is nice to break the silence with another sensational result by Annika Heckel. I first heard about it some time ago and Noam Lifshitz just informed be that the<a href="https://arxiv.org/abs/1906.11808"> paper is on the arXive</a>!</p>
<h2><a href="https://arxiv.org/abs/1906.11808">Non-concentration of the chromatic number of a random graph</a></h2>
<p>And here is the <strong>abstract:</strong> We show that the chromatic number of $latex <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="msubsup" id="MathJax-Span-3"><span class="mi" id="MathJax-Span-4">G(</span><span class="texatom" id="MathJax-Span-5"><span class="mrow" id="MathJax-Span-6"><span class="mi" id="MathJax-Span-7">n</span><span class="mo" id="MathJax-Span-8">,</span><span class="mfrac" id="MathJax-Span-9"><span class="mn" id="MathJax-Span-10">1/</span><span class="mn" id="MathJax-Span-11">2)$</span></span></span></span></span></span></span></span> is not concentrated on fewer than <img alt="n^{1/4-\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2F4-%5Cepsilon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{1/4-\epsilon}"/>  consecutive values. This addresses a longstanding question raised by Erdős and several other authors.</p>
<p>The Introduction tells the history of the problem very nicely.</p></div>
    </content>
    <updated>2019-06-28T10:37:04Z</updated>
    <published>2019-06-28T10:37:04Z</published>
    <category term="Combinatorics"/>
    <category term="Annika Heckel"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-06T21:20:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5703612485495687112</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5703612485495687112/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/fcrc-2019.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5703612485495687112" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5703612485495687112" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/fcrc-2019.html" rel="alternate" type="text/html"/>
    <title>FCRC 2019</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-1zksIfUU-7U/XRToEil7J7I/AAAAAAABpiI/nR4seqK1l08FfLoalQyM2OzdK3iZTwL2ACKgBGAs/s1600/MVIMG_20190626_112006_1.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" height="240" src="https://1.bp.blogspot.com/-1zksIfUU-7U/XRToEil7J7I/AAAAAAABpiI/nR4seqK1l08FfLoalQyM2OzdK3iZTwL2ACKgBGAs/s320/MVIMG_20190626_112006_1.jpg" width="320"/></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Georgia Tech FCRC Participants</td></tr>
</tbody></table>
I'm heading home from the <a href="https://fcrc.acm.org/">2019 ACM Federated Computing Research Conference</a> in Phoenix, a collection of computer science meetings including <a href="http://acm-stoc.org/stoc2019/">STOC</a>, <a href="http://learningtheory.org/colt2019/">COLT</a> and <a href="http://www.sigecom.org/ec19/">EC</a>.<br/>
<br/>
Geoff Hinton and Yann LeCun gave their Turing award lectures, their co-winner Yoshua Bengio not in attendance. Hinton talked about how machine learning triumphed over symbolic AI. LeCun argued that under uncertainty, one should learn the distribution instead of just the average. If you want more, just <a href="https://fcrc.acm.org/turing-lecture-at-fcrc-2019">watch it yourself</a>.<br/>
<div style="text-align: left;">
<br/></div>
<div style="text-align: left;">
To get to the STOC lectures you go up and down escalators and pass through ISCA (Computer Architecture) and PLDI (Programming Languages). It's like you are going up the computing stack until you reach algorithms and complexity. </div>
<div style="text-align: left;">
<br/>
The conference center was just two blocks from Chase Field so we could take in a Diamondbacks baseball game. They opened the roof because the temperature dropped into the double digits. Last night, Paul McCartney played at an arena just a block from the conference center, but instead I hung out at an Uber reception for the EC conference.<br/>
<br/></div>
<div style="text-align: left;">
Let me mention a best paper awardee, <a href="https://doi.org/10.1145/3313276.3316369">The Reachability Problem for Petri Nets is Not Elementary</a> by Wojciech Czerwinski, Slawomir Lasota, Ranko Lazic, Jerome Leroux and Filip Mazowiecki. In a Petri net you have a list of vectors of integers and an initial and final vector. You start with the initial vector and can add any of the other vectors nondeterministically as often as you like as long as no coordinate goes negative. Can you get to the final vector? This problem was known to be computable in "Ackermannian" time and EXPSPACE-hard. This paper shows the problem is not elementary, i.e. not solvable in running time a tower of 2's to the n. A <a href="https://arxiv.org/abs/1903.08575">recent result</a> shows Petri Nets reachability is primitive recursive for fixed dimensions.<br/>
<br/>
Avi Wigderson gave the Knuth Prize lecture exploring deep connections between mathematics and algorithms. Hopefully the video will be online soon.<br/>
<br/>
STOC next year in Chicago, EC as part of the Game Theory Congress in Budapest. </div></div>
    </content>
    <updated>2019-06-27T20:34:00Z</updated>
    <published>2019-06-27T20:34:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-07-06T08:23:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/06/27/student-combinatorics-day-at-bar-ilan-university-israel/</id>
    <link href="https://cstheory-events.org/2019/06/27/student-combinatorics-day-at-bar-ilan-university-israel/" rel="alternate" type="text/html"/>
    <title>Student Combinatorics Day at Bar Ilan University (Israel)</title>
    <summary>July 7, 2019 Bar Ilan University, Israel https://sites.google.com/a/math.biu.ac.il/student-combinatorics-day/ A one-day workshop in Combinatorics composed of a keynote talk of Noga Alon on list coloring and of 15 short talks by graduate students and postdocs.</summary>
    <updated>2019-06-27T16:35:25Z</updated>
    <published>2019-06-27T16:35:25Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-07-06T21:21:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/090</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/090" rel="alternate" type="text/html"/>
    <title>TR19-090 |  Quasilinear time list-decodable codes for space bounded channels | 

	Ronen Shaltiel, 

	Swastik Kopparty, 

	Jad Silbak</title>
    <summary>We consider codes for space bounded channels. This is a model for communication under noise that was studied by Guruswami and Smith (J. ACM 2016) and lies between the Shannon (random) and Hamming (adversarial) models. In this model, a channel is a space bounded procedure that reads the codeword in one pass, and modifies at most a $p$ fraction of the bits of the codeword.

Guruswami and Smith, and later work by Shaltiel and Silbak (RANDOM 2016), gave constructions of list-decodable codes with rate approaching $1-H(p)$ against channels with space $s=c \log n$, with encoding/decoding time $\poly(2^s)=\poly(n^c)$.

In this paper we show that for every constant $0 \le p 0$, there are codes with rate $R \ge 1-H(p)-\epsilon$, list size $\poly(1/\epsilon)$, and furthermore:
\begin{itemize}
\item Our codes can handle channels with space $s=n^{\Omega(1)}$, which is much larger than $O(\log n)$ achieved by previous work.
\item We give encoding and decoding algorithms that run in time $n \cdot \polylog(n)$. Previous work achieved large and unspecified $\poly(n)$ time (even for space $s=1 \cdot \log n$ channels).
\item We can handle space bounded channels that read the codeword in any order, whereas previous work considered channels that read the codeword in the standard order.
\end{itemize}
Our construction builds on the machinery of Guruswami and Smith (with some key modifications) replacing some nonconstructive codes and pseudorandom objects (that are found in exponential time by brute force) with efficient explicit constructions.
For this purpose we exploit recent results of Haramaty, Lee and Viola (SICOMP 2018) on pseudorandom properties of ``$t$-wise independence + low weight noise'' which we quantitatively improve using techniques by Forbes and Kelly (FOCS 2018).

To make use of such distributions, we give new explicit constructions of binary linear codes that have dual distance of $n^{\Omega(1)}$, and are also polynomial time list-decodable from relative distance $\half-\epsilon$, with list size $\poly(1/\epsilon)$. To the best of our knowledge, no such construction was previously known.

Somewhat surprisingly, we show that Reed-Solomon codes with dimension $k&lt;\sqrt{n}$, have this property if interpreted as binary codes (in some specific interpretation) which we term: ``Raw Reed-Solomon Codes''. A key idea is viewing Reed-Solomon codes as ``bundles'' of certain dual-BCH codewords.</summary>
    <updated>2019-06-27T11:33:00Z</updated>
    <published>2019-06-27T11:33:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-06T21:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1278</id>
    <link href="https://thmatters.wordpress.com/2019/06/26/edit-a-thon-update/" rel="alternate" type="text/html"/>
    <title>Edit-a-thon update</title>
    <summary>This event was a great success! Thank you to all of the participants for contributing your time. Please keep up the momentum and continue to edit the pages you made a start on. Please continue to record your progress on the list of topics. Special thanks to Aviad Rubinstein and Yuval Filmus for offering expert […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://thmatters.wordpress.com/2019/06/11/wikipedia-edit-a-thon-at-stoc19/">This event</a> was a great success! Thank you to all of the participants for contributing your time. Please keep up the momentum and continue to edit the pages you made a start on. Please continue to record your progress on the <a href="https://docs.google.com/spreadsheets/d/1zVUdxKk9nqR5Itwc37v26aRHMqgV9qI8XexssoFq_CE/edit?usp=sharing">list of topics</a>. Special thanks to Aviad Rubinstein and Yuval Filmus for offering expert advice at the event.</p>
<p>We plan to organize this event again at future STOCs, and hope many more people can participate. Even an hour of your time can have a huge impact on the community!</p>
<p><img alt="20190625_212644" class="  wp-image-1279 aligncenter" height="352" src="https://thmatters.files.wordpress.com/2019/06/20190625_212644.jpg?w=470&amp;h=352" width="470"/></p></div>
    </content>
    <updated>2019-06-26T15:54:42Z</updated>
    <published>2019-06-26T15:54:42Z</published>
    <category term="Uncategorized"/>
    <category term="workshops"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2019-07-06T21:20:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=645</id>
    <link href="https://emanueleviola.wordpress.com/2019/06/25/we-knew-the-best-threshold-circuit-lower-bounds-long-ago/" rel="alternate" type="text/html"/>
    <title>We knew the best threshold-circuit lower bounds long ago</title>
    <summary>For more than 20 years we’ve had lower bounds for threshold circuits of depth [IPS97], for a fixed . There have been several “explanations” for the lack of progress [AK10]. Recently Chen and Tell have given a better explanation showing that you can’t even improve the result to a better without proving “the whole thing.” […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>     <!--http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd-->                 <!-- html,xhtml,-css,NoFonts -->     </p>
<p style="text-align: justify;">For more than 20 years we’ve had <img alt="n^{1+c^{-d}}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2Bc%5E%7B-d%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{1+c^{-d}}"/> lower bounds for threshold circuits of depth <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XImpagliazzoPS97">IPS97</a>]</span>, for a fixed <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/>. There have been several “explanations” for the lack of progress <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XAllenderK10">AK10</a>]</span>. Recently <a href="https://www.google.com/url%3Fsa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D1%26cad%3Drja%26uact%3D8%26ved%3D2ahUKEwiVrqqOgoXjAhWF1FkKHUgiBjcQFjAAegQIAhAC%26url%3Dhttps%3A%2F%2Feccc.weizmann.ac.il%2Freport%2F2018%2F199%2Fdownload%26usg%3DAOvVaw342o4eSvZMdt_vhMYa7kwk">Chen and Tell</a> have given a better explanation showing that you can’t even improve the result to a better <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/> without proving “the whole thing.” </p>
<p style="text-align: justify;">   Say you have a finite group <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> and you want to compute the <em>iterated product</em> of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> elements. </p>
<p style="text-align: justify;">   <b>Warm-up <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XAllenderK10">AK10</a>]</span>.</b>. </p>
<p style="text-align: justify;">   Suppose you can compute this with circuits of size <img alt="s(n)=n^{10}" class="latex" src="https://s0.wp.com/latex.php?latex=s%28n%29%3Dn%5E%7B10%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="s(n)=n^{10}"/> and depth <img alt="10" class="latex" src="https://s0.wp.com/latex.php?latex=10&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="10"/>. Now we show how you can trade size for depth. Put a complete tree with fan-in <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f"/> on top of the group product, where each node computes the product of its children (this is correct by associativity, in general this works for a monoid). This tree needs depth <img alt="\log _{f}n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+_%7Bf%7Dn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\log _{f}n"/>. If you stick your circuit of size <img alt="s(n)" class="latex" src="https://s0.wp.com/latex.php?latex=s%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="s(n)"/> and depth <img alt="O(1)" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(1)"/> at each node, the depth of the overall circuit would be obviously <img alt="O(\log _{f}n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Clog+_%7Bf%7Dn%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(\log _{f}n)"/> and the overall size would be dominated by the input layer which is <img alt="s(f)\cdot n/f&lt;s(f)\cdot n" class="latex" src="https://s0.wp.com/latex.php?latex=s%28f%29%5Ccdot+n%2Ff%3Cs%28f%29%5Ccdot+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="s(f)\cdot n/f&lt;s(f)\cdot n"/>. If you are aiming for overall depth <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>, you need <img alt="f=n^{O(1/d)}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3Dn%5E%7BO%281%2Fd%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f=n^{O(1/d)}"/>. This gives size <img alt="n^{1+O(1/d)}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2BO%281%2Fd%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{1+O(1/d)}"/>. </p>
<p style="text-align: justify;">   Hence we have shown that proving bounds <img alt="n^{1+\omega (1/d)}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2B%5Comega+%281%2Fd%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{1+\omega (1/d)}"/> for some depth <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> suffices to prove <img alt="n^{10}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B10%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{10}"/> lower bounds for depth <img alt="10" class="latex" src="https://s0.wp.com/latex.php?latex=10&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="10"/>. </p>
<p style="text-align: justify;">   <b>Chen and Tell.</b>. </p>
<p style="text-align: justify;">   The above is not the most efficient way to build a tree! I am writing this post following their paper to understand what they do. As they say, the idea is quite simple. While above the size will be dominated by the input layer, we want to balance things so that every layer has roughly the same contribution. </p>
<p style="text-align: justify;">   Let’s say we are aiming for size <img alt="n^{1+\epsilon }" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2B%5Cepsilon+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{1+\epsilon }"/> and let’s see what depth we can get. Let’s say now the size is <img alt="s(n)=n^{k}" class="latex" src="https://s0.wp.com/latex.php?latex=s%28n%29%3Dn%5E%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="s(n)=n^{k}"/>. Let us denote by <img alt="n_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{i}"/> the number of nodes at level <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> with <img alt="i=0" class="latex" src="https://s0.wp.com/latex.php?latex=i%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i=0"/> being the root. The fan-in at level <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> is <img alt="(n^{1+\epsilon }/n_{i})^{1/k}" class="latex" src="https://s0.wp.com/latex.php?latex=%28n%5E%7B1%2B%5Cepsilon+%7D%2Fn_%7Bi%7D%29%5E%7B1%2Fk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(n^{1+\epsilon }/n_{i})^{1/k}"/> so that the cost is <img alt="n^{1+\epsilon }" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2B%5Cepsilon+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{1+\epsilon }"/> as desired. We have the recursion <img alt="n_{i+1}=n_{i}\cdot (n^{1+\epsilon }/n_{i})^{1/k}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Bi%2B1%7D%3Dn_%7Bi%7D%5Ccdot+%28n%5E%7B1%2B%5Cepsilon+%7D%2Fn_%7Bi%7D%29%5E%7B1%2Fk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{i+1}=n_{i}\cdot (n^{1+\epsilon }/n_{i})^{1/k}"/>. </p>
<p style="text-align: justify;">   The solution to this recursion is <img alt="n_{i}=n^{(1+\epsilon )(1-(1-1/k)^{i})}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Bi%7D%3Dn%5E%7B%281%2B%5Cepsilon+%29%281-%281-1%2Fk%29%5E%7Bi%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{i}=n^{(1+\epsilon )(1-(1-1/k)^{i})}"/>, see below. </p>
<p style="text-align: justify;">   So that’s it. We need to get to <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> nodes. So if you set <img alt="i=O(k\log (1/\epsilon ))" class="latex" src="https://s0.wp.com/latex.php?latex=i%3DO%28k%5Clog+%281%2F%5Cepsilon+%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i=O(k\log (1/\epsilon ))"/> you get say <img alt="n_{i}=n^{(1+\epsilon )(1-\epsilon ^{2})}&gt;n" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Bi%7D%3Dn%5E%7B%281%2B%5Cepsilon+%29%281-%5Cepsilon+%5E%7B2%7D%29%7D%3En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{i}=n^{(1+\epsilon )(1-\epsilon ^{2})}&gt;n"/>. Going back to <img alt="k=10" class="latex" src="https://s0.wp.com/latex.php?latex=k%3D10&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k=10"/>, we have exhibited circuits of size <img alt="n^{1+\epsilon }" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2B%5Cepsilon+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{1+\epsilon }"/> and depth just <img alt="O(\log 1/\epsilon )" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Clog+1%2F%5Cepsilon+%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(\log 1/\epsilon )"/>. So proving stronger bounds than this would rule out circuits of size <img alt="n^{10}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B10%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{10}"/> and depth <img alt="10" class="latex" src="https://s0.wp.com/latex.php?latex=10&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="10"/>. </p>
<p style="text-align: justify;">   <b>Added later: About the recurrence</b>. </p>
<p style="text-align: justify;">   Letting <img alt="a_{i}:=\log _{n}n_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=a_%7Bi%7D%3A%3D%5Clog+_%7Bn%7Dn_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_{i}:=\log _{n}n_{i}"/> we have the following recurrence for the exponents of <img alt="n_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{i}"/>. </p>
<p style="text-align: justify;">
</p><div style="text-align: center;"> <img alt="\begin{aligned} a_{0} &amp; =0\\ a_{i+1} &amp; =a_{i}(1-1/k)+(1+\epsilon )/k=:a_{i}b+c. \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+a_%7B0%7D+%26+%3D0%5C%5C+a_%7Bi%2B1%7D+%26+%3Da_%7Bi%7D%281-1%2Fk%29%2B%281%2B%5Cepsilon+%29%2Fk%3D%3Aa_%7Bi%7Db%2Bc.+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\begin{aligned} a_{0} &amp; =0\\ a_{i+1} &amp; =a_{i}(1-1/k)+(1+\epsilon )/k=:a_{i}b+c. \end{aligned}"/> </div>
<p/>
<p style="text-align: justify;">   This gives </p>
<div style="text-align: center;"> <img alt="\begin{aligned} a_{i}=c\sum _{j\le i}b{}^{j}=c\frac {1-b^{i+1}}{1-b}=(1+\epsilon )(1-b^{i+1}). \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+a_%7Bi%7D%3Dc%5Csum+_%7Bj%5Cle+i%7Db%7B%7D%5E%7Bj%7D%3Dc%5Cfrac+%7B1-b%5E%7Bi%2B1%7D%7D%7B1-b%7D%3D%281%2B%5Cepsilon+%29%281-b%5E%7Bi%2B1%7D%29.+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\begin{aligned} a_{i}=c\sum _{j\le i}b{}^{j}=c\frac {1-b^{i+1}}{1-b}=(1+\epsilon )(1-b^{i+1}). \end{aligned}"/> </div>
<p/>
<p style="text-align: justify;">   If it was <img alt="a'_{i+1}=a'_{i}+(1+\epsilon )/k" class="latex" src="https://s0.wp.com/latex.php?latex=a%27_%7Bi%2B1%7D%3Da%27_%7Bi%7D%2B%281%2B%5Cepsilon+%29%2Fk&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a'_{i+1}=a'_{i}+(1+\epsilon )/k"/> obviously <img alt="a'_{k}" class="latex" src="https://s0.wp.com/latex.php?latex=a%27_%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a'_{k}"/> would already be <img alt="1+\epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=1%2B%5Cepsilon+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1+\epsilon "/>. Instead for <img alt="a_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=a_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_{i}"/> we need to get to <img alt="k\log (1/\epsilon )" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Clog+%281%2F%5Cepsilon+%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k\log (1/\epsilon )"/>. </p>
<p style="text-align: justify;">   <b>My two cents.</b>. </p>
<p style="text-align: justify;">   I am not sure I need more evidence that making progress on long-standing bounds in complexity theory is hard, but I do find it interesting to prove these links; we have quite a few by now! The fact that we have been stuck forever just short of proving “the whole thing” makes me think that these long-sought bounds may in fact be false. Would love to be proved wrong, but it’s 2019, this connection is proved by balancing a tree better, and you feel confident that P <img alt="\ne " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cne+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ne "/> NP?    </p>
<h3 class="likesectionHead"><a id="x1-1000"/>References</h3>
<p style="text-align: justify;">
</p><div class="thebibliography">
<p class="bibitem"><span class="biblabel">  [AK10]<span class="bibsp">   </span></span><a id="XAllenderK10"/>Eric Allender and Michal Koucký.  Amplifying lower bounds by         means of self-reducibility. J. of the ACM, 57(3), 2010.         </p>
<p class="bibitem"><span class="biblabel">  [IPS97]<span class="bibsp">   </span></span><a id="XImpagliazzoPS97"/>Russell Impagliazzo, Ramamohan Paturi, and Michael E. Saks.         Size-depth  tradeoffs  for  threshold  circuits.    SIAM  J.  Comput.,         26(3):693–707, 1997. </p>
<p/></div></div>
    </content>
    <updated>2019-06-25T18:02:17Z</updated>
    <published>2019-06-25T18:02:17Z</published>
    <category term="Uncategorized"/>
    <category term="lower bounds"/>
    <category term="review"/>
    <author>
      <name>Emanuele</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2019-07-06T21:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1503</id>
    <link href="https://theorydish.blog/2019/06/24/on-the-importance-of-disciplinary-pride-for-multidisciplinary-collaboration/" rel="alternate" type="text/html"/>
    <title>On the Importance of Disciplinary Pride for Multidisciplinary Collaboration</title>
    <summary>    On the Importance of Disciplinary Pride for Multidisciplinary Collaboration I am a big fan of collaborations, even if they come with their own challenges. I always got further and enjoyed research much more because of my collaborators. I’m forever indebted to so many colleagues and dear, dear friends. Each and every one of them was better than me in some ways. To contribute, I had to remember my own strengths and bring them to the table. The premise of this post is that the same holds for collaboration between fields. It should be read as a call for theoreticians to bring the tools and the powerful way of thinking of TOC into collaborations. We shouldn’t be blind to the limitation of our field but obsessing on those limitations is misguided and would only limit our impact. Instead we should bring our best and trust on the other disciplines we collaborate with to do the same (allowing each to complement and compensate for the other). The context in which these thoughts came to my mind is Algorithmic Fairness. In this and other areas on the interface between society and computing, true collaboration is vital. Not surprisingly, attending multidisciplinary programs [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> </p>
<p> </p>
<p>On the Importance of Disciplinary Pride for Multidisciplinary Collaboration</p>
<p>I am a big fan of collaborations, even if they <a href="https://windowsontheory.org/2014/07/01/collaboration-competition-and-competition-within-collaboration/">come with their own challenges</a>. I always got further and enjoyed research much more because of my collaborators. I’m forever indebted to so many colleagues and dear, dear friends. Each and every one of them was better than me in some ways. To contribute, I had to remember my own strengths and bring them to the table. The premise of this post is that the same holds for collaboration between fields. It should be read as a call for theoreticians to bring the tools and the powerful way of thinking of TOC into collaborations. We shouldn’t be blind to the limitation of our field but obsessing on those limitations is misguided and would only limit our impact. Instead we should bring our best and trust on the other disciplines we collaborate with to do the same (allowing each to complement and compensate for the other).</p>
<p>The context in which these thoughts came to my mind is Algorithmic Fairness. In this and other areas on the interface between society and computing, true <a href="https://theorydish.blog/2018/11/01/simons-cluster-on-algorithmic-fairness/">collaboration is vital</a>. Not surprisingly, attending multidisciplinary programs on Algorithm Fairness, is a major part of my professional activities these days. And I love it – I get to learn so much from people and disciplines that have been thinking about fairness for many decades and centuries. In addition, the Humanities are simply splendid. Multidisciplinary collaborations come with even more challenges than other collaborations: the language, tools and perspectives are different. But for exactly the same reasons they can be even more rewarding. Nevertheless, my fear and the reason for this post is that my less experienced TOC colleagues might come out from those interdisciplinary meetings frustrated and might lose confidence in what TOC can contribute. It feels to me that <a href="http://www.wisdom.weizmann.ac.il/~oded/PDF/toc-sp2.pdf">old lessons</a> about the value of TOC need to be learned again. There is a lot to be proud of, and holding to this pride would in fact make us better collaborators not worse.</p>
<p>In the context of Algorithmic Fairness, we should definitely acknowledge (as we often do) that science exists within political structures, that algorithms are not objective and that mathematical definitions cannot replace social norms as expressed by policy makers. But let’s not take these as excuses for inaction and let’s not withdraw to the role of spectators. In this era of algorithms, other disciplines need us just as much as we need them .</p>
<p> </p></div>
    </content>
    <updated>2019-06-24T16:43:23Z</updated>
    <published>2019-06-24T16:43:23Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-07-06T21:21:28Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8803689043688761239</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8803689043688761239/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/are-you-smarter-than-5th-grade-amoeba.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8803689043688761239" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8803689043688761239" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/are-you-smarter-than-5th-grade-amoeba.html" rel="alternate" type="text/html"/>
    <title>Are you smarter than a 5th grade amoeba?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">(title of this blog is due to Henry Baker who posted an article about this elsewhere)<br/>
<br/>
Amoeba finds approx solution to TSP in linear time:<a href="https://phys.org/news/2018-12-amoeba-approximate-solutions-np-hard-problem.html">here</a>.<br/>
<br/>
Over the years we have seen models of computation that claim to solve NPC or other hard problems quickly. I ask non-rhetorically and with and open mind how they have panned out.<br/>
<br/>
In no particular order:<br/>
<br/>
1) Parallelism. For solving problems faster YES. For speeding up how to solve NPC problems I think YES. For making P=NP somehow NO.  Even so, parallel computers have been a definite practical success.<br/>
<br/>
2) Quantum Computing. Will they factor large numbers anytime soon? Ever? Should we view the effort to build them as an awesome and insightful Physics experiment? Are there any problems that they are NOW doing faster? Is Quantum Crypto (I know, not the same thing) actually used? Will other things of interest come out of the study of quantum computing? It already has, see <a href="https://theoryofcomputing.org/articles/gs002/gs002.pdf">here</a>.<br/>
<br/>
3) DNA computing. Did that lead to practical solutions to NPC problems? I do not think it did. Did that lead to interesting math? Interesting biology? Interesting science?  I do not know.<br/>
<br/>
4) Autistic  computing for finding primes: see <a href="https://blog.computationalcomplexity.org/2009/09/possibly-recruits-for-polymath-primes.html">here</a>. Oliver Sacks, the neurologist ,claimed that two autistic twin brothers could generate large primes quickly. This story was never put to a rigorous test and may not be quite right.<br/>
<br/>
5) Amoeba computing: Too early to tell. The article seems to say it succeeded on 8 cities<br/>
<br/>
The problem with all of these non-standard models of computing is SCALE.  And the more powerful classic computers get, the harder it is for these nonstandard models to compete.<br/>
<br/>
Are these models interesting even if they don't end up getting us fast algorithms? They can be:<br/>
<br/>
1) Do they lead to mathematics of interest? (Quantum- Yes, Parallelism- Yes)<br/>
<br/>
2) Did they inspire algorithms for classical computers? (Quantum- Yes)<br/>
<br/>
3) Do they give insight into other fields? (Quantum for Physics yes, DNA-computing for bio-??)<br/>
<br/>
4) Have they ACTUALLY sped up  up computations in meaningful ways for problems we care about (Parallelism  has)<br/>
<br/>
If  you know of any result which I missed<br/>
<br/>
 (e.g.,<br/>
<br/>
 Amoeba-computing giving insight into evolution,<br/>
<br/>
Autistic computing being used by the NSA to find primes,<br/>
<br/>
 DNA computing  leading to interesting mathematics)<br/>
<br/>
 then leave polite comments!<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-06-24T05:03:00Z</updated>
    <published>2019-06-24T05:03:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-07-06T08:23:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16034</id>
    <link href="https://rjlipton.wordpress.com/2019/06/23/computer-science-gender-gap/" rel="alternate" type="text/html"/>
    <title>Computer Science Gender Gap</title>
    <summary>NY Times article on the paper LinkedIn source Lucy Lu Wang is the lead author of a paper released this Friday on gender parity in computer science. The paper is from the Allen Institute for Artificial Intelligence. The authors are Wang, Gabriel Stanovsky, Luca Weihs, and Oren Etzioni. We will call them WSWE for short. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>NY Times article on the paper</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/23/computer-science-gender-gap/unknown-123/" rel="attachment wp-att-16036"><img alt="" class="alignright size-full wp-image-16036" src="https://rjlipton.files.wordpress.com/2019/06/unknown.jpeg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">LinkedIn <a href="https://www.linkedin.com/in/lucylw/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Lucy Lu Wang is the lead author of a paper released this Friday on gender parity in computer science. The <a href="https://arxiv.org/pdf/1906.07883.pdf">paper</a> is from the Allen Institute for Artificial Intelligence. The authors are Wang, Gabriel Stanovsky, Luca Weihs, and Oren Etzioni.  We will call them WSWE for short. </p>
<p>
Today we will discuss some of the issues this study raises.</p>
<p>
The paper was highlighted by the New York Times in an <a href="https://www.nytimes.com/2019/06/21/technology/gender-gap-tech-computer-science.html">article</a> titled, “The Gender Gap in Computer Science Research Won’t Close for 100 Years.”  The news article begins with an equally sobering statement of this conclusion:</p>
<blockquote><p><b> </b> <em>Women will not reach parity with men in writing published computer science research in this century if current trends hold, according to a study released on Friday. </em>
</p></blockquote>
<p/><p>
We are for gender-neutral opportunities and have always promoted this—see our earlier discussions <a href="https://rjlipton.wordpress.com/2010/03/23/its-ada-lovelace-day/">here</a> and <a href="https://rjlipton.wordpress.com/2016/10/29/absolute-firsts/">here</a>. We are for doing a better job in supporting women in computer science. The study by WSWE is an important paper that helps frame the problem. Quoting them:</p>
<blockquote><p><b> </b> <em> The field has made more of an effort to reach a more balanced gender status. But the data seems to show that even with all the progress, we are still not making the change fast enough. </em>
</p></blockquote>
<p/><p>
I suggest that you might wish to read the paper. Unfortunately there are many papers with similar conclusions—see <a href="https://aclweb.org/anthology/D18-1301">this</a> by Natalie Schluter, for example. </p>
<p>
</p><p/><h2> Main Points </h2><p/>
<p/><p>
Here are some points of the paper by WSWE:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>There is a measurable gap</i>. No one would, I believe, doubt this. But it is important to see that it is measurable. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>The gap is shrinking, but slowly</i>. Again this seems correct, but whether it is shrinking in all relevant measures of publication weight is still an issue.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>The predictions</i>. Perhaps it will not be closed for over a century. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>Modern technology allows such a study</i>. This is one aspect that we can all applaud. WSWE used automated tools that allowed this study to search millions of papers.</p>
<p>
</p><p/><h2> Issues </h2><p/>
<p>
WSWE filtered a corpus of <b>2.87 million</b> papers tagged as in computer science.  The volume constrained their approaches to handling several basic issues.</p>
<p/><p><br/>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> How to tell the gender of an author?  They use first names and try to detect gender from that alone. This is not easy.  Not only can differently-gendered names in different nations or language groups have the same Romanized form, many names apply to both genders within those groups.  The names Taylor and Kelley are perfect examples pf the latter. </p>
<p>
WSWE used a statistical weighing method. So “Taylor,” for example, would be weighted as 55 percent female, 45 percent male.  The weightings come from a large database called <i>Gender API</i> compiled from government and social agencies not directly related to computer science. </p>
<p/><p><br/>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> Another issue concerns the prediction part of their paper. They attempt to extrapolate and guess when there will be parity between female and male authorship. </p>
<p>
As all predictions this is not easy. It is my main complaint with this and other papers on the gender-gap issue. They predict that parity will not be reached until 2167, in 168 years. An earlier <a href="https://www.computerworld.com.au/article/640548/gender-parity-computer-science-could-take-280-years/">study</a> puts the parity point at 280 years away.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I believe that a major issue is hiring by computer science departments and other institutions. A major CS department just hired <img alt="{N&gt;1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%3E1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N&gt;1}"/> assistant professors, of which <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> were male. This is a problem. </p>
<p>
Should studies on the gender gap count all papers? Perhaps they should weight the papers by some citation indices. Are women writing more impactful papers? What percent of papers by gender have citations rate above X?—you get the idea. </p>
<p>
Finally I wonder if parity is the right goal? <b>How about aiming for more women papers than men</b>? Why not?</p>
<p/><p><br/>
[various formatting and word edits]</p></font></font></div>
    </content>
    <updated>2019-06-23T22:33:38Z</updated>
    <published>2019-06-23T22:33:38Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="gender gap"/>
    <category term="gender parity"/>
    <category term="parity"/>
    <category term="study"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-07-06T21:20:42Z</updated>
    </source>
  </entry>
</feed>
