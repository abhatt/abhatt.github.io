<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-09-13T20:22:19Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-09-13-synchronous-consensus-omission-faults/</id>
    <link href="https://decentralizedthoughts.github.io/2020-09-13-synchronous-consensus-omission-faults/" rel="alternate" type="text/html"/>
    <title>Using the Commit-Notify Paradigm for Synchronous Consensus with Omission Faults</title>
    <summary>We continue our series of posts on State Machine Replication (SMR). In this post, we move from consensus under crash failures to consensus under omission failures. We still keep the synchrony assumption. Let’s begin with a quick overview of what we covered in previous posts: Upper bound: We can tolerate...</summary>
    <updated>2020-09-13T19:09:00Z</updated>
    <published>2020-09-13T19:09:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-09-13T20:22:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17570</id>
    <link href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/" rel="alternate" type="text/html"/>
    <title>Convex Algorithms</title>
    <summary>Continuous can beat discrete Nisheeth Vishnoi is a professor at Yale University in the computer science department. The faculty there is impressive and includes many of the top researchers in the world. The CS faculty is pretty good too. As Nisheeth’s PhD advisor, years ago, I am proud that he is at Yale. Today I […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Continuous can beat discrete</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/nv/" rel="attachment wp-att-17573"><img alt="" class="alignright  wp-image-17573" src="https://rjlipton.files.wordpress.com/2020/09/nv.png?w=150" width="150"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Nisheeth Vishnoi is a professor at Yale University in the computer science department. The faculty <a href="https://cpsc.yale.edu/people/faculty">there</a> is impressive and includes many of the top researchers in the world. The CS faculty is pretty good too. As Nisheeth’s PhD advisor, years ago, I am proud that he is at Yale. </p>
<p>
Today I wish to discuss a new book by Nisheeth. </p>
<p>
The title is <a href="https://convex-optimization.github.io/ACO-v1.pdf">Algorithms for Convex Optimization</a>. Let me jump ahead and say that I like the book and especially this insight: </p>
<blockquote><p><b> </b> <em> <i>One way to solve discrete problems is to apply continuous methods.</i> </em>
</p></blockquote>
<p>This is not a new insight, but is an important one. Continuous math is older than discrete and often is more powerful. Some examples of this are:</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> Analytic number theory is <a href="https://en.wikipedia.org/wiki/Analytic_number_theory">based</a> on the behavior of continuous functions. Some of the deepest theorems on prime numbers use such methods. Think of the Riemann zeta function 	</p>
<p align="center"><img alt="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} = \frac{1}{1^s} + \frac{1}{2^s} + \frac{1}{3^s} + \cdots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Czeta%28s%29+%3D+%5Csum_%7Bn%3D1%7D%5E%5Cinfty+%5Cfrac%7B1%7D%7Bn%5Es%7D+%3D+%5Cfrac%7B1%7D%7B1%5Es%7D+%2B+%5Cfrac%7B1%7D%7B2%5Es%7D+%2B+%5Cfrac%7B1%7D%7B3%5Es%7D+%2B+%5Ccdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} = \frac{1}{1^s} + \frac{1}{2^s} + \frac{1}{3^s} + \cdots "/></p>
<p>as a function of complex numbers <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> Additive number theory is <a href="https://encyclopediaofmath.org/wiki/Additive_number_theory">based</a> on the behavior of continuous functions. Think of generating functions and Fourier methods. </p>
<p>
The power of continuous methods is one that I sometimes forget. Nisheeth’s book is a testament to the power of this idea. </p>
<p>
</p><p/><h2> Convexity </h2><p/>
<p/><p>
Nisheeth’s book uses another fundamental idea from complexity theory. This is: restrict problems in some way. Allowing too large a class usually makes complexity high. For example, trees are easier in general than planar graphs, and sparse graphs are easier than general graphs. Of course “in general” must be controlled, but restricting the problem types does often reduce complexity. </p>
<p>
Convexity adds to this tradition since <em>convex</em> generalizes the notion of <em>linear</em>. And convex problems of all kinds are abundant in practice, abundant in theory, and are important.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/mw/" rel="attachment wp-att-17574"><img alt="" class="aligncenter size-full wp-image-17574" src="https://rjlipton.files.wordpress.com/2020/09/mw.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
The MW dictionary says <a href="https://www.merriam-webster.com/dictionary/convex">convex</a> means: </p>
<blockquote><p><b> </b> <em> : being a continuous function or part of a continuous function with the property that a line joining any two points on its graph lies on or above the graph. </em>
</p></blockquote>
<p>
</p><p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/con2/" rel="attachment wp-att-17575"><img alt="" class="aligncenter size-full wp-image-17575" src="https://rjlipton.files.wordpress.com/2020/09/con2.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
Here is a passage by Roman Dwilewicz on the <a href="http://www.mathem.pub.ro/dgds/v11/D11-DW.pdf">history</a> of the convexity concept:</p>
<blockquote><p><b> </b> <em> It was known to the ancient Greeks that there are only five regular <i>convex</i> polyhedra. </em></p><em>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/solid/" rel="attachment wp-att-17576"><img alt="" class="aligncenter size-full wp-image-17576" src="https://rjlipton.files.wordpress.com/2020/09/solid.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>It seems that the first more rigorous definition of convexity was given by <a href="https://en.wikipedia.org/wiki/Archimedes">Archimedes</a> of Syracuse, (ca 287 – ca 212 B.C.) in his treatise: <a href="https://en.wikipedia.org/wiki/On_the_Sphere_and_Cylinder">On the sphere and cylinder</a>. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/arch/" rel="attachment wp-att-17578"><img alt="" class="aligncenter size-full wp-image-17578" src="https://rjlipton.files.wordpress.com/2020/09/arch.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
</em><p><em>
These definitions and postulates of Archimedes were dormant for about two thousand years! </em>
</p></blockquote>
<p>I say it’s lucky that Archimedes was not up for tenure.</p>
<p>
</p><p/><h2> Nisheeth’s Book </h2><p/>
<p/><p>
Nisheeth’s book is now available at this <a href="https://convex-optimization.github.io">site</a>. I have just started to examine it and must say I like the book. Okay, I am not an expert on convex algorithms, nor am I an expert on this type of geometric theory. But I definitely like his viewpoint. Let me explain in a moment. </p>
<p>
First I cannot resist adding some statistics about his book created <a href="https://countwordsfree.com/">here</a>:</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/stat/" rel="attachment wp-att-17579"><img alt="" class="aligncenter size-full wp-image-17579" src="https://rjlipton.files.wordpress.com/2020/09/stat.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
No way I can read the book in nine hours. But I like seeing how many characters and so on the book has. I will have to calculate the same for other books. </p>
<p>
</p><p/><h2> Discrete vs Continuous Methods </h2><p/>
<p/><p>
Nisheeth in his introduction explains how continuous methods help in many combinatorial problems, like finding flows on graphs. He uses the <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>—<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> flow problem as his example. The <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>—<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>-maximum flow problem arises in real-world scheduling problems, but is also a fundamental combinatorial problem that can be used to find a maximum matching in a bipartite graph, for example.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/flow-3/" rel="attachment wp-att-17581"><img alt="" class="aligncenter size-full wp-image-17581" src="https://rjlipton.files.wordpress.com/2020/09/flow.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
<i>Combinatorial algorithms for the maximum flow problem</i>. He points out that by building on the Ford-Fulkerson method, various polynomial-time results were proved and other bounds were improved. But he states that the <i>improvements stopped in 1998.</i> Discrete methods seem to be unable to improve complexity for flow problems. </p>
<p>
<i>Convex programming-based algorithms</i>. He adds: </p>
<blockquote><p><b> </b> <em> Starting with the <a href="https://arxiv.org/pdf/1010.2921.pdf">paper</a> by Paul Christiano, Jonathan Kelner, Aleksander Mądry, Daniel Spielman, the last decade has seen striking progress on the <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{s}"/>–<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{t}"/> maximum flow problem. One of the keys to this success has been to abandon combinatorial approaches and view the <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{s}"/>–<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{t}"/> maximum flow problem through the lens of continuous optimization.</em></p><em>
</em><p><em>
Thus, at this point it may seem like we are heading in the wrong direction. We started off with a combinatorial problem that is a special type of a linear programming problem, and here we are with a nonlinear optimization formulation for it. Thus the questions arise: which formulation should we chose? and, why should this convex optimization approach lead us to faster algorithms? </em>
</p></blockquote>
<p>Indeed. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Take a look at Nisheeth’s <a href="https://convex-optimization.github.io">site</a> for the answers.</p>
<p>
I wish I were better informed about continuous methods in general. They are powerful and pretty. Maybe I could solve an open problem that I have thought about if I knew this material better. Hmmm. Maybe it will help you solve some open problem of your own. Take a look at his book.</p></font></font></div>
    </content>
    <updated>2020-09-13T18:27:14Z</updated>
    <published>2020-09-13T18:27:14Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Algorithms"/>
    <category term="continuous"/>
    <category term="convexity"/>
    <category term="discrete mathematics"/>
    <category term="flow"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-13T20:20:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/138</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/138" rel="alternate" type="text/html"/>
    <title>TR20-138 |  Pseudorandom Generators for Unbounded-Width Permutation Branching Programs | 

	William Hoza, 

	Edward Pyne, 

	Salil Vadhan</title>
    <summary>We prove that the Impagliazzo-Nisan-Wigderson (STOC 1994) pseudorandom generator (PRG) fools ordered (read-once) permutation branching programs of unbounded width with a seed length of $\widetilde{O}(\log d + \log n \cdot \log(1/\varepsilon))$, assuming the program has only one accepting vertex in the final layer. Here, $n$ is the length of the program, $d$ is the degree (equivalently, the alphabet size), and $\varepsilon$ is the error of the PRG. In contrast, we show that a randomly chosen generator requires seed length $\Omega(n \log d)$ to fool such unbounded-width programs. Thus, this is an unusual case where an explicit construction is "better than random."

Except when the program's width $w$ is very small, this is an improvement over prior work. For example, when $w = \text{poly}(n)$ and d = 2, the best prior PRG for permutation branching programs was simply Nisan's PRG (Combinatorica 1992), which fools general ordered branching programs with seed length $O(\log(wn/\varepsilon) \log n)$. We prove a seed length lower bound of $\widetilde{\Omega}(\log d + \log n \cdot \log(1/\varepsilon))$ for fooling these unbounded-width programs, showing that our seed length is near-optimal. In fact, when $\varepsilon \leq 1 / \log n$, our seed length is within a constant factor of optimal. Our analysis of the INW generator uses the connection between the PRG and the derandomized square of Rozenman and Vadhan (RANDOM 2005) and the recent analysis of the latter in terms of unit-circle approximation by Ahmadinejad et al. (FOCS 2020).</summary>
    <updated>2020-09-13T03:39:42Z</updated>
    <published>2020-09-13T03:39:42Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-13T20:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.05039</id>
    <link href="http://arxiv.org/abs/2009.05039" rel="alternate" type="text/html"/>
    <title>On Light Spanners, Low-treewidth Embeddings and Efficient Traversing in Minor-free Graphs</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen=Addad:Vincent.html">Vincent Cohen-Addad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Arnold.html">Arnold Filtser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klein:Philip_N=.html">Philip N. Klein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Le:Hung.html">Hung Le</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.05039">PDF</a><br/><b>Abstract: </b>Understanding the structure of minor-free metrics, namely shortest path
metrics obtained over a weighted graph excluding a fixed minor, has been an
important research direction since the fundamental work of Robertson and
Seymour. A fundamental idea that helps both to understand the structural
properties of these metrics and lead to strong algorithmic results is to
construct a "small-complexity" graph that approximately preserves distances
between pairs of points of the metric. We show the two following structural
results for minor-free metrics:
</p>
<p>1. Construction of a light subset spanner. Given a subset of vertices called
terminals, and $\epsilon$, in polynomial time we construct a subgraph that
preserves all pairwise distances between terminals up to a multiplicative
$1+\epsilon$ factor, of total weight at most $O_{\epsilon}(1)$ times the weight
of the minimal Steiner tree spanning the terminals.
</p>
<p>2. Construction of a stochastic metric embedding into low treewidth graphs
with expected additive distortion $\epsilon D$. Namely, given a minor free
graph $G=(V,E,w)$ of diameter $D$, and parameter $\epsilon$, we construct a
distribution $\mathcal{D}$ over dominating metric embeddings into
treewidth-$O_{\epsilon}(\log n)$ graphs such that the additive distortion is at
most $\epsilon D$.
</p>
<p>One of our important technical contributions is a novel framework that allows
us to reduce \emph{both problems} to problems on simpler graphs of bounded
diameter. Our results have the following algorithmic consequences: (1) the
first efficient approximation scheme for subset TSP in minor-free metrics; (2)
the first approximation scheme for vehicle routing with bounded capacity in
minor-free metrics; (3) the first efficient approximation scheme for vehicle
routing with bounded capacity on bounded genus metrics.
</p></div>
    </summary>
    <updated>2020-09-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04992</id>
    <link href="http://arxiv.org/abs/2009.04992" rel="alternate" type="text/html"/>
    <title>Near-linear Size Hypergraph Cut Sparsifiers</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Yu.html">Yu Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khanna:Sanjeev.html">Sanjeev Khanna</a>, Ansh Nagda <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04992">PDF</a><br/><b>Abstract: </b>Cuts in graphs are a fundamental object of study, and play a central role in
the study of graph algorithms. The problem of sparsifying a graph while
approximately preserving its cut structure has been extensively studied and has
many applications. In a seminal work, Bencz\'ur and Karger (1996) showed that
given any $n$-vertex undirected weighted graph $G$ and a parameter $\varepsilon
\in (0,1)$, there is a near-linear time algorithm that outputs a weighted
subgraph $G'$ of $G$ of size $\tilde{O}(n/\varepsilon^2)$ such that the weight
of every cut in $G$ is preserved to within a $(1 \pm \varepsilon)$-factor in
$G'$. The graph $G'$ is referred to as a {\em $(1 \pm \varepsilon)$-approximate
cut sparsifier} of $G$.
</p>
<p>A natural question is if such cut-preserving sparsifiers also exist for
hypergraphs. Kogan and Krauthgamer (2015) initiated a study of this question
and showed that given any weighted hypergraph $H$ where the cardinality of each
hyperedge is bounded by $r$, there is a polynomial-time algorithm to find a $(1
\pm \varepsilon)$-approximate cut sparsifier of $H$ of size
$\tilde{O}(\frac{nr}{\varepsilon^2})$. Since $r$ can be as large as $n$, in
general, this gives a hypergraph cut sparsifier of size
$\tilde{O}(n^2/\varepsilon^2)$, which is a factor $n$ larger than the
Bencz\'ur-Karger bound for graphs. It has been an open question whether or not
Bencz\'ur-Karger bound is achievable on hypergraphs. In this work, we resolve
this question in the affirmative by giving a new polynomial-time algorithm for
creating hypergraph sparsifiers of size $\tilde{O}(n/\varepsilon^2)$.
</p></div>
    </summary>
    <updated>2020-09-12T23:22:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04979</id>
    <link href="http://arxiv.org/abs/2009.04979" rel="alternate" type="text/html"/>
    <title>Quick Streaming Algorithms for Maximization of Monotone Submodular Functions in Linear Time</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhnle:Alan.html">Alan Kuhnle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04979">PDF</a><br/><b>Abstract: </b>We consider the problem of monotone, submodular maximization over a ground
set of size $n$ subject to cardinality constraint $k$. For this problem, we
introduce streaming algorithms with linearquery complexity and linear number of
arithmetic operations; these algorithms are the first deterministic algorithms
for submodular maximization that require a linear number of arithmetic
operations. Specifically, for any $c \ge 1, \epsilon &gt; 0$, we propose a
single-pass, deterministic streaming algorithm with ratio $1/(4c)-\epsilon$,
query complexity $\lceil n / c \rceil + c$, memory complexity $O(k \log k)$,
and $O(n)$ total running time. As $k \to \infty$, the ratio converges to $(1 -
1/e)/(c + 1)$. In addition, we propose a deterministic, multi-pass streaming
algorithm with $O(1 / \epsilon)$ passes that achieves ratio $1-1/e - \epsilon$
in $O(n/\epsilon)$ queries, $O(k \log (k))$ memory, and $O(n)$ time. We prove a
lower bound that implies no constant-factor approximation exists using $o(n)$
queries, even if queries to infeasible sets are allowed. An experimental
analysis demonstrates that our algorithms require fewer queries (often
substantially less than $n$) to achieve better objective value than the current
state-of-the-art algorithms.
</p></div>
    </summary>
    <updated>2020-09-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04942</id>
    <link href="http://arxiv.org/abs/2009.04942" rel="alternate" type="text/html"/>
    <title>Revisiting Tardos's Framework for Linear Programming: Faster Exact Solutions using Approximate Solvers</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dadush:Daniel.html">Daniel Dadush</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Natura:Bento.html">Bento Natura</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/V=eacute=gh:L=aacute=szl=oacute=_A=.html">László A. Végh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04942">PDF</a><br/><b>Abstract: </b>In breakthrough work, Tardos (Oper. Res. '86) gave a proximity based
framework for solving linear programming (LP) in time depending only on the
constraint matrix in the bit complexity model. In Tardos's framework, one
reduces solving the LP $\min \langle c,{x}\rangle$, $Ax=b$, $x \geq 0$, $A \in
\mathbb{Z}^{m \times n}$, to solving $O(nm)$ LPs in $A$ having small integer
coefficient objectives and right-hand sides using any exact LP algorithm. This
gives rise to an LP algorithm in time poly$(n,m\log\Delta_A)$, where $\Delta_A$
is the largest subdeterminant of $A$. A significant extension to the real model
of computation was given by Vavasis and Ye (Math. Prog. '96), giving a
specialized interior point method that runs in time poly$(n,m,\log\bar\chi_A)$,
depending on Stewart's $\bar{\chi}_A$, a well-studied condition number.
</p>
<p>In this work, we extend Tardos's original framework to obtain such a running
time dependence. In particular, we replace the exact LP solves with approximate
ones, enabling us to directly leverage the tremendous recent algorithmic
progress for approximate linear programming. More precisely, we show that the
fundamental "accuracy" needed to exactly solve any LP in $A$ is inverse
polynomial in $n$ and $\log\bar{\chi}_A$. Plugging in the recent algorithm of
van den Brand (SODA '20), our method computes an optimal primal and dual
solution using ${O}(m n^{\omega+1} \log (n)\log(\bar{\chi}_A+n))$ arithmetic
operations, outperforming the specialized interior point method of Vavasis and
Ye and its recent improvement by Dadush et al (STOC '20).
</p>
<p>At a technical level, our framework combines together approximate LP
solutions to compute exact ones, making use of constructive proximity theorems
-- which bound the distance between solutions of "nearby" LPs -- to keep the
required accuracy low.
</p></div>
    </summary>
    <updated>2020-09-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04938</id>
    <link href="http://arxiv.org/abs/2009.04938" rel="alternate" type="text/html"/>
    <title>Dune-CurvedSurfaceGrid -- A Dune module for surface parametrization</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Praetorius:Simon.html">Simon Praetorius</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stenger:Florian.html">Florian Stenger</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04938">PDF</a><br/><b>Abstract: </b>In this paper we introduce and describe an implementation of curved surface
geometries within the Dune framework for grid-based discretizations. Therefore,
we employ the abstraction of geometries as local-functions bound to a grid
element, and the abstraction of a grid as connectivity of elements together
with a grid-function that can be localized to the elements to provide element
local parametrizations of the curved surface.
</p></div>
    </summary>
    <updated>2020-09-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04892</id>
    <link href="http://arxiv.org/abs/2009.04892" rel="alternate" type="text/html"/>
    <title>Toward Probabilistic Checking against Non-Signaling Strategies with Constant Locality</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Mohammad Mahdi Jahanara, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koroth:Sajin.html">Sajin Koroth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shinkar:Igor.html">Igor Shinkar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04892">PDF</a><br/><b>Abstract: </b>Non-signaling strategies are a generalization of quantum strategies that have
been studied in physics over the past three decades. Recently, they have found
applications in theoretical computer science, including to proving
inapproximability results for linear programming and to constructing protocols
for delegating computation. A central tool for these applications is
probabilistically checkable proof (PCPs) systems that are sound against
non-signaling strategies.
</p>
<p>In this paper we show, assuming a certain geometrical hypothesis about noise
robustness of non-signaling proofs (or, equivalently, about robustness to noise
of solutions to the Sherali-Adams linear program), that a slight variant of the
parallel repetition of the exponential-length constant-query PCP construction
due to Arora et al. (JACM 1998) is sound against non-signaling strategies with
constant locality.
</p>
<p>Our proof relies on the analysis of the linearity test and agreement test
(also known as the direct product test) in the non-signaling setting.
</p></div>
    </summary>
    <updated>2020-09-12T23:20:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04827</id>
    <link href="http://arxiv.org/abs/2009.04827" rel="alternate" type="text/html"/>
    <title>A Normal Sequence Compressed by PPM$^*$ but not by Lempel-Ziv 78</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jordon:Liam.html">Liam Jordon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moser:Philippe.html">Philippe Moser</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04827">PDF</a><br/><b>Abstract: </b>In this paper we compare the difference in performance of two of the
Prediction by Partial Matching (PPM) family of compressors (PPM$^*$ and the
original Bounded PPM algorithm) and the Lempel-Ziv 78 (LZ) algorithm. We
construct an infinite binary sequence whose worst-case compression ratio for
PPM$^*$ is $0$, while Bounded PPM's and LZ's best-case compression ratios are
at least $1/2$ and $1$ respectively. This sequence is an enumeration of all
binary strings in order of length, i.e. all strings of length $1$ followed by
all strings of length $2$ and so on. It is therefore normal, and is built using
repetitions of de Bruijn strings of increasing order
</p></div>
    </summary>
    <updated>2020-09-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04821</id>
    <link href="http://arxiv.org/abs/2009.04821" rel="alternate" type="text/html"/>
    <title>Pushdown and Lempel-Ziv Depth</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jordon:Liam.html">Liam Jordon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moser:Philippe.html">Philippe Moser</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04821">PDF</a><br/><b>Abstract: </b>This paper expands upon existing and introduces new formulations of Bennett's
logical depth. In previously published work by Jordon and Moser, notions of
finite-state-depth and pushdown-depth were examined and compared. These were
based on finite-state transducers and information lossless pushdown compressors
respectively. Unfortunately a full separation between the two notions was not
established. This paper introduces a new formulation of pushdown-depth based on
restricting how fast a pushdown compressor's stack can grow. This improved
formulation allows us to do a full comparison by demonstrating the existence of
sequences with high finite-state-depth and low pushdown-depth, and vice-versa.
A new notion based on the Lempel-Ziv `78 algorithm is also introduced. Its
difference from finite-state-depth is shown by demonstrating the existence of a
Lempel-Ziv deep sequence that is not finite-state deep and vice versa.
Lempel-Ziv-depth's difference from pushdown-depth is shown by building
sequences that have a pushdown-depth of roughly $1/2$ but low Lempel-Ziv depth,
and a sequence with high Lempel-Ziv depth but low pushdown-depth. Properties of
all three notions are also discussed and proved.
</p></div>
    </summary>
    <updated>2020-09-12T23:20:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04742</id>
    <link href="http://arxiv.org/abs/2009.04742" rel="alternate" type="text/html"/>
    <title>Backtracking algorithms for constructing the Hamiltonian decomposition of a 4-regular multigraph</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Alexander V. Korostil, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nikolaev:Andrei_V=.html">Andrei V. Nikolaev</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04742">PDF</a><br/><b>Abstract: </b>We consider a Hamiltonian decomposition problem of partitioning a regular
graph into edge-disjoint Hamiltonian cycles. It is known that verifying vertex
nonadjacency in the 1-skeleton of the symmetric and asymmetric traveling
salesperson polytopes is an NP-complete problem. On the other hand, a
sufficient condition for two vertices to be nonadjacent can be formulated as a
combinatorial problem of finding a Hamiltonian decomposition of a 4-regular
multigraph. We present two backtracking algorithms for verifying vertex
nonadjacency in the 1-skeleton of the traveling salesperson polytope and
constructing a Hamiltonian decomposition: an algorithm based on a simple path
extension and an algorithm based on the chain edge fixing procedure. According
to the results of computational experiments for undirected multigraphs, both
backtracking algorithms lost to the known heuristic general variable
neighborhood search algorithm. However, for directed multigraphs, the algorithm
based on chain edge fixing showed comparable results with heuristics on
instances with the existing solution and better results on instances of the
problem where the Hamiltonian decomposition does not exist.
</p></div>
    </summary>
    <updated>2020-09-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04636</id>
    <link href="http://arxiv.org/abs/2009.04636" rel="alternate" type="text/html"/>
    <title>A performance study of some approximation algorithms for minimum dominating set in a graph</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Jonathan S. Li, Rohan Potru, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shahrokhi:Farhad.html">Farhad Shahrokhi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04636">PDF</a><br/><b>Abstract: </b>We implement and test the performances of several approximation algorithms
for computing the minimum dominating set of a graph. These algorithms are the
standard greedy algorithm, the recent LP rounding algorithms and a hybrid
algorithm that we design by combining the greedy and LP rounding algorithms.
All algorithms perform better than anticipated in their theoretical analysis,
and have small performance ratios, measured as the size of output divided by
the LP objective lower-bound. However, each may have advantages over the
others. For instance, LP rounding algorithm normally outperforms the other
algorithms on sparse real-world graphs. On a graph with 400,000+ vertices, LP
rounding took less than 15 seconds of CPU time to generate a solution with
performance ratio 1.011, while the greedy and hybrid algorithms generated
solutions of performance ratio 1.12 in similar time. For synthetic graphs, the
hybrid algorithm normally outperforms the others, whereas for hypercubes and
k-Queens graphs, greedy outperforms the rest. Another advantage of the hybrid
algorithm is to solve very large problems where LP solvers crash, as
demonstrated on a real-world graph with 7.7 million+ vertices.
</p></div>
    </summary>
    <updated>2020-09-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04567</id>
    <link href="http://arxiv.org/abs/2009.04567" rel="alternate" type="text/html"/>
    <title>Diverse Pairs of Matchings</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovach:Petr_A=.html">Petr A. Golovach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaffke:Lars.html">Lars Jaffke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Philip:Geevarghese.html">Geevarghese Philip</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sagunov:Danil.html">Danil Sagunov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04567">PDF</a><br/><b>Abstract: </b>We initiate the study of the Diverse Pair of (Maximum/ Perfect) Matchings
problems which given a graph $G$ and an integer $k$, ask whether $G$ has two
(maximum/perfect) matchings whose symmetric difference is at least $k$. Diverse
Pair of Matchings (asking for two not necessarily maximum or perfect matchings)
is NP-complete on general graphs if $k$ is part of the input, and we consider
two restricted variants. First, we show that on bipartite graphs, the problem
is polynomial-time solvable, and second we show that Diverse Pair of Maximum
Matchings is FPT parameterized by $k$. We round off the work by showing that
Diverse Pair of Matchings has a kernel on $\mathcal{O}(k^2)$ vertices.
</p></div>
    </summary>
    <updated>2020-09-12T23:38:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.04556</id>
    <link href="http://arxiv.org/abs/2009.04556" rel="alternate" type="text/html"/>
    <title>Sensitivity Analysis of the Maximum Matching Problem</title>
    <feedworld_mtime>1599868800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoshida:Yuichi.html">Yuichi Yoshida</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.04556">PDF</a><br/><b>Abstract: </b>We consider the sensitivity of algorithms for the maximum matching problem
against edge and vertex modifications. Algorithms with low sensitivity are
desirable because they are robust to edge failure or attack. In this work, we
show a randomized $(1-\epsilon)$-approximation algorithm with worst-case
sensitivity $O_{\epsilon}(1)$, which substantially improves upon the
$(1-\epsilon)$-approximation algorithm of Varma and Yoshida (arXiv 2020) that
obtains average sensitivity $n^{O(1/(1+\epsilon^2))}$ sensitivity algorithm,
and show a deterministic $1/2$-approximation algorithm with sensitivity
$\exp(O(\log^*n))$ for bounded-degree graphs. We show that any deterministic
constant-factor approximation algorithm must have sensitivity $\Omega(\log^*
n)$. Our results imply that randomized algorithms are strictly more powerful
than deterministic ones in that the former can achieve sensitivity independent
of $n$ whereas the latter cannot. We also show analogous results for vertex
sensitivity, where we remove a vertex instead of an edge. As an application of
our results, we give an algorithm for the online maximum matching with
$O_{\epsilon}(n)$ total replacements in the vertex-arrival model. By
comparison, Bernstein et al. (J. ACM 2019) gave an online algorithm that always
outputs the maximum matching, but only for bipartite graphs and with $O(n\log
n)$ total replacements.
</p>
<p>Finally, we introduce the notion of normalized weighted sensitivity, a
natural generalization of sensitivity that accounts for the weights of deleted
edges. We show that if all edges in a graph have polynomially bounded weight,
then given a trade-off parameter $\alpha&gt;2$, there exists an algorithm that
outputs a $\frac{1}{4\alpha}$-approximation to the maximum weighted matching in
$O(m\log_{\alpha} n)$ time, with normalized weighted sensitivity $O(1)$. See
paper for full abstract.
</p></div>
    </summary>
    <updated>2020-09-12T23:21:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/137</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/137" rel="alternate" type="text/html"/>
    <title>TR20-137 |  Deterministic and Efficient Interactive Coding from Hard-to-Decode Tree Codes | 

	Zvika Brakerski, 

	Yael Tauman Kalai, 

	Raghuvansh Saxena</title>
    <summary>The field of Interactive Coding studies how an interactive protocol can be made resilient to channel errors. Even though this field has received abundant attention since Schulman's seminal paper (FOCS 92), constructing interactive coding schemes that are both deterministic and efficient, and at the same time resilient to adversarial errors (with constant information and error rates), remains an elusive open problem.

An appealing approach towards resolving this problem is to show efficiently encodable and decodable constructions of a combinatorial object called tree codes (Schulman, STOC 93). After a lot of effort in this direction, the current state of the art has deterministic constructions of tree codes that are efficiently encodable but require a logarithmic (instead of constant) alphabet (Cohen, Haeupler, and Schulman, STOC 18). However, we still lack (even heuristic) candidate constructions that are efficiently decodable. 

In this work, we show that tree codes that are efficiently encodable, {\em but not efficiently decodable}, also imply deterministic and efficient interactive coding schemes that are resilient to adversarial errors. Our result immediately implies a deterministic and efficient interactive coding scheme with a logarithmic alphabet (i.e., $1/\log \log$ rate). We show this result using a novel implementation of hashing through deterministic tree codes that is powerful enough to yield interactive coding schemes.</summary>
    <updated>2020-09-11T16:55:01Z</updated>
    <published>2020-09-11T16:55:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-13T20:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/136</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/136" rel="alternate" type="text/html"/>
    <title>TR20-136 |  Explicit and structured sum of squares lower bounds from high dimensional expanders | 

	Irit Dinur, 

	Yuval Filmus, 

	Prahladh Harsha, 

	Madhur Tulsiani</title>
    <summary>We construct an explicit family of 3XOR instances which is hard for Omega(sqrt(log n)) levels of the Sum-of-Squares hierarchy. In contrast to earlier constructions, which involve a random component, our systems can be constructed explicitly in deterministic polynomial time.
Our construction is based on the high-dimensional expanders devised by Lubotzky, Samuels and Vishne, known as LSV complexes or Ramanujan complexes, and our analysis is based on two notions of expansion for these complexes: cosystolic expansion, and a local isoperimetric inequality due to Gromov.
Our construction offers an interesting contrast to the recent work of Alev, Jeronimo and the last author (FOCS 2019). They showed that 3XOR instances in which the variables correspond to vertices in a high-dimensional expander are easy to solve. In contrast, in our instances the variables correspond to the edges of the complex.</summary>
    <updated>2020-09-11T04:58:17Z</updated>
    <published>2020-09-11T04:58:17Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-13T20:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17542</id>
    <link href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/" rel="alternate" type="text/html"/>
    <title>Hybrid Versus Remote Teaching</title>
    <summary>Which is best for students? Cropped from Wikipedia src Moshe Vardi holds multiple professorships at Rice University. He is also the Senior Editor of Communications of the ACM. His is therefore a voice to be reckoned with in the current debate over how best to teach during the pandemic. Much of the debate is over […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Which is best for students?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/330px-moshe_vardi_img_0010/" rel="attachment wp-att-17544"><img alt="" class="alignright wp-image-17544" height="150" src="https://rjlipton.files.wordpress.com/2020/09/330px-moshe_vardi_img_0010.jpg?w=121&amp;h=150" width="121"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Wikipedia <a href="https://en.wikipedia.org/wiki/Moshe_Vardi#/media/File:Moshe_Vardi_IMG_0010.jpg">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Moshe Vardi holds multiple professorships at Rice University. He is also the Senior Editor of <em>Communications of the ACM</em>. His is therefore a voice to be reckoned with in the current debate over how best to teach during the pandemic. Much of the debate is over whether all should hear his voice the same way, or some hear it in the classroom while others hear it remotely. </p>
<p>
Today we note his recent <a href="https://medium.com/@vardi/covid-19-the-ford-pinto-and-american-higher-ed-2b191920b065">column</a> for <em>Medium</em> advocating the former. Then I (Ken) give some of my own impressions.</p>
<p>
His September 5 column followed an August 8 <a href="https://www.ricethresher.org/article/2020/08/return-to-campus-but-to-what-end">opinion</a> given to the Rice student newspaper. Both begin with concern over the conflict between <em>safety</em> and <em>value</em> for students. Much of the value of college—<em>most</em> according to statistics he cites—comes from being collegial: outside the classroom. But many such activities, not only evening parties but informal games and gatherings, are the most unsafe. </p>
<p>
We will focus however on what Moshe says about the nature of instruction for lecture courses. Certainly for laboratory courses there is a sharp trade-off between safety and in-person interaction. But we focus here on what he says about the nature of teaching in the lecture hall, where one can take safety as a given requirement. </p>
<p>
</p><p/><h2> An In-Person Remoteness Paradox </h2><p/>
<p/><p>
I have just returned from sabbatical at the University at Buffalo (UB) and am teaching this fall a small elective 4xx/5xx theory course. It has 15 students, smaller than the 25 in the hypothetical class Moshe describes but of the same order of magnitude. In the spring I will be teaching a larger undergraduate course which is also on target for his concerns. I have taught such a class every spring for a decade. While this assignment is not a new to me, the issue of safety raises tough choices about the delivery options. My options are: </p>
<ol>
<li>
<em>remote-only</em>; <p/>
</li><li>
<em>in-person only</em>; <p/>
</li><li>
<em>hybrid</em> use of 1 and 2 for designated course components; <p/>
</li><li>
<em>hybrid-flexible</em>, meaning 1 and 2 are conducted simultaneously with students free to choose either option, even on a per-lecture basis.
</li></ol>
<p>
I have committed to hybrid-flexible. For my current fall course, I made this commitment in early summer when there was uncertainty over in-person instruction requirements for student visas and registration. I believe that my larger course will be implemented as safely in a large room as my current course. The question is quality.</p>
<p>
Moshe notes right away a paradox for his hypothetical class that could apply to any of modes 2–4; to include the last expressly, I’ve inserted the word “even”:</p>
<blockquote><p> <em> …I realized that [even] the students <b>in the classroom</b> will have to be communicating with me on Zoom, to be heard and recorded. All this, while both the students and I are wearing face masks. It dawned on me then that I will be conducting remote teaching <b>in the classroom</b>. </em>
</p></blockquote>
<p/><p/>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/lecture/" rel="attachment wp-att-17545"><img alt="" class="size-full wp-image-17545" src="https://rjlipton.files.wordpress.com/2020/09/lecture.jpg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><em>Business Insider</em> <a href="https://www.businessinsider.com/us-colleges-shutting-down-coronavirus-impact-when-classes-move-online-2020-3">source</a>—yet another variation</font>
</td>
</tr>
</tbody></table>
<p>
In fact, I have one volunteer now in the room logging into Zoom to help with interaction from those attending remotely. This helps because my podium has less space to catch faces and detect questions right away. I do repeat questions so they are picked up in the recording and often redirect them to the class. Still, the mere fact of my not seeing faces alongside the notes and interactive drawings I am sharing makes me feel Moshe’s paradox all the time. This is even though my room allows denser spacing than at Rice, so a class of 25 could sit closer.  Let me, however, say why I love stand-up teaching before addressing his paramount question of what is best for the students at this time.</p>
<p>
</p><p/><h2> From Whiteboards to Tutorials </h2><p/>
<p/><p>
Dick once wrote a <a href="https://rjlipton.wordpress.com/2013/11/07/in-praise-of-chalk-talks/">post</a>, “In Praise of Chalk Talks.” First, with reference to talks pre-made using PowerPoint or LaTeX slides, Dick wrote:</p>
<blockquote><p><b> </b> <em> Such talks can be informative and easy to follow, yet sometimes PowerPoint is not well suited to giving a proof. The slides do not hold enough <b>state</b> for us to easily follow the argument. </em>
</p></blockquote>
<p/><p>
Moreover, when I contributed to the open-problems session of the workshop at IAS in Princeton, NJ that we <a href="https://rjlipton.wordpress.com/2018/06/06/princeton-is-invariant/">covered</a> two years ago, Avi Wigderson insisted that everyone use chalk, not slides. I’ve used slides for UB’s data-structures and programming languages courses, but I think students benefit from seeing proofs and problem-solving ideas <em>grow</em>.</p>
<p>
I find furthermore that the feel of immersion in a process of discovery is enhanced by an in-person presence. I had this in mind when I followed Dick’s post with a long one <a href="https://rjlipton.wordpress.com/2013/11/15/the-graph-of-math/">imagining</a> Kurt Gödel expounding the distinctive points of his set theory (joint with Paul Bernays and John von Neumann), all on one chalkboard. My classes are not as interactive as in that post, but I prepare junctures in lectures for posing questions and doing a little bit of Socratic method. And I try to lead this with body language as well as voice inflection, whether at a whiteboard or drawing on hard paper via a document camera. </p>
<p>
Still, it exactly this “extra” that gets diminished for those who are remote. When I share my screen for notes or a drawing (both in <a href="https://www.mathcha.io/">MathCha</a>), they see my movements only in a small second window if at all. They do hear my voice—but I do not hear theirs even if they unmute themselves. Nor can I read their state of following as I do in the room. Without reiterating the safety factor as Moshe does, I can reformulate his key question as:</p>
<blockquote><p><b> </b> <em> Does the non-uniformity and inequality of hybrid delivery outweigh the benefits of making in-person instruction available to some? </em>
</p></blockquote>
<p/><p>
I must quickly add that in-person teaching is perceived as a collective need at UB. The web form I filled for Spring 2021 stated that some in-person classes must be available at all levels, 1xx through 7xx. I am happy to oblige. But the fact that I chose a flexible structure, especially in a small class, does allow the students to give opinion on this question, as well as on something Moshe says next:</p>
<blockquote><p><b> </b> <em> “Remote teaching” actually can do a better job of reproducing the intimacy that we take for granted in small classes. </em>
</p></blockquote>
<p/><p>
Toward this end, I am implementing a remote version of the <a href="https://en.wikipedia.org/wiki/Tutorial_system">tutorial system</a> I was part of for two eight-week terms at Oxford while a junior fellow of Merton College. When Cambridge University <a href="https://www.bbc.com/news/education-52732814">declared</a> already last May that there would be no in-person lectures all the way through summer 2021, this is because most lectures there are formally optional anyway. The heart of required teaching is via weekly tutorial hours in groups of one-to-three students. They are organized separately by each of thirty-plus constituent colleges rather than by department-centered staff, so the numbers are divided to be manageable. In my math-course tutorials the expectation was for each student to present a solved problem and participate in discussions that build on the methods. </p>
<p>
I am doing this every other week this fall, alternating with weeks of problem-set review that will be strictly optional and classed as enhanced office hours. All UB office hours must be remote anyway. The tutorial requirement was agreed by student voice-vote in a tradeoff with lowering the material in timed exams to compensate for differences in home situations. After a few weeks of this, the class will take stock for opinions on which delivery options work best. UB has already committed to being remote-only after Thanksgiving, and it is possible that the on-campus medical situation will trigger an earlier conversion anyway.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We would like to throw the floor open for comment on Moshe’s matters that we’ve highlighted and on his other opinions about the university mission amid the current crisis more generally. </p>
<p>
[edited to reflect that at Rice too, the hypothetical class could be in any of modes 2–4, and that spacing is further than in my UB room.  “Princeton IAS” -&gt; “IAS in Princeton, NJ”]</p></font></font></div>
    </content>
    <updated>2020-09-10T22:26:40Z</updated>
    <published>2020-09-10T22:26:40Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Teaching"/>
    <category term="education quality"/>
    <category term="hybrid instruction"/>
    <category term="Moshe Vardi"/>
    <category term="online courses"/>
    <category term="pandemic"/>
    <category term="remote instruction"/>
    <category term="teaching"/>
    <category term="universities"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-13T20:20:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7794</id>
    <link href="https://windowsontheory.org/2020/09/10/sigact-research-highlights-call-for-nominations/" rel="alternate" type="text/html"/>
    <title>SIGACT research highlights – call for nominations</title>
    <summary>TL;DR: Know of a great recent paper that should be highlighted to the theory community and beyond? Email a nomination to sigact.highlights.nominations@outlook.com by October 19th. The goal of the SIGACT Research Highlights Committee is to help promotetop computer science theory research via identifying results that are ofhigh quality and broad appeal to the general computer […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>TL;DR:</strong> Know of a great recent paper that should be highlighted to the theory community and beyond? Email a nomination to <a href="mailto:sigact.highlights.nominations@outlook.com" rel="noreferrer noopener" target="_blank">sigact.highlights.nominations@outlook.com</a> by October 19th.</p>



<p>The goal of the SIGACT Research Highlights Committee is to help promote<br/>top computer science theory research via identifying results that are of<br/>high quality and broad appeal to the general computer science audience.<br/>These results would then be recommended for consideration for the <a href="http://cacm.acm.org">CACM</a> <em>Research Highlights</em> section as well as other general-audience computer science research outlets.</p>



<p><strong>Nomination and Selection Process:</strong></p>



<p>The committee solicits two types of nominations:</p>



<p>1) <strong>Conference nominations.</strong> Each year, the committee will ask the PC<br/>chairs of theoretical computer science conferences to send a selection<br/>of up to three top papers from these conferences (selected based on both<br/>their technical merit and breadth of interest to non-theory audience)<br/>and forwarding them to the committee for considerations.</p>



<p>2) <strong>Community nominations. </strong>The committee will accept nominations from the members of the community. Each such nomination should summarize the contribution of the nominated paper and also argue why this paper is<br/>suitable for broader outreach. The nomination should be no more than a<br/>page in length and can be submitted at any time by emailing it to<br/><a href="mailto:sigact.highlights.nominations@outlook.com" rel="noreferrer noopener" target="_blank">sigact.highlights.nominations@outlook.com</a>. Self-nominations are<br/>discouraged.</p>



<p>The nomination deadline is <strong>Monday, October 19, 2020 </strong>.</p>



<p><strong>Committee:</strong></p>



<p>The SIGACT Research Highlights Committee currently comprises the<br/>following members:</p>



<p>Boaz Barak, Harvard University<br/>Irit Dinur, Weizmann Institute of Science<br/>Aleksander Mądry, Massachusetts Institute of Technology (chair)<br/>Jelani Nelson, University of California, Berkeley</p></div>
    </content>
    <updated>2020-09-10T14:17:37Z</updated>
    <published>2020-09-10T14:17:37Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-09-13T20:21:17Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8975826682758317937</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8975826682758317937/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8975826682758317937" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8975826682758317937" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html" rel="alternate" type="text/html"/>
    <title>When are both x^2+3y and y^2+3x both squares, and a more general question</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> In my last post (see <a href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html">here</a>) I asked two math questions. In this post I discuss one of them. (I will discuss the other one later, probably Monday Sept 14.)</p><p><br/>For which positive naturals x,y are x^2+3y and y^2+3x both squares?</p><p>I found this in a math contest book and could not solve it, so I posted it to see what my readers would come up with. They came up with two solutions, which you can either read in the comments on that post OR read my write up <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/sq3.pdf">here</a>.)</p><p>The problem raises two more general questions</p><p>1) I had grad student Daniel Smolyak write a program that showed that if  1\le x,y \le 1000 then the only solutions were (1,1) and (11,16) and (16,11).  (See write up for why the program did not have to look like anything close to all possibly (x,y).)  </p><p>Is there some way to prove that if the only solutions for 1\le x,y\le N (some N) are the three given above, then there are no other solutions?</p><p><br/></p><p>2) Is the following problem solvable: Given p,q in Z[x,y] determine if the number of a,b such that both p(a,b) and q(a,b) are squares is finite or infinite.  AND if finite then determine how many, or a bound on how many.</p><p><br/></p><p>Can replace squares with other sets, but lets keep it simple for now. </p></div>
    </content>
    <updated>2020-09-10T13:33:00Z</updated>
    <published>2020-09-10T13:33:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-13T13:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=448</id>
    <link href="https://tcsplus.wordpress.com/2020/09/09/tcs-talk-thursday-september-17-richard-peng-georgia-tech/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Thursday, September 17 — Richard Peng, Georgia Tech</title>
    <summary>The next TCS+ talk will take place this coming Thursday, September 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Richard Peng from Georgia Tech will speak about “Solving Sparse Linear Systems Faster than Matrix Multiplication” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Thursday, September 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Richard Peng</strong> from Georgia Tech will speak about “<em>Solving Sparse Linear Systems Faster than Matrix Multiplication</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>. </p>



<blockquote class="wp-block-quote"><p>Abstract: Can linear systems be solved faster than matrix multiplication? While there has been remarkable progress for the special cases of graph structured linear systems, in the general setting, the bit complexity of solving an n-by-n linear system <img alt="Ax=b" class="latex" src="https://s0.wp.com/latex.php?latex=Ax%3Db&amp;bg=fff&amp;fg=444444&amp;s=0" title="Ax=b"/> is <img alt="n^\omega" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%5Comega&amp;bg=fff&amp;fg=444444&amp;s=0" title="n^\omega"/>, where <img alt="\omega &lt; 2.372864" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega+%3C+2.372864&amp;bg=fff&amp;fg=444444&amp;s=0" title="\omega &lt; 2.372864"/> is the matrix multiplication exponent. Improving on this has been an open problem even for sparse linear systems with <img alt="\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\text{poly}(n)"/> condition number.</p><p>We present an algorithm that solves linear systems in sparse matrices asymptotically faster than matrix multiplication for any <img alt="\omega&gt;2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%3E2&amp;bg=fff&amp;fg=444444&amp;s=0" title="\omega&gt;2"/>. This speedup holds for any input matrix <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=fff&amp;fg=444444&amp;s=0" title="A"/> with <img alt="o(n^{\omega-1}/\log(\kappa(A)))" class="latex" src="https://s0.wp.com/latex.php?latex=o%28n%5E%7B%5Comega-1%7D%2F%5Clog%28%5Ckappa%28A%29%29%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="o(n^{\omega-1}/\log(\kappa(A)))"/> non-zeros, where <img alt="\kappa(A)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ckappa%28A%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\kappa(A)"/> is the condition number of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=fff&amp;fg=444444&amp;s=0" title="A"/>. For <img alt="\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\text{poly}(n)"/>-conditioned matrices with <img alt="O(n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(n)"/> nonzeros, and the current value of <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=fff&amp;fg=444444&amp;s=0" title="\omega"/>, the bit complexity of our algorithm to solve to within any <img alt="1/\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="1/\text{poly}(n)"/> error is <img alt="O(n^{2.331645})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2.331645%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(n^{2.331645})"/>.</p><p>Our algorithm can be viewed as an efficient randomized implementation of the block Krylov method via recursive low displacement rank factorizations. It is inspired by the algorithm of [Eberly-Giesbrecht-Giorgi-Storjohann-Villard ISSAC <code>06</code>07] for inverting matrices over finite fields. In our analysis of numerical stability, we develop matrix anti-concentration techniques to bound the smallest eigenvalue and the smallest gap in eigenvalues of semi-random matrices.</p><p>Joint work with Santosh Vempala, manuscript at <a href="https://arxiv.org/abs/2007.10254" rel="nofollow">https://arxiv.org/abs/2007.10254</a>.</p></blockquote></div>
    </content>
    <updated>2020-09-10T03:09:50Z</updated>
    <published>2020-09-10T03:09:50Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-09-13T20:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/135</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/135" rel="alternate" type="text/html"/>
    <title>TR20-135 |  Estimation of Graph Isomorphism Distance in the Query World | 

	Sayantan Sen, 

	Sourav Chakraborty, 

	Arijit Ghosh, 

	Gopinath Mishra</title>
    <summary>The graph isomorphism distance between two graphs $G_u$ and $G_k$ is the fraction of entries in the adjacency matrix that has to be changed to make $G_u$ isomorphic to $G_k$. We study the problem of estimating, up to a constant additive factor, the graph isomorphism distance between two graphs in the query model. In other words, if $G_k$ is a known graph and $G_u$ is an unknown graph whose adjacency matrix has to be accessed by querying the entries, what is the query complexity for testing whether the graph isomorphism distance between $G_u$ and $G_k$ is less than $\gamma_1$ or more than $\gamma_2$, where $\gamma_1$ and $\gamma_2$ are two constants with $0\leq \gamma_1 &lt; \gamma_2 \leq 1$. It is also called the tolerant property testing of graph isomorphism in the dense graph model. The non-tolerant version (where $\gamma_1$ is $0$) has been studied by Fischer and Matsliah (SICOMP'08). 


In this paper, we study both the upper and lower bounds of tolerant graph isomorphism testing. We prove an upper bound of $\widetilde{{\cal O}}(n)$ for this problem. Our upper bound algorithm crucially uses the tolerant testing of the well studied Earth Mover Distance (EMD), as the main subroutine, in a slightly different setting from what is generally studied in property testing literature.


Testing tolerant EMD between two probability distributions is equivalent to testing EMD between two multi-sets, where the multiplicity of each element is taken appropriately, and we sample elements from the unknown multi-set with replacement. In this paper, our (main conceptual) contribution is to introduce the problem of tolerant EMD testing between multi-sets (over Hamming cube) when we get samples from the unknown multi-set without replacement and to show that this variant of tolerant testing of EMD is as hard as tolerant testing of graph isomorphism between two graphs. Thus, while testing of equivalence between distributions is at the heart of the non-tolerant testing of graph isomorphism, we are showing that the estimation of the EMD over a Hamming cube (when we are allowed to sample without replacement) is at the heart of 
tolerant graph isomorphism. We believe that the introduction of the problem of testing EMD between multi-sets (when we get samples without replacement) opens an entirely new direction in the world of testing properties of distributions.</summary>
    <updated>2020-09-09T20:48:09Z</updated>
    <published>2020-09-09T20:48:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-13T20:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1387</id>
    <link href="https://ptreview.sublinear.info/?p=1387" rel="alternate" type="text/html"/>
    <title>News for August 2020</title>
    <summary>Last month saw action in property testing across the board: graphs, distributions and functions were all objects of study in papers that came out last month. Also included is a separation result between quantum and classical query complexities resolving a conjecture of Aaronson and Ambainis. Here is a brief report. Testing asymmetry in bounded degree […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Last month saw action in property testing across the board: graphs, distributions and functions were all objects of study in papers that came out last month. Also included is a separation result between quantum and classical query complexities resolving a conjecture of Aaronson and Ambainis. Here is a brief report.</p>



<p><strong>Testing asymmetry in bounded degree graphs</strong>, by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2020/118/">ECCC</a>). This paper studies a natural graph property hitherto not considered in the property testing literature. Namely, the question of testing whether a graph is <em>asymmetric</em> or whether it is far from being asymmetric. A graph is said to be asymmetric if its automorphism group is trivial (that is, it only contains the identity permutation). One of the results in the paper says that this problem is easy in the dense graph model – which is a side result of the paper. This is because all dense graphs are \(O(\log n/n)\)-close to being asymmetric. To see this, the paper points out that a simple randomized process which takes \(G\) as input and returns an asymmetric graph by changing very few edges. This process asks you to do the following: Take a set \(S \subseteq V\) with \(|S| = O(\log n)\) nodes and replace the subgraph they induce with a random graph. Moreover,  randomize all the edges between \(S\) and \(V \setminus S\). What you can show is that in this modified graph, any automorphism (whp) will map \(S\) to itself. And all the remaining vertices behave (whp) in a unique manner which is peculiar to it. In particular, this means that any automorphism better not map a vertex \(v\) in \(V \setminus S\) to any other vertex in \(V \setminus S\). And this finishes the argument. The main result explores the bounded degree model. By a simple argument, you can show that testing asymmetry is easy if all the connected components have size \(s(n) \leq o\left(\frac{\log n}{\log {\log n}}\right)\). Thus, the challenging case is when you have some connected components of a larger size. In this case, what Goldreich shows is the following: If all the components have size at most \(s(n)\) where \(s(n) \geq \Omega\left(\frac{\log n}{\log {\log n}}\right)\), then you can test asymmetry (with one-sided-error) in \(O(n^{1/2} \cdot s(n)/\epsilon)\) queries.  Moreover, the paper also shows a two-sided-lower bound of \(\Omega\left(n/s(n)\right)^{1/2}\) queries which holds as long as \(\epsilon \leq O(1/s(n)\). This leaves open the bounded degree question of determining the query complexity of testing asymmetry in the general case as the paper also points out.</p>



<p/>



<p><strong>On testability of first order properties in Bounded degree graphs</strong>, by Isolde Adler, Noleen Köhler and Pan Peng (<a href="https://arxiv.org/pdf/2008.05800.pdf">arXiv</a>). One of the well understood motivations for property testing begins in the following manner. Decision problems are typically hard to solve if they involve a universal quantifier — either \(\exists\) or  \(\forall\). One way around this hardness is to do the following ritual: relax the problem by dropping the universal quantifier, define a notion of distance between objects in your universe and ask a promise problem instead. Indeed, if you take your favorite property testing problem, you will note that it precisely fits in the template above. How about making this business more rigorous in bounded degree graph property model? This is precisely the content of this work which considers the face off between (First Order) logic and property testing in the bounded degree model. The authors show some interesting results. They begin by showing that in the bounded degree model, you can show that all first order graph properties which involve a single quantifier \(Q \in \{\forall, \exists\}\) are testable with constant query complexity.<br/>If you find this baffling, it would be good to remind yourself that not all graph properties can be expressed in the language of first order logic with a single quantifier! So, you can rest easy. The graph properties you know are not constant time testable are most assuredly not expressible with a single quantifier in First Order Logic. However, this work shows more. It turns out, that “any FO property that is defined by a formula with quantifier prefix \(\exists^* \forall^*\) is testable”. Moreover, there do exist FO graph properties defined by the quantifier prefix \(\forall^* \exists^*\) which are not testable. Thus, this work achieves results in the bounded degree model which are kind of analogous to the results in the dense graph model by Alon et al [1]. On a final note, I find the following one sentence summary of the techniques used to prove the lower bound rather intriguing: “[the paper] obtains the lower bound by a first-order formula that defines a class of bounded-degree expanders, based on zig-zag products of graphs.”  </p>



<p/>



<p><strong>On graphs of bounded degree that are far from being Hamiltonian</strong>, by Isolde Adler and Noleen Köhler (<a href="https://arxiv.org/abs/2008.05801">arXiv</a>). This paper explores the question of testing Hamiltonicity in the bounded degree model. The main result of the paper is that Hamiltonicity is not testable with one-sided error with \(o(n)\) queries. PTReview readers might recall from our <a href="https://ptreview.sublinear.info/?p=1371">July Post</a> a concurrent paper by Goldriech [2] which achieves the same lower bound on query complexity in the two-sided error model (the authors call attention to [2] as well). One of the interesting feature of this result is that the lower bounds are obtained by an explicit deterministic reduction as opposed to the usual randomized reduction. Like the authors point out, this offers more insights into structural complexity of instances that are far from being Hamiltonian. We point out that this also differs from how the lower bound is derived in [2] — which is via local hardness reductions to a promise problem of 3 CNF satisfiability.</p>



<p/>



<p><strong>An optimal tester for \(k\)-linear</strong>, by Nader Bshouty (<a href="https://eccc.weizmann.ac.il/report/2020/123/">ECCC</a>). This paper explores two related questions. We call a function \(f \colon \{0,1\}^n \to \{0,1\}\) \(k\)-linear if it equals the \(\sum_{i \in S} x_i\) for some \(S \subseteq [n]\) of size exactly \(k\). A boolean function is said to be \(k\)-linear<strong>*</strong> if it is \(j\) linear for a fixed  \(j\) where \(j \in \{0,1,2, \cdots, k\}\). The paper proves the following theorems.</p>



<ol><li>There exists a non-adaptive <em>one-sided</em> distribution free tester for \(k\)-linear<strong>*</strong> with query complexity being \(O\left(k \log k + \frac{1}{\varepsilon}\right)\). This matches the two-sided lower bound (where the underlying distribution is uniform) by Blais et al [3].</li><li>Using a reduction from \(k\)-linear<strong>*</strong> to \(k\)-linear, the paper shows one can obtain a non-adpative <em>two-sided</em> distribution free tester for \(k\)-linear with same query complexity as the above result. The lower bound from Blais et al applies here also (in fact, they prove a lower bound on \(k\)-linearity).</li><li>Next up, the paper has a couple of lower bound results to accompany this. One of these results reveals the price you pay for being <em>one-sided</em> and <em>exact</em> (that is, you insist on the function being exactly \(k\)-linear). Turns out, now you have a non-adaptive one-sided uniform distribution lower bound of \(\widetilde{\Omega}(k) \log n + \Omega(1/\varepsilon)\).  If you allow adaptivity instead, the paper shows a lower bound of \(\widetilde{\Omega}(\sqrt k)\log n + \Omega(1/\varepsilon)\).</li></ol>



<p/>



<p><strong>Amortized Edge Sampling</strong>, by Talya Eden, Saleet Mossel and Ronitt Rubinfeld (<a href="https://arxiv.org/abs/2008.08032">arXiv</a>). Consider the following setup. You are given query access to adjacency list of a graph \(G\) with \(n\) vertices and \(m\) edges. You can make degree queries and neighbor queries. Suppose I ask you to sample a single edge from this graph from a distribution that is pointwise \(\varepsilon\) close to the uniform distribution. Eden and Rosenbaum already showed how you can achieve this with a budget of \(\widetilde{\Theta}(n/\sqrt m)\) queries. Starting from this jump off point, the authors ask whether you can circumvent this lower bound if you want to return multiple samples from a distribution which is again pointwise close to uniform. The paper answers this question in the affirmative and shows that if you know the number of samples, \(q\), in advance you can get away with an amortized bound of \(O*(\sqrt q n/\sqrt m)\) on the total number of queries needed.</p>



<p/>



<p><strong>On the High Accuracy Limitation of Adaptive Property Estimation</strong>, by Yanjun Han (<a href="https://arxiv.org/abs/2008.11964">arXiv</a>). Take a discrete distribution \(\mathcal{P}\) with support size \(k\) and consider the task of estimating some symmetric property of \(\mathcal{P}\) to a small \(\pm \varepsilon\) additive error. Here, a symmetric property refers to a “nice” functional defined over the probability simplex, i.e., it refers to functions \(F \colon \Delta_k \to \mathbb{R}\) where \(F(p) = \sum_{i=1}^{k} f(p_i)\) where \(f \colon (0,1) \to \mathbb{R}\). A naive attack to these estimation tasks goes through the following ritual: you get your hands on the empirical distribution, you plug it in \(F\) and you hope for the best. Turns out, you are out of luck if the function \(f\) is non-smooth and in these cases you end up with a suboptimal estimator. Previous works have also looked at more sophisticated estimators (like the <em>local moment matching or LMM and profile maximum likelihood or PML</em> estimator). Turns out, using the LMM or PML estimator leads to optimal sample complexity for a handful of symmetric properties (as long as \(\varepsilon \geq n^{-1/3}\)). This paper considers the question of what can you say for supersmall values of \(\varepsilon\) where \(n^{-1/2} \leq \varepsilon \leq n^{-1/3}\). (The \(n^{-1/2}\) appears because there are estimators that use the knowledge of \(f\) and \(\varepsilon\) can be driven all the way down to \(n^{-1/2}\) for these estimators). The paper focuses on estimators which do not exploit any structure in \(f\). In particular, the paper specializes this question to PML and shows a fundamental limitation on PML which means that the PML approach fails to be sample optimal for the entire range of \(\varepsilon\) and is sample optimal only for \(\varepsilon &gt;&gt; n^{-1/3}\) — which also confirms a conjecture of Han and Shiragur (and refutes a conjecture of Acharya et al. who postulated this is sample optimal for the entire range of \(\varepsilon\)).</p>



<p/>



<p><strong>\(k\)-Forrelation Optimally Separates Quantum and Classical Query<br/>Complexity</strong>, by Nikhil Bansal and Makrand Sinha (<a href="https://arxiv.org/abs/2008.07003">arXiv</a>). Understanding the power of quantum query over classical queries is a well motivated problem with a rich history. One of the biggest questions in this area asks for the largest separation between classical and quantum query complexities. In a breakthrough, Aaronson and Ambainis [4] showed a fundamental simulation result which confirmed that you can simulate \(q\) quantum queries with \(O(N^{1 – 1/2q})\) classical queries in the randomized decision tree model of computation as long as \(q = O(1)\). In the same paper, the authors also showed that the standard <em>forrelation</em>* problem exhibits a \(1\) versus \(\widetilde{\Omega}(\sqrt n)\) separation. This means that for \(q = 1\), you essentially have optimal separation. But what about \(q &gt; 1\)? To this end, Aaronson and Ambainis conjectured that a certain problem which they called \(k\)-forrelation — which can be computed with \(q = k/2\) queries requires at least \(\widetilde{\Omega}(n^{1-1/k})\) classical queries. The current work precisely confirms this conjecture.</p>



<p>(*) The forrelation problem asks you to decide whether one Boolean function is highly correlated with the Fourier transform of a second function.</p>



<p><em><strong>(Edit:</strong> Added Later)</em>  Simon Apers points out a paper by Shrestov, Storozhenko and Wu that we missed. (Thanks Simon)! Here is a brief report on that paper.<br/></p>



<p><strong>An optimal separation of randomized and quantum query complexity</strong> (by Alexander Shrestov, Andrey Storozhenko and Pei Wu)(<a href="https://arxiv.org/abs/2008.10223">arXiv</a>) Similar to the paper by Bansal and Sinha [BS20] mentioned above, this paper also resolves the conjecture by Aaronson and Ambainis proving the same result. Like the paper also notes, the techniques in both of these works are completely different and incomparable. On the one hand [BS20] proves the separation for an explicit function as opposed to a function chosen uniformly at random from a certain set as considered in this work. On the other hand,  the separation result shown in [BS20] only applies when the query algorithm returns the correct answer with probability at least \(1/2 + 1/poly(\log n)\) — in contrast the results in this paper apply even when the query algorithm is required to have probability of correctness be a constant at least \(1/2\). In addition, this work also proves the \(\ell\)-Fourier weight conjecture of Tal which is of independent interest beyond quantum computing.</p>



<p/>



<p>So, it looks like all in all we had a great month with two concurrent papers both resolving Aaronson Ambainis conjecture (yet again after two concurrent papers on testing Hamiltonicity)!</p>



<p><strong>References</strong>:</p>



<p>[1] Noga Alon, Eldar Fischer, Michael Krivelevich, and Mario Szegedy. Efficient testing of<br/>large graphs. Combinatorica, 20(4):451–476, 2000.</p>



<p><br/>[2] Oded Goldreich. On testing hamiltonicity in the bounded degree graph model. Electronic Colloquium on Computational Complexity (ECCC), (18), 2020</p>



<p>[3] Eric Blais, Joshua Brody, and Kevin Matulef. Property testing lower bounds via communication complexity. <em>CCC 2011</em></p>



<p>[4] Scott Aaronson and Andris Ambainis. Forrelation: A problem that optimally separates quantum from classical computing. SIAM J. Comput., 47(3):982–1038, 2018</p></div>
    </content>
    <updated>2020-09-09T03:55:44Z</updated>
    <published>2020-09-09T03:55:44Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>akumar</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-09-12T23:53:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/134</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/134" rel="alternate" type="text/html"/>
    <title>TR20-134 |  Tight Bounds on Sensitivity and Block Sensitivity of Some Classes of Transitive Functions | 

	Anna Gal, 

	Siddhesh Chaubal</title>
    <summary>Nisan and Szegedy conjectured that block sensitivity is at most
polynomial in sensitivity for any Boolean function.
Until a recent breakthrough of Huang, the conjecture had been
wide open in the general case,
and was proved only for a few special classes
of Boolean functions.
Huang's result implies that block sensitivity is at most
the 4th power of sensitivity for any Boolean function.
It remains open if a tighter relationship between
sensitivity and block sensitivity holds for arbitrary Boolean functions;
the largest known gap between these measures is quadratic.

We prove tighter bounds showing that block sensitivity is at most
3rd power, and in some cases at most square of sensitivity for
subclasses of transitive functions,
defined by various properties of their DNF (or CNF) representation.
Our results improve and extend previous results regarding
transitive functions. We obtain these results by
proving tight (up to constant factors) lower bounds on the
smallest possible sensitivity of functions in these classes.

In another line of research, it has also been examined what is the
smallest possible block sensitivity of transitive functions.
Our results yield tight (up to constant factors) lower bounds
on the block sensitivity of the classes we consider.</summary>
    <updated>2020-09-08T23:39:46Z</updated>
    <published>2020-09-08T23:39:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-13T20:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/133</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/133" rel="alternate" type="text/html"/>
    <title>TR20-133 |  Query complexity lower bounds for local list-decoding and hard-core predicates (even for small rate and huge lists) | 

	Noga Ron-Zewi, 

	Ronen Shaltiel, 

	Nithin Varma</title>
    <summary>A binary code $\text{Enc}:\{0,1\}^k \rightarrow \{0,1\}^n$ is $(\frac{1}{2}-\varepsilon,L)$-list decodable if for every $w \in \{0,1\}^n$, there exists a set $\text{List}(w)$ of size at most $L$, containing all messages $m \in \{0,1\}^k$ such that the relative Hamming distance between $\text{Enc}(m)$ and $w$ is at most $\frac{1}{2}-\varepsilon$.
A $q$-query local list-decoder is a randomized procedure that when given oracle access to a string $w$, makes at most $q$ oracle calls, and for every message $m \in \text{List}(w)$, with high probability, there exists $j \in [L]$ such that for every $i \in [k]$, with high probability, $\text{Dec}^w(i,j)=m_i$.

We prove lower bounds on $q$, that apply even if $L$ is huge (say $L=2^{k^{0.9}}$) and the rate of $\text{Enc}$ is small (meaning that $n \ge 2^{k}$):

* For $\varepsilon = 1/k^{\nu}$ for some constant $\nu&gt;0$, we prove a lower bound of $q=\Omega(\frac{\log(1/\delta)}{\varepsilon^2})$, where $\delta$ is the error probability of the local list-decoder. This bound is tight as there is a matching upper bound by Goldreich and Levin (STOC 1989) of $q=O(\frac{\log(1/\delta)}{\varepsilon^2})$ for the Hadamard code (which has $n=2^k$). This bound extends an earlier work of Grinberg, Shaltiel and Viola (FOCS 2018) which only works if $n \le 2^{k^{\nu}}$ and the number of coins tossed by $\text{Dec}$ is small (and therefore does not apply to the Hadamard code, or other codes with low rate).

* For smaller $\varepsilon$, we prove a lower bound of roughly $q = \Omega(\frac{1}{\sqrt{\varepsilon}})$. To the best of our knowledge, this is the first lower bound on the number of queries of local list-decoders that gives $q \ge k$ for small $\varepsilon$.

Local list-decoders with small $\varepsilon$ form the key component in the celebrated theorem of Goldreich and Levin that extracts a hard-core predicate from a one-way function.
We show that black-box proofs cannot improve the Goldreich-Levin theorem and produce a hard-core predicate that is hard to predict with probability $\frac{1}{2}+\frac{1}{\ell^{\omega(1)}}$ when provided with a one-way function $f:\{0,1\}^{\ell} \rightarrow \{0,1\}^{\ell}$, such that circuits of size $\text{poly}(\ell)$ cannot invert $f$ with probability $\rho=1/2^{\sqrt{\ell}}$ (or even $\rho=1/2^{\Omega(\ell)}$). This limitation applies to any proof by black-box reduction (even if the reduction is allowed to use nonuniformity and has oracle access to $f$).</summary>
    <updated>2020-09-08T17:41:18Z</updated>
    <published>2020-09-08T17:41:18Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-13T20:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite</id>
    <link href="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite.html" rel="alternate" type="text/html"/>
    <title>Eberhard’s theorem for bipartite polyhedra with one big face</title>
    <summary>Eberhard’s theorem is a topic in the combinatorial theory of convex polyhedra that once saw a lot of research, but has faded from more recent interest. It’s named after Victor Eberhard, a German mathematician from the late 19th and early 20th century who worked in geometry despite becoming blind at age 12 or 13. I find this hard to imagine, as my own research in geometry is based very heavily on visual thinking, but he was far from the only successful blind mathematician; Leonhard Euler, Lev Pontryagin, and Bernard Morin also come to mind, and there are more.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://en.wikipedia.org/wiki/Eberhard%27s_theorem">Eberhard’s theorem</a> is a topic in the combinatorial theory of convex polyhedra that once saw a lot of research, but has faded from more recent interest. It’s named after <a href="https://en.wikipedia.org/wiki/Victor_Eberhard">Victor Eberhard</a>, a German mathematician from the late 19th and early 20th century who worked in geometry despite becoming blind at age 12 or 13. I find this hard to imagine, as my own research in geometry is based very heavily on visual thinking, but he was far from the only successful blind mathematician; <a href="https://en.wikipedia.org/wiki/Leonhard_Euler">Leonhard Euler</a>, <a href="https://en.wikipedia.org/wiki/Lev_Pontryagin">Lev Pontryagin</a>, and <a href="https://en.wikipedia.org/wiki/Bernard_Morin">Bernard Morin</a> also come to mind, and there are more.</p>

<p>Anyway, Eberhard’s theorem concerns the following question. Suppose I tell you that a polyhedron has a certain number of faces of certain types. For instance, after Archimedes’ work on polytopes was lost, all we knew about the <a href="https://en.wikipedia.org/wiki/Archimedean_solid">Archimedean solids</a> until their rediscovery in the Renaissance was a brief listing from <a href="https://en.wikipedia.org/wiki/Pappus_of_Alexandria">Pappus of Alexandria</a> giving this information: there is one with 8 triangles and 6 squares, etc. How can we tell that these counts of faces actually determine a polyhedron?</p>

<p>The given information for Eberhard’s theorem, then, is just a collection of counts of face types (triangles, quadrilaterals, etc.), without specifying the exact shapes of these faces. The goal is to use these faces to build a simple polyhedron, one for which three edges meet at every vertex (like a cube, unlike an octahedron). One necessary condition for this to be possible is that the polyhedron must obey Euler’s polyhedral formula \(v-e+f=2\). And it’s easy to calculate the numbers of vertices, edges, and faces appearing in this formula, from the face counts. Plugging these numbers into Euler’s formula leads to a linear equation that the face counts must obey. Crucially, this linear equation omits the count of hexagons: adding or removing hexagons will not change whether Euler’s formula holds. What Eberhard’s theorem states is that, as long as the face counts obey Euler’s formula in this way, there is always some number of hexagons that can be added or removed so that the remaining faces will form a polyhedron.</p>

<p>However, calculating the fewest number of hexagons needed, or even determining whether a given number of faces of all types (including hexagons) can be put together into a polyhedron, remains somewhat mysterious. So I thought I’d play with a case that would be both simple enough to solve and still interesting: the bipartite simple polyhedra (famous from <a href="https://en.wikipedia.org/wiki/Barnette%27s_conjecture">Barnette’s conjecture</a>), with one big face (a \(2n\)-gon for some \(n&gt;3\)), many small faces (\(n+3\) quadrilaterals, the number needed to make Euler’s formula hold), and a mysterious number of hexagons. What is the smallest number of hexagons that will allow the construction of a simple polyhedron with these face counts? The answer turns out to be \(\lfloor (3n-6)/2\rfloor\), achieved with polyhedra (or polyhedral graphs) in which the outer \(2n\)-gon surrounds a <a href="https://en.wikipedia.org/wiki/Cactus_graph">cactus tree</a> of 6-vertex cycles (and possibly one 4-vertex cycle), connected to each other by bridge edges:</p>

<p style="text-align: center;"><img alt="Three hexagon-minimal bipartite simple polyhedra" src="https://11011110.github.io/blog/assets/2020/eberhard.svg"/></p>

<p>The central cactus tree can be rearranged, as long as no two bridge edges have adjacent endpoints. For instance, in the graph with the dodecagon outer face, at the bottom of the figure, it’s possible for the middle six-vertex loop to have three connections to the outside polygon on one side and only one connection on the other side, or to have the four-vertex loop in the middle. But I can prove that all optimal solutions have the same overall central cactus tree structure.</p>

<p>I find it easier to think about the following equivalent rephrasing of the optimization problem: instead of finding a minimum number of hexagons that will allow us to build a polyhedron with those face counts, let’s build a polyhedron with one \(2n\)-gon face and the rest quadrilaterals and hexagons, and concentrate on minimizing the number of vertices in this polyhedron. The number of quadrilaterals will automatically come out right, and the number of hexagons will be minimized if the number of vertices is minimized.</p>

<p>Now suppose that we have any simple polyhedron with one \(2n\)-gon face and the rest quadrilaterals and hexagons. Remove the outer \(2n\)-gon from the graph, leaving a conncted subgraph, and look at the biconnected components of this subgraph. For any one component, its outer face in its induced planar embedding must be a simple cycle, with some vertices having degree two in the component (the endpoints of edges connecting the component to the rest of the graph) and some having degree three. If the component is a 4-cycle or 6-cycle, then all of its vertices have degree two. But if not, then at most four consecutive vertices of its outer cycle can have degree two, because they and the two vertices connected to them on both sides form part of the boundary of a face interior to the component, which can have at most six vertices. And the degree-three vertices of the outer cycle must come in consecutive pairs, which cannot be adjacent to the endpoints of bridge edges connecting to other biconnected components, because a degree-three vertex next to a bridge edge or next to two other degree-three vertices would combine with part of the outer \(2n\)-gon to form a face with seven or more vertices, and a degree-three vertex by itself would form a pentagon, neither of which is allowed.</p>

<p>So in a component that is not a 4-cycle or 6-cycle, the degree-two and degree-three vertices alternate around the outer cycle of the component in consecutive sequences of at most four and exactly two vertices. This implies that the number of degree-two vertices is even (because the whole cycle is even by bipartiteness) and that the number of degree-three vertices in the component (even just counting the ones on its boundary) is at least half of the number of degree-two vertices on its boundary. For the cactus trees that we’ve been using, on the other hand, the number of degree-three vertices in each cactus tree is strictly less than half of the number of degree-two vertices. So if we replace a whole non-cycle component by a cactus tree, we can get a graph with the same number of exposed degree-2 vertices, but fewer total vertices. After repeated replacement of biconnected components, at each step reducing the number of vertices, we would reach a state where the subgraph inside the \(2n\)-gon is a cactus tree. It might not meet the requirement that its bridge edges have nonadjacent endpoints, but it could always be rearranged to do so. And it might not be a cactus with at most one 4-cycle, but if not we could replace two 4-cycles by one 6-cycle and make it even smaller. So the only graphs that cannot be made smaller are the ones we started with, the cactus trees of 6-cycles and at most one 4-cycle, surrounded by an outer \(2n\)-gon.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104827671950147352">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-09-07T22:28:00Z</updated>
    <published>2020-09-07T22:28:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-09-08T07:49:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1108</id>
    <link href="http://corner.mimuw.edu.pl/?p=1108" rel="alternate" type="text/html"/>
    <title>IGAFIT Algorithmic Colloquium</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">We are excited to announce a new online seminar - IGAFIT Algorithmic Colloquium. This new event aims to integrate the European algorithmic community and keep it connected during the times of the pandemic. This online seminar will take place biweekly on … <a href="http://corner.mimuw.edu.pl/?p=1108">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are excited to announce a new online seminar - IGAFIT Algorithmic Colloquium. This new event aims to integrate the European algorithmic community and keep it connected during the times of the pandemic. This online seminar will take place biweekly on Thursday at 14:00 CET, with the talks lasting for 45 minutes. Each talk will be followed by a networking and discussion session on topics related to the talk. We cordially invite all participants to this session. The meeting will be run on <a href="https://www.airmeet.com/e/55923fa0-eee9-11ea-8530-b3eab1e75816" rel="noreferrer noopener" target="_blank">Airmeet</a>. More details on the event can be found on <a href="http://igafit.mimuw.edu.pl/?page_id=483786" rel="noreferrer noopener" target="_blank">IGAFIT web page</a>.</p>



<p>The first talk will be held on the 1st of October 2020.</p>



<p>October 1, 2020<br/>Vera Traub, University of Bonn<br/>Title: An improved approximation algorithm for ATSP<br/>Abstract: In a recent breakthrough, Svensson, Tarnawski, and Végh gave the first constant-factor approximation algorithm for the asymmetric traveling salesman problem (ATSP). In this work we revisit their algorithm. While following their overall framework, we improve on each part of it.</p>



<p>Svensson, Tarnawski, and Végh perform several steps of reducing ATSP to more and more structured instances. We avoid one of their reduction steps (to irreducible instances) and thus obtain a simpler and much better reduction to vertebrate pairs. Moreover, we show that a slight variant of their algorithm for vertebrate pairs has a much smaller approximation ratio.</p>



<p>Overall we improve the approximation ratio from 506 to 22 + ε for any ε &gt; 0. We also improve the upper bound on the integrality ratio of the standard LP relaxation from 319 to 22.</p>



<p>This is joint work with Jens Vygen.</p>



<p>Other upcoming talks include:</p>



<p>October 15, 2020<br/>Thatchaphol Saranurak, Toyota Technological Institute at Chicago<br/>Title: An almost-linear time deterministic algorithm for expander decomposition</p>



<p>October 29, 2020<br/>Nathan Klein, University of Bonn<br/>Title: A (Slightly) Improved Approximation Algorithm for Metric TSP</p>



<p>For more details please contact the Organization Committee:<br/>Nikhil Bansal<br/>Artur Czumaj<br/>Andreas Feldmann<br/>Adi Rosén<br/>Eva Rotenberg<br/>Piotr Sankowski<br/>Christian Sohler <br/></p></div>
    </content>
    <updated>2020-09-07T20:33:10Z</updated>
    <published>2020-09-07T20:33:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2020-09-12T23:52:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=2727</id>
    <link href="https://francisbach.com/integration-by-parts-randomized-smoothing-score-functions/" rel="alternate" type="text/html"/>
    <title>The many faces of integration by parts – II : Randomized smoothing and score functions</title>
    <summary>This month I will follow-up on last month blog post and look at another application of integration by parts, which is central to many interesting algorithms in machine learning, optimization and statistics. In this post, I will consider extensions in higher dimensions, where we take integrals on a subset of \(\mathbb{R}^d\), and focus primarily on...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">This month I will follow-up on last month blog post and look at another application of integration by parts, which is central to many interesting algorithms in machine learning, optimization and statistics. In this post, I will consider extensions in higher dimensions, where we take integrals on a subset of \(\mathbb{R}^d\), and focus primarily on property of the so-called “score function” of a density \(p: \mathbb{R}^d \to \mathbb{R}\), namely the gradient of its logarithm: $$\nabla  \log  p(z)  = \frac{1}{p(z)} \nabla p(z) \in \mathbb{R}^d,$$ or, done coordinate by coordinate, $$ \big(\nabla \log p(z)\big)_i = \frac{\partial [ \log p]}{\partial z_i}(z) = \frac{1}{p(z)} \frac{\partial  p }{\partial z_i}(z) .$$ Note here that we take derivatives with respect to \(z\) and not with respect to some hypothetical external parameter, which is often the case in statistics (see <a href="https://en.wikipedia.org/wiki/Score_(statistics)">here</a>).</p>



<p class="justify-text">As I will show below, this quantity comes up in many different areas, most often used with integration by parts. After a short review on integration by parts and its applications to score functions, I will present four quite diverse applications, to (1) optimization and randomized smoothing, (2) differentiable perturbed optimizers, (3) learning single-index models in statistics, and (4) score matching for density estimation.</p>



<h2>Integration by parts in multiple dimensions</h2>



<p class="justify-text">I will focus only on situations where we have some random variable \(Z\) defined on \(\mathbb{R}^d\), with differentiable strictly positive density \(p(\cdot)\) with respect to the Lebesgue measure (I could also consider bounded supports, but then I would need to use the <a href="https://en.wikipedia.org/wiki/Divergence_theorem">divergence theorem</a>). I will consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), and my goal is to provide an expression of \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] \in \mathbb{R}^d\) using the gradient of \(f\).</p>



<p class="justify-text">Assuming that \(f(z) p(z)\) goes to zero when \(\| z\| \to +\infty\), we have: $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big]  = \int_{\mathbb{R}^d} f(z)\Big( \frac{1}{p(z)} \nabla p (z) \Big) p(z) dz = \int_{\mathbb{R}^d}  f (z) \nabla p(z) dz .$$ We can then use integration by parts (together with the zero limit at infinity), to get $$\int_{\mathbb{R}^d} f (z) \nabla p(z) dz = \ – \int_{\mathbb{R}^d} p (z) \nabla f(z) dz.$$ This leads to $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\  – \mathbb{E} \big[ \nabla f(Z) \big]. \tag{1}$$ In other words, expectations of the gradient of \(f\) can be obtained through expectations of \(f\) times the negative of the score function.  </p>



<p class="justify-text">Note that Eq. (1) can be used in the two possible directions: to estimate the right hand side (expectation of gradients) when the score function is known, and vice-versa to estimate expectations (as a simple example, when \(f\) is constant equal to one, we get the traditional identity \(\mathbb{E} \big[ \nabla \log p(Z) \big] = 0\)).</p>



<p class="justify-text"><strong>Gaussian distribution.</strong> Assuming that \(p(z) = \frac{1}{(2\pi \sigma^2)^{d/2}} \exp\big( – \frac{1}{2 \sigma^2}\|  z – \mu\|_2^2 \big)\), that is, \(Z\) is normally distributed with mean vector \(\mu \in \mathbb{R}^d\) and covariance matrix \(\sigma^2 I\), we get a particularly simple expression $$\frac{1}{\sigma^2} \mathbb{E} \big[ f(Z) (Z-\mu)  \big] =  \mathbb{E} \big[ \nabla f(Z) \big],$$ which is often referred to as <a href="https://en.wikipedia.org/wiki/Stein%27s_lemma">Stein’s lemma</a> (see for example an application to <a href="https://en.wikipedia.org/wiki/Stein%27s_unbiased_risk_estimate">Stein’s unbiased risk estimation</a>).</p>



<p class="justify-text"><strong>Vector extension.</strong> If now \(f\) has values in \(\mathbb{R}^d\), still with the product \(f(z) p(z)\) going to zero when \(\| z\| \to +\infty\), we get $$\mathbb{E} \big[ f(Z)^\top \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla \!\cdot \! f(Z) \big], \tag{2}$$ where \(\nabla\! \cdot \! f\) is the <a href="https://en.wikipedia.org/wiki/Divergence">divergence</a> of \(f\) defined as \(\displaystyle \nabla\! \cdot\! f(z) = \sum_{i=1}^d \frac{\partial f}{\partial z_i}(z)\). </p>



<h2>Optimization and randomized smoothing</h2>



<p class="justify-text">We consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), which is  non-differentiable everywhere. There are several ways of <em>smoothing</em> it. A very traditional way is to convolve it with a smooth function. In our context, this corresponds to considering $$f_\varepsilon(x) = \mathbb{E} f(x+ \varepsilon Z) = \int_{\mathbb{R}^d} f(x+\varepsilon z) p(z) dz,$$ where \(z\) is a random variable with strictly positive sufficiently differentiable density, and \(\varepsilon \) is a positive parameter. Typically, if \(f\) is Lipschitz-continuous, \(| f – f_\varepsilon|\) is uniformly bounded by a constant times \(\varepsilon\).</p>



<p class="justify-text">Let us now assume that we can take gradients within the integral, leading to: $$\nabla f_\varepsilon(x) = \int_{\mathbb{R}^d}   \nabla f(x+\varepsilon z) p(z) dz = \mathbb{E} \big[  \nabla f(x+\varepsilon z) \big].$$ This derivation is problematic as the whole goal is to apply this to functions \(f\) which are not everywhere differentiable, so the gradient \(\nabla f\) is not always defined. It turns out that when \(p\) is sufficiently differentiable, integration by parts exactly provides an expression which does not imply the gradient of \(f\).</p>



<p class="justify-text">Indeed, still imagining that \(f\) is differentiable, we can apply Eq. (1) to the function \(z \mapsto \frac{1}{\varepsilon} f(x+\varepsilon z)\), whose gradient is the function \(z \mapsto \nabla f(x+\varepsilon z)\), and get $$\nabla f_\varepsilon(x) = \ – \frac{1}{\varepsilon} \int_{\mathbb{R}^d} f(x+\varepsilon z) \nabla p(z) dz = \frac{1}{\varepsilon} \mathbb{E} \big[ – f(x+\varepsilon Z) \nabla \log p(Z)\big].$$ These computations can easily be made rigorous and we obtain an expression of the gradient of \(f_\varepsilon\) without invoking the gradient of \(f\) (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>, <a href="https://arxiv.org/pdf/2002.08676">14</a>] for details).</p>



<p class="justify-text">Moreover, if \(p\) is a differentiable function, we can expect the expectation in the right hand side of the equation above to be bounded, and therefore the function \(f_\varepsilon\) has gradients bounded by \(\frac{1}{\varepsilon}\).</p>



<p class="justify-text">This can be used within (typically convex) optimization in two ways:</p>



<ul class="justify-text"><li><strong>Zero-th order optimization</strong>: if our goal is to minimize the function \(f\), which is non-smooth, and for which we only have access to function values (so-called “zero-th order oracle), then we can obtain an unbiased stochastic gradient of the smoothed version \(f_\varepsilon\) as \(– f(x+\varepsilon z) \nabla \log p(z)\) where \(z\) is sampled from \(p\). The variance of the stochastic gradient grows with \(1/\varepsilon\) and the bias due to the use of \(f_\varepsilon\) instead of \(f\) is proportional to \(\varepsilon\). There is thus a sweet spot for the choice of \(\varepsilon\), with many variations; see, e.g., [<a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">5</a>, <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">6</a>, <a href="https://arxiv.org/pdf/cs/0408007">7</a>]. </li><li><strong>Randomized smoothing with acceleration</strong> [<a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">8</a>, <a href="https://arxiv.org/pdf/1204.0665">9</a>]: Here the goal is to follow the “Nesterov smoothing” idea [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">10</a>] and minimize a non-smooth function \(f\) using accelerated gradient descent on the smoothed version \(f_\varepsilon\), but this time with a stochastic gradient. Stochastic versions of Nesterov accelerations are then needed; this is useful when a full deterministic smoothing of \(f\) is too costly, see [<a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">11</a>, <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">12</a>] for details.</li></ul>



<p class="justify-text"><strong>Example.</strong> We consider minimizing a quadratic function in two dimensions, and we compare below plain gradient descent, stochastic gradient descent (left) and zero-th order optimization where we take a step towards the direction \(– f(x+\varepsilon Z) \nabla \log p(Z)\) for a standard normal \(Z\). We compare stochastic zero-th order optimization to plain stochastic gradient descent (SGD) below: SGD is a first-order method requiring access to stochastic gradients with a variance that is bounded, while zero-th order optimization only requires function values, but with significantly higher variance and thus requiring more iterations to converge.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4633" height="254" src="https://francisbach.com/wp-content/uploads/2020/09/paths_video_zeroth_order.gif" width="556"/>Left: gradient descent (GD) and stochastic gradient descent (SGD). Right: zero-th order optimization. All with constant step-sizes.</figure></div>



<h2>Differentiable perturbed optimizers</h2>



<p class="justify-text">The randomized smoothing technique can be used in a different context with applications to differentiable programming. We now assume that the function \(f\) can be written as the <a href="https://en.wikipedia.org/wiki/Support_function">support function</a> of a polytope \(\mathcal{C}\), that is, for all \(u \in \mathbb{R}^d\), $$f(u) = \max_{y \in \mathcal{C}} u^\top y,$$ where \(\mathcal{C}\) is the convex hull of a finite family \((y_i)_{i \in I}\). </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4598" height="292" src="https://francisbach.com/wp-content/uploads/2020/09/polytope_intro-1024x842.png" width="357"/>Polytope \(C\), convex hull of 8 vectors in \(\mathbb{R}^2\).</figure></div>



<p class="justify-text">Typically, the family is very large (e.g., \(|I|\) is exponential in \(d\)), but a polynomial-time algorithm exists for computing an arg-max \(y^\ast(u)\) above. Classical examples, from simpler to more interesting, are:</p>



<ul class="justify-text"><li><strong>Simplex</strong>: \(\mathcal{C}\) is the set of vectors with non-negative components that sum to one, and is the convex hull of canonical basis vectors. Then \(f\) is the maximum function, and there are many classical ways of smoothing it (see link with the <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> below).</li><li><strong>Hypercube</strong>: \(\mathcal{C} = [0,1]^n\), which is the convex hull of all vectors in \(\{0,1\}^n\). The maximization of linear functions can then be done independently for each bit.</li><li><strong>Permutation matrices</strong>: \(\mathcal{C}\) is then the <a href="https://en.wikipedia.org/wiki/Birkhoff_polytope">Birkhoff polytope</a>, the convex hull of all <a href="https://en.wikipedia.org/wiki/Permutation_matrix">permutation matrices</a> (square matrices with elements in \(\{0,1\}\), and with exactly a single \(1\) in each row and column). Maximizing linear functions is the classical <a href="https://en.wikipedia.org/wiki/Assignment_problem">linear assignment problem</a>.</li><li><strong>Shortest paths</strong>: given a graph, a path is a sequence of vertices which are connected to each other in the graph. They can classically be represented as a vector of of 0’s and 1’s indicating the edges which are followed by the paths. Minimizing linear functions is then equivalent to <a href="https://en.wikipedia.org/wiki/Shortest_path_problem">shortest path</a> problems.</li></ul>



<p class="justify-text">In many supervised applications, the vector \(u\) is as a function of some input \(x\) and some parameter vector \(\theta\). In order to learn the pararameter \(\theta\) from data, one needs to be able to differentiate with respect to \(\theta\), and this is typically done through the chain rule by differentiating \(y^\ast(u)\) with respect to \(u\). There come two immediate obstacles: (1) the element \(y^\ast(u)\) is not even well-defined when the arg-max is not unique, which is not a real problem because this can only be the case for a set of \(u\)’s with zero Lebesgue measure; and (2) the function \(y^\ast(u)\) is locally constant for most \(u\)’s, that is, the gradient is equal to zero almost everywhere. Thus, in the context of differentiable programming, this is non informative and essentially useless.</p>



<p class="justify-text">Randomized smoothing provides a simple and generic way to define an approximation which is differentiable and with informative gradient everywhere (there are others, such as adding a strongly convex regularizer \(\psi(y)\), and maximizing \(u^\top y\  – \psi(y)\) instead, see [<a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">20</a>] for details. See also [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>]).</p>



<p class="justify-text">In order to obtain a differentiable function through randomized smoothing, we can consider \(y^\ast(u + \varepsilon z)\), for a random \(z\), which is an instance of the more general “perturb-and-MAP” paradigm [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242">21</a>, <a href="https://icml.cc/Conferences/2012/papers/528.pdf">22</a>].</p>



<p class="justify-text">Since \(y^\ast(u)\) is a subgradient of \(f\) at \(u\) and \(f_\varepsilon(u) = \int_{\mathbb{R}^d} f(u+\varepsilon z) p(z) dz\), by swapping integration (with respect to \(z\)) and differentiation (with respect to \(u\)), we have the following identities: $$ \mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big] = \nabla f_\varepsilon(u),$$ that is, the expectation of the perturbed arg-max is the gradient of the smoothed function \(f_\varepsilon\). I will use the notation \(y^\ast_\varepsilon(u) =\mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big]\) to denote this gradient; see an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4545" height="322" src="https://francisbach.com/wp-content/uploads/2020/08/polytope-1024x671.png" width="491"/>Polytope \(\mathcal{C}\), with a direction \(u\), the non-perturbed maximizer \(y^\ast(u)\), a perturbed direction \(u+\varepsilon Z\) and the perturbed maximizer \(y^\ast(u+\varepsilon Z)\). The areas of the red circles are proportional to the probability of selecting the corresponding extreme point after the perturbation. The expected perturbed maximizer \(y^\ast_\varepsilon(u)\) is in the interior of \(\mathcal{C}\).</figure></div>



<p class="justify-text">In a joint work with Quentin Berthet, Mathieu Blondel, Oliver Teboul, Marco Cuturi, and Jean-Philippe Vert [<a href="http://arxiv.org/pdf/2002.08676(opens in a new tab)">14</a>], we detail theoretical and practical properties of \(y^\ast_\varepsilon(u)\), in particular:</p>



<ul class="justify-text"><li>Estimation: \(y^\ast_\varepsilon(u)\) can be estimated by replacing the expectation by empirical averages.</li><li>Differentiability: if \(Z\) has a strictly positive density over \(\mathbb{R}^d\), then the function \(y^\ast_\varepsilon\) is infinitely differentiable, with simple expression of  the Jacobian, obtained by integration by parts (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>] for details).</li><li>The <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> is the simplest instance of such a smoothing technique, with \(\mathcal{C}\) being the simplex, and \(Z\) having independent Gumbel distributions. The function \(f_\varepsilon\) is then a “<a href="https://en.wikipedia.org/wiki/LogSumExp">log-sum-exp</a>” function.</li></ul>



<p class="justify-text"><strong>Illustration</strong>. Following [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>], this can be applied to learn the travel costs in graphs based on features. The vectors \(y_i\) represent shortest path between the top-left and bottom-right corners, with costs corresponding to the terrain type. See [<a href="https://arxiv.org/pdf/2002.08676">14</a>] for details on the learning procedure. Here I just want to highlight the effect of varying the amount of smoothing characterized by \(\varepsilon\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4605" height="225" src="https://francisbach.com/wp-content/uploads/2020/09/paths-1024x518.png" width="446"/>Left: Warcraft terrain. Right: Cost associated to each terrain type.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full"><img alt="" class="wp-image-4624" height="288" src="https://francisbach.com/wp-content/uploads/2020/09/anim_smoothed.gif" width="432"/>Shortest paths \(y^\ast_\varepsilon(u)\), from \(\varepsilon=0\) (no smoothing) to \(\varepsilon=2\). From an essentially single shortest path, as smoothing increases, we obtain a mixture of two potential paths, before having many extreme points.</figure></div>



<h2>Learning single-index models</h2>



<p class="justify-text">Given a random vector \((X,Y) \in \mathbb{R}^d \times \mathbb{R}\), we assume that \(Y = f(X) + \varepsilon\), where \(f(x) = \sigma(w^\top x)\) for some unknown function \(\sigma: \mathbb{R} \to \mathbb{R}\) and \(w \in \mathbb{R}^d\), with \(\varepsilon\) a zero-mean noise independent from \(X\).  Given some observations \((x_1,y_1), \dots, (x_n,y_n)\) in \(\mathbb{R}^d \times \mathbb{R}\), the goal is to estimate the direction \(w \in \mathbb{R}^d\). This model is referred to as single-index regression models in the statistics literature [<a href="https://www.jstor.org/stable/pdf/1913713.pdf">1</a>, <a href="https://www.jstor.org/stable/pdf/3035585.pdf">2</a>]</p>



<p class="justify-text">One possibility if \(\sigma\) was known would be to perform least-squares estimation and minimize with respect to \(w\) $$ \frac{1}{2n} \sum_{i=1}^n \big( y_i\  – \sigma(w^\top x_i) \big)^2, $$ which is a non-convex optimization problem in general. When \(\sigma\) is unknown, one could imagine adding the estimation of \(\sigma\) into the optimization, making it even more complicated.</p>



<p class="justify-text">Score functions provide an elegant solution that leads to the “average derivative method” (ADE) [<a href="https://www.jstor.org/stable/pdf/1914309.pdf">3</a>], which I will now describe. We consider \(p\) the density of \(X\). We then have, using Eq. (1): $$ \mathbb{E} \big[ Y \nabla \log p(X) \big] =\mathbb{E} \big[ f(X) \nabla \log p(X) \big] = \ – \mathbb{E} \big[ \nabla f(X)  \big] =\ –  \Big( \mathbb{E} \big[ \sigma'(w^\top X) \big] \Big) w, $$ which is proportional to \(w\). When replacing the expectation by an empirical mean, this provides a way to estimate \(w\) (up to a constant factor) without even knowing the function \(\sigma\), but assuming the density of \(X\) is known so that the score function is available.</p>



<p class="justify-text"><strong>Extensions.</strong> The ADE method can be extended in a number of ways to deal with more complex situations. Here are some examples below:</p>



<ul class="justify-text"><li><em>Multiple index models</em>: if the response/output \(Y\) is instead assumed of the form $$ Y = f(X) + \varepsilon =  \sigma(W^\top x) + \varepsilon, $$ where \(W \in \mathbb{R}^{d \times k}\) is a matrix with \(k\) columns, we obtained a “multiple index model”, for which a similar technique seems to apply since now \(\nabla f(x) = W \nabla  \sigma(W^\top x) \in \mathbb{R}^d\), and thus, for the assumed model \(\mathbb{E} \big[ Y \nabla \log p(X) \big]\) is in the linear span of the columns of \(W\); this is not enough for recovering the entire subspace if \(k&gt;1\) because we have only a single element of the span. There are two solutions for this. The first one is is to condition on some values of \(Y\) being in some set \(\mathcal{Y}\), where one can show that \(\mathbb{E} \big[ Y \nabla \log p(X) | Y \in \mathcal{Y} \big]\) is also in the desired subspace; thus, with several sets \(\mathcal{Y}\), one can generate several elements, and after \(k\) of these, one can expect to estimate the full \(k\)-dimensional subspace. The idea of conditioning on \(Y\) is called <a href="https://en.wikipedia.org/wiki/Sliced_inverse_regression">sliced inverse regression</a> [<a href="https://www.jstor.org/stable/pdf/2290563.pdf">15</a>], and the application to score function can be found in [<a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>]. The second one is to consider higher-order moments and derivatives of the score functions, that is, using integration by parts twice! (see [<a href="https://arxiv.org/pdf/1412.2863">17</a>, <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">18</a>, <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>] for details).</li><li><em>Neural networks</em>: when the function \(\sigma\) is the sum of functions that depends on single variables, multiple-index models are exactly one-hidden-layer neural networks. Similar techniques can be used for deep networks with more than a single hidden layer (see [<a href="https://arxiv.org/pdf/1506.08473">19</a>]).</li></ul>



<p class="justify-text"><strong>Moment matching vs. empirical risk minimization. </strong>In all cases mentioned above, the use of score functions can be seen as an instance of the <a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">method of moments</a>: we assume a specific model for the data, derive identities satisfied by expectations of some functions under the model, and use these identities to identify a parameter vector. In the situations above, direct empirical risk minimization would lead to a potentially hard optimization problem. However, moment matching techniques rely heavily on the model being well-specified, which is often not the case in practice, while empirical risk minimization techniques try to fit the data as much as the model allows, and is thus typically more robust to model misspecification.</p>



<h2>Score matching for density estimation</h2>



<p class="justify-text">We consider the problem of density estimation. That is, given some observations \(x_1,\dots,x_n \in \mathbb{R}^d\) sampled independently and identically distributed from some distribution with density \(p\), we want to estimate \(p\) from the data. Given a model \(q_\theta \) with some parameters \(\theta\), the most standard method is maximum likelihood estimation, which corresponds to the following optimization problem: $$\max_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n \log q_\theta(x_i).$$ It requires <em>normalized</em> densities that is, \(\int_{\mathbb{R}^d} q_\theta(x) dx = 1\), and dealing with normalized densities often requires to explicitly normalize them and thus to compute integrals, which is difficult when the underlying dimension \(d\) gets large.</p>



<p class="justify-text">Score matching is a recent method proposed by Aapo Hyvärinen [4] based on score functions. The simple (yet powerful) idea is to perform least-squares estimation on the score functions. That is, in the population case, the goal is to minimize $$\mathbb{E} \big\| \nabla \log p(X) \ – \nabla  \log q_\theta(X) \big\|_2^2 = \int_{\mathbb{R}^d} \big\| \nabla \log p(x)\  – \nabla  \log q_\theta(x) \big\|_2^2 p(x) dx.$$ Apparently, this expectation does not lead to an estimation procedure where \(p(x)\) is replaced by the empirical distribution of the data because of the presence of \(\nabla \log p(x)\). Integration by parts will solve this.</p>



<p class="justify-text">We can expand \(\mathbb{E} \big\| \nabla \log p(X) \ – \nabla \log q_\theta(X) \big\|_2^2\) as  $$ \mathbb{E} \big\| \nabla \log p(X) \|_2^2 + \mathbb{E} \big\|\nabla \log q_\theta(X) \big\|_2^2 – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big]. $$ The first term is independent of \(q_\theta\) so it does not count when minimizing. The second term is an expectation with respect to \(p(\cdot)\) so it can be replaced by the empirical mean. The third term can be dealt with with integration by parts, that is Eq. (2), leading to: $$ – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \nabla \cdot \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \Delta \log q_\theta(X) \big],$$ where \(\Delta\) is the <a href="https://en.wikipedia.org/wiki/Laplace_operator">Laplacian</a>.</p>



<p class="justify-text">We now have an expectation with respect to the data distribution \(p\), and we can replace the expectation with an empirical average to estimate the parameter \(\theta\) from data \(x_1,\dots,x_n\). We then use the cost function $$\frac{1}{n} \sum_{i=1}^n \big\|\nabla \log q_\theta(x_i) \big\|_2^2 + \frac{2}{n} \sum_{i=1}^n \Delta \log q_\theta(x_i), $$ which is linear in \(\log q_\theta\). Hence, when the unnormalized log-density is linearly parameterized, which is common, we obtain a quadratic problem. This procedure has a number of attractive properties, in particular consistency [<a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">4</a>], but the key benefit is to allow estimation without requiring normalizing constants.</p>



<h2>Conclusion</h2>



<p class="justify-text">Overall, the simple identity from Eq. (1), that is, \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla f(Z) \big]\), has many applications in diverse somewhat unrelated areas of machine learning, optimization and statistics. There are of course many other uses of integration by parts within this field. Feel free to add your preferred one as comment.</p>



<p class="justify-text">It has been a while since the last post on polynomial magic. I will revive the thread next month. I let you guess which polynomials will be the stars of my next blog post.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Quentin Berthet for producing the video of shortest paths, proofreading this blog post, and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] James L. Powell, James H. Stock, Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1913713.pdf">Semiparametric estimation of index coefficients</a>. <em>Econometrica: Journal of the Econometric Society</em>. 57(6):1403-1430, 1989.<br/>[2] Wolfgang Hardle, Peter Hall, Hidehiko Ichimura. <a href="https://www.jstor.org/stable/pdf/3035585.pdf">Optimal smoothing in single-index models</a>. <em>Annals of Statistics</em>. 21(1): 157-178(1993): 157-178.<br/>[3] Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1914309.pdf">Consistent Estimation of Scaled Coefficients</a>. <em>Econometrica</em>, 54(6):1461-1481, 1986.<br/>[4] Aapo Hyvärinen. <a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Estimation of non-normalized statistical models by score matching</a>. <em>Journal of Machine Learning Research</em>, <em>6</em>(Apr), 695-709, 2005.<br/>[5] Yurii Nesterov. <a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">Random gradient-free minimization of convex functions</a>. Technical report, Université Catholique de Louvain (CORE), 2011.<br/>[6] Boris T. Polyak and Alexander B. Tsybakov. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">Optimal order of accuracy of search algorithms in stochastic optimization</a>. <em>Problemy Peredachi Informatsii</em>, 26(2):45–53, 1990.<br/>[7]  Abraham D. Flaxman, Adam Tauman Kalai, H. Brendan McMahan. <a href="https://arxiv.org/pdf/cs/0408007">Online convex optimization in the bandit setting: gradient descent without a gradient</a>. In Proc. Symposium on Discrete algorithms (SODA), 2005.<br/>[8] John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. <a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">Randomized Smoothing for Stochastic Optimization</a>. SIAM Journal on Optimization, 22(2), 674–701, 2012.<br/>[9] Alexandre d’Aspremont, Nourredine El Karoui, <a href="https://www.di.ens.fr/~aspremon/stochsmooth.html">A Stochastic Smoothing Algorithm for Semidefinite Programming.</a> <em>SIAM Journal on Optimization</em>, 24(3): 1138-1177, 2014.<br/>[10] Yurii Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions</a>. Mathematical Programming, 103(1):127–152, 2005.<br/>[11] Lin Xiao. <a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a>. <em>Journal of Machine Learning Research</em>, 11(88): 2543−2596, 2010.<br/>[12] Guanghui Lan. <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">An optimal method for stochastic composite optimization</a>. <em>Mathematical Programming</em>, 133(1):365–397, 2012.<br/>[13] Tamir Hazan, George Papandreou, and Daniel Tarlow. <a href="https://mitpress.mit.edu/books/perturbations-optimization-and-statistics">Perturbation, Optimization, and Statistics</a>. MIT Press, 2016.<br/>[14] Quentin Berthet, Matthieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach, <a href="https://arxiv.org/pdf/2002.08676">Learning with differentiable perturbed optimizers</a>. Technical report arXiv 2002.08676, 2020.<br/>[15] Ker-Chau Li. <a href="https://www.jstor.org/stable/pdf/2290563.pdf">Sliced inverse regression for dimension reduction</a>. <em>Journal of the American Statistical Association</em>, <em>86</em>(414), 316-327, 1991.<br/>[16] Dmitry Babichev and Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">Slice inverse regression with score functions</a>. <em>Electronic Journal of Statistics</em>, 12(1):1507-1543, 2018.<br/>[17] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1412.2863">Score function features for discriminative learning: Matrix and tensor framework</a>. Technical report arXiv:1412.2863, 2014.<br/>[18] David R. Brillinger. <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">A generalized linear model with “Gaussian” regressor variables</a>.  <em>Selected Works of David Brillinger</em>, 589-606, 2012.<br/>[19] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1506.08473">Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods</a>.  Technical report arXiv:1506.08473, 2015.<br/>[20] Vlad Niculae, André F. T. Martins, Mathieu Blondel, and Claire Cardie. <a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">SparseMAP: Differentiable sparse structured inference</a>. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 2017.<br/>[21] George Papandreou and Alan L. Yuille.<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242"> Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models</a>. <em>International Conference on Computer Vision</em>, 2011.<br/>[22] Tamir Hazan and Tommi Jaakkola. <a href="https://icml.cc/Conferences/2012/papers/528.pdf">On the partition function and random maximum a-posteriori perturbations</a>. <em>Proceedings of the International Conference on International Conference on Machine Learning (ICML),</em> 2012.<br/>[23] Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. <a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">Perturbation techniques in online learning and optimization</a>. <em>Perturbations, Optimization, and Statistics</em>, 233-264, 2016.<br/>[24] Marin Vlastelica, Anselm Paulus, Vít Musil, Georg Martius, Michal Rolínek. <a href="https://openreview.net/pdf?id=BkevoJSYPB">Differentiation of Blackbox Combinatorial Solvers</a>. <em>International Conference on Learning Representations</em>. 2019.</p></div>
    </content>
    <updated>2020-09-07T19:06:36Z</updated>
    <published>2020-09-07T19:06:36Z</published>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-09-13T20:22:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/132</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/132" rel="alternate" type="text/html"/>
    <title>TR20-132 |  Towards Stronger Counterexamples to the Log-Approximate-Rank Conjecture | 

	Arkadev Chattopadhyay, 

	Ankit Garg, 

	Suhail Sherif</title>
    <summary>We give improved separations for the query complexity analogue of the log-approximate-rank conjecture i.e. we show that there are a plethora of total Boolean functions on $n$ input bits, each of which has approximate Fourier sparsity at most $O(n^3)$ and randomized parity decision tree complexity $\Theta(n)$. This improves upon the recent work of Chattopadhyay, Mande and Sherif (JACM '20) both qualitatively (in terms of designing a large number of examples) and quantitatively (improving the gap from quartic to cubic). We leave open the problem of proving a randomized communication complexity lower bound for XOR compositions of our examples. A linear lower bound would lead to new and improved refutations of the log-approximate-rank conjecture. Moreover, if any of these compositions had even a sub-linear cost randomized communication protocol, it would demonstrate that randomized parity decision tree complexity does not lift to randomized communication complexity in general (with the XOR gadget).</summary>
    <updated>2020-09-07T15:45:54Z</updated>
    <published>2020-09-07T15:45:54Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-13T20:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-83517349672236531</id>
    <link href="https://blog.computationalcomplexity.org/feeds/83517349672236531/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/83517349672236531" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/83517349672236531" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html" rel="alternate" type="text/html"/>
    <title>Two Math Problems of interest (at least to me)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I will give two math problems that are of interest to me.</p><p>These are not new problems, however you will have more fun if you work on them yourself and leave comments on what you find. So if you want to work on it without hints, don't read the comments.</p><p><br/></p><p>I will post about the answers (not sure I will post THE answers) on Thursday.</p><p><br/></p><p>1) Let x(1)&gt;0. Let x(n+1) = (  1 + (1/x(n))  )^n. </p><p><br/></p><p>For how many values of x(1) does this sequence go to infinity?</p><p><br/></p><p>2) Find all (x,y) \in N \times N such that x^2+3y and y^2+3x are both squares. </p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2020-09-06T21:05:00Z</updated>
    <published>2020-09-06T21:05:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-13T13:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/131</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/131" rel="alternate" type="text/html"/>
    <title>TR20-131 |  A Direct Product Theorem for One-Way Quantum Communication | 

	Srijita Kundu, 

	Rahul  Jain</title>
    <summary>We prove a direct product theorem for the one-way entanglement-assisted quantum communication complexity of a general relation $f\subseteq\mathcal{X}\times\mathcal{Y}\times\mathcal{Z}$. For any $\varepsilon, \zeta &gt; 0$ and any $k\geq1$, we show that
\[ \mathrm{Q}^1_{1-(1-\varepsilon)^{\Omega(\zeta^6k/\log|\mathcal{Z}|)}}(f^k) = \Omega\left(k\left(\zeta^5\cdot\mathrm{Q}^1_{\varepsilon + 12\zeta}(f) - \log\log(1/\zeta)\right)\right),\]
where $\mathrm{Q}^1_{\varepsilon}(f)$ represents the one-way entanglement-assisted quantum communication complexity of $f$ with worst-case error $\varepsilon$ and $f^k$ denotes $k$ parallel instances of $f$.

As far as we are aware, this is the first direct product theorem for quantum communication -- direct sum theorems were previously known for one-way quantum protocols. Our techniques are inspired by the parallel repetition theorems for the entangled value of two-player non-local games, under product distributions due to Jain, Pereszl\'{e}nyi and Yao, and under anchored distributions due to Bavarian, Vidick and Yuen, as well as message-compression for quantum protocols due to Jain, Radhakrishnan and Sen. In particular, we show that a direct product theorem holds for the distributional one-way quantum communication complexity of $f$ under any distribution $q$ on $\mathcal{X}\times\mathcal{Y}$ that is anchored on one side, i.e., there exists a $y^*$ such that $q(y^*)$ is constant and $q(x|y^*) = q(x)$ for all $x$. This allows us to show a direct product theorem for general distributions, since for any relation $f$ and any distribution $p$ on its inputs, we can define a modified relation $\tilde{f}$ which has an anchored distribution $q$ close to $p$, such that a protocol that fails with probability at most $\varepsilon$ for $\tilde{f}$ under $q$ can be used to give  a protocol that fails with probability at most $\varepsilon + \zeta$ for $f$ under $p$.

Our techniques also work for entangled non-local games which have input distributions anchored on any one side, i.e., either there exists a $y^*$ as previously specified, or there exists an $x^*$ such that $q(x^*)$ is constant and $q(y|x^*) = q(y)$ for all $y$. In particular, we show that for any game $G = (q, \mathcal{X}\times\mathcal{Y}, \mathcal{A}\times\mathcal{B}, V)$ where $q$ is a distribution on $\mathcal{X}\times\mathcal{Y}$ anchored on any one side with anchoring probability $\zeta$, then
\[ \omega^*(G^k) = \left(1 - (1-\omega^*(G))^5\right)^{\Omega\left(\frac{\zeta^2 k}{\log(|\mathcal{A}|\cdot|\mathcal{B}|)}\right)}\]
where $\omega^*(G)$ represents the entangled value of the game $G$. This is a generalization of the result of Bavarian, Vidick and Yuen, who proved a parallel repetition theorem for games anchored on both sides, i.e., where both a special $x^*$ and a special $y^*$ exist, and potentially a simplification of their proof.</summary>
    <updated>2020-09-06T05:33:09Z</updated>
    <published>2020-09-06T05:33:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-13T20:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/130</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/130" rel="alternate" type="text/html"/>
    <title>TR20-130 |  Optimal Inapproximability of Satisfiable k-LIN over Non-Abelian Groups | 

	Amey Bhangale, 

	Subhash Khot</title>
    <summary>A seminal result of H\r{a}stad [J. ACM, 48(4):798–859, 2001]  shows that it is NP-hard to find an assignment that satisfies $\frac{1}{|G|}+\varepsilon$ fraction of the constraints of a given $k$-LIN instance over an abelian group, even if there is an assignment that satisfies $(1-\varepsilon)$ fraction of the constraints, for any constant $\varepsilon&gt;0$.  Engebretsen et al. [Theoretical Computer Science, 312(1):17–45, 2004] later showed that the same hardness result holds for $k$-LIN instances over any finite non-abelian group.

Unlike the abelian case, where we can efficiently find a solution if the instance is satisfiable, in the non-abelian case, it is NP-complete to decide if a given system of linear equations is satisfiable or not, as shown by Russell and Goldmann [Information and Computation, 178(1):253–262, 2002].  

Surprisingly, for certain non-abelian groups $G$, given a satisfiable $k$-LIN instance over $G$, one can in fact do better than just outputting a random assignment using a simple but clever algorithm. The approximation factor achieved by this algorithm varies with the underlying group. In this paper, we show that this algorithm is {\em optimal} by proving a  tight hardness of approximation of satisfiable $k$-LIN instance over {\em any} non-abelian $G$, assuming $P \neq NP$.

As a corollary, we also get $3$-query probabilistically checkable proofs with perfect completeness over large alphabets with improved soundness.</summary>
    <updated>2020-09-06T04:42:45Z</updated>
    <published>2020-09-06T04:42:45Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-13T20:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17507</id>
    <link href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/" rel="alternate" type="text/html"/>
    <title>Closing An Open Problem</title>
    <summary>Crawl, then walk, then run. Bogdan Grechuk is a lecturer in the math department at the University of Leicester. His office is in the Michael Atiyah Building. Pretty cool. He works in risk analysis, but is more broadly interested in math of all kinds. See his wonderful book Theorems of the 21st Century. Or go […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Crawl, then walk, then run.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/bg/" rel="attachment wp-att-17512"><img alt="" class="alignright  wp-image-17512" src="https://rjlipton.files.wordpress.com/2020/09/bg.png?w=150" width="150"/></a></p>
<p>Bogdan Grechuk is a lecturer in the math department at the University of Leicester. His office is in the Michael Atiyah Building. Pretty cool. He works in risk analysis, but is more broadly interested in math of all kinds. See his wonderful book <a href="https://link.springer.com/book/10.1007%2F978-3-030-19096-5">Theorems of the 21st Century</a>. Or go to his web <a href="https://theorems.home.blog/theorems-list/">site</a>.</p>
<p>
Today Ken and I want to talk about solving open problems.<br/>
<span id="more-17507"/></p>
<p>
Grechuk’s site got us thinking about results that solve open problems. Most of us like to think our research solves an open problem. Personally I can say that I have tried to solve problems for the first time, but did not always succeed. </p>
<p>
Open problems usually mean something stronger. To be an open problem, a problem must be known to some community for some time. Advances in math and in complexity theory roughly fall into two categories: </p>
<ol>
<li>
Results that prove or disprove something that was explicitly stated before. The more who knew the problem the better. The longer the problem was known the better, too. <p/>
</li><li>
Results that prove something that is new. Something that no one had explicitly asked before.
</li></ol>
<p>Both type of results are important. The latter kind may ultimately be more important. They raise new questions, often contain new methods, and move the field ahead in a new direction. See our<br/>
<a href="https://rjlipton.wordpress.com/2011/02/01/godel&#x2019;s-lost-letter-is-two-years-old/">discussion</a> of Freeman Dyson and frogs and birds.</p>
<p>
Think of how Kurt Gödel’s incompleteness theorem was unsuspected, or how Alan Turing’s proof of undecidability of the halting problem came in tandem with settling the criterion for computability, or years latter the definition of public-key crypto-systems. But we will focus on open problems in the sense (1). </p>
<p/><h2> Claiming A Solution </h2><p/>
<p/><p>
Ken and I are amazed that when an open problem is claimed, especially for P versus NP, the claim swallows it whole. That is the claim is that the full problem is solved. We do not recall once when the claim was: </p>
<ul>
<li>
We are able to prove that SAT requires quadratic time, or <p/>
</li><li>
We can show that SAT is in co-NP.
</li></ul>
<p>Either of these would be a “stop the press” result. </p>
<p>
For a more concrete example, suppose you claim to have a polynomial-time algorithm for finding a maximum clique in an undirected graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. Of course this implies P<img alt="{=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{=}"/>NP. Your algorithm may require a chain of difficult lemmas that obscure its workings. Can you perhaps analyze its effectiveness more easily on <em>random</em> graphs? Here are two relevant facts:</p>
<ul>
<li>
In 1976, David Matula <a href="https://s2.smu.edu/~matula/Tech-Report76.pdf">proved</a> that with high probability for a random <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-vertex graph of edge probability <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/>, the size of the maximum clique is one of the two integers flanking <img alt="{2\log_{1/p}(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\log_{1/p}(n)}"/>. <p/>
</li><li>
As observed in 1976 by Dick Karp in his <a href="https://www.semanticscholar.org/paper/The-probabilistic-analysis-of-some-combinatorial-Karp/9a9558d79b93fd884354f1ae27463be2836d2ec0">paper</a>, “The Probabilistic Analysis of Some Combinatorial Search Algorithms,” no polynomial time algorithm is known to achieve size <img alt="{(1+\epsilon)\log_{1/p}(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2B%5Cepsilon%29%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1+\epsilon)\log_{1/p}(n)}"/>, for any <img alt="{\epsilon &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt; 0}"/> and sufficiently large <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>.
</li></ul>
<p>
You only need to close a gap of a factor of <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>, not to hit the maximum value exactly, and you do not need to succeed for all graphs. The behavior of random graphs should help your analysis. A more-recent mention of Karp’s open problem is in these 2005 <a href="https://www.math.cmu.edu/~af1p/MAA2005/L7.pdf">slides</a>.</p>
<p>
</p><p/><h2> Some Examples </h2><p/>
<p/><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b> Collatz Conjecture </b>: Terence Tao made important <a href="https://terrytao.wordpress.com/2019/09/10/almost-all-collatz-orbits-attain-almost-bounded-values/">progress</a> on this notorious <a href="https://terrytao.files.wordpress.com/2020/02/collatz.pdf">problem</a>. He said: </p>
<blockquote><p><b> </b> <em> In mathematics, when we cannot solve a problem completely, we look for partial results. Even if they do not lead to a complete solution, they often reveal insights about the problem. </em>
</p></blockquote>
<p>Recall this is also called the <img alt="{3n+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3n%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3n+1}"/> problem. It asks for the long-term behavior of the function: <img alt="f(n) " class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="f(n) "/> which is equal to <img alt="n/2 " class="latex" src="https://s0.wp.com/latex.php?latex=n%2F2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n/2 "/> for <img alt="n " class="latex" src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n "/> even and <img alt="3n+1 " class="latex" src="https://s0.wp.com/latex.php?latex=3n%2B1+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="3n+1 "/> for <img alt="n " class="latex" src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n "/> odd. </p>
<p>The conjecture is that for every <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, iterating the function eventually hits <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, i.e., <img alt="{f^i(n) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f^i(n) = 1}"/> for some <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>.</p>
<p>
There are two ways the conjecture can fail:</p>
<ul>
<li>
There is a finite cycle besides the trivial cycle <img alt="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Crightarrow+4+%5Crightarrow+2+%5Crightarrow+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}"/>. <p/>
</li><li>
For some <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, the sequence <img alt="{f^i(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f^i(n)}"/> goes off to infinity.
</li></ul>
<p>
What Tao proved is that “many” values <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> achieve: <img alt="f^i(n) &lt; \log^* n " class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ei%28n%29+%3C+%5Clog%5E%2A+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="f^i(n) &lt; \log^* n "/> for some <img alt="i " class="latex" src="https://s0.wp.com/latex.php?latex=i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i "/>.</p>
<p>Grechuk's <a href="https://theorems.home.blog/2020/04/29/almost-all-orbits-of-the-collatz-map-attain-almost-bounded-value/">page</a> includes the definition of “many,” which turns out to be <em>weaker</em> than saying a <img alt="{(1-\epsilon)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281-%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1-\epsilon)}"/> proportion of values <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> “swing low” in this sense. Moreover, Tao proved this for any unbounded function <img alt="{g(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(n)}"/> in place of the iterated logarithm, such as the inverse Ackermann function. Note this is a case where randomized analysis worked—in the hands of a master.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b> Twin Prime Conjecture </b>: Yitang Zhang made tremendous progress on this long standing <a href="https://en.wikipedia.org/wiki/Twin_prime">conjecture</a>. He proved that infinitely often is a prime <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> so there is another prime <img alt="{q&gt;p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%3Ep%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q&gt;p}"/> bounded above by <img alt="{p +C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%2BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p +C}"/>. His <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> was <img alt="{70}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{70}"/> million, but this was still a breakthrough. Previously no bounded <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> was known. We discussed this before <a href="https://rjlipton.wordpress.com/2013/05/21/twin-primes-are-useful/">here</a>. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b> Sensitivity Conjecture </b>: Hao Huang is given as a <a href="http://mentalfloss.com/article/52698/how-does-exception-prove-rule">counterexample</a> to partial progress—it is the exception that proves the rule. He solved the full <a href="https://arxiv.org/pdf/1907.00847.pdf">conjecture</a>. He proved that every <img alt="{2^{n}-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%7D-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{n}-1}"/> vertex induced subgraph of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-dimensional cube graph has maximum degree at least <img alt="{\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{n}}"/>. The previous best was only order logarithm in <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. But there does remain some slack: He proved a fourth-power bound, but can it be closed to cubic or even quadratic? We <a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/">discussed</a> this last year.</p>
<p>
By the <a href="https://www.mentalfloss.com/article/52698/how-does-exception-prove-rule">way</a>: </p>
<blockquote><p><b> </b> <em> The expression comes from the Latin legal principle exceptio probat regulam (the exception proves the rule), also rendered as exceptio firmat regulam (the exception establishes the rule) and exceptio confirmat regulam (the exception confirms the rule). The principle provides legal cover for inferences such as the following: if I see a sign reading “no swimming allowed after 10 pm,” I can assume swimming is allowed before that time. </em>
</p></blockquote>
<p>
</p><p/><h2> Advice To Claimers </h2><p/>
<p/><p>
Our general advice to claimers: </p>
<blockquote><p><b> </b> <em> <i>Okay you are sure you have solved the big problem. Write up the weakest new result that you can.</i> </em>
</p></blockquote>
<p/><p>
Use your methods, your insights, to minimize the work needed for someone to be 99.99% convinced that you have proved something <em>new</em>, rather than a lower confidence of your having proved something <em>huge</em>. For P<img alt="{=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{=}"/>NP show that you have a exponential algorithm that is better than known. Or for P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP give a non-linear lower bound. </p>
<p>
The rationale is: You are more likely to get someone to read your paper if you make a weaker claim. A paper titled: <i>A New SAT Algorithm that Runs in Sub-exponential Time</i> is more likely to get readers than a paper tiled <i>P=NP</i>. This shows that readership is non-monotone. </p>
<p>
This is consequence of two phenomena: One is believability. The weaker paper is more likely to be correct. One is human. The stronger paper, if correct, may not be easy to improve. A weaker paper could have results that the reader could improve and write a follow-on paper: </p>
<blockquote><p><b> </b> <em> In Carol Fletcher’s recent paper an <img alt="{2^{n^{1/3}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%5E%7B1%2F3%7D%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{2^{n^{1/3}}}"/> algorithm is found for SAT. She required the full Riemann Hypothesis. We remove that requirement and <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\dots}"/> </em>
</p></blockquote>
<p/><h2> Open Problems </h2><p/>
<p/><p>
What about our advice: what would you do if you solved a major open problem? Note that the examples we highlighted all have slack for improvement short of the optimum statements.</p></font></font></div>
    </content>
    <updated>2020-09-05T17:25:13Z</updated>
    <published>2020-09-05T17:25:13Z</published>
    <category term="News"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Advice"/>
    <category term="open problems"/>
    <category term="proof checking"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-13T20:20:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/129</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/129" rel="alternate" type="text/html"/>
    <title>TR20-129 |  A Lower Bound on Determinantal Complexity | 

	Mrinal Kumar, 

	Ben Lee Volk</title>
    <summary>The determinantal complexity of a polynomial $P \in \mathbb{F}[x_1,  \ldots, x_n]$ over a field $\mathbb{F}$ is the dimension of the smallest matrix $M$ whose entries are affine functions in $\mathbb{F}[x_1,  \ldots, x_n]$ such that $P = Det(M)$. We prove that the determinantal complexity of the polynomial $\sum_{i = 1}^n x_i^n$ is at least $1.5n - 3$. 

For every $n$-variate polynomial of degree $d$, the determinantal complexity is trivially at least $d$, and it is a long standing open problem to prove a lower bound which is super linear in $\max\{n,d\}$. Our result is the first lower bound for any explicit polynomial which is bigger by a constant factor than $\max\{n,d\}$, and improves upon the prior best bound of $n + 1$, proved by Alper, Bogart and Velasco [ABV17] for the same polynomial.</summary>
    <updated>2020-09-05T03:25:44Z</updated>
    <published>2020-09-05T03:25:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-13T20:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5519</id>
    <link href="https://adamsheffer.wordpress.com/2020/09/04/mathematics-for-human-flourishing/" rel="alternate" type="text/html"/>
    <title>Mathematics for Human Flourishing</title>
    <summary>I read a lot of “popular math” books. I also wrote about some in past posts. But I’ve never read a book similar to Mathematics for Human Flourishing by Francis Su. I already knew the math presented in this book and almost all other topics covered. I even knew some of Su’s personal stories from […]</summary>
    <updated>2020-09-04T21:21:41Z</updated>
    <published>2020-09-04T21:21:41Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2020-09-13T20:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20205</id>
    <link href="https://gilkalai.wordpress.com/2020/09/04/alef-corner-math-collaboration/" rel="alternate" type="text/html"/>
    <title>Alef Corner: Math Collaboration</title>
    <summary>Another artistic view by Alef on mathematical collaboration.   Other Alef’s corner posts</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Another artistic view by Alef on mathematical collaboration.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/09/mathcoll.jpg"><img alt="" class="alignnone size-full wp-image-20207" height="640" src="https://gilkalai.files.wordpress.com/2020/09/mathcoll.jpg?w=640&amp;h=640" width="640"/></a></p>
<p> </p>
<p>Other <a href="https://gilkalai.wordpress.com/tag/alefs-corner/">Alef’s corner</a> posts</p></div>
    </content>
    <updated>2020-09-04T07:17:13Z</updated>
    <published>2020-09-04T07:17:13Z</published>
    <category term="Art"/>
    <category term="Combinatorics"/>
    <category term="What is Mathematics"/>
    <category term="Alef's corner"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-09-13T20:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=52</id>
    <link href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/" rel="alternate" type="text/html"/>
    <title>Friday, Sept 11 — Bin Yu from UC Berkeley</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Friday, Sept 11th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  Bin Yu from UC Berkeley will speak about “Veridical Data Science”. Abstract: Building and expanding on principles of statistics, machine learning, and the sciences, we propose<a class="more-link" href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/">Continue reading <span class="screen-reader-text">"Friday, Sept 11 — Bin Yu from UC Berkeley"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next Foundations of Data Science virtual talk will take place on Friday, Sept 11th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Bin Yu </strong>from UC Berkeley will speak about “<em>Veridical Data Science</em>”.</p>



<p><strong>Abstract</strong>: Building and expanding on principles of statistics, machine learning, and the sciences, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework is comprised of both a workflow and documentation and aims to provide responsible, reliable, reproducible, and transparent results across the entire data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle for the data science life cycle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. We develop inference procedures that build on PCS, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations.</p>



<p>Moreover, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis.</p>



<p>The PCS framework will be illustrated through our DeepTune approach to model and characterize neurons in the difficult visual cortex area V4.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-09-04T00:47:54Z</updated>
    <published>2020-09-04T00:47:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-09-13T20:22:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsmath.wordpress.com/?p=2298</id>
    <link href="https://tcsmath.wordpress.com/2020/09/03/itcs-2021-call-for-papers-deadline-extension/" rel="alternate" type="text/html"/>
    <title>ITCS 2021 Call For Papers (deadline extension)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">NOTE: The deadline has been extended to Tuesday, September 8th to account for the Labor Day Holiday in the US. The 12th Innovations in Theoretical Computer Science (ITCS) conference will be held online from January 6-8, 2021. The submission deadline is now September 8, 2020. The program committee encourages you to send your papers our way! See the call for papers for information about submitting to the … <a class="more-link" href="https://tcsmath.wordpress.com/2020/09/03/itcs-2021-call-for-papers-deadline-extension/">Continue reading <span class="screen-reader-text">ITCS 2021 Call For Papers (deadline extension)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>NOTE</strong>:  The deadline has been extended to Tuesday, September 8th to account for the Labor Day Holiday in the US.</p>



<p>The <strong>12th Innovations in Theoretical Computer Science (ITCS)</strong> conference will be held <strong>online</strong> from <strong>January 6-8, 2021</strong>. The <strong>submission deadline</strong> is now <strong>September 8, 2020</strong>.</p>



<p>The <a href="http://itcs-conf.org/">program committee</a> encourages you to send your papers our way! See the <a href="http://itcs-conf.org/">call for papers</a> for information about submitting to the conference.</p>



<p>ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the greater theory community.</p>



<h3>Important dates</h3>



<ul><li><strong>Submission deadline: </strong>September 8, 2020 (05:59PM PDT)</li><li><strong>Notification to authors:</strong> November 1, 2020</li><li><strong>Conference dates: </strong>January 6-8, 2021</li></ul></div>
    </content>
    <updated>2020-09-03T21:53:56Z</updated>
    <published>2020-09-03T21:53:56Z</published>
    <category term="Math"/>
    <author>
      <name>James</name>
    </author>
    <source>
      <id>https://tcsmath.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsmath.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsmath.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsmath.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsmath.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>some mathematics &amp; computation</subtitle>
      <title>tcs math</title>
      <updated>2020-09-13T20:20:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4949</id>
    <link href="https://www.scottaaronson.com/blog/?p=4949" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4949#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4949" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">My Utility+ podcast with Matthew Putman</title>
    <summary xml:lang="en-US">Another Update (Sep. 8): A reader wrote to let me know about a fundraiser for Denys Smirnov, a 2015 IMO gold medalist from Ukraine who needs an expensive bone marrow transplant to survive Hodgkin’s lymphoma. I just donated and I hope you’ll consider it too! Update (Sep. 5): Here’s another quantum computing podcast I did, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Another Update (Sep. 8):</span></strong> A reader wrote to let me know about a <a href="https://www.gofundme.com/f/help-denis-smirnov">fundraiser for Denys Smirnov</a>, a 2015 IMO gold medalist from Ukraine who needs an expensive bone marrow transplant to survive Hodgkin’s lymphoma.  I just donated and I hope you’ll consider it too!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Sep. 5):</span></strong> <a href="https://dunctank.podbean.com/e/scott-aaronson-infinite-universes/">Here’s another quantum computing podcast I did</a>, “Dunc Tank” with Duncan Gammie.  Enjoy!</p>



<p/><hr/>
<p><br/>Thanks so much to <em>Shtetl-Optimized</em> readers, so far we’ve raised $1,371 for the <a href="https://secure.actblue.com/donate/duforjoe">Biden-Harris campaign</a> and $225 for the <a href="https://lincolnproject.us/donate/">Lincoln Project</a>, which I intend to match for $3,192 total.  If you’d like to donate by tonight (Thursday night), there’s still $404 to go!</p><p/>



<p>Meanwhile, a mere three days after declaring my <a href="https://www.scottaaronson.com/blog/?p=4942">“new motto,”</a> I’ve come up with a <em>new</em> new motto for this blog, hopefully a more cheerful one:</p>



<blockquote class="wp-block-quote"><p>When civilization seems on the brink of collapse, sometimes there’s nothing left to talk about but maximal separations between randomized and quantum query complexity.</p></blockquote>



<p>On that note, please enjoy my new <a href="https://nanotronics.co/thinkspace/24-scott-aaronson-before-and-after-the-machine/">one-hour podcast on Spotify</a> (if that link doesn’t work, <a href="https://overcast.fm/+UZFvS_CT8">try this one</a>) with <a href="https://en.wikipedia.org/wiki/Matthew_Putman_(scientist)">Matthew Putman</a> of Utility+.  Alas, my umming and ahhing were more frequent than I now aim for, but that’s partly compensated for by Matthew’s excellent decision to speed up the audio.  This was an unusually wide-ranging interview, covering everything from SlateStarCodex to quantum gravity to interdisciplinary conferences to the challenges of teaching quantum computing to 7-year-olds.  I hope you like it!</p></div>
    </content>
    <updated>2020-09-03T16:43:36Z</updated>
    <published>2020-09-03T16:43:36Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-09-08T18:48:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/128</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/128" rel="alternate" type="text/html"/>
    <title>TR20-128 |  An Optimal Separation of Randomized and Quantum Query Complexity | 

	Alexander A. Sherstov, 

	Andrey Storozhenko, 

	Pei Wu</title>
    <summary>We prove that for every decision tree, the absolute values of the Fourier coefficients of given order $\ell\geq1$ sum to at most $c^{\ell}\sqrt{{d\choose\ell}(1+\log n)^{\ell-1}},$ where $n$ is the number of variables, $d$ is the tree depth, and $c&gt;0$ is an absolute constant. This bound is essentially tight and settles a conjecture due to Tal (arxiv 2019; FOCS 2020). The bounds prior to our work degraded rapidly with $\ell,$ becoming trivial already at $\ell=\sqrt{d}.$

As an application, we obtain, for any positive integer $k,$ a partial Boolean function on $n$ bits that has bounded-error quantum query complexity at most $\lceil k/2\rceil$ and randomized query complexity $\tilde{\Omega}(n^{1-1/k}).$ This separation of bounded-error quantum versus randomized query complexity is best possible, by the results of Aaronson and Ambainis (STOC 2015). Prior to our work, the best known separation was polynomially weaker: $O(1)$ versus $n^{2/3-\epsilon}$ for any $\epsilon&gt;0$ (Tal, FOCS 2020).</summary>
    <updated>2020-09-03T13:24:26Z</updated>
    <published>2020-09-03T13:24:26Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-13T03:20:50Z</updated>
    </source>
  </entry>
</feed>
