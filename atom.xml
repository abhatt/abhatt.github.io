<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-10-22T23:32:53Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/</id>
    <link href="https://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/" rel="alternate" type="text/html"/>
    <title>Tenure-Track Assistant Professor at Rutgers University (apply by January 15, 2021)</title>
    <summary>The Computer Science Department at Rutgers University invites applications for a Tenure-Track Assistant Professor position in Theoretical Computer Science. We welcome candidates working on computational complexity theory but outstanding applicants in all areas of TCS will be considered. Website: http://jobs.rutgers.edu/postings/120527 Email: martin@farach-colton.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science Department at Rutgers University invites applications for a Tenure-Track Assistant Professor position in Theoretical Computer Science. We welcome candidates working on computational complexity theory but outstanding applicants in all areas of TCS will be considered.</p>
<p>Website: <a href="http://jobs.rutgers.edu/postings/120527">http://jobs.rutgers.edu/postings/120527</a><br/>
Email: martin@farach-colton.com</p></div>
    </content>
    <updated>2020-10-22T18:10:24Z</updated>
    <published>2020-10-22T18:10:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-22T23:31:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/" rel="alternate" type="text/html"/>
    <title>Professorship/Chair at TU Hamburg (apply by November 29, 2020)</title>
    <summary>TU Hamburg invites applications for a full professorship (chair) in Hardware-aware Combinatorial Optimization, at its School of Electrical Engineering, Computer Science and Mathematics. Our goal is to establish a group that excels at developing and implementing state-of-the-art optimization techniques on modern computing architectures at hardware level. The chair is endowed by Fujitsu. Website: https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1 Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>TU Hamburg invites applications for a full professorship (chair) in Hardware-aware Combinatorial Optimization, at its School of Electrical Engineering, Computer Science and Mathematics. Our goal is to establish a group that excels at developing and implementing state-of-the-art optimization techniques on modern computing architectures at hardware level. The chair is endowed by Fujitsu.</p>
<p>Website: <a href="https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1">https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1</a><br/>
Email: berufungen@tuhh.de</p></div>
    </content>
    <updated>2020-10-22T16:13:33Z</updated>
    <published>2020-10-22T16:13:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-22T23:31:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17694</id>
    <link href="https://rjlipton.wordpress.com/2020/10/22/vaccines-are-not-developing/" rel="alternate" type="text/html"/>
    <title>Vaccines are Not Developing</title>
    <summary>The search for a vaccine—is not a development. Edward Jenner was an English physician who created the first vaccine, one for smallpox. In 1798 he used the weak cox-pox to fool our immune system to create a protection against the deadly small-pox. Jenner is said to have saved more lives than any other human. Today […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="color: #0044cc;"><br/>
<em>The search for a vaccine—is not a development.</em><br/>
</span></p>
<p>Edward Jenner was an English physician who created the first vaccine, one for smallpox.<br/>
<a href="https://rjlipton.wordpress.com/jenner/"><img alt="" class="alignright size-full wp-image-17696" src="https://rjlipton.files.wordpress.com/2020/10/jenner.jpg?w=600"/></a><br/>
In 1798 he used the weak cox-pox to fool our immune system to create a protection against the deadly small-pox. Jenner is said to have <i>saved more lives than any other human.</i></p>
<p>Today there is an attempt to create a vaccine against our small-pox of the 21st century.</p>
<p>In his day small-pox killed 10% or more of populations. In our day there is a similar threat. and thus the immense interest in the development of a vaccine. However, there is a misunderstanding about vaccines for COVID-19 that is pervasive. Read the New York Times or watch cable news—CNN, FOX, MSNBC—where “experts” explain how AstraZeneca, Johnson &amp; Johnson, Novavax, and other drug companies are developing a vaccine. What developing means could potentially affect all of us, and a better understanding could save millions of lives.</p>
<p>They are not currently <i>developing</i> the vaccines, they are <i>testing</i> them.  The point we want to emphasize is:</p>
<blockquote>
<p><b> </b> <em> <i>The development of a vaccine does not change the vaccine. The vaccine is the same at the start of its testing trials, and remains the same throughout.</i> </em></p>
</blockquote>
<p>The Oxford vaccine AZD1222 is the same today as it was months ago when it was created. The same is true for the other vaccines currently being tested around the world.</p>
<p>A vaccine is <b>not</b> developed in the usual sense. Drug companies can modify: how the drug is made, how it is stored, how it is given, how many doses are needed, and so on. Drug companies cannot modify the vaccine without starting over—the vaccine must remain the same. Trials can lead to a vaccine being adopted, or it can cause the vaccine to be abandoned. In the later case the drug company can try again, but with a different vaccine.</p>
<h2>Not Development</h2>
<p>Think of the what development means elsewhere.</p>
<ul>
<li>In programming an app: We build a version and try it out. We find bugs and fix them. We use version numbers. Note, there is no AZD1222 version 3.</li>
<li>In writing a book: We make a draft. We have people read the draft. We fix typos and inaccuracies. Our quantum book’s <img alt="{2^{nd}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bnd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2^{nd}}"/> edition is on version 11.</li>
<li>In engineering a product: You get the idea.</li>
</ul>
<p>Here is a sample explaining vaccine <a href="https://www.rivm.nl/en/novel-coronavirus-covid-19/vaccine-against-covid-19">development</a>:</p>
<ul>
<li>Phase I: The vaccine is given to healthy volunteers to see whether it is safe and what dosage (quantity) is most effective.</li>
<li>Phase II: The vaccine is given to target groups that would be vaccinated to see what dosage (quantity) is most effective and how well the immune system responds to it.</li>
<li>Phase III: The vaccine is given to an even larger group, consisting of thousands of people, to see how well the vaccine works to prevent COVID-19. People who do receive the vaccine are then compared with people who did not receive the vaccine.Note: there is no step that modifies the vaccine.
</li></ul>
<h2>Consequences</h2>
<p>There are several consequences from this insight about vaccines. For one it makes sense to order millions of doses of a vaccine, even one that has not yet been proved to be safe and effective. For example,</p>
<blockquote>
<p><b> </b> <em> The European Commission has placed its first advance order for a coronavirus vaccine, snapping up 300 million doses of AstraZeneca’s AZD1222 candidate developed by the University of Oxford, with an option on another 100 million. </em></p>
</blockquote>
<p>Note we would never order a large number of copies of a book before all editing and typos were fixed. This is a “proof” that the vaccine is the same.</p>
<p>Actually it may make sense to even begin to take the vaccine. Especially for high risk people. In the past inventors of vaccines have often taken their own new vaccine, even before they were sure they worked.</p>
<h2>Open Problems</h2>
<p>I am a computer scientist with no experience in vaccines. In 1954 I did help test the Jonas Salk polio vaccine. My help was in the form supplying an arm that got a shot of the Salk polio vaccine, I was nine years old then. But I have a math view of vaccines—a viewpoint that sheds light on this misunderstanding.</p>

</div>
    </content>
    <updated>2020-10-22T15:50:23Z</updated>
    <published>2020-10-22T15:50:23Z</published>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-10-22T23:31:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=499</id>
    <link href="https://tcsplus.wordpress.com/2020/10/22/tcs-talk-wednesday-october-28-omar-montasser-ttic/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 28 — Omar Montasser, TTIC</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). Omar Montasser from TTIC will speak about “Adversarially Robust Learnability: Characterization and Reductions” (abstract below). You can reserve a spot as an individual or a group to join […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong>Omar Montasser</strong> from TTIC will speak about “<em>Adversarially Robust Learnability: Characterization and Reductions</em>” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: We study the question of learning an adversarially robust predictor from uncorrupted samples. We show that any VC class <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="H"/> is robustly PAC learnable, but we also show that such learning must sometimes be improper (i.e. use predictors from outside the class), as some VC classes are not robustly properly learnable.  In particular, the popular robust empirical risk minimization approach (also known as adversarial training), which is proper, cannot robustly learn all VC classes.  After establishing learnability, we turn to ask whether having a tractable non-robust learning algorithm is sufficient for tractable robust learnability and give a reduction algorithm for robustly learning any hypothesis class <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="H"/> using a non-robust PAC learner for <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="H"/>, with nearly-optimal oracle complexity.<br/>This is based on joint work with Steve Hanneke and Nati Srebro, available at <a href="https://arxiv.org/abs/1902.04217">https://arxiv.org/abs/1902.04217</a>.</p></div>
    </content>
    <updated>2020-10-22T14:48:14Z</updated>
    <published>2020-10-22T14:48:14Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-10-22T23:32:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/22/computer-science-tenured-tenure-track-at-nyu-shanghai-apply-by-february-1-2021/</id>
    <link href="https://cstheory-jobs.org/2020/10/22/computer-science-tenured-tenure-track-at-nyu-shanghai-apply-by-february-1-2021/" rel="alternate" type="text/html"/>
    <title>Computer Science, Tenured/Tenure-track at NYU Shanghai (apply by February 1, 2021)</title>
    <summary>NYU Shanghai is currently inviting applications for a Tenured or Tenure-track position in Computer Science Theory. The search is not restricted to any rank. We invite candidates with a strong research record in CS theory to apply, including research in algorithms, data structures, computational complexity, cryptography, learning theory, and so on. Website: https://apply.interfolio.com/80168 Email: shanghai.faculty.recruitment@nyu.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>NYU Shanghai is currently inviting applications for a Tenured or Tenure-track position in Computer Science Theory. The search is not restricted to any rank. We invite candidates with a strong research record in CS theory to apply, including research in algorithms, data structures, computational complexity, cryptography, learning theory, and so on.</p>
<p>Website: <a href="https://apply.interfolio.com/80168">https://apply.interfolio.com/80168</a><br/>
Email: shanghai.faculty.recruitment@nyu.edu</p></div>
    </content>
    <updated>2020-10-22T13:41:49Z</updated>
    <published>2020-10-22T13:41:49Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-22T23:31:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/156</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/156" rel="alternate" type="text/html"/>
    <title>TR20-156 |  Codes over integers, and the singularity of random matrices with large entries | 

	Sankeerth Rao Karingula, 

	Shachar Lovett</title>
    <summary>The prototypical construction of error correcting codes is based on linear codes over finite fields. In this work, we make first steps in the study of codes defined over integers. We focus on Maximum Distance Separable (MDS) codes, and show that MDS codes with linear rate and distance can be realized over the integers with a constant alphabet size. This is in contrast to the situation over finite fields, where a linear size finite field is needed.

The core of this paper is a new result on the singularity probability of random matrices. We show that for a random $n \times n$ matrix with entries chosen independently from the range $\{-m,\ldots,m\}$, the probability that it is singular is at most $m^{-cn}$ for some absolute constant $c&gt;0$.</summary>
    <updated>2020-10-22T05:36:16Z</updated>
    <published>2020-10-22T05:36:16Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-22T23:31:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11064</id>
    <link href="http://arxiv.org/abs/2010.11064" rel="alternate" type="text/html"/>
    <title>Smoothed Analysis of Pareto Curves in Multiobjective Optimization</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/R=ouml=glin:Heiko.html">Heiko Röglin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11064">PDF</a><br/><b>Abstract: </b>In a multiobjective optimization problem a solution is called Pareto-optimal
if no criterion can be improved without deteriorating at least one of the other
criteria. Computing the set of all Pareto-optimal solutions is a common task in
multiobjective optimization to filter out unreasonable trade-offs.
</p>
<p>For most problems the number of Pareto-optimal solutions increases only
moderately with the input size in applications. However, for virtually every
multiobjective optimization problem there exist worst-case instances with an
exponential number of Pareto-optimal solutions. In order to explain this
discrepancy, we analyze a large class of multiobjective optimization problems
in the model of smoothed analysis and prove a polynomial bound on the expected
number of Pareto-optimal solutions.
</p>
<p>We also present algorithms for computing the set of Pareto-optimal solutions
for different optimization problems and discuss related results on the smoothed
complexity of optimization problems.
</p></div>
    </summary>
    <updated>2020-10-22T23:25:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10997</id>
    <link href="http://arxiv.org/abs/2010.10997" rel="alternate" type="text/html"/>
    <title>Rigid continuation paths II. Structured polynomial systems</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=uuml=rgisser:Peter.html">Peter Bürgisser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cucker:Felipe.html">Felipe Cucker</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lairez:Pierre.html">Pierre Lairez</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10997">PDF</a><br/><b>Abstract: </b>We design a probabilistic algorithm that, given $\epsilon&gt;0$ and a polynomial
system $F$ given by black-box evaluation functions, outputs an approximate zero
of $F$, in the sense of Smale, with probability at least $1-\epsilon$. When
applying this algorithm to $u \cdot F$, where $u$ is uniformly random in the
product of unitary groups, the algorithm performs $\operatorname{poly}(n,
\delta) \cdot L(F) \cdot \left( \Gamma(F) \log \Gamma(F) + \log \log
\epsilon^{-1} \right)$ operations on average. Here $n$ is the number of
variables, $\delta$ the maximum degree, $L(F)$ denotes the evaluation cost of
$F$, and $\Gamma(F)$ reflects an aspect of the numerical condition of $F$.
Moreover, we prove that for inputs given by random Gaussian algebraic branching
programs of size $\operatorname{poly}(n,\delta)$, the algorithm runs on average
in time polynomial in $n$ and $\delta$. Our result may be interpreted as an
affirmative answer to a refined version of Smale's 17th question, concerned
with systems of \emph{structured} polynomial equations.
</p></div>
    </summary>
    <updated>2020-10-22T23:20:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10881</id>
    <link href="http://arxiv.org/abs/2010.10881" rel="alternate" type="text/html"/>
    <title>Multi-Dimensional Randomized Response</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Domingo=Ferrer:Josep.html">Josep Domingo-Ferrer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soria=Comas:Jordi.html">Jordi Soria-Comas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10881">PDF</a><br/><b>Abstract: </b>In our data world, a host of not necessarily trusted controllers gather data
on individual subjects. To preserve her privacy and, more generally, her
informational self-determination, the individual has to be empowered by giving
her agency on her own data. Maximum agency is afforded by local anonymization,
that allows each individual to anonymize her own data before handing them to
the data controller. Randomized response (RR) is a local anonymization approach
able to yield multi-dimensional full sets of anonymized microdata that are
valid for exploratory analysis and machine learning. This is so because an
unbiased estimate of the distribution of the true data of individuals can be
obtained from their pooled randomized data. Furthermore, RR offers rigorous
privacy guarantees. The main weakness of RR is the curse of dimensionality when
applied to several attributes: as the number of attributes grows, the accuracy
of the estimated true data distribution quickly degrades. We propose several
complementary approaches to mitigate the dimensionality problem. First, we
present two basic protocols, separate RR on each attribute and joint RR for all
attributes, and discuss their limitations. Then we introduce an algorithm to
form clusters of attributes so that attributes in different clusters can be
viewed as independent and joint RR can be performed within each cluster. After
that, we introduce an adjustment algorithm for the randomized data set that
repairs some of the accuracy loss due to assuming independence between
attributes when using RR separately on each attribute or due to assuming
independence between clusters in cluster-wise RR. We also present empirical
work to illustrate the proposed methods.
</p></div>
    </summary>
    <updated>2020-10-22T23:21:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10879</id>
    <link href="http://arxiv.org/abs/2010.10879" rel="alternate" type="text/html"/>
    <title>Tangent Quadrics in Real 3-Space</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brysiewicz:Taylor.html">Taylor Brysiewicz</a>, Claudia Fevola, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sturmfels:Bernd.html">Bernd Sturmfels</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10879">PDF</a><br/><b>Abstract: </b>We examine quadratic surfaces in 3-space that are tangent to nine given
figures. These figures can be points, lines, planes or quadrics. The numbers of
tangent quadrics were determined by Hermann Schubert in 1879. We study the
associated systems of polynomial equations, also in the space of complete
quadrics, and we solve them using certified numerical methods. Our aim is to
show that Schubert's problems are fully real.
</p></div>
    </summary>
    <updated>2020-10-22T23:26:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10809</id>
    <link href="http://arxiv.org/abs/2010.10809" rel="alternate" type="text/html"/>
    <title>A Note on the Approximability of Deepest-Descent Circuit Steps</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Borgwardt:Steffen.html">Steffen Borgwardt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brand:Cornelius.html">Cornelius Brand</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldmann:Andreas_Emil.html">Andreas Emil Feldmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kouteck=yacute=:Martin.html">Martin Koutecký</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10809">PDF</a><br/><b>Abstract: </b>Linear programs (LPs) can be solved through a polynomial number of so-called
deepest-descent circuit steps, each step following a circuit direction to a new
feasible solution of most-improved objective function value. A computation of
deepest-descent steps has recently been shown to be NP-hard [De Loera et al.,
arXiv, 2019]. This is a consequence of the hardness of what we call the optimal
circuit-neighbor problem (OCNP) for LPs with non-unique optima. However, the
non-uniqueness assumption is crucial to the hardness of OCNP, because we show
that OCNP for LPs with a unique optimum is solvable in polynomial time.
Moreover, in practical applications one is usually only interested in finding
some optimum of an LP, in which case a simple perturbation of the objective
yields an instance with a unique optimum. It is thus natural to ask whether
deepest-descent steps are also easy to compute for LPs with unique optima, or
whether this problem is hard despite OCNP being easy.
</p>
<p>We show that deepest-descent steps can be efficiently approximated within a
factor of $n$, where $n$ is the dimension of the polyhedron at hand, but not
within a factor of $O(n^{1-\epsilon})$ for any $\epsilon &gt; 0$. While we prove
that OCNP can be solved efficiently for LPs with a unique optimum, our
different hardness approach allows us to show strong negative results:
computing deepest-descent steps is NP-hard and inapproximable even for 0/1
linear programs with a unique optimum which are defined by a totally unimodular
constraint matrix.
</p></div>
    </summary>
    <updated>2020-10-22T23:26:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10726</id>
    <link href="http://arxiv.org/abs/2010.10726" rel="alternate" type="text/html"/>
    <title>MINVO Basis: Finding Simplexes with Minimum Volume Enclosing Polynomial Curves</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tordesillas:Jesus.html">Jesus Tordesillas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/How:Jonathan_P=.html">Jonathan P. How</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10726">PDF</a><br/><b>Abstract: </b>Outer polyhedral representations of a given polynomial curve are extensively
exploited in computer graphics rendering, computer gaming, path planning for
robots, and finite element simulations. B\'ezier curves (which use the
Bernstein basis) or B-Splines are a very common choice for these polyhedral
representations because their non-negativity and partition-of-unity properties
guarantee that each interval of the curve is contained inside the convex hull
of its control points. However, the convex hull provided by these bases is not
the one with smallest volume, producing therefore undesirable levels of
conservatism in all of the applications mentioned above. This paper presents
the MINVO basis, a polynomial basis that generates the smallest $n$-simplex
that encloses any given $n^\text{th}$-order polynomial curve. The results
obtained for $n=3$ show that, for any given $3^{\text{rd}}$-order polynomial
curve, the MINVO basis is able to obtain an enclosing simplex whose volume is
$2.36$ and $254.9$ times smaller than the ones obtained by the Bernstein and
B-Spline bases, respectively. When $n=7$, these ratios increase to $902.7$ and
$2.997\cdot10^{21}$, respectively.
</p></div>
    </summary>
    <updated>2020-10-22T23:27:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2020/10/21/intrinsicLR/</id>
    <link href="http://offconvex.github.io/2020/10/21/intrinsicLR/" rel="alternate" type="text/html"/>
    <title>Mismatches between Traditional Optimization Analyses and Modern Deep Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>You may remember our <a href="http://www.offconvex.org/2020/04/24/ExpLR1/">previous blog post</a> showing that it is possible to do state-of-the-art deep learning with learning rate that increases exponentially during training.  It was meant to be a dramatic illustration that what we learned in optimization classes and books isn’t always a good fit for modern deep learning, specifically, <em>normalized nets</em>, which is our term for nets that use any one of popular normalization schemes,e.g. <a href="https://arxiv.org/abs/1502.03167">BatchNorm (BN)</a>, <a href="https://arxiv.org/abs/1803.08494">GroupNorm (GN)</a>, <a href="https://arxiv.org/abs/1602.07868">WeightNorm (WN)</a>. Today’s post (based upon <a href="https://arxiv.org/abs/2010.02916">our paper</a> with Kaifeng Lyu at NeurIPS20)  identifies other surprising incompatibilities between normalized nets and traditional analyses. We hope this will change the way you teach and think about deep learning!</p>

<p>Before diving into the results, we recall that normalized nets  are typically trained with weight decay (aka $\ell_2$ regularization). Thus the $t$th iteration of Stochastic Gradient Descent (SGD) is:</p>

\[w_{t+1} \gets (1-\eta_t\lambda)w_t - \eta_t \nabla \mathcal{L}(w_t; \mathcal{B}_t),\]

<p>where $\lambda$ is the weight decay (WD) factor (or $\ell_2$-regularization coefficient),  $\eta_t$ the learning rate, $\mathcal{B}_t$ the batch, and $\nabla \mathcal{L}(w_t,\mathcal{B}_t)$ the batch gradient.</p>

<p>As sketched in our previous blog post, under fairly mild assumptions (namely, fixing the top layer during random initialization —which empirically does not hurt final accuracy) the loss function for training such normalized nets is <em>scale invariant</em>, which means $\mathcal{L}(w _ t; \mathcal{B}_ t)=\mathcal{L}(cw _ t; \mathcal{B} _ t)$, $\forall c&gt;0$.</p>

<p>A consequence of scale invariance is that the $ \nabla _ w \mathcal{L} \vert _ {w = w _ 0} = c \nabla _ w \mathcal{L}\vert _  {w = cw _ 0}$ and $\nabla ^ 2 _ w \mathcal{L} \vert _ {w = w _ 0} = c ^ 2 \nabla ^ 2 _ w \mathcal{L} \vert _  {w = cw _ 0}$, for any $c&gt;0$.</p>

<h2 id="some-conventional-wisdoms-cws">Some Conventional Wisdoms (CWs)</h2>

<p>Now we briefly describe some conventional wisdoms. Needless to say, by the end of this post these will turn out to be very very suspect! Possibly they were OK in earlier days of deep learning, and with shallower nets.</p>

<blockquote>
  <p>CW 1) As we reduce LR to zero, optimization dynamic converges to a deterministic path (Gradient Flow) along which training loss strictly decreases.</p>
</blockquote>

<p>Recall that in traditional explanation of (deterministic) gradient descent, if LR is smaller than roughly the inverse of the smoothness of the loss function, then each step reduces the loss. SGD, being stochastic, has a distribution over possible paths. But very tiny LR can be thought of as full-batch Gradient Descent (GD), which in the limit of infinitesimal step size approaches Gradient Flow (GF).</p>

<p>The above reasoning shows very small LR is guaranteed to decrease the loss at least, as well as any higher LR, can. Of course, in deep learning, we care not only about optimization but also generalization. Here small LR is believed to hurt.</p>

<blockquote>
  <p>CW 2) To achieve the best generalization the LR must be large initially for quite a few epochs.</p>
</blockquote>

<p>This is primarily an empirical finding: using too-small learning rates or too-large batch sizes from the start (all other hyper-parameters being fixed) is known to lead to worse generalization (<a href="https://arxiv.org/pdf/1206.5533.pdf">Bengio, 2012</a>; <a href="https://arxiv.org/abs/1609.04836">Keskar et al., 2017</a>).</p>

<p>A popular explanation for this phenomenon is  that the noise in gradient estimation during SGD is beneficial for generalization. (As noted, this noise tends to average out when LR is very small.)  Many authors have suggested that the noise helps becauses it keeps the trajectory away from sharp minima which are believed to generalize worse, although there is some difference of opinion here (<a href="http://www.bioinf.jku.at/publications/older/3304.pdf">Hochreiter&amp;Schmidhuber, 1997</a>; <a href="https://arxiv.org/abs/1609.04836">Keskar et al., 2017</a>; <a href="https://arxiv.org/abs/1712.09913">Li et al., 2018</a>; <a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>; <a href="https://arxiv.org/pdf/1902.00744.pdf">He et al., 2019</a>). <a href="https://arxiv.org/abs/1907.04595">Li et al., 2019</a> also gave an example (a simple two-layer net) where this observation of worse generalization due to small LR is mathematically proved and also experimentally verified.</p>

<blockquote>
  <p>CW 3) Modeling SGD via a Stochastic Differential Equation (SDE) in the continuous-time limit with a fixed Gaussian noise. Namely, think of SGD as a diffusion process that <strong>mixes</strong>  to some Gibbs-like distribution on trained nets.</p>
</blockquote>

<p>This is the usual approach to  formal understanding of CW 2 (<a href="https://arxiv.org/abs/1710.06451">Smith&amp;Le, 2018</a>; <a href="https://arxiv.org/abs/1710.11029">Chaudhari&amp;Soatto, 2018</a>; <a href="https://arxiv.org/abs/2004.06977">Shi et al., 2020</a>). The idea is that SGD is gradient descent with a noise term, which has a continuous-time approximation as a diffusion process described as</p>

\[dW_t = - \eta_t \lambda W_t dt - \eta_t \nabla \mathcal{L}(W_t) dt + \eta_t \Sigma_{W_t}^{1/2} dB_t,\]

<p>where �$\sigma_{W_t}$ is the covariance of stochastic gradient $ \nabla \mathcal{L}(w_t; \mathcal{B}_t)$,  and $B_t$ denotes Brownian motion of the appropriate dimension. Several works have adopted this SDE view and given some rigorous analysis of the effect of noise.</p>

<p>In this story, SGD turns into a geometric random walk in the landscape, which can in principle explore the landscape more thoroughly, for instance by occasionally making loss-increasing steps. While an appealing view, rigorous analysis is difficult because we lack a mathematical description of the loss landscape.  Various papers assume the noise in SDE is isotropic Gaussian, and then derive an expression for the stationary distribution of the random walk in terms of the familiar Gibbs distribution. This view gives intuitively appealing explanation of some deep learning phenomena since the magnitude of noise (which is related to LR and batch size) controls the convergence speed and other properties. For instance it’s well-known that this SDE approximation implies the well-known <em>linear scaling rule</em> (Goyal et. al., 2017](https://arxiv.org/pdf/1706.02677.pdf)).</p>

<p>Which raises the question: <em>does SGD really behave like a diffusion process that mixes in the loss landscape?</em></p>

<!--[A few lines explaining for why noise term has this form? e.g., show one step discretization]!-->

<h2 id="conventional-wisdom-challenged">Conventional Wisdom challenged</h2>

<p>We now describe the actual discoveries for normalized nets, which show that the above CW’s are quite off.</p>

<blockquote>
  <p>(Against CW1): Full batch gradient descent $\neq$ gradient flow.</p>
</blockquote>

<p>It’s well known that if LR is smaller than the inverse of the smoothness, then the trajectory of gradient descent will be close to that of gradient flow. But for normalized networks, the loss function is scale-invariant and thus provably non-smooth (i.e., smoothness becomes unbounded)  around the origin (<a href="https://arxiv.org/abs/1910.07454">Li&amp;Arora, 2019</a>). We show that this non-smoothness issue is very real and makes training unstable and even chaotic for full batch SGD with any nonzero learning rate. This occurs both empirically and provably so with some toy losses.</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/gd_not_gf.png" style="width: 60%;"/>
</div>

<p><strong>Figure 1.</strong> WD makes GD on scale-invariant loss unstable and chaotic.
(a) Toy model with scale-invariant loss $L(x,y) = \frac{x^2}{x^2+y^2}$  (b)(c) Convergence never truly happens for  ResNet trained on sub-sampled
CIFAR10 containing 1000 images with full-batch GD (without momentum).  ResNet
can easily get to 100% training accuracy but then veers off.  When WD is turned off at epoch 30000 it converges.</p>

<p>Note that WD plays a crucial role in this effect since without WD the parameter norm increases monotonically 
 (<a href="https://arxiv.org/abs/1812.03981">Arora et al., 2018</a>) which implies SGD moves away from the origin at all times.</p>

<p>Savvy readers might wonder whether using a smaller LR could fix this issue. Unfortunately, getting close to the origin is unavoidable because once the gradient gets small,  WD will dominate the dynamics and decrease the norm at a geometric rate, causing the gradient to rise again due to the scale invariance! (This happens so long as the gradient gets arbitrarily small, but not actually zero, as is the case in practice.)</p>

<p>In fact, this is an excellent (and rare) place where early stopping is necessary even for correct optimization of the loss.</p>

<blockquote>
  <p>(Against CW 2) Small LR can generalize equally well as large LR.</p>
</blockquote>

<p>This actually was a prediction of the new theoretical analysis we came up with. We ran extensive experiments to test this prediction and found that initial large LR is <strong>not necessary</strong> to match the best performance, even when <em>all the other hyperparameters are fixed</em>. See Figure 2.</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/blog_sgd_8000_test_acc.png" style="width: 300px;"/>
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/blog_sgd_8000_train_acc.png" style="width: 300px;"/>
</div>

<p><strong>Figure 2</strong>. ResNet trained on CIFAR10 with SGD with normal LR schedule (baseline) as well as a schedule with 100 times smaller initial LR.  The latter matches performance of baseline after one more LR decay!  Note it needs  5000 epochs which is 10x higher! See our paper for details. (Batch size is 128, WD is 0.0005, and LR is divided by 10 for each decay.)</p>

<p>Note the  surprise here is that generalization was not hurt from drastically smaller LR even  <em>when no other hyperparameter changes</em>.  It is known empirically as well as rigorously (Lemma 2.4 in <a href="https://arxiv.org/abs/1910.07454">Li&amp;Arora, 2019</a>)  that it is possible to compensate for small LR by other hyperparameter changes.</p>

<blockquote>
  <p>(Against Wisdom 3) Random walk/SDE view of SGD is way off. There is no evidence of mixing as  traditionally understood, at least within normal training times.</p>
</blockquote>

<p>Actually the evidence against global mixing exists already via the phenomenon of Stochastic Weight Averaging (SWA) (<a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>). Along the trajectory of SGD, if  the network parameters from two different epochs are averaged, then the average has test loss lower than either.  Improvement via averaging continues to  work for run times 10X longer  than usual as shown in Figure 3. However, the accuracy improvement doesn’t happen for SWA between two solutions obtained from different initialization.  Thus checking whether SWA holds distinguishes between  pairs of solutions drawn from the same trajectory and pairs drawn from different trajectories, which  shows the diffusion process hasn’t mixed to stationary distribution within normal training times. (This is not surprising, since the theoretical analysis of mixing does not suggest it happens rapidly at all.)</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/swa_sgd_test_acc.png" style="width: 300px;"/>
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/swa_sgd_dist.png" style="width: 300px;"/>
</div>

<p><strong>Figure 3</strong>. Stochastic Weight Averaging improves the test accuracy of ResNet trained with
SGD on CIFAR10. <strong>Left:</strong> Test accuracy. <strong>Right:</strong> Pairwise distance between parameters from different epochs.</p>

<p>Actually <a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a> already noticed the implication that SWA rules out that SGD is a diffusion process which mixes to a unique global equilibrium. They suggested instead that perhaps the trajectory of SGD could be well-approximated by a multivariate Ornstein-Uhlenbeck (OU) process around the <em>local minimizer</em> $W^ * $, assuming the loss surface is locally strongly convex. As the corresponding stationary is multi-dimensional Gaussian, $N(W^ *, \Sigma)$, around the local minimizer, $W^ *$, this explains why SWA helps to reduce the training loss.</p>

<p>However, we note that (<a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>)’s suggestion is also refuted by the fact that we can show $\ell_2$ distance between weights from epochs $T$ and $T+\Delta$ monotonically increases with $\Delta$ for every $T$ (See Figure 3), while $ \mathbf{E} [ | W_ T-W_ {T+\Delta} |^2]$ should converge to the constant $2Tr[\Sigma]$ as $T, \Delta \to +\infty$ in the OU process. This suggests that all these weights are correlated, unlike the hypothesized OU process.</p>

<h2 id="so-whats-really-going-on">So what’s really going on?</h2>

<p>We develop a new theory (some parts rigorously proved and others supported by experiments) suggesting that <strong>LR doesn’t play the role assumed in most discussions.</strong></p>

<p>It’s widely believed that LR $\eta$ controls the convergence rate of SGD and affects the generalization via changing the magnitude of noise because LR $\eta$ adjusts the magnitude of gradient update per step. 
 <!--It's also worth noting that for vanilla SGD, changing LR is equivalent to rescaling the loss function. -->
 However, for normalized networks trained with SGD + WD, the effect of LR is more subtle as now it has two roles: (1). the multiplier before the gradient of the loss. (2). the multiplier before WD. Intuitively, one imagines the WD part is  useless since the loss function is scale-invariant, and thus the first role must be more important. But surprisingly, this intuition is completely wrong and it turns out that the second role is way more important than the first one. 
Further analysis shows that a better measure of speed of learning is   $\eta \lambda$, which we call the <em>intrinsic learning rate</em> or <em>intrinsic LR</em>, denoted $\lambda_e$.</p>

<p>While previous papers have noticed qualitatively that LR and WD have a close interaction, our ExpLR paper   <a href="https://arxiv.org/abs/1910.07454">Li&amp;Arora, 2019</a>)  gave mathematical proof that <em>if WD* LR, i.e., $\lambda\eta$ is fixed, then the effect of changing LR on the dynamics is equivalent to rescaling the initial parameters</em>.  As far as we can tell, performance of SGD on modern architectures is quite robust to (indeed usually independent of) scale of the initialization, so the effect of changing initial LR while keeping intrinsic LR fixed is also negligible.</p>

<p>Our paper gives insight into the role of intrinsic LR $\lambda_e$ by giving a new SDE-style analysis of SGD for normalized nets, leading to the following conclusion (which rests in part on experiments):</p>

<blockquote>
  <p>In normalized nets SGD does indeed lead to rapid mixing, but in <strong>function space</strong> (i.e., input-output behavior of the net). Mixing happens after $O(1/\lambda_e)$ iterations, in contrast to the exponentially slow mixing guaranteed in the parameter space by traditional analysis of diffusion walks.</p>
</blockquote>

<p>To explain the meaning of mixing in function space, let’s view SGD (carried out for a fixed number of steps) as a way to sample a trained net from a  distribution over trained nets. Thus the end result of SGD from a fixed initialization can be viewed as a probabilistic classifier whose output on any datapoint is the $K$-dimenstional vector whose $i$th coordinate is the probability of outputting label $i$. (Here $K$ is the total number of labels.) Now if two different initializations both cause SGD to produce classifiers with error $5$ percent on heldout datapoints, then  <em>a priori</em> one would imagine that  on a given held-out datapoint the classifier from the first distribution <strong>disagrees</strong>  with the classifier from the second distribution with roughly $2 * 5 =10$ percent probability. (More precisely, $2 * 5 * (1-0.05) = 9.5$ percent.) However, convergence to an equilibrium distribution in function space means that the probability of disagreement is almost $0$, i.e., the distribution is almost the same regardless of the initialization! This is indeed what we experimentally find, to our big surprise. Our theory is built around this new phenomenon.</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/conjecture.png" style="width: 500px;"/>
</div>
<p><strong>Figure 4</strong>: A simple 4-layer normalized CNN trained on MNIST with three schedules converge to the same equilibrium after intrinsic LRs become equal at epoch 81. We use Monte Carlo ($500$ trials) to estimate $\ell_1$ distances between distributions.</p>

<p>In the next post, we will explain our new theory and the partial new analysis of SDEs arising from SGD in normalized nets.</p></div>
    </summary>
    <updated>2020-10-21T22:00:00Z</updated>
    <published>2020-10-21T22:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2020-10-22T23:32:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/21/tenure-track-faculty-at-portland-state-university-apply-by-december-31-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/21/tenure-track-faculty-at-portland-state-university-apply-by-december-31-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track faculty at Portland State University (apply by December 31, 2020)</title>
    <summary>The Department of Computer Science at Portland State University invites applications for several Assistant Professor positions. Exceptional candidates will also be considered for appointment at the rank of Associate Professor. Candidates in all areas of Computer Science will be considered including theoretical computer science. Website: https://www.pdx.edu/computer-science/open-faculty-position Email: cssearch@pdx.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at Portland State University invites applications for several Assistant Professor positions. Exceptional candidates will also be considered for appointment at the rank of<br/>
Associate Professor. Candidates in all areas of Computer Science will be considered including theoretical computer science.</p>
<p>Website: <a href="https://www.pdx.edu/computer-science/open-faculty-position">https://www.pdx.edu/computer-science/open-faculty-position</a><br/>
Email: cssearch@pdx.edu</p></div>
    </content>
    <updated>2020-10-21T16:42:54Z</updated>
    <published>2020-10-21T16:42:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-22T23:31:27Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/reconstruction-theory/</id>
    <link href="https://differentialprivacy.org/reconstruction-theory/" rel="alternate" type="text/html"/>
    <title>The Theory of Reconstruction Attacks</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We often see people asking whether or not differential privacy might be overkill.  Why do we need strong privacy protections like differential privacy when we’re only releasing approximate, aggregate statistical information about a dataset?  Is it really possible to extract information about specific users from releasing these statistics?  The answer turns out to be a resounding yes!  The textbook by Dwork and Roth <a href="https://www.cis.upenn.edu/~aaroth/privacybook.html">[DR14]</a> calls this phenomenon the Fundamental Law of Information Recovery:</p>

<blockquote>
  <p>Giving overly accurate answers to too many questions will inevitably destroy privacy.</p>
</blockquote>

<p>So what exactly does this fundamental law mean precisely, and how can we prove it?  We can formalize and prove the law via <em>reconstruction attacks</em>, where an attacker can recover secret information from nearly every user in the dataset, simply by observing noisy answers to a modestly large number of (surprisingly simple) queries on the dataset. Reconstruction attacks were introduced in a seminal paper by Dinur and Nissim in 2003 <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>.  Although this paper predates differential privacy by a few years, the discovery of reconstruction attacks directly led to the definition of differential privacy, and shaped a lot of the early research on the topic. We now know that differentially private algorithms can, in some cases, match the limitations on accuracy implied by reconstruction attacks. When this is the case, we have a remarkably sharp transition from a blatant privacy violation when the accuracy is high enough to enable a reconstruction attack, to the strong protection given by differential privacy at the cost of only slightly lower accuracy.</p>

<p>Aside from the theoretical importance of reconstruction attacks, one may wonder if they can be carried out in practice, or if the attack model is unrealistic and can be avoided with some simple workarounds?  In this series of posts, we argue that reconstruction attacks can be quite practical.  In particular, we describe successful attacks by some of this post’s authors on a family of systems called <em>Diffix</em>, that attempt to prevent reconstruction without introducing as much noise as the reconstruction attacks suggest is necessary. To the best of our knowledge, these attacks represent the first successful attempt to reconstruct data from a commercial statistical-database system that is specifically designed to protect the privacy of the underlying data.  A larger and much more significant demonstration of the practical power of reconstruction attacks was carried out by the US Census Bureau in 2018, motivating the Bureau’s adoption of differential privacy for data products derived from the 2020 decennial census <a href="https://queue.acm.org/detail.cfm?ref=rss&amp;id=3295691">[GAM18]</a>.</p>

<p>This series will come in two parts: In this post, we will review the theory of reconstruction attacks, and present a model for reconstruction attacks that corresponds more directly to real attacks than the one that is typically presented.  In the second post, we will describe attacks that were launched against various iterations of the <em>Diffix</em> system. \(
\newcommand{\uni}{\mathcal{X}} % The universe
\newcommand{\usize}{T} % Universe size
\newcommand{\elem}{x} % Generic universe element. 
\newcommand{\pbs}{z} %Non-secret bits
\newcommand{\pbsuni}{\mathcal{Z}}
\renewcommand{\sb}{b} % Secret bit
\newcommand{\pds}{Z} %non-secret part of the data set
\newcommand{\ddim}{d} % Data dimension
\newcommand{\queries}{Q} % A set/workload of queries
\newcommand{\qmat}{\mat{Q}} % Query matrix
\newcommand{\qent}{w} % Entry of the query matrix
\newcommand{\hist}{h} % Histogram vector
\newcommand{\mech}{\mathcal{M}} % Generic Mechanism
\newcommand{\query}{q}
\newcommand{\queryfunc}{\varphi}
\newcommand{\ans}{a} % query answer
\newcommand{\qsize}{k}
\newcommand{\ds}{X}
\newcommand{\dsrow}{\elem} % same as elem above
\newcommand{\dsize}{n}
\newcommand{\priv}{\eps}
\newcommand{\privd}{\delta}
\newcommand{\acc}{\alpha}
\newcommand{\from}{:}
\newcommand{\set}[1]{\left{#1\right}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\eps}{\varepsilon}
\newcommand{\pmass}{\mathbbm{1}}
\newcommand{\zo}{\{0,1\}}
\newcommand{\mat}[1]{#1} % matrix notation: for now nothing
\)</p>
<h3 id="a-model-of-reconstruction-attacks">A Model of Reconstruction Attacks</h3>

<p>This part presents the basic theory of reconstruction attacks.  We’ll introduce a model of reconstruction attacks that is a little different from what you would see if you read the papers, and then describe the main results of Dinur and Nissim.  At the end we will briefly mention some variations that have been considered in the nearly two decades since.</p>

<p>Let us fix a dataset model, so that we can describe the attack precisely. (These attacks are very flexible and the ideas can usually be adapted to new models, as we’ll see at the end of this part.) We take the dataset to be a collection of \(\dsize\) records \(\ds = \{\elem_1,\dots,\elem_n\}\), each corresponding to the data of a single person.  The attacker’s goal is to learn some piece of secret information about as many individuals as possible, so we think of each record as having the form \(\elem_i = (\pbs_i,\sb_i)\) where \(\pbs_i\) is some identifying information, and \(\sb_i \in \zo\) is some secret. We assume that the secret is binary, although this aspect of the model can be generalized. We can visualize such a dataset as a matrix \([\pds \mid \sb]\) with two blocks as follows:
\[ \left[ \begin{array}{c|c} \pbs_1 &amp; \sb_1 \\ \vdots &amp; \vdots \\ \pbs_n &amp; \sb_n \end{array} \right] \]
For a concrete example, suppose each element in the dataset contains \(d\) binary attributes, and the attacker’s goal is to learn the last attribute of each user.  In this case we would write each element as a pair \((\pbs_i, \sb_i)\) where \(\pbs_i \in \zo^{d-1}\) and \(\sb_i \in \zo\).</p>

<p>Note that this distinction between \(\pbs_i\) and \(\sb_i\) is only in the mind of the attacker, who has some prior information about the users, but is trying to learn some specific secret information.  In order to make the attack simpler to describe, we will also assume that the attacker knows \(\pbs_1,\dots,\pbs_\dsize\), which is everything about the dataset except the secret bits, although this assumption can also be relaxed to a large extent. As a shorthand, we will refer to \(\pbs_1, \ldots, \pbs_\dsize\) as the prior information, and to \(\sb_1, \ldots,\sb_\dsize\) as the secret bits.</p>

<p>Our goal is to understand whether asking aggregate queries defined by the prior information can allow an attacker to learn non-trivial information about the secret bits.  Perhaps the most basic type of aggregate query we can ask is a <em>counting query</em>, which is a query that asks what number of the data points satisfy a given property. The Dinur-Nissim attacks assume that the attacker can get approximate answers to a type of counting queries that ask how many data points satisfy some property defined in terms of the prior information, and also have the sensitive bit set to \(1\).  Let us use the notation \(\pbsuni\) for the set of all possible values that the prior information can take. For the purposes of the attack, each query \(\query\) will be specified by a function \(\queryfunc \from \pbsuni \to \zo\) and have the specific form
\[
\query(\ds) = \sum_{j=1}^{\dsize} \queryfunc(\pbs_j) \cdot \sb_j.
\]
This is a good time to make one absolutely crucial point about this model, which is that</p>
<blockquote>
  <p>all the users are treated completely symmetrically by the queries, and the attacker cannot issue a query that targets a specific user \(x_i\) by name or a specific subset of users.  The different users are distinguished only by their data.  Nonetheless, we will see how to learn information about specific users from the answers to these queries.</p>
</blockquote>

<p>Returning to our example with binary attributes, consider the very natural set of queries that asks for the inner product of the secret bits with each attribute in the prior information, which is a measure of the correlation between these two attributes.  Then each query takes the form \(\query_i(\ds) = \sum_{j=1}^{n} \pbs_{j,i} \cdot \sb_{j}\).</p>

<p>The nice thing about this type of query is that we can express the answers to a set of queries \({\query_1,\dots,\query_\qsize}\) defined by \(\queryfunc_1, \ldots, \queryfunc_\qsize\) as the following matrix-vector product \(\qmat_{\pds}\cdot \mat{b}\):
\[ \left[ \begin{array}{c}\query_1(\ds) \\ \vdots  \\ \query_\qsize(\ds) \end{array} \right] = \left[ \begin{array}{ccc} \queryfunc_1(\pbs_1) &amp; \dots &amp; \queryfunc_1(\pbs_\dsize) \\ \vdots &amp;  \ddots  &amp; \vdots \\ \queryfunc_\qsize(\pbs_1) &amp;   \dots &amp; \queryfunc_k(\pbs_\dsize)  \end{array} \right] \left[ \begin{array}{c} \sb_1 \\ \vdots  \\ \sb_n  \end{array} \right]
\]
so we can study this model using tools from linear algebra.</p>

<h3 id="an-inefficient-attack">An Inefficient Attack</h3>

<p>Exact answers to such queries are clearly revealing, because, the attacker can use the predicates \[ \queryfunc_i(z) = \begin{cases} 1 &amp; \textrm{if } \pbs = \pbs_i  \\ 0 &amp;  \textrm{otherwise} \end{cases} \] to single out a specific user and receive their bit \(\sb_i\).  It is less obvious, however, that an attacker can learn a lot about the private bits even given noisy answers to the queries.</p>

<p>The first Dinur-Nissim attack shows that this is indeed possible—if the attacker can ask an unbounded number of counting queries, and each query is answered with, for example, 5% error, then the attacker can reconstruct 80% of the secret bits.  This attack requires exponentially many queries to run, making it somewhat impractical, but it is a proof of concept that an attack can reconstruct a large amount of private information even from very noisy statistics. Later we will see how to scale down the attack to use fewer queries at the cost of requiring more accurate answers.</p>

<p>The attack itself is quite simple:</p>

<ul>
  <li>
    <p>For simplicity, assume all the \(\pbs_1, \ldots, \pbs_\dsize\) are distinct so that each user is uniquely identified by the prior information.</p>
  </li>
  <li>
    <p>The attacker chooses the queries \(\query_1, \ldots, \query_\qsize\) so that the matrix \(\qmat_\pds\) has as its rows all of \(\zo^\dsize\). Namely, \(\qsize=2^\dsize\) and the functions \(\queryfunc_1, \ldots, \queryfunc_\qsize\) defining the queries take all possible values on \(\pbs_1, \ldots, \pbs_\dsize\).</p>
  </li>
  <li>
    <p>The attacker receives a vector \(\ans\) of noisy answers to the queries, where \( |\query_{i}(\ds) - \ans_{i}| &lt; \acc \dsize \) for each query \( \query_i \).  In matrix notation, this means \[ \max_{i = 1}^\qsize  |(\qmat_\pds\cdot {\sb})_i -\ans_i|= \| \qmat_\pds \cdot \sb -\ans\|_\infty  \leq \alpha \dsize. \]
  Note that, for \(\{0,1\}\)-valued queries, the answers range from \(0\) to \(\dsize\), so answers with additive error \(\pm 5\%\) corresponds to \(\acc = 0.05\).</p>
  </li>
  <li>
    <p>Finally, the attacker outputs any guess \(\hat{\sb} = (\hat{\sb}_{1}, \ldots, \hat{\sb}_{n})\) of the private bits vector that is consistent with the answers and the additive error bound \(\acc\). In other words, \(\hat{\sb}\) just needs to satisfy \[\max_{i = 1}^\qsize |\ans_i - (\qmat_\pds\cdot \hat{\sb})_i|= \| \qmat_\pds \cdot \hat\sb - a \|_{\infty} \leq \alpha \dsize \]
  Note that a solution always exists, since the true private bits \(\sb\) will do.</p>
  </li>
</ul>

<p>Our claim is that any such guess \(\hat{b}\) in fact agrees with the true private bits \(b\) for all but \(4\acc \dsize\) of the users. The reason is that if \(\hat{\sb}\) disagreed with more than \(4\acc \dsize\) of the secret bits, then the answer to some query would have eliminated \(\hat{\sb}\) from contention.  To see this, fix some \(\hat{\sb}\in \zo^\dsize\), and let \[ S_{01} = \{j: \hat{\sb}_j = 0,  \sb_j = 1\} \textrm{ and } S_{10} = \{j: \hat{\sb}_j = 1,  \sb_j = 0\}\] 
If \(\hat{\sb}\) and \(\sb\) disagree on more than \(4\acc \dsize\) bits, then at least one of these two sets has size larger than \(2\acc \dsize\). Let us assume that this set is \(S_{01}\), and we’ll deal with the other case by symmetry.  Suppose that the \(i\)-th row of \(\qmat_\pds\) is the indicator vector of \(S_{01}\), i.e., \[(\qmat_\pds)_{i,j} = 1 \iff j \in S_{01}.\] We then have
\[
|(\qmat_{\pds}\cdot {\sb})_i - (\qmat_{\pds}\cdot \hat{\sb})_i|= |S_{01}| &gt; 2 \acc \dsize,
\]
but, at the same time, if \(\hat{\sb}\) were output by the attacker, we would have
\[
|(\qmat_{\pds}\cdot {\sb})_i - (\qmat_{\pds}\cdot \hat{\sb})_i| \le |\ans_i - (\qmat_\pds\cdot \hat{\sb})_i| + |(\qmat_\pds \cdot \sb)_i - \ans_{i}| \le 2\acc \dsize, \]
which is a contradiction. An important point to note is that the attacker does not need to know the set \(S_{10}\), or the corresponding \(i\)-th row of \(\qmat_\pds\) and query \(\query_i\). Since the attacker asks all possible queries determined by the prior information, we can be sure \(\query_i\) is one of these queries, and an accurate answer to it rules out this particular bad choice of \(\hat{\sb}\).  To give you something concrete to cherish, we can summarize this discussion in the following theorem.</p>

<blockquote>
  <p><strong>Theorem <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>:</strong> There is a reconstruction attack that issues \(2^n\) queries to a dataset of \(n\) users, obtains answers with error \(\alpha n\), and reconstructs the secret bits of all but \(4 \alpha n\) users.</p>
</blockquote>

<h3 id="an-efficient-attack">An Efficient Attack</h3>

<p>The exponential Dinur-Nissim attack is quite powerful, as it recovers 80% of the secret bits even from answers with 5% error, but it has the drawback that it requires asking \(2^\dsize\) queries to a dataset with \(\dsize\) users.  Note that this is inherent to some extent.  Suppose we randomly subsample 50% of the dataset and answer the queries using only this subset by rescaling appropriately.  Although this random subsampling does not guarantee any meaningful privacy, clearly no attacker can reconstruct 75% of the secret bits, since some of them are effectively deleted.  However, the guarantees of random sampling tell us that any set of \(\qsize\) queries will be answered with maximum error \( \acc n =  O(\sqrt{n \log \qsize})\), so we can answer \( 2^{\Omega(n)} \) queries with \(5\%\) error while provably preventing this sort of reconstruction.</p>

<p>However, Dinur and Nissim showed that if we obtain <em>highly accurate</em> answers—still noisy, but with error smaller than the sampling error—then we can reconstruct the dataset to high accuracy.  We can also make the reconstruction process computationally efficient by using linear programming to replace the exhaustive search over all \(2^\dsize\) possible vectors of secrets.  Specifically, we change the attack as follows:</p>

<ul>
  <li>
    <p>The attacker now chooses \(\qsize\) <em>randomly chosen</em> functions \( \varphi_i \from \pbsuni \to \{0,1\} \) for a much smaller \(\qsize = O(\dsize) \).</p>
  </li>
  <li>
    <p>Upon receiving an answer vector \(\ans\), the attacker now searches for a <em>real-valued</em> \( \tilde{b} \in [0,1]^{\dsize} \) such that \( \| \ans - \qmat_\pds \cdot \tilde{b} \|_{\infty} \leq \acc n \).  Note that this vector can be found efficiently via linear programming.  The attacker then rounds each \( \tilde{b}_{i} \) to the nearest \( \hat{b}_{i} \in \{0,1\}\).</p>
  </li>
</ul>

<p>It’s now much trickier to analyze this attack and show that it achieves low reconstruction error, and we won’t go into details in this post.  However, the key idea is that, because the queries are chosen randomly, \( \qmat_\pds \) is a random matrix with entries in \( \{0,1\} \), and we can use the statistical properties of this random matrix to argue that, with high probability,
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty^2 \gtrsim |{i: \sb_i \neq \hat{\sb}_i}|.
\]
By the way we chose \(\tilde{\sb}\), we have 
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty \le \|\qmat_\pds \cdot \sb - \ans\|_\infty + \| \ans - \qmat_\pds \cdot \tilde{b} \|_{\infty} \leq 2\acc n,
\]
so, by combining the inequalities we get that the reconstruction error is about \( O(\alpha^2 n^2) \). Note that, in order to reconstruct 80% of the secret bits using this attack, we now need the error to be \( \alpha n  \ll \sqrt{n} \), but as long as this condition on the error is satisfied, we will have a highly accurate reconstruction.  Let’s add this theorem to your goodie bags:</p>

<blockquote>
  <p><strong>Theorem <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>:</strong> There is an efficient reconstruction attack that issues \(O(n)\) random queries to a dataset of \(n\) users, obtains answer with error \(\alpha n\), and, with high probability, reconstructs the secret bits of all but \( O(\alpha^2 n^2)\) users.</p>
</blockquote>

<p>Although we modeled the queries, and thus the matrix \(\qmat_\pds\) as uniformly random, it’s important to note that we really only relied on the fact that 
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty^2 \gtrsim
|\{i: \sb_i \neq \hat{\sb}_i\}|,
\]
and we can reconstruct while tolerating the same \(\Omega(\sqrt{n})\) error for any family of queries that gives rise to a matrix with this property.  Intuitively, any <em>random-enough</em> family of queries will have this property.  More specifically, the property is satisfied by any matrix with no small singular values <a href="https://dl.acm.org/doi/10.1007/978-3-540-85174-5_26">[DY08]</a> or with large discrepancy <a href="https://arxiv.org/abs/1203.5453">[MN12]</a>.  There is a large body of work showing that many specific families of queries lead to reconstruction. For example, we can perform reconstruction using <em>conjunction queries</em> that ask for the marginal distribution of small subsets of the attributes <a href="https://dl.acm.org/doi/abs/10.1145/1806689.1806795">[KLSU10]</a>.  That is, queries of the form “count the number of people with blue eyes and brown hair and a birthday in August.”  In fairness, there are also families of queries that do not satisfy the property, or only satisfy quantitatively weaker versions of it, such as histograms and threshold queries, and for these queries it is indeed possible to achieve differential privacy with \( \ll \sqrt{n} \) error.</p>

<h3 id="conclusion">Conclusion</h3>

<p>This is going to be the end of our technical discussion, but before signing off, let’s mention some of the important extensions of this theorem that have been developed over the years:</p>

<ul>
  <li>
    <p>We can allow the secret information \(\sb\) to be integers or real numbers, rather than bits. The queries still return \(\qmat_\pds\cdot \sb\). The exponential attack then guarantees that, given answers with error \(\acc n\), the reconstruction \(\hat{\sb}\) satisfies \(\|\hat{\sb}-\sb\|_1 \le 4\acc n\). This means, for example, that the reconstructed secrets of all but \(4\alpha n\) users are within \(\pm 1\) of the true secrets. The efficient attack guarantees that \(\|\hat{\sb}-\sb\|_2^2 \le O(\acc^2 n^2)\), which means that the reconstructed secrets are within \(\pm 1\) for all but \(O(\acc^2 n^2)\) users.</p>
  </li>
  <li>
    <p>It’s not crucial that <em>every</em> query be answered with error \( \ll \sqrt{n} \).  If we are willing to settle 
  for an inefficient attack, then we can reconstruct even if only 51% of the queries have small error.  If at least 75% have small error, then we can reconstruct efficiently <a href="https://dl.acm.org/doi/10.1145/1250790.1250804">[DMT07]</a>.</p>
  </li>
  <li>
    <p>The reconstruction attacks still apply to the seemingly more general data model in which the private 
  dataset \(\ds\) is a subset of some arbitrary (but public) data universe \(\uni\).  To see this, note that we can take \(\uni = \{\pbs_1, \ldots, \pbs_\dsize\}\), and we can interpret the secret bits \(\sb_i\) to indicate whether \(\pbs_i\) is an element of \(\ds\). Then the reconstruction attacks allow us to determine, up to some error, which elements of \(\uni\) are contained in \(\ds\). In the setting, the attack is sometimes called <em>membership inference</em>.</p>
  </li>
  <li>
    <p>The fact that the efficient Dinur-Nissim reconstruction attack fails when the error is \( \gg \sqrt{n} \) 
  does not mean it’s easy to achieve privacy with error of that magnitude.  As we mentioned earlier, we can achieve non-trivial error guarantees for a large number of queries simply by using a random subsample of half of the dataset, which is not a private algorithm in any reasonable sense of the word, as it can reveal everything about the chosen subset.  As this example shows,</p>

    <blockquote>
      <p>preventing reconstruction attacks does not mean preserving privacy.</p>
    </blockquote>

    <p>In particular, there are membership-inference attacks that succeed in violating privacy even when the queries are answered with \( \gg \sqrt{n}\) error. We refer the reader to the survey <a href="https://privacytools.seas.harvard.edu/publications/exposed-survey-attacks-private-data">[DSSU17]</a> for a somewhat more in-depth survey of reconstruction and membership-inference attacks.</p>
  </li>
</ul>

<p>Many types of queries give rise to the conditions under which reconstruction is possible.  Stay tuned for our next post, where we show how to generate those types of queries in practice against a family of systems known as <em>Diffix</em> that are specifically designed to thwart reconstruction.</p></div>
    </summary>
    <updated>2020-10-21T16:30:00Z</updated>
    <published>2020-10-21T16:30:00Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-10-22T23:32:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/21/assistant-and-associate-professors-to-contribute-to-the-future-of-the-department-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-january-11-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/21/assistant-and-associate-professors-to-contribute-to-the-future-of-the-department-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-january-11-2020/" rel="alternate" type="text/html"/>
    <title>Assistant and Associate Professors to contribute to the future of the Department of Computer Science at Department of Computer Science, Aarhus University (apply by January 11, 2020)</title>
    <summary>Aarhus University – an international top-100 University – has made an ambitious strategic investment in a 5-year recruitment plan to radically expand the Department of Computer Science. We expect to hire four candidates this year. Therefore, we invite applications from candidates that are driven by excellence in research and teaching as well as external collaboration […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Aarhus University – an international top-100 University – has made an ambitious strategic investment in a 5-year recruitment plan to radically expand the Department of Computer Science. We expect to hire four candidates this year. Therefore, we invite applications from candidates that are driven by excellence in research and teaching as well as external collaboration on societal challenges.</p>
<p>Website: <a href="https://au.career.emply.com/ad/aarhus-university-is-hiring-assistant-and-associate-professors-to-contribute-to-t/lqwlj1/en">https://au.career.emply.com/ad/aarhus-university-is-hiring-assistant-and-associate-professors-to-contribute-to-t/lqwlj1/en</a><br/>
Email: kgronbak@cs.au.dk</p></div>
    </content>
    <updated>2020-10-21T07:02:53Z</updated>
    <published>2020-10-21T07:02:53Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-22T23:31:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/20/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-17-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/20/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-17-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track Faculty Positions at Simon Fraser University (apply by December 17, 2020)</title>
    <summary>The School of Computing Science at Simon Fraser University (SFU) invites applications for tenure-track faculty positions. The School has multiple openings and will consider applications at all ranks, including assistant, associate and full professor. Excellent applicants in all areas of computer science will be considered. Website: https://www.sfu.ca/computing/job-opportunities.html#tenure Email: cs_faculty_affairs@sfu.ca</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The School of Computing Science at Simon Fraser University (SFU) invites applications for tenure-track faculty positions. The School has multiple openings and will consider applications at all ranks, including assistant, associate and full professor. Excellent applicants in all areas of computer science will be considered.</p>
<p>Website: <a href="https://www.sfu.ca/computing/job-opportunities.html#tenure">https://www.sfu.ca/computing/job-opportunities.html#tenure</a><br/>
Email: cs_faculty_affairs@sfu.ca</p></div>
    </content>
    <updated>2020-10-20T20:36:40Z</updated>
    <published>2020-10-20T20:36:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-22T23:31:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/20/three-year-and-tenure-track-positions-at-toyota-technological-institute-at-chicago-apply-by-december-1-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/20/three-year-and-tenure-track-positions-at-toyota-technological-institute-at-chicago-apply-by-december-1-2020/" rel="alternate" type="text/html"/>
    <title>Three-year and tenure-track positions at Toyota Technological Institute at Chicago (apply by December 1, 2020)</title>
    <summary>TTIC invites applications for the following faculty positions: research assistant professor (3-year term), tenure-track assistant professor, full or associate professor, and visiting professor. Applicants for research assistant professor positions (RAPs) are encouraged to simultaneously apply for the TTIC RAP program and the Simons-Berkeley Research Fellowship. Website: https://www.ttic.edu/faculty-hiring/ Email: recruiting@ttic.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>TTIC invites applications for the following faculty positions: research assistant professor (3-year term), tenure-track assistant professor, full or associate professor, and visiting professor. Applicants for research assistant professor positions (RAPs) are encouraged to simultaneously apply for the TTIC RAP program and the Simons-Berkeley Research Fellowship.</p>
<p>Website: <a href="https://www.ttic.edu/faculty-hiring/">https://www.ttic.edu/faculty-hiring/</a><br/>
Email: recruiting@ttic.edu</p></div>
    </content>
    <updated>2020-10-20T17:06:50Z</updated>
    <published>2020-10-20T17:06:50Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-22T23:31:27Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable</id>
    <link href="https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable.html" rel="alternate" type="text/html"/>
    <title>The graphs of stably matchable pairs</title>
    <summary>The stable matching problem takes as input the preferences from two groups of agents (most famously medical students and supervisors of internships), and pairs up agents from each group in a way that encourages everyone to play along: no pair of agents would rather go their own way together than take the pairings they were both given. A solution can always be found by the Gale–Shapley algorithm, but there are generally many solutions, described by the lattice of stable matchings. Some pairs of agents are included in at least one stable matching, while some other pairs are never matched. In this way, each instance of stable matchings gives rise to a graph, the graph of stably matchable pairs. This graph is the subject and title of my latest preprint, arXiv:2010.09230, which asks: Which graphs can arise this way? How hard is it to recognize these graphs, and infer a stable matching instance that might have generated them? How does the graph structure relate to the lattice structure?</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="https://en.wikipedia.org/wiki/Stable_marriage_problem">stable matching problem</a> takes as input the preferences from two groups of agents (most famously medical students and supervisors of internships), and pairs up agents from each group in a way that encourages everyone to play along: no pair of agents would rather go their own way together than take the pairings they were both given. A solution can always be found by the <a href="https://en.wikipedia.org/wiki/Gale%E2%80%93Shapley_algorithm">Gale–Shapley algorithm</a>, but there are generally many solutions, described by the <a href="https://en.wikipedia.org/wiki/Lattice_of_stable_matchings">lattice of stable matchings</a>. Some pairs of agents are included in at least one stable matching, while some other pairs are never matched. In this way, each instance of stable matchings gives rise to a graph, the <em>graph of stably matchable pairs</em>. This graph is the subject and title of my latest preprint, <a href="https://arxiv.org/abs/2010.09230">arXiv:2010.09230</a>, which asks: Which graphs can arise this way? How hard is it to recognize these graphs, and infer a stable matching instance that might have generated them? How does the graph structure relate to the lattice structure?</p>

<p>For some answers, see the preprint. One detail is connected to <a href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html">my previous post, on polyhedra with no two disjoint faces</a> (even though there are no polyhedra in the new preprint): the (prism,\(K_{3,3}\))-minor-free graphs discussed there come up in proving an equivalence between outerplanar graphs of stably matchable pairs and lattices of <a href="https://en.wikipedia.org/wiki/Closure_problem">closures</a> of <a href="https://en.wikipedia.org/wiki/Polytree">oriented trees</a>. Instead of providing any technical details of any the other results in the paper, though, I thought it would be more fun to show a few visual highlights.</p>

<p>The following figure shows a cute mirror-inversion trick (probably already known, although I don’t know where or by whom) for embedding an arbitrary bipartite graph as an induced subgraph of a regular bipartite graph. I use it to show that graphs of stably matchable pairs have no forbidden induced subgraphs:</p>

<p style="text-align: center;"><img alt="Embedding a bipartite graph as an induced subgraph of a regular bipartite graph" src="https://11011110.github.io/blog/assets/2020/regularize.svg" width="60%"/></p>

<p>This next one depicts a combinatorial description of a stable matching instance having a \(6\times 5\) grid as its graph, in terms of the top and bottom matchings in the lattice of matchings, the “rotations” that can be used to move between matchings in this lattice, and a partial order on the rotations. For what I was doing in this paper, these rotation systems were much more convenient to work with than preferences.</p>

<p style="text-align: center;"><img alt="Rotation system describing a system of stable matchings having a 6x5 grid as its graph" src="https://11011110.github.io/blog/assets/2020/5x6.svg" width="80%"/></p>

<p>All the main ideas for a proof of NP-completeness of recognizing these graphs, by reduction from <a href="https://en.wikipedia.org/wiki/Not-all-equal_3-satisfiability">not-all-equal 3-satisfiability</a>, are visible in the next picture. The proof now in the paper is significantly more complicated, though, because the construction in this image produces nonplanar graphs but I wanted a proof that would also apply in the planar case.</p>

<p style="text-align: center;"><img alt="NP-completeness reduction from NAE3SAT to recognizing graphs of stably matchable pairs" src="https://11011110.github.io/blog/assets/2020/nae3sat-to-matching.svg"/></p>

<p>The last one shows a sparse graph that can be represented as a graph of stably-matching pairs (because it’s outerplanar, bipartite, and biconnected) but has a high-degree vertex. If we tried to test whether it could be realized by doing a brute-force search over preference systems, the time would be factorial in the degree, but my preprint provides faster algorithms that are only singly exponential in the number of edges.</p>

<p style="text-align: center;"><img alt="Outerplanar graph of stably matchable pairs with a factorial number of potential preference systems" src="https://11011110.github.io/blog/assets/2020/factorial.svg"/></p>

<p>(<a href="https://mathstodon.xyz/@11011110/105065476283424319">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-10-19T20:29:00Z</updated>
    <published>2020-10-19T20:29:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-10-20T05:33:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6623846086903606036</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6623846086903606036/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6623846086903606036" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6623846086903606036" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html" rel="alternate" type="text/html"/>
    <title>Nature vs Nurture close to my birthday</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Since I was born on Oct 1, 1960 (that's not true---if I posted  my real birthday I might get my  identity stolen), I will do a nature vs nurture post based on my life, which seems less likely to offend then doing it on someone else's life. I'll just rattle off some random points on Nature vs Nurture.</p><p>1) Is it plausible that I was born with some math talent? Is plausible that I was born with some talent to understand the polynomial van der Warden theorem? What is the granularity of nature-given or nurture-given abilities?</p><p>2) My dad was a HS English teacher and later Vice-Principal. My mom taught English at a Community college. Readers of the blog might think, given my spelling and grammar, that I was switched at birth. My mom says (jokingly?) that I was switched at birth since she thinks I am good at math.</p><p>a) I am not THAT good at math. Also see next point.</p><p>b) While there are some math families, there are not many. See my post <a href="https://blog.computationalcomplexity.org/2009/02/baseball-families-and-math-families.html">here</a>.</p><p>c) I think being raised in an intellectual atmosphere by two EDUCATORS who had the money to send me to college and allowed me the freedom to study what I wanted to  is far more important than the rather incidental matter of what field I studied.</p><p>d) Since my parents never went into math or the sciences it is very hard to tell  if they were `good at math' or even what that means.</p><p>3) There were early signs I was INTERESTED in math, though not that I was good at it.</p><p>a) In fourth grade I wanted to know how many SECONDS were in a century so I spend some time figuring it out on paper. Did I get the right answer?  I forgot about leap years.</p><p>b) I was either a beneficiary of, or a victim of, THE NEW MATH. So I learned about comm. and assoc. operations in 8th grade. We were asked to come up with our own operations. I wanted to come up with an operation that was comm. but not assoc. I did! Today I would write it as f(x,y) = |x-y|. This is the earliest I can think of where I made up a nice math problem. Might have been the last time I made up a nice math problem AND solved it without help. </p><p>c) In 10th grade I took some Martin Gardner books out of the library. The first theorem I learned not-in-school was that a graph is Eulerian iff every vertex has even degree. I read the chapter on Soma cubes and bought a set. (Soma cubes are explained <a href="https://en.wikipedia.org/wiki/Soma_cube">here</a>.) </p><p>d) I had a talent (nature?) at Soma Cubes.  I did every puzzle in the book in a week, diagrammed them, and even understood (on some level) the proofs that some could not be done. Oddly I am NOT good at 3-dim geom. Or even 2-dim geom.  For 1-dim I hold my own!</p><p>e) Throughout my childhood I noticed odd logic and odd math things that were said: </p><p>``Here at WCOZ (a radio station) we have an AXIOM, that's like a saying man, that weekends should be SEVEN DAYS LONG'' (Unless that axiom resolves CH, I don't think it should be assumed.) </p><p>``To PROVE we have the lowest prices in town we will give you a free camera!'' (how does that prove anything?) </p><p>``This margarine tastes JUST LIKE BUTTER'' (Okay-- so why not just buy butter?)</p><p>e) In 9th grade when I learned the quadratic formula I re-derived it once-a-month since I though it was important that one can prove such things.  I heard (not sure from where) that there was no 5th degree equation. At that very moment I told my parents:</p><p><i>I am going to major in math so I can find out why there is no 5th degree equation.</i></p><p>There are worse things for parents to hear from their children. See <a href="https://blog.computationalcomplexity.org/2019/06/a-proof-that-227-pi-0-and-more.html">here</a> for dad's reaction. </p><p>f) When I learned that the earth's orbit around the sun is an ellipse and that the earth was one of the foci, I wondered where the other foci is and if its important. I still wonder about this one. Google has not helped me here, though perhaps I have not phrased the question properly. If you know the answer please leave a comment. </p><p>g) I also thought about The Ketchup problem and other problems, that I won't go into since I already blogged about them  <a href="https://blog.computationalcomplexity.org/2012/06/ketchup-problem.html">here</a></p><p>4) I was on the math team in high school, but wasn't very good at it. I WAS good at making up math team problems. I am now on the committee that makes up the Univ of MD HS math competition. I am still not good at solving the problems but good at making them up. </p><p>5) From 9th grade on before I would study for an exam by making up what I thought would be a good exam and doing that. Often my exam was a better test of knowledge than the exam given. In college I helped people in Math and Physics by making up exams for them to work on as practice. </p><p>6) I was good at reading, understanding, and explaining papers. </p><p>7) I was never shy about asking for help. My curiosity exeeded by ego... by a lot!</p><p>8) Note that items 5,6, and 7 above do not mention SOLVING problems. The papers I have written are of three (overlapping) types:</p><p>a) I come up with the problem, make some inroads on it based on knowledge, and then have people cleverer (this is often) or with more knowledge (this is rarer) help me solve the problems.</p><p>b) I come up with the problem, and combine two things I know from other papers to solve it. </p><p>c) Someone else asks for my help on something and I have the knowledge needed. I can only recall one time where this lead to a paper. </p><p>NOTE- I do not think I have ever had a clever or new technique. CAVEAT: the diff between combining  known knowledge in new ways and having a clever or new technique is murky. </p><p>8) Over time these strengths and weaknesses have gotten more extreme. It has become a self-fulfilling prophecy where I spend A LOT of time making up problems without asking for help, but when I am trying to solve a problem I early on ask for help. Earlier than I should? Hard to know. </p><p>9) One aspect is `how good am I at math' But a diff angle is that I like to work on things that I KNOW are going to work out, so reading an article is better than trying to create new work. This could be a psychological thing. But is that nature or nurture?  </p><p>10) Could I be a better problem solver? Probably. Could I be a MUCH better problem solver? NO. Could I have been a better problem solver  I did more work on that angle when I was younger? Who knows? </p><p>11) Back to the Quintic: I had the following thought in ninth grade, though I could not possibly have expressed it: The question of, given a problem, how hard is it, upper and lower bounds, is a fundamental one that is worth a lifetime of study. As such my interest in complexity theory and recursion theory goes back to ninth grade or even further. My interest in Ramsey Theory for its own sake (and not in the service of complexity theory) is much more recent and does not quite fit into my narrative. But HEY- real life does not have as well defined narratives as fiction does. </p><p>12) Timing and Luck: IF I had been in grad student at a slight diff time I can imagine doing work on  algorithmic  Galois theory. <a href="https://singer.math.ncsu.edu/Algorithmic_slides.pdf">Here</a>  is a talk on Algorithmic  Galois theory. Note that one of the earliest results is by Landau and Miller from 1985---I had a course from Miller on Alg. Group Theory in 1982. This is NOT a wistful `What might have been' thought. Maybe I would have sucked at it, so its just as well I ended up doing recursion theory, then Ramsey theory, then recursion-theoretic Ramsey Theory, then muffins. </p><p><br/></p><p><br/></p><div><br/></div></div>
    </content>
    <updated>2020-10-19T15:55:00Z</updated>
    <published>2020-10-19T15:55:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-22T11:46:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7803</id>
    <link href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/" rel="alternate" type="text/html"/>
    <title>Understanding generalization requires rethinking deep learning?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Yamini Bansal, Gal Kaplun, and Boaz Barak (See also paper on arxiv, code on gitlab, upcoming talk by Yamini&amp;Boaz, video of past talk) A central puzzle of deep learning is the question of generalization. In other words, what can we deduce from the training performance of a neural network about its test performance on fresh … <a class="more-link" href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/">Continue reading <span class="screen-reader-text">Understanding generalization requires rethinking deep learning?</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em><a href="https://yaminibansal.com/about/">Yamini Bansal</a>, <a href="https://www.galkaplun.com/">Gal Kaplun</a>, and Boaz Barak</em></p>



<p>(See also <em><a href="https://arxiv.org/abs/2010.08508">paper on arxiv</a></em>,  <a href="https://gitlab.com/harvard-machine-learning/rationality-generalization">code on gitlab</a>,  <a href="https://cmsa.fas.harvard.edu/10-28-2020-new-technologies-in-mathematics-seminar/">upcoming talk by Yamini&amp;Boaz</a>,  <a href="https://youtu.be/89ixhju1hJ0">video of past talk</a>)</p>



<p>A central <a href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/">puzzle</a> of deep learning is the question of <em>generalization</em>. In other words, what can we deduce from the <em>training performance</em> of a neural network about its <em>test performance</em> on <em>fresh unseen examples</em>. An <a href="https://arxiv.org/abs/1611.03530">influential paper</a> of Zhang, Bengio, Hardt, Recht, and Vinyals showed that the answer could be “nothing at all.” </p>



<p>Zhang et al. gave examples where modern deep neural networks achieve 100% accuracy on classifying their training data, but their performance on unseen data may be no better than chance. Therefore we cannot give meaningful guarantees for deep learning using traditional “generalization bounds” that bound the difference between test and train performance by some quantity that tends to zero as the number of datapoints <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> increases. This is why (to quote their title), Zhang et al. claimed that <strong>“understanding deep learning requires rethinking generalization”</strong>.</p>



<p>But what if the issue isn’t that we’ve been doing generalization bounds wrong, but rather that we’ve been doing deep learning (or more accurately, supervised deep learning) wrong?</p>



<h3>Self Supervised + Simple fit (SSS) learning</h3>



<p>To explain what we mean, let’s take a small detour to contrast “traditional” or “end-to-end” supervised learning with a different approach to supervised learning, which we’ll call here “Self-Supervised + Simple fit” or “SSS algorithms.” (While the name “SSS algorithms” is new, the approach itself has a <a href="http://people.idsia.ch/~juergen/FKI-126-90_%28revised%29bw_ocr.pdf">long history</a> and has recently been used with great success in practice; our work gives no new methods—only new analysis.)</p>



<p>The classical or “end-to-end” approach for supervised learning can be phrased as <em>“ask and you shall receive”</em>. Given labeled data, you ask (i.e., run an optimizer) for a complex classifier (e.g., a deep neural net) that fits the data (i.e., outputs the given labels on the given data points) and hope that it will be successful on future, unseen, data points as well. End-to-end supervised learning achieves state-of-art results for many classification problems, particularly for computer vision datasets ImageNet and CIFAR-10.</p>



<p>However, end-to-end learning does not directly correspond to the way humans learn to recognize objects (see also <a href="https://youtu.be/7I0Qt7GALVk?t=2475">this talk of LeCun</a>). A baby may see millions of images in the first year of her life, but most of them do not come with explicit labels. After seeing those images, a baby can make future classifications using very few labeled examples. For example, it might be enough to show her once what is a dog and what is a cat for her to correctly classify future dogs and cats, even if they look quite different from these examples.</p>



<figure class="wp-block-image"><img alt="End-to-end learning vs SSS algorithms." src="https://i.imgur.com/LXRaTQq.png"/><strong>Figure 1:</strong> Cartoon of end-to-end vs SSS learning </figure>



<p>In recent years, practitioners have proposed algorithms that are more similar to human learning than supervised learning. Such methods separate the process into two stages. In the <em>first stage</em>, we do <strong>representation learning</strong> whereby we use <em>unlabeled</em> data to learn a <em>representation</em>: a complex map (e.g., a deep neural net) mapping the inputs into some “representation space.” In the <em>second stage</em>, we fit a simple classifier (e.g., a linear threshold function) to the representation of the datapoints and the given labels. We call such algorithms <strong>“Self-Supervision + Simple fit”</strong> or <strong>SSS algorithms</strong>. (Note that, unlike other representation-learning based classifiers, the complex representation is “frozen” and not “fine-tuned” in the second stage, where only a simple classifier is used on top of it.)</p>



<p>While we don’t have a formal definition, a “good representation” should make downstream tasks easier, in the sense of allowing for fewer examples or simpler classifiers. We typically learn a representation via <strong>self supervision</strong> , whereby one finds a representation minimizing an objective function that intuitively requires some “insight” into the data. Approaches for self-supervision include reconstruction, where the objective involves recovering data points from partial information (e.g., recover missing <a href="https://arxiv.org/abs/1810.04805">words</a> or <a href="https://arxiv.org/abs/1604.07379">pixels</a>), and <em><a href="https://arxiv.org/abs/2002.05709">contrastive learning</a></em>, where the objective is to find a representation that make similar points close and dissimilar points far (e.g., in Euclidean space).</p>



<p>SSS algorithms have been traditionally used in natural language processing, where unlabeled data is plentiful, but labeled data for a particular task is often scarce. But recently SSS algorithms were also <a href="https://arxiv.org/abs/1902.06162">used with great success</a> even for vision tasks such as ImageNet and CIFAR10 where <em>all data is labeled!</em> While SSS algorithms do not yet beat the state-of-art supervised learning algorithms, they do get <a href="https://arxiv.org/abs/2006.10029">pretty close</a>. SSS algorithms also have other practical advantages over “end-to-end supervised learning”: they can make use of unlabeled data, the representation could be useful for non-classification tasks, and may have improved out of distribution performance. There has also been recent theoretical analysis of contrastive and reconstruction learning under certain statistical assumptions (see <a href="https://arxiv.org/abs/1902.09229">Arora et al</a> and  <a href="https://arxiv.org/abs/2008.01064">Lee et al</a>).</p>



<h3>The generalization gap of SSS algorithms</h3>



<p>In <a href="https://arxiv.org/abs/2010.08508">a recent paper</a>, we show that SSS algorithms not only work in practice, but work in theory too.</p>



<p>Specifically, we show that such algorithms have <strong>(1)</strong> small generalization gap and <strong>(2)</strong> we can <strong>prove</strong> (under reasonable assumptions) that their generalization gap tends to zero with the number of samples, with bounds that are meaningful for many modern classifiers on the CIFAR-10 and ImageNet datasets. We consider the setting where all data is labeled, and the <em>same dataset</em> is used for both learning the representation and fitting a simple classifier. The resulting classifier includes the overparameterized representation, and so we cannot simply apply “off the shelf” generalization bounds. Indeed, a priori it’s not at all clear that the generalization gap for SSS algorithms should be small.</p>



<p>To get some intuition for the generalization gap of SSS algorithms, consider the experiment where we inject some <em>label noise</em> into our distribution. That is, we corrupt an <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta"/> fraction of the labels in both the train and test set, replacing them with random labels. Already in the noiseless case (<img alt="\eta=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=0"/>), the generalization gap of SSS algorithms is noticeably smaller than that of end-to-end supervised learning. As we increase the noise, the difference becomes starker. End-to-end supervised learning algorithms can always achieve 100% training accuracy, even as the test accuracy deteriorates, since they can “memorize” all the training labels they are given. In contrast, for SSS algorithms, both training and testing accuracy decrease together as we increase the noise, with training accuracy correlating with test performance. </p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif"><img alt="" class="wp-image-7826" src="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif?w=812"/></a><strong>Figure 2:</strong> Generalization gap of end-to-end and SSS algorithms on CIFAR 10 as a function of noise (since there are 10 classes, 90% noisy samples corresponds to the Zhang et al experiment). See also <a href="https://plotly.com/~yaminibansal/1.embed" rel="noreferrer noopener" target="_blank">interactive version</a>.</figure>



<p/>



<p>Our main theoretical result is a formal proof of the above statement. To do so, we consider training with a small amount of label noise (say <img alt="\eta=5\%" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D5%5C%25&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=5\%"/>) and define the following quantities:</p>



<ul><li>The <strong>robustness gap</strong> is the amount by which training accuracy degrades between the “clean” (<img alt="\eta=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=0"/>) experiment and the noisy one. (In this and all other quantities, the training accuracy is measured with respect to the original uncorrupted labels.)</li><li>The <strong>memorization gap</strong> considers the noisy experiment (<img alt="\eta=5\%" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D5%5C%25&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=5\%"/>) and measures the amount by which performance on the corrupted data samples (where we received the wrong label) is worse than performance on the overall training set. If the algorithm can memorize all given labels, it will be perfectly wrong on the corrupted data samples, leading to a large memorization gap.</li><li>The <strong>rationality gap</strong> is the difference between the performance on the corrupted data samples and performance on unseen test examples. For example, if <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is an image of a dog, then it measures the difference between the probability that <img alt="f(x)=\text{&quot;dog&quot;}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3D%5Ctext%7B%22dog%22%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)=\text{&quot;dog&quot;}"/> when <img alt="(x,\text{&quot;cat&quot;})" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C%5Ctext%7B%22cat%22%7D%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,\text{&quot;cat&quot;})"/> is in the training set and the probability that <img alt="f(x)=\text{&quot;dog&quot;}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3D%5Ctext%7B%22dog%22%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)=\text{&quot;dog&quot;}"/> when <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is not in the training set at all. Since intuitively, getting the wrong label should be worse than getting no label at all, we typically expect the rationality gap to be around zero or negative. Formally we define the rationality gap to the maximum between <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> and the difference above, so it is always non-negative. We think of an algorithm with a significant positive rationality gap as “irrational.”</li></ul>



<p>By summing up the quantities above, we get the following inequality, which we call the <strong>RRM bound</strong></p>



<p><em><span class="has-inline-color" style="color: #0693e3;">generalization gap</span></em> <img alt="\leq" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\leq"/> <em><span class="has-inline-color" style="color: #00d084;">robustness gap</span></em> <img alt="+" class="latex" src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="+"/> <em><span class="has-inline-color" style="color: #fcb900;">rationality gap</span></em> <img alt="+" class="latex" src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="+"/> <em><span class="has-inline-color" style="color: #cf2e2e;">memorization gap</span></em></p>



<p>In practice, the <strong>robustness</strong> and <strong>rationality</strong> gaps are always small, both for end-to-end supervised algorithms (which have a large generalization gap), and for SSS algorithms (which have a small generalization gap). Thus the main contribution to the generalization gap comes from the <strong>memorization gap</strong>. Roughly speaking, our main result is the following:</p>



<p><em>If the complexity of the second-stage classifier of an SSS algorithm is smaller than the number of samples then the generalization gap is small.</em></p>



<p>See the <a href="https://arxiv.org/abs/2010.08508">paper</a> for the precise definition of “complexity,” but it is bounded by the number of bits that it takes to describe the simple classifier (no matter how complex is the representation used in the first stage). Our bound yields non-vacuous results in various practical settings; see the figures below or their <a href="https://share.streamlit.io/yaminibansal/streamlit-apps/main/figure1.py" rel="noreferrer noopener" target="_blank">interactive version</a>. </p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/intro-cifar.png"><img alt="" class="wp-image-7815" src="https://windowsontheory.files.wordpress.com/2020/10/intro-cifar.png?w=1024"/></a><strong>Figure 3:</strong> Empirical study of the generalization gap of a variety of of SSS algorithms on CIFAR-10. Each vertical line corresponds to one model, sorted by generalization gap. The RRM bound is typically near-tight, and our complexity upper bound is often non vacuous. Use <a href="https://share.streamlit.io/yaminibansal/streamlit-apps/main/figure1.py" rel="noreferrer noopener" target="_blank">this webpage</a> to interact with figures 3 and 4.</figure>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/imagenet_gaps.png"><img alt="" class="wp-image-7817" src="https://windowsontheory.files.wordpress.com/2020/10/imagenet_gaps.png?w=1024"/></a><strong>Figure 4:</strong> Empirical study of gaps for the ImageNet dataset. Because of limited computational resources, we only evaluated the theoretical bound for two models in this dataset.</figure>



<h3>What’s next</h3>



<p>There are still many open questions. Can we prove rigorous bounds on robustness and rationality? We have some preliminary results in the paper, but there is much room for improvement. Similarly, our complexity-based upper bound is far from tight at the moment, though the RRM bound itself is often surprisingly tight. Our work only applies to SSS algorithms, but people have the intuition that even end-to-end supervised learning algorithms implicitly learn a representation. So perhaps these tools can apply to such algorithms as well. As mentioned, we don’t yet have formal definitions for “good representations,” and the choice of the self-supervision task is still somewhat of a “black art” – can we find a more principled approach?</p></div>
    </content>
    <updated>2020-10-19T00:30:40Z</updated>
    <published>2020-10-19T00:30:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-10-22T23:31:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint</id>
    <link href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html" rel="alternate" type="text/html"/>
    <title>Polyhedra without disjoint faces</title>
    <summary>Some research I’ve been doing led me to consider the (prism,\(K_{3,3}\))-minor-free graphs. It’s not always easy to go from forbidden minors to the graphs that forbid them, or vice versa, but in this case I think there’s a nice characterization, which I’m posting here because it doesn’t fit into the research writeup: these are the graphs whose nontrivial triconnected components are \(K_5\), wheel graphs, or the graph \(K_5-e\) of the triangular bipyramid. The illustration below shows an example of a graph with this structure, with its nontrivial triconnected components colored red and yellow. There’s a simpler and more geometric way to say almost the same thing: the only convex polyhedra that do not have two vertex-disjoint faces are the pyramids and the triangular bipyramid.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Some research I’ve been doing led me to consider the (prism,\(K_{3,3}\))-minor-free graphs. It’s not always easy to go from <a href="https://en.wikipedia.org/wiki/Forbidden_graph_characterization">forbidden minors</a> to the graphs that forbid them, or vice versa, but in this case I think there’s a nice characterization, which I’m posting here because it doesn’t fit into the research writeup: these are the graphs whose nontrivial triconnected components are \(K_5\), <a href="https://en.wikipedia.org/wiki/Wheel_graph">wheel graphs</a>, or the graph \(K_5-e\) of the <a href="https://en.wikipedia.org/wiki/Triangular_bipyramid">triangular bipyramid</a>. The illustration below shows an example of a graph with this structure, with its nontrivial triconnected components colored red and yellow. There’s a simpler and more geometric way to say almost the same thing: the only convex polyhedra that do not have two vertex-disjoint faces are the pyramids and the triangular bipyramid.</p>

<p style="text-align: center;"><img alt="A (prism, K_{3,3})-minor-free graph, with its nontrivial triconnected components colored red and yellow" src="https://11011110.github.io/blog/assets/2020/prism-k33-free.svg"/></p>

<p>Some definitions:</p>

<ul>
  <li>
    <p>Here by the prism graph I mean the graph of the triangular prism. Any other prism has this one as a minor, and so is irrelevant as a forbidden minor. However, the pyramids in this structure can have any polygon as their base, corresponding to wheel graphs with arbitrarily many vertices.</p>
  </li>
  <li>
    <p>\(K_{3,3}\) is a complete bipartite graph with three vertices on each side of its bipartition, famous as the <a href="https://en.wikipedia.org/wiki/Three_utilities_problem">utility graph</a>, one of the two forbidden minors for planar graphs. The triangular prism graph and \(K_{3,3}\) are the only two <a href="https://en.wikipedia.org/wiki/Cubic_graph">3-regular graphs</a> with six vertices.</p>
  </li>
</ul>

<p style="text-align: center;"><img alt="The prism graph and K_{3,3}" src="https://11011110.github.io/blog/assets/2020/prism-k33.svg"/></p>

<ul>
  <li>
    <p>The triconnected components of a graph are the graphs associated with the nodes of its <a href="https://en.wikipedia.org/wiki/SPQR_tree">SPQR tree</a>, or of the SPQR trees of its biconnected components. These are cycle graphs, dipole multigraphs, or 3-connected graphs, and by “nontrivial” I mean the ones that are not cycles or dipoles. A triconnected component might not be a subgraph of the given graph, because it can have additional edges that correspond to paths in the given graph. For instance, subdividing the edges of any graph into paths, or more generally replacing edges by arbitrary series-parallel graphs, does not change its set of nontrivial triconnected components.</p>
  </li>
  <li>
    <p>I’m using “face” in the usual three-dimensional meaning, a two-dimensional subset of the boundary of the polyhedron. For higher-dimensional polytopes, “face” has a different meaning that also includes vertices and edges, and “facet” would be used to refer to the \((d-1)\)-dimensional faces, but using that terminology seems overly pedantic here.</p>
  </li>
</ul>

<p>Sketch of proof of the characterization of polyhedra without two disjoint faces: Consider any polyhedron without disjoint faces. If one face shares an edge with all the others, it’s a <a href="https://en.wikipedia.org/wiki/Halin_graph">Halin graph</a>, a graph formed by linking the leaves of a tree into a cycle; if the tree is a star, it’s a pyramid, and otherwise contracting all but one of the interior edges of the tree, and then all but four of the cycle edges, will produce a prism minor. In the remaining case, some two faces share only a vertex \(v\), which must have degree four or more. Each face that is disjoint from \(v\) must touch all that faces incident to \(v\), which can only happen when there is one face disjoint from \(v\) (a pyramid) or two faces disjoint from \(v\), neither of which has an edge disjoint from the other one (a bipyramid).</p>

<p>Sketch of a lemma that every convex polyhedron with two disjoint faces has a prism minor: glue a pyramidal cap into each of the two faces, producing a larger convex polyhedron which by either <a href="https://en.wikipedia.org/wiki/Steinitz%27s_theorem">Steinitz’s theorem</a> or <a href="https://en.wikipedia.org/wiki/Balinski%27s_theorem">Balinski’s theorem</a> is necessarily 3-connected, and find three vertex-disjoint paths between the apexes of the attached pyramids. The parts of these paths outside the two glued pyramids, together with the boundaries of the two faces, form a subdivision of a prism.</p>

<p>Sketch of proof of the characterization of (prism,\(K_{3,3}\))-minor-free graphs: The nontrivial triconnected components are exactly the maximal triconnected minors of the given graph, so if either of the two triconnected forbidden minors is to be found in the given graph, it will be found in one of the triconnected components. \(K_5\) and the triangular bipyramid are too small to have one of the forbidden minors. The only 3-connected minors of the pyramid graphs are smaller pyramids, obtained by contracting one of the cycle edges of the pyramid, so these also do not have a forbidden minor. Therefore the graphs of the stated form are all (prism,\(K_{3,3}\))-minor-free.</p>

<p>In the other direction, suppose that a graph is (prism,\(K_{3,3}\))-minor-free.
Each triconnected component is a minor, so it must also be (prism,\(K_{3,3}\))-minor-free. What can these components look like? Forbidding \(K_{3,3}\) as a minor rules out nonplanar components other than \(K_5\), by a theorem of Wagner<sup id="fnref:wagner"><a class="footnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fn:wagner">1</a></sup> and Hall.<sup id="fnref:hall"><a class="footnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fn:hall">2</a></sup> So the remaining components that we need to consider are triconnected planar graphs with no prism minor. These cannot have two disjoint faces by the lemma, and so they can only be pyramids or the triangular bipyramid.</p>

<div class="footnotes">
  <ol>
    <li id="fn:wagner">
      <p>K. Wagner. Über eine Erweiterung des Satzes von Kuratowski. <em>Deutsche Mathematik</em>, 2:280–285, 1937. <a class="reversefootnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fnref:wagner">↩</a></p>
    </li>
    <li id="fn:hall">
      <p>D. W. Hall. A note on primitive skew curves. <em>Bulletin of the American Mathematical Society</em>, 49(12):935–936, 1943. <a href="https://doi.org/10.1090/ S0002-9904-1943-08065-2">doi:10.1090/ S0002-9904-1943-08065-2</a>. <a class="reversefootnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fnref:hall">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/105058649830809584">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-10-18T17:06:00Z</updated>
    <published>2020-10-18T17:06:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-10-20T05:33:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/155</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/155" rel="alternate" type="text/html"/>
    <title>TR20-155 |  Log-rank and lifting for AND-functions | 

	Sam McGuire, 

	Shachar Lovett, 

	Alexander Knop, 

	Weiqiang Yuan</title>
    <summary>Let $f: \{0,1\}^n \to \{0, 1\}$ be a boolean function, and let $f_\land (x, y) = f(x \land y)$ denote the AND-function of $f$, where $x \land y$ denotes bit-wise AND. We study the deterministic communication complexity of $f_\land$ and show that, up to a $\log n$ factor, it is bounded by a polynomial in the logarithm of the real rank of the communication matrix of $f_\land$. This comes within a $\log n$ factor of establishing the log-rank conjecture for AND-functions with no assumptions on $f$. Our result stands in contrast with previous results on special cases of the log-rank 
conjecture, which needed significant restrictions on $f$ such as monotonicity or low $\mathbb{F}_2$-degree. Our techniques can also be used to prove (within a $\log n$ factor) a lifting theorem for AND-functions, stating that the deterministic communication complexity of $f_\land$ is polynomially-related to the AND-decision tree complexity of $f$.

The results rely on a new structural result regarding boolean functions $f:\{0, 1\}^n \to \{0, 1\}$ with a sparse polynomial representation, which may be of independent interest. We show that if the polynomial computing $f$ has few monomials then the set system of the monomials has a small hitting set, of size poly-logarithmic in its sparsity. We also establish extensions of this result to multi-linear polynomials $f:\{0,1\}^n \to \mathbb{R}$ with a larger range.</summary>
    <updated>2020-10-18T14:44:03Z</updated>
    <published>2020-10-18T14:44:03Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-22T23:31:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=493</id>
    <link href="https://tcsplus.wordpress.com/2020/10/16/tcs-talk-wednesday-october-21-aayush-jain-ucla/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 21 — Aayush Jain, UCLA</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 21th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Aayush Jain from UCLA will speak about “Indistinguishability Obfuscation from Well-Founded Assumptions” (abstract below). You can reserve a spot as an individual or a group to join us […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 21th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Aayush Jain</strong> from UCLA will speak about “<em>Indistinguishability Obfuscation from Well-Founded Assumptions</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: We present a construction of an indistinguishability obfuscation scheme, whose security rests on the subexponential hardness of four well-founded assumptions. We show the existence of an indistinguishability Obfuscation scheme for all circuits assuming sub-exponential security of the following assumptions:</p>



<ul class="wp-block-quote"><li>The Learning with Errors (LWE) assumption with arbitrarily small subexponential modulus-to-noise ratio,</li><li>The SXDH assumption with respect to bilinear groups of prime order <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="p"/>,</li><li>Existence of a Boolean Pseudorandom Generator (PRG) in <img alt="\mathsf{NC}^0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathsf{NC}^0"/> with arbitrary polynomial stretch, that is, mapping <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n"/> bits to <img alt="n^{1+\tau}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2B%5Ctau%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n^{1+\tau}"/> bits, for any constant \tau&gt;0.</li><li>The Learning Parity with Noise (LPN) assumption over <img alt="\mathbb{Z}_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathbb{Z}_p"/> with error-rate <img alt="\ell^{-\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell%5E%7B-%5Cdelta%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\ell^{-\delta}"/>, where <img alt="\ell" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\ell"/> is the dimension of the secret and <img alt="\delta&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%3E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\delta&gt;0"/> is an arbitrarily small constant.<br/>Further, assuming only polynomial security of these assumptions, there exists a compact public-key functional encryption scheme for all circuits.</li></ul>



<p class="wp-block-quote">The main technical novelty is the introduction and construction of a cryptographic pseudorandom generator that we call a Structured-Seed PRG (sPRG), assuming LPN over <img alt="\mathbb{Z}_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathbb{Z}_p"/> and PRGs in <img alt="\mathsf{NC}^0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathsf{NC}^0"/>. During the talk, I will discuss how structured seed PRGs have evolved from different notions of novel pseudorandom generators proposed in the past few years, and how an interplay between different areas of theoretical computer science played a major role in providing valuable insights leading to this work. Time permitting, I will go into the details of how to construct sPRGs. <br/><br/>Joint work with Huijia (Rachel) Lin and Amit Sahai</p></div>
    </content>
    <updated>2020-10-16T06:33:00Z</updated>
    <published>2020-10-16T06:33:00Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-10-22T23:32:06Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/10/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Mirzakhani and meanders (\(\mathbb{M}\)). On some more-than-coincidental similarities in formulas found by Mirzakhani for numbers of geodesics on hyperbolic surfaces and by Vincent Delecroix, Elise Goujard, Peter Zograf, and Anton Zorich in a new preprint for numbers of meanders, closed curves with a given number of intersections with a line.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://scilogs.spektrum.de/hlf/mirzakhani-and-meanders/">Mirzakhani and meanders</a> (<a href="https://mathstodon.xyz/@11011110/104963847400612388">\(\mathbb{M}\)</a>). On some more-than-coincidental similarities in formulas found by Mirzakhani for numbers of geodesics on hyperbolic surfaces and by Vincent Delecroix, Elise Goujard, Peter Zograf, and Anton Zorich <a href="https://arxiv.org/abs/1705.05190">in a new preprint</a> for numbers of <a href="https://en.wikipedia.org/wiki/Meander_(mathematics)">meanders</a>, closed curves with a given number of intersections with a line.</p>
  </li>
  <li>
    <p><a href="https://mycqstate.wordpress.com/2020/09/29/it-happens-to-everyonebut-its-not-fun/">Retraction of “a proof of soundness of the Raz-Safra low-degree test against entangled-player strategies, a key ingredient in the proof of the quantum low-degree test, itself a key ingredient in the \(\mathsf{MIP}^*=\mathsf{RE}\) paper”</a> (<a href="https://mathstodon.xyz/@11011110/104969573344196233">\(\mathbb{M}\)</a>). \(\mathsf{MIP}^*=\mathsf{RE}\) is patched and remains believed true but not fully refereed. This post provides a lot more than the standard we-found-a-bug notice: a good description of what happened, what it implies technically, and how it affects the authors and community.</p>
  </li>
  <li>
    <p><a href="https://blogs.lse.ac.uk/impactofsocialsciences/2020/09/30/for-academic-publishing-to-be-trans-inclusive-authors-must-be-allowed-to-retroactively-change-their-names/">For academic publishing to be trans-inclusive, authors must be allowed to retroactively change their names</a> (<a href="https://mathstodon.xyz/@11011110/104972193066839079">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2020/10/03/weekend-reads-unicorn-poo-and-other-fraudulent-covid-19-treatments-disgraced-researchers-and-drug-company-payouts-a-fictional-account-of-real-fraud/">via</a>). I agree — more than once in researching Wikipedia bios I found past publications under deadnames. If the authors prefer this to be better hidden, while continuing to be credited for their past work, we should try to honor that preference.</p>
  </li>
  <li>
    <p>It’s easy to point and laugh at the <a href="https://tex.stackexchange.com/questions/565387/mathbb-r-is-not-showing-in-reference-bibtex">researcher who thought bibtex from Google scholar was usable</a> (<a href="https://mathstodon.xyz/@11011110/104980666583964923">\(\mathbb{M}\)</a>), but their question brings up a more serious question: why is Google’s bibtex so bad? Even the junk I get from <code class="language-plaintext highlighter-rouge">curl -LH "Accept: application/x-bibtex" http://doi.org/...</code> is mostly usable in comparison. I’m tempted to suggest that they go to MathSciNet for the good stuff but I’m worried they won’t have access.</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2020/09/30/ten-kinetic-sculptures-by-anne-lilly.html">Ten kinetic sculptures by Anne Lilly</a> (<a href="https://mathstodon.xyz/@11011110/104988795486796768">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://jix.one/the-assembly-language-of-satisfiability/">The assembly language of satisfiability</a> (<a href="https://mathstodon.xyz/@jix/104971574457861322">\(\mathbb{M}\)</a>). Why Boolean satisfiability is too low-level to work well as a way to express the kind of problems satisfiability-solvers can solve, and how <a href="https://en.wikipedia.org/wiki/Satisfiability_modulo_theories">satisfiability modulo theories</a> can help.</p>
  </li>
  <li>
    <p><a href="https://cp4space.hatsya.com/2020/10/01/subsumptions-of-regular-polytopes/">Which regular polytopes have their vertices a subset of other regular polytopes in the same dimension</a> (<a href="https://mathstodon.xyz/@11011110/104998010300898992">\(\mathbb{M}\)</a>)? We don’t know! The answer is closely connected to the existence of <a href="https://en.wikipedia.org/wiki/Hadamard_matrix">Hadamard matrices</a>, which are famously conjectured to exist in dimensions divisible by four. A solution to the Hadamard matrix existence problem would also solve the polytope problem.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/computer-scientists-break-traveling-salesperson-record-20201008/">Computer scientists break traveling salesperson record</a> (<a href="https://mathstodon.xyz/@11011110/105006269895209659">\(\mathbb{M}\)</a>). I <a href="https://11011110.github.io/blog/2020/07/15/linkage.html">linked to this back in July</a> when <a href="https://arxiv.org/abs/2007.01409">Karlin, Klein, and Gharan’s preprint</a> giving a \((1/2-\varepsilon)\)-approximation to TSP first came out, but now it’s getting wider publicity in <em>Quanta</em>. See also <a href="https://www.sciencenews.org/article/shayan-oveis-gharan-theoretical-computer-scientist-sn-10-scientists-watch">an earlier (paywalled) piece on the same story in <em>ScienceNews</em></a>.</p>
  </li>
  <li>
    <p>Symmetry, quasisymmetry, and kite-rhomb tessellations in the mathematical modeling of virus surface structures: <a href="https://ima.org.uk/721/fighting-infections-with-symmetry/">IMA</a>,
<a href="https://inference-review.com/article/mathematical-virology"><em>Inference</em></a>,
<a href="https://archive.bridgesmathart.org/2018/bridges2018-237.pdf">Bridges</a> (<a href="https://mathstodon.xyz/@11011110/105009372623320055">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://shop.deutschepost.de/freies-quadrat-briefmarke-zu-1-70-eur-10er-bogen">New German postage stamp features the missing square puzzle</a> (<a href="https://muensterland.social/@rgx/105007333917605810">\(\mathbb{M}\)</a>, <a href="https://en.wikipedia.org/wiki/Missing_square_puzzle">see also</a>).</p>
  </li>
  <li>
    <p><a href="https://www.newstatesman.com/international/science-tech/2020/07/ra-fisher-and-science-hatred">R. A. Fisher and the science of hatred</a> (<a href="https://mathstodon.xyz/@11011110/105020588148970072">\(\mathbb{M}\)</a>). If you’ve been wondering why noted academics of yesteryear like <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">R. A. Fisher</a> (a major figure in statistics) and <a href="https://en.wikipedia.org/wiki/David_Starr_Jordan">David Starr Jordan</a> (founding president of Stanford University) have been having their names taken off things lately, the link looks like a good explainer of their views on eugenics, and why those views are now regarded as deeply racist, even for their times.</p>
  </li>
  <li>
    <p><a href="http://hardmath123.github.io/minimal-surface.html">Sol LeWitt and the soapy pit</a> (<a href="https://mathstodon.xyz/@11011110/105023612862469185">\(\mathbb{M}\)</a>, <a href="https://abhikjain360.github.io/2020/08/01/The-186th-Carnival-of-Mathematics.html">via</a>, <a href="https://aperiodical.com/2020/10/carnival-of-mathematics-186/">via2</a>). LeWitt was an artist who in 1974 made a piece exhibiting all of the possible subsets of edges of the cube. The comfortably numbered blog examines what you get if you use these as frames for making soap films.</p>
  </li>
  <li>
    <p><a href="http://landezine.com/index.php/2013/02/funenpark-by-landlab/">Funenpark</a> (<a href="https://mathstodon.xyz/@11011110/105029637909838642">\(\mathbb{M}\)</a>). To be clear, Funenpark is not a fun-park. It is a high-density residential development on former industrial land near Amsterdam. What interests me is their <a href="https://www.flickr.com/photos/shiratski/2242870712/">pentagonal tiles</a>. It’s not one of the <a href="https://en.wikipedia.org/wiki/Pentagonal_tiling">15 monohedral pentagon tilings</a>: the tiles have two shapes, one forming half of a regular hexagon (all angles \(&gt; 60^\circ\)) and another surrounding the hexagons (sharp angle \(= 60^\circ\)). Still, a nice pattern.</p>
  </li>
  <li>
    <p>Sometimes when I’ve been doing big literature searches on jstor (manually clicking on dozens of links because jstor’s search results don’t tell me which book is being reviewed, delayed by maybe a second or so per click so that I don’t get stopped by jstor’s anti-bot filters) I then get locked out of Google Scholar for a day or so on the same IP address because Google thinks I’m a bot. It doesn’t happen when I search Scholar directly. Has anyone else noticed this? Any idea how to avoid it? (<a href="https://mathstodon.xyz/@11011110/105037500352288970">\(\mathbb{M}\)</a>)</p>
  </li>
  <li>
    <p>While I’m linking Dutch pentagonal tiling architecture, here’s <a href="https://www.19hetatelier.nl/nieuws/wiskundige-vijfhoek-op-gevel-basisschool-de-garve-lochem/">an elementary school in Lochem decorated with the Mann–McLoud–Von Derau tile</a> (<a href="https://mathstodon.xyz/@11011110/105042753206122004">\(\mathbb{M}\)</a>, <a href="https://twitter.com/alexvdbrandhof/status/1004661466149085184">via</a>), which in 2015 became the 15th and final Euclidean monohedral pentagonal tile to be found. The link is in Dutch but Google translate works well except at one point: the school’s name, “De Garve”, means “the sheaf”, and the article remarks that this is appropriate for a pattern that looks like ears of corn.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-10-15T22:15:00Z</updated>
    <published>2020-10-15T22:15:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-10-20T05:33:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4895611208507130576</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4895611208507130576/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/50-years-of-pbs.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4895611208507130576" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4895611208507130576" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/50-years-of-pbs.html" rel="alternate" type="text/html"/>
    <title>50 Years of PBS</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Public Broadcasting Service (PBS) launched fifty years ago this month in the United States. The New York Times talks about its <a href="https://www.nytimes.com/2020/10/13/arts/television/pbs-50-anniversary.html">fifty reasons</a> how the network mattered. I'll throw in my thoughts.</p><p>I was just slightly too old for shows like Sesame Street, Electric Company, Mr. Rogers and Zoom, not that that stopped me from watching them. My kids grew up on Barney and Friends. My daughter even had a toy Barney that interacted with the show, which went <a href="https://blog.computationalcomplexity.org/2012/02/barney-evil-dinosaur.html">as well as you'd expect</a>. </p><p>PBS introduced me to those great British TV shows for young nerds like me including Monty Python and Doctor Who. I wasn't into Nova but did watch Carl Sagan's Cosmos religiously in high school.</p><p>My favorite PBS show was the American Experience, short documentaries about US culture. I remember learning about this history of Coney Island and the quiz show scandals before Robert Redford made a movie about it.</p><p>Siskel and Ebert got their start on PBS and became my go to source for movie reviews.</p><p>In 1987 PBS broadcasted Ivy League football games. One Saturday I sat down expecting to watch my alma mater and instead got supreme court hearings. Only on PBS could Cornell football get Borked.</p></div>
    </content>
    <updated>2020-10-15T13:00:00Z</updated>
    <published>2020-10-15T13:00:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-22T11:46:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7800</id>
    <link href="https://windowsontheory.org/2020/10/14/itc-2021-guest-post-by-benny-applebaum/" rel="alternate" type="text/html"/>
    <title>ITC 2021 (guest post by Benny Applebaum)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Following last year’s successful launch, we are happy to announce the second edition of the conference on Information-Theoretic Cryptography (ITC). The call for papers for ITC 2021 is out, and, to cheer you up during lockdowns, we prepared a short theme song https://youtu.be/kZT1icVoTp8   Feel free to add your own verse 😉 The submission deadline is … <a class="more-link" href="https://windowsontheory.org/2020/10/14/itc-2021-guest-post-by-benny-applebaum/">Continue reading <span class="screen-reader-text">ITC 2021 (guest post by Benny Applebaum)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Following last year’s successful launch, we are happy to announce the second edition of the conference on <a href="https://itcrypto.github.io/" rel="noreferrer noopener" target="_blank"><em>Information-Theoretic Cryptography</em></a><em> (ITC)</em>.</p>



<p>The <a href="https://itcrypto.github.io/2021/" rel="noreferrer noopener" target="_blank">call for papers</a> for ITC 2021 is out, and, to cheer you up during lockdowns, we prepared a short theme song <a href="https://youtu.be/kZT1icVoTp8" rel="noreferrer noopener" target="_blank">https://youtu.be/kZT1icVoTp8</a>  </p>



<p>Feel free to add your own verse <img alt="&#x1F609;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png" style="height: 1em;"/></p>



<p>The submission deadline is <strong>February 1st</strong>. Please submit your best work to ITC 2021! We hope to see many of you there!</p></div>
    </content>
    <updated>2020-10-14T22:05:24Z</updated>
    <published>2020-10-14T22:05:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-10-22T23:31:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blogs.princeton.edu/imabandit/?p=1437</id>
    <link href="https://blogs.princeton.edu/imabandit/2020/10/13/2020/" rel="alternate" type="text/html"/>
    <title>2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest post on this blog was on December 30th 2019. It seems like a lifetime away. The rate at which paradigm shifting events have been happening in 2020 is staggering. And it might very well be that the worst … <a href="https://blogs.princeton.edu/imabandit/2020/10/13/2020/">Continue reading <span class="meta-nav">→</span></a></p></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest post on this blog was on December 30th 2019. It seems like a lifetime away. The rate at which paradigm shifting events have been happening in 2020 is staggering. And it might very well be that the worst of 2020 is ahead of us, especially for those of us currently in the USA. </p>
<p>When I started communicating online broadly (blog, twitter) I promised myself to keep it strictly about science (or very closely neighboring topics), so the few lines above is all I will say about the current worldwide situation.</p>
<p>In other news, as is evident from the 10 months hiatus in blogging, I have taken elsewhere (at least temporarily) my need for rapid communication about theorems that currently excite me. Namely to <a class="liinternal" href="https://www.youtube.com/sebastienbubeck">youtube</a>. Since the beginning of the pandemic I have been recording home videos of what would have been typically blog posts, with currently 5 such videos:</p>
<ol>
<li><a class="liinternal" href="https://youtu.be/uRarIjJGmhs">A law of robustness for neural networks</a> : I explain the conjecture we recently made that, for random data, any interpolating two-layers neural network must have its Lipschitz constant larger than the squareroot of the ratio between the size of the data set and the number of neurons in the network. This would prove that overparametrization is *necessary* for robustness.</li>
<li><a class="liinternal" href="https://youtu.be/U-XsUB69mvc">Provable limitations of kernel methods</a> : I give the proof by Zeyuan Allen-Zhu and Yuanzhi Li that there are simple noisy learning tasks where *no kernel* can perform well while simple two-steps procedures can learn.</li>
<li><a class="liinternal" href="https://youtu.be/6-GBDpe2kuI">Memorization with small neural networks</a> : I explain old (classical combinatorial) and new (NTK style) construction of optimally-sized interpolating two-layers neural networks.</li>
<li><a class="liinternal" href="https://youtu.be/HIwZH2C--nA">Coordination without communication</a> : This video is the only one in the current series where I don’t talk at all about neural networks. Specifically it is about the cooperative multiplayer multiarmed bandit problem. I explain the strategy we devised with Thomas Budzinski to solve this problem (for the stochastic version) without *any* collision at all between the players.</li>
<li><a class="liinternal" href="https://youtu.be/O84mcq7P_es">Randomized smoothing for certified robustness</a> : Finally, in the first video chronologically, I explain the only known technique for provable robustness guarantees in neural networks that can scale up to large models.</li>
</ol>
<p>The next video will be about basic properties of tensors, and how it can be used for smooth interpolation (in particular in the context of our law of robustness conjecture). After that, we will see, maybe more neural networks, maybe more bandits, maybe some non-convex optimization ….</p>
<p>Stay safe out there!</p><p/></div>
    </content>
    <updated>2020-10-14T03:14:07Z</updated>
    <published>2020-10-14T03:14:07Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Sebastien Bubeck</name>
    </author>
    <source>
      <id>https://blogs.princeton.edu/imabandit</id>
      <link href="https://blogs.princeton.edu/imabandit/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blogs.princeton.edu/imabandit" rel="alternate" type="text/html"/>
      <subtitle>Random topics in optimization, probability, and statistics. By Sébastien Bubeck</subtitle>
      <title>I’m a bandit</title>
      <updated>2020-10-22T23:32:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5013</id>
    <link href="https://www.scottaaronson.com/blog/?p=5013" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5013#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5013" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Vote in person if you can</title>
    <summary xml:lang="en-US">[If you’re not American, or you’re American but a masochist who enjoys the current nightmare, this post won’t be relevant to you—sorry!] Until recently, this blog had a tagline that included “HOLD THE NOVEMBER US ELECTION BY MAIL.” So I thought I should warn readers that circumstances have changed in ways that have important practical […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><figure class="wp-block-image"><img alt="Image may contain: 1 person, eyeglasses and closeup" src="https://scontent.faus1-1.fna.fbcdn.net/v/t1.0-9/121693905_4150116585002989_4795318421355902630_n.jpg?_nc_cat=103&amp;_nc_sid=730e14&amp;_nc_ohc=BM8zCXAwmT4AX_q5CWD&amp;_nc_ht=scontent.faus1-1.fna&amp;oh=9fa3f3ccf4147209ce95a470d3dfef58&amp;oe=5FABBE94"/></figure>



<p><em>[If you’re not American, or you’re American but a masochist who enjoys the current nightmare, this post won’t be relevant to you—sorry!]</em></p>



<p>Until recently, this blog had a tagline that included “HOLD THE NOVEMBER US ELECTION BY MAIL.”  So I thought I should warn readers that circumstances have changed in ways that have important practical implications over the next few weeks.  It’s no longer that we <em>don’t know</em> whether Trump and Pence will acknowledge a <a href="https://projects.fivethirtyeight.com/2020-election-forecast/">likely loss</a>—rather, it’s that <em>we know they won’t</em>.  They were repeatedly asked; we all heard their answers.</p>



<p>That means that the <em>best</em> case, the ideal scenario, is already without precedent in the country’s 240-year history.  It’s a president who never congratulates the winner, who refuses to meet him or coordinate a transfer of power, who skips the inauguration, and who’s basically dragged from the White House on January 20, screaming to his supporters (and continuing to scream until his dying breath) that the election was faked.</p>



<p>As I said, that banana-republic outcome is now the <em>best</em> case.  But it’s also plausible that Trump simply declares himself the winner on election night, because the mail-in votes, urban votes, yet-to-be-counted votes, or any other votes that trend the wrong way are fake; social media and the Murdoch press amplify this fantasy; Trump calls on Republican-controlled state legislatures to set aside the “rigged” results and appoint their own slates of electors; the legislatures dutifully comply; and the Supreme Court A-OKs it all.  If you think none of that could happen, <a href="https://www.theatlantic.com/magazine/archive/2020/11/what-if-trump-refuses-concede/616424/?utm_source=facebook&amp;utm_medium=social&amp;utm_campaign=share&amp;fbclid=IwAR28w8ZJIZSlvqoOT-To3UWgMj4wVc9PkDA7Vbs60S_J7T0kOo1_RyVGe98">read this <em>Atlantic</em> article</a> from a few weeks ago, carefully to the end, and be more terrified than you’ve ever been in your life.  And don’t pretend that you know what would happen next.</p>



<p>I know, I know, I’m mentally ill, it’s Trump Derangement Syndrome, I see Nazis behind every corner just because they killed most of my relatives, a little global pandemic here and economic collapse there and riots and apocalyptic fires and resurgent fascism and I act as though it’s the whole world coming to an end.  A few months from now, after everything has gone swimmingly, this post will still be here and you can come back and tell me how crazy I was.  I accept that risk.</p>



<p>For now, though, the best chance to avert a catastrophe is for Trump not merely to lose, but <strong>lose in a landslide that’s already clear by election night</strong>.  Which means: as Michelle Obama <a href="https://www.vox.com/2020/8/18/21373177/michelle-obama-dnc-2020-speech-voting-post-office">advised</a> already in August, put on your mask, brave the virus, and vote in person if you can—<em>especially</em> if you live in a state that’s in play, and that won’t start tallying mail-in ballots till after election day.  If your state allows it, and if early votes will be counted by election night (check this!), vote early, when the lines are shorter.  That’s what Dana and I did this morning; Texas <a href="https://www.washingtonpost.com/outlook/2020/10/04/joe-biden-win-texas/">going blue</a> on election night would be one dramatic way to foreclose shenanigans.  If you can’t vote in person, or if your state counts mail-in ballots earlier, then vote by mail or drop-box, but <em>do it now</em>, so you have a chance to fix any problems well before Election Day.  (Note that, even in normal circumstances—which these aren’t—a substantial fraction of all mail-in ballots get rejected because of trivial errors.)  I welcome other tips in the comments, from the many readers more immersed in this stuff than I am.</p>



<p>And if this post helped spur you in any way, please say so in the comments.  It will improve my mood, thereby helping me finish my next post, which will be on the Continuum Hypothesis.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> It’s always fascinating to check my comments and see the missives from parallel universes, where Trump is a normal candidate who one might decide to vote for based on normal criteria, rather than what he himself has announced he is: a knife to the entire system that underlies such decisions.  For a view from <em>this</em> universe, see (e.g.) <a href="https://www.nature.com/articles/d41586-020-02852-x?utm_source=twitter&amp;utm_medium=social&amp;utm_content=organic&amp;utm_campaign=NGMT_USG_JC01_GL_Nature&amp;fbclid=IwAR1tLfLbZwXPd6U3a8BnqxvvMxo3SUDPWxhQccXYz9BU9_dL8x8deY3_dRc">today’s <em>Nature</em> editorial</a>.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Another Update:</span></strong> If it allays anyone’s fears, I was pleasantly surprised by the level of pandemic preparedness when Dana and I went to vote.  It was in a huge, cavernous gym on the UT campus, the lines were very short, masks and 6ft distancing were strictly enforced, and finger-coverings and hand sanitizer were offered to everyone.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Unrelated Update (10/16):</span></strong> For those who are interested, here’s a <a href="https://mattasher.com/2020/10/16/scott-aaronson-on-the-hunt-for-real-randomness/">new podcast with me and Matt Asher</a>, where we talk about the use of quantum mechanics (especially Bell inequality violations) to generate certified random numbers.</p></div>
    </content>
    <updated>2020-10-14T01:35:53Z</updated>
    <published>2020-10-14T01:35:53Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-10-16T15:33:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6616178737091923837</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6616178737091923837/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/hugh-woodin-kurt-godel-dwayne-rock.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6616178737091923837" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6616178737091923837" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/hugh-woodin-kurt-godel-dwayne-rock.html" rel="alternate" type="text/html"/>
    <title>Hugh Woodin, Kurt Godel, Dwayne `The Rock' Johnson, Robert De Niro, David Frum, Tom Selleck: Do I care what they think? Should I?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> MATH:</p><p>My last <a href="https://blog.computationalcomplexity.org/2020/10/revisiting-continuum-hypothesis.html">post</a> on CH mentioned that Hugh Woodin used to think NOT(CH) but now thinks CH. In both cases his reasons have some math content to them. Also, note that Hugh Woodin seems to believe that CH somehow HAS an answer. Kurt Godel also thought CH HAS an answer. It has been said that he could have announced  his result that CH is consistent by saying  L is THE model, and the problem is now solved. </p><p>Should we care what Hugh Woodin and Kurt Godel think about CH?</p><p>YES- they have both studied the issue A LOT. If you think CH should have an answer, then surely you would care what they think. </p><p>NO-  CH has no answer so there opinions are no better than mine. If you think CH does not have an answer then you might think this; however, I think you should still be at least INTERESTED in what people who have thought about the problem A LOT have to say, even if you will disagree with them.</p><p>But with MATH there are people who clearly know more than you on topics you care about, so it is worth hearing what they have to say. </p><p>POLITICS:</p><p>Recently Dwayne THE ROCK Johnson (by Wikipedia: actor, producer, businessman, and former professional wrestler) ENDORSED Joe Biden. Should we care about his opinion? Maybe, if wrestling fans and former pro wrestler tend to be Republicans, so this may indicate a shift. I do not know if this is the case. </p><p>Robert De Niro was in favor of impeaching Donald Trump. He also said that Trump was like a Gangster. He would know because he was in the movie GOODFELLOWS and later THE IRISHMAN (about Jimmy Hoffa). To be fair I do not think he said that is how he would know. Even so, I don't think I care what he thinks, unless he has some specialized knowledge I do not know about. </p><p>David Frum is a republican who had a break with the party NOT over Donald Trump, but over Obamacare- which you may recall was originally a CONSERVATIVE response to Hillarycare by the Heritage Foundation.  He has a good article on this <a href="https://www.theatlantic.com/politics/archive/2017/03/the-republican-waterloo/520833/">here</a>. Because he is an intelligent  republican in favor of Obamacare (or some version of it) he is worth listening to.</p><p>In POLITICS its trickier- who is worth listening to and why. For all I know, THE ROCK has made a detailed study of the Republican and Democratic platforms (actually this cannot be true since the Republicans did not have a platform this time). </p><p>COMMERCIALS:</p><p>Tom Selleck (Actor-Magnum PI a while back, Blue Bloods now)  does commercials for reverse mortgages. A while back I asked a group of people WHY he is doing them. Here were some answers and reactions</p><p>a) He needs the money. Not likely, he seems to have done well and does not seem to have the kind of bad habits (e.g., drugs) that need money. Maybe he has expensive tastes (my only expensive tastes is in fine European Kit Kat bars--- which actually are not that expensive). </p><p>b) He likes doing commercials. Maybe.</p><p>c) He believes in the product. At this, everyone cracked up in laughter.</p><p>This raises a more general point: Why does ANYONE believe ANY commercial since we KNOW the actor is being PAID to say it. I ask non rhetorically as always. </p></div>
    </content>
    <updated>2020-10-12T17:49:00Z</updated>
    <published>2020-10-12T17:49:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-22T11:46:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5005</id>
    <link href="https://www.scottaaronson.com/blog/?p=5005" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5005#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5005" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">My second podcast with Lex Fridman</title>
    <summary xml:lang="en-US">Here it is—enjoy! (I strongly recommend listening at 2x speed.) We recorded it a month ago—outdoors (for obvious covid reasons), on a covered balcony in Austin, as it drizzled all around us. Topics included: Whether the universe is a simulation Eugene Goostman, GPT-3, the Turing Test, and consciousness Why I disagree with Integrated Information Theory […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://www.youtube.com/watch?v=nAMjv0NAESM&amp;list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4">Here it is—enjoy!</a>  (I strongly recommend listening at 2x speed.)</p>



<p>We recorded it a month ago—outdoors (for obvious covid reasons), on a covered balcony in Austin, as it drizzled all around us.  Topics included:</p>



<ul><li>Whether the universe is a simulation</li><li>Eugene Goostman, GPT-3, the Turing Test, and consciousness</li><li>Why I disagree with Integrated Information Theory</li><li>Why I disagree with Penrose’s ideas about physics and the mind</li><li>Intro to complexity theory, including P, NP, PSPACE, BQP, and SZK</li><li>The US’s catastrophic failure on covid</li><li>The importance of the election</li><li>My objections to cancel culture</li><li>The role of love in my life (!)</li></ul>



<p>Thanks so much to Lex for his characteristically probing questions, apologies as always for my verbal tics, and here’s our <a href="https://lexfridman.com/scott-aaronson-2/">first podcast</a> for those who missed that one.</p></div>
    </content>
    <updated>2020-10-12T14:38:07Z</updated>
    <published>2020-10-12T14:38:07Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Metaphysical Spouting"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-10-16T15:33:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17669</id>
    <link href="https://rjlipton.wordpress.com/2020/10/11/are-black-holes-necessary/" rel="alternate" type="text/html"/>
    <title>Are Black Holes Necessary?</title>
    <summary>Our congratulations on the 2020 Nobel Prize in Physics Composite crop of src1, src2 Roger Penrose, Reinhard Genzel, and Andrea Ghez have won the 2020 Nobel Prize in Physics. The prize is divided half to Penrose for theoretical work and half to Genzel and Ghez for finding a convincing and appreciably large practical example. Today […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc">
<em>Our congratulations on the 2020 Nobel Prize in Physics</em>
<font color="#000000">


</font></font></p><table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/10/penrosegenzelghez.jpg"><img alt="" class="alignright wp-image-17675" height="115" src="https://rjlipton.files.wordpress.com/2020/10/penrosegenzelghez.jpg?w=300" width="320"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite crop of <a href="https://www.bbc.com/news/science-environment-54439150">src1</a>, <a href="https://theconversation.com/nobel-prize-how-penrose-genzel-and-ghez-helped-put-black-holes-at-the-centre-of-modern-astrophysics-147613">src2</a></font></td>
</tr>
</tbody>
</table><font color="#0044cc"><font color="#000000">



<p>
Roger Penrose, Reinhard Genzel, and Andrea Ghez have won the 2020 Nobel Prize in Physics. The prize is divided half to Penrose for theoretical work and half to Genzel and Ghez for finding a convincing and appreciably large practical example.

</p><p>
Today we congratulate the winners and give further musings on the nature of knowledge and the role of theory.

</p><p>
The physics Nobel has always had the rule that it cannot be for a theory alone, no matter how beautiful and how many mathematical discoveries follow from its development. Stephen Hawking’s theory of black-hole <a href="https://en.wikipedia.org/wiki/Hawking_radiation">radiation</a> is almost universally accepted, despite its association with <a href="https://en.wikipedia.org/wiki/Black_hole_information_paradox">paradox</a>, yet it was said that only an empirical confirmation such as mini-black holes being discovered to explode in an accelerator core would have brought it a Nobel. The official citation to Sir Roger says that his prize is:

</p><p>

</p><blockquote><b> </b> <em> “for the discovery that black hole formation is a robust prediction of the general theory of relativity.” </em>
</blockquote>




<p>
What is a “robust” prediction? The word strikes us as having overtones of <em>necessity</em>. Necessary knowledge is the kind we deal with in mathematics. The citation to Genzel and Ghez stays on empirical grounds:

</p><p>

</p><blockquote><b> </b> <em> “for the discovery of a supermassive compact object at the centre of our galaxy.” </em>
</blockquote>


<p>



</p><p>
The “object” <em>must</em> be a black hole—given relativity and its observed gravitational effects, it cannot be otherwise. Among many possible witnesses for the reality of black holes—one being the evident origin of the gravitational waves whose <a href="https://rjlipton.wordpress.com/2016/02/16/waves-hazards-and-guesses/">detection</a> brought the 2017 Nobel—the centers of galaxies are hefty examples. The combination of these citations opens several threads we’d like to discuss.

</p><p>



</p><p>

</p><h2> The Proof Horizon of a Black Hole </h2>


<p>



</p><p>
Dick and I are old enough to remember when black holes had the status of conjecture. One of my childhood astronomy books stated that the <a href="https://en.wikipedia.org/wiki/Cygnus_X-1">Cygnus X-1</a> X-ray source was the best known candidate for a black hole. In 1974, Hawking bet Kip Thorne that it was not a black hole. The bet lasted until 1990, when Hawking conceded. He wrote the following in his famous <a href="https://en.wikipedia.org/wiki/A_Brief_History_of_Time">book</a>, <em>A Brief History of Time</em>:

</p><p>

</p><blockquote><b> </b> <em> This was a form of insurance policy for me. I have done a lot of work on black holes, and it would all be wasted if it turned out that black holes do not exist. But in that case, I would have the consolation of winning my bet. … When we made the bet in 1975, we were 80% certain that Cygnus X-1 was a black hole. By now [1988], I would say that we are about 95% certain, but the bet has yet to be settled. </em>
</blockquote>


<p>



</p><p>
In the 1980s, I was a student and then postdoc in Penrose’s department, so I was imbued with the ambience of black holes and never had a thought about doubting their existence. I even once spent an hour with John Wheeler, who coined the term “black hole,” when Penrose delegated me to accompany Wheeler to Oxford’s train station for his return to London. But it seems from the record that the progression to regarding black holes as proven entities was as gradual as many argue the act of crossing a large black hole’s event horizon to be. Although the existence of a central black hole from data emanating from Sagittarius had been proposed at least as far back as 1971, the work by Ghez and then Genzel cited for their prize began in 1995. The official <a href="https://www.nobelprize.org/prizes/physics/2002/press-release/">announcement</a> for Riccardo Giacconi’s share of the 2002 physics Nobel stated:

</p><p>

</p><blockquote><b> </b> <em> “He also detected sources of X-rays that most astronomers now consider to contain black holes.” </em>
</blockquote>


<p>



</p><p>
This speaks lingering doubt at least about <em>where</em> black holes might be judged to exist, if not their existence at all.

</p><p>
However their time of confirmation might be pinpointed, it is the past five years that have given by far the greatest flood of evidence, including the first visual <a href="https://www.jpl.nasa.gov/edu/news/2019/4/19/how-scientists-captured-the-first-image-of-a-black-hole/">image</a> of a black hole last year. The fact of their presence in our universe is undeniable. But <em>necessity</em> is a separate matter, and with Penrose this goes back to 1964.

</p><p>



</p><p>

</p><h2> Relativity and Necessity </h2>


<p>



</p><p>
We have <a href="https://rjlipton.wordpress.com/2011/10/31/an-interview-with-kurt-gdel/">mentioned</a> Kurt Gödel’s solution to the equations of general relativity (GR) in which time travel is possible. This does not mean that time travel must be possible, or that it is possible in our universe. A “solution” to GR is more like a <em>model</em> in logic: it may satisfy a theory’s axioms but have other properties that are contingent (unless the theory is <em>categorical</em>, meaning that all of its models are isomorphic). Gödel’s model has a negative value for Einstein’s <a href="https://en.wikipedia.org/wiki/Cosmological_constant">cosmological constant</a>; the 2011 physics Nobel went to the discovery that in our universe the constant has a tiny <a href="https://rjlipton.wordpress.com/2011/10/12/empirical-humility/">positive</a> value. GR also allows solutions in which some particles (called <em>tachyons</em>) travel faster than light.

</p><p>
That GR has solutions allowing black holes had been known from its infancy in work by Karl Schwarzschild and Johannes Droste. There are also solutions without black holes; a universe with no mass is legal in GR in <a href="https://en.wikipedia.org/wiki/Vacuum_solution_(general_relativity)">many ways</a> besides the case of special relativity. Penrose took the opposite tack, of giving minimal conditions under which black holes are <em>necessary</em>. Following this <a href="https://theconversation.com/nobel-prize-how-penrose-genzel-and-ghez-helped-put-black-holes-at-the-centre-of-modern-astrophysics-147613">article</a>, we list them informally as follows:

</p><p>


</p><ol> 
<li>
Sufficiently large concentrations of mass exerting gravity exist. 
</li><li>
Gravity always attracts, never repels. 
</li><li>
No physical effect can travel faster than light. 
</li><li>
Gravity determines how light bends and moves. 
</li><li>
The space-time manifold is metrically complete. 
</li></ol>



<p>
Penrose showed that any system obeying these properties and evolving in accordance with GR must develop black holes. He showed this without any symmetry assumptions on the system. Thus he derived black holes as a prediction with the force of a theorem derived from minimal axioms.

</p><p>
His 1965 <a href="http://quantum-gravitation.de/media/2d2cde3ec9c38fffffff80d0fffffff1.pdf">paper</a> actually used a proof by contradiction. He derived five properties needed in order for the system to avoid forming a singularity. Then he showed they are mutually inconsistent—a proof by contradiction. Here is the crux of his paper:

</p><p>




</p><p>
</p><table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/10/penrosediagram1965.jpg"><img alt="" class="aligncenter wp-image-17673" height="442" src="https://rjlipton.files.wordpress.com/2020/10/penrosediagram1965.jpg?w=600" width="600"/></a>
<p>
</p></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Snip from <a href="http://quantum-gravitation.de/media/2d2cde3ec9c38fffffff80d0fffffff1.pdf">paper</a> ]</font>
</td>
</tr>
</tbody></table>



<p>
In the diagram, time flows up. The point in a nutshell—a very tight nutshell—is that once a surface flows inside the cylinder at the Schwarzschild radius then light and any other motion from it can go only inward toward a singularity. The analysis is possible without the kind of symmetry assumption that had been used to tame the algebraic complexity of the equations of GR. The metric completeness mandates a singularity apart from any symmetries; a periodic equilibrium is ruled out by analysis of Cauchy surfaces.

</p><p>



</p><p>

</p><h2> Necessary For Us? </h2>


<p>



</p><p>
Like Richard Feynman’s famous diagrams for quantum field theory, Penrose developed his diagrams as tools for shortcutting the vicissitudes of GR. We could devote entire other posts to his famous <a href="https://en.wikipedia.org/wiki/Penrose_tiling">tiles</a> and <a href="https://en.wikipedia.org/wiki/Penrose_triangle">triangle</a> and other combinatorial inventions. His tools enable quantifying black-hole formation from observations in our universe.

</p><p>
The question of <em>necessity</em>, however, pertains to other possible universes. Let us take for granted that GR and quantum theory are facets of a physical theory that governs the entire cosmos—the long-sought “theory of everything”—and let us also admit the contention of inflationary theorists that multiple universes are a necessary consequence of any inflation theory. The question remains, <em>are black holes necessary in those universes?</em>

</p><p>
It is possible that those universes might not satisfy axiom 1 above, or might have enough complexity for existence of black holes but not large-scale formation of them. The question then becomes whether black holes must exist in any universe rich enough for sentient life forms such as ourselves to develop. This is a branch of the <a href="https://en.wikipedia.org/wiki/Anthropic_principle">anthropic principle</a>.

</p><p>
Lee Smolin <a href="https://en.wikipedia.org/wiki/Cosmological_natural_selection">proposed</a> a mechanism via which black holes engender new universes and so propagate the complexity needed for their large-scale formation. Since complexity also attends the development of sentient life forms, this would place our human existence in the wake of consequence, as opposed to the direction of logic when reasoning by the anthropic principle.

</p><p>



</p><p>

</p><h2> A Little More About Science </h2>


<p>



</p><p>
The 2020 Nobel Prize in Chemistry was awarded this week to Jennifer Doudna and Emmanuelle Charpentier for their lead roles in developing the <a href="https://en.wikipedia.org/wiki/CRISPR">CRISPR</a> gene-editing technology, specifically around the protein <a href="https://en.wikipedia.org/wiki/Cas9">Cas9</a>. 

</p><p>
We argue that two more different types of results cannot be found: 

</p><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> Penrose shows that black holes and general relativity are connected, which is a math result. We still cannot create black holes in a lab to experiment with—or maybe we could but should be very afraid of going anywhere near doing so. It was not clear that there could ever be a real application of this result.

</p><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> Charpentier and Doudna discover that an existing genetic mechanism could be used to edit genetic material. Clearly this can and was experimented on in labs. Also clear that there are applications of this result. Actually it is now a standard tool used in countless labs. There even are patent battles over the method.

</p><p>
We like the fact that Nobels are given for such diverse type of research. It is not just that one is for astrophysics and one for chemistry. It is that Nobels can be given for very different types of research. We think this is important.

</p><p>
But wait. These results do have something in common, something that sets them apart from any research we can do in complexity theory. Both operate like this:

</p><p>

</p><blockquote><b> </b> <em> 	Observe something important from nature. Something that is there independent of us. Then in Penrose’s case explain why it is true. Then in Charpentier and Doudna’s case, use it to solve some important problems. </em>
</blockquote>


<p>



</p><p>
We wonder if anything like this could be done in our research world—say in complexity theory?

</p><p>



</p><p>

</p><h2> Open Problems </h2>


<p>



</p><p>
Besides our congratulations to all those mentioned in this post, Ken expresses special thanks to Sir Roger among other Oxford Mathematical Institute fellows for the kindness recorded <a href="https://rjlipton.wordpress.com/2011/03/09/tex-is-great-what-is-tex/#comment-11172">here</a>.

</p>

<p>
[changed note about massless universe]</p></font></font>



<p/></div>
    </content>
    <updated>2020-10-11T19:19:58Z</updated>
    <published>2020-10-11T19:19:58Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="Proofs"/>
    <category term="Andrea Ghez"/>
    <category term="black hole"/>
    <category term="Chemistry"/>
    <category term="Emmanuelle Charpentier"/>
    <category term="Jennifer Doudna"/>
    <category term="John Wheeler"/>
    <category term="knowledge"/>
    <category term="Lee Smolin"/>
    <category term="necessity"/>
    <category term="Nobel Prize"/>
    <category term="Physics"/>
    <category term="progress in knowledge"/>
    <category term="Reinhard Genzel"/>
    <category term="relativity"/>
    <category term="Riccardo Giacconi"/>
    <category term="Roger Penrose"/>
    <category term="science"/>
    <category term="scientific proof"/>
    <category term="Stephen Hawking"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-10-22T23:31:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/154</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/154" rel="alternate" type="text/html"/>
    <title>TR20-154 |  A Structural Theorem for Local Algorithms with Applications to Coding, Testing, and Privacy | 

	Marcel Dall&amp;#39;Agnol, 

	Tom Gur, 

	Oded Lachish</title>
    <summary>We prove a general structural theorem for a wide family of local algorithms, which includes property testers, local decoders, and PCPs of proximity. Namely, we show that the structure of every algorithm that makes $q$ adaptive queries and satisfies a natural robustness condition admits a sample-based algorithm with $n^{1- 1/O(q^2 \log^2 q)}$ sample complexity, following the definition of Goldreich and Ron (TOCT 2016). We prove that this transformation is nearly optimal. Our theorem also admits a scheme for constructing privacy-preserving local algorithms. 

Using the unified view that our structural theorem provides, we obtain results regarding various types of local algorithms, including the following.


- We strengthen the state-of-the-art lower bound for relaxed locally decodable codes, obtaining an exponential improvement on the dependency in query complexity; this resolves an open problem raised by Gur and Lachish (SODA 2020).
- We show that any (constant-query) testable property admits a sample-based tester with sublinear sample complexity; this resolves a problem left open in a work of Fischer, Lachish, and Vasudev (FOCS 2015) by extending their main result to adaptive testers.
- We prove that the known separation between proofs of proximity and testers is essentially maximal; this resolves a problem left open by Gur and Rothblum (ECCC 2013, Computational Complexity 2018) regarding sublinear-time delegation of computation.

Our techniques strongly rely on relaxed sunflower lemmas and the Hajnal–Szemerédi theorem.</summary>
    <updated>2020-10-10T11:21:31Z</updated>
    <published>2020-10-10T11:21:31Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-22T23:31:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/153</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/153" rel="alternate" type="text/html"/>
    <title>TR20-153 |  Total Functions in the Polynomial Hierarchy | 

	Robert Kleinberg, 

	Daniel Mitropolsky, 

	Christos Papadimitriou</title>
    <summary>We identify several genres of search problems beyond NP for which existence of solutions is guaranteed.  One class that seems especially rich in such problems is PEPP (for "polynomial empty pigeonhole principle"), which includes problems related to existence theorems proved through the union bound, such as finding a bit string that is far from all codewords, finding an explicit rigid matrix, as well as a problem we call Complexity, capturing Complexity Theory's quest.  When the union bound is generous, in that solutions constitute at least a polynomial fraction of the domain, we have a family of seemingly weaker classes $\alpha$-PEPP, which are inside FP}$^{\text{NP}}|$poly.  Higher in the hierarchy, we identify the constructive version of the Sauer-Shelah lemma and the appropriate generalization of PPP that contains it.  The resulting total function hierarchy turns out to be more stable than the polynomial hierarchy: it is known that, under oracles, total functions within FNP may be easy, but total functions a level higher may still be harder than FP$^{\text{NP}}$.</summary>
    <updated>2020-10-08T22:31:10Z</updated>
    <published>2020-10-08T22:31:10Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-22T23:31:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/152</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/152" rel="alternate" type="text/html"/>
    <title>TR20-152 |  Variants of the Determinant polynomial and VP-completeness | 

	Prasad Chaugule, 

	Nutan Limaye, 

	Shourya Pandey</title>
    <summary>The determinant is a canonical VBP-complete polynomial in the algebraic complexity setting. In this work, we introduce two variants of the determinant polynomial which we call $StackDet_n(X)$ and $CountDet_n(X)$ and show that they are VP and VNP complete respectively under $p$-projections. The definitions of the polynomials are inspired by a combinatorial characterisation of the determinant developed by Mahajan and Vinay (SODA 1997). We extend the combinatorial object in their work, namely clow sequences, by introducing additional edge labels on the edges of the underlying graph. The idea of using edge labels is inspired by the work of Mengel (MFCS 2013).</summary>
    <updated>2020-10-08T22:27:25Z</updated>
    <published>2020-10-08T22:27:25Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-22T23:31:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=1380</id>
    <link href="https://francisbach.com/hermite-polynomials/" rel="alternate" type="text/html"/>
    <title>Polynomial magic III : Hermite polynomials</title>
    <summary>After two blog posts earlier this year on Chebyshev and Jacobi polynomials, I am coming back to orthogonal polynomials, with Hermite polynomials. This time, in terms of applications to machine learning, no acceleration, but some interesting closed-form expansions in positive-definite kernel methods. Definition and first properties There are many equivalent ways to define Hermite polynomials....</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">After two blog posts earlier this year on <a href="https://francisbach.com/chebyshev-polynomials/">Chebyshev</a> and <a href="https://francisbach.com/jacobi-polynomials/">Jacobi</a> polynomials, I am coming back to orthogonal polynomials, with Hermite polynomials. </p>



<p class="justify-text">This time, in terms of applications to machine learning, no acceleration, but some interesting closed-form expansions in positive-definite kernel methods. </p>



<h2>Definition and first properties</h2>



<p class="justify-text">There are many equivalent ways to define Hermite polynomials. A natural one is through the so-called <a href="https://en.wikipedia.org/wiki/Rodrigues%27_formula">Rodrigues’ formula</a>: $$H_k(x) = (-1)^k e^{x^2} \frac{d^k}{d x^k}\big[ e^{-x^2} \big],$$ from which we can deduce \(H_0(x) = 1\), \(H_1(x) =\   – e^{x^2} \big[ -2x e^{-x^2} \big] = 2x\), \(H_2(x) = e^{x^2} \big[ (-2x)^2e^{-x^2} -2 e^{-x^2}  \big] =  4x^2 – 2\), etc.</p>



<p class="justify-text">Other simple properties which are consequences of the definition (and can be shown by recursion) are that \(H_k\) is a polynomial of degree \(k\), with the same parity as \(k\), and with a leading coefficient equal to \(2^k\).</p>



<p class="justify-text"><strong>Orthogonality for Gaussian distribution.</strong> Using integration by parts, one can show (see end of the post) that for \(k \neq \ell\), we have $$\int_{-\infty}^{+\infty}  \!\!\!H_k(x) H_\ell(x) e^{-x^2} dx =0, $$ and that for \(k=\ell\), we have $$\int_{-\infty}^{+\infty} \!\!\! H_k(x)^2 e^{-x^2}dx = \sqrt{\pi} 2^k k!.$$ </p>



<p class="justify-text">In other words, the Hermite polynomials are orthogonal for the Gaussian distribution with mean \(0\) and variance \(\frac{1}{2}\). Yet in other words, defining the <em>Hermite functions</em> as \( \displaystyle \psi_k(x) = (\sqrt{\pi} 2^k k!)^{-1/2} H_k(x) e^{-x^2/2}\), we obtain an orthonormal basis of \(L_2(dx)\). As illustrated below, the Hermite functions, as the index \(k\) increases, have an increasing “support” (the support is always the entire real line, but most of the mass is concentrated in centered balls of increasing sizes, essentially at \(\sqrt{k}\)) and, like cosines and sines, an increasingly oscillatory behavior.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4579" height="305" src="https://francisbach.com/wp-content/uploads/2020/08/hermite.gif" width="349"/>Plot of Hermite functions \(\psi_k(x) = (\sqrt{\pi} 2^k k!)^{-1/2} H_k(x) e^{-x^2/2}\), from \(k=0\) to \(k=20\).</figure></div>



<p class="justify-text">Among such orthonormal bases, the Hermite functions happen to be diagonalizing the Fourier tranform operator.  In other words, the Fourier transform of \(\psi_k\) (for the definition making it an isometry of \(L_2(dx)\)) is equal to $$ \mathcal{F}(\psi_k)(\omega)  =  \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{+\infty} \psi_k(x) e^{- i \omega x} dx = (-i)^k \psi_k(\omega).$$ (note that the eigenvalues are all of unit modulus as we have an isometry). See a proof at the end of the post. I am not aware of any applications of this property in machine learning or statistics (but there are probably some).</p>



<p class="justify-text"><strong>Recurrence.</strong> In order to compute Hermite polynomials, the following recurrence relation is the most useful $$ H_{k+1}(x) = 2x H_k(x) \ – 2k H_{k-1}(x). \tag{1}$$  Such recursions are always available for orthogonal polynomials (see [4]), but it takes here a particularly simple form (see a proof at the end of the post).</p>



<p class="justify-text"><strong>Generating function.</strong> The following property is central in many proofs of properties of Hermite polynomials: for all \(t \in \mathbb{R}\), we have $$\sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) =e^{ 2xt \ – \ t^2}, \tag{2}$$ with a proof at the end of the post based on the residue theorem.</p>



<h2>Further (less standard) properties</h2>



<p class="justify-text">For the later developments, we need other properties which are less standard (there are many other interesting properties, which are not useful for this post, see <a href="https://en.wikipedia.org/wiki/Hermite_polynomials">here</a>).</p>



<p class="justify-text"><strong>Mehler formula. </strong>For \(|\rho| &lt; 1\), it states: $$ \exp \Big( – \frac{\rho}{1- \rho^2} (x-y)^2\Big) = \sqrt{1-\rho^2} \sum_{k=0}^\infty \frac{\rho^k}{2^k k!} H_k(x) H_k(y) \exp \Big( – \frac{\rho}{1+\rho} (x^2 + y^2) \Big).$$ The proof is significantly more involved; see [<a href="https://academic.oup.com/jlms/article-pdf/s1-8/3/194/2363185/s1-8-3-194.pdf">1</a>] for details (with a great last sentence: “Prof. Hardy tells me that he has not seen his proof in print, though the inevitability of the successive steps makes him think that it is unlikely to be new”). Note that we will in fact obtain a new proof from the relationship with kernel methods (see below).</p>



<p class="justify-text"><strong>Expectation for Gaussian distributions. </strong>We will need this property for \(|\rho|&lt;1\) (see proof at the end of the post), which corresponds to the expectation of \(H_k(x)\) for \(x\) distributed as a non-centered Gaussian distribution: $$\int_{-\infty}^\infty H_k(x) \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big)dx= \sqrt{\pi} \rho^k \sqrt{1-\rho^2} H_k (y). \tag{3}$$</p>



<p class="justify-text">Given the relationship with the Gaussian distribution, it is no surprise that Hermite polynomials pop up whenever Gaussians are used, as distributions or kernels. Before looking into it, let’s first give a brief review of kernel methods.</p>



<h2>From positive-definite kernel to Hilbert spaces</h2>



<p class="justify-text">Given a prediction problem with inputs in a set \(\mathcal{X}\), a traditional way of parameterizing real-valued functions on \(\mathcal{X}\) is to use <em>positive-definite kernels</em>.</p>



<p class="justify-text">A positive-definite kernel is a function \(K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\) such that for all sets \(\{x_1,\dots,x_n\}\) of \(n\) elements of \(\mathcal{X}\), the “kernel matrix” in \(\mathbb{R}^{n \times n}\) composed of pairwise evaluations is symmetric positive semi-definite. This property happens to be equivalent to the existence of a Hilbert feature space \(\mathcal{H}\) and a feature map \(\varphi: \mathcal{X} \to \mathcal{H}\) such that $$K(x,x’) = \langle \varphi(x), \varphi(x’) \rangle_{\mathcal{H}},$$ with an elegant constructive proof [<a href="https://www.ams.org/journals/tran/1950-068-03/S0002-9947-1950-0051437-7/S0002-9947-1950-0051437-7.pdf">15</a>].</p>



<p class="justify-text">This allows to define the space of linear functions on the features, that is, functions of the form $$f(x) = \langle f, \varphi(x) \rangle_{\mathcal{H}},$$ for \(f \in \mathcal{H}\). </p>



<p class="justify-text">This space is often called the <a class="" href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space</a> (RKHS) associated to the kernel \(K\) (we can prove that it is indeed uniquely defined). In such a space, we can also define the squared norm of the function \(f\), namely \(\| f\|_{\mathcal{H}}^2\), which can be seen as a specific regularization term in kernel methods.</p>



<p class="justify-text">The space satisfies the so-called reproducing property (hence its name): \(f(x) = \langle f, K(\cdot,x) \rangle_{\mathcal{H}}\). In other words, the feature \(\varphi(x)\) is the kernel function evaluated at \(x\), that is,  \(\varphi(x) = K(\cdot,x)\). These spaces have been a source of many developments in statistics [5] and machine learning [6, 7].</p>



<p class="justify-text"><strong>Orthonormal basis.</strong> A difficulty in working with infinite-dimensional Hilbert spaces of functions is that it is sometimes hard to understand what functions are actually considered. One simple way to enhance understanding of the regularization property is to have an orthonormal basis (in very much the same way as the Fourier basis), as we can then identify \(\mathcal{H}\) to the space of squared-integrable sequences.</p>



<p class="justify-text">For kernel-based Hilbert spaces, if we have an orthonormal basis \((g_k)_{k \geqslant 0}\) of the Hilbert space \(\mathcal{H}\), then, by decomposing \(\varphi(x)\) in the basis, we have $$\varphi(x) = \sum_{k =0}^\infty \langle \varphi(x), g_k \rangle_\mathcal{H} g_k,$$ we get $$K(x,y) = \langle \varphi(y), \varphi(x) \rangle = \sum_{k =0}^\infty \langle \varphi(x), g_k \rangle_\mathcal{H} \langle  \varphi(y), g_k \rangle_\mathcal{H} =\sum_{k=0}^\infty g_k(x) g_k(y), \tag{4}$$ that is, we have an expansion of the kernel as an infinite sum (note here, that we ignore summability issues).</p>



<p class="justify-text">Among orthonormal bases, some are more interesting than others. The ones composed of eigenfunctions for particular operators are really more interesting, in particular for the covariance operator that we now present, and their use in statistical learning theory.</p>



<h2>Analyzing ridge regression through covariance operators</h2>



<p class="justify-text">The most classical problem where regularization by RKHS norms occurs is <em>ridge regression</em>, where, given some observations \((x_1,y_1),\dots,(x_n,y_n) \in \mathcal{X} \times \mathbb{R}\), one minimizes with respect to \(f \in \mathcal{H}\): $$ \frac{1}{n} \sum_{i=1}^n \big( y_i \ – \langle f, \varphi(x_i) \rangle_{\mathcal{H}} \big)^2 +  \lambda \| f\|_{\mathcal{H}}^2.$$</p>



<p class="justify-text">In finite dimensions, the convergence properties are characterized by the (non-centered) covariance matrix \(\Sigma = \mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big]\), where the expectation is taken with respect to the underlying distribution of the observations \(x_1,\dots,x_n\) (which are assumed independently and identically distributed for simplicity). If \(\mathcal{H} = \mathbb{R}^d\), then \(\Sigma\) is a \(d \times d\) matrix. </p>



<p class="justify-text">For infinite-dimensional \(\mathcal{H}\), the same expression \(\Sigma = \mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big]\) defines a linear <em>operator</em> from \(\mathcal{H}\) to  \(\mathcal{H}\), so that for \(f,g \in \mathcal{H}\), we have $$\langle f, \Sigma g \rangle_{\mathcal{H}} = \mathbb{E} \big[ \langle f, \varphi(x)\rangle_{\mathcal{H}}\langle g, \varphi(x)\rangle_{\mathcal{H}}\big] = \mathbb{E} \big[ f(x) g(x) \big].$$</p>



<p class="justify-text">The generalization property of ridge regression has been thoroughly studied (see, e.g., [8, 9]), and if there exists \(f_\ast \in \mathcal{H}\) such that \(y_i = \langle f_\ast, \varphi(x_i) \rangle + \varepsilon_i\) for a noise \(\varepsilon_i\) which is independent of \(x_i\), with zero mean and variance equal to \(\sigma^2\), then the expected error on unseen data is asymptotically upper-bounded by $$\sigma^2 + \lambda \| f_\ast\|_{\mathcal{H}}^2 + \frac{\sigma^2}{n} {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big].$$ The first term \(\sigma^2\) is the best possible expected performance, the term \(\lambda \| f_\ast\|_{\mathcal{H}}^2\) is usually referred to as the <em>bias</em> term and characterizes the bias introduced by regularizing towards zero, while the third term \(\frac{\sigma^2}{n} {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]\) is the <em>variance</em> term, which characterizes the loss in performance due to the observation of only \(n\) observations.</p>



<p class="justify-text">The quantity \({\rm df}(\lambda) = {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]\) is often referred to as the degrees of freedom [10]. When \(\lambda\) tends to infinity, then \({\rm df}(\lambda)\) tends to zero; when \(\lambda\) tends to zero, then \({\rm df}(\lambda)\) tends to the number of non-zero eigenvalues of \(\Sigma\). Thus, in finite dimension, this typically leads to the underlying dimension. Given the usual variance term in \(\sigma^2 \frac{d}{n}\) for ordinary least-squares with \(d\)-dimensional features, \({\rm df}(\lambda)\) is often seen as an implicit number of parameters for kernel ridge regression.</p>



<p class="justify-text">In infinite dimensions, under mild assumptions, there are infinitely many eigenvalues for \(\Sigma\), which form a decreasing sequence \((\lambda_i)_{i \geqslant 0}\) that tends to zero (and is summable, with a sum equal to the trace of \(\Sigma\)). The rate of such a decay is key to understanding the generalization capabilities of kernel methods. With the following classical types of decays:</p>



<ul class="justify-text"><li><em>Polynomial decays</em>: If \(\lambda_i \leqslant \frac{C}{(i+1)^{\alpha}}\) for \(\alpha &gt; 1\), then one can upper bound the sum by an integral as $$ {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big] = \sum_{i=0}^\infty \frac{\lambda_i}{\lambda_i + \lambda} \leqslant \sum_{i=1}^\infty \frac{1}{1  + \lambda i^\alpha / C} \leqslant \int_0^\infty \frac{1}{1+\lambda t^\alpha / C} dt.$$ With the change of variable \(u = \lambda t^\alpha / C\), we get that \({\rm df}(\lambda) = O(\lambda^{-\alpha})\). We can then balance bias and variance with \(\lambda \sim n^{-\alpha/(\alpha+1)}\) and an excess risk proportional to \(n^{-\alpha/(\alpha+1)}\). This type of decay is typical of <a href="https://en.wikipedia.org/wiki/Sobolev_space">Sobolev spaces</a>.</li><li><em>Exponential decays</em>: If \(\lambda_i \leqslant {C}e^{-\alpha i}\), for some \(\alpha &gt;0\), we have $$ {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]  \leqslant \sum_{i=0}^\infty \frac{{C}e^{-\alpha i}}{  \lambda + {C}e^{-\alpha i}} \leqslant \int_{0}^\infty \frac{{C}e^{-\alpha t}}{ \lambda + {C}e^{-\alpha t}}dt.$$ With the change of variable \(u = e^{-\alpha t}\), we get an upper bound $$\int_{0}^1 \frac{C}{\alpha}\frac{1}{ \lambda + {C}u}du = \frac{1}{\alpha}\big[ \log(\lambda + C) \ – \log (\lambda) \big] = \frac{1}{\alpha} \log \big( 1+\frac{C}{\lambda} \big).$$ We can then balance bias and variance with \(\lambda \sim 1/n \) and an excess risk proportional to \((\log n) / n \), which is very close to the usual parametric (finite-dimensional) rate in \(O(1/n)\). We will see an example of this phenomenon for the Gaussian kernel.</li></ul>



<p class="justify-text">In order to analyze the generalization capabilities, we consider a measure \(d \mu\) on \(\mathcal{X}\), and the following (non-centered) <em>covariance operator</em> defined above as $$\mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big],$$ which is now an self-adjoint operator from \(\mathcal{H}\) to \(\mathcal{H}\) with a finite trace. The traditional empirical estimator \(\hat\Sigma = \frac{1}{n} \sum_{i=1}^n \varphi(x_i) \otimes \varphi(x_i)\), whose eigenvalues are the same as the eigenvalues of \(1/n\) times the \(n \times n\) kernel matrix of pairwise kernel evaluations (see simulation below).</p>



<p class="justify-text"><strong>Characterizing eigenfunctions.</strong> If \((g_k)\) is the eigenbasis associated to the eigenfunctions of \(\Sigma\), then it has to be an orthogonal family that span the entire space \(\mathcal{H}\) and such that \(\Sigma g_k = \lambda_k g_k\). Applying it to \(\varphi(y) = K(\cdot,y)\), we get $$ \langle K(\cdot,y), \Sigma g_k \rangle_{\mathcal{H}} = \mathbb{E} \big[ K(x,y) g_k(x) \big] = \lambda_k \langle g_k, \varphi(y)\rangle_\mathcal{H} =  \lambda_k g_k(y),$$ which implies that the functions also have to be eigenfunctions of the self-adjoint so-called <em>integral operator</em> \(T\) defined on \(L_2(d\mu)\) as \(T f(y) = \int_{\mathcal{X}} K(x,y) f(y) d\mu(y)\). Below, we will check this property. Note that this other notion of integral operator (defined on \(L_2(d\mu)\) and not in \(\mathcal{H}\)), which has the same eigenvalues and eigenfunctions, is important to deal with mis-specified models (see [9]). Note that the eigenfunctions \(g_k\) are orthogonal for both dot-products in \(L_2(d\mu)\) and \(\mathcal{H}\), but that the normalization to unit norm differs. If \(\| g_k \|_{L_2(d\mu)}=1\) for all \(k \geqslant 0\), then we have \( \| g_k \|^2_\mathcal{H}=  \lambda_k^{-1} \langle g_k, \Sigma g_k \rangle_\mathcal{H} = \lambda_k^{-1}\mathbb{E} [ g_k(x)^2] =\lambda_k^{-1}\) , and thus, \(\| \lambda_k^{1/2} g_k \|_{\mathcal{H}}=1\), and we have the kernel expansion from an orthonormal basis of \(\mathcal{H}\): $$K(x,y) = \sum_{k=0}^\infty\lambda_k g_k(x) g_k(y),$$ which will lead to a new proof for Mehler formula.</p>



<h2>Orthonormal basis for the Gaussian kernel</h2>



<p class="justify-text">Hermite polynomials naturally lead to orthonormal basis of some reproducing kernel Hilbert spaces (RKHS). For simplicity, I will focus on one-dimensional problems, but this extends to higher dimension. </p>



<p class="justify-text"><strong>Translation-invariant kernels.</strong> We consider a function \(q: \mathbb{R} \to \mathbb{R}\) which is integrable, with <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a> (note the different normalization than before) which is defined for all \(\omega \in \mathbb{R}\) because of the integrability: $$\hat{q}(\omega) = \int_{\mathbb{R}} q(x) e^{-i \omega x} dx.$$ We consider the kernel $$K(x,y) = q(x-y).$$ It can be check that as soon as  \(\hat{q}(\omega) \in \mathbb{R}_+\)  for all \(\omega \in \mathbb{R}\), then the kernel \(K\) is positive-definite.</p>



<p class="justify-text">For a translation-invariant kernel, we can write using the inverse Fourier transform formula: $$K(x,y) = q(x-y) = \frac{1}{2\pi} \int_{\mathbb{R}} \hat{q}(\omega) e^{i \omega ( x- y)} d \omega = \int_{\mathbb{R}} \varphi_\omega(x)^* \varphi_\omega(y) d \omega,$$ with \(\varphi_\omega(x) = \sqrt{\hat{q}(\omega) / (2\pi) } e^{i\omega x}\). Intuitively, for a function \(f: \mathbb{R} \to \mathbb{R}\), with \(\displaystyle f(x) = \frac{1}{2\pi} \int_{\mathbb{R}} \hat{f}(\omega)e^{i\omega x} d\omega = \int_{\mathbb{R}} \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}}\varphi_\omega(x) d\omega\), which is a “dot-product” between the family \((\varphi_\omega(x))_\omega\) and \(\Big( \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}} \Big)_\omega\), the squared norm \(\| f\|_{\mathcal{H}}^2\) is equal to the corresponding “squared norm” of \(\Big( \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}}\Big)_\omega\), and we thus have $$ \| f\|_{\mathcal{H}}^2 = \int_{\mathbb{R}} \Big| \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}} \Big|^2 d\omega =  \frac{1}{2\pi} \int_{\mathbb{R}} \frac{ | \hat{f}(\omega) |^2}{\hat{q}(\omega)} d\omega,$$ where \(\hat{f}\) is the Fourier transform of \(f\). While the derivation above is not rigorous, the last expression is.</p>



<p class="justify-text">In this section, I will focus on the Gaussian kernel defined as \(K(x,y) = q(x-y) =  \exp \big( – \alpha ( x- y )^2 \big)\), for which \(\displaystyle \hat{q}(\omega)= \sqrt{\frac{\pi}{\alpha}} \exp\big( – \frac{\omega^2}{4 \alpha} \big)\).</p>



<p class="justify-text">Given that \(\displaystyle \frac{1}{\hat{q}(\omega)} = \sqrt{\frac{\alpha}{\pi}} \exp\big(  \frac{\omega^2}{4 \alpha} \big)= \sqrt{\frac{\alpha}{\pi}} \sum_{k=0}^\infty  \frac{\omega^{2k}}{(4 \alpha)^k k!} \), the penalty \(\|f\|_\mathcal{H}^2\) is a linear combination of squared \(L_2\)-norm of \(\omega^k \hat{f}(\omega)\), which is the squared \(L_2\)-norm of the \(k\)-th derivative of \(f\). Thus, functions in the RKHS are infinitely differentiable, and thus very smooth (this implies that to have the fast rate \((\log n) / n \) above, the optimal regression function has to be very smooth).</p>



<p class="justify-text"><strong>Orthonormal basis of the RKHS</strong>. As seen in Eq. (4), an expansion in an infinite sum is necessary to obtain an orthonormal basis. We have: $$K(x,y) = e^{-\alpha x^2} e^{-\alpha y^2} e^{2 \alpha x y} = e^{-\alpha x^2} e^{-\alpha y^2} \sum_{k=0}^\infty \frac{ (2\alpha)^k}{k!} x^k y^k.$$ Because of Eq. (4), with \(g_k(x) = \sqrt{ \frac{(2\alpha)^k}{k!}} x^k \exp \big( – \alpha x^2 \big)\), we have a good candidate for an orthonornal basis. Let us check that this is the case. Note that the expansion above alone cannot be used as a proof that \((g_k)\) is an orthonormal basis of \(\mathcal{H}\).</p>



<p class="justify-text">Given the function \(f_k(x) = x^k \exp \big( – \alpha x^2 \big)\), we can compute its Fourier transform as $$ \hat{f}_k(\omega) = i^{-k} ( 4 \alpha)^{-k/2} \sqrt{\frac{\pi}{\alpha}} H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big) .$$ Indeed, we have, from Rodrigues’ formula, $$H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big) =(-1)^k (4 \alpha)^{k/2} \frac{d^k}{d \omega^k}\big[ \exp\big( – \frac{\omega^2}{4 \alpha} \big) \big],$$ and thus its inverse Fourier transform is equal to \((ix)^k\) times the one of \((-1)^k (4 \alpha)^{k/2} \exp\big( – \frac{\omega^2}{4 \alpha} \big)\), which is thus equal to \((-i)^k (4 \alpha)^{k/2} \sqrt{ \alpha / \pi }e^{-\alpha x^2} \), which leads to the Fourier transform formula above.</p>



<p class="justify-text">We can now compute the RKHS dot products, to show how to obtain the orthonormal basis described in [11]. This leads to $$ \langle f_k, f_\ell \rangle = \frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k/2}( 4 \alpha)^{-\ell/2}  \int_{\mathbb{R}} H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) H_\ell \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big)  d\omega,$$ which leads to, with a change of variable $$ \langle f_k, f_\ell \rangle = \frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k/2}( 4 \alpha)^{-\ell/2} \sqrt{4 \alpha} \int_{\mathbb{R}} H_k (u) H_\ell (u)  \exp(-u^2) du,$$ which is equal to zero if \(k \neq \ell\), and equal to \(\frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k} \sqrt{4 \alpha} \sqrt{\pi} 2^k k!   = ( 2 \alpha)^{-k} k!\) if \(k = \ell\). Thus the sequence \((f_k)\) is an orthogonal basis of the RKHS, and the sequence \((g_k)\) defined as \(g_k(x) = \sqrt{ \frac{(2\alpha)^k}{k!}} f_k(x)\) is an orthonormal basis of the RKHS, from which, using the expansion as in Eq. (4), we recover the expansion: $$K(x,y) = \sum_{k=0}^\infty g_k(x) g_k(y) = e^{-\alpha x^2} e^{-\alpha y^2} \sum_{k=0}^\infty \frac{ (2\alpha)^k}{k!} x^k y^k.$$</p>



<p class="justify-text">This expansion can be used to approximate the Gaussian kernel by finite-dimensional explicit feature spaces, by just keeping the first basis elements (see an application to optimal transport in [<a href="https://arxiv.org/pdf/1810.10046">12</a>], with an improved behavior using an adaptive low-rank approximation through the Nyström method in [<a href="https://papers.nips.cc/paper/8693-massively-scalable-sinkhorn-distances-via-the-nystrom-method.pdf">13</a>]).</p>



<h2>Eigenfunctions for the Gaussian kernels</h2>



<p class="justify-text">In order to obtain explicit formulas for the eigenvalues of the covariance operator, we need more than a mere orthonormal basis, namely an eigenbasis.</p>



<p class="justify-text">An orthogonal basis will now be constructed with arguably better properties as it is also an orthonormal basis for both the RKHS and \(L_2(d\mu)\) for a Gaussian measure, that diagonalizes the integral operator associated to this probability measure, as well as the covariance operator.</p>



<p class="justify-text">As seen above, we simply need an orthogonal family \((f_k)_{k \geqslant 0}\), such that given a distribution \(d\mu\),  \((f_k)_{k \geqslant 0}\) is a family in \(L_2(d\mu)\) such that $$\int_{\mathbb{R}} f_k(x) K(x,y) d\mu(x) = \lambda_k f_k(y), \tag{5}$$ for eigenvalues \((\lambda_k)\). In the next paragraph, we will do exactly this for the Gaussian kernel \(K(x,y) = e^{-\alpha (x-y)^2}\) for \(\alpha = \frac{\rho}{1- \rho^2}\) for some \(\rho \in (0,1)\); this particular parameterization in \(\rho\) is to make the formulas below not (too) complicated.</p>



<p class="justify-text">With \(f_k(x) = \frac{1}{\sqrt{N_k}} H_k(x) \exp \Big( – \frac{\rho}{1+\rho} x^2 \Big)\), where \(N_k = {2^k k!} \sqrt{ \frac{1-\rho}{1+\rho}}\), then \((f_k)_{k \geqslant 0}\) is an <em>orthonormal</em> basis for \(L_2(d\mu)\) for \(d\mu\) the Gaussian distribution with mean zero and variance \(\frac{1}{2} \frac{1+\rho}{1-\rho}\) (this is a direct consequence of the orthogonality property of Hermite polynomials).</p>



<p class="justify-text">Moreover, the moment of the Hermite polynomial in Eq. (3) exactly leads to Eq. (5) for the chosen kernel and \(\lambda_k = (1-\rho) \rho^k\). Since the eigenvalues sum to one, and the trace of \(\Sigma\) is equal to one (as a consequence of \(K(x,x)=1\)), the family \((f_k)\) has to be a basis of \(\mathcal{H}\).</p>



<p>From properties of the eigenbasis, since \((f_k)\) is an orthonormal eigenbasis of \(L_2(d\mu)\) and the eigenvalues are \(\lambda_k = (1-\rho)\rho^k\),  we get: $$ K(x,y) = \exp \Big( – \frac{\rho}{1- \rho^2} (x-y)^2\Big) = \sum_{k=0}^\infty (1-\rho)\rho^k f_k(x) f_k(y),$$ which is exactly the Mehler formula, and thus we obtain an alternative proof.</p>



<p class="justify-text">We then get an explicit basis and the exponential decay of eigenvalues, which was first outlined by [2]. See an application to the estimation of the Poincaré constant in [<a href="https://hal.archives-ouvertes.fr/hal-02327453v1/document">14</a>] (probably a topic for another post in a few months).</p>



<p class="justify-text"><strong>Experiments.</strong> In order to showcase the exact eigenvalues of the expectation \(\Sigma\) (for the correct combination of Gaussian kernel and Gaussian distribution), we compare the eigenvalues with the ones of the empirical covariance operator \(\hat\Sigma\), for various values of the number of observations. We see that as \(n\) increases, the empirical eigenvalues match the exact ones for higher \(k\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4889" height="307" src="https://francisbach.com/wp-content/uploads/2020/10/gaussian_kernel-1.gif" width="363"/>Eigenvalues of the covariance operator \(\Sigma\) (“expectation”) compared to the ones of the empirical covariance operator \(\hat\Sigma\), averaged over 20 replications (“empirical”), for several values of \(n\).</figure></div>



<h2>Conclusion</h2>



<p class="justify-text">In this post, I only presented applications of Hermite polynomials to the Gaussian kernel, but these polynomials appear in many other areas of applied mathematics, for other types of kernels within machine learning such as dot-product kernels [3], in random matrix theory (see <a href="https://terrytao.wordpress.com/2011/02/20/topics-in-random-matrix-theory/">here</a>), in statistics for <a href="https://en.wikipedia.org/wiki/Edgeworth_series">Edgeworth expansions</a>, and of course for <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Hermite_quadrature">Gauss-Hermite quadrature</a>.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Loucas Pillaud-Vivien and Alessandro Rudi for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] George Neville Watson. <a href="https://academic.oup.com/jlms/article-pdf/s1-8/3/194/2363185/s1-8-3-194.pdf">Notes on Generating Functions of Polynomials: (2) Hermite Polynomials</a>. <em>Journal of the London Mathematical Soc</em>iety, 8, 194-199, 1933.<br/>[2] Huaiyu Zhu, Christopher K. I. Williams, Richard Rohwer, and Michal Morciniec. <a href="https://publications.aston.ac.uk/id/eprint/38366/1/NCRG_97_011.pdf">Gaussian regression and optimal finite dimensional linear models</a>. In <em>Neural Networks and Machine Learning</em>. Springer-Verlag, 1998.<br/>[3] A. Daniely, R. Frostig, and Y. Singer. <a href="https://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity.pdf">Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity</a>. In Advances In Neural Information Processing Systems, 2016.<br/>[4] Gabor Szegö. <em>Orthogonal polynomials</em>. American Mathematical Society, 1939.<br/>[5] Grace Wahba. <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970128">Spline models for observational data</a>. Society for Industrial and Applied Mathematics, 1990.<br/>[6] Bernhard Schölkopf, Alexander J. Smola. <a href="https://mitpress.mit.edu/books/learning-kernels">Learning with kernels: support vector machines, regularization, optimization, and beyond</a>. MIT Press, 2002.<br/>[7] John Shawe-Taylor, Nello Cristianini. <em>Kernel methods for pattern analysis</em>. Cambridge University Press, 2004.<br/>[8] Andrea Caponnetto, Ernesto De Vito. <a href="https://link.springer.com/content/pdf/10.1007/s10208-006-0196-8.pdf">Optimal rates for the regularized least-squares algorithm</a>. Foundations of Computational Mathematics 7.3: 331-368, 2007.<br/>[9] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. <a href="http://web.stanford.edu/~hastie/Papers/ESLII.pdf">The elements of statistical learning</a>. Vol. 1. No. 10. Springer series in statistics, 2001.<br/>[10] Trevor Hastie and Robert Tibshirani. <em>Generalized Additive Models</em>. Chapman &amp; Hall, 1990.<br/>[11] Ingo Steinwart, Don Hush, and Clint Scovel. <a href="http://[PDF] ieee.org">An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels</a>. <em>IEEE Transactions on Information Theory</em>, 52.10:4635-4643, 2006.<br/>[12] Jason Altschuler, Francis Bach, Alessandro Rudi, Jonathan Niles-Weed. <a href="https://arxiv.org/pdf/1810.10046">Approximating the quadratic transportation metric in near-linear time</a>. Technical report arXiv:1810.10046, 2018.<br/>[13] Jason Altschuler, Francis Bach, Alessandro Rudi, Jonathan Niles-Weed. <a href="https://papers.nips.cc/paper/8693-massively-scalable-sinkhorn-distances-via-the-nystrom-method.pdf">Massively scalable Sinkhorn distances via the Nyström method</a>. <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019.<br/>[14] Loucas Pillaud-Vivien, Francis Bach, Tony Lelièvre, Alessandro Rudi, Gabriel Stoltz. <a href="https://hal.archives-ouvertes.fr/hal-02327453v1/document">Statistical Estimation of the Poincaré constant and Application to Sampling Multimodal Distributions</a>. <em>Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),</em> 2020.<br/>[15] Nachman Aronszajn. <a href="https://www.ams.org/journals/tran/1950-068-03/S0002-9947-1950-0051437-7/S0002-9947-1950-0051437-7.pdf">Theory of Reproducing Kernels</a>. <em>Transactions of the American Mathematical Society</em>, 68(3): 337–404, 1950.</p>



<h2>Proof of properties of Hermite polynomials</h2>



<p class="justify-text">In this small appendix, I give “simple” proofs (that sometimes require knowledge of <a href="https://en.wikipedia.org/wiki/Complex_analysis">complex analysis</a>) to the properties presented above.</p>



<p class="justify-text"><strong>Generating function.</strong> We have, using <a href="https://en.wikipedia.org/wiki/Residue_theorem">residue theory</a>, $$H_k(x)=(-1)^k e^{x^2} \frac{d^k}{d x^k}\big[ e^{-x^2} \big] = (-1)^k \frac{k!}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{(z-x)^{k+1}}dz, $$ where \(\gamma\) is a contour in the complex plane around \(x\). This leads to, for any \(t\) (here, we ignore on purpose the summability issues, for more details, see [4, Section 5.5]): $$ \sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) =  \frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{z-x} \sum_{k=0}^\infty \frac{t^k} {(x-z)^{k}}dz, $$ which can be simplified using the sum of the geometric series, leading to $$\frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{z-x} \frac{z-x}{z-x- t} dz =  \frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}} {z-x- t} dz.$$ Using the first-order residue at \(x+t\). This is thus equal to \(e^{x^2-(t+x)^2} = e^{-t^2 + 2tx}\), which is exactly the generating function statement from Eq. (2).</p>



<p class="justify-text"><strong>Orthogonality for Gaussian distribution.</strong> We can prove through integration by parts, but there is a nicer proof through the generating function. Indeed, with $$ a_{k \ell} = \int_{-\infty}^{+\infty} e^{-x^2} H_k(x) H_\ell(x) dx, $$ for \(k, \ell \geqslant 0\), we get $$\sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!} = \int_{-\infty}^{+\infty}e^{-x^2}\Big(  \sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!}  H_k(x) H_\ell(x) \Big) dx.$$ Using the generating function, this leads to $$\sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!} =  \int_{-\infty}^{+\infty} e^{-x^2 + 2xu-u^2 + 2xt – t^2} dx= e^{2uv} \int_{-\infty}^{+\infty} e^{-(x-u-v)^2}dx, $$ which can be computed explicitly using normalization constants of the Gaussian distribution, as \( \sqrt{\pi} e^{2uv} = \sqrt{\pi} \sum_{k=0}^\infty \frac{ (2  u v)^k}{k!},\) leading to all desired orthogonality relationships using the uniqueness of all coefficients for factors \(t^k u^\ell\).</p>



<p class="justify-text"><strong>Recurrence relationship.</strong> Taking the derivative of the generating function with respect to \(t\), one gets \( \displaystyle (2x-2t) e^{2tx-t^2} = \sum_{k=0}^\infty \frac{t^{k-1}}{(k-1)!} H_k(x),\) which is equal to (using again the generating function) \(\displaystyle \sum_{k=0}^\infty \frac{t^{k}}{k!} 2x H_k(x) \ – \sum_{n=0}^\infty \frac{t^{k+1}}{k!} 2 H_k(x).\) By equating the coefficients for all powers of \(t\), this leads to the desired recursion in Eq. (1).</p>



<p class="justify-text"><strong>Fourier transform.</strong> Again using the generating function, written $$ e^{-x^2/2 + 2xt – t^2} = \sum_{k=0}^\infty \frac{t^k}{k!} e^{-x^2/2} H_k(x), $$ we can take Fourier transforms and use the fact that the Fourier transform of \(e^{-x^2/2}\) is itself (for the chosen normalization), and then equate coefficients for all powers of \(t\) to conclude (see more details <a href="https://en.wikipedia.org/wiki/Hermite_polynomials#Hermite_functions_as_eigenfunctions_of_the_Fourier_transform">here</a>).</p>



<p class="justify-text"><strong>Expectation for Gaussian distributions.</strong> We finish the appendix by proving Eq. (3). We consider computing for any \(t\), $$\sum_{k=0}^\infty \rho^k \frac{t^k}{k!} H_k (y) = e^{2\rho t y – \rho^2 t^2},$$ using the generating function from Eq. (2). We then compute $$A=\int_{-\infty}^\infty \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big) \sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) dx = \int_{-\infty}^\infty \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big) \exp( 2tx – t^2) dx.$$ We then use \( \frac{(x-\rho y)^2}{1-\rho^2} – 2tx + t^2 = \frac{x^2}{1-\rho^2}  – \frac{2x[ t(1-\rho^2) + \rho y]}{1-\rho^2}  + t^2 + \frac{\rho^2 y^2}{1-\rho^2}\), leading to $$A = \sqrt{\pi} \sqrt{1-\rho^2} \exp\Big( -t^2 – \frac{\rho^2 y^2}{1-\rho^2} +(1-\rho^2) \big( t + \frac{\rho y}{1-\rho^2} \big)^2 \Big) = \sqrt{\pi} \sqrt{1-\rho^2} e^{2\rho t y – \rho^2 t^2}.$$ By equating powers of \(t\), this leads to Eq. (3).</p></div>
    </content>
    <updated>2020-10-08T19:33:36Z</updated>
    <published>2020-10-08T19:33:36Z</published>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-10-22T23:32:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/151</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/151" rel="alternate" type="text/html"/>
    <title>TR20-151 |  Pseudobinomiality of the Sticky Random Walk | 

	Venkatesan Guruswami, 

	Vinayak Kumar</title>
    <summary>Random walks on expanders are a central and versatile tool in pseudorandomness.  If an arbitrary half of the vertices of an expander graph are marked, known Chernoff bounds for expander walks imply that the number $M$ of marked vertices visited in a long $n$-step random walk strongly concentrates around the expected $n/2$ value. Surprisingly, it was recently shown that the parity of $M$ also has exponentially small bias.

Is there a common unification of these results? What other statistics about $M$ resemble the binomial distribution (the Hamming weight of a random $n$-bit string)? To gain insight into such questions, we analyze a simpler model called the sticky random walk. This model is a natural stepping stone towards understanding expander random walks, and we also show that it is a necessary step. The sticky random walk starts with a random bit and then each subsequent bit independently equals the previous bit with probability $(1+\lambda)/2$. Here $\lambda$ is the proxy for the expander's (second largest) eigenvalue.
    
Using Krawtchouk expansion of functions, we derive several probabilistic results about the sticky random walk. We show an asymptotically tight $\Theta(\lambda)$ bound on the total variation distance between the (Hamming weight of the) sticky walk and the binomial distribution. We prove that the correlation between the majority and parity bit of the sticky walk is bounded by $O(n^{-1/4})$. This lends hope to unifying Chernoff bounds and parity concentration, as well as establishing other interesting statistical properties, of expander random walks.</summary>
    <updated>2020-10-08T14:03:25Z</updated>
    <published>2020-10-08T14:03:25Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-22T23:31:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/150</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/150" rel="alternate" type="text/html"/>
    <title>TR20-150 |  Almost-Everywhere Circuit Lower Bounds from Non-Trivial Derandomization | 

	Lijie Chen, 

	Xin Lyu, 

	Ryan Williams</title>
    <summary>In certain complexity-theoretic settings, it is notoriously difficult to prove complexity separations which hold almost everywhere, i.e., for all but finitely many input lengths. For example, a classical open question is whether $\mathrm{NEXP} \subset \mathrm{i.o.-}\mathrm{NP}$; that is, it is open whether nondeterministic exponential time computations can be simulated on infinitely many input lengths by $\mathrm{NP}$ algorithms. This difficulty also applies to Williams' algorithmic method for circuit lower bounds [Williams, J. ACM 2014]. In particular, although [Murray and Williams, STOC 2018] proved $\mathrm{NTIME}[2^{\mathrm{polylog}(n)}] \not\subset \mathrm{ACC}^0$, it has remained an open problem to show that $\mathrm{E}^{\mathrm{NP}}$ ($2^{O(n)}$ time with an $\mathrm{NP}$ oracle) is not contained in $\mathrm{i.o.-}\mathrm{ACC}^0$. 
	
In this paper, we show how many infinitely-often circuit lower bounds proved by the algorithmic method can be adapted to establish almost-everywhere lower bounds. 

- We show there is a function $f \in \mathrm{E}^{\mathrm{NP}}$ such that for all sufficiently large input lengths $n$ and $\varepsilon \leq o(1)$, $f$ cannot be $(1/2+2^{-n^{\varepsilon}})$-approximated by $2^{n^\varepsilon}$-size $\mathrm{ACC}^0$ circuits on inputs of length $n$, improving lower bounds in [Chen and Ren, STOC 2020] and [Viola, ECCC 2020]. 

- We construct rigid matrices in $\mathrm{P}^{\mathrm{NP}}$ for all but finitely many inputs, rather than infinitely often as in [Alman and Chen, FOCS 2019] and [Bhangale et al., FOCS 2020]. 

- We show there are functions in $\mathrm{E}^{\mathrm{NP}}$ requiring constant-error probabilistic degree at least $\Omega(n/\log^2 n)$ for all large enough $n$, improving an infinitely-often separation of [Viola, ECCC 2020].
	
Our key to proving almost-everywhere worst-case lower bounds is a new ``constructive'' proof of an NTIME hierarchy theorem proved by [Fortnow and Santhanam, CCC 2016], where we show for every ``weak'' nondeterminstic algorithm (with smaller running-time and short witness), a ``refuter algorithm'' exists that can construct ``bad'' inputs for the hard language. We use this refuter algorithm to construct an almost-everywhere hard function. To extend our lower bounds to the average case, we prove a new XOR Lemma based on approximate linear sums, and combine it with the PCP-of-proximity applications developed in [Chen and Williams, CCC 2019] and [Chen and Ren, STOC 2020]. As a byproduct of our new XOR Lemma, we obtain a nondeterministic pseudorandom generator for poly-size $\mathrm{ACC}^0$ circuits with seed length $\mathrm{polylog}(n)$, which resolves an open question in [Chen and Ren, STOC 2020].</summary>
    <updated>2020-10-08T13:53:53Z</updated>
    <published>2020-10-08T13:53:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-22T23:31:02Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7346788885933681031</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7346788885933681031/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/revisiting-continuum-hypothesis.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7346788885933681031" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7346788885933681031" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/revisiting-continuum-hypothesis.html" rel="alternate" type="text/html"/>
    <title>Revisiting the Continuum Hypothesis</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have been thinking about CH lately for two reasons</p><p>1) I reread the article</p><p>Hilbert's First Problem: The Continuum Hypothesis by Donald Martin from <b>Proceedings of Symposia </b> i<b>n Pure Mathematics: Mathematical developments arising from Hilbert Problems</b>. 1976. (For a book review of the symposia and, <b>The Honor Class</b>, also about Hilbert's problems, see <a href="https://www.cs.umd.edu/users/gasarch/bookrev/44-4.pdf">here</a>.)</p><p>The article takes the point of view that CH CAN have an answer. He discusses large cardinals (why assuming they exist is plausible, but alas, that assumption does not seem to resolve CH) and Projective Det.  (why assuming it is true is plausible, but alas, that assumption does not seem to resolve CH).</p><p>(A set A \subseteq {0,1}^omega is DETERMINED if either Alice or Bob has a winning strategy in the following non-fun game: they alternate picking bits a_1, b_1, a_2, b_2, ... with Alice going first. If a_1 b_1 a_2 b_2... IS IN A then Alice wins, IF NOT then Bob wins. Martin showed that all Borel sets are determined. Proj Det is the statement that all projections of Borel sets are determined. AD is the axiom that ALL sets A are determined. It contradicts AC.)</p><p>But what really inspired this post is the last paragraph:</p><p><i>Throughout the latter part of my discussion, I have been assuming a naive and uncritical attitude towards CH. While this <b>is</b> in fact my attitude, I by no means wish to dismiss the opposite viewpoint.  Those that argue that the concept of set is not sufficiently clear to fix the truth-value of CH have a position that is at present difficult to assail. As long as no new axiom is found which decides CH, their case will continue to grow stronger, and our assertions that the meaning of CH is clear will sound more and more empty.</i></p><p>2) Scott Aaronson mentioned in a blog post (see <a href="https://www.scottaaronson.com/blog/?p=4962">here</a>) that  he has read and understood the proof that CH is independent of set theory.</p><p>SO, this seemed like a good time to revisit thoughts on CH.</p><p> I took a very short poll, just two people, about CH: Stephen Fenner (in a perfect world he would be a set theorists) and Scott Aaronson (having JUST read the proof that CH is ind.  he has thought about it recently).</p><p>Here are some thoughts of theirs and mine</p><p>1) All three of us are Platonists with regard to the Naturals (I was surprised to find recently that there are people who are not!) but not with regard to the reals.  So we would be OKAY with having CH have no answer.</p><p>2) All three of us  agree that it would be nice if SOME axiom was both</p><p>a) Intuitively appealing or aesthetically appealing ,  and</p><p>b) resolved CH.</p><p>I always thought that (a) would be the hard part-- or at least getting everyone (not sure who we are talking about) to AGREE on a new axiom. But even getting an axiom to resolve CH seems hard.  Large cardinals don't seem to do it, and various forms of Determinacy don't seem to do it.</p><p>Scott reminded me of Freiling's Axiom of Symmetry (see <a href="https://en.wikipedia.org/wiki/Freiling%27s_axiom_of_symmetry">here</a>) which IS intuitive and DOES resolve CH (its false) though there are problems with it--- a minor variant   of it contradicts AC (I am QUITE FINE with that since AC implies Banach-Tarski which Darling says shows `Math is broken'.)</p><p>Stephen recalled some of Hugh Woodin's opinions of CH, but Hugh seems to have changed his mind from NOT(CH): 2^{aleph_0} = aleph_2, to CH:  2^{aleph_0} = aleph_1.(See <a href="https://www.jstor.org/stable/24569622?seq=1#metadata_info_tab_contents">here</a>.)</p><p>3) All three of would be okay with V=L, though note that this would put many set theorists out of work. All the math that applies to the real world would still be intact.  I wonder if in an alternative history the reaction to Russell's paradox would be a formulation of set theory where V=L. We would KNOW that CH is true, KNOW that AC is true. We would know a lot about L but less about forcing.</p><p>4) Which Geometry is true: Euclidian, Riemannian, others? This is now regarded as a silly question: Right Tool, Right Job! If you build a bridge use Euclid. If you are doing astronomy use Riemann. Might Set Theory go the same way? It would be AWESOME if Scott Aaronson found some quantum thing where assuming 2^{aleph_0} = aleph_2 was the right way to model it.</p><p>5) If I was more plugged into the set theory community I might do a poll of set theorists, about CH. Actually, someone sort-of already has. Penelope Maddy has two excellent and readable articles where she studies what set theorists believe and why.</p><p><b>Believing The Axioms I</b>: <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/belaxioms1.pdf">here</a></p><p><b>Believing The Axioms II</b>: <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/belaxioms2.pdf">here</a></p><p>Those articles were written in 1988. I wonder if they need an update.</p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2020-10-08T13:52:00Z</updated>
    <published>2020-10-08T13:52:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-22T11:46:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1420</id>
    <link href="https://ptreview.sublinear.info/?p=1420" rel="alternate" type="text/html"/>
    <title>News for September 2020</title>
    <summary>Apologies dear readers for the late posting. The beginning of the school year is always frenzied, and the pandemic has only added to that frenzy. We have an exciting September, with four papers on graph property testing, one two papers on distribution testing, and one paper that connects both topics. (Ed: we normally scan through […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Apologies dear readers for the late posting. The beginning of the school year is always frenzied, and the pandemic has only added to that frenzy. We have an exciting September, with four papers on graph property testing, <s>one</s> two papers on distribution testing, and one paper that connects both topics.</p>



<p><em>(Ed: we normally scan through ECCC and arXiv, but are happy to post about papers that appear elsewhere. Thanks to the reader who pointed out a relevant COLT 2020 paper.)</em></p>



<p><strong>Estimation of Graph Isomorphism Distance in the Query World</strong> by Sourav Chakraborty, Arijit Ghosh, Gopinath Mishra, and Sayantan Sen (<a href="https://eccc.weizmann.ac.il/report/2020/135/">ECCC</a>). Graph isomorphism is about as fundamental as it gets, and this papers studies approximating the graph isomorphism distance for dense graphs. There is a known graph \(G_k\) (with \(n\) vertices). The algorithm is given query access to an input graph \(G_u\) and needs to approximate the number of edge inserts/deletes required to make the graphs isomorphic. This is the tolerant testing version; the property testing version is known to be doable in \(\widetilde{O}(\sqrt{n})\) queries (<a href="https://epubs.siam.org/doi/abs/10.1137/070680795?journalCode=smjcat">Matsliah-Fischer</a>). The main insight of this paper is to relate the tolerant testing complexity to a distribution testing problem. Consider distributions over the \(\{0,1\}^n\) defined by multisets of \(n\) hypercube points. Our aim is to estimate the earthmover distance between a known distribution and an unknown distribution. Interestingly, the query model is different: one can sample the underlying multisets <em>without</em> replacement. It turns out that the optimal complexity of this problem is (upto polylog factors) is the same as the optimal complexity of tolerant testing of graph isomorphism. A direct corollary is that the isomorphism distance can be approximated upto additive \(\epsilon n^2\) using \(\widetilde{O}(n)\) samples. This equivalence also gives an alternate proof for lower bounds for property testing graph isomorphism.</p>



<p><strong>Robustly Self-Ordered Graphs: Constructions and Applications to Property Testing </strong>by Oded Goldreich and Avi Wigderson (<a href="https://eccc.weizmann.ac.il/report/2020/149/">ECCC</a>). Let’s start from the application. The aim is to prove the following property testing lower bounds for the bounded-degree graph setting: an exponential separation between tolerant and vanilla testing, and finding an efficiently decidable property (in polynomial time) that cannot be property tested in sublinear time. For binary strings, results of this form are known. Can these be “ported” to the bounded-degree graph world? Can we construct graphs such that adjacency queries reduce to bit queries in strings? Naturally, one can simply represent the adjacency list as a string and treat graph queries as bit queries. But the problem is that of isomorphisms: different bit strings could represent the same graph and therefore, the different bit strings must have the same status with respect to the underlying property. The key insight in this paper is to introduce <em>robustly self-ordered graph</em>s, as a tool to port bit string property testing lower bounds to bounded-degree graphs. Such graphs essentially have a unique (identity) automorphism, even after a few edge insert/deletes. The actual definition is more technical, but that is the essence. The main result is an explicit construction of such graphs, from which the lower bound can be ported directly through a convenient lemma.</p>



<p><strong>Modifying a Graph’s Degree Sequence and the Testablity of Degree Sequence Properties </strong>by Lior Gishboliner (<a href="https://arxiv.org/pdf/2009.12697.pdf">arXiv</a>). A sequence of numbers \(D = (d_1, d_2, \ldots, d_n)\) is graphic if there exists an undirected graph on \(n\) vertices whose degrees are precisely the numbers of the sequence. Graphical sequences have been characterized by classic results of <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Gallai_theorem">Erdös-Gállai</a> and <a href="https://en.wikipedia.org/wiki/Havel%E2%80%93Hakimi_algorithm">Havel-Hakimi</a>. This paper first proves the following theorem. Suppose a graphic sequence \(D’\) has \(l_1\)-distance at most \(\delta\) from the degree sequence \(D\) of a graph \(G\). Then, there exists a graph \(G’\) with degree sequence \(D’\) such that the (dense graph) distance between \(G\) and \(G’\) is \(O(\sqrt{\delta})\). This theorem is used to prove an interesting property testing result. Let \(\mathcal{D}\) be a subset of graphic sequences that are closed under permutation. Let \(\mathcal{G}\) be the set of graphs that have a degree sequence in \(\mathcal{D}\). Then \(\mathcal{G}\) can be tested in \(poly(1/\epsilon)\) queries.</p>



<p><strong>Sampling an Edge Uniformly in Sublinear Time</strong> by Jakub Têtek (<a href="https://arxiv.org/pdf/2009.11178.pdf">arXiv</a>). In the general model for sublinear algorithms on graphs, an important choice is whether one allows uniform random edge queries. A natural question is whether such queries can simulated efficiently, using only random vertex, degree, and neighbor queries. This problem appears somewhat implicitly in previous sublinear subgraph counting algorithms, and <a href="http://drops.dagstuhl.de/opus/volltexte/2019/10628/pdf/LIPIcs-ICALP-2019-52.pdf">Eden-Ron-Rosenbaum</a> study it explicitly. They prove that one can sample from an \(\epsilon\)-approximate uniform distribution (over edges) using \(O(n/\sqrt{\epsilon m})\) samples. The problem of sampling from exactly the uniform distribution is left open. Until this paper. The main result shows that by modifying the Eden-Ron-Rosenbaum algorithm parameters, one can generate edge samples from an \(\epsilon\)-approximate uniform distribution using \(O((n/\sqrt{m})\log \epsilon^{-1})\) samples. The exact uniform distribution is achieved by setting \(\epsilon = 1/n\), to get a sample complexity of \(O((n\log n)/\sqrt{m})\).</p>



<p><strong>Faster Property Testers in a Variation of the Bounded Degree Model </strong>by Isolde Adler and Polly Fahey (<a href="https://arxiv.org/pdf/2009.07770.pdf">arXiv</a>). The setting of bounded-degree graph property testing naturally extends to bounded-degree relational databases, which can be thought of as “directed” hypergraphs. This is an interesting new direction of research, that combines property testing with database theory (see <a href="https://core.ac.uk/download/pdf/157699299.pdf">Adler-Harwath</a> and <a href="https://dl.acm.org/doi/10.1145/3294052.3319679">Chen-Yoshida</a>). One of the main contributions of this work is to consider another notion of distance: edge and vertex inserts/deletes. This is a natural extension, and we can now compare distances between graphs/databases with different numbers of vertices. The main result is that, under this notion of distance, a large class of properties can be tested in constant running time on databases with bounded degree and treewidth. Specifically, any property expressible in Counting Monadic Second-Order Logic (CMSO) can be tested in constant time. Previous results by Alder-Harwath showed that such properties can be tested (under the standard distance notion) in constant queries, but polylogarithmic time. </p>



<p><strong>Optimal Testing of Discrete Distributions with High Probability</strong> by Ilias Diakonikolas, Themis Gouleakis, Daniel M. Kane, John Peebles, and Eric Price (<a href="https://arxiv.org/abs/2009.06540">arXiv</a>, <a href="https://eccc.weizmann.ac.il/report/2020/140/">ECCC</a>). The focus of this paper is distribution testing in the “high probability” regime, where we wish the error of the tester to be \(&lt; \delta\). Typically, most results just get an error of at most \(1/3\), from which standard probabilistic boosting would tack on an extra \(O(\log 1/\delta)\) factor. In standard TCS settings, one doesn’t focus on optimizing this dependence, but in statistics, there is significant focus on the optimal sample complexity. And indeed, for practical applications, it is crucial to have sharp bounds on the right number of samples required for hypothesis testing. The paper also argues that getting the optimal sample complexity requires new algorithms, even for uniformity testing. There are optimal results given for closeness and independence testing. The optimal sample complexity only pays a multiplicative factor of \(\log^{1/3} (1/\delta)\) or \(\log^{1/2}(1/\delta)\) over the optimal bound for constant error (with other additive terms depending on \(\log(1/\delta)\)).</p>



<p><strong>Bessel Smoothing and Multi-Distribution Property Estimation</strong> by Yi Hao and Ping Li (<a href="http://proceedings.mlr.press/v125/hao20a.html">COLT 2020</a>). Let us consider some standard (tolerant) distribution testing questions, phrases as approximation algorithms. Given sample access to two distributions \(p\) and \(q\) over \([n]\), we may wish to estimate the \(l_1\)-distance, \(l_2\)-distance, relative entropy, etc. between these distributions. One can phrases this problem abstractly as estimating \(\sum_{i \in [n]} f(p_i, q_i)\), where \(f\) is some explicit function. This papers shows that for any 1-Lipschitz function \(f\) that satisfies some “regularity” property, the sum \(\sum_{i \in [n]} f(p_i, q_i)\) can be \(\epsilon\)-approximated with \(O(\epsilon^{-3}n/\sqrt{\log n})\) samples (apologies to the authors to replacing their \(k\) with the more familiar \(n\) for our readers). Thus, we can get sublinear sampling complexity for a very general class of estimation problems. Moreover, this was actually the simplest setting consider in the paper. One can deal with such functions of \(d\) distributions, not just two distributions. One of the corollaries of the theorems is a sublinear tolerant tester for the property of being a mixture of distributions.</p></div>
    </content>
    <updated>2020-10-07T23:20:27Z</updated>
    <published>2020-10-07T23:20:27Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-10-22T23:32:29Z</updated>
    </source>
  </entry>
</feed>
