<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-06-28T03:22:46Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5479161699112157490</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5479161699112157490/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/someone-thinks-i-am-fine-artist-why.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5479161699112157490" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5479161699112157490" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/someone-thinks-i-am-fine-artist-why.html" rel="alternate" type="text/html"/>
    <title>Someone thinks I am a fine artist! Why?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A while back I got an  email asking me to submit to a Fine Arts Journal. Why me? Here are some possibilities:</p><p>1) They were impressed with my play: </p><p><b>Sure he created the universe, but would he get Tenure?</b> (see <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/god.html">here</a>) which did get into a play-writing contest and was performed (one of the actresses  scolded me since I took a slot from <i>a real</i> <i>playwrigh</i>t).</p><p>2) They were impressed  with my <b>Daria Fan Fiction</b> (see the four entries <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/mywritings.html">here</a> labelled as Daria Fan Fiction).</p><p>3) They were impressed with my play <b>JFK: The Final chapter</b> (see <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/jfk.html">here</a>). Unlikely since this was rejected by a play writing contest and is not well known (as opposed to my other works in the fine arts which are well known?)</p><p>4) They were impressed with my collection of satires of  Nobel Laureate Bob Dylan (<a href="https://www.cs.umd.edu/users/gasarch/dylan/dylan.html">here</a>) .</p><p>5) They were impressed with some subset of (a) complexityblog, (b) <a href="https://www.amazon.com/Problems-Point-Exploring-Computer-Science/dp/9813279729/ref=sr_1_3?dchild=1&amp;keywords=gasarch&amp;qid=1609867944&amp;sr=8-3" target="_blank">Problems with a Point</a>,  (c)  <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215979/ref=sr_1_2?dchild=1&amp;keywords=gasarch&amp;qid=1609868018&amp;sr=8-2">Mathematical Muffin Morsels</a>, and (d) <a href="https://www.amazon.com/Bounded-Queries-Recursion-Progress-Computer-ebook/dp/B000W98WU4/ref=sr_1_4?dchild=1&amp;keywords=gasarch&amp;qid=1609868084&amp;sr=8-4">Bounded Queries in Recursion Theory</a>. Or maybe just having 3 books on amazon is their threshold.  If it's complexityblog then Lance and I should co-author something for them.</p><p>6) It is a vanity-journal where you pay to  publish. So why email me who (a)  is not an artist, (b) is  not a fine artist, and most important (3) <b>does not think of himself as a fine artist</b>. The PRO of emailing me or people like me is they cast a wide net. The CON is--- there is no CON! It costs nothing to email me, and emailing me does not affect their credibility. That still raises the question of how they got my name.</p><p>7) Could it be a phishing? If I click on something in the email would they  get my credit card number? (In the copy below I have replaced the links with plain text.) Note that it begins <i>Dear Professor</i> not  <i>Dear Professor Gasarch. </i>  So they know I am a professor. Then again, I have known of ugrads who get emails that begin <i>Dear Professor</i>. (The emails to HS student Naveen and ugrad Nichole in the story I tell <a href="https://blog.computationalcomplexity.org/search?q=Naveen">here</a> were addressed to <i>Dear Professor.) </i></p><p>8) They mistook me for my parents who, in 1973,  put together an anthology of short stories titled <i>Fiction:The Universal elements</i>,  for a Freshman Comp course my mom taught, see <a href="https://www.amazon.com/Fiction-Universal-Element-P-Gasarch/dp/0442226322">here</a>. I note that their book ranks around 18,000,000, so even that explanation is unlikely. Actually the rank changes a lot- it was 12,000,000 this morning. Still, not what one would call a best seller. It's fun to see what is doing better: <i>Bounded Queries in Recursion Theory (currently at around rank 6.000.000) </i> or <i>Fiction: The Universal Elements.</i></p><p> If I ever get one of these emails from a History Journal I will submit my Satirical <i>Ramsey Theory and the History of Pre-Christian England: An Example of Interdisciplinary Research</i> (see <a href="https://www.cs.umd.edu/~gasarch/COURSES/389/W14/ramseykings.pdf">here</a>) just to see what happens- but  I will stop short of paying-to-publish. Or maybe I will pay-to-publish so that the next time I try to fool a class with it I can point to a seemingly real journal which has the article. </p><p><br/></p><div class="gE iv gt" style="background-color: white; color: #222222; cursor: auto; font-family: Roboto, RobotoDraft, Helvetica, Arial, sans-serif; font-size: 0.875rem; padding: 20px 0px 0px;"><table cellpadding="0" class="cf gJ"><tbody style="display: block;"><tr class="acZ" style="display: flex; height: auto;"><td class="gF gK" style="display: block; line-height: 20px; margin: 0px; padding: 0px; vertical-align: top; white-space: nowrap; width: 571.172px;"><br/></td></tr></tbody></table></div></div>
    </content>
    <updated>2021-06-28T03:00:00Z</updated>
    <published>2021-06-28T03:00:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-06-28T03:00:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/090</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/090" rel="alternate" type="text/html"/>
    <title>TR21-090 |  On Secret Sharing, Randomness, and Random-less Reductions for Secret Sharing | 

	Divesh Aggarwal, 

	Maciej Obremski, 

	Eldon Chung, 

	Joao Ribeiro</title>
    <summary>Secret-sharing is one of the most basic and oldest primitives in cryptography, introduced by Shamir and Blakely in the 70s. It allows to strike a meaningful balance between availability and confidentiality of secret information. It has a host of applications most notably in threshold cryptography and multi-party computation. All known constructions of secret sharing (with the exception of those with a pathological choice of parameters) require access to uniform randomness. In practice, it is extremely challenging to generate a source of uniform randomness. This has led to a large body of research devoted to designing randomized algorithms and cryptographic primitives from imperfect sources of randomness.

Motivated by this, 15 years ago, Bosley and Dodis asked whether it is even possible to build 2-out-of-2 secret sharing without access to uniform randomness. In this work, we make progress towards resolving this question.

We answer this question for secret sharing schemes with important additional properties, i.e., either leakage-resilience or non-malleability. We prove that, unfortunately, for not too small secrets, it is impossible to construct any of 2-out-of-2 leakage-resilient secret sharing or 2-out-of-2 non-malleable secret sharing without access to uniform randomness.

Given that the problem whether 2-out-of-2 secret sharing requires uniform randomness has been open for a long time, it is reasonable to consider intermediate problems towards resolving the open question. In a spirit similar to NP-completeness, we study how the existence of a t-out-of-n secret sharing without access to uniform randomness is related to the existence of a t'-out-of-n' secret sharing without access to uniform randomness for a different choice of the parameters t,n,t',n'.</summary>
    <updated>2021-06-27T05:51:49Z</updated>
    <published>2021-06-27T05:51:49Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-28T03:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.13210</id>
    <link href="http://arxiv.org/abs/2106.13210" rel="alternate" type="text/html"/>
    <title>Optimal Fine-grained Hardness of Approximation of Linear Equations</title>
    <feedworld_mtime>1624752000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bafna:Mitali.html">Mitali Bafna</a>, Nikhil Vyas <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.13210">PDF</a><br/><b>Abstract: </b>The problem of solving linear systems is one of the most fundamental problems
in computer science, where given a satisfiable linear system $(A,b)$, for $A
\in \mathbb{R}^{n \times n}$ and $b \in \mathbb{R}^n$, we wish to find a vector
$x \in \mathbb{R}^n$ such that $Ax = b$. The current best algorithms for
solving dense linear systems reduce the problem to matrix multiplication, and
run in time $O(n^{\omega})$. We consider the problem of finding
$\varepsilon$-approximate solutions to linear systems with respect to the
$L_2$-norm, that is, given a satisfiable linear system $(A \in \mathbb{R}^{n
\times n}, b \in \mathbb{R}^n)$, find an $x \in \mathbb{R}^n$ such that $||Ax -
b||_2 \leq \varepsilon||b||_2$. Our main result is a fine-grained reduction
from computing the rank of a matrix to finding $\varepsilon$-approximate
solutions to linear systems. In particular, if the best known $O(n^\omega)$
time algorithm for computing the rank of $n \times O(n)$ matrices is optimal
(which we conjecture is true), then finding an $\varepsilon$-approximate
solution to a dense linear system also requires $\tilde{\Omega}(n^{\omega})$
time, even for $\varepsilon$ as large as $(1 - 1/\text{poly}(n))$. We also
prove (under some modified conjectures for the rank-finding problem) optimal
hardness of approximation for sparse linear systems, linear systems over
positive semidefinite matrices, well-conditioned linear systems, and
approximately solving linear systems with respect to the $L_p$-norm, for $p
\geq 1$. At the heart of our results is a novel reduction from the rank problem
to a decision version of the approximate linear systems problem. This reduction
preserves properties such as matrix sparsity and bit complexity.
</p></div>
    </summary>
    <updated>2021-06-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.13154</id>
    <link href="http://arxiv.org/abs/2106.13154" rel="alternate" type="text/html"/>
    <title>The complexity of quantified constraints: collapsibility, switchability and the algebraic formulation</title>
    <feedworld_mtime>1624752000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carvalho:Catarina.html">Catarina Carvalho</a>, Florent Madelaine, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martin:Barnaby.html">Barnaby Martin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhuk:Dmitriy.html">Dmitriy Zhuk</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.13154">PDF</a><br/><b>Abstract: </b>Let A be an idempotent algebra on a finite domain. By mediating between
results of Chen and Zhuk, we argue that if A satisfies the polynomially
generated powers property (PGP) and B is a constraint language invariant under
A (that is, in Inv(A)), then QCSP(B) is in NP. In doing this we study the
special forms of PGP, switchability and collapsibility, in detail, both
algebraically and logically, addressing various questions such as decidability
on the way.
</p>
<p>We then prove a complexity-theoretic converse in the case of infinite
constraint languages encoded in propositional logic, that if Inv(A) satisfies
the exponentially generated powers property (EGP), then QCSP(Inv(A)) is
co-NP-hard. Since Zhuk proved that only PGP and EGP are possible, we derive a
full dichotomy for the QCSP, justifying what we term the Revised Chen
Conjecture. This result becomes more significant now the original Chen
Conjecture is known to be false.
</p>
<p>Switchability was introduced by Chen as a generalisation of the already-known
collapsibility. For three-element domain algebras A that are switchable and
omit a G-set, we prove that, for every finite subset D of Inv(A), Pol(D) is
collapsible. The significance of this is that, for QCSP on finite structures
(over a three-element domain), all QCSP tractability (in P) explained by
switchability is already explained by collapsibility.
</p></div>
    </summary>
    <updated>2021-06-27T22:41:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.13078</id>
    <link href="http://arxiv.org/abs/2106.13078" rel="alternate" type="text/html"/>
    <title>Linear Space Streaming Lower Bounds for Approximating CSPs</title>
    <feedworld_mtime>1624752000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chou:Chi=Ning.html">Chi-Ning Chou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovnev:Alexander.html">Alexander Golovnev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sudan:Madhu.html">Madhu Sudan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Velingker:Ameya.html">Ameya Velingker</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Velusamy:Santhoshini.html">Santhoshini Velusamy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.13078">PDF</a><br/><b>Abstract: </b>We consider the approximability of constraint satisfaction problems in the
streaming setting. For every constraint satisfaction problem (CSP) on $n$
variables taking values in $\{0,\ldots,q-1\}$, we prove that improving over the
trivial approximability by a factor of $q$ requires $\Omega(n)$ space even on
instances with $O(n)$ constraints. We also identify a broad subclass of
problems for which any improvement over the trivial approximability requires
$\Omega(n)$ space. The key technical core is an optimal,
$q^{-(k-1)}$-inapproximability for the case where every constraint is given by
a system of $k-1$ linear equations $\bmod\; q$ over $k$ variables. Prior to our
work, no such hardness was known for an approximation factor less than $1/2$
for any CSP. Our work builds on and extends the work of Kapralov and Krachun
(Proc. STOC 2019) who showed a linear lower bound on any non-trivial
approximation of the max cut in graphs. This corresponds roughly to the case of
Max $k$-LIN-$\bmod\; q$ with $k=q=2$. Each one of the extensions provides
non-trivial technical challenges that we overcome in this work.
</p></div>
    </summary>
    <updated>2021-06-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.12969</id>
    <link href="http://arxiv.org/abs/2106.12969" rel="alternate" type="text/html"/>
    <title>Approximability of (Simultaneous) Class Cover for Boxes</title>
    <feedworld_mtime>1624752000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cardinal:Jean.html">Jean Cardinal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dallant:Justin.html">Justin Dallant</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iacono:John.html">John Iacono</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.12969">PDF</a><br/><b>Abstract: </b>Bereg et al. (2012) introduced the Boxes Class Cover problem, which has its
roots in classification and clustering applications: Given a set of n points in
the plane, each colored red or blue, find the smallest cardinality set of
axis-aligned boxes whose union covers the red points without covering any blue
point. In this paper we give an alternative proof of APX-hardness for this
problem, which also yields an explicit lower bound on its approximability. Our
proof also directly applies when restricted to sets of points in general
position and to the case where so-called half-strips are considered instead of
boxes, which is a new result.
</p>
<p>We also introduce a symmetric variant of this problem, which we call
Simultaneous Boxes Class Cover and can be stated as follows: Given a set S of n
points in the plane, each colored red or blue, find the smallest cardinality
set of axis-aligned boxes which together cover S such that all boxes cover only
points of the same color and no box covering a red point intersects a box
covering a blue point. We show that this problem is also APX-hard and give a
polynomial-time constant-factor approximation algorithm.
</p></div>
    </summary>
    <updated>2021-06-27T22:48:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.12959</id>
    <link href="http://arxiv.org/abs/2106.12959" rel="alternate" type="text/html"/>
    <title>Differentially Private Algorithms for Clustering with Stability Assumptions</title>
    <feedworld_mtime>1624752000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shechner:Moshe.html">Moshe Shechner</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.12959">PDF</a><br/><b>Abstract: </b>We study the problem of differentially private clustering under
input-stability assumptions. Despite the ever-growing volume of works on
differential privacy in general and differentially private clustering in
particular, only three works (Nissim et al. 2007, Wang et al. 2015, Huang et
al. 2018) looked at the problem of privately clustering "nice" k-means
instances, all three relying on the sample-and-aggregate framework and all
three measuring utility in terms of Wasserstein distance between the true
cluster centers and the centers returned by the private algorithm. In this work
we improve upon this line of works on multiple axes. We present a far simpler
algorithm for clustering stable inputs (not relying on the sample-and-aggregate
framework), and analyze its utility in both the Wasserstein distance and the
k-means cost. Moreover, our algorithm has straight-forward analogues for "nice"
k-median instances and for the local-model of differential privacy.
</p></div>
    </summary>
    <updated>2021-06-27T22:42:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.12858</id>
    <link href="http://arxiv.org/abs/2106.12858" rel="alternate" type="text/html"/>
    <title>Adaptive Relaxations for Multistage Robust Optimization</title>
    <feedworld_mtime>1624752000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hartisch:Michael.html">Michael Hartisch</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.12858">PDF</a><br/><b>Abstract: </b>Multistage robust optimization problems can be interpreted as two-person
zero-sum games between two players. We exploit this game-like nature and
utilize a game tree search in order to solve quantified integer programs
(QIPs). In this algorithmic environment relaxations are repeatedly called to
asses the quality of a branching variable and for the generation of bounds. A
useful relaxation, however, must be well balanced with regard to its quality
and its computing time. We present two relaxations that incorporate scenarios
from the uncertainty set, whereby the considered set of scenarios is
continuously adapted according to the latest information gathered during the
search process. Using selection, assignment, and runway scheduling problems as
a testbed, we show the impact of our findings.
</p></div>
    </summary>
    <updated>2021-06-27T22:47:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.12856</id>
    <link href="http://arxiv.org/abs/2106.12856" rel="alternate" type="text/html"/>
    <title>The maximum discrete surface-to-volume ratio of space-filling curve partitions</title>
    <feedworld_mtime>1624752000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gadouleau:Maximilien.html">Maximilien Gadouleau</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weinzierl:Tobias.html">Tobias Weinzierl</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.12856">PDF</a><br/><b>Abstract: </b>Space-filling curves (SFCs) are used in high performance computing to
distribute a computational domain or its mesh, respectively, amongst different
compute units, i.e.~cores or nodes or accelerators. The part of the domain
allocated to each compute unit is called a partition. Besides the balancing of
the work, the communication cost to exchange data between units determines the
quality of a chosen partition. This cost can be approximated by the
surface-to-volume ratio of partitions: the volume represents the amount of
local work, while the surface represents the amount of data to be transmitted.
Empirical evidence suggests that space-filling curves yield advantageous
surface-to-volume ratios. Formal proofs are available only for regular grids.
We investigate the surface-to-volume ratio of space-filling curve partitions
for adaptive grids and derive the maximum surface-to-volume ratio as a function
of the number of cells in the partition. In order to prove our main theorem, we
construct a new framework for the study of adaptive grids, notably introducing
the concepts of a shape and of classified partitions. The new methodological
framework yields insight about the SFC-induced partition character even if the
grids refine rather aggressively in localised areas: it quantifies the obtained
surface-to-volume ratio. This framework thus has the potential to guide the
design of better load balancing algorithms on the long term.
</p></div>
    </summary>
    <updated>2021-06-27T22:47:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.12725</id>
    <link href="http://arxiv.org/abs/2106.12725" rel="alternate" type="text/html"/>
    <title>Breaking the $O(n)$-Barrier in the Construction of Compressed Suffix Arrays</title>
    <feedworld_mtime>1624752000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kempa:Dominik.html">Dominik Kempa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kociumaka:Tomasz.html">Tomasz Kociumaka</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.12725">PDF</a><br/><b>Abstract: </b>The suffix array, describing the lexicographic order of suffixes of a given
text, is the central data structure in string algorithms. The suffix array of a
length-$n$ text uses $\Theta(n \log n)$ bits, which is prohibitive in many
applications. To address this, Grossi and Vitter [STOC 2000] and,
independently, Ferragina and Manzini [FOCS 2000] introduced space-efficient
versions of the suffix array, known as the compressed suffix array (CSA) and
the FM-index. For a length-$n$ text over an alphabet of size $\sigma$, these
data structures use only $O(n \log \sigma)$ bits. Immediately after their
discovery, they almost completely replaced plain suffix arrays in practical
applications, and a race started to develop efficient construction procedures.
Yet, after more than 20 years, even for $\sigma=2$, the fastest algorithm
remains stuck at $O(n)$ time [Hon et al., FOCS 2003], which is slower by a
$\Theta(\log n)$ factor than the lower bound of $\Omega(n / \log n)$ (following
simply from the necessity to read the entire input). We break this
long-standing barrier with a new data structure that takes $O(n \log \sigma)$
bits, answers suffix array queries in $O(\log^{\epsilon} n)$ time, and can be
constructed in $O(n\log \sigma / \sqrt{\log n})$ time using $O(n\log \sigma)$
bits of space. Our result is based on several new insights into the recently
developed notion of string synchronizing sets [STOC 2019]. In particular,
compared to their previous applications, we eliminate orthogonal range queries,
replacing them with new queries that we dub prefix rank and prefix selection
queries. As a further demonstration of our techniques, we present a new
pattern-matching index that simultaneously minimizes the construction time and
the query time among all known compact indexes (i.e., those using $O(n \log
\sigma)$ bits).
</p></div>
    </summary>
    <updated>2021-06-27T22:42:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.12710</id>
    <link href="http://arxiv.org/abs/2106.12710" rel="alternate" type="text/html"/>
    <title>Certifying solution geometry in random CSPs: counts, clusters and balance</title>
    <feedworld_mtime>1624752000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hsieh:Jun=Ting.html">Jun-Ting Hsieh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mohanty:Sidhanth.html">Sidhanth Mohanty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Jeff.html">Jeff Xu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.12710">PDF</a><br/><b>Abstract: </b>An active topic in the study of random constraint satisfaction problems
(CSPs) is the geometry of the space of satisfying or almost satisfying
assignments as the function of the density, for which a precise landscape of
predictions has been made via statistical physics-based heuristics. In
parallel, there has been a recent flurry of work on refuting random constraint
satisfaction problems, via nailing refutation thresholds for spectral and
semidefinite programming-based algorithms, and also on counting solutions to
CSPs. Inspired by this, the starting point for our work is the following
question: what does the solution space for a random CSP look like to an
efficient algorithm?
</p>
<p>In pursuit of this inquiry, we focus on the following problems about random
Boolean CSPs at the densities where they are unsatisfiable but no refutation
algorithm is known.
</p>
<p>1. Counts. For every Boolean CSP we give algorithms that with high
probability certify a subexponential upper bound on the number of solutions. We
also give algorithms to certify a bound on the number of large cuts in a
Gaussian-weighted graph, and the number of large independent sets in a random
$d$-regular graph.
</p>
<p>2. Clusters. For Boolean $3$CSPs we give algorithms that with high
probability certify an upper bound on the number of clusters of solutions.
</p>
<p>3. Balance. We also give algorithms that with high probability certify that
there are no "unbalanced" solutions, i.e., solutions where the fraction of
$+1$s deviates significantly from $50\%$.
</p>
<p>Finally, we also provide hardness evidence suggesting that our algorithms for
counting are optimal.
</p></div>
    </summary>
    <updated>2021-06-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2106.12655</id>
    <link href="http://arxiv.org/abs/2106.12655" rel="alternate" type="text/html"/>
    <title>Fast Linking Numbers for Topology Verification of Loopy Structures</title>
    <feedworld_mtime>1624752000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qu:Ante.html">Ante Qu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/James:Doug_L=.html">Doug L. James</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2106.12655">PDF</a><br/><b>Abstract: </b>It is increasingly common to model, simulate, and process complex materials
based on loopy structures, such as in yarn-level cloth garments, which possess
topological constraints between inter-looping curves. While the input model may
satisfy specific topological linkages between pairs of closed loops, subsequent
processing may violate those topological conditions. In this paper, we explore
a family of methods for efficiently computing and verifying linking numbers
between closed curves, and apply these to applications in geometry processing,
animation, and simulation, so as to verify that topological invariants are
preserved during and after processing of the input models. Our method has three
stages: (1) we identify potentially interacting loop-loop pairs, then (2)
carefully discretize each loop's spline curves into line segments so as to
enable (3) efficient linking number evaluation using accelerated kernels based
on either counting projected segment-segment crossings, or by evaluating the
Gauss linking integral using direct or fast summation methods (Barnes-Hut or
fast multipole methods). We evaluate CPU and GPU implementations of these
methods on a suite of test problems, including yarn-level cloth and chainmail,
that involve significant processing: physics-based relaxation and animation,
user-modeled deformations, curve compression and reparameterization. We show
that topology errors can be efficiently identified to enable more robust
processing of loopy structures.
</p></div>
    </summary>
    <updated>2021-06-27T22:48:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/089</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/089" rel="alternate" type="text/html"/>
    <title>TR21-089 |  A Relativization Perspective on Meta-Complexity | 

	Rahul Santhanam, 

	Hanlin Ren</title>
    <summary>Meta-complexity studies the complexity of computational problems about complexity theory, such as the Minimum Circuit Size Problem (MCSP) and its variants. We show that a relativization barrier applies to many important open questions in meta-complexity. We give relativized worlds where:

* MCSP can be solved in deterministic polynomial time, but the search version of MCSP cannot be solved in deterministic polynomial time, even approximately. In contrast, Carmosino, Impagliazzo, Kabanets, Kolokolova [CCC'16] gave a randomized approximate search-to-decision reduction for MCSP with a relativizing proof.

* The complexities of MCSP[2^{n/2}] and MCSP[2^{n/4}] are different, in both worst-case and average-case settings. Thus the complexity of MCSP is not "robust" to the choice of the size function.

* Levin's time-bounded Kolmogorov complexity Kt(x) can be approximated to a factor (2+epsilon) in polynomial time, for any epsilon &gt; 0.

* Natural proofs do not exist, and neither do auxiliary-input one-way functions. In contrast, Santhanam [ITCS'20] gave a relativizing proof that the non-existence of natural proofs implies the existence of one-way functions under a conjecture about optimal hitting sets.

* DistNP does not reduce to GapMINKT by a family of "robust" reductions. This presents a technical barrier for solving a question of Hirahara [FOCS'20].</summary>
    <updated>2021-06-25T15:28:02Z</updated>
    <published>2021-06-25T15:28:02Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-28T03:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8156</id>
    <link href="https://windowsontheory.org/2021/06/25/stoc-feedback-and-tcs-wikipedia-guest-post-by-clement-canonne/" rel="alternate" type="text/html"/>
    <title>STOC feedback and TCS Wikipedia (guest post by Clément Canonne )</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The 53rd Annual ACM Symposium on Theory of Computing (STOC’21) concludes today, after 5 days of action-packed, Gather-power talks, workshops, plenary talks, and posters. A huge thank you to all volunteers, organizers, speakers, and attendees, who helped make this virtual conference a success! We would like to ask for your feedback on the conference. Whether … <a class="more-link" href="https://windowsontheory.org/2021/06/25/stoc-feedback-and-tcs-wikipedia-guest-post-by-clement-canonne/">Continue reading <span class="screen-reader-text">STOC feedback and TCS Wikipedia (guest post by Clément Canonne )</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The 53rd Annual ACM <a href="http://acm-stoc.org/stoc2021/">Symposium on Theory of  Computing </a> (STOC’21) concludes today, after 5 days of action-packed, Gather-power talks, workshops, plenary talks, and posters. A huge thank you to all volunteers, organizers, speakers, and attendees, who helped make this virtual conference a success!</p>



<p>We would like to ask for your feedback on the conference. Whether you attended this virtual edition of STOC or not, please fill <a href="https://forms.gle/r1A4zCS6umbhZTrMA">this form </a> to help us make future TheoryFests even better!</p>



<p>Moreover, as part of one of the STOC social event (and in line with initiatives in 2017 by Shuchi Chawla and the <a href="https://thmatters.wordpress.com/2019/06/11/wikipedia-edit-a-thon-at-stoc19/">Edit-a-Thon from STOC 2019</a> , a spreadsheet aiming to crowdsource which TCS Wikipedia pages need improvement/creation has been set up:<br/><a href="https://docs.google.com/spreadsheets/d/1ZswaweUvsjVHnItMBnIxaiBZxcs-gEaUoODFoH4FGms/edit" rel="noreferrer noopener" target="_blank">https://docs.google.com/spreadsheets/d/1ZswaweUvsjVHnItMBnIxaiBZxcs-gEaUoODFoH4FGms/edit</a>. </p>



<p>Feel free to use or edit it in view of improving the TCS coverage in Wikipedia.</p>



<p>Clément Canonne (on behalf of the STOC and TheoryFest organizers)</p></div>
    </content>
    <updated>2021-06-25T14:38:34Z</updated>
    <published>2021-06-25T14:38:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-06-28T03:21:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/06/25/postdoc-at-uc-san-diego-apply-by-july-1-2021/</id>
    <link href="https://cstheory-jobs.org/2021/06/25/postdoc-at-uc-san-diego-apply-by-july-1-2021/" rel="alternate" type="text/html"/>
    <title>postdoc at UC San Diego (apply by July 1, 2021)</title>
    <summary>The UCSD CSE Fellows Program is intended to support exceptional postdoctoral researchers in computer science. The program seeks to recruit 1-3 fellows a year for a two year postdoctoral appointment working alongside a UCSD CSE faculty mentor. Website: https://cse.ucsd.edu/research/uc-san-diego-cse-fellows-program Email: shachar.lovett@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The UCSD CSE Fellows Program is intended to support exceptional postdoctoral researchers in computer science. The program seeks to recruit 1-3 fellows a year for a two year postdoctoral appointment working alongside a UCSD CSE faculty mentor.</p>
<p>Website: <a href="https://cse.ucsd.edu/research/uc-san-diego-cse-fellows-program">https://cse.ucsd.edu/research/uc-san-diego-cse-fellows-program</a><br/>
Email: shachar.lovett@gmail.com</p></div>
    </content>
    <updated>2021-06-25T14:08:42Z</updated>
    <published>2021-06-25T14:08:42Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-06-28T03:20:53Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-4913288096208854339</id>
    <link href="http://processalgebra.blogspot.com/feeds/4913288096208854339/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=4913288096208854339" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/4913288096208854339" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/4913288096208854339" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/06/the-detecter-runtime-verification-tool.html" rel="alternate" type="text/html"/>
    <title>The detectEr runtime-verification tool for Erlang programs</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Thanks to the huge amount of excellent work done by <a href="https://duncanatt.github.io/" target="_blank">Duncan Paul Attard</a> and <a href="http://staff.um.edu.mt/afra1/" target="_blank">Adrian Francalanza</a>, we now have a tutorial on detectEr that some of you might want to check out. See <a href="https://duncanatt.github.io/detecter/index.html">this web page</a> for all the material, tool download and links to the videos of the tutorial Duncan delivered at the <a href="https://www.discotec.org/2021/tutorials" target="_blank">DisCoTec 2021 Tutorial Day</a>. </p><p>detectEr is a runtime verification tool for asynchronous component systems that run on the <a href="https://blog.erlang.org/a-brief-BEAM-primer/" target="_blank">Erlang Virtual Machine</a>. It also supports monitoring systems that can execute outside of the EVM, so long as these can produce traces that are formatted in a way that is parsable by detectEr. The tool itself is developed in <a href="https://www.erlang.org/" target="_blank">Erlang</a>, and is the product of five years of theoretical and practical development. (Erlang is a programming language used to build massively scalable soft real-time systems with requirements on high availability.)  </p> <p>Enjoy!</p></div>
    </content>
    <updated>2021-06-24T08:16:00Z</updated>
    <published>2021-06-24T08:16:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-06-24T08:16:51Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6958131418382007261</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6958131418382007261/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6958131418382007261" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6958131418382007261" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html" rel="alternate" type="text/html"/>
    <title>I went to the ``debate'' about Program Verif and the Lipton-Demillo-Perlis paper</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>On Thursday June 17 I went to (on zoom- does that need to be added anymore?)</p><p><i>A Debate on Program Correctness</i></p><p>There was no subtitle but it could have been:</p><p>Have the points made in <i>Social Processes and Proofs of Theorems and Programs</i> by DeMillo, Lipton, Perlis, survived the test of time ? (Spoiler Alert: Yes.)</p><p>I found out about it from the Lipton-Regan blog <a href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/">here</a></p><p>The debaters were Richard DeMillo and Richard Lipton and the moderator was Harry Lewis (Alan Perlis passed away in 1990).  Calling it a debate is not correct since DeMillo and Lipton (and Lewis) all agree. (DeMillo and Lipton even have the same first name!)  The DLP paper is in Harry Lewis's collection<i> Ideas that created the future.</i>  The event  should have been advertised as a discussion. However, it was a good discussion so this is not a complaint.</p><p>Here are some things that came out of the discussion.</p><p>1) The main topic was the 1979 DeMillo-Lipton-Perlis paper (see <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">here</a>) that gave arguments why Proofs of Program correctness could not work.</p><p>An all-to-brief summary of the DLP paper: Some researchers are trying to set up frameworks for doing proofs that programs are correct, analogous to the certainty  we get with a proof of a Theorem in Mathematics. But proofs in Mathematics are, in reality, NOT that rigorous. Often details are left out or left to the reader. This is fine for mathematics (more on that later) but unacceptable for programs which need rather precise and rigorous proofs.</p><p>How do theorems in mathematics really get verified? By having enough people look at them and make sure they match intuitions, what DLP call <i>A Social Process</i>.  (NOTE FROM BILL: Papers that are not important do not get looked at so there may well be errors.)</p><p>2) The notion of proving-programs-correct was very seductive; however, the people who were trying to do this had a blind spot about how the analogy of proving-programs-correct and proving-theorem-correct differ.  In particular, a program is rather complicated and even stating carefully what you want to prove is difficult. By contrast, for most math statements, what you want to prove is clear. Note also that a program has lots of code (far more now than when DLP was written) and so much can happen that you cannot account for.</p><p>3) The DLP paper had a large effect on the field of program verification.  Funding for it was reduced and students were discouraged from going into it.</p><p>4) When DLP appeared DeMillo and Lipton were pre-tenure. Hence it took lots of courage to publish it. Alan Perlis had tenure and had already won a Turing award.  This did give DeMillo and Lipton some cover; however, it still took courage.</p><div><div>5) How did the Program Verification  Community deal with the objections in DLP?  DeMillo said that he looked at a large set of papers in the field, and very few even mentioned DLP. He recommends reading the book <i>Mechanizing Proof: Computing, Risk, and Trust by David McKenzie </i>see <a href="https://www.amazon.com/exec/obidos/ASIN/0262632950">here</a>.</div><div><br/></div><div>6) So how can we be more certain that programs are correct?</div><div><br/></div><div>a) Testing.</div><div>b) Modularize and test. Fix errors. Modularize  and test. Fix errors...</div><div>c) Try to isolate side effects.</div><div>d) More testing.</div><div><br/></div><div>Some point to Model Checking, which could be considered very sophisticated testing, but that's used to verify circuits and perhaps low-level code, not programs. Model checking is a success story and note that Ed Clark, E. Allen Emerson, and Joseph Sifakis shared a (well deserved) Turing award for this work. But see next note.</div><div><br/></div><div>6.5) An audience member pointed out that Program Verification people have won several Turing Awards</div><div><br/></div><div>Dijkstra 1972</div><div><br/></div><div>Floyd 1978 </div><div><br/></div><div>Hoare 1980</div><div><br/></div><div>Pnueli 1996</div><div><br/></div><div>(Are there more?) </div><div><br/></div><div>so the field is alive and healthy. DeMillo responded that prizes for academic research are a poor measure of  success. </div><div><br/></div><div>7) Can computers themselves help with proofs of correctness? That is the only hope; however, there are scaling problems.</div><div><br/></div><div>8) When DLP was written a program with 100,000 lines of code was considered large. Now we have programs with millions of lines of code. And now we have more concurrency. So the lessons of the DLP paper are probably more relevant now then they were then.</div><div><br/></div><div>9) Since Program Verification does not seem to be used, how come we don't have a Software crisis?</div><div><br/></div><div>a) We do! The Q+A mechanism at the meeting was terrible. </div><div>b) We do! FILL IN YOUR OWN FAVORITE STORY OF BAD SOFTWARE.</div><div>c) See the answer to question 6.</div><div><br/></div><div>10) SPECS are a problem. Tony Hoare once gave a talk where he proves that a program sorted correctly and then pointed out that if the program just output 0,...0 that would have also satisfied the SPEC since all that was required was that the output be sorted, not the (overlooked!) requirement that it be the same numbers as the input. So one needs to be careful!</div><div><br/></div><div>11) Despite being a leader in the field, Tony Hoare has come to see the limitations of the Proofing-programs-correct approach to Software Verification.  His paper An Axiomatic basis for Computer Programming (1969)  (which is also in Harry Lewis's collection <i>Ideas that Created the Future</i>).</div><div>Much later, commenting on the paper,  Hoare says the following:</div><div><br/></div><div>Ten years ago, researchers into formal methods (and I was the most mistaken among them) predicted that the programming world would embrace with gratitude every assistance promised by formalization to solve the problems of reliability that arise when programs get large and more safety-critical. Programs have now got very large and very critical--well beyond the scale which can be comfortably tackled by formal methods. There have been many problems and failures, but these have nearly always been attributable to inadequate analysis of requirements or inadequate management control. It has turned out that the world just does not suffer significantly from the kind of problem that our research was originally intended to solve.'</div><div><br/></div></div><div><div>12) Richard Lipton told a story where he showed that the program in question satisfied the SPEC, but the SPEC was a tautology that any program would satisfy.  Again, one needs to be careful!</div><div><br/></div><div>13) The test of time: Verifying large scale programs does not seem to be common in industry. Is industrial adaptation a fair measure? </div><div><br/></div><div>14) Harry Lewis's  book <i>Ideas that created the future</i> collects up, edits, and comments on 46 important papers in Computer Science (I reviewed it in the issue of SIGACT News that is in your mailbox---I will blog about it at a later time.) There are several papers in it about program verification, including DLP, Hoare's paper, and three papers by Dijkstra.</div><div><br/></div><div>a) When Harry discussed including DLP some people said `You're going to include that!  Its a polemic, not a paper!'</div><div><br/></div><div>b) When Harry teaches a course from this book (it must be an awesome class!) and asks the students at the end which papers they learned the most from, the top two are an excerpt from Fred Brooks <i>The</i> <i>Mythical Man Month</i> (see my post on Brook's work <a href="https://blog.computationalcomplexity.org/2021/05/the-mythical-man-month-hen-day-and-cat.html">here</a> ) and DLP.</div><div><br/></div><div>c) I am hoping that this is just one of 46 talks with authors of the papers in his book.  I look forward to his interview with Aristotle, Leibnitz, Boole, Turing, ...</div></div><div><br/></div></div>
    </content>
    <updated>2021-06-24T03:37:00Z</updated>
    <published>2021-06-24T03:37:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-06-28T03:00:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5549</id>
    <link href="https://www.scottaaronson.com/blog/?p=5549" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5549#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5549" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">STOC’2021 and BosonSampling</title>
    <summary xml:lang="en-US">Happy birthday to Alan Turing! This week I’m participating virtually in STOC’2021, which today had a celebration of the 50th anniversary of NP-completeness (featuring Steve Cook, Richard Karp, Leonid Levin, Christos Papadimitriou, and Avi Wigderson), and which tomorrow will have a day’s worth of quantum computing content, including a tutorial on MIP*=RE, two quantum sessions, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Happy birthday to Alan Turing!</p>



<p>This week I’m participating virtually in <a href="http://acm-stoc.org/stoc2021/">STOC’2021</a>, which today had a celebration of the 50th anniversary of NP-completeness (featuring Steve Cook, Richard Karp, Leonid Levin, Christos Papadimitriou, and Avi Wigderson), and which tomorrow will have a day’s worth of quantum computing content, including a tutorial on MIP*=RE, two quantum sessions, and an invited talk on quantum supremacy by John Martinis.  I confess that I’m not a fan of GatherTown, the platform being used for STOC.  Basically, you get a little avatar who wanders around a virtual hotel lobby and enters sessions—but it seems to reproduce all of the frustrating and annoying parts of experience without any of the good parts.</p>



<p>Ah!  But I got the surprising news that Alex Arkhipov and I are among the winners of STOC’s first-ever <a href="https://sigact.org/prizes/stoc_tot.html">“Test of Time Award,”</a> for our <a href="https://www.scottaaronson.com/papers/optics.pdf">paper on BosonSampling</a>.  It feels strange to win a “Test of Time” award for work that we did in 2011, which still seems like yesterday to me.  All the more since the experimental status and prospects of quantum supremacy via BosonSampling are still very much live, unresolved questions.</p>



<p>Speaking of which: on Monday, Alexey Rubtsov, of the Skolkovo Institute in Moscow, gave a talk for our quantum information group meeting at UT, about his <a href="https://arxiv.org/abs/2106.01445">recent work with Popova</a> on classically simulating Gaussian BosonSampling.  From the talk, I learned something extremely important.  I had imagined that their simulation must take advantage of the high rate of photon loss in actual experiments (like the <a href="https://www.scottaaronson.com/blog/?p=5159">USTC experiment</a> from late 2020), because how else are you going to simulate BosonSampling efficiently?  But Rubtsov explained that that’s not how it works at all.  While their algorithm is heuristic and remains to be rigorously analyzed, numerical studies suggest that it works even with <em>no</em> photon losses or other errors.  Having said that, their algorithm works:</p>



<ul><li>only for Gaussian BosonSampling, not Fock-state BosonSampling (as Arkhipov and I had originally proposed),</li><li>only for threshold detectors, not photon-counting detectors, and</li><li>only for a small number of modes (say, linear in the number of photons), not for a large number of modes (say, quadratic in the number of photons) as in the original proposal.</li></ul>



<p>So, bottom line, it now looks like the USTC experiment, amazing engineering achievement though it was, is not hard to spoof with a classical computer.  If so, this is because of multiple ways in which the experiment differed from my and Arkhipov’s original theoretical proposal.  We know exactly what those ways are—indeed, you can find them in my earlier blog posts on the subject—and hopefully they can be addressed in future experiments.  All in all, then, we’re left with a powerful demonstration of the continuing relevance of formal hardness reductions, and the danger of replacing them with intuitions and “well, it still seems hard to <em>me</em>.”  So I hope the committee won’t rescind my and Arkhipov’s Test of Time Award based on these developments in the past couple weeks!</p></div>
    </content>
    <updated>2021-06-23T16:44:51Z</updated>
    <published>2021-06-23T16:44:51Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-06-23T16:44:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1325</id>
    <link href="https://thmatters.wordpress.com/2021/06/23/tcs-visioning-2020-report-and-slides/" rel="alternate" type="text/html"/>
    <title>TCS Visioning 2020 report and slides</title>
    <summary>In July 2020, the CATCS organized a visioning workshop. We are happy to announce the release of a report and posters based on this workshop. Material produced from this workshop is available and free to use by any member of the TCS community. We gratefully acknowledge financial as well as organizational support by the SIGACT […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In July 2020, the <a href="https://thmatters.wordpress.com/catcs/">CATCS</a> organized a <a href="https://thmatters.wordpress.com/2020/06/05/tcs-visioning-workshop-call-for-participation/">visioning workshop</a>. We are happy to announce the release of a report and posters based on this workshop. Material produced from this workshop is available and free to use by any member of the TCS community. We gratefully acknowledge financial as well as organizational support by the <a href="https://www.sigact.org/">SIGACT</a> and <a href="https://cra.org/ccc/">CCC</a> for this activity.</p>



<p>We are planning a follow-up event for disseminating the report and posters to funding agencies in the next few months. Details are forthcoming.</p>



<p>TCS Visioning Full Report: <a href="https://thmatters.files.wordpress.com/2021/06/visioning-report.pdf">here</a></p>



<p>Short Report (CCC Quadrennial paper): <a href="https://cra.org/ccc/wp-content/uploads/sites/2/2020/10/Theoretical-Computer-Science_.pdf">here</a></p>



<p>TCS Visioning Slides: <a href="https://thmatters.files.wordpress.com/2021/06/visioning-slides.pptx">in PPT</a> and <a href="https://thmatters.files.wordpress.com/2021/06/visioning-slides-1.pdf">in PDF</a></p></div>
    </content>
    <updated>2021-06-23T16:21:23Z</updated>
    <published>2021-06-23T16:21:23Z</published>
    <category term="reports"/>
    <category term="Visioning"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2021-06-28T03:21:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/088</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/088" rel="alternate" type="text/html"/>
    <title>TR21-088 |  Open Problems in Property Testing of Graphs | 

	Oded Goldreich</title>
    <summary>We briefly discuss a few open problems in the study of various models of testing graph properties, focusing on the query complexity of the various tasks. In the dense graph model, we discuss several open problems, including:

* Determining the complexity of testing triangle-freeness.
* Characterizing the class of properties that are testable within extremely low complexity. 

Turning to the bounded-degree graph model, we discuss several open problems, including:

* Characterizing the class of properties that are testable within size-oblivious complexity.
* Determining the complexity of graph isomorphism. 
In each of the foregoing models, we also discuss a favorite open problem that was recently resolved. Lastly, we discuss the vast lack of knowledge with respect to testing graph properties in the general graph model.</summary>
    <updated>2021-06-23T13:30:16Z</updated>
    <published>2021-06-23T13:30:16Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-28T03:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/087</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/087" rel="alternate" type="text/html"/>
    <title>TR21-087 |  Eliminating Intermediate Measurements using Pseudorandom Generators | 

	Uma Girish, 

	Ran Raz</title>
    <summary>We show that quantum algorithms of time T and space $S \ge \log T$ with intermediate measurements can be simulated by quantum algorithms of time $T\cdot \mathrm{poly}(S)$ and space $O(S\cdot \log T)$ without intermediate measurements. The best simulations prior to this work required either $\Omega(T)$ space (by the deferred measurement principle) or $\mathrm{poly}(2^S)$ time [FR21, GRZ21]. Our result is thus a time-efficient and space-efficient simulation of algorithms with intermediate measurements by algorithms without intermediate measurements.

To prove our result, we study pseudorandom generators for quantum space-bounded algorithms. We show that (an instance of) the INW pseudorandom generator for classical space-bounded algorithms [INW94] also fools quantum space-bounded algorithms. More precisely, we show that for quantum space-bounded algorithms that have access to a read-once tape consisting of random bits, the final state of the algorithm when the random bits are drawn from the uniform distribution is nearly identical to the final state when the random bits are drawn using the INW pseudorandom generator.</summary>
    <updated>2021-06-22T16:04:22Z</updated>
    <published>2021-06-22T16:04:22Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-28T03:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/086</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/086" rel="alternate" type="text/html"/>
    <title>TR21-086 |  Linear Space Streaming Lower Bounds for Approximating CSPs | 

	Chi-Ning  Chou, 

	Alexander Golovnev, 

	Madhu Sudan, 

	Ameya Velingker, 

	Santhoshini Velusamy</title>
    <summary>We consider the approximability of constraint satisfaction problems in the streaming setting. For every constraint satisfaction problem (CSP) on $n$ variables taking values in $\{0,\ldots,q-1\}$, we prove that improving over the trivial approximability by a factor of $q$ requires $\Omega(n)$ space even on instances with $O(n)$ constraints. We also identify a broad subclass of problems for which any improvement over the trivial approximability requires $\Omega(n)$ space. The key technical core is an optimal, $q^{-(k-1)}$-inapproximability for the case where every constraint is given by a system of $k-1$ linear equations $\bmod\; q$ over $k$ variables. Prior to our work, no such hardness was known for an approximation factor less than $1/2$ for any CSP. Our work builds on and extends the work of Kapralov and Krachun (Proc. STOC 2019) who showed a linear lower bound on any non-trivial approximation of the max cut in graphs. This corresponds roughly to the case of  Max $k$-LIN-$\bmod\; q$ with $k=q=2$. Each one of the extensions provides non-trivial technical challenges that we overcome in this work.</summary>
    <updated>2021-06-22T14:47:13Z</updated>
    <published>2021-06-22T14:47:13Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-28T03:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/</id>
    <link href="https://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/" rel="alternate" type="text/html"/>
    <title>Annual Symposium on Combinatorial Pattern Matching (summer school + conference))</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 4, 2021 – July 7, 2021 Wrocław, Poland and online https://cpm2021.ii.uni.wroc.pl/ The Annual Symposium on Combinatorial Pattern Matching (CPM) has by now over 30 years of tradition and is considered to be the leading conference for the community working on Stringology. The objective of the annual CPM meetings is to provide an international forum … <a class="more-link" href="https://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/">Continue reading <span class="screen-reader-text">Annual Symposium on Combinatorial Pattern Matching (summer school + conference))</span></a></div>
    </summary>
    <updated>2021-06-22T08:09:22Z</updated>
    <published>2021-06-22T08:09:22Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-06-28T03:22:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/085</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/085" rel="alternate" type="text/html"/>
    <title>TR21-085 |  The Final Nail in the Coffin of Statistically-Secure Obfuscator. | 

	Ilya Volkovich</title>
    <summary>We present an elementary, self-contained proof of the result of Goldwasser and Rothblum [GR07] that the existence of a (perfect) statistically secure obfuscator implies a collapse of the polynomial hierarchy. In fact, we show that an existence of a weaker object implies a somewhat stronger statement. In addition, we extend the result of [GR07] to the case of imperfect statistically secure obfuscator.</summary>
    <updated>2021-06-21T16:17:33Z</updated>
    <published>2021-06-21T16:17:33Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-28T03:20:40Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2146860754670937912</id>
    <link href="https://blog.computationalcomplexity.org/feeds/2146860754670937912/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/collusion-rings.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2146860754670937912" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2146860754670937912" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/collusion-rings.html" rel="alternate" type="text/html"/>
    <title>Collusion Rings</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><i>Grigory Yaroslavtsev now maintains the <a href="https://docs.google.com/spreadsheets/d/1kobGLgx5QgJb9AvDENg5c8wEbwBUUkVlMTsBV9quEEY/edit?usp=sharing">Theory Jobs Spreadsheet</a>, a crowd-sourced listing of where your favorite CS theorists have taken new positions. Read his <a href="http://grigory.us/blog/theory-jobs-2021/">blog post</a> for more info.</i></p><p><i>Michael Littman wrote a <a href="https://cacm.acm.org/magazines/2021/6/252840-collusion-rings-threaten-the-integrity-of-computer-science-research/fulltext">CACM Viewpoint</a> on collusion rings designed to tilt the odds into getting papers into conferences. See also <a href="https://medium.com/@tnvijayk/potential-organized-fraud-in-acm-ieee-computer-architecture-conferences-ccd61169370d">this post</a> by T. N. Vijaykumar. I started to write a post on the topic but couldn't get it going so I asked the AI algorithm GPT-3 to write a guest post. Here is its story.</i></p><hr/><p>I have been part of a collusion ring and I know how it feels. I was a postdoc at the University of California, Berkeley, in the early 2000s when I first learned about the practice of “paper rings.” A paper ring is a group of researchers who agree to submit each other’s papers to conferences without disclosing this fact to the conference organizers. The idea is that if you have several submissions from the same conference, your chances of getting accepted are higher. The pressure to be part of such a ring can be immense. You are told that you need to publish or perish and that you need to get papers into top conferences. If you don’t play along, you will be left behind. I was lucky enough to have a mentor who warned me against being part of such rings. He told me that if I did not want my name on a paper, then I should not submit it to a conference. He also told me that if I did not want to be part of a paper ring, then I should not submit papers for others in the ring. He said that he would help me with my own research and that he would help me get my own papers into conferences. He was right: My own papers were accepted into conferences and they were published in journals.</p><p>I am now an associate professor at the University of Massachusetts Amherst and I have been on the other side of this issue as well. In 2012, I was asked by an editor at a top computer science conference to review a paper submitted by one of my students. The student had been part of a paper ring and had submitted his own paper along with those submitted by his collaborators in the ring. The editor told me that he had rejected the student’s paper because it was too similar to other papers submitted by members of the ring. However, he wanted me to review it so that he could reconsider accepting it for publication if I recommended doing so.</p><p>What should I do? Should I tell him about the paper ring? Should I recommend accepting the paper? Should I tell him that there is no way he should accept it? Should I tell him that he should reject it but then publish it anyway? What would you do? I decided not to tell him about the paper ring and not to recommend accepting the paper for publication. Instead, I told him that he should reject it but then publish it anyway because this would send a strong message about what we expect from our students and what we think about such practices. The editor agreed with my decision and published the student’s paper along with two other papers submitted by members of the same paper ring.</p><p>The student received his PhD degree but did not get any credit for his work on these three papers because they were considered as part of his dissertation work when he defended his thesis later that year. In retrospect, I think that my decision was wrong because it allowed these students to continue their unethical behavior without any consequences for them or their advisors (who were also part of this ring). But at least in this case, there were no negative consequences for anyone involved in this incident except for these students who did not get credit for their work on these three papers because they were considered as part of their dissertation work when they defended their thesis later that year.</p></div>
    </content>
    <updated>2021-06-21T14:06:00Z</updated>
    <published>2021-06-21T14:06:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-06-28T03:00:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/06/21/postdoc-at-bar-ilan-university-apply-by-july-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/06/21/postdoc-at-bar-ilan-university-apply-by-july-15-2021/" rel="alternate" type="text/html"/>
    <title>postdoc at Bar-Ilan University (apply by July 15, 2021)</title>
    <summary>A postdoc position is available at the Network Computation group at Bar-Ilan University (Israel). We invite applicants interested in the fields of distributed, online, and approximated computing on graph and network problems; Algorithms in hardware; circuit complexity; communication complexity; coding theory and distributed coding schemes. Website: https://docs.google.com/document/d/1rCXAByE0J8VDLSXXq4p5a6LDVkBBRaRBrRQhOiCuo7U/edit Email: moti.medina@biu.ac.il</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A postdoc position is available at the Network Computation group<br/>
at Bar-Ilan University (Israel). We invite applicants interested in the fields of distributed, online, and approximated computing on graph and network problems; Algorithms in hardware; circuit complexity; communication complexity; coding theory and distributed coding schemes.</p>
<p>Website: <a href="https://docs.google.com/document/d/1rCXAByE0J8VDLSXXq4p5a6LDVkBBRaRBrRQhOiCuo7U/edit">https://docs.google.com/document/d/1rCXAByE0J8VDLSXXq4p5a6LDVkBBRaRBrRQhOiCuo7U/edit</a><br/>
Email: moti.medina@biu.ac.il</p></div>
    </content>
    <updated>2021-06-21T12:38:26Z</updated>
    <published>2021-06-21T12:38:26Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-06-28T03:20:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/084</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/084" rel="alternate" type="text/html"/>
    <title>TR21-084 |  PCPs and Instance Compression from a Cryptographic Lens | 

	Ron Rothblum, 

	Liron Bronfman</title>
    <summary>Modern cryptography fundamentally relies on the assumption that the adversary trying to break the scheme is computationally bounded. This assumption lets us construct cryptographic protocols and primitives that are known to be impossible otherwise. In this work we explore the effect of bounding the adversary's power in other information theoretic proof-systems and show how to use this assumption to bypass impossibility results.

We first consider the question of constructing succinct PCPs. These are PCPs whose length is polynomial only in the length of the original NP witness (in contrast to standard PCPs whose length is proportional to the non-deterministic verification time). 
Unfortunately, succinct PCPs are known to be impossible to construct under standard complexity assumptions. Assuming the sub-exponential hardness of the learning with errors (LWE) problem, we construct succinct probabilistically checkable arguments or PCAs (Zimand 2001, Kalai and Raz 2009), which are PCPs in which soundness is guaranteed against efficiently generated false proofs. Our PCA construction is for every NP relation that can be verified by a small-depth circuit (e.g., SAT, clique, TSP, etc.) and in contrast to prior work is publicly verifiable and has constant query complexity. Curiously, we also show, as a proof-of-concept, that such publicly-verifiable PCAs can be used to derive hardness of approximation results.

Second, we consider the notion of Instance Compression (Harnik and Naor, 2006). An instance compression scheme lets one compress, for example, a CNF formula $\varphi$ on $m$ variables and $n \gg m$ clauses to a new formula $\varphi'$ with only $poly(m)$ clauses, so that $\varphi$ is satisfiable if and only if $\varphi'$ is satisfiable. Instance compression has been shown to be closely related to succinct PCPs and is similarly highly unlikely to exist. We introduce a computational analog of instance compression in which we require that if $\varphi$ is unsatisfiable then $\varphi'$ is effectively unsatisfiable, in the sense that it is computationally infeasible to find a satisfying assignment for $\varphi'$ (although such an assignment may exist). Assuming the same sub-exponential LWE assumption, we construct such computational instance compression schemes for every bounded-depth NP relation. As an application, this lets one compress $k$ formulas $\phi_1,\dots,\phi_k$ into a single short formula $\phi$ that is effectively satisfiable if and only if at least one of the original formulas was satisfiable.</summary>
    <updated>2021-06-21T10:53:41Z</updated>
    <published>2021-06-21T10:53:41Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-28T03:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://kamathematics.wordpress.com/?p=324</id>
    <link href="https://kamathematics.wordpress.com/2021/06/21/social-at-stoc-2021/" rel="alternate" type="text/html"/>
    <title>Social at STOC 2021</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I’ve been asked to pass along a message by Clément Canonne, social chair for STOC 2021. This STOC might have the best social program of any I’ve ever seen, either virtual or in-person, so be sure to check it out! STOC’21 is around the corner, starting tomorrow; [don’t forget to register](http://acm-stoc.org/stoc2021/), if you haven’t yet! … <a class="more-link" href="https://kamathematics.wordpress.com/2021/06/21/social-at-stoc-2021/">Continue reading<span class="screen-reader-text"> "Social at STOC 2021"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I’ve been asked to pass along a message by <a href="https://ccanonne.github.io/">Clément Canonne</a>, social chair for STOC 2021. This STOC might have the best social program of any I’ve ever seen, either virtual or in-person, so be sure to check it out!</p>



<hr class="wp-block-separator"/>



<p>STOC’21 is around the corner, starting tomorrow; [don’t forget to register](<a href="http://acm-stoc.org/stoc2021/">http://acm-stoc.org/stoc2021/</a>), if you haven’t yet! This year, the (virtual) conference will include several social activities (games, TCS trivia, mystery hunt…); among which, two “junior/senior lunches,” on Monday and Friday.<br/><br/>Those both will be held in the Gather space for STOC (<a href="http://acm-stoc.org/stoc2021/venue.html">http://acm-stoc.org/stoc2021/venue.html</a>), and — as in previous years — are the occasion for senior researchers in the field, broadly construed, to have an informal chat with students, postdocs, and junior faculty, answer their questions, discuss their research, and generally have a nice conversation.<br/><br/>If you are interested, don’t forget to sign up! This is done through the “feedback box” placed on the Information Desk in the Gather space’s Lobby, which gives access to a spreadsheet.<br/><br/>Hoping to see you at STOC!</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-325" src="https://kamathematics.files.wordpress.com/2021/06/image.png?w=778"/></figure></div>
    </content>
    <updated>2021-06-21T02:16:07Z</updated>
    <published>2021-06-21T02:16:07Z</published>
    <category term="Events"/>
    <author>
      <name>Gautam</name>
    </author>
    <source>
      <id>https://kamathematics.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://kamathematics.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://kamathematics.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://kamathematics.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://kamathematics.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Kamathematics</title>
      <updated>2021-06-28T03:22:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/083</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/083" rel="alternate" type="text/html"/>
    <title>TR21-083 |  Tight Space Complexity of the Coin Problem | 

	Mark Braverman, 

	Sumegha Garg, 

	Or Zamir</title>
    <summary>In the coin problem we are asked to distinguish, with probability at least $2/3$, between $n$ $i.i.d.$ coins which are heads with probability $\frac{1}{2}+\beta$ from ones which are heads with probability $\frac{1}{2}-\beta$. We are interested in the space complexity of the coin problem, corresponding to the width of a read-once branching program solving the problem. 

The coin problem becomes more difficult as $\beta$ becomes smaller. Statistically, it can be solved whenever $\beta = \Omega(n^{-1/2})$, using counting. It has been previously shown that for $\beta = O(n^{-1/2})$, counting is essentially optimal (equivalently, width $poly(n)$ is necessary [Braverman-Garg-Woodruff FOCS'20]). On the other hand, the coin problem only requires $O(\log n)$ width for $\beta&gt;n^{-c}$ for any constant $c&gt;\log_2(\sqrt{5}-1)\approx 0.306$ (following low-width simulation of AND-OR tree of [Valiant Journal of Algorithms'84]).

In this paper, we close the gap between the bounds, showing a tight threshold between the values of $\beta=n^{-c}$ where $O(\log n)$ width suffices and the regime where $poly(n)$ width is needed, with a transition at $c=1/3$. This gives a complete characterization (up to constant factors) of the memory complexity of solving the coin problem, for all values of bias $\beta$. 

We introduce new techniques in both bounds. For the upper bound, we give a construction based on recursive majority that does not require a memory stack of size $\log n$ bits. For the lower bound, we introduce new combinatorial techniques for analyzing progression of the success probabilities in read-once branching programs.</summary>
    <updated>2021-06-20T23:01:29Z</updated>
    <published>2021-06-20T23:01:29Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-28T03:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18926</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/" rel="alternate" type="text/html"/>
    <title>The Shape of This Summer</title>
    <summary>Looking farther than Father’s Day src Jordan Ellenberg is the author of the new book Shape: The Hidden Geometry of Information, Biology, Strategy, Democracy, and Everything Else. I just received it as a Father’s Day present. Today we convey some short musings on some ideas in the book. Ellenberg is both a biological father and […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Looking farther than Father’s Day</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/ellenbergbucksdec2019/" rel="attachment wp-att-18928"><img alt="" class="alignright wp-image-18928" height="170" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/EllenbergBucksDec2019.jpg?resize=127%2C170&amp;ssl=1" width="127"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://twitter.com/JSEllenberg/status/1209913559720837121">src</a></font></td>
</tr>
</tbody>
</table>
<p>Jordan Ellenberg is the author of the new <a href="https://www.penguinrandomhouse.com/books/612131/shape-by-jordan-ellenberg/9781984879059/">book</a> <em>Shape: The Hidden Geometry of Information, Biology, Strategy, Democracy, and Everything Else</em>. I just received it as a Father’s Day present.</p>
<p>
Today we convey some short musings on some ideas in the book.</p>
<p>
Ellenberg is both a biological father and a <em>Doktorvater</em>. Indeed, if we make a correct inference from his Mathematics Genealogy <a href="https://www.genealogy.math.ndsu.nodak.edu/id.php?id=31096">count</a> of “12 students and 13 descendants,” he must be a doctor-grandfather. His own advisor was Barry Mazur. Ellenberg holds the John D. MacArthur professorship in Mathematics at UW-Madison and a distinguished professorship named for former Wisconsin senator William Vilas. (We’ve had UW-Madison on our minds for over a month about something else, but various events including new kinds of chess-cheating cases have delayed the work needed to do it justice. While writing this paragraph, I have had a seconding role in a disqualification by an online chess platform of a player in a tournament played today.)</p>
<p>
I made my inference in the last paragraph by counting—I did not take time to click on his twelve students to verify which one has a graduated student. What strikes us about the new book is the aspect of making inferences <em>not</em> by counting.</p>
<p>
</p><p/><h2> Pandemic Phasing </h2><p/>
<p/><p>
Our picture above was <a href="https://twitter.com/JSEllenberg/status/1209913559720837121">tweeted</a> by Ellenberg on Christmas Day, 2019, a half-hour before the start of the Milwaukee Bucks <a href="https://www.basketball-reference.com/boxscores/201912250PHI.html">playing</a> at the Philadelphia 76ers. Notice that the seats behind him have not yet filled, as of course they did for a pre-pandemic holiday game. Last night, I had on the terrific Game 7 between the Bucks and the Brooklyn Nets while processing chess data. The stands at the Brooklyn Barclays Center were within 1,000 of full capacity, but there seemed to be some spaced-out sections nearest the court that would have been teeming usually.</p>
<p>
My wife and I ventured out two weeks ago to see the Toronto Blue Jays play in Buffalo’s <a href="https://www.milb.com/buffalo">Sahlen Field</a>, their home until the border with Canada reopens. Our stadium has increased the allowed capacity from 35% that day to 80% now, but our paper noted today that by month’s end it may be the only venue not allowing 100%. Will the return to large close-packed crowds be safe—will there be sufficient “herd immunity”—involves questions raised in two chapters of Ellenberg’s book. Here is a main idea—in my words, because I haven’t had time to actually <em>read</em> the book yet (and while writing this section, it seems I am having a primary role in another impending ban from the same overseas chess event).</p>
<blockquote><p><b> </b> <em> Often the most immediately valuable inferences are made from judging the shapes of curves rather than calculating numerical projections. </em>
</p></blockquote>
<p/><p>
Here is an example composed from today’s new-cases charts of Florida and the UK from the <em>Worldometer</em> coronavirus <a href="https://www.worldometers.info/coronavirus/">pages</a>:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/floridaukcov2/" rel="attachment wp-att-18929"><img alt="" class="aligncenter wp-image-18929" height="342" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/FloridaUKCov2.png?resize=352%2C342&amp;ssl=1" width="352"/></a></p>
<p>
The recent uptick in the UK is said to be caused by the new “delta” strain of the virus. I’ve chosen Florida rather than the whole US for similarity of scale and because Florida began a June upsurge this time a year ago. The US on the whole and most states show a similar fully-downward trend. Of course we hope it stays that way everywhere as the reopening phase continues. </p>
<p>
The point we are making is that if you just go by the numbers—and if you give highest weight to recent trends in those numbers—then your numerical projections can vary wildly. The logic of shape rather than number may be a more stable basis for judgment. This aligns with what we said about Ayanna Howard’s giving primacy to human rules of judgment in designs for robots in our <a href="https://rjlipton.wpcomstaging.com/2021/05/24/acm-athena-lecturer-award/">post</a> a month ago. </p>
<p>
</p><p/><h2> A Caveat </h2><p/>
<p/><p>
There is, however, a caveat that falls in with the opening example of Ellenberg’s chapter 11, “The Terrible Law of Increase”:</p>
<blockquote><p><b> </b> <em> An early judgment based on a seemingly best-fitting shape has maximum potential to go wrong. </em>
</p></blockquote>
<p/><p>
The example is an over-optimistic projection of Covid-19 cases and fatalities based on an initial expectation of a cubic curve fit. We neither wish to be academically impassive or blaming over the terrible toll which we have all had to endure, but to promote depth and wisdom in spatial thinking in line with the book’s purpose.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
How can we best get quick and accurate judgments of what is in store this summer and fall?</p>
<p/></font></font></div>
    </content>
    <updated>2021-06-20T19:09:04Z</updated>
    <published>2021-06-20T19:09:04Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="COVID-19"/>
    <category term="geometry"/>
    <category term="Jordan Ellenberg"/>
    <category term="predictions"/>
    <category term="reopening"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-06-28T03:20:50Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-1664874822018320731</id>
    <link href="http://processalgebra.blogspot.com/feeds/1664874822018320731/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=1664874822018320731" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/1664874822018320731" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/1664874822018320731" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/06/interview-with-concur-2021-tot-award.html" rel="alternate" type="text/html"/>
    <title>Interview with CONCUR 2021 ToT Award recipients, Part 1: Rajeev Alur, Thomas Henzinger, Orna Kupferman and Moshe Vardi</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Last year, the CONCUR conference series inaugurated its Test-of-Time Award, whose purpose is to recognise important achievements in  Concurrency Theory that were published at the CONCUR conference and have  stood the test of time.  This year, the following <a href="https://qonfest2021.lacl.fr/test-of-time.php" target="_blank">four papers</a> were chosen to receive the CONCUR Test-of-Time  Awards for the periods 1994–1997 and 1996–1999 by a jury consisting of Rob van Glabbeek (chair), Luca de Alfaro, Nathalie  Bertrand, Catuscia Palamidessi, and Nobuko Yoshida: </p><ul style="text-align: left;"><li>  David Janin and Igor Walukiewicz. <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=F31F028FFA1E754B87E7378032EAD6E7?doi=10.1.1.35.9156&amp;rep=rep1&amp;type=pdf" target="_blank">On the Expressive Completeness of the Propositional mu-Calculus with respect to Monadic Second Order Logic</a>. </li><li>Uwe Nestmann and Benjamin C. Pierce. <a href="http://www.cis.upenn.edu/~bcpierce/papers/choice.ps" target="_blank">Decoding Choice Encodings</a>.</li><li>Ahmed Bouajjani, Javier Esparza, and the late Oded Maler. <a href="http://www-verimag.imag.fr/~maler/Papers/pda.pdf" target="_blank">Reachability Analysis of Pushdown Automata: Application to Model-checking</a>.</li><li>Rajeev Alur, Thomas A. Henzinger, Orna Kupferman, and Moshe Y. Vardi. <a href="https://doi.org/10.1007/BFb0055622" target="_blank">Alternating Refinement Relations</a>. <br/></li></ul><p>Last year, I interviewed the <a href="http://concur2020.forsyte.at/test-of-time.html" target="_blank">CONCUR 2020 Test-of-Time Award recipients</a> and was asked by Javier Esparza (chair of the CONCUR SC) and Ilaria Castellani (outgoing chair of the IFIP WG 1.8 on Concurrency Theory) to do the same with the current batch of awardees. (In passing, let me thank <a href="http://people.rennes.inria.fr/Nathalie.Bertrand/" target="_blank">Nathalie Bertrand</a> and <a href="https://www.imperial.ac.uk/people/n.yoshida" target="_blank">Nobuko Yoshida</a> for their kind help with the interviews!)<br/></p><p>This post is devoted to the interview I conducted via email with <a href="https://www.cis.upenn.edu/~alur/" target="_blank">Rajeev Alur</a>, <a href="http://pub.ist.ac.at/~tah/" target="_blank">Thomas A. Henzinger</a>, <a href="https://www.cs.huji.ac.il/~ornak/" target="_blank">Orna Kupferman</a> and <a href="https://www.cs.rice.edu/~vardi/" target="_blank">Moshe Y. Vardi</a>. Reading the answers I received from that dream team of colleagues was like a masterclass for me and I trust that their thoughts on their award-winning paper will be of interest to many of the readers of this blog. Enjoy!<br/></p><p><b>Luca:</b> You receive the CONCUR ToT Award 2021 for your paper <a href="https://doi.org/10.1007/BFb0055622" target="_blank">Alternating Refinement Relations</a>, which appeared at CONCUR 1998. In that article, you gave what I consider to be a fundamental contribution, namely the introduction of refinement relations for alternating transition systems. Could you briefly explain to our readers what alternating transition systems are? Could you also tell us how you came to study the question addressed in your award-winning article and why you focused on simulation- and trace-based refinement relations? Which of the results in your paper did you find most surprising or challenging? </p><p><b>Answer:</b> When we model a system by a graph, our model abstracts away some details of the system. In particular, even when systems are deterministic, states in the model may have several successors. The nondeterminism introduced in the model often corresponds to different actions taken by the system when it responds to different inputs from its environment. Indeed, a transition in a graph that models a composite system corresponds to a step of the system that may involve some components. Alternating transition systems (ATSs) enable us to model composite systems in more detail. In an ATS, each transition corresponds to a possible move in a game between the components, which are called agents. In each move of the game, all agents choose actions, and the successor state is deterministically determined by all actions. Consequently, ATSs can distinguish between collaborative and adversarial relationships among components in a composite system. For example, the environment is typically viewed adversarially, meaning that a component may be required to meet its specification no matter how the environment behaves. </p><p>In an earlier <a href="https://www.cis.upenn.edu/~alur/Jacm02.pdf" target="_blank">paper</a>, some of us introduced ATSs and Alternating Temporal Logics, which can specify properties of agents in a composite system. The CONCUR 1998 paper provided refinement relations between ATSs which correspond to alternating temporal logics. Refinement is a central issue in a formal approach to the design and analysis of reactive systems. The relation “I refines S '' intuitively means that system S has more behaviors than system I. It is useful to think about S being a specification and I an implementation. Now, if we consider a composite implementation I||E and specification S||E and we want to check that the component I refines the component S, then the traditional refinement preorders are inappropriate, as they allow I to achieve refinement of I||E with respect to S||E by constraining its environment E.  Alternating refinement relations are defined with respect to ATSs that model the interaction among the underlying components, and they enable us to check, for example, that component I has fewer behaviors than component S no matter how component E behaves. They are called “alternating” because refinement may restrict implementation actions but must not restrict environment actions. In other words, refinement may admit fewer system actions but, at the same time, more environment actions. </p><p>It was nice to see how theoretical properties of preorders in the traditional setting are carried over to the game setting, and so are the results known then about the computational price of moving to a game setting. First, the efficiency of the local preorder of simulation with respect to the global preorder of trace containment is maintained. As in the traditional setting, alternating simulation can be checked in polynomial time, whereas alternating trace-containment is much more complex. Second, the branching vs. linear characterizations of the two preorders is preserved: alternating simulation implies alternating trace containment, and the logical characterization of simulation and trace-containment by CTL and LTL, respectively, is carried over to their alternating temporal logics counterparts. The doubly-exponential complexity of alternating trace containment, as opposed to the PSPACE complexity of trace containment, is nicely related to the doubly-exponential complexity of LTL synthesis, as opposed to its PSPACE model-checking complexity,</p><p>   <b>Luca:</b> In your paper, you give logical characterisations of your alternating refinement relations in terms of fragments of alternating temporal logic. Logical characterisations of refinement relations are classic results in our field and I find them very satisfying. Since I teach a number of those results in my courses, I'd be interested in hearing how you would motivate their interest and usefulness to a student or a colleague. What would your "sales pitch" be? </p><p><b>Answer:</b> There is extensive research on the expressive power of different formalisms. Logical characterization of refinement relations tells us something about the distinguishing power of formalisms. For example, while the temporal logic CTL* is more expressive than the temporal logic CTL, the two logics have the same distinguishing power: if you have two systems and can distinguish between them with a CTL* formula (that is, your formula is satisfied only in one of the systems), then you should be able to distinguish between the two systems also with a CTL formula. Moreover, while CTL is not more expressive than LTL, we know that CTL is “more distinguishing” than LTL. These results have to do with the logical characterizations of trace containment and simulation. The distinguishing power of a specification formalism is useful when we compare systems, in particular an implementation and its abstraction: if we know that the properties we care about are specified in some formalism L, and our system refines the abstraction according to a refinement relation in which the satisfaction of specifications in L is preserved, then we can perform verification on the abstraction.</p><p> <b>Luca:</b> I am interested in how research collaborations start, as I like to tell "research-life stories" to PhD students and young researchers of all ages. Could you tell us how you started your collaboration on the award-winning paper? </p><p><b>Answer:</b>Subsets of us were already collaborating on other topics related to reactive models and model checking, and all of us shared a common belief that the field was in need to move from the limited setting of closed systems to a more general setting of open systems, that is, systems that interact with an environment. Open systems occur not only when the environment is fully or partly unknown, but also when a closed system is decomposed into multiple components, each of them representing an open system. To build “openness” into models and specifications as first-class citizens quickly leads to the game-theoretic (or “alternating”) setting. It was this realization and the joint wish to provide a principled and systematic foundation for the modeling and verification of open systems which naturally led to this collaboration.</p><p> <b>Luca:</b> Did any of your subsequent research build explicitly on the results and the techniques you developed in your award-winning paper? Which of your subsequent results on alternating transition systems and their refinement relations do you like best? Is there any result obtained by other researchers that builds on your work and that you like in particular or found surprising? </p><p><b>Answer: </b>Various subsets of us pursued multiple research directions that developed the game-theoretic setting for modeling and verification further, and much remains to be done. Here are two examples. First, the game-theoretic setting and the alternating nature of inputs and outputs are now generally accepted as providing the proper semantic foundation for interface and contract formalisms for component-based design. Second, studying strategic behavior in multi-player games quickly leads to the importance of probabilistic behavior, say in the form of randomized decisions and strategies, of equilibria, when players have non-complementary objectives, and of auctions, when players need to spend resources for decisions. All of these are still very active topics of research in computer-aided verification, and they also form a bridge to the algorithmic game theory community.    </p><p><b>Luca:</b> One can view your work as a bridge between concurrency theory and multi-agent systems. What impact do you think that your work has had on the multi-agent-system community? And what has our community learnt from the work done in the field of multi-agent systems? To your mind, what are the main differences and points of contact in the work done within those communities? </p><p><b>Answer:</b> Modeling interaction in multi-agent systems is of natural interest to planning problems studied in the AI community. In 2002, the <a href="http://www.ifaamas.org/index.html" target="_blank">International Foundation for Autonomous Agents and Multiagent Systems</a> (IFAAMAS) was formed and the annual <a href="https://aamas2021.soton.ac.uk/" target="_blank">International Conference on Autonomous Agents and Multiagent Systems</a> (AAMAS) was launched. The models, logics, and algorithms developed in the concurrency and formal methods communities have had a strong influence on research presented at AAMAS conferences over the past twenty years. Coincidentally, this year our <a href="https://www.cis.upenn.edu/~alur/Jacm02.pdf" target="_blank">paper on Alternating-Time Temporal Logic</a> was chosen for the <a href="http://www.ifaamas.org/award-influential.html#:~:text=IFAAMAS%3A%20Awards%3A%20Influential%20Paper&amp;text=The%20Influential%20Paper%20Award%20seeks,lasting%20contributions%20to%20the%20field." target="_blank">IFAAMAS Influential Paper Award</a>. </p> <b>Luca:</b> What are the research topics that you find most interesting right now? Is there any specific problem in your current field of interest that you'd like to see solved? <p><b>Answer:</b>Research on formal verification and synthesis, including our paper, assumes that the model of the system is known. Reinforcement learning has emerged as a promising approach to the design of policies in scenarios where the model is not known and has to be learned by agents by exploration. This leads to an opportunity for research at the intersection of reactive synthesis and reinforcement learning. A potentially promising direction is to consider reinforcement learning for systems with multiple agents with both cooperative and adversarial interactions. </p><p>The realization that reactive systems have to satisfy their specifications in all environments has led to extensive research relating formal methods with game theory. Our paper added alternation to refinement relations. The transition from one to multiple players has been studied in computer science in several other contexts. For the basic problem of reachability in graphs, it amounts to moving from reachability to alternating reachability. We recently studied this shift in other fundamental graph problems, like the generation of weighted spanning trees, flows in networks, vertex covers, and more. In all these extensions, we consider a game between two players that take turns in jointly generating the outcome. One player aims at maximizing the value of the outcome (e.g., maximize the weight of the spanning tree, the amount of flow that travels in the network, or the size of the vertex cover), whereas the second aims at minimizing the value. It is interesting to see how some fundamental properties of graph algorithms are lost in the alternating setting. For example, following a greedy strategy is not beneficial in alternating spanning trees, optimal strategies in alternating flow networks may use fractional flows, and while the vertex-cover problem is NP-complete, an optimal strategy for the maximizer player can be found in polynomial time. Many more questions in this setting are still open. </p><p><b>Luca:</b> What advice would you give to a young researcher who is keen to start working on topics related to alternating transition systems and logics? </p><b>Answer:</b> One important piece of advice to young researchers is to question the orthodoxy. Sometimes it is necessary to learn everything that is known about a topic but then take a step back, look at the bigger picture, reexamine some of the fundamental assumptions behind the established ways of thinking, change the models that everyone has been using, and go beyond the incremental improvement of previous results. This is particularly true in formal methods, where no single model or approach fits everything. And young researchers stand a much better chance of having a really fresh new thought than those who have been at it for many years.</div>
    </content>
    <updated>2021-06-18T16:37:00Z</updated>
    <published>2021-06-18T16:37:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-06-24T08:16:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=5861</id>
    <link href="https://francisbach.com/quest-for-adaptivity/" rel="alternate" type="text/html"/>
    <title>The quest for adaptivity</title>
    <summary>Most machine learning classes and textbooks mention that there is no universal supervised learning algorithm that can do reasonably well on all learning problems. Indeed, a series of “no free lunch theorems” state that even in a simple input space, for any learning algorithm, there always exists a bad conditional distribution of outputs given inputs...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Most machine learning classes and textbooks mention that there is no universal supervised learning algorithm that can do reasonably well on all learning problems. Indeed, a series of “no free lunch theorems” state that even in a simple input space, for any learning algorithm, there always exists a bad conditional distribution of outputs given inputs where this algorithm performs arbitrarily bad. </p>



<p class="justify-text">For example, from the classic book of Luc Devroye, László Györfi, and Gábor Lugosi [<a href="https://www.szit.bme.hu/~gyorfi/pbook.pdf">1</a>, Theorem 7.2 and its extensions], for inputs uniformly distributed in \([0,1]\), for any decreasing sequence \((\varepsilon_n)_{n \geqslant 0}\) which is less than 1/16, for any learning algorithm (which takes pairs of observations and outputs a prediction function), there exists a conditional distribution on \(\{-1,1\}\) for which the expected risk of the classifier learned from \(n\) independent and identically distributed observations of (input,output) pairs is greater than \(\varepsilon_n\) for all \(n \geqslant 1\), while the best possible expected error rate is zero.</p>



<p class="justify-text">Such theorems do not imply that all learning methods are equally bad, but rather that all learning methods will suffer from some weaknessess. Throughout this blog we will try to better understand the weaknessess and strengths of popular methods through learning theory.</p>



<p class="justify-text">The key is to control the potential weaknesses of a learning method by making sure that in “favorable” scenarios, it leads to strong guarantees. When taking the simplest example of vectorial inputs in \(\mathbb{R}^d\), we can construct model classes of increasing complexity for which we can start to draw useful comparisons between learning methods.</p>



<p class="justify-text">Several aspects of the joint distribution of (input,output) \((X,Y)\) make the problem easy or hard. For concreteness and simplicity, I will focus on regression problems where the output space is \(\mathcal{Y} = \mathbb{R}\) and with the square loss, so that the optimal function, the “target” function, is \(f^\ast(x) = \mathbb{E}(Y|X=x)\). But much of the discussion extends to classification or even more complex outputs (see, e.g., [1]).</p>



<p class="justify-text"><strong>Curse of dimensionality.</strong> In the context of vectorial inputs, the slowness of universal learning algorithms can be characterized more precisely, and leads to the classical <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>. Only assuming that the optimal target function is Lipschitz-continuous, that is, for all \(x,x’\), \(| f^\ast(x)-f^\ast(x’)| \leqslant  L \| x – x’\|\) (for any arbitrary norm on \(\mathbb{R}^d\)), the optimal excess risk of a prediction function \(\hat{f}\) obtained from \(n\) observations, that is, the expected squared difference between \(f^\ast(x)\) and \(\hat{f}(x)\) cannot be less than a constant times \(n^{-2/(d+2)}\), that is, in order for this rate to be smaller than some \(\varepsilon &lt;1 \), we need \(n\) to be larger than \(\displaystyle ( {1}/{\varepsilon} )^{d/2+1}\), with thus an exponential dependence in \(d\) (see [<a href="https://web.stanford.edu/class/ee378a/books/book1.pdf">2</a>]).</p>



<p class="justify-text">In other words, exponentially many observations are needed for a reasonable performance on all problems with a minimal set of assumptions (here Lipschitz-continuity), and this bad behavior is unavoidable unless extra assumptions are added, which we now describe.</p>



<h2>Support, smoothness and latent variables</h2>



<p class="justify-text"><strong>Low-dimensional support. </strong>If the data occupy only a \(r\)-dimensional subspace of \(\mathbb{R}^d\), with \(r \leqslant d\) (and typically much smaller), then one should expect a better convergence rate. This also extends to data supported on a (smooth) <a href="https://en.wikipedia.org/wiki/Manifold">manifold</a>, as illustrated below. Note that this assumption does not concern the outputs, and can reasonably be checked given the data by performing principal component analysis or some form of <a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">manifold learning</a>. Essentially, in terms of convergence rates, \(d\) is replaced by \(r\). This is obvious if the learning algorithm has access to the \(r\)-dimensional representation, and it requires more work if not.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5959" height="225" src="https://francisbach.com/wp-content/uploads/2021/06/support.png" width="508"/>Left: samples from the uniform distribution in the unit disk in \(\mathbb{R}^2\). Right: samples from the uniform distribution in the unit circle in \(\mathbb{R}^2\), which is a one-dimensional smooth manifold.</figure></div>



<p class="justify-text"><strong>Smoothness of the target function. </strong>This is done by assuming some bounded derivatives for the target function \(f^\ast\). With bounded \(s\)-th order derivatives, we could expect that the problem is easier (note that Lipschitz-continuity corresponds to \(s=1\)). This is illustrated below with one-dimensional inputs. Essentially, in terms of rates, \(d\) is replaced by \(d/s\) [<a href="https://web.stanford.edu/class/ee378a/books/book1.pdf">2</a>, Theorem 3.2]. Therefore, when the smoothness order \(s\) is of order \(d\), the dependence in the dimension disappears.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5961" height="225" src="https://francisbach.com/wp-content/uploads/2021/06/smooth-1024x445.png" width="517"/>Regression problem with one-dimensional inputs, with target function in red, and observations in blue. Left: non-smooth (even not Lipschitz-continuous) function, right: smooth function.</figure></div>



<p class="justify-text"><strong>Latent variables.</strong> If we assume that the target function depends only on a \(r\)-dimensional linear projection of the input, then we should expect a better complexity. The most classical example is the dependence on a subset of the \(d\) original variables. Essentially, in terms of rates, \(d\) is replaced by \(r\). This is obvious when the latent variables are known (as we can replace input data by the \(r\) latent variables), totally not otherwise, as this requires some form of <em>adaptivity</em> (see below).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5964" height="252" src="https://francisbach.com/wp-content/uploads/2021/06/latent-1024x467.png" width="553"/>Regression problem with two-dimensional inputs, with target function (with no noise). Left:  function that cannot be written with a latent variable, right: function that has a (linear) latent variable representation (there exists a direction with no variation).</figure></div>



<p class="justify-text"><strong>Need for adaptivity.</strong> We typically don’t know in advance if these properties are satisfied or not, as some are easily testable (support of distribution) without knowing the target function \(f^\ast\), while others are not.</p>



<p class="justify-text">The goal is to have a single method that can adapt to all of these situations (which are non-exclusive). That is, if the problem has any of these reasons to be easy, will the learning method benefit from it? Typically, most learning methods have at least one hyperparameter controlling overfitting (e.g., a regularization parameter), and the precise value of this hyperparameter will depend on the difficulty of the problem, and we will assume that we have a reasonable way to estimate this hyperparameter (e.g., cross-validation). A method is then said adaptive if with a well chosen value of the hyperparameter, we get the optimal (or close to optimal) rate of estimation that benefits from the extra assumption.</p>



<p class="justify-text"><strong>Quest for adaptivity: who wins?</strong> Among classical learning techniques, which ones are adaptive to which properties? In short, <em>barring computational and optimization issues</em>: $$\mbox{ local averaging } &lt; \mbox{ positive definite kernels } &lt; \mbox{ neural networks }.$$ Every time, the next method in the list gains adaptivity to support, smoothness and then latent variables. <span class="has-inline-color has-vivid-red-color">Note however that optimization for neural networks is more delicate (see below).</span></p>



<p class="justify-text">Let’s now briefly look at these methods one by one. I will assume basic knowledge of these, for more details see [<a href="https://www.dropbox.com/s/7voitv0vt24c88s/10290.pdf?dl=1">4</a>, <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">5</a>] or my <a href="https://francisbach.com/i-am-writing-a-book/">new book in preparation</a>.</p>



<h2>Local averaging</h2>



<p class="justify-text">The earliest and simplest learning methods that could adapt to any target functions were <em>local averaging</em> methods aiming at approximating directly the conditional expectation \(f^\ast(x) = \mathbb{E}(Y|X=x)\), with the most classical examples being <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"><em>k</em>-nearest neighbors</a> and <a href="https://en.wikipedia.org/wiki/Kernel_regression">Nadaraya-Watson</a> estimators.</p>



<p class="justify-text">These methods are naturally adaptive to having a reduced support for inputs. Indeed, in the simplest case of a distribution supported in a low-dimensional subspace, the global metric is equivalent to a local metric on the support, without any need to explicitly know the subspace, a situation which extends to smooth manifolds. In order to be adaptive, the hyperparameter has to depend on the dimension \(r\) of the manifold, e.g., for \(k\)-nearest-neighbors, \(k\) has to be taken proportional to \(n^{2/(2+r)}\) (see [<a href="http://www.columbia.edu/~skk2175/Papers/kNNRegressionLocRatesFullVersion.pdf">3</a>]).</p>



<p class="justify-text">However, there is no adaptivity to the smoothness of the target function or to potential latent variables unless dedicated algorithms are used such as <a href="https://en.wikipedia.org/wiki/Local_regression">local regression</a>. Kernel methods and neural networks lead to such adaptivity.</p>



<h2>From kernels to neural networks</h2>



<p class="justify-text">We consider prediction functions of the form $$f(x) = \sum_{j=1}^m a_j ( b_j^\top x )_+,$$ which is the traditional single hidden layer fully connected neural network with ReLU activation functions (a constant term can be added to the linear term within the ReLU by simply appending \(1\) to \(x\), please do not call these terms “biases” as this has <a href="https://en.wikipedia.org/wiki/Bias_(statistics)">another meaning in statistics</a>).</p>



<p class="justify-text">The vector \(a \in \mathbb{R}^m\) represents <em>output weights</em>, while the matrix \(b \in \mathbb{R}^{m \times d}\) represents <em>input weights</em>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5975" height="300" src="https://francisbach.com/wp-content/uploads/2021/06/nn_blog-1024x546.png" width="564"/></figure></div>



<p class="justify-text"><strong>Empirical risk minimization (ERM). </strong>We assume given \(n\) i.i.d. observations \((x_1,y_1),\dots,(x_n,y_n) \in \mathbb{R}^d \times \mathbb{R}\), and we will fit models by minimizing the \(\ell_2\)-regularized empirical risk (you can call it “weight decay” but it  already has a better name in machine learning and statistics). That is, we minimize $$R(a,b) = \frac{1}{2n} \sum_{i=1}^n \Big( y_i \, – \sum_{j=1}^m a_j ( b_j^\top x_i )_+ \Big) ^2 + \frac{\lambda}{2} \sum_{j=1}^m \Big\{ a_j^2 + \| b_j\|_2^2 \Big\},$$ where \(\lambda &gt; 0\) is a regularization parameter.</p>



<p class="justify-text"><strong>Overparametrization. </strong>We will consider the limit of large number \(m\) of hidden neurons. Depending whether we optimize over both input weights \(b\) and output weights \(a\), or simply output weights (with then a proper initialization of the input weights), we get different behaviors.</p>



<h2>Kernel regime</h2>



<p class="justify-text">We assume that the input weights \(b_j \in \mathbb{R}^d\) are sampled uniformly from the Euclidean sphere of radius \(1 / \sqrt{m}\), and that we only optimize over the output weights \(a_j, j = 1,\dots,m\). This is exactly a <a href="https://en.wikipedia.org/wiki/Ridge_regression">ridge regression</a> problem (square loss and squared Euclidean penalty), for which the theory of positive definite kernels applies and will lead to an interesting behavior for infinite \(m\) [7, <a href="https://papers.nips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf">8</a>]. That is, the solution can be obtained by using the kernel function \(\hat{k}\) defined as $$\hat{k}(x,x’) = \sum_{j=1}^m ( b_j^\top x )_+( b_j^\top x’ )_+,$$ and looking for prediction functions of the form $$f(x) = \sum_{i=1}^n \alpha_i \hat{k}(x,x_i).$$ See, e.g., [6] for details. In particular, as long as \(\hat{k}(x,x’)\) can be computed efficiently, the complexity of solving the ERM problem is independent of the number of neurons \(m\). We will now discuss in further detail the over-parameterized case where \(m=\infty\).</p>



<p class="justify-text">When \(m\) tends to infinity, If the input weights are fixed and initialized randomly from the \(\ell_2\)-sphere of radius \(1/\sqrt{m}\), then by the law of large numbers, \(\hat{k}(x,x’)\) tends to $$k(x,x’) = \mathbb{E}_{b \sim {\rm uniform} (\mathbb{S}^{d-1}) } (b^\top x )_+ (b^\top x’)_+,$$ where \(\mathbb{S}^{d-1}\) is the unit \(\ell_2\)-sphere in \(d\) dimensions. This kernel has a closed form and happens to be equal to (see [<a href="https://papers.nips.cc/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf">9</a>]) $$ \frac{1}{2\pi d}  \|x\|_2   \| x’\|_2 \big[ ( \pi \, – \varphi) \cos \varphi + \sin \varphi \big],$$ where \(\cos \varphi = \frac{ x^\top x’  }{{ \|x\|_2  } { \| x’\|_2   } }\). It can also be seen (see, e.g., [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>] for details) as using predictors of the form $$f(x) = \int_{\mathbb{S}^{d-1}}  ( b^\top x)_+ d \mu(b),$$ for some measure \(\mu\) on \(\mathbb{S}^{d-1}\), with the penalty $$\frac{\lambda}{2} \int_{\mathbb{S}^{d-1}} \big| \frac{d\mu}{d\tau}(b) \big|^2 d\tau(b),$$ for the uniform probability measure \(d\tau\) on the hypersphere \(\mathbb{S}^{d-1}\). This representation will be useful for the comparison with neural networks.</p>



<p class="justify-text">Note that here we use random features in a different way from their common use [<a href="https://papers.nips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf">8</a>], where the kernel \(k\) comes first and is approximated by \(\hat{k}\) to obtain fast algorithms, typically with \(m &lt; n\). Here we start from \(\hat{k}\) and find the limiting \(k\) to understand the behavior of overparameterization. The resulting function space is a (<a href="https://en.wikipedia.org/wiki/Sobolev_space">Sobolev</a>) space of functions on the sphere with all \(s\)-th order derivatives which are square-integrable, with \(s = d/2+3/2\) [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>].</p>



<p>Finally, the number of neurons \(m\) needed to reach the kernel regime is well understood, at least in simple situations [<a href="https://papers.nips.cc/paper/2015/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf">15</a>, <a href="https://proceedings.neurips.cc/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf">16</a>].</p>



<p class="justify-text">We can now look at the various forms of adaptivity of kernel methods based on Sobolev spaces.</p>



<p class="justify-text"><strong>Adaptivity to reduced support.</strong> Like model averaging techniques, adaptivity to input data supported on a subspace is rather straightforward, and it extends to smooth manifolds (see, e.g., [<a href="https://arxiv.org/pdf/2003.06202.pdf">12</a>] for a proof for the Gaussian kernel).</p>



<p class="justify-text"><strong>Adaptive to smoothness.</strong> A key attractive feature of kernel methods is that they can circumvent the curse of dimensionality for <em>smooth</em> target functions, and, by simply ajusting the regularization parameter \(\lambda\), ridge regression will typically adapt to the smoothness of the target function, and thus benefit from easy problems.</p>



<p class="justify-text">The simplest instance is when the target function is within the space of functions defined above, where we immediately get estimation rates which are independent of dimension (at least in the exponent). This however requires at least \(s&gt;d/2\) derivatives (because Sobolev spaces are <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert spaces</a> (RKHS) only in this situation), but the adaptivity extends to functions outside of the RKHS (see, e.g., [11]).</p>



<p class="justify-text"><strong>Adaptivity to linear latent variables.</strong> Unfortunately, kernel methods are not adaptive even to basic linear structures. That is, if \(f^\ast\) depends only on the first component of \(x\), then ridge regression, if not modified, will not take advantage of it. In the context of neural networks, one striking example is that a single neuron \(x \mapsto (b^\top x)_+\) <em>does not belong to the RKHS</em>, and leads to bad estimation rates (see, e.g., [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>, <a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>]).</p>



<p class="justify-text">Before looking at some experiments, let’s look at some optimization considerations, that will be important later for neural networks.</p>



<p class="justify-text"><strong>Avoiding overparameterized neuron representations.</strong> In the kernel regime, optimizing directly the cost function with \(m\) neurons by gradient descent is problematic for two reasons when \(m\) is large:</p>



<ul class="justify-text"><li>The optimization problem, although convex, is severely ill-conditioned as can be seen from the spectrum of the covariance matrix of the feature vector, with the \(i\)-th eigenvalue equivalent to \(i^{-1-3/d}\) for the uniform input distribution (see [14, Section 2.3]). Therefore, (stochastic) gradient descent will take a lot of time to converge.</li><li>We can use kernels directly (for \(m = +\infty\)) to avoid using \(m\) very large (which would be useless in practice). The running-time complexity is then of the order \(O(n^2)\) if done naively, with lots of ways to go below \(O(n^2)\), such as column sampling (see a nice analysis in [<a href="https://papers.nips.cc/paper/2015/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf">15</a>]).</li></ul>



<p class="justify-text"><strong>Experiments.</strong> We first consider a very simple one-dimensional example, where we look at how the estimation with the kernel function \(k\) varies as a function of the hyperparameter \(\lambda\), from underfitting to overfitting. We can observe that all learned functions are smooth, as expected.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-6045" height="252" src="https://francisbach.com/wp-content/uploads/2021/06/rff_biplot.gif" width="574"/>Regression problems in one dimension with kernels (left: smooth target, right: non-smooth target), with \(n=32\) observations, with varying regularization parameters, with the optimal one shown below.</figure></div>



<p class="justify-text">We can now compare the convergence rates for the excess risk (expected squared distance beween \(f\) and \(f^\ast\)). We can see that the rates are better for smooth functions, and with the proper choice of regularization parameter, the kernel method adapts to it.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-6042" height="241" src="https://francisbach.com/wp-content/uploads/2021/06/simulations_adaptivity_rff_subplots.png" width="925"/>Left: estimation with the optimal regularization parameter for smooth target function and kernel methods. Middle: estimation with a non-smooth target function. Right: convergence rates when the number \(n\) of observations increases, averaged over 32 replications.</figure></div>



<h2>Neural networks</h2>



<p class="justify-text">We can now optimize over all weights, output \(a\) <em>and</em> input \(b\). Four natural questions come to mind:</p>



<ol class="justify-text"><li>Where does it converge to when the number \(m\) of hidden neurons tends to infinity?</li><li>What are the adaptivity properties of the resulting predictor?</li><li>How big \(m\) needs to be to achieve the infinite width behavior?</li><li>Can we actually solve the non-convex optimization problem?</li></ol>



<p class="justify-text"><strong>Overparameterization limit (feature learning regime).</strong> When optimizing over both sets of weights, we can first note that the prediction function is invariant by the change of variable $$ a_j \leftarrow \mu_j a_j \ , \ \ b_j \leftarrow \mu_j^{-1} b_j.$$ Then by optimizing over \(\mu_j\) which is only involved in the penalty term \(\frac{\lambda}{2} \big( a_j^2 + \|b_j\|_2^2 \big)\), we get \(\mu_j^\ast = a_j^{-2} \| b_j \|_2^2\), and the penalty \(\lambda |a_j| \| b_j\|_2\). We thus get the equivalent optimization problem of minimizing $$\tilde{R}(a,b) = \frac{1}{2n} \sum_{i=1}^n \Big( y_i \, – \sum_{j=1}^m a_j ( b_j^\top x_i )_+ \Big) ^2 + {\lambda} \sum_{j=1}^m |a_j| \| b_j\|_2 .$$ By restricting the \(b_j\) on the unit sphere (which is OK because by optimizing over \(\mu_j\) we become scale-invariant), we can write $$\sum_{j=1}^m a_j ( b_j^\top x_i )_+ = \int_{\mathbb{S}^{d-1}} (b^\top x)_+ d\mu(b), $$ for $$ \mu = \sum_{j=1}^m a_j \delta_{b_j}$$ a weighted sum of Diracs, and \(\sum_{j=1}^m |a_j| \| b_j\|_2 = \int_{\mathbb{S}^{d-1}} \! | d\mu(b)|\) the <a href="https://en.wikipedia.org/wiki/Total_variation">total variation</a> of \(\mu\). Thus, letting \(m\) go to infinity, the infinite sums become integrals of general measures, and we end up considering the set of functions that can be written as (see [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>] and the many references therein for details): $$f(x) = \int_{\mathbb{S}^{d-1}}   ( b^\top x)_+ d \mu(b),$$ with the penalty $$\lambda \int_{\mathbb{S}^{d-1}} \! | d\mu(b)|.$$ It has an \(\ell_1\)-norm flavor (as opposed to the \(\ell_2\)-norm for kernels), and leads to further adaptivity, if optimization problems are resolved (which is itself not easy in polynomial time, see below).</p>



<p class="justify-text">The minimal number of neurons to achieve the limiting behavior in terms of predictive performance (assuming optimization problems are resolved) can also be characterized, and grows exponentially in dimension without strong assumptions like the ones made in this blog post (see [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>]). We will now study the adaptivity properties of using the function space above.</p>



<p class="justify-text"><strong>Adaptive to low-dimensional support and smoothness.</strong> It turns out that the adaptivity properties of kernel methods are preserved, both with respect to the support and the smoothness of the target function (see, e.g., [17, <a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>]). However, neural networks can do better!</p>



<p class="justify-text"><strong>Adaptive to linear substructures for one hidden layer.</strong> Given that the training of neural networks involves finding the best possible input weights, which are involved in the first linear layer, it is no surprise that we obtain adaptivity to linear latent variables. That is, if we can write \(f^\ast(x) = g(c_1^\top x, \dots, c_r^\top x)\) for some function \(g: \mathbb{R}^r \to \mathbb{R}\), then the vectors \(c_j\)’s will be essentially estimated among the \(b_j\)’s by the optimization algorithm. Therefore, when using the \(\ell_1\)-based function space, the convergence rate in the excess risk will depend on \(r\) and not \(d\) in the exponent [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>, <a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>]. In particular, neural networks can perform <em>non-linear</em> variable selection (while the <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Lasso</a> only performs <em>linear</em> variable selection). See a nice experiment in [<a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>] for binary classification.</p>



<p class="justify-text">One could imagine that with more hidden layers, this extends to non-linear smooth projections of the data, that is, to cases where we assume that \(f^\ast(x) = g(h(x))\) where \(h: \mathbb{R}^d \to \mathbb{R}^r\) is a smooth function. </p>



<p class="justify-text">Thus, we obtain a stronger adaptivity for infinite \(m\) and the good choice of regularization parameter \(\lambda\). We could then try to reproduce for neural networks the figures obtained for kernel methods with varying \(\lambda\). Unfortunately, this is where non-convex optimization will make everything harder.</p>



<h2>Overfitting with a single hidden layer is hard!</h2>



<p class="justify-text"><strong>Global convergence of gradient flow for infinite width. </strong>The traditional algorithm to minimize the empirical risk is gradient descent and its stochastic extensions. In this blog post, we consider gradient descent with small step-sizes, which can be approximated by a gradient flow (as explained in <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">last June blog post</a>). All parameters are randomly initialized, and we consider \(b_j\) uniformly distributed on the sphere of radius \(1/\sqrt{m}\), and \(a_j\) uniformly distributed in \(\{-1/\sqrt{m},1/\sqrt{m}\}\) (this is essentially equivalent to the traditional “Glorot” initialization [<a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">18</a>]).</p>



<p class="justify-text">This corresponds to the “mean-field” scaling of initialization where neurons from both layers move (see more details in <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">last </a><a href="https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/">July</a><a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/"> blog post</a> for other scalings). As shown in a joint work with Lénaïc Chizat [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">19</a>] and explained in <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">last June blog post</a>, when \(m\) tends to infinity, then the gradient flow can only converge to the global minimum of the objective function, which is a non-trivial result because the cost function is not convex in \((a,b)\). </p>



<p class="justify-text"><strong>Apparent good properties for small \(m\).</strong> A key property which is not yet well understood is that the global convergence behavior can be observed for \(m\) relatively small. For example, considering the one-dimensional regression example below, where the target function is the combination of 5 hidden neurons, with \(m=32\) hidden neurons, it is already possible to learn a good function with high probability (this would not be the case with \(m=5\)).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-6048" height="292" src="https://francisbach.com/wp-content/uploads/2021/06/regular_flow_triangular_noisy_32.png" width="319"/>One-dimensional regression from \(n = 64\) observations, by minimizing with gradient descent (and small step-size) the unregularized empirical risk.</figure></div>



<p class="justify-text">One extra benefit here is that no regularization (e.g., no penalty) seems needed to obtain this good behavior. Therefore it seems that we get the best of both worlds: reasonably small \(m\) (so not too costly algorithmically), and no need to regularize.</p>



<p class="justify-text">However, like in any situation where overfitting is not observed, there is the possibility of <em>underfitting</em>. To study this, let us consider a noiseless problem where \(y\) is a deterministic function of \(x\), and with a sligthly more complicated target function, which now requires 9 hidden neurons.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-6077" height="277" src="https://francisbach.com/wp-content/uploads/2021/06/regular_flow_hard_noiseless_8192.png" width="327"/>One-dimensional noiseless regression from \(n = 64\) observations, by minimizing with gradient descent (and small step-size) the unregularized empirical risk.</figure></div>



<p class="justify-text">Even with \(m = 8192\) hidden neurons, the resulting function is not able to fit the data, which is one extreme form of simplicity bias [<a href="https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf">21</a>]. There is thus strong underfitting with a single hidden layer (the situation seems to be different with deeper networks). How can this be alleviated? </p>



<h2>Kernel learning to the rescue</h2>



<p class="justify-text">A simple way to avoid the underfitting phenomenon above (but potentially get overfitting, see below) is to minimize the cost \(R(a,b)\) by leveraging a convex sub-problem, as done in the glorious days of <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel methods</a>.</p>



<p class="justify-text">In the cost function $$R(a,b) = \frac{1}{2n} \sum_{i=1}^n \Big( y_i \, – \sum_{j=1}^m a_j ( b_j^\top x_i )_+ \Big) ^2 + \frac{\lambda}{2} \sum_{j=1}^m \Big\{ a_j^2 + \| b_j\|_2^2 \Big\},$$ the problem is convex with respect to all \((a_j)\)’s; moreover, it is a least-squares problems which can be solved in closed form by matrix inversion (with algorithms that are much more robust to ill-conditioning than gradient descent). </p>



<p class="justify-text">Then, if we denote by \(\Phi(b) \in \mathbb{R}^{n \times m}\) the matrix with elements \((b_j^\top x_i)_+\), the cost function can be written as a function of \(a \in \mathbb{R}^m\) as $$R(a,b) = \frac{1}{2n} \| y \, – \Phi(b) a \|_2^2 +\frac{\lambda}{2}\| a\|_2^2 +  \frac{\lambda}{2} \sum_{j=1}^m \| b_j\|_2^2.$$ It can be minimized in closed form as $$ a = ( \Phi(b)^\top \Phi(b) + n \lambda I)^{-1} \Phi(b)^\top y = \Phi(b)^\top ( \Phi(b)\Phi(b)^\top + n \lambda I)^{-1} y,$$ where the matrix inversion lemma has been used. This leads to: $$S(b) = \inf_{a \in \mathbb{R}^m} R(a,b) = \frac{\lambda}{2} y^\top  ( \Phi(b)\Phi(b)^\top + n \lambda I)^{-1} y + \frac{\lambda}{2} \sum_{j=1}^m \| b_j\|_2^2.$$ The matrix \(\Phi(b)\Phi(b)^\top \in \mathbb{R}^{n \times n}\) is the traditional kernel matrix of the associated non-linear features. The resulting optimization problem is exactly one used in kernel learning [<a href="https://link.springer.com/content/pdf/10.1023/a:1012450327387.pdf">20</a>].</p>



<p class="justify-text">Computing \(S(b)\) and its gradient with respect to \(b\) can be done in time \(O(n^2 m)\) when \(m\) is large. Since given the hidden neurons, we get the global optimum, we avoid the underfitting problem as well as the ill-conditioning issues.</p>



<p class="justify-text">When \(m\) tends to infinity, then the optimization problem fits our analysis in [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">19</a>], that is, we can see the limiting flow as a Wasserstein gradient flow that can only converge to the global optimum of the associated cost function although the overall problem is not convex (as opposed to what happens in multiple kernel learning described in this <a href="https://francisbach.com/the-eta-trick-reloaded-multiple-kernel-learning/">older post</a>). While there is currently no further quantitative theoretical evidence that a good optimization behavior can be reached for a smaller \(m\), it empirically solves the underfitting issue above, as shown below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-6050" height="254" src="https://francisbach.com/wp-content/uploads/2021/06/two_flows_hard_noiseless_128.png" width="607"/>Comparison of the the two flows. Right: regular gradient flow on both sets of weights (normal training), left: gradient flow on the cost function where the output weights are maximized out.</figure></div>



<p>We can now perform experiments with varying \(\lambda\)’s.</p>



<p class="justify-text"><strong>Experiments. </strong>The two plots below depict the learnt functions for a smooth and non-smooth objective as \(\lambda\) is varied, highlighting the importance of regularization.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-6046" height="261" src="https://francisbach.com/wp-content/uploads/2021/06/nn_biplot.gif" width="592"/>Regression problems in one dimension with neural networks (left: smooth target, right: non-smooth target), with \(n=32\) observations, with varying regularization parameters, with the optimal one shown below.</figure></div>



<p class="justify-text">We can now compare the convergence rates for the excess risk (expected squared distance beween \(f\) and \(f^\ast\)). We can see that the rates are better for smooth functions, and with the proper choice of regularization parameter, neural networks adapt to it. For adaptivity to linear latent variables, see [<a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>].</p>



<figure class="wp-block-image size-full justify-text"><img alt="" class="wp-image-6043" height="480" src="https://francisbach.com/wp-content/uploads/2021/06/simulations_adaptivity_nn_subplots.png" width="1833"/>Left: estimation with the optimal regularization parameter for smooth target function and neural networks. Middle: estimation with a non-smooth target function. Right: convergence rates when the number \(n\) of observations increases, averaged over 32 replications.</figure>



<h2>Conclusion</h2>



<p class="justify-text">As I tried to show in this blog post, adaptivity is a key driver of good predictive performance of learning methods. It turns out that for the three classes of methods I considered, the more complex algorithms led to more adaptivity: no optimization for local averaging, finite-dimensional convex optimization for kernel methods and then non-convex optimization for neural networks (or equivalently infinite-dimensional convex optimization).</p>



<p class="justify-text">While neural networks led to more adaptivity, they are still not totally well understood, in particular in terms of the various biases that their training implies, both for shallow and deep networks. This makes a lot of research directions to follow!</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Lénaïc Chizat and Lawrence Stewart for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Luc Devroye, László Györfi, and Gábor Lugosi. <em><a href="https://www.szit.bme.hu/~gyorfi/pbook.pdf">A Probabilistic Theory of Pattern Recognition</a></em>,<br/>volume 31. Springer Science &amp; Business Media, 1996.<br/>[2] László Györfi, Michael Kohler, Adam Krzyżak, Harro Walk<em>.</em> <em><a href="https://web.stanford.edu/class/ee378a/books/book1.pdf">A Distribution-free Theory of Nonparametric Regression</a></em>. New York : Springer, 2002.<br/>[3] Samory Kpotufe.<a href="http://www.columbia.edu/~skk2175/Papers/kNNRegressionLocRatesFullVersion.pdf"> k-NN regression adapts to local intrinsic dimension</a>. In Advances in Neural Information Processing Systems, 2011.<br/>[4] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. <em><a href="https://www.dropbox.com/s/7voitv0vt24c88s/10290.pdf?dl=1">Foundations of Machine Learning</a></em>. MIT Press, 2018.<br/>[5] Shai Shalev-Shwartz and Shai Ben-David. <em><a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a></em>. Cambridge University Press, 2014.<br/>[6] Bernhard Schölkopf and Alexander J. Smola. <em>Learning with Kernels</em>. MIT Press, 2001.<br/>[7] Radford M. Neal. <a href="https://www.cs.toronto.edu/~radford/ftp/thesis.ps">Bayesian Learning for Neural Networks</a>. PhD thesis, University of Toronto, 1995.<br/>[8] Ali Rahimi and Benjamin Recht. <a href="https://papers.nips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf">Random features for large-scale kernel machines</a>. In Advances in Neural Information Processing Systems, 2008.<br/>[9] Youngmin Cho and Lawrence K. Saul. <a href="https://papers.nips.cc/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf">Kernel methods for deep learning</a>. In Advances in Neural Information Processing Systems, 2009.<br/>[10] Francis Bach. <a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the curse of dimensionality with convex neural networks</a>. The Journal of Machine Learning Research, 18(1):629–681, 2017.<br/>[11] Andreas Christmann, Ingo Steinwart. Support Vector Machines. Springer, 2008.<br/>[12] Thomas Hamm, Ingo Steinwart. <a href="https://arxiv.org/pdf/2003.06202.pdf">Adaptive Learning Rates for Support Vector Machines Working on Data with Low Intrinsic Dimension</a>. Technical report, Arxiv 2003.06202, 2021.<br/>[13] Lénaïc Chizat, Francis Bach. <a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss</a>. <em>Proceedings of the Conference on Learning Theory (COLT)</em>, 2020.<br/>[14] Francis Bach. <a href="http://jmlr.org/papers/volume18/15-178/15-178.pdf">On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions</a>. <em>Journal of Machine Learning Research</em>, 18(19):1-38, 2017. <br/>[15] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. <a href="https://papers.nips.cc/paper/2015/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf">Less is more: Nyström computational regularization</a>. In Advances in Neural Information Processing Systems, pages 1657–1665, 2015.<br/>[16] Alessandro Rudi, Lorenzo Rosasco. <a href="https://proceedings.neurips.cc/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf">Generalization properties of learning with random features</a>. In Advances in Neural Information Processing Systems, 2017.<br/>[17] Ryumei Nakada, Masaaki Imaizumi. <a href="https://jmlr.org/papers/volume21/20-002/20-002.pdf">Adaptive Approximation and Generalization of Deep Neural Network with Intrinsic Dimensionality</a>. <em>Journal of Machine Learning Research</em>, 21(174):1−38, 2020.<br/>[18] Xavier Glorot, Yoshua Bengio. <a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a>. <em>Proceedings of the International Conference on Artificial Intelligence and Statistics</em>, 2010.<br/>[19] Lénaïc Chizat, Francis Bach. <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport</a>. <em>Advances in Neural Information Processing Systems</em>, 2018<br/>[20] Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, Sayan Mukherjee. <a href="https://link.springer.com/content/pdf/10.1023/a:1012450327387.pdf">Choosing multiple parameters for support vector machines</a>. <em>Machine Learning</em>, 46(1):131-159, 2002.<br/>[21] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, Praneeth Netrapalli. <a href="https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf">The Pitfalls of Simplicity Bias in Neural Networks</a>. In Advances in Neural Information Processing Systems, 2020.</p></div>
    </content>
    <updated>2021-06-17T16:51:38Z</updated>
    <published>2021-06-17T16:51:38Z</published>
    <category term="Machine learning"/>
    <category term="Optimization"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2021-06-28T03:22:40Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1222818111730421374</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1222818111730421374/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/benny-chor-1956-2021.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1222818111730421374" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1222818111730421374" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/benny-chor-1956-2021.html" rel="alternate" type="text/html"/>
    <title>Benny Chor (1956-2021)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p/><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-MVD3X8JUWlI/YMtAmUG2rnI/AAAAAAAB8VM/GFZrtRW5Y707KiQ4KZOTB3c7vaCVV-kHgCLcBGAsYHQ/s137/benny3.jpg" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" height="200" src="https://1.bp.blogspot.com/-MVD3X8JUWlI/YMtAmUG2rnI/AAAAAAAB8VM/GFZrtRW5Y707KiQ4KZOTB3c7vaCVV-kHgCLcBGAsYHQ/w118-h200/benny3.jpg" width="118"/></a></div><br/>Benny Chor passed away on June 10, 2021. Luca Trevisan had a <a href="https://lucatrevisan.wordpress.com/2021/06/10/benny-chor/">blog post</a> on the news.<p/><p>We present a guest post on Benny Chor's life and works by Oded Goldreich. The post has many refs to papers. They will appear at the end with pointers to them.</p><hr/><p>Benny Chor was born on December 23rd 1956  and grew-up in Tel-Aviv, Israel. He studied Mathematics in the Hebrew University, receiving a B.Sc. in 1980 and an M.Sc. in 1981. He then switched to studying Computer Science at MIT, and graduated in 1985 with a PhD thesis titled</p><p> <i>Two Issues in Public Key Cryptography -- RSA Bit Security and a New Knapsack Type System</i></p><p>which received an ACM Distinguished Dissertation award. After post-doctoral periods at MIT and Harvard, he took a faculty position at the Computer Science Department of the Technion (1987-2001), and then at Tel-Aviv University, where he served as the chairman of the department for two years. He died on June 10th 2021, from a terminal disease.</p><p>Although Benny was a very articulated and verbal person, I find it impossible to describe his personality in words. The point is that words cannot capture the experience of interacting with him, which was always sheer fun. Still, I guess I should say something personal as a close friend of his. So I will just mention that he lived happily with Metsada, whom he met in the summer of 1981, and that they lovingly raised three kids: Arnon, Omer and Aya. Actually, let me also mention his life-long close relationship with his brother, Kobi.</p><p>Focusing on his contributions to science, I will confine myself to the areas of cryptography and randomized computation. Still, let me mention that in the mid 1990s, Benny's research interests gradually shifted to computational biology, but I will not review his contributions to that area, since it is very remote from my own expertise.</p><div><div>In my opinion, Benny's most important contribution to cryptography is the fundamental</div><div>paper on Verifiable Secret Sharing [CGMA].  Loosely speaking, Verifiable Secret Sharing</div><div>is a method to share a secret so that any majority of share-holders may reconstruct the</div><div>secret, whereas no minority may do so, and still each share-holder can verify that the</div><div>share that they got is a valid one.  This primitive had a tremendous impact on the</div><div>development of secure multi-party protocols, and almost all subsequent works in this</div><div>area utilize it.</div><div><br/></div><div>Additional research milestones of Benny, in the area of Cryptography, include the proof</div><div>of the existence of a ``hard-core'' in the RSA and Rabin functions [BCS, ACGS], the</div><div>introduction and design of Private Information Retrieval (PIR) schemes [CGKS] (as well</div><div>as their computational counterparts [CG97]), and the introduction and design of</div><div>Tracing Traitor schemes [CFNP].  Each of these works deserves more than the foregoing</div><div>brief mention, yet I'm not even mentioning other important works like [CK].</div><div><br/></div><div>Turning to the area of Randomized Computation, Benny's contributions span diverse areas</div><div>ranging from the manipulation of randomness to the use of randomness in complexity theory</div><div>and distributed computing.  In particular, his work on weak sources of randomness [CG88]</div><div>identified min-entropy (of the source) as the key parameter for randomness extraction and</div><div>presented a simple two-source extractor.  Distilling an aspect of [ACGS], his work on</div><div>pairwise-independent sampling [CG89] demonstrates the general applicability of this method. His works in distributed computing include a randomized Byzantine Agreement protocol [CC] that beats the deterministic bound without using unproven assumptions.  Lastly, let me just mention a couple of his complexity-theoretic works: [BCGL, CCGHHRR].</div><div><br/></div><div>Benny was a superb teacher and a devoted graduate student adviser.  It is tempting to try</div><div>listing the numerous educational projects that he initiated, and his former students, but</div><div>these lists will be too long and the risk of a crucial omissions is too high.  So let me</div><div>end by expressing the feeling of loss that is shared by many in our community,</div><div>which adds to my personal sorrow.</div></div><div><br/></div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-VxGM4xSHMwU/YMtBVdfDCMI/AAAAAAAB8Vo/47D34yqw-pEpjSLuzOk-lvHn6HFOPTyVACLcBGAsYHQ/s634/benny2.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://1.bp.blogspot.com/-VxGM4xSHMwU/YMtBVdfDCMI/AAAAAAAB8Vo/47D34yqw-pEpjSLuzOk-lvHn6HFOPTyVACLcBGAsYHQ/s320/benny2.jpg" width="320"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Oded and Benny</td></tr></tbody></table><br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-TJNVzF4e9HE/YMtBVWGdtXI/AAAAAAAB8Vs/jvxumCSZxPw-Oqr4cqQGf35n8ds3KLaDwCLcBGAsYHQ/s1415/benny4.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://1.bp.blogspot.com/-TJNVzF4e9HE/YMtBVWGdtXI/AAAAAAAB8Vs/jvxumCSZxPw-Oqr4cqQGf35n8ds3KLaDwCLcBGAsYHQ/s320/benny4.jpg" width="320"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Benny with his family's namesake (Chor = Bull)</td></tr></tbody></table><br/><div>[ACGS] Werner Alexi, Benny Chor, Oded Goldreich, Claus-Peter Schnorr:</div><div><div>RSA and Rabin Functions: Certain Parts are as Hard as the Whole.</div><div>SIAM J. Comput. 17(2): 194-209 (1988) <a href="https://epubs.siam.org/doi/abs/10.1137/0217013">LINK</a></div><div><br/></div><div>[BCGL] Shai Ben-David, Benny Chor, Oded Goldreich, Michael Luby:</div><div>On the Theory of Average Case Complexity. J. Comput. Syst. Sci. 44(2): 193-219 (1992) <a href="https://www.sciencedirect.com/science/article/pii/002200009290019F">LINK</a></div><div><br/></div><div>[BCS] Michael Ben-Or, Benny Chor, Adi Shamir: On the Cryptographic Security of Single RSA Bits STOC 1983: 421-430. <a href="https://dl.acm.org/doi/10.1145/800061.808773">LINK</a></div><div><br/></div><div>[CCGHHRR] Richard Chang, Benny Chor, Oded Goldreich, Juris Hartmanis, Johan Håstad, Desh Ranjan, Pankaj Rohatgi: The Random Oracle Hypothesis Is False.</div><div>J. Comput. Syst. Sci. 49(1): 24-39 (1994) <a href="https://www.sciencedirect.com/science/article/pii/S0022000005800844">LINK</a></div><div><br/></div><div>[CC] Benny Chor, Brian A. Coan:</div><div>A Simple and Efficient Randomized Byzantine Agreement Algorithm.</div><div>IEEE Trans. Software Eng. 11(6): 531-539 (1985) <a href="https://ieeexplore.ieee.org/document/1702050">LINK</a></div><div><br/></div><div>[CFNP] Benny Chor, Amos Fiat, Moni Naor, Benny Pinkas:</div><div>Tracing traitors. IEEE Trans. Inf. Theory 46(3): 893-910 (2000) <a href="https://ieeexplore.ieee.org/document/841169">LINK</a></div><div><br/></div><div>[CG97] Benny Chor, Niv Gilboa:</div><div>Computationally Private Information Retrieval. STOC 1997: 304-313 <a href="https://dl.acm.org/doi/10.1145/258533.258609">LINK</a></div><div><br/></div><div>[CG88] Benny Chor, Oded Goldreich:</div><div>Unbiased Bits from Sources of Weak Randomness and Probabilistic Communication Complexity.</div><div>SIAM J. Comput. 17(2): 230-261 (1988) <a href="https://epubs.siam.org/doi/10.1137/0217015">LINK</a></div><div><br/></div><div>[CG89] Benny Chor, Oded Goldreich:</div><div>On the power of two-point based sampling. J. Complex. 5(1): 96-106 (1989) <a href="https://doi.org/10.1016/0885-064X(89)90015-0">LINK</a></div><div><br/></div><div>[CGKS] Benny Chor, Oded Goldreich, Eyal Kushilevitz, Madhu Sudan:</div><div>Private Information Retrieval. J. ACM 45(6): 965-981 (1998) <a href="https://doi.org/10.1145/293347.293350">LINK</a></div><div><br/></div><div>[CGMA] Benny Chor, Shafi Goldwasser, Silvio Micali, Baruch Awerbuch:</div><div>Verifiable Secret Sharing and Achieving Simultaneity in the Presence of Faults.</div><div>FOCS 1985: 383-395. <a href="https://doi.org/10.1109/SFCS.1985.64">LINK</a></div></div><div><div><br/></div><div>[CK] Benny Chor, Eyal Kushilevitz:</div><div>A Zero-One Law for Boolean Privacy.  STOC 1989: 62-72. <a href="https://doi.org/10.1145/73007.73013">LINK</a></div><div><br/></div></div><div><br/></div><div><br/></div><div><br/></div></div>
    </content>
    <updated>2021-06-17T14:07:00Z</updated>
    <published>2021-06-17T14:07:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-06-28T03:00:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/082</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/082" rel="alternate" type="text/html"/>
    <title>TR21-082 |  Hardness on any Samplable Distribution Suffices: New Characterizations of One-Way Functions by Meta-Complexity | 

	Rahul Santhanam, 

	Hanlin Ren, 

	Rahul Ilango</title>
    <summary>We show that one-way functions exist if and only if there is some samplable distribution D such that it is hard to approximate the Kolmogorov complexity of a string sampled from D. Thus we characterize the existence of one-way functions by the average-case hardness of a natural \emph{uncomputable} problem on samplable distributions, extending a recent line of work by Liu and Pass (FOCS'20, STOC'21) and Ren and Santhanam (CCC'21).

    We also show that the average-case hardness of approximating Minimum Circuit Size on a locally samplable distribution (where the sampler runs in sub-linear time by using random access to its input) is equivalent to the existence of one-way functions. This is the first characterization of one-way functions by a natural average-case hardness assumption on the Minimum Circuit Size Problem. We present several other characterizations and connections between one-way functions and average-case hardness of meta-complexity problems (problems about complexity) on samplable distributions.

    We give various applications of these results to the foundations of cryptography and the theory of meta-complexity. We show that the average-case hardness of deciding k-SAT or Clique on any samplable distribution of high enough entropy implies the existence of one-way functions. Thus one-way functions follow from general assumptions on the average-case hardness of NP-complete problems. We observe that our assumptions are implied by standard cryptographic assumptions such as the Planted Clique hypothesis and the pseudorandomness of Goldreich's local functions.

    Our results imply a range of \emph{equivalences} between various meta-complexity problems, showing that the theory of meta-complexity is very \emph{robust} when considering average-case complexity. We use our results to unconditionally solve various meta-complexity problems in CZK (computational zero-knowledge) on average, and give implications of our results for the classic question of proving NP-hardness for the Minimum Circuit Size Problem.</summary>
    <updated>2021-06-16T15:57:38Z</updated>
    <published>2021-06-16T15:57:38Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-28T03:20:40Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/06/15/linkage</id>
    <link href="https://11011110.github.io/blog/2021/06/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>What is a natural question? (\(\mathbb{M}\)) Gasarch on distinguishing notions of interestingness of mathematical problems based on ability to answer them, versus whether they lead to deeper mathematics.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2021/05/what-is-natural-question-who-should.html">What is a natural question?</a> (<a href="https://mathstodon.xyz/@11011110/106339474749439957">\(\mathbb{M}\)</a>) Gasarch on distinguishing notions of interestingness of mathematical problems based on ability to answer them, versus whether they lead to deeper mathematics.</p>
  </li>
  <li>
    <p>Bad geometry in an illustration of parallel parking advice to change steering lock-to-lock when you reach a 45° angle (<a href="https://mathstodon.xyz/@11011110/106345620085904810">\(\mathbb{M}\)</a>): this car is going to end up halfway over the curb. Really the inflection angle depends on the distance from curb and your car’s turning radius. You should switch direction when the rear axle midpoint is halfway between its starting and ending lines of motion, or approximately when the passenger rear wheel is a bit less than halfway to the curb. This car went past that point and can’t straighten before reaching the curb.</p>

    <p style="text-align: center;"><img alt="Top-down view of parallel parking, with the path of the rear-axle centerline shown in red. With the centerline ending up on the curb at the end of the path, the car itself will be well over the curb." src="https://11011110.github.io/blog/assets/2021/parallel-park.jpg" width="60%"/></p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-identify-threshold-at-which-shapes-give-way-20210603/">How smooth can Nash–Kuiper embeddings be?</a> (<a href="https://mathstodon.xyz/@11011110/106347809760355726">\(\mathbb{M}\)</a>) Recent progress identifies a strict threshold at \(C^{1,1/2}\). A new popularized and high-level survey on <em>Quanta</em>. See Wikipedia for more on the <a href="https://en.wikipedia.org/wiki/Nash_embedding_theorem">Nash embedding theorem</a> (here, in the form that all 2d surfaces have non-smooth embeddings in 3d preserving distances along surface curves) and on <a href="https://en.wikipedia.org/wiki/H%C3%B6lder_condition">the notions of fractional smoothness considered here</a>, left unexplained by the <em>Quanta</em> article.</p>
  </li>
  <li>
    <p><a href="https://xenaproject.wordpress.com/2021/06/05/half-a-year-of-the-liquid-tensor-experiment-amazing-developments/">Interesting progress report by Peter Scholze on the formalized and computer-verified proof of some difficult material in analytic geometry</a> (<a href="https://mathstodon.xyz/@11011110/106359088815982703">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="http://web.stanford.edu/~mleake/projects/paperpiecing/">A mathematical foundation for foundation paper pieceable quilts</a> (<a href="https://mathstodon.xyz/@11011110/106362497443237500">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/191666/Quilters-are-the-best-at-graph-theory">via</a>). They formalize things by hypergraphs, but I prefer <a href="https://en.wikipedia.org/wiki/Antimatroid">antimatroids</a>: a pattern is piecable if and only if you can slice off one piece at a time by guillotine cuts, eventually slicing everything. The possible slicing orders form an antimatroid, because a piece, once sliceable, remains so until it is sliced. The piecing order is the reverse of a slicing order.</p>
  </li>
  <li>
    <p>Firefox’s “reader mode”, in which you can click on little page-with-text icon next to the url, and get view of text stripped of extraneous framing, is useful (<a href="https://mathstodon.xyz/@11011110/106370852134898457">\(\mathbb{M}\)</a>), but it would be more useful <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1466044">if it actually worked for pages with mathematical content</a>.</p>
  </li>
  <li>
    <p>One of the footpaths through my neighborhood is next to a small field with two chess tables (<a href="https://mathstodon.xyz/@11011110/106372538313055882">\(\mathbb{M}\)</a>). They’ve been overgrown and largely unused for quite a while, but a few months back the neighborhood association’s gardeners cleared the field and planted some ginkgo trees around the tables. Someone has been playing chess there with pine cones:</p>

    <p style="text-align: center;"><img alt="UCI Ecological Preserve" src="https://www.ics.uci.edu/~eppstein/pix/uhchess/Chess-m.jpg" style="border-style: solid; border-color: black;" width="80%"/></p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/why-is-quantum-computing-so-hard-to-explain-20210608/">Scott Aaronson, in <em>Quanta</em>, on why the glib popularizations of quantum computing typically found in publications like <em>Quanta</em> are usually wrong</a> (<a href="https://mathstodon.xyz/@11011110/106377151244377180">\(\mathbb{M}\)</a>, <a href="https://www.scottaaronson.com/blog/?p=5539">see also</a>).</p>
  </li>
  <li>
    <p><a href="https://nautil.us/issue/23/dominoes/the-amazing-autotuning-sandpile">The amazing, autotuning sandpile</a> (<a href="https://mathstodon.xyz/@11011110/106389220646874351">\(\mathbb{M}\)</a>). Jordan Ellenberg on <a href="https://en.wikipedia.org/wiki/Abelian_sandpile_model">abelian sandpile models</a>, with pretty pictures of the fractal patterns they generate, from 2015. These patterns are still a topic of active research; see e.g. “<a href="https://annals.math.princeton.edu/2017/186-1/p01">The Apollonian structure of integer superharmonic matrices</a>”, <em>Ann. Math.</em> 2017, , and “<a href="https://doi.org/10.1073/pnas.1812015116">Harmonic dynamics of the abelian sandpile</a>”, <em>PNAS</em> 2019.</p>
  </li>
  <li>
    <p><a href="https://mathemalchemy.org/2021/01/19/dodecahedral-trajectory/">Dodecahedral trajectory</a> (<a href="https://mathstodon.xyz/@11011110/106395952293245848">\(\mathbb{M}\)</a>). Mathematical glass artist Bronna Butler visualizes closed geodesics from and returning to a single vertex on the dodecahedron, using stained glass. These paths exist only on the dodecahedron, among the Platonic solids: geodesics starting from a vertex on the other solids will either hit another vertex before returning, or never return.</p>
  </li>
  <li>
    <p><a href="https://www.wired.com/story/twitter-photo-crop-algorithm-favors-white-faces-women/">Automatic face-recognition systems used to auto-crop social media photos discovered to be racist and sexist</a> (<a href="https://mathstodon.xyz/@11011110/106402230076815482">\(\mathbb{M}\)</a>). This sort of thing is part of a long line of stories of computer systems learning to replicate the biases of their developers, and one of many reasons we need greater diversity in CS.</p>
  </li>
  <li>
    <p><em><a href="https://doi.org/10.1007/978-981-15-4470-5">Introduction to Computational Origami</a></em> (<a href="https://mathstodon.xyz/@11011110/106407675350595758">\(\mathbb{M}\)</a>), Ryuhei Uehara’s 2020 book, is the subject of <a href="https://www.maa.org/press/maa-reviews/introduction-to-computational-origami">a new featured review by Tom Hull on <em>MAA Reviews</em></a>.</p>
  </li>
  <li>
    <p><a href="http://gregegan.net/SCIENCE/Howell/Howell.html">Howell’s moving orbits</a> (<a href="https://mathstodon.xyz/@11011110/106413218848247270">\(\mathbb{M}\)</a>). Greg Egan explains the early research of <a href="https://en.wikipedia.org/wiki/Kathleen_Howell">Kathleen Howell</a> on the existence of first-order-stable <a href="https://en.wikipedia.org/wiki/Halo_orbit">halo orbits</a> of a third small body under the gravitational influence of two larger bodies, related to the (unstable) L1 and L2 Lagrange points.</p>
  </li>
  <li>
    <p><a href="http://gallery.bridgesmathart.org/exhibitions/2021-bridges-conference">The 2021 Bridges Conference Mathematical Art Galleries</a> (<a href="https://mathstodon.xyz/@11011110/106418890356129959">\(\mathbb{M}\)</a>) are online even though <a href="https://www.bridgesmathart.org/b2021/">the online conference itself is not until August</a>. I haven’t yet had time to explore all of them but there are lots of good pieces in there.</p>
  </li>
</ul></div>
    </content>
    <updated>2021-06-15T23:31:00Z</updated>
    <published>2021-06-15T23:31:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-06-16T06:33:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18896</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/" rel="alternate" type="text/html"/>
    <title>Thursday—June 17th—A Debate on Program Correctness</title>
    <summary>There are two ways to write error free programs; only the third one works–Alan Perlis Perlis on Coding Joy source Alan Perlis, the first Turing Award winner, summarized the whole issue of program correctness in his single quote. Maybe there is nothing more to say about it. But this coming Thursday, if you want to […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>There are two ways to write error free programs; only the third one works–Alan Perlis</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/1_qkgjghs7wcpoq09rklc_jq/" rel="attachment wp-att-18919"><img alt="" class="alignright wp-image-18919" height="130" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/1_QkGJGhS7wCPoq09rkLC_JQ.jpeg?resize=200%2C130&amp;ssl=1" width="200"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Perlis on Coding Joy <a href="https://www.freecodecamp.org/news/perlis-on-tech-evangelism-10-epigrams-5ea6dcf3faf5/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Alan Perlis, the first Turing Award winner, summarized the whole issue of program correctness in his single quote.  Maybe there is nothing more to say about it.</p>
<p>
But this coming Thursday, if you want to hear more about it, you can tune in for a debate with Rich de Millo and myself.  It will be 7:00–8:30pm Eastern time.</p>
<p>
Harry Lewis of Harvard University will moderate.  It will be broadcast <a href="https://scp.cc.gatech.edu/2021/05/26/debate-that-changed-programming-living-history/">LIVE HERE</a>. It’s free.</p>
<p>
 <br/>
<a href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/pic-10/" rel="attachment wp-att-18898"><img alt="" class="aligncenter wp-image-18898" height="312" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/pic.png?resize=550%2C312&amp;ssl=1" width="550"/></a></p>
<p/><p>
 <br/>
</p><h2> The Paper </h2><p/>
<p/><p>
Perlis and Rich DeMillo and I wrote the <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">paper</a> “Social Processes and Proofs of Theorems<br/>
and Programs” in the middle of the second half of the last century—pushing towards fifty years ago. We were responding to the then-common view that programs should be proved correct—proved in the same manner that one proves theorems like: </p>
<blockquote><p><b>Theorem 1 (Euclid)</b> <em> There are an infinite number of prime numbers. </em>
</p></blockquote>
<p>We begged to object: We felt that correctness of programs was fundamentally a different issue. This is what the debate is all about. </p>
<p>
The debate will give us all a chance to reflect now almost 50 years later when programs enrich and regulate so much more of our lives.  Perhaps the real winner in society goes according to the proverb: “the one who ends up with the most toys.”  In that case, Harry the moderator will win hands down:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/harrylewis_harvard_officeknickknacks/" rel="attachment wp-att-18914"><img alt="" class="aligncenter wp-image-18914" height="160" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/HarryLewis_Harvard_OfficeKnickKnacks.jpg?resize=240%2C160&amp;ssl=1" width="240"/></a></p>
<p>
</p><p/><h2> The Writing </h2><p/>
<p/><p>
Mary-Claire van Leunen is an expert on writing, especially for technical articles. See her famous <a href="https://www.amazon.com/s?i=stripbooks&amp;rh=p_27%3AMary-Claire+van+Leunen&amp;s=relevancerank&amp;text=Mary-Claire+van+Leunen&amp;ref=dp_byline_sr_book_1">book</a>.</p>
<p>
The article was heavily written by Rich with input from Alan and myself. The ideas are due to all of us. The actual details, the words, the punctuation owe more to Rich with strong input from Mary-Claire. </p>
<p>
Our paper is one of forty-six papers included in the <a href="https://mitpress.mit.edu/books/ideas-created-future">book</a> <i>Ideas That Created the Future: Classic Papers of Computer Science</i>, which was edited by Harry for MIT Press (2020).  They are presented in chronological order beginning with Aristotle (~350 BCE).  The median year is 1962-63.  </p>
<p/><h2> Open Problems </h2><p/>
<p/><p>
See <a href="https://www.sigplan.org/Resources/Advice/VanLeunen-Lipton/">How to Have Your Abstract Rejected</a> for a tongue-in-cheek view of writing technical material. It was written by Mary-Claire and myself.</p>
<p>
See you Thursday. </p>
<p/></font></font></div>
    </content>
    <updated>2021-06-15T14:57:13Z</updated>
    <published>2021-06-15T14:57:13Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="correctness"/>
    <category term="debate"/>
    <category term="DeMillo"/>
    <category term="Lipton"/>
    <category term="Perlis"/>
    <category term="programs"/>
    <category term="Proof"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-06-28T03:20:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/081</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/081" rel="alternate" type="text/html"/>
    <title>TR21-081 |  Superpolynomial Lower Bounds Against Low-Depth Algebraic Circuits | 

	Sébastien Tavenas, 

	Srikanth Srinivasan, 

	Nutan Limaye</title>
    <summary>An Algebraic Circuit for a polynomial $P\in F[x_1,\ldots,x_N]$ is a computational model for constructing the polynomial $P$ using only additions and multiplications. It is a \emph{syntactic} model of computation, as opposed to the Boolean Circuit model, and hence lower bounds for this model are widely expected to be easier to prove than lower bounds for Boolean circuits. Despite this, we do not have superpolynomial lower bounds against general algebraic circuits of depth 3 (except over constant-sized finite fields) and depth 4 (over any field), while constant-depth Boolean circuit lower bounds have been known since the early 1980s.

In this paper, we prove the first superpolynomial lower bounds against general algebraic circuits of all constant depths over all fields of characteristic $0$ (or large). We also prove the first lower bounds against homogeneous algebraic circuits of constant depth over any field.

Our approach is surprisingly simple. We first prove superpolynomial lower bounds for constant-depth Set-Multilinear circuits. While strong lower bounds were already known against such circuits, most previous lower bounds were of the form $f(d)\cdot poly(N)$, where $d$ denotes the degree of the polynomial.  In analogy with Parameterized complexity, we call this an FPT lower bound. We extend a well-known technique of Nisan and Wigderson (FOCS 1995) to prove non-FPT lower bounds against constant-depth set-multilinear circuits computing the Iterated Matrix Multiplication polynomial $IMM_{n,d}$ (which computes a fixed entry of the product of $d$ $n\times n$ matrices). More precisely, we prove that  any set-multilinear circuit of depth $\Delta$ computing $IMM_{n,d}$ must have size at least $n^{d^{\exp(-O(\Delta))}}.$ This result holds over any field.

We then show how to convert any constant-depth algebraic circuit of size $s$ to a constant-depth set-multilinear circuit with a blow-up in size that is exponential in $d$ but only polynomial in $s$ over fields of characteristic $0$. (For depths greater than $3$, previous results of this form increased the depth of the resulting circuit to $\Omega(\log s)$.) This implies our constant-depth circuit lower bounds for $d$ that is a slow-growing function of $n$.

Finally, we observe that our superpolynomial lower bound for constant-depth circuits  implies the first deterministic sub-exponential time algorithm for solving the Polynomial Identity Testing (PIT) problem for all small depth circuits  using the known connection between algebraic hardness and randomness.</summary>
    <updated>2021-06-15T05:50:00Z</updated>
    <published>2021-06-15T05:50:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-06-28T03:20:40Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2021-06-14-neither-non-equivocation-nor-transferability-alone-is-enough-for-tolerating-minority-corruptions-in-asynchrony/</id>
    <link href="https://decentralizedthoughts.github.io/2021-06-14-neither-non-equivocation-nor-transferability-alone-is-enough-for-tolerating-minority-corruptions-in-asynchrony/" rel="alternate" type="text/html"/>
    <title>Neither Non-equivocation nor Transferability alone is enough for tolerating minority corruptions in asynchrony</title>
    <summary>In this post, we explore a theorem of Clement, Junqueira, Kate, and Rodrigues from PODC 2012 regarding the limits of non-equivocation. Informally, this theorem says that neither Non-equivocation nor Transferability alone is enough for tolerating minority corruptions in asynchrony. Theorem CJKR12: Neither non-equivocation nor transferability is individually sufficient to solve...</summary>
    <updated>2021-06-14T16:04:00Z</updated>
    <published>2021-06-14T16:04:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2021-06-27T22:53:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8148</id>
    <link href="https://windowsontheory.org/2021/06/14/machine-learning-for-algorithms-virtual-workshop/" rel="alternate" type="text/html"/>
    <title>Machine Learning for Algorithms – virtual workshop</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">[H/T Jelani Nelson] In recent years there has been increasing interest in using machinelearning to improve the performance of classical algorithms incomputer science, by fine-tuning their behavior to adapt to theproperties of the input distribution. This “data-driven” or“learning-based” approach to algorithm design has the potential tosignificantly improve the efficiency of some of the most widely … <a class="more-link" href="https://windowsontheory.org/2021/06/14/machine-learning-for-algorithms-virtual-workshop/">Continue reading <span class="screen-reader-text">Machine Learning for Algorithms – virtual workshop</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>[H/T Jelani Nelson]</p>



<p>In recent years there has been increasing interest in using machine<br/>learning to improve the performance of classical algorithms in<br/>computer science, by fine-tuning their behavior to adapt to the<br/>properties of the input distribution. This “data-driven” or<br/>“learning-based” approach to algorithm design has the potential to<br/>significantly improve the efficiency of some of the most widely used<br/>algorithms. For example, it has been used to design better data<br/>structures, online algorithms, streaming and sketching algorithms,<br/>market mechanisms and algorithms for combinatorial optimization,<br/>similarity search and inverse problems. This virtual workshop will<br/>feature talks from experts at the forefront of this exciting area.</p>



<p>The workshop will take place virtually on <strong>July 13-14, 2021</strong>. <br/>Registration is <strong>free but mandatory</strong>. Link to register: <a href="https://fodsi.us/ml4a.html" rel="noreferrer noopener" target="_blank">https://fodsi.us/ml4a.html</a></p>



<p><strong>Confirmed Speakers:</strong></p>



<ul><li>Alex Dimakis (UT Austin)</li><li>Yonina Eldar (Weizmann)</li><li>Anna Goldie (Google Brain, Stanford)</li><li>Reinhard Heckel (Technical University of Munich)</li><li>Stefanie Jegelka (MIT)</li><li>Tim Kraska (MIT)</li><li>Benjamin Moseley (CMU)</li><li>David Parkes (Harvard)</li><li>Ola Svensson (EPFL)</li><li>Tuomas Sandholm (CMU, Optimized Markets, Strategy Robot, Strategic Machine)</li><li>Sergei Vassilvitski (Google)</li><li>Ellen Vitercik (CMU/UC Berkeley)</li><li>David Woodruff (CMU)</li></ul>



<p><strong>Organizers:</strong></p>



<ul><li>Costis Daskalakis (MIT)</li><li>Paul Hand (Northeastern)</li><li>Piotr Indyk (MIT)</li><li>Michael Mitzenmacher (Harvard)</li><li>Ronitt Rubinfeld (MIT)</li><li>Jelani Nelson (UC Berkeley)</li></ul>



<h2 class="has-text-color" style="color: #a30023;"><strong>Unrelated announcement: JACM looking for editor in chief</strong> </h2>



<p>(h/t Salil Vadhan)</p>



<p>The Journal of the ACM is looking for a new editor in chief: see the <a href="https://dl.acm.org/pb-assets/static_journal_pages/jacm/pdf/JACM-Call-2021-BAM-1623181914103.pdf">call for nominations</a>. The (soft) deadline to submit nominations (including self nominations) is <strong>July 19th</strong> and you can do so by emailing  Chris Hankin at c.hankin@imperial.ac.uk </p>



<p/></div>
    </content>
    <updated>2021-06-14T13:29:56Z</updated>
    <published>2021-06-14T13:29:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-06-28T03:21:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18876</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/06/12/tardos-is-off-with-good-behavior/" rel="alternate" type="text/html"/>
    <title>Tardos Is Off With Good Behavior</title>
    <summary>A call for nominations for her successor In her original orange Éva Tardos’s sentence as Editor-in-Chief (EiC) of the Journal of the ACM (JACM) is coming to an end. She has served almost six years. She will be paroled soon for her good behavior. Today we announce that a grand jury is being convened. This […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A call for nominations for her successor</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/06/12/tardos-is-off-with-good-behavior/20170912-142623-1_edit/" rel="attachment wp-att-18887"><img alt="" class="alignright size-full wp-image-18887" height="123" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/20170912-142623-1_edit.jpg?resize=114%2C123&amp;ssl=1" width="114"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">In her original <a href="https://research.cornell.edu/researchers/eva-tardos">orange</a></font></td>
</tr>
</tbody>
</table>
<p>
Éva Tardos’s sentence as Editor-in-Chief (EiC) of the <i>Journal of the ACM</i> (JACM) is coming to an end. She has served almost six years. She will be paroled soon for her good behavior. </p>
<p>
Today we announce that a grand jury is being convened. This group is charged with finding the right person to be indicted as the next editor-in-chief of the JACM.<br/>
<span id="more-18876"/></p>
<p>
Tips on possible suspects should be directed to lieutenant Chris Hankin <a href="https://www.imperial.ac.uk/people/c.hankin">here</a>. Note that the location is not far from Scotland Yard. He is in charge of the grand jury:</p>
<p>
Laura Haas (University of Massachusetts Amherst, US) <br/>
Orna Kupferman (Hebrew University, Israel) <br/>
Marta Kwiatkowska (University of Oxford, UK) <br/>
Brad Myers (Carnegie Mellon University, US) <br/>
Salil Vadhan (Harvard University, US) <br/>
Albert Zomaya (University of Sydney, Australia) <br/>
Divesh Srivastava (AT&amp;T, US), ACM Pubs Board Liaison</p>
<p>
</p><p/><h2> History </h2><p/>
<p/><p>
The JACM was established in 1954 and has published over 3,000 papers, since then. There were 196 submissions received in 2020 alone. The current editorial board is <a href="https://dl.acm.org/journal/jacm/editorial-board#editor-in-chief">this</a>. Previous editors-in-chief are:</p>
<p>
1954 – 1958: Franz Alt<br/>
1959 – 1962: Mario Juncosa<br/>
1963 – 1965: Richard Hamming<br/>
1966 – 1968: Calvin Gotlieb<br/>
1969 – 1972: Gerard Salton<br/>
1973 – 1975: Raymond Miller<br/>
1976 – 1979: Edward Coffman, Jr.<br/>
1979 – 1982: Michael Garey<br/>
1983 – 1986: Michael Fischer<br/>
1986 – 1990: Daniel Rosenkrantz<br/>
1991 -1997: Tom Leighton<br/>
1997 – 2003: Joseph Halpern<br/>
2003 – 2009: Prabhakar Raghavan<br/>
2009 – 2015: Victor Vianu<br/>
2015 – currently: Éva Tardos</p>
<p>
</p><p/><h2> Selection </h2><p/>
<p/><p>
Typically many candidates are available, but only the most dramatic cases will be considered. Nominations should include a vita along with a brief statement of why the nominee should be considered. Why they are <b>un</b>-worthy to be made into the editor-in-chief? A vision statement is highly encouraged. The relevant law is found here:</p>
<p>
<a href="https://www.acm.org/publications/policies/roles-and-responsibilities">Roles and Responsibilities in ACM Publishing</a></p>
<p>
<a href="https://www.acm.org/publications/policies/eic-evaluation">ACM’s Evaluation Criteria for Editors-in-Chief</a></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>All kidding aside. We thank Éva for her hard work keeping the JACM one of the top journals in the world. We can only hope that the next chief will be nearly as sucessful as she has been.</p></font></font></div>
    </content>
    <updated>2021-06-12T12:25:16Z</updated>
    <published>2021-06-12T12:25:16Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="chief"/>
    <category term="editor"/>
    <category term="JACM"/>
    <category term="search"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-06-28T03:20:50Z</updated>
    </source>
  </entry>
</feed>
