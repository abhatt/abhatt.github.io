<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-04-23T20:44:18Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8083</id>
    <link href="https://windowsontheory.org/2021/04/23/tcs-women-rising-star-nominations/" rel="alternate" type="text/html"/>
    <title>TCS Women Rising star nominations</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">(Guest post by Virginia Vassilevska Williams) Dear colleagues We invite you to nominate speakers for our TCS Women Rising Star talks at the TCS Women Spotlight Workshop at STOC 2021. To be eligible, your nominee has to be a theoretical computer science researcher (all topics represented at STOC are welcome) who is female or an … <a class="more-link" href="https://windowsontheory.org/2021/04/23/tcs-women-rising-star-nominations/">Continue reading <span class="screen-reader-text">TCS Women Rising star nominations</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>(Guest post by Virginia Vassilevska Williams) <br/></em><br/></p>



<p>Dear colleagues</p>



<p>We invite you to nominate speakers for our TCS Women Rising Star talks at the TCS Women Spotlight Workshop at STOC 2021. To be eligible, your nominee has to be a theoretical computer science researcher (all topics represented at STOC are welcome) who is female or an underrepresented minority, and is a graduating PhD student or a postdoc. You can make your nomination by filling this form by May 15th:  <a href="https://forms.gle/g4mTS2MJzkenKrry6" rel="noreferrer noopener" target="_blank">https://forms.gle/g4mTS2MJzkenKrry6</a></p>



<p>The TCS Women Spotlight workshop at STOC 2021 will take place virtually between June 21st and June 25th (most likely on Tuesday, June 22<sup>nd</sup>, to be confirmed later on).</p>



<p>You can see the list of speakers from last year here: <a href="https://sigact.org/tcswomen/3rd-tcs-women-meeting/tcs-women-2020/">https://sigact.org/tcswomen/3rd-tcs-women-meeting/tcs-women-2020/</a></p>



<p/>



<p>Looking forward to your nominations and to seeing you at the TCS Women Spotlight Workshop!</p>



<p>Virginia Vassilevska Williams, Barna Saha, Sofya Raskhodnikova, Mary Wootters and Elena Grigorescu</p></div>
    </content>
    <updated>2021-04-23T15:10:25Z</updated>
    <published>2021-04-23T15:10:25Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-04-23T20:41:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/057</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/057" rel="alternate" type="text/html"/>
    <title>TR21-057 |  Hardness of KT Characterizes Parallel Cryptography | 

	Hanlin Ren, 

	Rahul Santhanam</title>
    <summary>A recent breakthrough of Liu and Pass (FOCS'20) shows that one-way functions exist if and only if the (polynomial-)time-bounded Kolmogorov complexity K^t is bounded-error hard on average to compute. In this paper, we strengthen this result and extend it to other complexity measures:

1. We show, perhaps surprisingly, that the KT complexity is bounded-error average-case hard if and only if there exist one-way functions in constant parallel time (i.e. NC^0). This result crucially relies on the idea of randomized encodings. Previously, a seminal work of Applebaum, Ishai and Kushilevitz (FOCS'04; SICOMP'06) used the same idea to show that NC^0-computable one-way functions exist if and only if logspace-computable one-way functions exist.
	
2. Inspired by the above result, we present randomized average-case reductions among the NC^1-versions and logspace-versions of K^t complexity, and KT complexity. Our reductions preserve both bounded-error average-case hardness and zero-error average-case hardness. To the best of our knowledge, this is the first reduction between KT complexity and a variant of K^t complexity.

3. We prove tight connections between the hardness of K^t complexity and the hardness of (the hardest) one-way functions. In analogy with the Exponential-Time Hypothesis and its variants, we define and motivate the Perebor Hypotheses for complexity measures such as K^t and KT. We show that a Strong Perebor Hypothesis for K^t implies the existence of (weak) one-way functions of near-optimal hardness 2^{n-o(n)}. To the best of our knowledge, this is the first construction of one-way functions of near-optimal hardness based on a natural complexity assumption about a search problem.

4. We show that a Weak Perebor Hypothesis for MCSP implies the existence of one-way functions, and establish a partial converse. This is the first unconditional construction of one-way functions from hardness of MCSP over a natural distribution.

5. Finally, we study the average-case hardness of MKtP. We show that it characterizes cryptographic pseudorandomness in one natural regime of parameters, and complexity-theoretic pseudorandomness in another natural regime.</summary>
    <updated>2021-04-23T13:36:00Z</updated>
    <published>2021-04-23T13:36:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-23T20:38:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/056</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/056" rel="alternate" type="text/html"/>
    <title>TR21-056 |  On the Possibility of Basing Cryptography on $\EXP \neq \BPP$ | 

	Yanyi Liu, 

	Rafael Pass</title>
    <summary>Liu and Pass (FOCS'20) recently demonstrated an equivalence between the existence of one-way functions (OWFs) and mild average-case hardness of the time-bounded Kolmogorov complexity problem. In this work, we establish a similar equivalence but to a different form of time-bounded Kolmogorov Complexity---namely, Levin's notion of Kolmogorov Complexity---whose hardness is closely related to the problem of whether $\EXP \neq \BPP$. In more detail, let $Kt(x)$ denote the Levin-Kolmogorov Complexity of the string $x$; that is, $Kt(x) = \min_{\desc \in \bitset^*, t \in \N}\{|\desc| + \lceil \log t \rceil: U(\desc, 1^t) = x\}$, where $U$ is a universal Turing machine, and $U(\desc,1^t)$ denotes the output of the program $\Pi$ after $t$ steps, and let $\mktp$ denote the language of pairs $(x,k)$ having the property that $Kt(x) \leq k$.
We demonstrate that:
- $\mktp \notin \HeurpBPP$ (i.e., $\mktp$ is infinitely-often \emph{two-sided error} mildly average-case hard) iff infinititely-often OWFs exist.
- $\mktp \notin \AvgpBPP$ (i.e., $\mktp$ is infinitely-often \emph{errorless} mildly average-case hard) iff $\EXP \neq \BPP$.
Thus, the only ``gap'' towards getting (infinitely-often) OWFs from the assumption that $\EXP \neq \BPP$ is the seemingly ``minor'' technical gap between two-sided error and errorless average-case hardness of the $\mktp$ problem. As a corollary of this result, we additionally demonstrate that any reduction from errorless to two-sided error average-case hardness for $\mktp$ implies (unconditionally) that $\NP \neq \P$. 

We finally consider other alternative notions of Kolmogorov complexity---including space-bounded Kolmogorov complexity and conditional Kolmogorov complexity---and show how average-case hardness of problems related to them characterize log-space computable OWFs, or OWFs in $\NC^0$.</summary>
    <updated>2021-04-23T09:45:30Z</updated>
    <published>2021-04-23T09:45:30Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-23T20:38:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1199907254169036789</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1199907254169036789/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/the-million-dollar-sermon.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1199907254169036789" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1199907254169036789" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/the-million-dollar-sermon.html" rel="alternate" type="text/html"/>
    <title>The Million Dollar Sermon</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Illinois Tech has one of the greatest origin stories for a university. In 1890 Frank Gunsaulus, a pastor on the south side of Chicago, gave a sermon where he said "If I had a million dollars I would build a school to provide students from all backgrounds meaningful roles in a changing industrial society". Philip Armour, a wealthy industrialist in the congregation, went up to Gunsaulus after the sermon and said, "if you give me five years for this school, I'll give you the million dollars". Thus started the Armour Institute of Technology which after some mergers became the Illinois Institute of Technology.</p><p>The "million dollar sermon" really happened, though the exact wording and even the exact year are lost to posterity or, as speculated, hidden in a cornerstone of one of the original campus buildings. </p><p>In 1890 we were in the beginnings of the second industrial revolution, a revolution of communication, transportation, electrification and soon the assembly line. The first revolution happened a century earlier with mechanization and the steam engine. The third revolution was computers and automation, and we are now in the early parts of the fourth industrial revolution, one based on data and information. </p><p>There are many parallels between 1890 and today. Like 1890, the private economy is dominated by a small number of large companies that have an outsized influence in our society. Like 1890, technology is changing faster than we can manage it. Like 1890, many workers are finding their skills quickly outdated.</p><p>Today Gunsaulus's words ring truer than ever. We more than ever need to provide students of all backgrounds meaningful roles in a changing technological society. </p></div>
    </content>
    <updated>2021-04-22T14:52:00Z</updated>
    <published>2021-04-22T14:52:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-04-23T07:12:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18622</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/" rel="alternate" type="text/html"/>
    <title>Ken Turns 40</title>
    <summary>Every chess master was once a beginner—Irving Chernev Ken Regan started his Ph.D. 40 years ago from Oxford, in complexity theory, advised by Dominic Welsh. Ken continues to be a researcher in complexity theory, a teacher of computer science, a mentor of graduate students, a writer of books and more. He has long been on […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Every chess master was once a beginner—Irving Chernev</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/ken2/" rel="attachment wp-att-18624"><img alt="" class="alignright  wp-image-18624" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ken2.png?w=150&amp;ssl=1"/></a></p>
<p>
Ken Regan started his Ph.D. 40 years ago from Oxford, in complexity theory, advised by Dominic <a href="https://en.wikipedia.org/wiki/Dominic_Welsh">Welsh</a>. Ken continues to be a researcher in complexity theory, a teacher of computer science, a mentor of graduate students, a writer of books and more. He has long been on the faculty of the <a href="https://cse.buffalo.edu/~regan/">Department of Computer Science</a> and Engineering at Buffalo. </p>
<p>
Today I thought I would thank Ken for his many wonderful accomplishments. </p>
<p>
Ken is a great friend and long time co-author of mine. We also write this blog together. He has done many things—one colorful thing is the <a href="https://cse.buffalo.edu/~regan/papers/ComplexityPoster.jpg">poster</a>. Ken is a strong chess player—a chess International Master just below the grandmaster level and rated at 2372—roughly. </p>
<p>
We will highlight his continued work in complexity theory and his work in detecting chess cheating.</p>
<p/><h2> Complexity Theory </h2><p/>
<p/><p>
Ken and I have just released a second edition of our book on quantum algorithms: <a href="https://www.thriftbooks.com/w/introduction-to-quantum-algorithms-via-linear-algebra-second-edition_kenneth-w-regan_richard-j-lipton/26771326/item/44480377/?gclid=Cj0KCQjw1PSDBhDbARIsAPeTqrd5GXdcE5MG1a_Ue1T4vDhz4_adZNPdNAslCRxsyFUroKj6AgxRGlkaAulSEALw_wcB#idiq=44480377&amp;edition=41576984"> Introduction to Quantum Algorithms Via Linear Algebra, Second Edition</a> </p>
<blockquote><p><b> </b> <em> Quantum computing explained in terms of elementary linear algebra, emphasizing computation and algorithms and requiring no background in physics. This introduction to quantum algorithms is concise but comprehensive, covering many key algorithms. It is mathematically rigorous but requires minimal background and assumes no knowledge of quantum theory or quantum mechanics. </em>
</p></blockquote>
<p>I must add that Ken did the lion share of the work on writing this second edition.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/quantum/" rel="attachment wp-att-18625"><img alt="" class="aligncenter size-medium wp-image-18625" height="300" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/quantum.jpg?resize=201%2C300&amp;ssl=1" width="201"/></a>
</td>
</tr>
<tr>
</tr>
</tbody></table>
<p>Please note a special offer: Buy one, get another for full price. </p>
<p>
In another part of complexity theory, Ken has just had a paper accepted: <i>Multi-Structural Games and Number of Quantifiers</i> with Ronald Fagin, Jonathan Lenchner, and Nikhil Vyas. This will appear at <a href="http://easyconferences.eu/lics2021/">LICS 2021</a> this summer.</p>
<p>
The paper defines a new type of game that captures the number of quantifies needed for certain properties. This is neat—see the paper for details.</p>
<p>
</p><p/><h2> Chess Cheating—How? </h2><p/>
<p/><p>
The area of detecting chess cheating is one that Ken works in. Moreover he is perhaps the leader in the world. That is the leader in detecting when a player cheated at playing chess. </p>
<p>
What is this? In games like poker, games that rely on cards, it is long been an issue that people can <a href="https://en.wikipedia.org/wiki/Cheating_in_poker">cheat</a>. Cards can be marked for example. The dealer can try and control what cards a player gets. These are ancient issues for card games. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/card/" rel="attachment wp-att-18626"><img alt="" class="aligncenter size-medium wp-image-18626" height="182" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/card.png?resize=300%2C182&amp;ssl=1" width="300"/></a>
</td>
</tr>
<tr>
</tr>
</tbody></table>
<p>
What is this for <a href="https://en.wikipedia.org/wiki/Cheating_in_chess">chess</a>? The pieces cannot be marked to any advantage: a king is king, a pawn is a pawn and so on. But a player can cheat in a simple manner. They can use the moves of a computer program instead of their own. The problem in chess is that computer programs currently play at a level vastly higher than even a strong player. Stockfish, a top program, plays at around <a href="https://ccrl.chessdom.com/ccrl/4040/">3551</a>. </p>
<p>
Recall Ken is around 2372. That places him via the Elo <a href="https://en.wikipedia.org/wiki/Chess_rating_system">rating</a> at IM: </p>
<ul>
<li>
2500-2700	most Grandmasters (GM) <p/>
</li><li>
2400-2500	most International Masters (IM) and some Grandmasters (GM) <p/>
</li><li>
2300-2400	most FIDE Masters (FM) and some International Masters (IM) <p/>
</li><li>
2200-2300	FIDE Candidate Masters (CM), most national masters
</li></ul>
<p>Note: the computer therefore plays much better than all known GM’s. </p>
<p>
</p><p/><h2> Chess Cheating—Detecting? </h2><p/>
<p/><p>
Enter Ken. He has been studying how to detect whether or not a player is using a computer’s moves rather than their own moves. He has worked on this for many years, with good success. Note: this is a real <a href="https://www.theguardian.com/sport/2020/oct/16/chesss-cheating-crisis-paranoia-has-become-the-culture">problem</a>: </p>
<blockquote><p><b> </b> <em> In one chess tournament, five of the top six were disqualified for cheating. In another, the doting parents of 10-year-old competitors furiously rejected evidence that their darlings were playing at the level of the world No 1. </em>
</p></blockquote>
<p/><p>
An obvious idea is to not let a player have access to any computer. This is naive for several reasons: </p>
<ol>
<li>
A player can use their cellphone secretly—in the toilet? <p/>
</li><li>
A player could conceal the computer on themselves—computers are small? <p/>
</li><li>
A player could be playing chess online—impossible to stop them?
</li></ol>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/heel/" rel="attachment wp-att-18627"><img alt="" class="aligncenter size-medium wp-image-18627" height="169" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/heel.png?resize=300%2C169&amp;ssl=1" width="300"/></a>
</td>
</tr>
<tr>
</tr>
</tbody></table>
<p>
</p><p/><h2> Chess Cheating—How to Detect </h2><p/>
<p/><p>
This led Ken to consider the following problem: Suppose that some player makes a series of moves during a game. Check how well their moves agree with Stockfish or some other computer program. Then claim the player cheated provided they agree too often with the program. </p>
<p>
This sounds simple. Compare the computer’s move and the player’s moves. If these agree too often then say the player has cheated.</p>
<p>
Simple. Unfortunately not. There are many complications: </p>
<ol>
<li>
In chess sometimes moves are “in book”. Especially at the beginning the best move may be well known. <p/>
</li><li>
In chess sometimes moves are “forced”. There may be a unique legal move. <p/>
</li><li>
A player may only use the computer when the position is “hard”. <p/>
</li><li>
The computer may suggest several moves. These moves may all be about the same quality.
</li></ol>
<p>Clearly all these make the problem—did the player cheat—complex. Difficult. A further complicating factor is that even a weak player will sometimes randomly agree with the computer. That is even a weaker player can make a great move. </p>
<p>
Perhaps the shock is that Ken has been able to build software that is able to correctly decide when a player cheated or not. His method is good enough that it is used in practice on real players. It has led to actual penalties on players, and has saved others from false claims. </p>
<p>
I must add that his software is quite impressive. The test to see if a player cheated involves the running of chess computer programs. These programs like <a href="https://stockfishchess.org">Stockfish</a> were made to be used by one player. They were not designed to be used as a subroutine in an automated search. Perhaps for a complexity theorist Ken might have a top ranking for writing the most real programs. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>
Correction: Ken is 40th from Princeton as an undergrad. Ken is 35th from Oxford for Ph.D. See Ken on a 40th Reunion for Princeton Class of ’81 <a href="https://www.youtube.com/watch?v=S1LDJZglNUg">panel</a>   [<b>Ken:</b> A one-word fix did the trick, changing “obtained” to “started” in the first sentence.  It’s a Trans-Pond-er difference: I count as ’81 both at Princeton and Oxford (Merton College) because the latter goes by the starting year.  The photo is from Glenveagh Castle, Ireland, on a June 2019 trip with my family.]</p>
<p>
Ken <a href="https://www.newser.com/story/297646/pandemic-has-caused-cheating-crisis-in-chess.html">says</a>: “The pandemic has brought me as much work in a single day as I have had in a year previously,” said Prof Kenneth Regan, an international chess master and computer scientist whose model is relied on by the sport’s governing body, FIDE, to detect suspicious patterns of play. “It has ruined my sabbatical.”</p>
<p>
See this <a href="https://cse.buffalo.edu/~regan/personal/JuneCLarticleKWR.pdf">article</a> for more detail on Ken’s work. This is by Howard Goldowsky.</p>
<p/></font></font></div>
    </content>
    <updated>2021-04-22T13:00:19Z</updated>
    <published>2021-04-22T13:00:19Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="People"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-04-23T20:39:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/04/22/postdoc-in-algorithms-at-university-of-vienna-apply-by-may-16-2021/</id>
    <link href="https://cstheory-jobs.org/2021/04/22/postdoc-in-algorithms-at-university-of-vienna-apply-by-may-16-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc in Algorithms at University of Vienna (apply by May 16, 2021)</title>
    <summary>We are looking for an excellent researcher to join our project on “Fast Algorithms for a Reactive Network Layer” funded by the Austrian NSF. It will lay the algorithmic foundations of more adaptive networks, using algorithmic tools from many areas of algorithms research. This is a collaboration between the research groups of Monika Henzinger at […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are looking for an excellent researcher to join our project on “Fast Algorithms for a Reactive Network Layer” funded by the Austrian NSF. It will lay the algorithmic foundations of more adaptive networks, using algorithmic tools from many areas of algorithms research. This is a collaboration between the research groups of Monika Henzinger at U Vienna and of Stefan Schmid at TU Berlin.</p>
<p>Website: <a href="https://taa.cs.univie.ac.at/">https://taa.cs.univie.ac.at/</a><br/>
Email: applications.taa@univie.ac.at</p></div>
    </content>
    <updated>2021-04-22T09:02:27Z</updated>
    <published>2021-04-22T09:02:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-04-23T20:39:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.10668</id>
    <link href="http://arxiv.org/abs/2104.10668" rel="alternate" type="text/html"/>
    <title>On the rank of Z_2-matrices with free entries on the diagonal</title>
    <feedworld_mtime>1619049600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kogan:Eugene.html">Eugene Kogan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.10668">PDF</a><br/><b>Abstract: </b>For an $n \times n$ matrix $M$ with entries in $\mathbb{Z}_2$ denote by
$R(M)$ the minimal rank of all the matrices obtained by changing some numbers
on the main diagonal of $M$. We prove that for each non-negative integer $k$
there is a polynomial in $n$ algorithm deciding whether $R(M) \leq k$ (whose
complexity may depend on $k$). We also give a polynomial in $n$ algorithm
computing a number $m$ such that $m/2 \leq R(M) \leq m$. These results have
applications to graph drawings on non-orientable surfaces.
</p></div>
    </summary>
    <updated>2021-04-22T22:42:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-04-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.10622</id>
    <link href="http://arxiv.org/abs/2104.10622" rel="alternate" type="text/html"/>
    <title>Voxel Structure-based Mesh Reconstruction from a 3D Point Cloud</title>
    <feedworld_mtime>1619049600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lv:Chenlei.html">Chenlei Lv</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Weisi.html">Weisi Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Baoquan.html">Baoquan Zhao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.10622">PDF</a><br/><b>Abstract: </b>Mesh reconstruction from a 3D point cloud is an important topic in the fields
of computer graphic, computer vision, and multimedia analysis. In this paper,
we propose a voxel structure-based mesh reconstruction framework. It provides
the intrinsic metric to improve the accuracy of local region detection. Based
on the detected local regions, an initial reconstructed mesh can be obtained.
With the mesh optimization in our framework, the initial reconstructed mesh is
optimized into an isotropic one with the important geometric features such as
external and internal edges. The experimental results indicate that our
framework shows great advantages over peer ones in terms of mesh quality,
geometric feature keeping, and processing speed.
</p></div>
    </summary>
    <updated>2021-04-22T22:44:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-04-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.10621</id>
    <link href="http://arxiv.org/abs/2104.10621" rel="alternate" type="text/html"/>
    <title>Towards a more efficient approach for the satisfiability of two-variable logic</title>
    <feedworld_mtime>1619049600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ting-wei Lin, Chia-hsuan Lu, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Tony.html">Tony Tan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.10621">PDF</a><br/><b>Abstract: </b>We revisit the satisfiability problem for two-variable logic, denoted by
SAT(FO2), which is known to be NEXP-complete. The upper bound is usually
derived from its well known "exponential size model" property. Whether it can
be determinized/randomized efficiently is still an open question.
</p>
<p>In this paper we present a different approach by reducing it to a novel
graph-theoretic problem that we call "Conditional Independent Set" (CIS). We
show that CIS is NP-complete and present three simple algorithms for it:
Deterministic, randomized with zero error and randomized with small one-sided
error, with run time O(1.4423^n), O(1.6181^n) and O(1.3661^n), respectively.
</p>
<p>We then show that without the equality predicate SAT(FO2) is in fact
equivalent to CIS in succinct representation. This yields the same three simple
algorithms as above for SAT(FO2) without the the equality predicate with run
time O(1.4423^(2^n)), O(1.6181^(2^n)) and O(1.3661^(2^n)), respectively, where
n is the number of predicates in the input formula. To the best of our
knowledge, these are the first deterministic/randomized algorithms for an
NEXP-complete decidable logic with time complexity significantly lower than
O(2^(2^n)). We also identify a few lower complexity fragments of SAT(FO2) which
correspond to the tractable fragments of CIS.
</p>
<p>For the fragment with the equality predicate, we present a linear time
many-one reduction to the fragment without the equality predicate. The
reduction yields equi-satisfiable formulas and incurs a small constant blow-up
in the number of predicates.
</p></div>
    </summary>
    <updated>2021-04-22T22:38:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-04-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.10593</id>
    <link href="http://arxiv.org/abs/2104.10593" rel="alternate" type="text/html"/>
    <title>Acyclic, Star, and Injective Colouring: Bounding the Diameter</title>
    <feedworld_mtime>1619049600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brause:Christoph.html">Christoph Brause</a>, Petr Golovach, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martin:Barnaby.html">Barnaby Martin</a>, Daniel Paulusma, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smith:Siani.html">Siani Smith</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.10593">PDF</a><br/><b>Abstract: </b>We examine the effect of bounding the diameter for well-studied variants of
the Colouring problem. A colouring is acyclic, star, or injective if any two
colour classes induce a forest, star forest or disjoint union of vertices and
edges, respectively. The corresponding decision problems are Acyclic Colouring,
Star Colouring and Injective Colouring. The last problem is also known as
$L(1,1)$-Labelling and we also consider the framework of $L(a,b)$-Labelling. We
prove a number of (almost-)complete complexity classifications. In particular,
we show that for graphs of diameter at most $d$, Acyclic $3$-Colouring is
polynomial-time solvable if $d\leq 2$ but NP-complete if $d\geq 5$, and Star
$3$-Colouring is polynomial-time solvable if $d\leq 3$ but NP-complete for
$d\geq 8$. As far as we are aware, Star $3$-Colouring is the first problem that
exhibits a complexity jump for some $d\geq 3$. Our third main result is that
$L(1,2)$-Labelling is NP-complete for graphs of diameter $2$; we relate the
latter problem to a special case of Hamiltonian Path.
</p></div>
    </summary>
    <updated>2021-04-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-04-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.10570</id>
    <link href="http://arxiv.org/abs/2104.10570" rel="alternate" type="text/html"/>
    <title>QCSP on Reflexive Tournaments</title>
    <feedworld_mtime>1619049600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Benoit Larose, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Markovic:Petar.html">Petar Markovic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martin:Barnaby.html">Barnaby Martin</a>, Daniel Paulusma, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smith:Siani.html">Siani Smith</a>, Stanislav Zivny <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.10570">PDF</a><br/><b>Abstract: </b>We give a complexity dichotomy for the Quantified Constraint Satisfaction
Problem QCSP(H) when H is a reflexive tournament. It is well-known that
reflexive tournaments can be split into a sequence of strongly connected
components H_1,...,H_n so that there exists an edge from every vertex of H_i to
every vertex of H_j if and only if i&lt;j. We prove that if H has both its initial
and final strongly connected component (possibly equal) of size 1, then QCSP(H)
is in NL and otherwise QCSP(H) is NP-hard.
</p></div>
    </summary>
    <updated>2021-04-22T22:41:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-04-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.10402</id>
    <link href="http://arxiv.org/abs/2104.10402" rel="alternate" type="text/html"/>
    <title>PTHash: Revisiting FCH Minimal Perfect Hashing</title>
    <feedworld_mtime>1619049600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pibiri:Giulio_Ermanno.html">Giulio Ermanno Pibiri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trani:Roberto.html">Roberto Trani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.10402">PDF</a><br/><b>Abstract: </b>Given a set $S$ of $n$ distinct keys, a function $f$ that bijectively maps
the keys of $S$ into the range $\{0,\ldots,n-1\}$ is called a minimal perfect
hash function for $S$. Algorithms that find such functions when $n$ is large
and retain constant evaluation time are of practical interest; for instance,
search engines and databases typically use minimal perfect hash functions to
quickly assign identifiers to static sets of variable-length keys such as
strings. The challenge is to design an algorithm which is efficient in three
different aspects: time to find $f$ (construction time), time to evaluate $f$
on a key of $S$ (lookup time), and space of representation for $f$. Several
algorithms have been proposed to trade-off between these aspects. In 1992, Fox,
Chen, and Heath (FCH) presented an algorithm at SIGIR providing very fast
lookup evaluation. However, the approach received little attention because of
its large construction time and higher space consumption compared to other
subsequent techniques. Almost thirty years later we revisit their framework and
present an improved algorithm that scales well to large sets and reduces space
consumption altogether, without compromising the lookup time. We conduct an
extensive experimental assessment and show that the algorithm finds functions
that are competitive in space with state-of-the art techniques and provide
$2-4\times$ better lookup time.
</p></div>
    </summary>
    <updated>2021-04-22T22:43:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-04-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.10343</id>
    <link href="http://arxiv.org/abs/2104.10343" rel="alternate" type="text/html"/>
    <title>Sensitivity as a Complexity Measure for Sequence Classification Tasks</title>
    <feedworld_mtime>1619049600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Michael Hahn, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jurafsky:Dan.html">Dan Jurafsky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Futrell:Richard.html">Richard Futrell</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.10343">PDF</a><br/><b>Abstract: </b>We introduce a theoretical framework for understanding and predicting the
complexity of sequence classification tasks, using a novel extension of the
theory of Boolean function sensitivity. The sensitivity of a function, given a
distribution over input sequences, quantifies the number of disjoint subsets of
the input sequence that can each be individually changed to change the output.
We argue that standard sequence classification methods are biased towards
learning low-sensitivity functions, so that tasks requiring high sensitivity
are more difficult. To that end, we show analytically that simple lexical
classifiers can only express functions of bounded sensitivity, and we show
empirically that low-sensitivity functions are easier to learn for LSTMs. We
then estimate sensitivity on 15 NLP tasks, finding that sensitivity is higher
on challenging tasks collected in GLUE than on simple text classification
tasks, and that sensitivity predicts the performance both of simple lexical
classifiers and of vanilla BiLSTMs without pretrained contextualized
embeddings. Within a task, sensitivity predicts which inputs are hard for such
simple models. Our results suggest that the success of massively pretrained
contextual representations stems in part because they provide representations
from which information can be extracted by low-sensitivity decoders.
</p></div>
    </summary>
    <updated>2021-04-22T22:39:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-04-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/04/21/postdoc-at-university-of-birmingham-uk-apply-by-may-20-2021/</id>
    <link href="https://cstheory-jobs.org/2021/04/21/postdoc-at-university-of-birmingham-uk-apply-by-may-20-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Birmingham, UK (apply by May 20, 2021)</title>
    <summary>We seek a Research Fellow (3 yr) to work on an UKRI-funded project on co-evolution. Co-evolutionary algorithms mimick natural selection, e.g. for minmax optimisation or learning game playing strategies. The postdoc will develop time-complexity analysis of co-evolutionary algorithms, and should have a track record in theory of evolutionary computation, algorithmic game theory, and/or algorithms. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We seek a Research Fellow (3 yr) to work on an UKRI-funded project on co-evolution. Co-evolutionary algorithms mimick natural selection, e.g. for minmax optimisation or learning game playing strategies.</p>
<p>The postdoc will develop time-complexity analysis of co-evolutionary algorithms, and should have a track record in theory of evolutionary computation, algorithmic game theory, and/or algorithms.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CFK453/research-fellow-in-theory-of-co-evolutionary-computation">https://www.jobs.ac.uk/job/CFK453/research-fellow-in-theory-of-co-evolutionary-computation</a><br/>
Email: p.k.lehre@cs.bham.ac.uk</p></div>
    </content>
    <updated>2021-04-21T14:30:28Z</updated>
    <published>2021-04-21T14:30:28Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-04-23T20:39:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=554</id>
    <link href="https://tcsplus.wordpress.com/2021/04/21/tcs-talk-wednesday-april-28-ronen-eldan-weizmann-institute/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, April 28 — Ronen Eldan, Weizmann Institute</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, April 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Ronen Eldan from the Weizmann Institute will speak about “Localization, stochastic localization, and Chen’s recent breakthrough on the Kannan-Lovasz-Simonivits conjecture” (abstract below). You can reserve a spot as […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, April 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="http://www.wisdom.weizmann.ac.il/~ronene/"><strong>Ronen Eldan</strong></a> from the Weizmann Institute will speak about “<em>Localization, stochastic localization, and Chen’s recent breakthrough on the Kannan-Lovasz-Simonivits conjecture</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: The Kannan-Lovasz and Simonovits (KLS) conjecture considers the following isoperimetric problem on high-dimensional convex bodies: Given a convex body <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>, consider the optimal way to partition it into two pieces of equal volume so as to minimize their interface. Is it true that up to a universal constant, the minimal partition is attained via a hyperplane cut? Roughly speaking, this question can be thought of as asking “to what extent is a convex set a good expander”?</p>
<p>In analogy to expander graphs, such lower bounds on the capacity would imply bounds on mixing times of Markov chains associated with the convex set, and so this question has direct implications on the complexity of many computational problems on convex sets. Moreover, it was shown that a positive answer would imply Bourgain’s slicing conjecture.</p>
<p>Very recently, Yuansi Chen obtained a striking breakthrough, nearly solving this conjecture. In this talk, we will overview some of the central ideas used in the proof. We will start with the classical concept of “localization” (a very useful tool to prove concentration inequalities) and its extension, stochastic localization – the main technique used in the proof.</p></blockquote></div>
    </content>
    <updated>2021-04-21T04:51:33Z</updated>
    <published>2021-04-21T04:51:33Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-04-23T20:42:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5460</id>
    <link href="https://www.scottaaronson.com/blog/?p=5460" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5460#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5460" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Doubts about teapot supremacy: my reply to Richard Borcherds</title>
    <summary xml:lang="en-US">Richard Borcherds is a British mathematician at Berkeley, who won the 1998 Fields Medal for the proof of the monstrous moonshine conjecture among many other contributions. A couple months ago, Borcherds posted on YouTube a self-described “rant” about quantum computing, which was recently making the rounds on Facebook and which I found highly entertaining. Borcherds […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://en.wikipedia.org/wiki/Richard_Borcherds">Richard Borcherds</a> is a British mathematician at Berkeley, who won the 1998 Fields Medal for the proof of the <a href="https://en.wikipedia.org/wiki/Monstrous_moonshine">monstrous moonshine conjecture</a> among many other contributions.  A couple months ago, Borcherds posted on YouTube a <a href="https://www.youtube.com/watch?v=sFhhQRxWTIM"><strong>self-described “rant” about quantum computing</strong></a>, which was recently making the rounds on Facebook and which I found highly entertaining.</p>



<p>Borcherds points out that the term “quantum supremacy” means only that quantum computers can outperform existing classical computers on <em>some</em> benchmark, which can be chosen to show maximum advantage for the quantum computer.  He allows that <a href="https://en.wikipedia.org/wiki/Boson_sampling">BosonSampling</a> could have some value, for example in calibrating quantum computers or in comparing one quantum computer to another, but he decries the popular conflation of quantum supremacy with the actual construction of a scalable quantum computer able (for example) to run Shor’s algorithm to break RSA.</p>



<p>Borcherds also proposes a “teapot test,” according to which any claim about quantum computers can be dismissed if an analogous claim would hold for a teapot (which he brandishes for the camera).  For example, there are many claims to solve practical optimization and machine learning problems by “quantum/classical hybrid algorithms,” wherein a classical computer does most of the work but a quantum computer is somehow involved.  Borcherds points out that, at least as things stand in early 2021, in most or all such cases, the classical computer could’ve probably done as well entirely on its own.  So then if you put a teapot on top of your classical computer while it ran, you could equally say you used a “classical/teapot hybrid approach.”</p>



<p>Needless to say, Borcherds is correct about all of this.  I’ve made similar points on this blog for 15 years, although less Britishly.  I’m delighted to have such serious new firepower on the scoffing-at-QC-hype team.</p>



<p>I do, however, have one substantive disagreement.  At one point, Borcherds argues that sampling-based quantum supremacy itself fails his teapot test.  For consider the computational problem of predicting how many pieces a teapot will break into if it’s dropped on the ground.  Clearly, he says, the teapot itself will outperform any simulation running on any existing classical computer at that task, and will therefore achieve “teapot supremacy.”  But who cares??</p>



<p>I’m glad that Borcherds has set out, rather crisply, an objection that’s been put to me many times over the past decade.  The response is simple: <em>I don’t believe the teapot really does achieve teapot supremacy on the stated task!  At the least, I’d need to be shown why.  You can’t just assert it without serious argument.</em></p>



<p>If we want to mirror the existing quantum supremacy experiments, then the teapot computational problem, properly formulated, should be: given as input a description of a teapot’s construction, the height from which it’s dropped, etc., <em>output a sample from the probability distribution</em> over the number of shards that the teapot will break into when it hits the floor.</p>



<p>If so, though, then clearly a classical computer can easily sample from the same distribution!  Why?  Because presumably we agree that there’s a negligible probability of more than (say) 1000 shards.  So the distribution is characterized by a list of at most 1000 probabilities, which can be estimated empirically (at the cost of a small warehouse of smashed teapots) and thereafter used to generate samples.  In the plausible event that the distribution is (say) a Gaussian, it’s even easier: just estimate the mean and variance.</p>



<p>A couple days ago, I was curious what the distribution looked like, so I decided to order some teapots from Amazon and check.  Unfortunately, real porcelain teapots are <em>expensive</em>, and it seemed vaguely horrific to order dozens (as would be needed to get reasonable data) for the sole purpose of smashing them on my driveway.  So I hit on what seemed like a perfect solution: I ordered <em>toy</em> teapots, which were much smaller and cheaper.  Alas, when my toy “porcelain” teapots arrived yesterday, they turned out (unsurprisingly in retrospect for a children’s toy) to be some sort of plastic or composite material, meaning that they <em>didn’t</em> break unless one propelled them downward forcefully.  So, while I can report that they tended to break into one or two large pieces along with two or three smaller shards, I found it impossible to get better data.  (There’s a reason why I became a <em>theoretical</em> computer scientist…)</p>



<figure class="wp-block-image size-large"><img alt="" src="https://www.scottaaronson.com/teapot.jpg"/></figure>



<p>The good news is that my 4-year-old son had an absolute <em>blast</em> smashing toy teapots with me on our driveway, while my 8-year-old daughter was thrilled to take the remaining, unbroken teapots for her dollhouse.  I apologize if this fails to defy gender stereotypes.</p>



<p>Anyway, it might be retorted that it’s not good enough to sample from a probability distribution: what’s wanted, rather, is to calculate how many pieces this <em>specific</em> teapot will break into, given all the microscopic details of it and its environment.  Aha, this brings us to a crucial conceptual point: in order for something to count as an “input” to a computer, <em>you need to be able to set it freely</em>.  Certainly, at the least, you need to be able to measure and record the input in its entirety, so that someone trying to reproduce your computation on a standard silicon computer would know exactly which computation to do.  You don’t get to claim computational supremacy based on a problem with secret inputs: that’s like failing someone on a math test without having fully told them the problems.</p>



<p>Ability to set and know the inputs is <em>the</em> key property that’s satisfied by Google’s quantum supremacy experiment, and to a lesser extent by the USTC BosonSampling experiment, but that’s not satisfied at all by the “smash a teapot on the floor” experiment.  Or perhaps it’s better to say: influences on a computation that vary uncontrollably and chaotically, like gusts of air hitting the teapot as it falls to the floor, shouldn’t be called “inputs” at all; they’re simply <em>noise sources</em>.  And what one does with noise sources is to try to estimate their distribution and average over them—but in that case, as I said, there’s no teapot supremacy.</p>



<p>A Facebook friend said to me: that’s well and good, but surely we could change Borcherds’s teapot experiment to address this worry?  For example: add a computer-controlled lathe (or even a 3D printer), with which you can build a teapot in an arbitrary shape of your choice.  Then consider the problem of sampling from the probability distribution over how many pieces <em>that</em> teapot will smash into, when it’s dropped from some standard height onto some standard surface.  I replied that this is indeed more interesting—in fact, it already seems more like what engineers do in practice (still, sometimes!) when building wind tunnels, than like a silly reductio ad absurdum of quantum supremacy experiments.  On the other hand, <em>if</em> you believe the <a href="https://inst.eecs.berkeley.edu/~cs191/fa08/lectures/lecture17.pdf">Extended Church-Turing Thesis</a>, then as long as your analog computer is governed by classical physics, it’s presumably inherently limited to an <a href="https://en.wikipedia.org/wiki/Avogadro_constant">Avogadro’s number</a> type speedup over a standard digital computer, whereas with a quantum computer, you’re limited only by the exponential dimensionality of Hilbert space, which seems more interesting.</p>



<p>Or maybe I’m wrong—in which case, I look forward to the first practical demonstration of teapot supremacy!  Just like with quantum supremacy, though, it’s not enough to <em>assert</em> it; you need to … put the tea where your mouth is.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> On the suggestion of <a href="https://cs.nyu.edu/davise/">Ernest Davis</a>, who I can now reveal as the Facebook friend mentioned above, I just ordered some <a href="https://www.ssww.com/item/terra-cotta-pots-3-1-8-PL807/index.php?cid=3358&amp;gclid=Cj0KCQjw9_mDBhCGARIsAN3PaFOKHOxEAHqA-WjzGS5lFgass_iNz27V88AZdGe5e26GTsrJUX0ZNL4aAqOzEALw_wcB&amp;fbclid=IwAR1eBc9xXODJ3a15ukebh1loN9XNpl7aD2Xiihsb2H7JvZVBKLKC10OwIys">terra cotta flower pots</a>, which look cheap, easily smashable, and environmentally friendly, and which will hopefully be acceptable substitutes for porcelain teapots in a new experiment.  (Not that my main arguments in this post hinge on the results of such an experiment!  That’s the power of theory.)</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Another Update:</span></strong> Some of you might enjoy <a href="https://www.scientificamerican.com/article/will-quantum-computing-ever-live-up-to-its-hype/">John Horgan’s <em>Scientific American</em> column</a> on reality vs. hype in quantum computing, based on conversations with me and with Terry Rudolph of PsiQuantum.</p></div>
    </content>
    <updated>2021-04-20T18:55:39Z</updated>
    <published>2021-04-20T18:55:39Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-04-21T18:19:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.let-all.com/blog/?p=39</id>
    <link href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/" rel="alternate" type="text/html"/>
    <title>Introducing ALT Highlights 2021</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The 32nd International Conference on Algorithmic Learning Theory (ALT 2021) just wrapped up, featuring a wide selection of exciting results at the frontiers of learning theory. The proceedings and all talk recordings are available online for perusal.  Did you miss out on the conference? Don’t have time to go through all the proceedings? Fear not, […]</p>
<p>The post <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/" rel="nofollow">Introducing ALT Highlights 2021</a> appeared first on <a href="https://www.let-all.com/blog" rel="nofollow">The Learning Theory Alliance Blog</a>.</p></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The 32nd International Conference on Algorithmic Learning Theory (<a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>) just wrapped up, featuring a wide selection of exciting results at the frontiers of learning theory. The <a href="http://proceedings.mlr.press/v132/">proceedings</a> and all <a href="https://www.youtube.com/channel/UC7wMo5OivSnsQJNfZm8zmJQ/videos">talk recordings</a> are available online for perusal. </p>



<p>Did you miss out on the conference? Don’t have time to go through all the proceedings? Fear not, the <a href="https://let-all.com/">Learning Theory Alliance</a> is pleased to bring you ALT Highlights, a series of blog posts spotlighting various happenings at ALT, including plenary talks, tutorials, trends in learning theory, and more!</p>



<p>In order to reach a broad audience in learning theory, we’ll be releasing these posts across a number of different blogs. All content will be linked from this post, so be sure to bookmark this post so you don’t miss anything!</p>



<p>ALT Highlights will be brought to you by an amazing team of junior researchers, written by <a href="https://people.eecs.berkeley.edu/~kush/">Kush Bhatia</a>, <a href="https://www.comp.nus.edu.sg/~sutanu/">Sutanu Gayen</a>, <a href="https://web.stanford.edu/~mglasgow/">Margalit Glasgow</a>, <a href="https://sites.google.com/view/michal-moshkovitz">Michal Moshkovitz</a>, <a href="https://www.ttic.edu/students/">Keziah Naggita</a>, and <a href="https://sites.google.com/site/cyrusrashtchian/">Cyrus Rashtchian</a>, and overseen and edited by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. </p>



<p>Links to articles:<br/>1. <a href="https://hunch.net/?p=13762948">An Interview with Joelle Pineau</a></p>



<p>Next article coming April 26!</p>
<p>The post <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/" rel="nofollow">Introducing ALT Highlights 2021</a> appeared first on <a href="https://www.let-all.com/blog" rel="nofollow">The Learning Theory Alliance Blog</a>.</p></div>
    </content>
    <updated>2021-04-20T16:26:22Z</updated>
    <published>2021-04-20T16:26:22Z</published>
    <category term="ALT Highlights"/>
    <author>
      <name>admin</name>
    </author>
    <source>
      <id>https://www.let-all.com/blog</id>
      <logo>https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/04/logo.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://www.let-all.com/blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://www.let-all.com/blog" rel="alternate" type="text/html"/>
      <title>The Learning Theory Alliance Blog</title>
      <updated>2021-04-23T20:44:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/04/20/early-career-scientists-at-amazon-apply-by-may-14-2021/</id>
    <link href="https://cstheory-jobs.org/2021/04/20/early-career-scientists-at-amazon-apply-by-may-14-2021/" rel="alternate" type="text/html"/>
    <title>Early career scientists at Amazon (apply by May 14, 2021)</title>
    <summary>Amazon Advertising is launching a new, 2-year program for recent PhDs. It offers full-time two-year positions, aimed at recent PhD graduates who want to innovate, publish, and have their work impact millions of customers. Key areas include but are not limited to: machine learning, economics, marketing, operations research, and statistics. The application deadline is May […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Amazon Advertising is launching a new, 2-year program for recent PhDs. It offers full-time two-year positions, aimed at recent PhD graduates who want to innovate, publish, and have their work impact millions of customers. Key areas include but are not limited to: machine learning, economics, marketing, operations research, and statistics. The application deadline is May 14.</p>
<p>Website: <a href="https://www.amazon.science/amazon-advertising-opens-applications-for-early-career-scientists">https://www.amazon.science/amazon-advertising-opens-applications-for-early-career-scientists</a><br/>
Email: advertising-es-program@amazon.com</p></div>
    </content>
    <updated>2021-04-20T15:56:51Z</updated>
    <published>2021-04-20T15:56:51Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-04-23T20:39:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/055</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/055" rel="alternate" type="text/html"/>
    <title>TR21-055 |  Cryptography from Sublinear-Time Average-Case Hardness of Time-Bounded Kolmogorov Complexity | 

	Yanyi Liu, 

	Rafael Pass</title>
    <summary>Let $\mktp[s]$ be the set of strings $x$ such that $K^t(x) \leq s(|x|)$, where $K^t(x)$ denotes the $t$-bounded Kolmogorov complexity of the truthtable described by $x$. Our main theorem shows that for an appropriate notion of mild average-case hardness, for every $\varepsilon&gt;0$, polynomial $t(n) \geq (1+\varepsilon)n$, and every ``nice'' class $\F$ of super-polynomial functions, the following are equivalent:
- the existence of some function $T \in \F$ such that $T$-hard one-way functions (OWF) exists (with non-uniform security);
- the existence of some function $T \in \F$ such that $\mktp[T^{-1}]$ is mildly average-case hard with respect to sublinear-time non-uniform algorithms (with running-time $n^{\delta}$ for some $0&lt;\delta&lt;1$).
For instance, existence of subexponentially-hard (resp. quasi-polynomially-hard) OWFs is equivalent to mild average-case hardness of $\mktp[\poly\log n]$ (resp. $\mktp[2^{O(\sqrt{\log n})})]$) w.r.t. sublinear-time non-uniform algorithms.

We additionally note that if we want to deduce $T$-hard OWFs where security holds w.r.t. \emph{uniform} $T$-time probabilistic attackers (i.e., uniformly-secure OWFs), it suffices to assume sublinear time hardness of $\mktp$ w.r.t. uniform probabilistic sublinear-time attackers. We complement this result by proving lower bounds that come surprisingly close to what is required to unconditionally deduce the existence of (uniformly-secure) OWFs: $\mktp[\poly\log n]$ is worst-case hard w.r.t. uniform probabilistic sublinear-time algorithms, and $\mktp[n-\log n]$ is mildly average-case hard for all $O(t(n)/n^3)$-time deterministic algorithms.</summary>
    <updated>2021-04-20T11:36:21Z</updated>
    <published>2021-04-20T11:36:21Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-23T20:38:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3517</id>
    <link href="https://agtb.wordpress.com/2021/04/20/the-stony-brook-international-conference-on-game-theory-july-5-8-2021/" rel="alternate" type="text/html"/>
    <title>The Stony Brook International Conference on Game Theory, July 5-8, 2021</title>
    <summary>Call for Participation and Poster submission: The 32nd annual Stony Brook International Conference on Game Theory  will be held online July 5 – 8, 2021. This year the conference will emphasize recent research at the intersection of computer science and economics. We will also celebrate the recent Nobel prize to Paul Milgrom and Robert Wilson, who will both deliver […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Call for Participation and Poster submission:</p>



<p><a href="http://www.gtcenter.org/?page=Conference.html" rel="noreferrer noopener" target="_blank">The 32nd annual Stony Brook International Conference on Game Theory </a> will be held online July 5 – 8, 2021. This year the conference will emphasize recent research at the intersection of computer science and economics. We will also celebrate the recent Nobel prize to Paul Milgrom and Robert Wilson, who will both deliver an extended version of their Nobel prize speech.</p>



<p>The conference will begin and end with a reception in honor of Paul Milgrom and Robert Wilson. The schedule is available <a href="http://www.gtcenter.org/index.php?page=Downloads/ConfSchedule.html" rel="noreferrer noopener" target="_blank">here</a>. The main program will consist of invited papers and talks.</p>



<p>We invite graduate students and junior faculty to submit recent work for <a href="https://docs.google.com/forms/d/1I4aipMKrhlmthfF8W0QeclGIJgF9oLa5xb5O-tJA7PA/viewform?edit_requested=true" rel="noreferrer noopener" target="_blank">poster sessions</a> which will be an integral part of the conference. The deadline for submission to the poster session is Friday, April 30<sup>th</sup>, 5pm est. The program for the poster session will be announced, Monday, May 17<sup>th</sup>.</p>



<p>If you would like to attend any of the sessions, please <a href="http://www.gtcenter.org/index.php?page=php/Login.php" rel="noreferrer noopener" target="_blank">Login</a> or <a href="http://www.gtcenter.org/index.php?page=php/Regform.php" rel="noreferrer noopener" target="_blank">Create Account</a> and RSVP for the events you would like to attend. As the conference date approaches, you will receive an email with more details on accessing and navigating Virtual Chair, the online, interactive event space that will host the conference.</p>



<p>Please feel free to share this information broadly. We look forward to your participation!</p>



<p><a href="https://campuspress.yale.edu/dirkbergemann/">Dirk Bergemann</a> and <a href="https://cs.tau.ac.il/~mfeldman">Michal Feldman</a></p>



<p>Organizers</p></div>
    </content>
    <updated>2021-04-20T10:40:49Z</updated>
    <published>2021-04-20T10:40:49Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>michalfeldman</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2021-04-23T20:39:19Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-628795981625377168</id>
    <link href="https://blog.computationalcomplexity.org/feeds/628795981625377168/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/an-investment-puzzle-and-speculation-as.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/628795981625377168" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/628795981625377168" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/an-investment-puzzle-and-speculation-as.html" rel="alternate" type="text/html"/>
    <title>An investment puzzle and speculation as to why some think its hard</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> This is a Guest Post by David Marcus. He gives a puzzle and its solution, which is interesting, and then speculates as to why some people get it wrong. </p><p>---------------------------------------------------------------------------</p><p>THE PROBLEM:</p><p>Investing Puzzle or Arithmetic Can Be Useful</p><p><br/></p><p>The following is an example of investment results that I saw in an</p><p>investment newsletter. There are two portfolios that use different</p><p>strategies. Both portfolios start with $1 million twenty years ago and</p><p>withdraw 5% each year. The idea is that you are retired and withdrawing</p><p>money to spend. Not all years are shown in the tables.</p><p><br/></p><p>Portfolio A</p><p><br/></p><p>Year   Return  Withdrawal    Balance</p><p>2000   15.31%      57,655  1,095,445</p><p>2005    1.81%      59,962  1,139,273</p><p>2008  -12.65%      51,000    969,004</p><p>2009   34.26%      65,049  1,235,936</p><p>2010   11.94%      69,175  1,314,331</p><p>2015   -2.48%      64,935  1,233,764</p><p>2020   10.27%      66,935  1,271,765</p><p>Total Withdrawal: 1,685,190</p><p>Change in Balance: 27.18%</p><p>======</p><p>Portfolio B</p><p>Year   Return  Withdrawal    Balance</p><p>2000   -0.95%      49,524    940,956</p><p>2005    3.80%      44,534    846,154</p><p>2008  -20.11%      35,922    682,523</p><p>2009   18.27%      40,360    766,833</p><p>2010   11.57%      42,777    812,764</p><p>2015    0.99%      50,767    964,567</p><p>2020   13.35%      65,602  1,246,433</p><p><br/></p><p>Total Withdrawal: 1,425,573</p><p>Change in Balance: 24.64%</p><p><br/></p><p>Portfolio A has a final balance that is 25,000 more than Portfolio B's and</p><p>had about 260,000 more in withdrawals. Does the example lend credence to</p><p>the Portfolio A strategy being better than the Portfolio B strategy?</p><p>---------------------------------------------------------------------------</p><p>THE ANSWER:</p><p>Investing Puzzle or Arithmetic Can Be Useful: Analysis</p><p><br/></p><p>Summary: The two portfolios have about the same performance over the 20</p><p>years. The difference is mainly due to Portfolio A having a good year or</p><p>years near the beginning before much money was withdrawn. The example</p><p>merely shows that it is better to withdraw money after a gain rather than</p><p>before.</p><p><br/></p><p>Detailed Analysis:</p><p><br/></p><p>The scenario is: Start with X = $1 million. Withdraw 5% a year.</p><p><br/></p><p>Define "gain factor" to be 1 plus the percentage return. For example, if a</p><p>portfolio returns 5%, then the gain factor is 1.05.</p><p><br/></p><p>Let A_j, j = 1, ..., 20, be the gain factors each year for portfolio A.</p><p><br/></p><p>Let B_j, j = 1, ..., 20 be the gain factors each year for portfolio B.</p><p><br/></p><p>The final amount in portfolio A is</p><p><br/></p><p>   F = X * A_1 * 0.95 * A_2 * 0.95 * ... * A_20 * 0.95 .</p><p><br/></p><p>The final amount in portfolio B is</p><p><br/></p><p>   G = X * B_1 * 0.95 * B_2 * 0.95 * ... * B_20 * 0.95 .</p><p><br/></p><p>From the "Change in Balance" values or the balances for year 2020, we see</p><p>that F and G are almost the same:</p><p><br/></p><p>   F = 1.271865 * X,</p><p>   G = 1.246433 * X.</p><p><br/></p><p>But, as we learned in elementary school, multiplication is commutative, so</p><p><br/></p><p>   F = X * 0.95^20 * \prod_{j=1}^20 A_j,</p><p>   G = X * 0.95^20 * \prod_{j=1}^20 B_j.</p><p><br/></p><p>Since F and G are almost the same, the total gains (product of the gain</p><p>factors) for the two portfolios are almost the same, i.e.,</p><p><br/></p><p>   \prod_{j=1}^20 A_j \approx \prod_{j=1}^20 B_j.</p><p><br/></p><p>Then what accounts for the big difference in the amounts withdrawn?</p><p>Portfolio A must have had some good years near the beginning. (We see in</p><p>the tables that Portfolio A did better in 2000 than Portfolio B.) So, all</p><p>the example shows is that it is better to withdraw your money after your</p><p>gains rather than before.</p><p><br/></p><p>To take an extreme example, suppose an investment is going to go up 100%</p><p>this year. It is better to take your money out at the end of the year</p><p>(after the gain) than at the beginning of the year (before the gain). This</p><p>is a triviality.</p><p><br/></p><p>The example tells us nothing useful about the two strategies.</p><p><br/></p><p>Note: The total gains aren't exactly the same, but the timing of the yearly</p><p>gains is what is driving the results. We have (rounding off)</p><p>   ( F - G ) / 0.95^20 = 70942.81 .</p><p>So, if there had been no withdrawals, the difference in the portfolio</p><p>balances would have been about $71,000, much less than the $260,000 +</p><p>$25,000 difference with the withdrawals.</p><p>---------------------------------------------------------</p><p>WHY IS THIS HARD FOR PEOPLE?</p><p>Many people have trouble with this puzzle. The difficulty may be that such</p><p>people don't make a mental model (or a written model) of the process that</p><p>is producing the balance. If you write down (or have in your head) a</p><p>formula for the balance, then you see that the gain factors are independent</p><p>of the withdrawal factors. That is, we could withdraw more or less money,</p><p>or even deposit money, without affecting the gain factors we would use in</p><p>the model. This then leads us to consider the gain factors on their own,</p><p>and to recognize that the gain factors are the true measures of how the</p><p>strategies perform.</p><p><br/></p><p><br/></p><div><br/></div></div>
    </content>
    <updated>2021-04-19T02:01:00Z</updated>
    <published>2021-04-19T02:01:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-04-23T07:12:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18580</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/" rel="alternate" type="text/html"/>
    <title>Summing Up the Primes</title>
    <summary>Given the millennia that people have contemplated prime numbers, our continuing ignorance concerning the primes is stultifying—Richard Crandall and Carl Pomerance Sources: her site, Oberlin interview Lola Thompson is an Associate Professor of Mathematics at Utrecht University. She did her 2012 PhD thesis, Products of Distinct Cyclotomic Polynomials, under Pomerance at Dartmouth. Of course, she […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Given the millennia that people have contemplated prime numbers, our continuing ignorance concerning the primes is stultifying—Richard Crandall and Carl Pomerance</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/lola-2/" rel="attachment wp-att-18596"><img alt="" class="alignright wp-image-18596" height="175" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/lola.png?resize=165%2C175&amp;ssl=1" width="165"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Sources: <a href="http://www.lolathompson.com/">her site</a>, Oberlin <a href="https://www.oberlin.edu/news/conversation-lola-thompson">interview</a></font></td>
</tr>
</tbody>
</table>
<p>
Lola Thompson is an Associate Professor of Mathematics at Utrecht University. She did her 2012 PhD <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/thesis.pdf">thesis</a>, <i>Products of Distinct Cyclotomic Polynomials</i>, under Pomerance at Dartmouth. Of course, she works in number theory.  Her thesis is numbered <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on her publications <a href="http://www.lolathompson.com/research.html">page</a>.  </p>
<p>
Today we thought we would highlight a recent result of hers and its connections to complexity theory—indeed, to the realm of <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> vs. <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and beyond.</p>
<p>The interview linked below her photo at right includes these words:</p>
<blockquote><p>
Even [as an undergrad math major], I didn’t see myself having a future as a mathematician. I think that part of the issue was that I had never seen a female mathematician (none of my math professors at UChicago were women). A major turning point came when my department sent me to the Nebraska Conference for Undergraduate Women In Mathematics (NCUWM). While at NCUWM, I received a huge amount of encouragement. The idea of going to graduate school and becoming a mathematics professor had never occurred to me. It was also extremely helpful for me to see examples of female mathematicians. For the first time, I could imagine myself as a mathematician!
</p></blockquote>
<p>
She is paying that forward in mentoring women. See this <a href="https://women-in-numbers-europe-4.sites.uu.nl/wp-content/uploads/sites/658/2021/04/Poster-WIN.pdf">poster</a> for Utrecht’s hosting of next year’s 4th Women in Numbers – Europe <a href="https://women-in-numbers-europe-4.sites.uu.nl/">conference</a>, which is affiliated to the Women in Number Theory <a href="http://womeninnumbertheory.org/">network</a>. I also like her comments on undergraduate education <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/jmm_2020_special_session_talk.pdf">here</a>. </p>
<p>
The <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/mertensnew.pdf">paper</a> is joint with Harald Helfgott. The key “players” are the Möbius function and the Mertens function—named for August Möbius and for Franz Mertens. </p>
<p>
</p><h2> The Key Players </h2><p/>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/mm-2/" rel="attachment wp-att-18590"><img alt="" class="aligncenter size-medium wp-image-18590" height="140" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/mm.png?resize=300%2C140&amp;ssl=1" width="300"/></a></p>
<p>
The Möbius <a href="https://en.wikipedia.org/wiki/Mobius_function">function</a> was invented by Möbius in 1832. It is an important function throughout number theory. It is denoted by <img alt="{\mu(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>: </p>
<ul>
<li><img alt="{\mu(n) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> if n is a square-free positive integer with an even number of prime factors.
</li><li><img alt="{\mu(n) = -1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> if n is a square-free positive integer with an odd number of prime factors.
</li><li><img alt="{\mu(n) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> if n has a squared prime factor.
</li></ul>
<p>It is multiplicative: 	</p>
<p align="center"><img alt="\displaystyle  \mu(1) = 1 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu%281%29+%3D+1+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>and 	</p>
<p align="center"><img alt="\displaystyle  \mu(ab) = \mu(a)\mu(b) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu%28ab%29+%3D+%5Cmu%28a%29%5Cmu%28b%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>when <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are co-prime. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/val/" rel="attachment wp-att-18586"><img alt="" class="aligncenter size-medium wp-image-18586" height="78" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/val.png?resize=300%2C78&amp;ssl=1" width="300"/></a></p>
<p>
Think of <img alt="{\mu(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as a function that returns <img alt="{-1,0,+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%2C0%2C%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which reveals important information about <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The <img alt="{M(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> part is the <a href="https://en.wikipedia.org/wiki/Mertens_function">Mertens</a> function, which gives average-like information on the value of <img alt="{\mu(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="{0 \le n \le x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%5Cle+n+%5Cle+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>: 	</p>
<p align="center"><img alt="\displaystyle  M(x) = \sum_{n \le x} \mu(n). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M%28x%29+%3D+%5Csum_%7Bn+%5Cle+x%7D+%5Cmu%28n%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Of course <img alt="{M(x)/x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%2Fx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is exactly the average value. 	 </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/m/" rel="attachment wp-att-18587"><img alt="" class="aligncenter size-medium wp-image-18587" height="240" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/M.png?resize=300%2C240&amp;ssl=1" width="300"/></a></p>
<p>
A key property of the Möbius function is: 	</p>
<p align="center"><img alt="\displaystyle  \sum_{d | n} \mu(d) = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bd+%7C+n%7D+%5Cmu%28d%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>unless <img alt="{n=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then the sum is <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
</p><h2> Computing <img alt="{M(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>—the easy way </h2><p/>
<p>
Computing <img alt="{\mu(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for one value <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> seems to rely on the factorization of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Thus even computing it at a single value could be hard. That is it could require super-polynomial time in worst case. Clearly computing <img alt="{M(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is even harder: 	</p>
<p align="center"><img alt="\displaystyle  M(x+1)-M(x) = \mu(x+1). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M%28x%2B1%29-M%28x%29+%3D+%5Cmu%28x%2B1%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>	 One way to compute <img alt="{M(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is via complexity theory. This is <i>not</i> what Thompson does. </p>
<blockquote><p><b>Theorem 1</b> <em> Suppose that <img alt="{\mathsf{P = \#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+%5C%23P%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then we can compute <img alt="{M(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> in time polynomial in <img alt="{\log(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </em>
</p></blockquote>
<p>
This would be an exponential improvement over Thompson’s result. It probably shows how unlikely it is to get this collapse. But this is still open.</p>
<p>
Recall a set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is in <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> provided given <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> one can decide whether <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is in the set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in time polynomial in <img alt="{\log x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Recall <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> given <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> computes the function that counts how many <img alt="{x \le N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cle+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are in <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> also in time polynomial in <img alt="{\log x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Certainly counting is at least as hard as deciding membership in <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We believe that in general it is much harder. Assuming they are equal means that any predicate that can be computed in polynomial time can also be exactly counted. For example: </p>
<ol>
<li>Given a graph: One can count the number of spanning trees.
</li><li>Given a graph: One can count the number of three colorings of the graph.
</li><li>Given a polynomial equation: One can count the number of solutions.
</li><li>And so on.
</li></ol>
<p>The conjecture is that <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is not equal to <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. If <img alt="{\mathsf{P = \#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then <img alt="{\mathsf{P=NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%3DNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and more. Thus it probably is the case that they are not equal. But like much of complexity theory, this is wide open. </p>
<p>
</p><h2> Computing <img alt="{M(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>—the hard way </h2><p/>
<p>
Another way to compute <img alt="{M(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is via number theory not via complexity theory. This is what Thompson does—with no unproved assumption about <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{\#P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%23P%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
She proves this main result in her <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/mertensnew.pdf">paper</a>—this is the paper we previously cited.</p>
<blockquote><p><b>Theorem 2</b> <em> There is an algorithm that computes <img alt="{M(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> in time 	</em></p><em>
<p align="center"><img alt="\displaystyle  O_{\epsilon}(x^{\delta} \log^{\delta+\epsilon} x). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++O_%7B%5Cepsilon%7D%28x%5E%7B%5Cdelta%7D+%5Clog%5E%7B%5Cdelta%2B%5Cepsilon%7D+x%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
</em><p><em>Here <img alt="{\delta=3/5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%3D3%2F5%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> and the constant in <img alt="{O_{\epsilon}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO_%7B%5Cepsilon%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> depends on <img alt="{\epsilon&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </em>
</p></blockquote>
<p>
Note the time used is <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to some power. Before on the assumption that <img alt="{P=\#P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%3D%5C%23P%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> we get that the time is polynomial in <img alt="{\log x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
The proof is non-trivial. In order to give some idea we will give a few comments. Standard identities allow one to write, 	</p>
<p align="center"><img alt="\displaystyle  M(x) = 2M(\sqrt{x}) - \sum \mu(m_1)\mu(m_2), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M%28x%29+%3D+2M%28%5Csqrt%7Bx%7D%29+-+%5Csum+%5Cmu%28m_1%29%5Cmu%28m_2%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>over 	</p>
<p align="center"><img alt="\displaystyle  m_1,m_2 \le \sqrt{x}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++m_1%2Cm_2+%5Cle+%5Csqrt%7Bx%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>The insight is to look hardest at those values	</p>
<p align="center"><img alt="\displaystyle  m_1,m_2 \le x^{2/5}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++m_1%2Cm_2+%5Cle+x%5E%7B2%2F5%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Their summary of what happens next:</p>
<blockquote><p><b> </b> <em> Our approach in Section 4 roughly amounts to analyzing the difference between reality and a model that we obtain via Diophantine approximation, in that we show that this difference has a simple description in terms of congruence classes and segments. This description allows us to compute the difference quickly, in part by means of table lookups. </em>
</p></blockquote>
<p/><p>
As often, we say to see the full paper for the details. Or you can follow her talk either in <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/laten_charla.pdf">Spanish</a> or in <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/copy_of_jmm_2021_talk.pdf">English</a>:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/mertens/" rel="attachment wp-att-18615"><img alt="" class="aligncenter wp-image-18615" height="308" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/Mertens.jpg?resize=480%2C308&amp;ssl=1" width="480"/></a></p>
<p/><p><br/>
The proof is clever and there seems to be no way to improve it to anything near what we get via complexity theory. Of course we can only do that in the presence of a very strong conjecture: 	</p>
<p align="center"><img alt="\displaystyle  \mathsf{P = \#P}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BP+%3D+%5C%23P%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
But what she could do was to implement the algorithm in her joint paper. The computation ran on a many-core machine for weeks. It gets <img alt="{M(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="{x \le 10^{22}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cle+10%5E%7B22%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </p>
<p>
This is one advantage of algorithms that do not rely on unproved conjectures. They can actually be implemented and executed. </p>
<p>
</p><h2> Open Problems </h2><p/>
<p>
Is there a richer fount for open problems than number theory?</p></font></font></div>
    </content>
    <updated>2021-04-18T13:25:29Z</updated>
    <published>2021-04-18T13:25:29Z</published>
    <category term="Ideas"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="primes"/>
    <category term="Proofs"/>
    <category term="counting"/>
    <category term="Harald Helfgott"/>
    <category term="Lola Thompson"/>
    <category term="Mertens function"/>
    <category term="Mobius function"/>
    <category term="sieve"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-04-23T20:39:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/04/17/picks-shoelaces</id>
    <link href="https://11011110.github.io/blog/2021/04/17/picks-shoelaces.html" rel="alternate" type="text/html"/>
    <title>Pick’s shoelaces</title>
    <summary>Two important methods for computing area of polygons in the plane are Pick’s theorem and the shoelace formula. For a simple lattice polygon (a polygon with a single non-crossing boundary cycle, all of whose vertex coordinates are integers) with \(i\) integer points in its interior and \(b\) on the boundary, Pick’s theorem computes the area as</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Two important methods for computing area of polygons in the plane are <a href="https://en.wikipedia.org/wiki/Pick%27s_theorem">Pick’s theorem</a> and the <a href="https://en.wikipedia.org/wiki/Shoelace_formula">shoelace formula</a>. For a simple lattice polygon (a polygon with a single non-crossing boundary cycle, all of whose vertex coordinates are integers) with \(i\) integer points in its interior and \(b\) on the boundary, Pick’s theorem computes the area as</p>

\[A=i+b/2-1.\]

<p>The shoelace formula has various formulations, but in the version I’m going to use, it is a sum over oriented edges of the polygon. Let \((x,y)\to (x',y')\) denote an edge of the polygon that connects the two points \((x,y)\) and \((x',y')\), oriented in the clockwise direction around the polygon so that if you travel from \((x,y)\) to \((x',y')\) along this line segment, then the interior of the polygon will be on your right. Then, according to the shoelace formula, the area is</p>

\[A= \frac{1}{2}\sum_{(x,y)\to (x',y')}(x'-x)(y'+y).\]

<p>The shoelace formula looks messier than Pick’s formula but it’s much easier for computers to evaluate, both because polygons are generally represented in computers by their boundary rather than by the points they contain, and because there are often many fewer boundary edges than interior lattice points. It also doesn’t need the coordinates to be integers. But since these two formulas produce the same value, it would be interesting to see a more direct relation between them, explaining one formula in terms of the other.</p>

<h1 id="proof-of-the-shoelace-formula">Proof of the shoelace formula</h1>

<p>It is convenient to shift the polygon to lie entirely above the \(x\)-axis, without changing its area or number of grid points. After this shift, the nonzero terms \(\tfrac{1}{2}(x'-x)(y'+y)\) of the shoelace formula can be interpreted as the signed areas of trapezoids, extending vertically down from each non-vertical edge \((x,y)\to (x',y')\) to the \(x\)-axis. The reason for doing this shift is to orient all of the trapezoids the same way, downwards from their edges, and to avoid complications with edges that cross the \(x\)-axis.</p>

<p style="text-align: center;"><img alt="Trapezoids extending downward from each edge of a polygon" src="https://11011110.github.io/blog/assets/2021/pick-trapezoids.svg"/></p>

<p>Now consider what you would see from a generic point in the upper half-plane, if you looked straight upwards. (By “generic”, I mean that the point isn’t on an edge or directly below a vertex, because these would introduce additional and unimportant cases to our analysis.) Your line of site would pass through a sequence of zero or more polygon edges, whose trapezoids have positive sign when the line passes from inside the polygon to outside and negative sign when the line passes from outside to inside. Because of this alternation between inside and outside, between positive and negative, a point outside the polygon sees equally many edges of each sign, but a point in the polygon sees one more positive edge than negative.</p>

<p>Another way of expressing the same counts of signed edges above each point involves the <em>characteristic functions</em> of the polygon and the trapezoids, functions that are 1 inside each shape and zero outside it. The area of a shape is just the integral of its characteristic function. We’ll leave these functions undefined on the boundary of the shapes, but that’s ok because the boundary points contribute nothing to the total area. Then because of the cancellation between positive and negative signs, at the generic points where all of these functions are defined, the characteristic function of the polygon equals the signed sum of  characteristic functions of trapezoids. By the sum rule for integrals, the area of the polygon equals the signed sum of trapezoid areas. With a little more complication, the same argument can also be made to work directly for unshifted polygons.</p>

<h1 id="counting-lattice-points-in-trapezoids">Counting lattice points in trapezoids</h1>

<p>Can we use the same argument to prove Pick’s theorem, in the form that the shoelace formula equals Pick’s formula?  We’d like to interpret each term of the shoelace formula as a count over grid points, decompose the polygon in the same way into a sum of trapezoids, and argue that counting points in each trapezoid and then summing produces the same result as summing the trapezoids and then counting points. The difficulty is that now we can’t ignore the points on the axis or on boundary of the trapezoids, because their contribution to the count is nonzero.</p>

<p>First, let’s see how we can interpret each shoelace term as a grid point count.
Rotating a trapezoid around the midpoint of its defining edge produces another trapezoid whose union with the first trapezoid is an axis-aligned rectangle. Pick’s theorem is easy to see for these rectangles, in the simplified form that the area is the sum of one unit for integer points inside the rectangle, half a unit for integer points on its edges, and a quarter of a unit for integer points at its vertices. These fractional numbers of units are merely the amounts of rectangle area nearest to each point.</p>

<p style="text-align: center;"><img alt="The area of a lattice trapezoid equals the sum of units of lattice points: a whole unit for points in the trapezoid, half a unit on the boundary, and a quarter unit for the four corners" src="https://11011110.github.io/blog/assets/2021/pick-rectangle.svg"/></p>

<p>For the trapezoids we will use the same assignment of units to lattice points: one unit for points in the trapezoid, half on its boundary, and a quarter at its vertices, regardless of the actual trapezoid angles at those vertices. The figure above shows each point decorated in blue with its number of units.
The equality of area with total units still holds true, because both the area of the trapezoid and the total number of units for its integer points are half that of the rectangle. Each point off the edge that contributes its units to the trapezoid has a reflection that does not contribute its units. And each point on the edge contributes half its units to the trapezoid and half to the reflection. So the same terms \(\tfrac{1}{2}(x'-x)(y'+y)\) of the shoelace formula also count the contributions of units in each trapezoid to Pick’s formula.</p>

<h1 id="picks-formula-from-sums-of-trapezoids">Pick’s formula from sums of trapezoids</h1>

<p>We know how much each lattice point contributes to Pick’s formula: one unit if it is inside the polygon, half if it is on the boundary, or zero if it is outside. We also know how much each lattice point contributes to each trapezoid of the shoelace formula: one unit if it is interior to the trapezoid, half if it is on an edge, a quarter if it is on a corner, and none if it is exterior. In order to prove that Pick’s formula and the shoelace formula are equal, we want to show that all points make equal contributions. And for most points, this turns out to be true.</p>

<p>The vertical ray from any point \(p\) may pass through the boundary of the polygon in multiple ways: \(p\) itself may lie on the boundary, the ray may cross an edge of the boundary, it may pass through a vertex of the boundary that has edges to its left and right, or it may brush past the boundary at a vertex that has edges only to the left or only to the right. We can skip over these last “brush past” cases, because the two trapezoids from these edges have opposite signs and cancel each other in the total trapezoid contribution of \(p\). If \(p\) lies on an edge, then it gets a (positive or negative) contribution of \(1/2\), and when \(p\) is a vertex of the polygon with edges extending left and right, then it gets the same contribution in two quarters. The remaining cases make a contribution of \(\pm 1\), and as for the area calculation they alternate in sign. Canceling these alternating contributions shows that \(p\) always has a total contribution from its trapezoids equal to its contribution to Pick’s formula.</p>

<p>I thought at first that the points on the \(x\)-axis might need a different calculation, because they get only half as much from each trapezoid. But half zero is still zero; they contribute nothing to the sum of trapezoids and nothing to the Pick formula. The points that do need special treatment are the polygon vertices at which the two incident edges do not extend to the left and right, either because one is vertical or because both extend in the same direction. The contribution to Pick’s formula for these vertices is still \(1/2\), but the total contribution from the trapezoids is either an integer (if the incident edges extend in the same direction) or a quarter-integer (if one edges is vertical). So we’ll have to count how much we’ve missed at those points.</p>

<p>Consider driving around the polygon clockwise, in the same direction that we oriented the edges. As you drive, you can make a right turn (from rightward to vertically down, vertically down to leftward, etc), a double-right u-turn, a left turn, or a double-left turn. Points at which the polygon makes an angle but continues in the same general left-to-right or right-to-left direction don’t count as turns. Then at each right turn Pick’s formula will run a deficit of 1/4 unit in total contributions, compared to the sum of trapezoids. A double-right u-turn gives a deficit of 1/2 unit, the same as two right turns. A left turn gives 1/4 more unit to Pick than to the sum of trapezoids, and a double-left gives 1/2 more unit. To return to your starting direction, you must make four more right turns than left, so the total difference in contributions from these terms comes out to exactly the \(-1\) correction term in Pick’s formula.</p>

<p style="text-align: center;"><img alt="Differences in contributions at turning points of the e polygon" src="https://11011110.github.io/blog/assets/2021/pick-deficits.svg"/></p>

<p>In summary, when we sum contributions over all points, the points inside the polygon contribute one unit to Pick’s formula and one unit to the sum of trapezoids. The points outside the polygon contribute zero to both. The points on the boundary that are not turns contribute \(1/2\) to both. And the points on the boundary that are turns contribute different amounts to Pick’s formula and to the sum of trapezoids, with the total of these differences equalling the \(-1\) correction term in Pick’s formula. Therefore, the overall value of Pick’s formula equals the sum of point contributions in trapezoids, which equals the signed sum of trapezoid areas, which equals the polygon area.</p>

<h1 id="generalizations-of-picks-formula">Generalizations of Pick’s formula</h1>

<p>The same idea lets us generalize Pick’s formula to a polygon with holes, defined as a connected region of the plane whose boundary is a disjoint union of simple polygons. The shoelace formula for area in the version we’re using here, and its proof, need no change for this generalization. For each hole, we should drive counterclockwise rather than clockwise, to stay consistent with the orientation we have given the edges; the same argument shows that each hole produces a positive, rather than negative, total difference between the contributions to Pick’s formula and to the shoelace formula. Therefore, for a polygon with \(h\) holes, the total area becomes</p>

\[A=i+b/2+h-1.\]

<p>The basic idea for all this, by the way, comes from the paper “Pick’s theorem”, by Branko Grünbaum and G. C. Shephard (<a href="https://doi.org/10.2307/2323771"><em>Amer. Math. Monthly</em> 1993</a>). Rather than using the trapezoid version of the shoelace formula, Grünbaum and Shephard use a version of the formula that sums, over each edge of the polygon, the signed area of the triangle formed by each edge plus the origin. I think this makes the analysis a little messier, but it’s otherwise much the same as the analysis here. The formula for polygons with holes can be found in “On the compactness of subsets of digital pictures” by Sankar and Krishnamurthy (<a href="https://doi.org/10.1016/s0146-664x(78)80021-5"><em>CGIP</em> 1978</a>) but without the careful definition of polygons with holes that is necessary to avoid problems here.</p>

<p>Grünbaum and Shephard also generalize Pick’s formula in a slightly different direction: rather than allowing separate holes in their polygons, they define polygons to have only one boundary polygon, but they allow that polygon to cross itself. Their version of Pick’s theorem for this kind of generalized polygons sums a certain half-integral index of each lattice point (essentially, the winding number of the polygon around that point, averaged between open and closed versions of the polygon), with a correction term coming from the turning number of the whole polygon. It should be possible to combine both generalizations, and allow polygon boundaries that both cross and have multiple components. There’s a little ambiguity about which way the boundary is connected at vertices of degree higher than two, though, which would need to be resolved somehow if such vertices are to be allowed, because different choices are likely to cause pieces of the boundary to have different orientations leading to different areas.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106084926459254807">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-04-17T23:23:00Z</updated>
    <published>2021-04-17T23:23:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-04-18T18:59:15Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/04/15/linkage</id>
    <link href="https://11011110.github.io/blog/2021/04/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Keller’s conjecture (\(\mathbb{M}\)), another new Good Article on Wikipedia. The conjecture was falsified in 1992 with all remaining cases solved by 2019, but the name stuck. It’s about tilings of \(n\)-space by unit cubes, and pairs of cubes that share \((n-1)\)-faces. In 2d, all squares share an edge with a neighbor, but a 3d tiling derived from tetrastix has many cubes with no face-to-face neighbor. Up to 7d, some cubes must be face-to-face, but tilings in eight or more dimensions can have no face-to-face pair.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Keller%27s_conjecture">Keller’s conjecture</a> (<a href="https://mathstodon.xyz/@11011110/105994491983819823">\(\mathbb{M}\)</a>), another new Good Article on Wikipedia. The conjecture was falsified in 1992 with all remaining cases solved by 2019, but the name stuck. It’s about tilings of \(n\)-space by unit cubes, and pairs of cubes that share \((n-1)\)-faces. In 2d, all squares share an edge with a neighbor, but a 3d tiling derived from <a href="https://en.wikipedia.org/wiki/Tetrastix">tetrastix</a> has many cubes with no face-to-face neighbor. Up to 7d, some cubes must be face-to-face, but tilings in eight or more dimensions can have no face-to-face pair.</p>
  </li>
  <li>
    <p><a href="https://lucatrevisan.wordpress.com/2021/04/02/bocconi-hired-poorly-qualified-computer-scientist/">Italians and bibliometrics</a> (<a href="https://mathstodon.xyz/@11011110/105996551762053680">\(\mathbb{M}\)</a>): Luca Trevisan (a leading theorist with 7 SODA papers, 2 FOCS papers, a JACM paper and a SICOMP paper in the last four years) gets dinged for poor productivity as the Italian system only counts journal papers that do not match conference papers. The fact that these are all in top venues is irrelevant, and the conference papers count only negatively against matching journal papers. Comments discuss similar problems in other countries.</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=5402">What is the computational complexity of dinosaur train tracks?</a> (<a href="https://mathstodon.xyz/@11011110/106009429519024889">\(\mathbb{M}\)</a>).  Answer: not very high, because the only usable junction, a Y that remembers which way you came through it and sends you the same way if you come back through the other direction, is just not powerful enough to do much.</p>
  </li>
  <li>
    <p>Congratulations to Martín Farach-Colton, Shang-Hua Teng, and all of the other <a href="https://sinews.siam.org/Details-Page/siam-announces-class-of-2021-fellows">new SIAM Fellows</a> (<a href="https://mathstodon.xyz/@11011110/106016827432463028">\(\mathbb{M}\)</a>)!</p>
  </li>
  <li>
    <p><a href="https://writings.stephenwolfram.com/2021/03/a-little-closer-to-finding-what-became-of-moses-schonfinkel-inventor-of-combinators/">Stephen Wolfram tries to track down</a> what happened to logician <a href="https://en.wikipedia.org/wiki/Moses_Sch%C3%B6nfinkel">Moses Schönfinkel</a> (<a href="https://mathstodon.xyz/@11011110/106025064389253132">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26694685">via</a>), who worked in Göttingen from 1914 to 1924, returned to Moscow, and then “basically vanished”. Wikipedia has more detail about what happened after (mental health issues, death around 1942), but Wolfram says the evidence for all that is weak. He doesn’t make direct progress on Schönfinkel himself but does find some relatives.</p>
  </li>
  <li>
    <p><a href="https://aperiodical.com/2021/04/my-robot-draws-tex/">How Christian Lawson-Perfect got a pen plotter to draw mathematical notation using TeX</a> (<a href="https://mathstodon.xyz/@christianp/106030474747952758">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://www.improbable.com/2021/04/08/a-look-way-back-at-some-bearded-mathematicians/">When mathematicians wore geometric beards</a> (<a href="https://mathstodon.xyz/@11011110/106034106736789855">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=5437">Aaronson on politicization of research prizes</a> (<a href="https://mathstodon.xyz/@11011110/106039607813461033">\(\mathbb{M}\)</a>). Jeff Ullman won the Turing Award despite deplorable (some say racist) treatment of grad applicants for the crime of being Iranian, and Oded Goldreich was blocked from the Israel Prize for anti-settlement politics. Politicization is two-edged. I’d rather see Ullman awarded for his worthy contributions, and use the opportunity to decry his abhorrent actions and statements, than subject prizes to litmus tests from all sides.</p>
  </li>
  <li>
    <p><a href="https://thatsmaths.com/2021/04/08/circles-polygons-and-the-kepler-bouwkamp-constant/">Circles, polygons and the Kepler-Bouwkamp constant</a> (<a href="https://mathstodon.xyz/@11011110/106045568373359503">\(\mathbb{M}\)</a>). On the limiting behavior of infinitely-nested shapes alternating between circles and polygons with increasing numbers of sides.</p>
  </li>
  <li>
    <p><a href="https://www.technologyreview.com/2021/04/09/1022217/facebook-ad-algorithm-sex-discrimination">Continuing gender bias in who sees job-opening ads on Facebook</a> (<a href="https://mathstodon.xyz/@11011110/106051162687419819">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26760790">via</a>): if an employer or industry has historically skewed male or female, Facebook replicates that bias, even for pairs of ads with identical qualifications. This is illegal, but Facebook appears unable to find a technical fix and unwilling to apply the obvious fix of not targeting its ads even when that targeting is illegal. As usual for Hacker News via links on topics related to social justice, don’t read the comments there.</p>
  </li>
  <li>
    <p>Amusing quote from McLarty’s 2003 “<a href="http://www.landsburg.com/grothendieck/mclarty1.pdf">Grothendieck on simplicity and generality</a>” (<a href="https://mathstodon.xyz/@11011110/106059214777848356">\(\mathbb{M}\)</a>, <a href="https://golem.ph.utexas.edu/category/2021/04/algebraic_closure.html">via</a>): “Serre created a series of concise elegant tools which Grothendieck and coworkers simplified into thousands of pages of category theory.” Nowadays I guess the people doing this sort of simplification are the ones formulating machine-verifiable proofs…</p>
  </li>
  <li>
    <p><a href="https://projects.cs.dal.ca/wads2021/wads-2021-accepted-papers/">WADS 2021 accepted papers</a> (<a href="https://mathstodon.xyz/@11011110/106062479142848045">\(\mathbb{M}\)</a>). 
The biennial Algorithms and Data Structures Symposium is usually in Canada, and this time was supposed to be in Halifax, but is looking very likely to be completely online, this August. I have one paper on the list; I’ll write more about it later when I have a preprint version ready to share.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=S5fPwE7GQOA">A “confounding topological curiosity”</a> (<a href="https://mathstodon.xyz/@11011110/106068163014347342">\(\mathbb{M}\)</a>, <a href="https://boingboing.net/2021/04/13/heres-a-confounding-topological-curiosity.html">via</a>): a double torus with a line through one of its holes can be continuously transformed so that the line instead goes through both holes.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.11818">Strange inverses in the group rings of torsion-free groups</a> (<a href="https://mathstodon.xyz/@11011110/106072203556292078">\(\mathbb{M}\)</a>, <a href="https://www.quantamagazine.org/mathematician-disproves-group-algebra-unit-conjecture-20210412/">via</a>, <a href="https://www.uni-muenster.de/MathematicsMuenster/news/artikel/2021-03-04.shtml">see also</a>). This result of Giles Gardam disproves the strongest of the three <a href="https://en.wikipedia.org/wiki/Kaplansky%27s_conjectures">Kaplansky conjectures on group rings</a>. It’s just an isolated example at this point but it does show that group rings are less well-behaved than had been hoped.</p>
  </li>
</ul></div>
    </content>
    <updated>2021-04-15T22:15:00Z</updated>
    <published>2021-04-15T22:15:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-04-18T18:59:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1863</id>
    <link href="https://theorydish.blog/2021/04/15/toc-a-personal-perspective-2021/" rel="alternate" type="text/html"/>
    <title>TOC: a Personal Perspective (2021)</title>
    <summary>Editorial note: this post has been written in celebration of 25 years for “TOC: a Scientific Perspective (1996),” by Oded Goldreich and Avi Wigderson. In the process, I have been made aware of a Facebook discussion from a few weeks ago (which I don’t know how to link to), and to Avi Wenderson’s recent talk that addresses this discussion (and more). I will not attribute statements from the Facebook discussion to individuals (nor will I specify its initiator), as they may not identify with the statements, when taken out of their original context or in the editorialized version here. In any case, this specific discussion is not the point. Feel free to claim ownership in comments . ———————————– I’m sure that, like me, you read on social media: “TOC is in crisis, scratch that, it is lost! We do not have agreed-upon challenges (that are not way out of reach) and do not know how to evaluate papers. Therefore our conferences are favoring progress in techniques and favor complicated papers on obscure problems over progress on important problems (in fact, there is shortage of interesting work on relevant problems). Our flagship conferences are broken and newer conferences, that aimed to [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Editorial note</strong>: this post has been written in celebration of 25 years for “<a href="http://www.wisdom.weizmann.ac.il/~oded/toc-sp.html">TOC: a Scientific Perspective (1996),</a>” by <a href="http://www.wisdom.weizmann.ac.il/~oded/">Oded Goldreich</a> and <a href="https://www.math.ias.edu/avi/home">Avi Wigderson</a>. In the process, I have been made aware of a Facebook discussion from a few weeks ago (which I don’t know how to link to), and to <a href="https://simons.berkeley.edu/talks/tbd-271">Avi Wenderson’s recent talk</a> that addresses this discussion (and more). I will not attribute statements from the Facebook discussion to individuals (nor will I specify its initiator), as they may not identify with the statements, when taken out of their original context or in the editorialized version here. In any case, this specific discussion is not the point. Feel free to claim ownership in comments .</p>



<p>———————————–</p>



<p>I’m sure that, like me, you read on social media: “TOC is in crisis, scratch that, it is lost! We do not have agreed-upon challenges (that are not way out of reach) and do not know how to evaluate papers. Therefore our conferences are favoring progress in techniques and favor complicated papers on obscure problems over progress on important problems (in fact, there is shortage of interesting work on relevant problems). Our flagship conferences are broken and newer conferences, that aimed to do better, have become just more of the same. This is not TCS as we remember it!”</p>



<p>There are many points with which I agree. Like others, I have been critical at times about the way we do some things, mourning papers and subfields that our conferences have missed. On several occasions, I have been pushing for change. At times I was successful but in other instances <a href="https://windowsontheory.org/2015/06/08/can-we-get-serious/">my suggestions </a>were deemed radical by the powers that be (and more modest/timid suggestions were adopted). <strong><em>But did TOC really lose its way?</em> </strong> </p>



<p>One of the commenter was saying “I have been hearing these complaints about focs and Stoc for more than ten years. They have come from powerful people that sit on committees. … So why nothing changes?“ This comment strikes a chord with me, but let me revise it and say that I have been hearing such complaints for the last 25 years (since attending my first conference), and it is almost always stated by the powerful people, those that have the responsibility to shape the field.</p>



<p>Following the continuous self-criticism, we are likely to assume that TOC is a dysfunctional field and has been so for many years. But if we look at the research achievements of TOC in the last quarter century, we must conclude that this was a glorious period. And the contributions of TCS were on different fronts. Contributions to applied CS and industry, growing contributions outside of CS as well as progress on fundamental questions within TOC. Said progress was obtained by simple papers and by complex and long papers. By papers developing new techniques, by papers making progress on known problems and by papers that introduced new problems, models or even papers that initiated new subfields. They have been made by breakthrough papers and by long sequences of modest papers. By papers in FOCS/STOC and papers in other conferences. So <strong>if TOC is in a continuous crisis, it is the most wonderful crisis possible</strong>.</p>



<p>During my studies (ages ago), I was intensely attracted to TOC. But at the same time, I felt that the field is under constant external attack. It was claimed that we are not as deep as Math and not as useful as CS. Many fewer universities than today have been hiring theoreticians. The field was grossly underfunded (still underfunded but less grossly) but still calls have been made to reduce funding to any area in which TOC is not directly serving other, more applied areas. The dissonance between my intuitive attraction and external criticism could have deterred me from TOC, but there were incredible leaders of TOC that effectively defended the field and shouted – look, something amazing is happening here. “TOC: a Scientific Perspective (1996),” by Oded Goldreich and Avi Wigderson gave me courage to continue. 25 years later, the case they once made in defense of TOC is so much easier to support (and they kept on making this case throughout the years in essays and <a href="https://www.math.ias.edu/avi/book">books</a>). <a href="https://simons.berkeley.edu/talks/tbd-271">As Avi argued</a>, TOC’s success have brought growth and diversification, influx of young talent, scientific respect, industrial respect, and societal respect. <strong>We should do better on self-respect.</strong></p>



<p>I am of course not advocating resting on our laurels’. Like in Alice’s adventures, in our fast field, it takes all the running we can do, to keep in the same place. If we want to get somewhere else, we must run at least twice as fast as that! Constructive criticism is a good thing but our tendency for alarmist/defeatist cries is not serving us well. As someone who grew (scientifically) in an atmosphere of struggle, I am grateful for the progress made in establishing our field by the generations that preceded me. We shouldn’t take for granted how easy we have it. But more importantly, confidence in our field and optimism towards our future are important for our impact on the world (I discussed one aspect of this <a href="https://theorydish.blog/2019/06/24/on-the-importance-of-disciplinary-pride-for-multidisciplinary-collaboration/">here</a>). Finally, thinking of students interested in TOC today and hearing the most powerful people in the field announcing that it is lost. I ask myself, who are the Avi and Oded that will give them the needed courage and optimism? The answer is still that their Avi and Oded are the very same Avi and Oded from my student years. But isn’t it about time that we lend a hand?</p>



<p/></div>
    </content>
    <updated>2021-04-15T14:50:46Z</updated>
    <published>2021-04-15T14:50:46Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-04-23T20:43:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18549</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/" rel="alternate" type="text/html"/>
    <title>Scott Wins a Prize</title>
    <summary>Quantum mechanics makes absolutely no sense—Roger Penrose Scott Answers Big Questions source Scott Aaronson has just been named the 2020 ACM Prize in Computing for groundbreaking contributions to quantum computing, Penrose’s comment notwithstanding. The prize citation also credits Scott’s multifaceted public outreach for making our fields accessible to many. Today Ken and I send congrats […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Quantum mechanics makes absolutely no sense—Roger Penrose</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/aaronsonhorgan/" rel="attachment wp-att-18565"><img alt="" class="alignright size-full wp-image-18565" height="170" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/AaronsonHorgan.jpg?resize=160%2C170&amp;ssl=1" width="160"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Scott Answers Big Questions <a href="https://blogs.scientificamerican.com/cross-check/scott-aaronson-answers-every-ridiculously-big-question-i-throw-at-him/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Scott Aaronson has just been named the 2020 ACM <a href="https://en.wikipedia.org/wiki/ACM_Prize_in_Computing">Prize</a> in Computing for groundbreaking contributions to quantum computing, Penrose’s comment notwithstanding.  The prize <a href="https://awards.acm.org/about/2020-acm-prize">citation</a> also credits Scott’s multifaceted public outreach for making our fields accessible to many.</p>
<p>
Today Ken and I send congrats to Scott for this singular honor.<br/>
<span id="more-18549"/></p>
<p>
The ACM Prize was founded in 2007.  It does not have the age limit of a Fields Medal but is similarly positioned.  Scott joins a impressive list of winners: </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/who-2/" rel="attachment wp-att-18562"><img alt="" class="aligncenter wp-image-18562" height="625" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/who-1.png?resize=550%2C625&amp;ssl=1" width="550"/></a></p>
<p/><p><br/>
</p><h2> Not Why He Did Win? </h2><p/>
<p>
Scott perhaps could have won for the following three accomplishments:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> His wonderful <a href="https://www.scottaaronson.com/blog/">blog</a>. About the prize, he <a href="https://www.scottaaronson.com/blog/?p=5448">says</a> there: </p>
<blockquote><p><b> </b> <em> Last week I got an email from Dina Katabi, my former MIT colleague, asking me to call her urgently. Am I in trouble? … Luckily, Dina only wanted to tell me that I’d been selected to receive the 2020 ACM Prize in Computing, a mid-career award founded in 2007 that comes with $250,000 from Infosys. Not the Turing Award but I’d happily take it! And I could even look back on 2020 fondly for something. </em>
</p></blockquote>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> His <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">book</a> on quantum: </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/book-6/" rel="attachment wp-att-18554"><img alt="" class="aligncenter wp-image-18554" height="240" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/book-1.jpg?resize=160%2C240&amp;ssl=1" width="160"/></a></p>
<p>
Democritus was known as “the laughing philosopher,” though some other depictions of the laugh range from world-weary to pained.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> His sense of <a href="https://www.scottaaronson.com/blog/?p=62">humor</a>. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/kl2/" rel="attachment wp-att-18563"><img alt="" class="aligncenter size-large wp-image-18563" height="351" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/kl2.png?resize=600%2C351&amp;ssl=1" width="600"/></a></p>
<p>
This is my returning thanks, in a way. Ken says that what impresses him is not the floor-length garment but the floor-length blackboard. Both of us hope that after handling “For All” and “Exists,” he went on to solve the problem of placing “Not” in English—for instance, “Why He Did Not Win?” is more grammatically natural but wrong.</p>
<p/><h2> Why He Did Win? </h2><p/>
<p>
Scott perhaps did win for the following four accomplishments:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> The theoretical foundations of the sampling-based quantum supremacy <a href="https://arxiv.org/pdf/1612.05903.pdf">experiments</a>—joint with Lijie Chen. Ken adds that this paper is also known for the “Schrödinger” and “Feynman” nomenclature for simulating quantum classically, and the idea of hybridizing those approaches.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> The algebrization <a href="https://www.scottaaronson.com/papers/alg.pdf">barrier</a> in complexity theory—joint with Avi Wigderson.  About Avi’s talk on this at my 60th birthday workshop, Ken <a href="https://blog.computationalcomplexity.org/2008/05/report-on-sym-for-liptons-60th-bday.html">wrote</a> that it “planted a Monty Python foot on further progress” on lower bounds.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Limitations on quantum computers—work on the quantum lower bound for the <a href="https://www.scottaaronson.com/papers/collision.ps">collision problem</a>.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> The opposite of limitations—getting quantum advantage from the simplest of components in linear <a href="https://dl.acm.org/doi/10.1145/1993636.1993682">optics</a>. And <a href="https://arxiv.org/abs/1309.7460">this</a>, likewise joint with Alex Arkhipov. This approach has been followed by many, notably last <a href="https://science.sciencemag.org/content/370/6523/1460">December</a> by a large team in China.  We just <a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/">wrote</a> about envy of big experiments, but Scott has arguably done the most of anyone in our field to launch them.</p>
<p>
There are many more. But maybe the one that will get us all rich is Scott’s <a href="https://arxiv.org/abs/1110.5353">work</a> on <a href="https://en.wikipedia.org/wiki/Quantum_money">quantum money</a>. Qubitcoin, anyone?</p>
<p/><h2> Open Problems </h2><p/>
<p>
Wonderful to add Scott to the list of winners of this award. Congrats again.</p>
<p>
We also congratulate Paul Beame on the SIGACT Distinguished Service Award.</p></font></font></div>
    </content>
    <updated>2021-04-15T13:31:22Z</updated>
    <published>2021-04-15T13:31:22Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="award"/>
    <category term="experiments"/>
    <category term="Paul Beame"/>
    <category term="Prize"/>
    <category term="quantum"/>
    <category term="quantum advantage"/>
    <category term="Scott Aaronson"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-04-23T20:39:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3988126252359540708</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3988126252359540708/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/ordering-beauty.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3988126252359540708" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3988126252359540708" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/ordering-beauty.html" rel="alternate" type="text/html"/>
    <title>Ordering Beauty</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>First, congratulations to fellow complexity theorist and <a href="https://www.scottaaronson.com/blog/">blogger</a> Scott Aaronson for <a href="https://awards.acm.org/about/2020-acm-prize">receiving the 2020 ACM Prize in Computing</a> for "groundbreaking contributions to quantum computing". The prize is ACM's highest honor for mid-career researchers. Well deserved! </p><p>Now back to our regularly scheduled post...</p><p>Every freshman at Cornell back in 1981 had to take two seminar courses, basically one-shot courses in an usually humanities area which required no prerequisites but lots of writing. I took my first course in philosophy. The instructor, a PhD student, at one point described his research, a philosophical argument that there is an intrinsic total ordering of beauty, say that Beethoven would always sit above the Beatles, no matter the beholder. I didn't believe him then and still don't today. A few months ago the Washington Post ran a story with the same theme entitled <a href="https://www.washingtonpost.com/entertainment/maradona-messi-ronaldo-zlatan-shakespeare-beatles/2020/12/23/27654712-38a9-11eb-9276-ae0ca72729be_story.html">Maradona was great, and maybe the greatest. Can we make similar claims about artists?</a></p><p>Somehow we have this belief when it comes to conference submissions, that there is some perfect ordering of the submissions and a good PC can suss it out. That's not really how it works. Let's say a conference has an accept rate of 30%. Typically 10% of the submissions are strong and will be accepted by any committee. About half the submissions are just okay or worse and would be rejected. The other 40% of the submissions will be chosen seemingly randomly based on the tastes of the specific members of the program committee. Experiments in the NeurIPS and ESA conferences have bourn this out. </p><p>Why not make the randomness explicit instead of implicit? Have the PC divide the papers into three piles, definite accepts, definite rejects and the middle. Take the third group and randomly choose which ones to accept. It will create a more interesting program. Also randomness removes biases, randomness doesn't care about gender, race and nationality or whether the authors are at senior professors at MIT or first year grad students at Southern North Dakota. </p><p>We put far too much weight on getting accepted into a conference given the implicit randomness of a PC. If we make the randomness explicit that would devalue that weight. We would have to judge researchers on the quality of their research instead of their luck in conferences.</p><p>Given that conferences, especially the virtual ones, have no real limits on the number of papers and talks, you might say why not just accept all the papers in the middle. Works for me.</p></div>
    </content>
    <updated>2021-04-15T12:31:00Z</updated>
    <published>2021-04-15T12:31:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-04-23T07:12:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/054</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/054" rel="alternate" type="text/html"/>
    <title>TR21-054 |  Encodings and the Tree Evaluation Problem | 

	Ian Mertz, 

	James  Cook</title>
    <summary>We show that the Tree Evaluation Problem with alphabet size $k$ and height $h$ can be solved by branching programs of size $k^{O(h/\log h)} + 2^{O(h)}$. This answers a longstanding challenge of Cook et al. (2009) and gives the first general upper bound since the problem's inception.</summary>
    <updated>2021-04-14T19:29:12Z</updated>
    <published>2021-04-14T19:29:12Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-23T20:38:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5448</id>
    <link href="https://www.scottaaronson.com/blog/?p=5448" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5448#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5448" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The ACM Prize thing</title>
    <summary xml:lang="en-US">Last week I got an email from Dina Katabi, my former MIT colleague, asking me to call her urgently. Am I in trouble? For what, though?? I haven’t even worked at MIT for five years! Luckily, Dina only wanted to tell me that I’d been selected to receive the 2020 ACM Prize in Computing, a […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Last week I got an email from Dina Katabi, my former MIT colleague, asking me to call her urgently.  <em>Am I in trouble?  For what, though??  I haven’t even worked at MIT for five years!</em></p>



<p>Luckily, Dina only wanted to tell me that I’d been <a href="https://awards.acm.org/about/2020-acm-prize">selected</a> to receive the 2020 <a href="https://en.wikipedia.org/wiki/ACM_Prize_in_Computing">ACM Prize in Computing</a>, a mid-career award founded in 2007 that comes with $250,000 from Infosys.  Not the Turing Award but I’d happily take it!  And I could even look back on 2020 fondly for something.</p>



<p>I was utterly humbled to see the <a href="https://awards.acm.org/acm-prize/award-winners">list</a> of past ACM Prize recipients, which includes amazing computer scientists I’ve been privileged to know and learn from (like Jon Kleinberg, Sanjeev Arora, and Dan Boneh) and others who I’ve admired from afar (like Daphne Koller, Jeff Dean and Sanjay Ghemawat of Google MapReduce, and David Silver of AlphaGo and AlphaZero).</p>



<p>I was even more humbled, later, to read my <a href="https://awards.acm.org/award_winners/aaronson_9555914">prize citation</a>, which focuses on four things:</p>



<ol><li>The theoretical foundations of the sampling-based quantum supremacy experiments now being carried out (and in particular, my and Alex Arkhipov’s <a href="https://www.theoryofcomputing.org/articles/v009a004/">2011 paper on BosonSampling</a>);</li><li>My and Avi Wigderson’s <a href="https://www.scottaaronson.com/papers/alg.pdf">2008 paper</a> on the algebrization barrier in complexity theory;</li><li>Work on the limitations of quantum computers (in particular, the 2002 <a href="https://www.scottaaronson.com/papers/collision.pdf">quantum lower bound for the collision problem</a>); and</li><li>Public outreach about quantum computing, including through <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">QCSD</a>, popular talks and articles, and this blog.</li></ol>



<p>I don’t know if I’m worthy of such a prize—but I know that if I am, then it’s mainly for work I did between roughly 2001 and 2012.  This honor inspires me to want to be more like I was back then, when I was driven, non-jaded, and obsessed with figuring out the contours of BQP and efficient computation in the physical universe.  It makes me want to justify the ACM’s faith in me.</p>



<p>I’m grateful to the committee and nominators, and more broadly, to the whole quantum computing and theoretical computer science communities—which I “joined” in some sense around age 16, and which were the first communities where I ever felt like I belonged.  I’m grateful to the mentors who made me what I am, especially Chris Lynch, Bart Selman, Lov Grover, Umesh Vazirani, Avi Wigderson, and (if he’ll allow me to include him) John Preskill.  I’m grateful to the slightly older quantum computer scientists who I looked up to and tried to emulate, like Dorit Aharonov, Andris Ambainis, Ronald de Wolf, and John Watrous.  I’m grateful to my wonderful colleagues at UT Austin, in the CS department and beyond.  I’m grateful to my students and postdocs, the pride of my professional life.  I’m grateful, of course, to my wife, parents, and kids.</p>



<p>By coincidence, my <a href="https://www.scottaaronson.com/blog/?p=5437">last post</a> was also about prizes to theoretical computer scientists—in that case, two prizes that attracted controversy because of the recipient’s (or would-be recipient’s) political actions or views.  It would understate matters to point out that not everyone has always agreed with everything I’ve said on this blog.  I’m <em>ridiculously</em> lucky, and I know it, that even living through this polarized and tumultuous era, I never felt forced to choose between academic success and the freedom to speak my conscience in public under my real name.  If there’s been one constant in my public stands, I’d like to think that—inspired by memories of my own years as an unknown, awkward, self-conscious teenager—it’s been my determination to nurture and protect talented young scientists, whatever they look like and wherever they come from.  And I’ve tried to live up to that ideal in real life, and I welcome anyone’s scrutiny as to how well I’ve done.</p>



<p>What should I do with the prize money? I confess that my first instinct was to donate it, in its entirety, to some suitable charity—specifically, something that would make all the strangers who’ve attacked me on Twitter, Reddit, and so forth over the years realize that I’m fundamentally a good person.  But I was talked out of this plan by my family, who pointed out that<br/>(1) in all likelihood, <em>nothing</em> will make online strangers stop hating me,<br/>(2) in any case this seems like a poor basis for making decisions, and<br/>(3) if I really want to give others a say in what to do with the winnings, then why not everyone who’s stood by me and supported me?</p>



<p>So, beloved commenters!  Please mention your favorite charitable causes below, especially weird ones that I wouldn’t have heard of otherwise.  If I support their values, I’ll make a small donation from my prize winnings.  Or a larger donation, especially if you donate yourself and challenge me to match.  Whatever’s left after I get tired of donating will probably go to my kids’ college fund.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> And by an amusing coincidence, today is apparently <a href="https://worldquantumday.org/">“World Quantum Day”</a>!  I hope your Quantum Day is as pleasant as mine (and stable and coherent).</p></div>
    </content>
    <updated>2021-04-14T15:41:51Z</updated>
    <published>2021-04-14T15:41:51Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-04-21T18:19:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/053</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/053" rel="alternate" type="text/html"/>
    <title>TR21-053 |  Information in propositional proofs and algorithmic proof search | 

	Jan  Krajicek</title>
    <summary>We study from the proof complexity perspective the (informal) proof search problem:
Is there an optimal way to search for propositional proofs?
We note that for any fixed proof system there exists a time-optimal proof search algorithm. Using classical proof complexity results about reflection principles we prove that a time-optimal proof search algorithm exists w.r.t. all proof systems iff a p-optimal proof system exists.
To characterize precisely the time proof search algorithms need for individual formulas we introduce a new proof complexity measure based on algorithmic information concepts. In particular, to a proof system P we attach {\bf information-efficiency function} $i_P(\tau)$ assigning to a tautology a natural number, and we show that:
- $i_P(\tau)$ characterizes time any $P$-proof search algorithm has to use on $\tau$ and that for a fixed $P$ there is such an information-optimal algorithm,
- a proof system is information-efficiency optimal iff it is p-optimal,
- for non-automatizable systems $P$ there are formulas $\tau$ with short proofs but having large information measure $i_P(\tau)$.
We isolate and motivate the problem to establish {\em unconditional} super-logarithmic lower bounds for $i_P(\tau)$ where no super-polynomial size lower bounds are known. We also point out connections of the new measure with some topics in proof complexity other than proof search.</summary>
    <updated>2021-04-13T07:34:11Z</updated>
    <published>2021-04-13T07:34:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-23T20:38:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18514</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/" rel="alternate" type="text/html"/>
    <title>Wobble in the Standard Model</title>
    <summary>Prediction is very difficult, especially if it’s about the future—Niels Bohr Boston Globe “Uncertainty” source Lisa Randall is a professor of theoretical physics at Harvard. Her research has touched on many of the basic questions of modern physics: supersymmetry, Standard Model observables, cosmological inflation, baryogenesis, grand unified theories, and general relativity. She has also written […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Prediction is very difficult, especially if it’s about the future—Niels Bohr</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/randalluncertainty/" rel="attachment wp-att-18526"><img alt="" class="alignright wp-image-18526" height="210" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/RandallUncertainty.jpg?resize=153%2C210&amp;ssl=1" width="153"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Boston Globe “Uncertainty” <a href="https://www.bostonglobe.com/ideas/2011/10/22/lisa-randall-physics-universe-uncertainty/BfSYjipZy7HkQPmmRs8ZRI/story.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Lisa Randall is a professor of theoretical physics at Harvard. Her research has touched on many of the basic questions of modern physics: supersymmetry, Standard Model observables, cosmological inflation, baryogenesis, grand unified theories, and general relativity. She has also written popular books about her work and science in general. Thus she has a handle on aspects of science that overlap my expertise—not to mention those of her sister Dana Randall, whom I have known as a colleague for many years.</p>
<p>
Today, Ken and I thought we would talk about recent developments in particle physics, and their connection to two topics dear to us.<br/>
<span id="more-18514"/></p>
<p>
Randall’s most recent popular book is <a href="https://en.wikipedia.org/wiki/Dark_Matter_and_the_Dinosaurs">Dark Matter and the Dinosaurs</a>. The idea she advances is that the periodic extinctions in Earth’s history may have been caused when the solar system passes through a plane of dark matter within our galaxy. But dark matter and also dark energy have come under <a href="https://www.sciencenews.org/article/dark-matter-mystery-deepens-demise-reported-detection">increasing</a> <a href="https://phys.org/news/2021-03-composition-percent-universe.html">recent</a> <a href="https://www.nbcnews.com/science/space/maybe-dark-matter-doesn-t-exist-after-all-new-research-n1252995">doubt</a>, even from their original <a href="https://www.newscientist.com/article/mg24632851-400-why-the-universe-i-invented-is-right-but-still-not-the-final-answer/">formulator</a>. Maybe Niels Bohr’s quote should also say: </p>
<blockquote><p><b> </b> <em> <i>Prediction is very difficult, especially if it’s about the past.</i> </em>
</p></blockquote>
<p>
Randall’s previous book, <a href="https://en.wikipedia.org/wiki/Knocking_on_Heaven's_Door_(book)">Knocking on Heaven’s Door</a>, is most relevant to this post. The 1973 Bob Dylan <a href="https://en.wikipedia.org/wiki/Knockin'_on_Heaven's_Door">song</a> title it pinches describes the feeling of doing frontier physical science. Insofar as her own work is mostly theoretical, much of it connects to feelings we have in computer science—especially complexity lower bounds where the door mostly feels slammed shut. </p>
<p>
But the book is also about the practice of experimental science—not only how to gather knowledge but when and how we can have confidence in it. Its long middle part is titled, “Machines, Measurements, and Probability.” All three elements are foremost in considering a new development that involves two measurements taken 20 years apart.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/randallbooks/" rel="attachment wp-att-18528"><img alt="" class="aligncenter wp-image-18528" height="254" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/RandallBooks.jpg?resize=406%2C254&amp;ssl=1" width="406"/></a></p>
<p/><h2> Muons </h2><p/>
<p>
Last Tuesday’s New York Times <a href="https://www.nytimes.com/2021/04/07/science/particle-physics-muon-fermilab-brookhaven.html">highlighted</a> a potential discovery in particle physics. It was in their Tuesday science section. </p>
<p>
The result is an experimental discovery that could show that the current model of matter is wrong. </p>
<blockquote><p><b> </b> <em> “This is our Mars rover landing moment,” said Chris Polly, a physicist at the Fermi National Accelerator Laboratory, or Fermilab, in Batavia, Ill., who has been working toward this finding for most of his career. </em>
</p></blockquote>
<p>
Indeed. It is a Mars landing moment. They both involved many people, lots of exotic machinery, lots of money, many years. Say three billion dollars or so for Mars. Say nearly the same amount for muons—the annual budget for Fermilab is over one half billion dollars. It was certainly enough to reassemble and upgrade a huge accelerator ring that was <a href="https://www.bnl.gov/newsroom/news.php?a=112259">first used</a> at Brookhaven National Lab on Long Island in 2001:</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/fermilabring/" rel="attachment wp-att-18539"><img alt="" class="aligncenter wp-image-18539" height="301" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/FermilabRing.jpg?resize=450%2C301&amp;ssl=1" width="450"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">NY Times <a href="https://www.nytimes.com/2021/04/07/science/particle-physics-muon-fermilab-brookhaven.html">src</a></font>
</td>
</tr>
</tbody></table>
<p>
The study used the muon to probe the Standard Model of physics. Muons are useful because they are charged like an electron, which helps control them in an accelerator. Yet their mass is roughly 207 times larger than an electron. The same charge helps control their motion and the large mass makes collisions more interesting. As Polly stated in Natalie Wolchover’s <a href="https://www.quantamagazine.org/muon-g-2-experiment-at-fermilab-finds-hint-of-new-particles-20210407/">story</a> for <em>Quanta</em>:</p>
<blockquote><p><b> </b> <em> “[I]f you’re looking for particles that could explain the missing mass of the universe—dark matter—or you’re looking for [supersymmetry], that’s where the muon has a unique role.” </em>
</p></blockquote>
<p/><h2> Theory and Jealousy </h2><p/>
<p>
Computer science theory is so different from high end physics. We are closer to the type of research that Randall does. We involve few people, no exotic machinery, and small amounts of money. Maybe the closest attribute we have to high-end research is we also take years and years.</p>
<p>
Perhaps we are also jealous of high-end physics. Not just for money, but for the ability of particle physicists to get announcements into the New York Times. Polly said the <a href="https://www.energy.gov/science/articles/first-person-science-chris-polly-muon-physics">following</a> about the ending day of the muon experiment two decades ago:</p>
<blockquote><p><b> </b> <em> When we revealed the results, people from all over the world flew in to visit the lab. These experiments take decades to build and analyze, so you don’t get to go to very many of these events. We did a little “Drumroll, please” and then had the postdoc managing the spreadsheet hit the button to show it on the projector. Lo and behold, you could see that there was still a three-sigma discrepancy! </em>
</p></blockquote>
<p>
At the time he was a graduate assistant assigned to machinery for measuring particle energies. He had fixed a problem where someone had touched a component with bare hands and thereby ruined its sheathing. All such problems were meant to be ironed-out by the drum-roll event. But all this raises two further interesting issues that connect the muon results with issues we think about in computer science. Let’s look at them next.</p>
<p/><h2> Three to Four Sigma </h2><p/>
<p>
In an experimental science one must be aware that results are not exact. They are samples from some random process. Flip a coin 10 times in a row. If they all come up heads what does that mean? Could be the coin is fair but this happens about one time in a thousand. Or the coin is biased. Or something else. </p>
<p>
Flip a muon many times. That is sample some muon experiment. The outcome is from a random process. Some of it comes from properties of the natural processes themselves and others from incidentals of the measurement apparatus. How do we decide if the experiment means what we think it does?</p>
<p>
The theory developed by Carl Gauss and others before and after to delineate the normal distribution was largely prompted by analysis of measurement errors <a href="https://www.maa.org/sites/default/files/images/upload_library/22/Allendoerfer/stahl96.pdf">to begin with</a>. This yields the “<a href="https://en.wikipedia.org/wiki/68-95-99.7_rule">rule of three</a>,” about the percentage of values that naturally lie within an interval estimate in a normal distribution: 68%, 95%, and 99.7% of the values lie within one, two, and three standard deviations of the mean, respectively.</p>
<p>
The question is, how to assess cases where the measurement result is well outside these intervals—when can we conclude it is more than a deviation by natural chance? In social sciences a result is “significant” provided it lies outside two-sigma. In particle physics, there is a convention of a five-sigma effect (99.99994% confidence) being required to qualify as a discovery. No Nobel prize for less. </p>
<p>
The situation with the muons has an extra factor of repeated measurements—but there have been only two measurements so far:</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/muoncharts/" rel="attachment wp-att-18531"><img alt="" class="aligncenter wp-image-18531" height="247" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/MuonCharts.png?resize=550%2C247&amp;ssl=1" width="550"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://news.fnal.gov/2021/04/first-results-from-fermilabs-muon-g-2-experiment-strengthen-evidence-of-new-physics/">src1</a>, <a href="https://www.sciencemag.org/news/2021/04/particle-mystery-deepens-physicists-confirm-muon-more-magnetic-predicted">src2</a></font>
</td>
</tr>
</tbody></table>
<p>
The blue line in the left figure is the original Brookhaven measurement; the red is the new one. There is also <a href="https://4gravitons.com/2021/04/09/theoretical-uncertainty-and-uncertain-theory/">theoretical uncertainty</a> in the calculation of the Standard Model prediction, and that combines with the measurement error bars to give the sigma baseline. The chart at right normalizes the deviation to parts per billion—the measurements need to be incredibly fine. This scale appears to be about 25% under the current <img alt="{\sigma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scale (it shows about <img alt="{2.8}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2.8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for Brookhaven compared to its <img alt="{3.7\sigma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3.7%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> after <a href="https://arxiv.org/pdf/2006.04822.pdf">revised</a> uncertainty) but it is close enough to get the picture.</p>
<p>
Although the new Fermilab result by itself deviates slightly less from the Standard Model, it corroborates the earlier measurement. It is not fully independent from it, but the combination is enough to raise the current claimed deviation to about <img alt="{4.2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> sigmas. This is well above social science level but below Nobel level. This is with respect to the probability that the effects are real. </p>
<p>
The social significance of <img alt="{4.2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is that it is above the “<img alt="{3+}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>” level where hoped-for anomalies have subsequently disappeared for reasons chalked up to natural chance. This is because physicists around the world do many a hundredfold amount of hopeful measurements. Some measurements get initial “bumps” up just because of the numbers. But <img alt="{4.2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> reduces the natural frequency under 1-in-40,000. This is why reproducing measurements is so important, why the new Fermilab team devoted all the expense and effort. A more independent measurement on other machines could give a higher boost that might get over the <img alt="{5.0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B5.0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> line. Time to break out the wallets and hammers?</p>
<p>
The <img alt="{4.2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is not, however, beyond the realm of <a href="https://en.wikipedia.org/wiki/Faster-than-light_neutrino_anomaly">recent</a> <a href="https://physics.aps.org/articles/v11/s78">experience</a> with apparatus faults and modeling error. On the latter, there is <a href="https://www.math.columbia.edu/~woit/wordpress/?p=12292">still</a> some <a href="http://resonaances.blogspot.com/2021/04/why-is-it-when-something-happens-it-is.html">doubt</a> about the theoretical prediction for the muon’s magnetic moment. In any event, the muon results are exciting but still below what is required for a true discovery. Time will tell.</p>
<p>
The second factor we draw attention to concerns the human “hoping” directly.</p>
<p/><h2> Blinding </h2><p/>
<p>
In any experimental science one must also be aware that people are not unbiased. Scientists have much invested in the outcome of their experiments. Think jobs, tenure even, funding, and more. So a big physics experiment like the muon one must be careful. They follow standard practice to perform <a href="https://en.wikipedia.org/wiki/blinded_experiment">blinded</a> data analysis. </p>
<p>
This surprised me. This blinding is a crypto-type protocol, which is something we computer scientists study. The muon team performed a protocol that protected against cheating. Here is how they did it:</p>
<blockquote><p><b> </b> <em> In this case, the master clock that keeps track of the muons’ wobble had been set to a rate unknown to the researchers. The figure was sealed in envelopes that were locked in the offices at Fermilab and the University of Washington in Seattle.</em></p><em>
<p>
In a ceremony on Feb. 25 that was recorded on video and watched around the world on Zoom, Dr. Polly opened the Fermilab envelope and David Hertzog from the University of Washington opened the Seattle envelope. The number inside was entered into a spreadsheet, providing a key to all the data, and the result popped out to a chorus of wows.</p>
</em><p><em>
“That really led to a really exciting moment, because nobody on the collaboration knew the answer until the same moment,” said Saskia Charity, a Fermilab postdoctoral fellow who has been working remotely from Liverpool, England, during the pandemic. </em>
</p></blockquote>
<p>
This mechanism for blinding suggests possible crypto questions. They hid the master clock rate. Can this be modeled as one of our crypto problems? Can we prove some security bounds? If they claim that hiding the rate protects against cheating then they should be able to make this claim precise. The <a href="https://en.wikipedia.org/wiki/First_observation_of_gravitational_waves">discovery</a> of gravitational waves used a <a href="https://www.ligo.org/news/blind-injection.php">blind injection</a> scheme tailored for that experiment. How can this be generalized?</p>
<p/><h2> Open Problems </h2><p/>
<p>
We have discussed two aspects that involve soft numbers rather than hard machines and hard-shelled particles. Perhaps they are interesting new problems for us? What do you think?</p>
<p/></font></font></div>
    </content>
    <updated>2021-04-12T23:11:21Z</updated>
    <published>2021-04-12T23:11:21Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="blind injection"/>
    <category term="Chris Polly"/>
    <category term="Lisa Randall"/>
    <category term="muon"/>
    <category term="particles"/>
    <category term="Physics"/>
    <category term="result"/>
    <category term="standard model"/>
    <category term="uncertainty"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-04-23T20:39:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/052</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/052" rel="alternate" type="text/html"/>
    <title>TR21-052 |  Upslices, Downslices, and Secret-Sharing with Complexity of $1.5^n$ | 

	Oded Nir, 

	Benny Applebaum</title>
    <summary>A secret-sharing scheme allows to distribute a secret $s$ among $n$ parties such that only some predefined ``authorized'' sets of parties can reconstruct the secret, and all other ``unauthorized'' sets learn nothing about $s$. 
The collection of authorized/unauthorized sets can be captured by a monotone function $f:\{0,1\}^n\rightarrow \{0,1\}$. 
In this paper, we focus on monotone functions that all their min-terms are sets of size $a$, and on their duals -- monotone functions whose max-terms are of size $b$. We refer to these classes as $(a,n)$-upslices and $(b,n)$-downslices, and note that these natural families correspond to monotone $a$-regular DNFs and monotone $(n-b)$-regular CNFs. We derive the following results.

1. (General downslices) Every downslice can be realized with total share size of $1.5^{n+o(n)}&lt;2^{0.585 n}$. Since every monotone function can be cheaply decomposed into $n$ downslices, we obtain a similar result for general access structures improving the previously known $2^{0.637n+o(n)}$ complexity of Applebaum, Beimel, Nir and Peter (STOC 2020). We also achieve a minor improvement in the exponent of linear secrets sharing schemes. 

2. (Random mixture of upslices) Following Beimel and Farras (TCC 2020) who studied the complexity of random DNFs with constant-size terms, we consider the following general distribution $F$ over monotone DNFs: For each width value $a\in [n]$, uniformly sample $k_a$ monotone terms of size $a$, where $k=(k_1,\ldots,k_n)$ is an arbitrary vector of non-negative integers. We show that, except with exponentially small probability, $F$ can be realized with share size of $2^{0.5 n+o(n)}$ and
    can be linearly realized with an exponent strictly smaller than $2/3$. Our proof also provides a candidate distribution for ``exponentially-hard'' access structure. 
    
We use our results to explore connections between several seemingly unrelated questions about the complexity of secret-sharing schemes such as worst-case vs. average-case, linear vs. non-linear and primal vs. dual access structures. We prove that, in at least one of these settings, there is a significant gap in secret-sharing complexity.</summary>
    <updated>2021-04-12T17:17:39Z</updated>
    <published>2021-04-12T17:17:39Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-23T20:38:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8147147831052480535</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8147147831052480535/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/is-following-reaction-to-getting-first.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8147147831052480535" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8147147831052480535" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/is-following-reaction-to-getting-first.html" rel="alternate" type="text/html"/>
    <title>Is the following reaction to getting the first COVID shot logical?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Alice works at a charity that puts together bag and box lunches for children.</p><p><br/>They all wear masks and they are 12 feet apart and very careful, and nobody there has gotten COVID.</p><p>Then Alice gets here first COVID shot and says:</p><p><br/></p><p><i>I am not going to work for that charity until I have had my second shot and waited  4 weeks so I am immune. </i></p><p><i><br/></i></p><p>She is really scared of getting COVID NOW that  she is on the verge of being immune. </p><p><br/></p><p>Is that logical? She was not scared before. So does it make sense to be scared now? I see where she is coming from emotionally, but is there a logical argument for her viewpoint? I ask nonrhetorically.</p><p><br/></p><p>bill g. </p></div>
    </content>
    <updated>2021-04-12T04:12:00Z</updated>
    <published>2021-04-12T04:12:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-04-23T07:12:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5437</id>
    <link href="https://www.scottaaronson.com/blog/?p=5437" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5437#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5437" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Just some prizes</title>
    <summary xml:lang="en-US">Oded Goldreich is a theoretical computer scientist at the Weizmann Institute in Rehovot, Israel. He’s best known for helping to lay the rigorous foundations of cryptography in the 1980s, through seminal results like the Goldreich-Levin Theorem (every one-way function can be modified to have a hard-core predicate), the Goldreich-Goldwasser-Micali Theorem (every pseudorandom generator can be […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://en.wikipedia.org/wiki/Oded_Goldreich">Oded Goldreich</a> is a theoretical computer scientist at the Weizmann Institute in Rehovot, Israel.  He’s best known for helping to lay the rigorous foundations of cryptography in the 1980s, through seminal results like the <a href="https://en.wikipedia.org/wiki/Hard-core_predicate">Goldreich-Levin Theorem</a> (every one-way function can be modified to have a hard-core predicate), the <a href="https://people.csail.mit.edu/silvio/Selected%20Scientific%20Papers/Pseudo%20Randomness/How%20To%20Construct%20Random%20Functions.pdf">Goldreich-Goldwasser-Micali Theorem</a> (every pseudorandom generator can be made into a pseudorandom function), and the <a href="https://www.cs.purdue.edu/homes/hmaji/teaching/Fall%202017/lectures/39.pdf">Goldreich-Micali-Wigderson protocol</a> for secure multi-party computation.  I first met Oded more than 20 years ago, when he lectured at a summer school at the Institute for Advanced Study in Princeton, barefoot and wearing a tank top and what looked like pajama pants.  It was a bracing introduction to complexity-theoretic cryptography.  Since then, I’ve interacted with Oded from time to time, partly around his <a href="http://www.wisdom.weizmann.ac.il/~oded/on-qc.html">firm belief</a> that quantum computing is impossible.</p>



<p>Last month a committee in Israel voted to award Goldreich the <a href="https://en.wikipedia.org/wiki/Israel_Prize">Israel Prize</a> (roughly analogous to the US National Medal of Science), for which I’d say Goldreich had been a plausible candidate for decades.  But alas, Yoav Gallant, Netanyahu’s Education Minister, then rather <a href="https://www.jpost.com/israel-news/high-court-revokes-israel-prize-in-math-to-pro-bds-professor-664538">non-gallantly blocked the award</a>, solely because he objected to Goldreich’s far-left political views (and apparently because of various statements Goldreich signed, including in support of a boycott of Ariel University, which is in the West Bank).  The case went all the way to the Israeli Supreme Court (!), which <a href="https://www.washingtonpost.com/world/middle_east/israeli-academic-wont-receive-prize-after-signing-petition/2021/04/08/d1e987ca-987b-11eb-8f0a-3384cf4fb399_story.html">ruled two days ago</a> in Gallant’s favor: he gets to “delay” the award to investigate the matter further, and in the meantime has apparently sent out invitations for an award ceremony next week that doesn’t include Goldreich.  Some are now calling for the other winners to boycott the prize in solidarity until this is righted.</p>



<p>I doubt readers of this blog need convincing that this is a travesty and an embarrassment, a <em><a href="https://en.wiktionary.org/wiki/shanda#:~:text=shanda%20(uncountable),(Jewish)%20shame%3B%20disgrace.">shanda</a></em>, for the Netanyahu government itself.  That I disagree with Goldreich’s far-left views (or <em>might</em> disagree, if I knew in any detail what they were) is totally immaterial to that judgment.  In my opinion, not even Goldreich’s belief in the impossibility of quantum computers should affect his eligibility for the prize. <img alt="&#x1F642;" class="wp-smiley" src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f642.png" style="height: 1em;"/></p>



<p>Maybe it would be better to say that, as far as his academic colleagues in Israel and beyond are concerned, Goldreich <em>has</em> won the Israel Prize; it’s only some irrelevant external agent who’s blocking his receipt of it.  Ironically, though, among Goldreich’s many heterodox beliefs is a <a href="http://www.wisdom.weizmann.ac.il/~oded/on-awards1.html">total rejection of the value of scientific prizes</a> (although Goldreich has also said he wouldn’t refuse the Israel Prize if offered it!).</p>



<p/><hr/><p/>



<p>In unrelated news, the 2020 Turing Award has been given to <a href="https://en.wikipedia.org/wiki/Alfred_Aho">Al Aho</a> and <a href="https://en.wikipedia.org/wiki/Jeffrey_Ullman">Jeff Ullman</a>.  Aho and Ullman have both been celebrated leaders in CS for half a century, having laid many of the foundations of formal languages and compilers, and having coauthored one of CS’s <a href="https://www.amazon.com/Design-Analysis-Computer-Algorithms/dp/0201000296/ref=pd_lpo_14_t_1/140-9181226-0879049?_encoding=UTF8&amp;pd_rd_i=0201000296&amp;pd_rd_r=4c6cd308-4669-45ee-ad4d-292ee24e043f&amp;pd_rd_w=WxNWv&amp;pd_rd_wg=AAt3G&amp;pf_rd_p=337be819-13af-4fb9-8b3e-a5291c097ebb&amp;pf_rd_r=T95EJ78DHVE1V1G604D1&amp;psc=1&amp;refRID=T95EJ78DHVE1V1G604D1">defining textbooks</a> with <a href="https://en.wikipedia.org/wiki/John_Hopcroft">John Hopcroft</a> (who already received a different Turing Award).</p>



<p>But again there’s a controversy.  <a href="https://lobelog.com/niac-calls-out-anti-iranian-stanford-professor/">Apparently</a>, in 2011, Ullman wrote to an Iranian student who wanted to work with him, saying that as “a matter of principle,” he would not accept Iranian students until the Iranian government recognized Israel.  Maybe I should say that I, like Ullman, am both a Jew and a Zionist, but I find it hard to imagine the state of mind that would cause me to hold some hapless student responsible for the misdeeds of their birth-country’s government.  Ironically, this is a mirror-image of the <a href="https://en.wikipedia.org/wiki/Academic_boycott_of_Israel#Mona_Baker,_Miriam_Shlesinger_and_Gideon_Toury">tactics</a> that the BDS movement has wielded against Israeli academics.  Unlike Goldreich, though, Ullman seems to have gone beyond merely expressing his beliefs, actually turning them into a one-man foreign policy.</p>



<p>I’m <a href="https://www.scottaaronson.com/blog/?p=3167">proud</a> of the Iranian students I’ve mentored and hope to mentor more.  While I don’t think this issue should affect Ullman’s Turing Award (and I haven’t seen anyone claim that it should), I do think it’s appropriate to use the occasion to express our opposition to all forms of discrimination.  I fully endorse Shafi Goldwasser’s <a href="https://www.facebook.com/SimonsInstitute/">response</a> in her capacity as Director of the Simons Institute for Theory of Computing in Berkeley:</p>



<blockquote class="wp-block-quote"><p>As a senior member of the computer science community and an American-Israeli, I stand with our Iranian students and scholars and outright reject any notion by which admission, support, or promotion of individuals in academic settings should be impeded by national origin or politics. Individuals should not be conflated with the countries or institutions they come from. Statements and actions to the contrary have no place in our computer science community. Anyone experiencing such behavior will find a committed ally in me.</p></blockquote>



<p>As for Al Aho?  I knew him fifteen years ago, when he became interested in quantum computing, in part due to his then-student <a href="https://www.microsoft.com/en-us/research/people/ksvore/">Krysta Svore</a> (who’s now the head of Microsoft’s quantum computing efforts).  Al struck me as not only a famous scientist but a gentleman who radiated kindness everywhere.  I’m not aware of any controversies he’s been involved in and never heard anyone say a bad word about him.</p>



<p>Anyway, this seems like a good occasion to recognize some foundational achievements in computer science, as well as the complex human beings who produce them!</p></div>
    </content>
    <updated>2021-04-09T18:15:33Z</updated>
    <published>2021-04-09T18:15:33Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-04-21T18:19:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/051</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/051" rel="alternate" type="text/html"/>
    <title>TR21-051 |  Binary Interactive Error Resilience Beyond $1/8$ (or why $(1/2)^3 &amp;gt; 1/8$) | 

	Raghuvansh Saxena, 

	Klim Efremenko, 

	Gillat Kol</title>
    <summary>Interactive error correcting codes are codes that encode a two party communication protocol to an error-resilient protocol that succeeds even if a constant fraction of the communicated symbols are adversarially corrupted, at the cost of increasing the communication by a constant factor. What is the largest fraction of corruptions that such codes can protect against? 

If the error-resilient protocol is allowed to communicate large (constant sized) symbols, Braverman and Rao  (STOC, 2011) show that the maximum rate of corruptions that can be tolerated is $1/4$. They also give a binary interactive error correcting protocol that only communicates bits and is resilient to $1/8$ fraction of errors, but leave the optimality of this scheme as an open problem.

We answer this question in the negative, breaking the $1/8$ barrier. Specifically, we give a binary interactive error correcting scheme that is resilient to $5/39 &gt; 1/8$ fraction of adversarial errors. Our scheme builds upon a novel construction of binary list-decodable interactive codes with small list size.</summary>
    <updated>2021-04-09T04:15:46Z</updated>
    <published>2021-04-09T04:15:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-23T20:38:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-709704913623244646</id>
    <link href="https://blog.computationalcomplexity.org/feeds/709704913623244646/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/709704913623244646" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/709704913623244646" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html" rel="alternate" type="text/html"/>
    <title>Quantum Stories</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Scott Aaronson <a href="https://www.scottaaronson.com/blog/?p=5387">wrote last month</a> about the hype over quantum computing. I'd thought I'd drop a few stories.</p><p>I was once asked to review a grant proposal (outside the US) that claimed it would find a quantum algorithm for NP-hard problems. I wrote a scathing review but the grant was funded because I failed to prove that it was impossible. I replied that they should fund my research to teleport people from Chicago to Paris because they couldn't prove I couldn't do it. I never got a response.</p><div>I was at an NSF sponsored meeting on quantum computing. I suggested, as a complexity theorist, that we need to explore the limits of quantum computing. A senior researcher said we shouldn't mention that in the report or it might hurt our chances of funding the field if they think quantum computing might not be a complete success.</div><p>I went to a Microsoft Faculty Research Summit which had a big focus on quantum computing. I complained of the quantum computing hype. My friends in the field denied the hype. Later at the summit a research head said that Microsoft will solve world hunger with quantum computing.</p><p>I was meeting with a congressional staffer who had worked on the National Quantum Initiative which coincidentally was being announced that day. I said something about high risk, high reward. He looked shocked--nobody had told him before that quantum computing is a speculative technology.</p><p>Quantum computing has generated a large number of beautiful and challenging scientific questions. Thinking about quantum has helped generate classical complexity and algorithmic results. But quantum computing having a real-world impact in the near or mid-term is unlikely. Most scientists I know working directly in quantum research are honest about the limitations and challenges in quantum computing. But somehow that message is not often getting to the next layers up, the policy makers, the research managers, the university administrators, the media and the venture capitalists. </p><p>But who knows, maybe some quantum heuristic that doesn't need much entanglement will change the world tomorrow. I can't prove it's impossible.</p></div>
    </content>
    <updated>2021-04-08T12:57:00Z</updated>
    <published>2021-04-08T12:57:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-04-23T07:12:33Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-21129445.post-1274107059735040105</id>
    <link href="http://mysliceofpizza.blogspot.com/feeds/1274107059735040105/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=21129445&amp;postID=1274107059735040105" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/21129445/posts/default/1274107059735040105" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/21129445/posts/default/1274107059735040105" rel="self" type="application/atom+xml"/>
    <link href="http://mysliceofpizza.blogspot.com/2021/04/postdoctoral-openings-at-amazon.html" rel="alternate" type="text/html"/>
    <title>Postdoctoral openings at Amazon</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://www.amazon.science/amazon-advertising-opens-applications-for-early-career-scientists" target="_blank">Amazon Advertising opens applications for early career scientists.</a> The new program, which offers full-time two-year positions, is aimed at recent PhD graduates who want to innovate, publish, and have their work impact millions of customers. The application deadline is May 14.</p></div>
    </content>
    <updated>2021-04-08T04:56:00Z</updated>
    <published>2021-04-08T04:56:00Z</published>
    <category scheme="http://www.blogger.com/atom/ns#" term="aggregator"/>
    <author>
      <name>metoo</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/07192519900962182610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-21129445</id>
      <category term="aggregator"/>
      <category term="Non-CS"/>
      <author>
        <name>metoo</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/07192519900962182610</uri>
      </author>
      <link href="http://mysliceofpizza.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/21129445/posts/default/-/aggregator" rel="self" type="application/atom+xml"/>
      <link href="http://mysliceofpizza.blogspot.com/search/label/aggregator" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/21129445/posts/default/-/aggregator/-/aggregator?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>books, stories, poems, algorithms, math and computer science. 

some art and anecdotes too.</subtitle>
      <title>my slice of pizza</title>
      <updated>2021-04-22T08:26:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2021/04/07/ripvanwinkle/</id>
    <link href="http://offconvex.github.io/2021/04/07/ripvanwinkle/" rel="alternate" type="text/html"/>
    <title>Rip van Winkle's Razor, a Simple New Estimate for Adaptive Data Analysis</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Can you trust a model whose designer had access to the test/holdout set?</em> This implicit question 
in <a href="https://science.sciencemag.org/content/349/6248/636.full">Dwork et al 2015</a> launched a new field, <em>adaptive data analysis</em>.
The question referred to the fact that in many scientific settings as well as modern machine learning (with its standardized datasets like CIFAR, 
ImageNet etc.) the model designer has full access to the holdout set and is free to ignore the</p>

<blockquote>
  <p>(Basic Dictum of Data Science) “Thou shalt not train on the test/holdout set.”</p>
</blockquote>

<p>Furthermore, even researchers who scrupulously follow the Basic Dictum may be unknowingly violating it when they take inspiration (and design choices) 
from published works by others who presumably published <em>only the best of the many models they evaluated on the test set.</em></p>

<p>Dwork et al. showed that if the test set has size $N$, and the designer is allowed to see the error of the first $i-1$ models on the test set before designing the $i$’th model, then a clever designer can  use so-called <a href="https://arxiv.org/pdf/1502.04585.pdf"><em>wacky boosting</em></a> (see this <a href="http://blog.mrtz.org/2015/03/09/competition.html">blog post</a>) to ensure the accuracy of the $t$’th model on the test set as high as $\Omega(\sqrt{t/N})$. In other words, the test set could become essentially useless once $t \gg N$, a 
condition that holds in ML, whereby in popular datasets (CIFAR10, CIFAR100, ImageNet etc.)  $N$ is no more than $100,000$ and the total number of models being trained 
world-wide is well in the millions if not higher (once you include hyperparameter searches).</p>

<blockquote>
  <p><strong>Meta-overfitting Error (MOE)</strong> of a model is the difference between its average error on the test data and its expected error on the full distribution.
(It is closely related to <a href="https://en.wikipedia.org/wiki/False_discovery_rate"><em>false discovery rate</em></a> in statistics.)</p>
</blockquote>

<p>This blog post concerns <a href="https://arxiv.org/pdf/2102.13189.pdf">our new paper</a>, which gives meaningful upper bounds on this sort of trouble for popular 
deep net architectures, whereas prior ideas from adaptive data analysis gave no nontrivial estimates. We call our estimate  <em>Rip van Winkle’s Razor</em> 
which combines references to <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam’s Razor</a> and the 
<a href="https://en.wikipedia.org/wiki/Rip_Van_Winkle">mythical person who fell asleep for  20 years</a>.</p>

<figure align="center">
<img alt="drawing" src="http://www.offconvex.org/assets/ripvanwinkle.jpg" width="50%"/>
   Rip Van Winkle wakes up from 20 years of sleep, clearly needing a Razor 
</figure>

<h2 id="adaptive-data-analysis-brief-tour">Adaptive Data Analysis: Brief tour</h2>

<p>It is well-known that for a model trained <strong>without</strong> ever querying the test set, MOE scales (with high probability over choice of the test set) as $1/\sqrt{N}$ where $N$ 
is the size of the test set.  Furthermore standard concentration bounds imply that even if we train $t$ models without ever referring to the test set (in other words, 
using proper data hygiene) then the maximum meta-overfitting error among the $t$ models scales whp as $O(\sqrt{\log(t)/ N})$. The trouble pinpointed by Dwork et al. 
can happen only if models are designed adaptively, with test error of the previous models shaping the design of the next model.</p>

<p>Adaptive Data Analysis has come up with many good practices for honest researchers to mitigate such issues. For instance, Dwork et al. showed that using 
Differential Privacy on labels while evaluating models can lower MOE. Or the <a href="https://arxiv.org/pdf/1502.04585.pdf">Ladder mechanism</a> helps in Kaggle-like 
settings where the test dataset resides on a server that can choose to answers only a  selected subset of queries, which essentially takes away the MOE issue.</p>

<p>For several good practices  matching lower bounds exist showing a way to construct cheating models with MOE matching the upper bound.</p>

<p>However such recommended best practices do not help with understanding the MOE in the performance numbers of a new model  since there is no guarantee that the 
inventors never tuned models using the test set, or didn’t get inspiration from existing models that may have been designed that way.  Thus statistically 
speaking the above results still give no reason to believe that a modern deep net such as ResNet152 has low MOE.</p>

<p><a href="http://proceedings.mlr.press/v97/recht19a/recht19a.pdf">Recht et al. 2019</a> summed up the MOE issue in a catchy title: <em>Do ImageNet Classifiers Generalize to ImageNet?</em>  They tried to answer their question experimentally by creating new test sets from scratch –we discuss their results later.</p>

<h2 id="moe-bounds-and-description-length">MOE bounds and description length</h2>

<p>The starting point of our work is the following classical concentration bounds:</p>

<blockquote>
  <p><strong>Folklore Theorem</strong> With high probability over the choice of a test set of size $N$, the MOE of <em>all</em> models with description length at most $k$ bits is  $O(\sqrt{k/N})$.</p>
</blockquote>

<p>At first sight this doesn’t seem to help us because one cannot imagine modern deep nets having a short description. The most obvious description involves reporting 
values of the net parameters, which requires millions or even hundreds of millions of bits, resulting in a vacuous upper bound  on MOE.</p>

<p>Another obvious description would be the computer program used to produce the model using the (publicly available) training and validation sets. However, these 
programs usually rely on imported libraries through layers of encapsulation and so the effective program size is pretty large as well.</p>

<h2 id="rip-van-winkles-razor">Rip van Winkle’s Razor</h2>
<p>Our new upper bound involves a more careful definition of <em>Description Length</em>: it is the smallest description that allows a referee  to reproduce a model of 
similar performance using the (universally available) training and validation datasets.</p>

<p>While this phrasing may appear reminiscent of the review process for conferences and journals, there is a subtle difference  with respect to what the referee 
can or cannot be assumed to know. (Clearly, assumptions about the referee can greatly affect description length —e.g,  a referee ignorant of even basic 
calculus might need a very long explanation!)</p>

<blockquote>
  <p><strong>Informed Referee:</strong> “Knows everything that was known to humanity (e.g., about deep learning, mathematics,optimization, statistics etc.) right up to the 
moment of creation of the Test set.”</p>
</blockquote>

<blockquote>
  <p><strong>Unbiased Referee:</strong> Knows nothing discovered since the Test set was created.</p>
</blockquote>

<p>Thus <em>Description Length</em> of a model is the number of bits in the shortest description that allows an informed but unbiased referee to reproduce the claimed result.</p>

<p>Note that informed referees let descriptions get shorter. Unbiased require longer descriptions that rule out any statistical “contamination” due to any interaction whatsoever with the test set. For example, momentum techniques in optimization were 
well-studied before the creation of ImageNet test set, so informed referees can be expected to understand a line like “SGD with momentum 0.9.” But a 
line like “Use Batch Normalization” cannot be understood by unbiased referees since conceivably this technique (invented after 2012) might have
 become popular precisely because it leads to better performance on the test set of ImageNet.</p>

<p>By now it should be clear why the estimate is named after  <a href="https://en.wikipedia.org/wiki/Rip_Van_Winkle">“Rip van Winkle”</a>: the referee can be thought 
of as an infinitely well-informed researcher who went into deep sleep at the moment of creation of the test set, and has just been woken up years later 
to start refereeing the  latest papers.  Real-life journal referees who luckily did not suffer this way should try to simulate the idealized Rip van Winkle 
in their heads while perusing the description submitted by the researcher.</p>

<p>To allow as short a  description as possible the researcher is allowed to compress the description of their new deep net non-destructively using any compression  that would make sense to Rip van Winkle (e.g., <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman Coding</a>). The description of the compression method itself 
is not counted towards the description length – provided the same method is used for all papers submitted to Rip van Winkle. To give an example, a 
technique appearing in a text known to Rip van Winkle could be succinctly referred to using the book’s ISBN number and page number.</p>

<h2 id="estimating-moe-of-resnet-152">Estimating MOE of ResNet-152</h2>
<p>As an illustration, here we provide a suitable description allowing  Rip van Winkle to reproduce a mainstream ImageNet model, ResNet-152, which achieves $4.49\%$ top-5 
test error.</p>

<p>The description consists of three types of expressions: English phrases, Math equations, and directed graphs. In the paper, we describe in detail how to encode 
each of them into binary strings and count their lengths.  The allowed vocabulary includes primitive concepts that were known before 2012, such 
as <em>CONV, MaxPool, ReLU, SGD</em> etc., as well as a graph-theoretic notation/shorthand  for describing net architecture. The newly introduced concepts 
including <em>Batch-Norm</em>, <em>Layer, Block</em> are defined precisely using Math, English, and other primitive concepts.</p>

<figure align="center">
<img alt="drawing" src="http://www.offconvex.org/assets/resnet_description.png" width="80%"/>
  <b>Description for reproducing ResNet-152</b>
</figure>

<p>According to our estimate, the length of the above description is $1032$ bits, which translates into a upper bound on meta-overfitting error of merely $5\%$! 
This suggests the real top-5 error of the model on full distribution is at most $9.49\%$. In the paper we also provide a $980$-bit long description for 
reproducing DenseNet-264, which leads to $5.06\%$ upper bound on its meta-overfitting error.</p>

<p>Note that the number $5.06$ suggests higher precision than actually given by the method, since it is possible to quibble about the coding assumptions 
that led to it.  Perhaps others might use a more classical coding mechanism and obtain an estimate of $6\%$ or $7\%$.</p>

<p>But the important point is that unlike  existing bounds in Adaptive Data Analysis, there is <strong>no</strong> dependence on $t$, the number of models that have been tested before, and the bound is non-vacuous.</p>

<h2 id="empirical-evidence-about-lack-of-meta-overfitting">Empirical evidence about lack of meta-overfitting</h2>

<p>Our estimates indicate that the issue of meta-overfitting on ImageNet for these mainstream models is mild. The reason is that despite the vast number
 of parameters and hyper-parameters in today’s deep nets, the <em>information content</em> of these models is not high given  knowledge circa 2012.</p>

<p>Recently Recht et al. <a href="https://arxiv.org/abs/1902.10811">tried to reach an empirical upper bound on MOE</a> for
ImageNet and <a href="https://arxiv.org/abs/1806.00451">CIFAR-10</a>. They created new tests sets by carefully replicating the methodology used for constructing the original ones. They found that error of famous published models of the past seven years is as much as 10-15% higher on the new test set as compared to the original.  On the face of it, this seemed to confirm a case of bad meta-overfitting. But they  also presented evidence  that the swing in test error was due to systemic effects during test set creation. For instance, a comparable swing happens also for models that predated the creation of ImageNet (and thus were not overfitted to the ImageNet test set). 
<a href="https://proceedings.neurips.cc/paper/2019/hash/ee39e503b6bedf0c98c388b7e8589aca-Abstract.html">A followup study</a> of a hundred Kaggle competitions used fresh, 
identically distributed test sets that were available from the official competition organizers. The authors concluded that MOE does not appear to be significant in modern ML.</p>

<h2 id="conclusions">Conclusions</h2>
<p>To us the  disquieting takeaway from Recht et al.’s results was that  estimating MOE by creating a new test set is rife with systematic bias at best, and perhaps impossible, especially in datasets concerning rare or one-time phenomena (e.g., stock prices).  Thus their work still left a pressing need for effective upper bounds on  meta-overfitting error. Our Rip van Winkle’s Razor is elementary, and easily deployable by the average researcher. We hope it becomes part of the standard toolbox in Adaptive Data Analysis.</p></div>
    </summary>
    <updated>2021-04-07T21:00:00Z</updated>
    <published>2021-04-07T21:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2021-04-22T22:55:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=1613</id>
    <link href="https://toc4fairness.org/ensuring-equity-in-high-stakes-online-advertising/" rel="alternate" type="text/html"/>
    <title>Ensuring equity in high-stakes online advertising</title>
    <summary>In this blog post, I outline how existing advertising platforms do not prevent high-stakes ads from reaching different demographics at different rates. The post then describes how pushing this responsibility ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In this blog post, I outline how existing advertising platforms do not prevent high-stakes ads from reaching different demographics at different rates. The post then describes how pushing this responsibility down to advertisers rather than addressing it at the platform leaves manipulating a complex system to those least aware of the system’s inner workings. She then proposes a simpler, more unified solution to this problem: advertising slots should be either targetable or untargetable, and high-stakes ads should be in the untargeted segment. Finally, the post concludes with a discussion of how this segmentation need not cost these systems substantial revenue if reserve prices are used appropriately.</p>



<p/>



<p><a href="https://jamiemmt-cs.medium.com/ensuring-equity-in-online-advertising-for-employment-housing-and-credit-82931668c420">https://jamiemmt-cs.medium.com/ensuring-equity-in-online-advertising-for-employment-housing-and-credit-82931668c420</a></p></div>
    </content>
    <updated>2021-04-07T20:47:28Z</updated>
    <published>2021-04-07T20:47:28Z</published>
    <category term="Blog"/>
    <author>
      <name>jamiemorgenstern</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i1.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2021-04-23T20:44:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=547</id>
    <link href="https://tcsplus.wordpress.com/2021/04/06/tcs-talk-wednesday-april-14-andrea-lincoln-uc-berkeley/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, April 14 — Andrea Lincoln, UC Berkeley</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, April 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Andrea Lincoln from UC Berkeley will speak about “New Techniques for Proving Fine-Grained Average-Case Hardness” (abstract below). You can reserve a spot as an individual or a group to […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: left;">The next TCS+ talk will take place this coming Wednesday, April 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://sites.google.com/site/andrealiresume/home"><strong>Andrea Lincoln</strong></a> from UC Berkeley will speak about “<em>New Techniques for Proving Fine-Grained Average-Case Hardness</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk)</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: In this talk I will cover a new technique for worst-case to average-case reductions. There are two primary concepts introduced in this talk: “factored” problems and a framework for worst-case to average-case fine-grained (WCtoACFG) self reductions.</p>
<p>We will define new versions of OV, kSUM and zero-k-clique that are both worst-case and average-case fine-grained hard assuming the core hypotheses of fine-grained complexity. We then use these as a basis for fine-grained hardness and average-case hardness of other problems. Our hard factored problems are also simple enough that we can reduce them to many other problems, e.g. to edit distance, k-LCS and versions of Max-Flow. We further consider counting variants of the factored problems and give WCtoACFG reductions for them for a natural distribution.</p>
<p>To show hardness for these factored problems we formalize the framework of [Boix-Adsera et al. 2019] that was used to give a WCtoACFG reduction for counting k-cliques. We define an explicit property of problems such that if a problem has that property one can use the framework on the problem to get a WCtoACFG self reduction. In total these factored problems and the framework together give tight fine-grained average-case hardness for various problems including the counting variant of regular expression matching.</p>
<p>Based on joint work with Mina Dalirrooyfard and Virginia Vassilevska Williams.</p></blockquote></div>
    </content>
    <updated>2021-04-06T20:51:50Z</updated>
    <published>2021-04-06T20:51:50Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-04-23T20:42:33Z</updated>
    </source>
  </entry>
</feed>
