<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-06-02T07:22:04Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00917</id>
    <link href="http://arxiv.org/abs/2006.00917" rel="alternate" type="text/html"/>
    <title>Evaluation of the general applicability of Dragoon for the k-center problem</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uhlig:Tobias.html">Tobias Uhlig</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hillmann:Peter.html">Peter Hillmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rose:Oliver.html">Oliver Rose</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00917">PDF</a><br/><b>Abstract: </b>The k-center problem is a fundamental problem we often face when considering
complex service systems. Typical challenges include the placement of warehouses
in logistics or positioning of servers for content delivery networks. We
previously have proposed Dragoon as an effective algorithm to approach the
k-center problem. This paper evaluates Dragoon with a focus on potential worst
case behavior in comparison to other techniques. We use an evolutionary
algorithm to generate instances of the k-center problem that are especially
challenging for Dragoon. Ultimately, our experiments confirm the previous good
results of Dragoon, however, we also can reliably find scenarios where it is
clearly outperformed by other approaches.
</p></div>
    </summary>
    <updated>2020-06-02T01:20:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00808</id>
    <link href="http://arxiv.org/abs/2006.00808" rel="alternate" type="text/html"/>
    <title>Note to "An efficient Data Structure for Lattice Operation"</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Talamo:Maurizio.html">Maurizio Talamo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vocca:Paola.html">Paola Vocca</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00808">PDF</a><br/><b>Abstract: </b>This note is to publicly answer to a paper recently accepted to SWAT 2020
\cite{Munro2019SpaceEfficientDS} that claims to have solved an error in our
papers \cite{paolaSiam,paolaTCS} by proposing a solution with worst
performances. In the following section we describe in detail sections 4.2
(Cluster collection) and 5 (Data Structure and Space Complexity) in
\cite{paolaSiam} to show the implementation of the data structure.
</p></div>
    </summary>
    <updated>2020-06-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00743</id>
    <link href="http://arxiv.org/abs/2006.00743" rel="alternate" type="text/html"/>
    <title>Provable guarantees for decision tree induction: the agnostic setting</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanc:Guy.html">Guy Blanc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lange:Jane.html">Jane Lange</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Li=Yang.html">Li-Yang Tan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00743">PDF</a><br/><b>Abstract: </b>We give strengthened provable guarantees on the performance of widely
employed and empirically successful {\sl top-down decision tree learning
heuristics}. While prior works have focused on the realizable setting, we
consider the more realistic and challenging {\sl agnostic} setting. We show
that for all monotone functions~$f$ and parameters $s\in \mathbb{N}$, these
heuristics construct a decision tree of size $s^{\tilde{O}((\log
s)/\varepsilon^2)}$ that achieves error $\le \mathsf{opt}_s + \varepsilon$,
where $\mathsf{opt}_s$ denotes the error of the optimal size-$s$ decision tree
for $f$. Previously, such a guarantee was not known to be achievable by any
algorithm, even one that is not based on top-down heuristics. We complement our
algorithmic guarantee with a near-matching $s^{\tilde{\Omega}(\log s)}$ lower
bound.
</p></div>
    </summary>
    <updated>2020-06-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00625</id>
    <link href="http://arxiv.org/abs/2006.00625" rel="alternate" type="text/html"/>
    <title>Neural Networks with Small Weights and Depth-Separation Barriers</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vardi:Gal.html">Gal Vardi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shamir:Ohad.html">Ohad Shamir</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00625">PDF</a><br/><b>Abstract: </b>In studying the expressiveness of neural networks, an important question is
whether there are functions which can only be approximated by sufficiently deep
networks, assuming their size is bounded. However, for constant depths,
existing results are limited to depths $2$ and $3$, and achieving results for
higher depths has been an important open question. In this paper, we focus on
feedforward ReLU networks, and prove fundamental barriers to proving such
results beyond depths $4$, by reduction to open problems and natural-proof
barriers in circuit complexity. To show this, we study a seemingly unrelated
problem of independent interest: Namely, whether there are polynomially-bounded
functions which require super-polynomial weights in order to approximate with
constant-depth neural networks. We provide a negative and constructive answer
to that question, by showing that if a function can be approximated by a
polynomially-sized, constant depth $k$ network with arbitrarily large weights,
it can also be approximated by a polynomially-sized, depth $3k+3$ network,
whose weights are polynomially bounded.
</p></div>
    </summary>
    <updated>2020-06-02T01:22:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00605</id>
    <link href="http://arxiv.org/abs/2006.00605" rel="alternate" type="text/html"/>
    <title>A Fast Algorithm for Online k-servers Problem on Trees</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khadiev:Kamil.html">Kamil Khadiev</a>, Maxim Yagafarov <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00605">PDF</a><br/><b>Abstract: </b>We consider online algorithms for the $k$-servers problem on trees. There is
an $k$-competitive algorithm for this problem, and it is the best competitive
ratio. M. Chrobak and L. Larmore suggested it. At the same time, the existing
implementation has $O(n)$ time complexity, where $n$ is a number of nodes in a
tree. We suggest a new time-efficient implementation of the algorithm. It has
$O(n)$ time complexity for preprocessing and $O\left(k(\log n)^2\right)$ for
processing a query.
</p></div>
    </summary>
    <updated>2020-06-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00602</id>
    <link href="http://arxiv.org/abs/2006.00602" rel="alternate" type="text/html"/>
    <title>Estimating Principal Components under Adversarial Perturbations</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Awasthi:Pranjal.html">Pranjal Awasthi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Xue.html">Xue Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vijayaraghavan:Aravindan.html">Aravindan Vijayaraghavan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00602">PDF</a><br/><b>Abstract: </b>Robustness is a key requirement for widespread deployment of machine learning
algorithms, and has received much attention in both statistics and computer
science. We study a natural model of robustness for high-dimensional
statistical estimation problems that we call the adversarial perturbation
model. An adversary can perturb every sample arbitrarily up to a specified
magnitude $\delta$ measured in some $\ell_q$ norm, say $\ell_\infty$. Our model
is motivated by emerging paradigms such as low precision machine learning and
adversarial training.
</p>
<p>We study the classical problem of estimating the top-$r$ principal subspace
of the Gaussian covariance matrix in high dimensions, under the adversarial
perturbation model. We design a computationally efficient algorithm that given
corrupted data, recovers an estimate of the top-$r$ principal subspace with
error that depends on a robustness parameter $\kappa$ that we identify. This
parameter corresponds to the $q \to 2$ operator norm of the projector onto the
principal subspace, and generalizes well-studied analytic notions of sparsity.
Additionally, in the absence of corruptions, our algorithmic guarantees recover
existing bounds for problems such as sparse PCA and its higher rank analogs. We
also prove that the above dependence on the parameter $\kappa$ is almost
optimal asymptotically, not just in a minimax sense, but remarkably for every
instance of the problem. This instance-optimal guarantee shows that the $q \to
2$ operator norm of the subspace essentially characterizes the estimation error
under adversarial perturbations.
</p></div>
    </summary>
    <updated>2020-06-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00571</id>
    <link href="http://arxiv.org/abs/2006.00571" rel="alternate" type="text/html"/>
    <title>On Dynamic Parameterized $k$-Path</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Jiehua.html">Jiehua Chen</a>, Wojciech Czerwiński, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Disser:Yann.html">Yann Disser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldmann:Andreas_Emil.html">Andreas Emil Feldmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hermelin:Danny.html">Danny Hermelin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nadara:Wojciech.html">Wojciech Nadara</a>, Michał Pilipczuk, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pilipczuk:Marcin.html">Marcin Pilipczuk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sorge:Manuel.html">Manuel Sorge</a>, Bartłomiej Wróblewski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zych=Pawlewicz:Anna.html">Anna Zych-Pawlewicz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00571">PDF</a><br/><b>Abstract: </b>We present a data structure that for a dynamic graph $G$, which is updated by
edge insertions and removals, maintains the answer to the query whether $G$
contains a simple path on $k$ vertices with amortized update time $2^{O(k^2)}$,
assuming access to a dictionary on the edges of $G$. Underlying this result
lies a data structure that maintains an optimum-depth elimination forest in a
dynamic graph of treedepth at most $d$ with update time $2^{O(d^2)}$. This
improves a result of Dvo\v{r}\'ak et al. [ESA 2014], who for the same problem
achieved update time $f(d)$ for a non-elementary function $f$.
</p></div>
    </summary>
    <updated>2020-06-02T01:26:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00386</id>
    <link href="http://arxiv.org/abs/2006.00386" rel="alternate" type="text/html"/>
    <title>Scheduling in the Random-Order Model</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Albers:Susanne.html">Susanne Albers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Janke:Maximilian.html">Maximilian Janke</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00386">PDF</a><br/><b>Abstract: </b>Makespan minimization on identical machines is a fundamental problem in
online scheduling. The goal is to assign a sequence of jobs to $m$ identical
parallel machines so as to minimize the maximum completion time of any job.
Already in the 1960s, Graham showed that Greedy is $(2-1/m)$-competitive. The
best deterministic online algorithm currently known achieves a competitive
ratio of 1.9201. No deterministic online strategy can obtain a competitiveness
smaller than 1.88.
</p>
<p>In this paper, we study online makespan minimization in the popular
random-order model, where the jobs of a given input arrive as a random
permutation. It is known that Greedy does not attain a competitive factor
asymptotically smaller than 2 in this setting. We present the first improved
performance guarantees. Specifically, we develop a deterministic online
algorithm that achieves a competitive ratio of 1.8478. The result relies on a
new analysis approach. We identify a set of properties that a random
permutation of the input jobs satisfies with high probability. Then we conduct
a worst-case analysis of our algorithm, for the respective class of
permutations. The analysis implies that the stated competitiveness holds not
only in expectation but with high probability. Moreover, it provides
mathematical evidence that job sequences leading to higher performance ratios
are extremely rare, pathological inputs. We complement the results by lower
bounds, for the random-order model. We show that no deterministic online
algorithm can achieve a competitive ratio smaller than 4/3. Moreover, no
deterministic online algorithm can attain a competitiveness smaller than 3/2
with high probability.
</p></div>
    </summary>
    <updated>2020-06-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00376</id>
    <link href="http://arxiv.org/abs/2006.00376" rel="alternate" type="text/html"/>
    <title>Lower Bounds for Caching with Delayed Hits</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manohar:Peter.html">Peter Manohar</a>, Jalani Williams <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00376">PDF</a><br/><b>Abstract: </b>Caches are a fundamental component of latency-sensitive computer systems.
Recent work of [ASWB20] has initiated the study of delayed hits: a phenomenon
in caches that occurs when the latency between the cache and backing store is
much larger than the time between new requests. We present two results for the
delayed hits caching model.
</p>
<p>(1) Competitive ratio lower bound. We prove that the competitive ratio of the
algorithm in [ASWB20], and more generally of any deterministic online algorithm
for delayed hits, is at least Omega(kZ), where k is the cache size and Z is the
delay parameter.
</p>
<p>(2) Antimonotonicity of the delayed hits latency. Antimonotonicity is a
naturally desirable property of cache latency: having a cache hit instead of a
cache miss should result in lower overall latency. We prove that the latency of
the delayed hits model is not antimonotone by exhibiting a scenario where
having a cache hit instead of a miss results in an increase in overall latency.
We additionally present a modification of the delayed hits model that makes the
latency antimonotone.
</p></div>
    </summary>
    <updated>2020-06-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00354</id>
    <link href="http://arxiv.org/abs/2006.00354" rel="alternate" type="text/html"/>
    <title>Grover Mixers for QAOA: Shifting Complexity from Mixer Design to State Preparation</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=auml=rtschi:Andreas.html">Andreas Bärtschi</a>, Stephan Eidenbenz <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00354">PDF</a><br/><b>Abstract: </b>We propose GM-QAOA, a variation of the Quantum Alternating Operator Ansatz
(QAOA) that uses Grover-like selective phase shift mixing operators. GM-QAOA
works on any NP optimization problem for which it is possible to efficiently
prepare an equal superposition of all feasible solutions; it is designed to
perform particularly well for constraint optimization problems, where not all
possible variable assignments are feasible solutions. GM-QAOA has the following
features: (i) It is not susceptible to Hamiltonian Simulation error (such as
Trotterization errors) as its operators can be implemented exactly using
standard gate sets and (ii) Solutions with the same objective value are always
sampled with the same amplitude.
</p>
<p>We illustrate the potential of GM-QAOA on several optimization problem
classes: for permutation-based optimization problems such as the Traveling
Salesperson Problem, we present an efficient algorithm to prepare a
superposition of all possible permutations of $n$ numbers, defined on O(n^2)
qubits; for the hard constraint $k$-Vertex-Cover problem, and for an
application to Discrete Portfolio Rebalancing, we show that GM-QAOA outperforms
existing QAOA approaches.
</p></div>
    </summary>
    <updated>2020-06-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00322</id>
    <link href="http://arxiv.org/abs/2006.00322" rel="alternate" type="text/html"/>
    <title>Bi-Criteria Multiple Knapsack Problem with Grouped Items</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Francisco Castillo-Zunino, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Keskinocak:Pinar.html">Pinar Keskinocak</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00322">PDF</a><br/><b>Abstract: </b>The multiple knapsack problem with grouped items aims to maximize rewards by
assigning groups of items among multiple knapsacks, considering knapsack
capacities. Either all items in a group are assigned or none at all. We propose
algorithms which guarantee that rewards are not less than the optimal solution,
with a bound on exceeded knapsack capacities. To obtain capacity-feasible
solutions, we propose a binary-search heuristic combined with these algorithms.
We test the performance of the algorithms and heuristics in an extensive set of
experiments on randomly generated instances and show they are efficient and
effective, i.e., they run reasonably fast and generate good quality solutions.
</p></div>
    </summary>
    <updated>2020-06-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00285</id>
    <link href="http://arxiv.org/abs/2006.00285" rel="alternate" type="text/html"/>
    <title>Motivating Good Practices for the Creation of Contiguous Area Cartograms</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Shi Tingsheng, Ian K. Duncan, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chang:Yen=Ning.html">Yen-Ning Chang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gastner:Michael_T=.html">Michael T. Gastner</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00285">PDF</a><br/><b>Abstract: </b>Cartograms are maps in which the areas of regions (e.g., countries or
provinces) are proportional to a thematic mapping variable (e.g., population or
gross domestic product). A cartogram is called contiguous if it keeps
geographically adjacent regions connected. Over the past few years, several web
tools have been developed for the creation of contiguous cartograms. However,
most of these tools do not advise how to use cartograms correctly. To mitigate
these shortcomings, we attempt to establish good practices through our recently
developed web application go-cart.io: (1) use cartograms to show numeric data
that add up to an interpretable total, (2) present a cartogram alongside a
conventional map that uses the same color scheme, (3) indicate whether the data
for a region are missing, (4) include a legend so that readers can infer the
magnitude of the mapping variable, (5) if a cartogram is presented
electronically, assist readers with interactive graphics.
</p></div>
    </summary>
    <updated>2020-06-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00216</id>
    <link href="http://arxiv.org/abs/2006.00216" rel="alternate" type="text/html"/>
    <title>Longest Square Subsequence Problem Revisited</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inoue:Takafumi.html">Takafumi Inoue</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inenaga:Shunsuke.html">Shunsuke Inenaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00216">PDF</a><br/><b>Abstract: </b>The longest square subsequence (LSS) problem consists of computing a longest
subsequence of a given string $S$ that is a square, i.e., a longest subsequence
of form $XX$ appearing in $S$. It is known that an LSS of a string $S$ of
length $n$ can be computed using $O(n^2)$ time [Kosowski 2004], or with
(model-dependent) polylogarithmic speed-ups using $O(n^2 (\log \log n)^2 /
\log^2 n)$ time [Tiskin 2013]. We present the first algorithm for LSS whose
running time depends on other parameters, i.e., we show that an LSS of $S$ can
be computed in $O(r \min\{n, M\}\log \frac{n}{r} + M \log n)$ time with $O(M)$
space, where $r$ is the length of an LSS of $S$ and $M$ is the number of
matching points on $S$.
</p></div>
    </summary>
    <updated>2020-06-02T01:22:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00187</id>
    <link href="http://arxiv.org/abs/2006.00187" rel="alternate" type="text/html"/>
    <title>An Efficient Planar Bundle Adjustment Algorithm</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Lipu.html">Lipu Zhou</a>, Daniel Koppel, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Ju:Hui.html">Hui Ju</a>, Frank Steinbruecker, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaess:Michael.html">Michael Kaess</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00187">PDF</a><br/><b>Abstract: </b>This paper presents an efficient algorithm for the least-squares problem
using the point-to-plane cost, which aims to jointly optimize depth sensor
poses and plane parameters for 3D reconstruction. We call this least-squares
problem \textbf{Planar Bundle Adjustment} (PBA), due to the similarity between
this problem and the original Bundle Adjustment (BA) in visual reconstruction.
As planes ubiquitously exist in the man-made environment, they are generally
used as landmarks in SLAM algorithms for various depth sensors. PBA is
important to reduce drift and improve the quality of the map. However, directly
adopting the well-established BA framework in visual reconstruction will result
in a very inefficient solution for PBA. This is because a 3D point only has one
observation at a camera pose. In contrast, a depth sensor can record hundreds
of points in a plane at a time, which results in a very large nonlinear
least-squares problem even for a small-scale space. Fortunately, we find that
there exist a special structure of the PBA problem. We introduce a reduced
Jacobian matrix and a reduced residual vector, and prove that they can replace
the original Jacobian matrix and residual vector in the generally adopted
Levenberg-Marquardt (LM) algorithm. This significantly reduces the
computational cost. Besides, when planes are combined with other features for
3D reconstruction, the reduced Jacobian matrix and residual vector can also
replace the corresponding parts derived from planes. Our experimental results
verify that our algorithm can significantly reduce the computational time
compared to the solution using the traditional BA framework. Besides, our
algorithm is faster, more accuracy, and more robust to initialization errors
compared to the start-of-the-art solution using the plane-to-plane cost
</p></div>
    </summary>
    <updated>2020-06-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.00061</id>
    <link href="http://arxiv.org/abs/2006.00061" rel="alternate" type="text/html"/>
    <title>Complexity of Maximum Cut on Interval Graphs</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adhikary:Ranendu.html">Ranendu Adhikary</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bose:Kaustav.html">Kaustav Bose</a>, Satwik Mukherjee, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roy:Bodhayan.html">Bodhayan Roy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.00061">PDF</a><br/><b>Abstract: </b>In this paper, we show that the Max Cut problem is NP-complete on interval
graphs.
</p></div>
    </summary>
    <updated>2020-06-02T01:22:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12937</id>
    <link href="http://arxiv.org/abs/1910.12937" rel="alternate" type="text/html"/>
    <title>Targeted sampling from massive Blockmodel graphs with personalized PageRank</title>
    <feedworld_mtime>1591056000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Fan.html">Fan Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Yini.html">Yini Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohe:Karl.html">Karl Rohe</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12937">PDF</a><br/><b>Abstract: </b>This paper provides statistical theory and intuition for Personalized
PageRank (PPR), a popular technique that samples a small community from a
massive network. We study a setting where the entire network is expensive to
thoroughly obtain or maintain, but we can start from a seed node of interest
and "crawl" the network to find other nodes through their connections. By
crawling the graph in a designed way, the PPR vector can be approximated
without querying the entire massive graph, making it an alternative to snowball
sampling. Using the Degree-Corrected Stochastic Blockmodel, we study whether
the PPR vector can select nodes that belong to the same block as the seed node.
We provide a simple and interpretable form for the PPR vector, highlighting its
biases towards high degree nodes outside of the target block. We examine a
simple adjustment based on node degrees and establish consistency results for
PPR clustering that allows for directed graphs. We illustrate the method with
the Twitter friendship graph and find that (i) the adjusted and unadjusted PPR
techniques are complementary approaches, where the adjustment makes the results
particularly localized around the seed node and (ii) the bias adjustment
greatly benefits from degree regularization.
</p></div>
    </summary>
    <updated>2020-06-02T01:21:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4830</id>
    <link href="https://www.scottaaronson.com/blog/?p=4830" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4830#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4830" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The US might die, but P and PSPACE are forever</title>
    <summary xml:lang="en-US">Today, I interrupt the news of the rapid disintegration of the United States of America, on every possible front at once (medical, economic, social…), to bring you something far more important: a long-planned two-hour podcast, where theoretical physicist and longtime friend-of-the-blog Sean Carroll interviews yours truly about complexity theory! Here’s Sean’s description of this historic […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Today, I interrupt the news of the rapid disintegration of the United States of America, on every possible front at once (medical, economic, social…), to bring you something far more important: a long-planned <a href="https://www.preposterousuniverse.com/podcast/2020/06/01/99-scott-aaronson-on-complexity-computation-and-quantum-gravity/">two-hour podcast</a>, where theoretical physicist and longtime friend-of-the-blog <a href="https://www.preposterousuniverse.com/">Sean Carroll</a> interviews yours truly about complexity theory!  Here’s Sean’s description of this historic event:</p>



<blockquote class="wp-block-quote"><p>There are some problems for which it’s very hard to find the answer, but very easy to check the answer if someone gives it to you. At least, we think there are such problems; whether or not they really exist is the famous <a href="https://en.wikipedia.org/wiki/P_versus_NP_problem">P vs NP problem</a>, and actually proving it will win you <a href="https://en.wikipedia.org/wiki/Millennium_Prize_Problems">a million dollars</a>. This kind of question falls under the rubric of “computational complexity theory,” which formalizes how hard it is to computationally attack a well-posed problem. Scott Aaronson is one of the world’s leading thinkers in computational complexity, especially the wrinkles that enter once we consider quantum computers as well as classical ones. We talk about how we quantify complexity, and how that relates to ideas as disparate as creativity, knowledge vs. proof, and what all this has to do with black holes and quantum gravity.</p></blockquote>



<p>So, OK, I guess I should also comment on the national disintegration thing.  As someone who was once himself the victim of a crazy police overreaction (albeit, trivial compared to what African-Americans regularly deal with), I was moved by the scenes of police chiefs in several American towns taking off their helmets and joining protesters to cheers.  Not only is that a deeply moral thing to do, but it serves a practical purpose of quickly defusing the protests.  Right now, of course, is an <em>even worse time than usual</em> for chaos in the streets, with a lethal virus still spreading that doesn’t care whether people are congregating for good or for ill.  If rational discussion of policy still matters, I support the current push to end the “qualified immunity” doctrine, end the provision of military training and equipment to police, and generally spur the nation’s police to rein in their psychopath minority.</p></div>
    </content>
    <updated>2020-06-01T19:00:33Z</updated>
    <published>2020-06-01T19:00:33Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-06-01T19:00:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7741</id>
    <link href="https://windowsontheory.org/2020/06/01/theory-of-machine-learning-summer-seminar/" rel="alternate" type="text/html"/>
    <title>Theory of Machine Learning summer seminar</title>
    <summary>[Note: While I and many others are fortunate to be able to go on with our work, deadlines, and (as mentioned in this post) seminars, this is not the case for many in the U.S. following yet another demonstration that black lives don’t matter as much as they should in this country. I would like […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Note: While I and many others are fortunate to be able to go on with our work, deadlines, and (as mentioned in this post) seminars, this is not the case for many in the U.S. following yet another demonstration that black lives don’t matter as much as they should in this country. I would like to relay <a href="https://twitter.com/red_abebe/status/1267145593991946245">Rediet Abebe’s call</a> to support local organizations. As Rediet says “These problems have been and will be here for a very long time. We’re not solving racism this month.”. –Boaz]</em></p>



<p>For the last year, I have been co organizing a <a href="https://mltheory.org/#talks">theory of machine learning seminar</a> at Harvard. Following the format of our prior Harvard/MSR/MIT theory reading group, these have been extended blackboard talks with plenty of audience interaction.</p>



<p>Following COVID-19, the last three talks in the semester (by Moritz Hardt,  Zico Kolter, and Anima Anandkumar)  were given virtually. Frankly, I was at first unsure whether these seminars can work in the virtual format but was pleasantly surprised. Talks have been very interactive, with plenty of audience participation in the chat channel. In fact, the virtual format has some <em>advantages </em>over physical talks. Sometimes a question will be asked and answered by a co-author over chat, without the speaker needing to interrupt the talk.</p>



<p>Since the seminars were so successful, we decided to continue holding them over the summer. We have an exciting line up of confirmed speakers, and more will come soon. See <a href="https://mltheory.org/#talks">our webpage</a> for more details, which also contains a google calendar and a mailing list you can sign up for to get the Zoom link.</p>



<p>Confirmed speakers so far include:</p>



<ul><li><a href="http://www.sohldickstein.com/">Jascha Sohl-Dickstein</a> – June 11</li><li><a href="https://www.neyshabur.net/">Behnam Neyshabur</a> – June 18</li><li><a href="https://users.ece.utexas.edu/~dimakis/">Alex Dimakis</a> – July 9</li><li><a href="https://www.cohennadav.com/">Nadav Cohen</a> – August 6</li><li><a href="https://maithraraghu.com/">Maithra Raghu</a> – date tbd</li><li><a href="https://research.google/people/HanieSedghi/">Hanie Sedghi</a> – date tbd</li></ul>



<p>More should be confirmed soon – join our <a href="https://groups.google.com/a/seas.harvard.edu/forum/#!forum/ml-theory-seminar/join">mailing list</a> to get updates.</p></div>
    </content>
    <updated>2020-06-01T16:17:12Z</updated>
    <published>2020-06-01T16:17:12Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-06-02T07:21:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=2566</id>
    <link href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/" rel="alternate" type="text/html"/>
    <link href="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m100.mp4" length="434960" rel="enclosure" type="video/mp4"/>
    <title>Gradient descent for wide two-layer neural networks – I : Global convergence</title>
    <summary>Supervised learning methods come in a variety of flavors. While local averaging techniques such as nearest-neighbors or decision trees are often used with low-dimensional inputs where they can adapt to any potentially non-linear relationship between inputs and outputs, methods based on empirical risk minimization are the most commonly used in high-dimensional settings. Their principle is...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Supervised learning methods come in a variety of flavors. While local averaging techniques such as <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">nearest-neighbors</a> or <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision trees</a> are often used with low-dimensional inputs where they can adapt to any potentially non-linear relationship between inputs and outputs, methods based on empirical risk minimization are the most commonly used in high-dimensional settings. Their principle is simple: optimize the (potentially regularized) risk on training data over prediction functions in a pre-defined set of functions.</p>



<p class="justify-text">When the set of a functions is a convex subset of a vector space with a finite-dimensional representation, with standard assumptions, the corresponding optimization problem is convex. This has the benefits of allowing a thorough theoretical understanding of the computational and statistical properties of learning methods, which often come with strong theoretical guarantees, in terms of running time [<a href="https://arxiv.org/pdf/1405.4980">1</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/16M1080173">2</a>, <a href="http://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">3</a>] or prediction performance on unseen data [<a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">4</a>, <a href="http://static.stevereads.com/papers_to_read/all_of_statistics.pdf">5</a>]. In particular, the linear parameterization can be done either explicitly by building a typically large finite set of features, or implicitly through the use of kernel methods and then a series of dedicated algorithms and theories can be leveraged for efficient non-linear predictions [6, <a href="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">7</a>, <a href="http://papers.nips.cc/paper/6978-falkon-an-optimal-large-scale-kernel-method.pdf">8</a>].</p>



<p class="justify-text">However, linearly-parameterized sets of functions do not include neural networks, which lead to state-of-the-art performance in most learning tasks in computer vision, natural language processing, speech processing, in particular through the use of deep and convolutional neural networks [<a href="https://www.deeplearningbook.org/">9</a>].</p>



<h2>Two-layer neural networks with “relu” activations</h2>



<p class="justify-text">The goal of this blog post is to provide some understanding of why supervised machine learning work for the simplest form of such models: $$ h(x) = \frac{1}{m} \sum_{i=1}^m a_i ( b_i^\top x)_+ = \frac{1}{m} \sum_{i=1}^m a_i \max\{ b_i^\top x,0\},$$ where the input \(x\) is a vector in \(\mathbb{R}^d\), and \(m\) is the number of hidden neurons. The weights \(a_i \in \mathbb{R}\), \(i=1,\dots,n\), are the <em>output weights</em>, while the weights \(b_i \in \mathbb{R}^d\), \(i=1,\dots,n\), are the <em>input weights</em>. The rectified linear unit (“relu”) [<a href="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">10</a>] activation is used, and our results will depend heavily on its positive homogeneity (that is, for \(\lambda &gt; 0\), \((\lambda u)_+ = \lambda u_+\)).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3829" height="292" src="https://francisbach.com/wp-content/uploads/2020/05/nn_single_blog.png" width="407"/>Two-layer neural network in dimension \(d = 6\) with \(m=4\) hidden neurons, and a single output.</figure></div>



<p class="justify-text">Note that this is an idealized and much simplified set-up for deep learning, as there is a single hidden-layer, no convolutions, no pooling, etc. As I will show below, this simple set-up is already complex to understand, and I believe it captures some of the core difficulties associated with non-convexity.</p>



<p class="justify-text">The first question that one may come to after decades of research in learning theory is: <em>why is it so hard to analyze?</em>  </p>



<p class="justify-text">There are at least two major difficulties:</p>



<ul class="justify-text"><li><strong>Non-linearity</strong>: the dependence on the input weights \(b_i\)’s is non-linear because of the activation function, typically leading to non-convex optimization problems.</li><li><strong>Overparameterization</strong>: The number \(m\) of hidden neurons is very large (often so large that the number of parameters \(m(d+1)\) exceeds the number of observations), which is hard in terms of optimization and potentially generalization to unseen data. </li></ul>



<p class="justify-text">In this blog post, we will leverage the overparameterization and take \(m\) tending to infinity (without any dependence on the number of observations), which will allow us to derive theoretical results. We will leverage two key properties of the problem:</p>



<ul class="justify-text"><li><strong>Separability</strong> of the model in \(w_i = (a_i,b_i)\), that is, the prediction function \(h(x)\) is the sum of terms which are independently parameterized, as \(h = \frac{1}{m} \sum_{i=1}^m \Phi(w_i)\), where \(\Phi: \mathbb{R}^p \to \mathcal{F}\), with \(\mathcal{F}\) a space of functions. In our situation, \(p = d+1\) and: $$ \Phi(w)(x) = a (b^\top x)_+. $$ In other words,  there is no parameter sharing among hidden neurons. Unfortunately, this does not generalize to more than a single hidden layer.</li><li><strong>Homogeneity</strong>: the relu activation is positively homogeneous so that as as function of \(w = (a,b) \in \mathbb{R} \times \mathbb{R}^d\), \(\Phi(w)(x) = a (b^\top x)_+\) is positively 2-homogeneous, that is, \(\Phi(\lambda w) = \lambda^2 \Phi(w)\) for \(\lambda &gt; 0\).</li></ul>



<p class="justify-text">In this sequence of two blog posts, following a recent trend in optimization and machine learning theory [<a href="http://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">11</a>, <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">12</a>], optimization and statistics cannot be separated and need to be tackled together. I will focus on gradient flows on empirical or expected risks.</p>



<p class="justify-text">In this blog post, I will cover optimization and how over-parameterization leads to global convergence for 2-homogeneous models, a recent result obtained two years ago with <a href="https://lchizat.github.io/">Lénaïc Chizat</a> [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>]. This requires tools from optimal transport which I will briefly describe (for more details, see, e.g., [<a href="https://arxiv.org/abs/1803.00567">14</a>]).</p>



<p class="justify-text">Next month, I will focus on generalization capabilities and the several implicit biases associated with gradient descent in this context [<a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">15</a>, <a href="https://arxiv.org/pdf/2002.04486">16</a>].</p>



<h2>Infinitely wide limit and probability measures</h2>



<p class="justify-text">Following the standard learning set-up, our goal will be to minimize with respect to the prediction function \(h\) the functional \(R\) defined as $$ R(h) = \mathbb{E}_{p(x,y)} \big[ \ell( y, h(x) ) \big],$$ where \(\ell(y,h(x))\) is the loss incurred by outputting \(h(x)\) when \(y\) is the true label. Even within deep learning, this loss is most often convex in its second argument, such as for least-squares or <a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">logistic</a> losses. Thus, I will assume that \(R\) is convex.</p>



<p>The expectation can be considered in two scenarios:</p>



<ul class="justify-text"><li><strong>Empirical risk</strong>: this corresponds to the situation where we have observations \((x_j,y_j)\), \(j=1,\dots,n\), coming from some joint distribution on \((x,y) \in \mathbb{R}^d \times \mathbb{R}\). Minimizing \(R\) then may not lead to any guarantee on unseen data unless some explicit or implicit regularization is used. In next blog post, I will consider the implicit regularization effect of gradient-based algorithms.</li><li><strong>Expected risk (or generalization performance)</strong>: The expectation is taken with respect to unseen data, and thus its value (or a gradient) cannot be computed. However, any training observation \((x_j,y_j)\) can lead to an unbiased estimate, and if single pass stochastic gradient is used, our guarantees will be on the expected risk.</li></ul>



<p class="justify-text">The main and very classical idea is to consider the minimization of $$ G(W) = G(w_1,\dots,w_m) = R \Big( \frac{1}{m} \sum_{i=1}^m \Phi(w_i) \Big),$$ and see it as the minimization of $$ F(\mu) = R \Big( \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \Big),$$ with respect to a probability measure \(\mu\), with the equivalence for $$ \mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i),$$ where \(\delta(w_i)\) is the Dirac measure at \(w_i\). See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3852" height="93" src="https://francisbach.com/wp-content/uploads/2020/05/diracs_measures-1-1024x156.png" width="615"/>Left: discrete probability measure. Right: measure with density.</figure></div>



<p class="justify-text">When \(m\) is large, we can represent any measure in the <a href="https://en.wikipedia.org/wiki/Convergence_of_measures#Weak_convergence_of_measures">weak sense</a> (that is, expectations of any continuous and bounded functions can be approximated). The benefits of considering the space of all measures instead of discrete measures have been used already in variety of contexts in machine learning, statistics and signal processing [<a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">17</a>, <a href="http://papers.nips.cc/paper/2800-convex-neural-networks.pdf">18</a>, <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">19</a>]. In this blog post, the key benefit is that the set of measures in convex and \(h = \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \) is linear in the measure \(\mu\), so that our optimization problem has become convex.</p>



<p class="justify-text">However, (1) It does not buy much in practice, as the set of probability measures is infinite-dimensional. <a href="http://Frank-Wolfe algorithms">Frank-Wolfe algorithms</a> can be used, but the choice of new neurons is a difficult optimization problem, NP-hard for the threshold activation function [<a href="https://epubs.siam.org/doi/pdf/10.1137/070685798">20</a>], with polynomial potentially high complexity for the relu activation [<a href="http://proceedings.mlr.press/v65/goel17a/goel17a.pdf">21</a>], and (2) this is not what is used in practice, which is (stochastic) gradient descent.</p>



<h2>Finite-dimensional gradient flow</h2>



<p class="justify-text">In this post, I will consider the gradient flow $$\dot{W} = \ – m \nabla G(W),$$ (where \(m\) is added as a normalization factor to allow a well-defined limit when \(m\) tends to infinity). This is still not exactly what is used in practice, but, as explained in <a href="https://francisbach.com/gradient-flows/">last month post</a>, this is a good approximation of gradient descent (if using the empirical risk, then leading to guarantees of global convergence on the empirical risk only), or stochastic gradient descent (if doing a single pass on the data, then leading to guarantees of global convergence on unseen data). This is a non-convex dynamics, with stationary points and local minima, even when \(m\) is large (see, e.g., [<a href="http://proceedings.mlr.press/v80/safran18a/safran18a.pdf">27</a>]).</p>



<p class="justify-text">Two main questions arise: (1) what does the gradient flow dynamics converge to when the number of neurons \(m\) tends to infinity, and (2) can we get any global convergence guarantees for the limiting dynamics?</p>



<h2>Mean-field limit and Wasserstein gradient flows</h2>



<p class="justify-text">When \(m\) tends to infinity, since we want to use the measure representation, we need to understand the effect of performing the gradient flow jointly on \(w_1,\dots,w_m\) on the measure $$ \mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i),$$ and see if we can take a limit when \(m\) tends to infinity. This process of taking limits is common in physics and often referred to as the “mean-field” limit, and has been considered in a series of recent works [<a href="https://arxiv.org/pdf/1712.05438">22</a>, <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>, <a href="https://www.pnas.org/content/pnas/115/33/E7665.full.pdf">23</a>, <a href="https://arxiv.org/pdf/1805.00915">24</a>]. To avoid too much technicality, I will assume that the map \(\Phi\) is sufficiently differentiable, which unfortunately exclude the relu activation; for dedicated results, see [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>].</p>



<p class="justify-text"><strong>Gradient flows on metric spaces.</strong> In order to understand the dynamics in the space of probability measures, we need to take a step backward and realize that gradient flows can be defined for many functions \(f\) on any metric space \(\mathcal{X}\). Indeed, it can be seen as the limit of taking infinitesimal steps of length \(\gamma\), where each new iterate \(x_{k+1}\) (corresponding to the value at time \(k\gamma\)) is defined recursively from \(x_k\) as $$x_{k+1} \in \arg\min_{x \in \mathcal{X}}\  f(x) + \frac{1}{2\gamma} d(x,x_k)^2.$$ As shown in [<a href="http://www2.stat.duke.edu/~sayan/ambrosio.pdf">25</a>, <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">26</a>], with some form of interpolation, this defines a curve with prescribed values \(x_k\) at each \(\gamma k\), and when the step-size \(\gamma\) goes to zero, this curves “converges” to the gradient flow.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3960" height="212" src="https://francisbach.com/wp-content/uploads/2020/05/euler-1024x520.png" width="419"/>Gradient flow in bold black, with interpolating curve in red from 14 points \(x_0,\dots,x_{13}\).</figure></div>



<p class="justify-text">For the space \(\mathcal{X} = \mathbb{R}^d\) with the Euclidean distance and a continuously differentiable function \(f\), we obtain that $$x_{k+1} = x_k – \gamma f'(x_k) + o(\gamma),$$ and we get the usual gradient flow associated to \(f\), and the scheme above is nothing less than <a href="https://en.wikipedia.org/wiki/Euler_method">Euler discretization</a> that was described <a href="https://francisbach.com/gradient-flows/">last month</a>.</p>



<p class="justify-text"><strong>Vector space gradient flows on probability measures.</strong> Probability measures are a convex subset of measures with finite <a href="https://en.wikipedia.org/wiki/Total_variation#Total_variation_of_probability_measures">total variation</a>, which is equal to the \(\ell_1\)-norm between densities when the two probability measures have densities with respect to the same base measure. It is a normed vector space for which we could derive our first type of gradient flow, which can be seen as a continuous version of Frank-Wolfe algorithm, where atoms are added one by one, until convergence.  </p>



<p class="justify-text">As mentioned above, the fact that atoms are created sequentially seems attractive computationally. However, (1) deciding which one to add is a computationally hard problem, and (2) the flow on measures cannot be approximated by a finite evolving set of “particles” (here hidden neurons each defined by a vector \(w \in \mathbb{R}^{d+1}\)).</p>



<p class="justify-text"><strong>Wasserstein gradient flows on probability measures.</strong> There is another natural distance here, namely the <a href="https://en.wikipedia.org/wiki/Wasserstein_metric">Wasserstein distance</a> (sometimes called the Kantorovich–Rubinstein distance). In order to remain short, I will only define it between empirical measures $$\mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i) \mbox{ and } \nu = \frac{1}{m} \sum_{i=1}^m \delta(v_i)$$ with the same number of points. The squared 2-Wasserstein distance is obtained by minimizing $$\frac{1}{m} \sum_{i=1}^m \| w_j – v_{\sigma(j)} \|_2^2$$ over all permutations \(\sigma: \{1,\dots,m\} \to \{1,\dots,m\}\). See illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3886" height="171" src="https://francisbach.com/wp-content/uploads/2020/05/Wasserstein-2-1024x307.png" width="573"/>Wasserstein distance between two empirical measures: (left) original observations of two empirical measures with \(m = 11\) points, (right) assigning all black points to red points by minimizing the sum of squared distances between assigned points.</figure></div>



<p class="justify-text">This can be extended to any pair of probability measures, and used within gradient flows, it has a very natural decoupling property: if \(\mu\) is fixed, and \(\nu\) is within a small distance of \(\mu\) in Wasserstein distance, then the optimal permutation above will always be the same, that is, locally, the Wasserstein distance is a sum of squared Euclidean distances. Then, the Wasserstein gradient flow will lead to \(m\) independent local regular Euclidean gradient flows, which interact through the gradient term as: $$ \dot{w}_i = \ –  \nabla \Phi(w_i)  \nabla R\Big(\int_{\mathbb{R}^p} \Phi d\mu \Big),$$ where the Jacobian \(\nabla \Phi(w_i)\) is a linear operator from \(\mathcal{F}\) to \(\mathbb{R}^p\), and \( \nabla R: \mathbb{R}^p \to \mathcal{F}\) the gradient operator of \(R\). Since \(\mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i)\), the dynamics of each particle interacts through the gradient of \(R\). </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3890" height="225" src="https://francisbach.com/wp-content/uploads/2020/05/Wasserstein_flows-1024x508.png" width="456"/>Gradient flow for \(m=7\) interacting particles.</figure></div>



<p class="justify-text">The intuitive reasoning above is behind the formal result for the function $$ F(\mu) = R \Big( \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \Big),$$ that the limit of the Euclidean gradient flow on each particle when \(m\) tends to infinity, is exactly the Wasserstein gradient flow of \(F\). While I have proposed an intuitive explanation, this can be made more formal in particular through the use of partial differential equations on the density of the measure [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>] (see also nice <a href="http://web.math.ucsb.edu/~kcraig/math/curriculum_vitae_files/NIPS_120917.pdf">slides</a> from Katy Craig on Wasserstein gradient flows).</p>



<p class="justify-text"><strong>Stationary points.</strong> Since \(R\) is assumed convex over the convex set of probability measures, all local minima of \(R\) are global, and we should expect the gradient flow to converge to global optimum from any initial measure. This is true for the gradient flow associated with the total variation metric. However this is not true for the Wasserstein gradient flow, for which stationary points which are not global minimizers exist (given that for discrete measures this corresponds to classical backpropagation, this is well known to anybody who has ever trained a neural network). Note that there exists a notion of convexity for Wasserstein gradient flows, namely <a href="https://en.wikipedia.org/wiki/Geodesic_convexity">geodesic convexity</a>, but the function \(F\) is not geodesically convex in general.</p>



<h2>Global convergence</h2>



<p class="justify-text">We can now describe the main result from our recent work with Lénaïc [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>]: under assumptions described below, for the function \(F\) defined above, if the Wasserstein gradient flow converges to a measure, this measure has to be a global minimum of \(F\) (note that we cannot prove it is always convergent).</p>



<p class="justify-text">On top of technical regularity assumptions that I will not describe here, we need two crucial broad assumptions:</p>



<ul class="justify-text"><li><strong>Homogeneity</strong> of the function \(\Phi: \mathbb{R}^{d+1} \to \mathcal{F}\). We need a condition of this form, since if \(R\) is a linear function, then \(F(\mu)\) is of the form \(F(\mu) = \int_{\mathbb{R}^p} \psi(w)d\mu(w)\) with \(\psi(w) = R(\Phi(w))\), and the Wasserstein gradient flow will converge to a weighted some of Diracs at all local minimizers of \(\psi\), which is typically not a global minimizer.</li><li><strong>Initialization with positive mass in all directions</strong>. That is, \(w_i\)’s are uniformly distributed on the sphere or Gaussian, which is the de facto choice in practice. </li></ul>



<p class="justify-text"><strong>Illustration</strong>. We illustrate the result above by considering \(R\) as the square loss and \(y\) being generated from \(x\) through a neural network with \(m_0=5\) neurons. When running the gradient flow above, as soon as \(m \geqslant 5\), the model is sufficiently flexible to attain zero loss, which is thus the global optimum of the cost function. However, the gradient flow may not reach it, as it gets trapped in a local optimum. Our theoretical result suggests that when \(m\) is large, we should converge to the original neurons, which we see below. The surprising (and still unexplained) phenomenon is that \(m\) does not need to be much larger than \(m_0\) to see practical global convergence.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3942" height="259" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m5-1024x683.png" width="389"/>Position of \(m = 5\) neurons, plotted as \(|a_i| b_i \in \mathbb{R}^2\) for a two-dimensional problem. The five dotted lines are the directions of the generating neurons. Although \(m\) is large enough to lead to the global optimum, the flow gets stuck in a local optimum.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3943" height="243" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m10-1024x683.png" width="365"/>Position of \(m = 10\) neurons; same setting as above. The flow converges to the global optimum, although \(m\) is not large.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3944" height="252" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m100-1024x683.png" width="379"/>Position of \(m = 100\) neurons; same setting as above. The flow converges to the global optimum, with \(m\) large. See video below.</figure></div>



<figure class="wp-block-video aligncenter justify-text"><video src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m100.mp4"/>Position of \(m = 100\) neurons; exact same setting as above.</figure>



<h2>Discussion and open problems</h2>



<p class="justify-text">In this blog post, I described theoretical results showing the benefits of overparameterization: when the number of hidden neurons \(m\) tends to infinity, then the corresponding gradient flow converges to the global optimum of the cost function. The proof relies notably on homogeneity properties of the relu activation. </p>



<p class="justify-text">The main weakness of  this result is that is only <em>qualitative</em>: we cannot quantify how big \(m\) need to be to be close to the infinite width limit, or how fast the gradient flow converges to the global optimum. These are still open problems. Additional interesting areas of research are to extend these results to convolutional and/or deep networks.</p>



<p class="justify-text">Now that we know that we can obtain global convergence, I will describe next month the generalization properties when interpolating the training data with an overparameterized relu network [<a href="https://arxiv.org/pdf/2002.04486">16</a>].</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Lénaïc Chizat for producing the nice figures and video of neural networks, proofreading this blog post, and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Sébastien Bubeck. <a href="https://arxiv.org/pdf/1405.4980">Convex Optimization: Algorithms and Complexity</a>. <em>Foundations and Trends in Machine Learning</em>, <em>8</em>(3-4), 231-357, 2015.<br/>[2] Léon Bottou, Frank E. Curtis, Jorge Nocedal. <a href="https://epubs.siam.org/doi/pdf/10.1137/16M1080173">Optimization methods for large-scale machine learning</a>. SIAM Review, 60(2):223-311, 2018.<br/>[3] Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski. <a href="http://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">Optimization with sparsity-inducing penalties</a>. <em>Foundations and Trends in Machine Learning, </em>4(1):1-106, 2012.<br/>[4] Shai Shalev-Shwartz, Shai Ben-David. <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding machine learning: From theory to algorithms</a>. Cambridge University Press, 2014.<br/>[5] Larry Wasserman. <a href="http://static.stevereads.com/papers_to_read/all_of_statistics.pdf">All of statistics: a concise course in statistical inference</a>. Springer Science &amp; Business Media, 2013.<br/>[6] Bernhard Schölkopf, Alexander J. Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2002.<br/>[7] Ali Rahimi and Benjamin Recht. <a href="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">Random features for large-scale kernel machines</a>. <em>Advances in neural information processing systems</em>, 2008.<br/>[8] Alessandro Rudi, Luigi Carratino, Lorenzo Rosasco. <a href="http://papers.nips.cc/paper/6978-falkon-an-optimal-large-scale-kernel-method.pdf">Falkon: An optimal large scale kernel method</a>. <em>Advances in Neural Information Processing Systems</em>, 2017.<br/>[9] Ian Goodfellow, Yoshua Bengio, Aaron Courville. <a href="https://www.deeplearningbook.org/">Deep learning</a>. MIT Press, 2016.<br/>[10] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. <a href="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">Deep sparse rectifier neural networks</a>. <em>International Conference on Artificial Intelligence and Statistics</em>, 2011.<br/>[11] Francis Bach and Eric Moulines. <a href="http://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)</a>. <em>Advances in Neural Information Processing Systems</em>, 2013.<br/>[12] MIkhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal. <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">Reconciling modern machine-learning practice and the classical bias–variance trade-off</a>. <em>Proceedings of the National Academy of Sciences</em>, 116(32), 15849-15854, 2019.<br/>[13] Lénaïc Chizat, Francis Bach. <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport</a>. <em>Advances in Neural Information Processing Systems</em>, 2018.<br/>[14] Gabriel Peyré, Marco Cututi. <em><a href="https://arxiv.org/abs/1803.00567">Computational Optimal Transport</a></em>. Foundations and Trends in Machine Learning, 51(1):1–44, 2019.<br/>[15] Lénaïc Chizat, Edouard Oyallon, Francis Bach. <a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">On Lazy Training in Differentiable Programming</a>. <em>Advances in Neural Information Processing Systems</em>, 2019.<br/>[16] Lénaïc Chizat, Francis Bach. <a href="https://arxiv.org/pdf/2002.04486">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss</a>. Technical report, arXiv:2002.04486, 2020.<br/>[17] Andrew R. Barron. <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">Universal approximation bounds for superpositions of a sigmoidal function</a>. <em>IEEE Transactions on Information theory</em>, <em>39</em>(3), 930-945, 1993.<br/>[18] Yoshua Bengio, Nicolas Le Roux, Pascal Vincent, Olivier Delalleau, Patrice Marcotte. <a href="http://papers.nips.cc/paper/2800-convex-neural-networks.pdf">Convex neural networks</a>. <em>Advances in neural information processing systems</em>, 2006.<br/>[19] Francis Bach. <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the Curse of Dimensionality with Convex Neural Networks</a>.<strong> </strong><em>Journal of Machine Learning Research</em>, 18(19):1-53, 2017.<br/>[20] Venkatesan Guruswami, Prasad Raghavendra. <a href="https://epubs.siam.org/doi/pdf/10.1137/070685798">Hardness of learning halfspaces with noise</a>. <em>SIAM Journal on Computing</em>, 39(2):742-765, 2009.<br/>[21] Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler. <a href="http://proceedings.mlr.press/v65/goel17a/goel17a.pdf">Reliably Learning the ReLU in Polynomial Time</a>. <em>Conference on Learning Theory</em>, 2017.<br/>[22] Atsushi Nitanda, Taiji Suzuki. <a href="https://arxiv.org/pdf/1712.05438">Stochastic particle gradient descent for infinite ensembles</a>. Technical report, arXiv:1712.05438, 2017.<br/>[23] Song Mei, Andrea Montanari, Phan-Minh Nguyen. <a href="https://www.pnas.org/content/pnas/115/33/E7665.full.pdf">A mean field view of the landscape of two-layer neural networks</a>. <em>Proceedings of the National Academy of Sciences</em> 115(33):E7665-E7671, 2018.<br/>[24] Grant M. Rotskoff, Eric Vanden-Eijnden. <a href="https://arxiv.org/pdf/1805.00915">Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error</a>. Technical report, arXiv:1805.00915, 2018.<br/>[25] Luigi Ambrosio, Nicola Gigli, Giuseppe Savaré. <a href="http://www2.stat.duke.edu/~sayan/ambrosio.pdf">Gradient flows: in metric spaces and in the space of probability measures</a>. Springer Science &amp; Business Media, 2008<br/>[26] Filippo Santambrogio. <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">{Euclidean, metric, and Wasserstein} gradient flows: an overview</a>. <em>Bulletin of Mathematical Sciences</em>, <em>7</em>(1), 87-154, 2017.<br/>[27] Itay Safran, Ohad Shamir. <a href="http://proceedings.mlr.press/v80/safran18a/safran18a.pdf">Spurious Local Minima are Common in Two-Layer ReLU Neural Networks</a>. <em>International Conference on Machine Learning</em>, 2018.</p></div>
    </content>
    <updated>2020-06-01T07:32:48Z</updated>
    <published>2020-06-01T07:32:48Z</published>
    <category term="Machine learning"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-06-02T07:22:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.14717</id>
    <link href="http://arxiv.org/abs/2005.14717" rel="alternate" type="text/html"/>
    <title>Differentially Private Decomposable Submodular Maximization</title>
    <feedworld_mtime>1590969600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chaturvedi:Anamay.html">Anamay Chaturvedi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Huy.html">Huy Nguyen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zakynthinou:Lydia.html">Lydia Zakynthinou</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.14717">PDF</a><br/><b>Abstract: </b>We study the problem of differentially private constrained maximization of
decomposable submodular functions. A submodular function is decomposable if it
takes the form of a sum of submodular functions. The special case of maximizing
a monotone, decomposable submodular function under cardinality constraints is
known as the Combinatorial Public Projects (CPP) problem [Papadimitriou et al.,
2008]. Previous work by Gupta et al. [2010] gave a differentially private
algorithm for the CPP problem. We extend this work by designing differentially
private algorithms for both monotone and non-monotone decomposable submodular
maximization under general matroid constraints, with competitive utility
guarantees. We complement our theoretical bounds with experiments demonstrating
empirical performance, which improves over the differentially private
algorithms for the general case of submodular maximization and is close to the
performance of non-private algorithms.
</p></div>
    </summary>
    <updated>2020-06-01T22:29:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.14620</id>
    <link href="http://arxiv.org/abs/2005.14620" rel="alternate" type="text/html"/>
    <title>Parameterized Complexity of Min-Power Asymmetric Connectivity</title>
    <feedworld_mtime>1590969600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bentert:Matthias.html">Matthias Bentert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haag:Roman.html">Roman Haag</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hofer:Christian.html">Christian Hofer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koana:Tomohiro.html">Tomohiro Koana</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nichterlein:Andr=eacute=.html">André Nichterlein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.14620">PDF</a><br/><b>Abstract: </b>We investigate parameterized algorithms for the NP-hard problem Min-Power
Asymmetric Connectivity (MinPAC) that has applications in wireless sensor
networks. Given a directed arc-weighted graph, MinPAC asks for a strongly
connected spanning subgraph minimizing the summed vertex costs. Here, the cost
of each vertex is the weight of its heaviest outgoing arc in the chosen
subgraph. We present linear-time algorithms for the cases where the number of
strongly connected components in a so-called obligatory subgraph or the
feedback edge number in the underlying undirected graph is constant.
Complementing these results, we prove that the problem is W[2]-hard with
respect to the solution cost, even on restricted graphs with one feedback arc
and binary arc weights.
</p></div>
    </summary>
    <updated>2020-06-01T22:30:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.14493</id>
    <link href="http://arxiv.org/abs/2005.14493" rel="alternate" type="text/html"/>
    <title>A Fully Dynamic Algorithm for k-Regret Minimizing Sets</title>
    <feedworld_mtime>1590969600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Yanhao.html">Yanhao Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yuchen.html">Yuchen Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wong:Raymond_Chi=Wing.html">Raymond Chi-Wing Wong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Kian=Lee.html">Kian-Lee Tan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.14493">PDF</a><br/><b>Abstract: </b>Selecting a small set of representatives from a large database is important
in many applications such as multi-criteria decision making, web search, and
recommendation. The $k$-regret minimizing set ($k$-RMS) problem was recently
proposed for representative tuple discovery. Specifically, for a large database
$P$ of tuples with multiple numerical attributes, the $k$-RMS problem returns a
size-$r$ subset $Q$ of $P$ such that, for any possible ranking function, the
score of the top-ranked tuple in $Q$ is not much worse than the score of the
$k$\textsuperscript{th}-ranked tuple in $P$. Although the $k$-RMS problem has
been extensively studied in the literature, existing methods are designed for
the static setting and cannot maintain the result efficiently when the database
is updated. To address this issue, we propose the first fully-dynamic algorithm
for the $k$-RMS problem that can efficiently provide the up-to-date result
w.r.t.~any insertion and deletion in the database with a provable guarantee.
Experimental results on several real-world and synthetic datasets demonstrate
that our algorithm runs up to four orders of magnitude faster than existing
$k$-RMS algorithms while returning results of near-equal quality.
</p></div>
    </summary>
    <updated>2020-06-01T22:21:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.14425</id>
    <link href="http://arxiv.org/abs/2005.14425" rel="alternate" type="text/html"/>
    <title>Query complexity of heavy hitter estimation</title>
    <feedworld_mtime>1590969600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Sahasrajit Sarmasarkar, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reddy:Kota_Srinivas.html">Kota Srinivas Reddy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karamchandani:Nikhil.html">Nikhil Karamchandani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.14425">PDF</a><br/><b>Abstract: </b>We consider the problem of identifying the subset
$\mathcal{S}^{\gamma}_{\mathcal{P}}$ of elements in the support of an
underlying distribution $\mathcal{P}$ whose probability value is larger than a
given threshold $\gamma$, by actively querying an oracle to gain information
about a sequence $X_1, X_2, \ldots$ of $i.i.d.$ samples drawn from
$\mathcal{P}$. We consider two query models: $(a)$ each query is an index $i$
and the oracle return the value $X_i$ and $(b)$ each query is a pair $(i,j)$
and the oracle gives a binary answer confirming if $X_i = X_j$ or not. For each
of these query models, we design sequential estimation algorithms which at each
round, either decide what query to send to the oracle depending on the entire
history of responses or decide to stop and output an estimate of
$\mathcal{S}^{\gamma}_{\mathcal{P}}$, which is required to be correct with some
pre-specified large probability. We provide upper bounds on the query
complexity of the algorithms for any distribution $\mathcal{P}$ and also derive
lower bounds on the optimal query complexity under the two query models. We
also consider noisy versions of the two query models and propose robust
estimators which can effectively counter the noise in the oracle responses.
</p></div>
    </summary>
    <updated>2020-06-01T22:29:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.14195</id>
    <link href="http://arxiv.org/abs/2005.14195" rel="alternate" type="text/html"/>
    <title>Two-Bar Charts Packing Problem</title>
    <feedworld_mtime>1590969600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Adil Erzin, Gregory Melidi, Stepan Nazarenko, Roman Plotnikov <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.14195">PDF</a><br/><b>Abstract: </b>We consider a Bar Charts Packing Problem (BCPP), in which it is necessary to
pack bar charts (BCs) in a strip of minimum length. The problem is, on the one
hand, a generalization of the Bin Packing Problem (BPP), and, on the other
hand, a particular case of the Project Scheduling Problem with
multidisciplinary jobs and one limited non-accumulative resource. Earlier, we
proposed a polynomial algorithm that constructs the optimal package for a given
order of non-increasing BCs. This result generalizes a similar result for BPP.
For Two-Bar Charts Packing Problem (2-BCPP), when each BC consists of two bars,
the algorithm we have proposed constructs a package in polynomial time, the
length of which does not exceed $2\ OPT+1$, where $OPT$ is the minimum possible
length of the packing. As far as we know, this is the first guaranteed estimate
for 2-BCPP. We also conducted a numerical experiment in which we compared the
solutions built by our approximate algorithms with the optimal solutions built
by the CPLEX package. The experimental results confirmed the high efficiency of
the developed algorithms.
</p></div>
    </summary>
    <updated>2020-06-01T22:29:05Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/05/31/linkage</id>
    <link href="https://11011110.github.io/blog/2020/05/31/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Diana Davis’s beautiful pentagons (). I briefly mentioned her regular-pentagon billiards-trajectory art in an earlier post but now Evelyn Lamb has a much more detailed column on her and her work.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://blogs.scientificamerican.com/roots-of-unity/diana-davis-beautiful-pentagons/">Diana Davis’s beautiful pentagons</a> (<a href="https://mathstodon.xyz/@11011110/104182521046531449"/>). I briefly mentioned her regular-pentagon billiards-trajectory art in <a href="https://11011110.github.io/blog/2019/02/15/linkage.html">an earlier post</a> but now Evelyn Lamb has a much more detailed column on her and her work.</p>
  </li>
  <li>
    <p><a href="https://inference-review.com/article/points-and-lines">Points and lines</a> (<a href="https://mathstodon.xyz/@11011110/104190473820256744"/>). A new review of my book <em><a href="https://www.ics.uci.edu/~eppstein/forbidden/">Forbidden Configurations in Discrete Geometry</a></em>, by Daniel Kleitman, in <em>Inference</em>.</p>
  </li>
  <li>
    <p><a href="https://www.mathi.uni-heidelberg.de/~roquette/noetherphoto-engl.pdf">About photos of Emmy Noether</a> (<a href="https://mathstodon.xyz/@11011110/104193371530927420"/>), in which Peter Roquette apologizes for having indirectly caused <a href="https://books.google.com/books/about/Emmy_Noether.html?id=IePqBgAAQBAJ">Margaret Tent’s young-adult historical-fiction about Noether</a> to have a photo of someone else on its cover. Via MarkH<sub>21</sub> on Wikipedia, in the context of <a href="https://commons.wikimedia.org/wiki/Commons:Deletion_requests/File:Noether.jpg">a discussion of the provenance of a different photo of Noether</a>.</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.4007/annals.2020.191.2.5">The Conway knot is not slice</a> (<a href="https://mathstodon.xyz/@11011110/104196369094406901"/>). Newly published result by Lisa Piccirillo in <em>Ann. Math.</em>, with <a href="https://www.quantamagazine.org/graduate-student-solves-decades-old-conway-knot-problem-20200519/">an overview of the significance of the result (if not much of its detail) in <em>Quanta</em></a>.</p>
  </li>
  <li>
    <p><a href="https://gowers.wordpress.com/2020/05/20/mathematical-research-reports-a-new-mathematics-journal-is-launched/">Tim Gowers relates the convoluted history of a mathematical announcements journal</a> (<a href="https://mathstodon.xyz/@11011110/104205046534922072"/>). <em>Electronic Research Announcements of the AMS</em> (founded 1995) moved to the American Inst. of Mathematical Sciences in 2007, but recent heavyhanded moves by the publisher led the editorial board to quit, and its name changed to <em>Electronic Research Archive</em>. In the meantime the old editorial board have a new journal: <em><a href="https://mrr.centre-mersenne.org/">Mathematical Research Reports</a></em>. See the Gowers link for details.</p>
  </li>
  <li>
    <p>In preparation for time travel week in graduate data structures, here’s <a href="http://www.tasteofcinema.com/2014/the-20-best-time-travel-movies-of-all-time/">a listicle of 20 time travel movies</a> (<a href="https://mathstodon.xyz/@11011110/104208167329418315"/>). There are any number of these lists but for me they should include at least <em>12 Monkeys</em>, <em>Primer</em>, <em>Safety Not Guaranteed</em>, <em>Donnie Darko</em>, and <em>Time Bandits</em>. This one adds <em>The Girl Who Leapt Through Time</em>, also good. I’d have thrown in <em>The Infinite Man</em> but it’s obscure enough that I’m not offended by its absence. The relevant one for my class, <em>Retroactive</em>, can stay off.</p>
  </li>
  <li>
    <p><a href="https://www.thisiscolossal.com/2020/05/xavier-puente-vilardell-wood-sculpture/">Spirals and loops twist through wooden sculptures by Xavier Puente Vilardell</a> (<a href="https://mathstodon.xyz/@11011110/104216153044887740"/>).</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/a/361166/440">Dmitri Panov constructs infinitely many convex 4-polytopes with no triangle or quadrilateral 2-faces</a> (<a href="https://mathstodon.xyz/@11011110/104222101065596394"/>). The construction is pretty: take some 120-cells (all 2-faces regular pentagons and all 3-faces regular dodecahedra), embedded into hyperbolic space so all dihedrals are right angles, and glue them together on shared facets. If at most two touch at each ridge, the result is convex. Some pairs of pentagons merge into hexagons, but you still have no triangles or quads.</p>
  </li>
  <li>
    <p><a href="https://cp4space.wordpress.com/2013/09/06/ten-things-you-possibly-didnt-know-about-the-petersen-graph/">Ten things you (possibly) didn’t know about the Petersen graph</a> (<a href="https://mathstodon.xyz/@11011110/104226455130356772"/>). Found while making <a href="https://en.wikipedia.org/wiki/The_Petersen_Graph">a Wikipedia article on the book <em>The Petersen Graph</em></a> (for the graph itself, see <a href="https://en.wikipedia.org/wiki/Petersen_graph">its Wikipedia article</a>).</p>
  </li>
  <li>
    <p><a href="https://www.taylorfrancis.com/books/9781351247771">Godfried Toussaint’s book <em>The Geometry of Musical Rhythm:
What Makes a “Good” Rhythm Good?</em>, on the mathematical analysis of drumming patterns, has a new expanded posthumous 2nd edition</a> (<a href="https://mathstodon.xyz/@11011110/104233265094045761"/>). I was able to download free from there but that may be via a campus subscription; your access may vary. For a description of the 1st edition of the book (probably undetailed enough that it can describe the 2nd as well) see <a href="https://en.wikipedia.org/wiki/The_Geometry_of_Musical_Rhythm">its Wikipedia article</a>.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=CfRSVPhzN5M">French video about self-replicating patterns in cellular automata, with English subtitles</a> (<a href="https://mathstodon.xyz/@11011110/104239007149697974"/>, <a href="https://cp4space.wordpress.com/2019/06/11/self-replicator-caught-on-video/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@erou/104245194532017940">@erou visualizes the complexity of Karatsuba’s algorithm for integer multiplication as a Sierpiński triangle</a>, inside a square, with a number of dark pixels proportional to the steps of the algorithm. The square itself counts in the same way the complexity of the naive algorithm.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/quicktakes/2020/05/28/proposed-legislation-would-bar-chinese-stem-graduate-students">Republicans propose legislation to bar Chinese from science</a> (<a href="https://mathstodon.xyz/@11011110/104248070190238732"/>). I’m having difficulty distinguishing this sort of move from “<a href="https://en.wikipedia.org/wiki/Nuremberg_Laws">Nazis propose legislation to bar Jews from science</a>”.</p>
  </li>
  <li>
    <p><a href="https://wrog.dreamwidth.org/63735.html">Arthur C. Clarke and the projective plane</a> (<a href="https://mathstodon.xyz/@ColinTheMathmo/104253017415627794"/>). Wrog beanplates Clarke’s “The Wall of Darkness” (1949).</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1112/blms/16.3.278">Volumes of projections of unit cubes</a> (<a href="https://mathstodon.xyz/@11011110/104261517104618440"/>), Peter McMullen, <em>Bull LMS</em> 1984. A cute theorem that deserves to be better known: if you hold a unit cube in the noonday sun, at any angle, its shadow’s area equals its height (elevation difference between lowest and highest point). It follows immediately that the biggest possible shadow is a hexagon with area = long diagonal length = , and the smallest shadow is a unit square. Similar things happen in higher dimensions.</p>
  </li>
  <li>
    <p><a href="http://roberthodgin.com/project/meander">Meander, a procedural system for generating historical maps of rivers that never existed</a> (<a href="https://mathstodon.xyz/@11011110/104265448606504741"/>, <a href="https://www.metafilter.com/187292/Meander-generating-historical-maps-of-rivers-that-never-existed">via</a>). The way this system models the motion of river beds over time looks a lot like the time-reversal of the curve-shortening flow, but with added tangential motion that causes bends to flow downstream (and maybe helps maintain smoothness) and with shortcutting of oxbows.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-05-31T16:08:00Z</updated>
    <published>2020-05-31T16:08:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-31T23:11:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/083</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/083" rel="alternate" type="text/html"/>
    <title>TR20-083 |  Proximity Gaps for Reed-Solomon Codes | 

	Eli Ben-Sasson, 

	Dan Carmon, 

	Yuval Ishai, 

	Swastik Kopparty, 

	Shubhangi Saraf</title>
    <summary>A collection of sets displays a proximity gap with respect to some property if for every set in the collection, either (i) all members are $\delta$-close to the property in relative Hamming distance or (ii) only a tiny fraction of members are $\delta$-close to the property. In particular, no set in the collection has roughly half of its members $\delta$-close to the property and the others $\delta$-far from it.

We show that the collection of affine spaces displays a proximity gap with respect to Reed-Solomon (RS) codes, even over small fields, of size polynomial in the dimension of the code, and the gap applies to any $\delta$ smaller than the Johnson/Guruswami-Sudan list-decoding bound of the RS code. We also show near-optimal gap results, over fields of (at least) linear size in the RS code dimension, for $\delta$ smaller than the unique decoding radius. Finally, we discuss several applications of our proximity gap results to distributed storage, multi-party cryptographic protocols, and concretely efficient proof systems.

We prove the proximity gap results by analyzing the execution of classical algebraic decoding algorithms for Reed-Solomon codes (due to Berlekamp-Welch and Guruswami-Sudan) on a formal element of an affine space. This involves working with Reed-Solomon codes whose base field is an (infinite) rational function field. Our proofs are obtained by developing an extension (to function fields) of a strategy of Arora and Sudan for analyzing low-degree tests.</summary>
    <updated>2020-05-30T19:25:58Z</updated>
    <published>2020-05-30T19:25:58Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-02T07:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17089</id>
    <link href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/" rel="alternate" type="text/html"/>
    <title>Just Arvind</title>
    <summary>Theory and practice [ MIT ] Arvind Mithal—almost always referred to as Arvind—is now the head of the faculty of computer science at a Boston trade school. The school, also known as MIT, is of course one of the top places for all things computer science. From education to service to startups to research, MIT […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Theory and practice</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/unknown-142/" rel="attachment wp-att-17098"><img alt="" class="alignright  wp-image-17098" src="https://rjlipton.files.wordpress.com/2020/05/unknown-2.jpeg?w=180" width="180"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ MIT ]</font></td>
</tr>
</tbody>
</table>
<p>
Arvind Mithal—almost always referred to as Arvind—is now the head of the faculty of computer science at a Boston trade school. The school, also known as MIT, is of course one of the top places for all things computer science. From education to service to startups to research, MIT is perhaps the best in the world.</p>
<p>
Today I thought we might discuss one of Arvind’s main research themes.</p>
<p>
Although this work started in the 1980’s I believe that it underscores an important point. This insight might apply to current research topics like <a href="https://en.wikipedia.org/wiki/Timeline_of_quantum_computing">quantum computers</a>.</p>
<p>
But first I cannot resist saying something personal about him. Arvind is a long time friend of mine, and someone who gets the <i>stuck in elevator measure</i> of many hours. This measure is one I made up, but I hope you get the idea.</p>
<p>
Arvind is the only name I have ever known for Arvind. I was a bit surprised to see that the Wikipedia reference for <a href="https://en.wikipedia.org/wiki/Arvind_(computer_scientist)">him</a> states his “full” name. He told me that he found having one name—not two—was a challenge. For example, he sometimes had to explain when arriving at a check-in desk at a conference that he only had one name. His name tag often became: </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/tag/" rel="attachment wp-att-17092"><img alt="" class="aligncenter size-medium wp-image-17092" height="222" src="https://rjlipton.files.wordpress.com/2020/05/tag.png?w=300&amp;h=222" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>because the conference software could not handle people with one name. </p>
<p>
</p><p/><h2> Dataflow—The prehistory </h2><p/>
<p/><p>
About forty years ago one of the major open problem in CS was how to make computers go faster. It is still an issue, but in the 1980’s this problem was one of all-hands-on-deck. It was worked on by software engineers, by electrical engineers, by researchers of all kinds including complexity theorists. Conferences like FOCS and STOC—hardcore theory conferences—often contained papers on how to speed up computations. </p>
<p>
Two examples come to mind:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> The idea of <a href="https://en.wikipedia.org/wiki/Systolic_array">systolic</a> arrays led by Hsiang-Tsung Kung then at CMU. Also his students, especially Charles Leiserson, made important contributions. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> The idea of the <a href="https://en.wikipedia.org/wiki/Ultracomputer">Ultracomputer</a> led by Jacob Schwartz at NYU. An ultracomputer has N processors, N memories and an N log N message-passing switch connecting them. Note, the use of “N” so you can <a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/">tell</a> that it came from a theorist. </p>
<p>
Arvind tried to make computers faster by inventing a new type of computer architecture. Computers then were based on the classic von Neumann architecture or control flow architecture. He and his colleagues worked for many years trying to replace this architecture by <a href="https://en.wikipedia.org/wiki/Dataflow#Hardware_architecture">dataflow</a>. </p>
<p>
A bottleneck in von Neumann style machines is caused by the program counter fetch cycle. Each cycle the program counter decides which instruction to get, and thus which data to get. These use the same hardware channel, which causes the famous <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">von Neumann bottleneck</a>. We have modified Wikipedia’s graphic to make the bottleneck aspect clearer:</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/neck2/" rel="attachment wp-att-17111"><img alt="" class="aligncenter size-medium wp-image-17111" height="204" src="https://rjlipton.files.wordpress.com/2020/05/neck2.png?w=300&amp;h=204" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
</p><p/><h2> Dataflow—The promise </h2><p/>
<p/><p>
Arvind is usually identified with the invention of <a href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a191029.pdf">dataflow</a> computer architecture. The key idea of this architecture is to avoid the above bottleneck by eliminating the program counter. If there are no instructions to fetch, then it would seem that we can beat the bottleneck. Great idea.</p>
<p>
Dataflow architectures do not have a program counter, and so data is king. Roughly data objects move around in such a machine, and they eventually appear at computational units. Roughly these machines operate, at a high level, like a directed graph from complexity theory. Data moves along edges to nodes that compute values. Moreover, nodes compute as soon as all the input data is present. After the node computes the value, the new data is sent to the next node; and so on. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/flow-2/" rel="attachment wp-att-17094"><img alt="" class="aligncenter size-medium wp-image-17094" height="229" src="https://rjlipton.files.wordpress.com/2020/05/flow.png?w=300&amp;h=229" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
The hope was that this would yield a way to increase the performance of computers. No program counter, no instruction fetching, could make dataflow machines faster. </p>
<p>
</p><p/><h2> Dataflow—The actual </h2><p/>
<p/><p>
The dataflow idea is clever. The difficulty is while dataflow can work, classic von Neumann machines have been augmented in various ways. The competition rarely stays put. These updates did not eliminate the von Neumann bottleneck, but they reduce the cost of it, and let von Neumann machines continue to get faster. Caches are one example of how they avoid the bottleneck, thus making the dataflow machines less attractive. Thus, a cache for program instructions makes it likely that the program fetch step does not actually happen. This is an attack on the main advantage of dataflow machines. </p>
<p>
The <a href="http://www.cs.cornell.edu/tve/papers-ucb/limits.pdf">paper</a> by David Culler, Klaus Schauser, and Thorsten von Eicken titled <i>Two Fundamental Limits on Dataflow Multiprocessing?</i> discusses these issues in detail. They start by saying: </p>
<blockquote><p><b> </b> <em> The advantages of dataflow architectures were argued persuasively in a seminal 1983 paper by Arvind and Iannucci and in a 1987 revision entitled “Two Fundamental Issues in Multiprocessing”. However, reality has proved less favorable to this approach than their arguments would suggest. This motivates us to examine the line of reasoning that has driven dataflow architectures and fine-grain multithreading to understand where the argument went awry. </em>
</p></blockquote>
<p>
</p><p/><h2> Dataflow—Lessons </h2><p/>
<p/><p>
What are the lessons? Is there one? </p>
<p>
I claim there are lessons. The work of Arvind on dataflow was and is important. It did not lead to the demise of von Neumann machines. I am using one right now to write this.</p>
<p>
Dataflow did lead to insights on programming that have many applications. The dataflow idea may yet impact special computational situations: there is interest in using them for data intensive applications.</p>
<p>Ken adds that dataflow has been realized in other ways. Caches and pipes and subsequent architecture innovations profit from designs that enhance <a href="https://en.wikipedia.org/wiki/Locality_of_reference">locality</a>. The <a href="https://en.wikipedia.org/wiki/MapReduce">MapReduce</a> programming model gives a general framework that fits this well. The paradigm of <a href="http://\href{https://en.wikipedia.org/wiki/Stream_processing">streaming</a> is even a better fit. Note that Wikipedia says both that it is “equivalent to dataflow programming” and “was explored within dataflow programming”—so perhaps the parent lost out to the wider adaptability of her children.</p>
<p>
I think there are lessons: Practical goals like making hardware go faster, are complex and many faceted. A pure theory approach such as systolic arrays or ultra computers or dataflow machines is unlikely to suffice. Also the existing technology like von Neumann machines will continue to evolve. </p>
<p>
A question is: Could quantum computers be subject to the same lesson? Will non-quantum machines continue to evolve in a way to make the quantum advantage less than we think? Or is this type of new architecture different? Could non-quantum machines incorporate tricks from quantum and <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Speaking about theory and practice: Noga Alon, Phillip Gibbons, Yossi Matias, and Mario Szegedy are the winners of the 2019 ACM Paris Kanellakis Theory and Practice <a href="https://awards.acm.org/kanellakis">Award</a>. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/paris/" rel="attachment wp-att-17096"><img alt="" class="aligncenter size-medium wp-image-17096" height="169" src="https://rjlipton.files.wordpress.com/2020/05/paris.png?w=300&amp;h=169" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>We applaud them on receiving this award. Sadly the Paris award reminds us that Paris Kanellakis died in the terrible plane crash of American Airlines Flight 965 on December 20, 1995. Also on that flight were his wife, Maria Otoya, and their two children, Alexandra and Stephanos.</p>
<p>
The citation for the award says: </p>
<blockquote><p><b> </b> <em> They pioneered a framework for algorithmic treatment of streaming massive datasets, and today their sketching and streaming algorithms remain the core approach for streaming big data and constitute an entire subarea of the field of algorithms. </em>
</p></blockquote>
<p/><p>
In short they invented streaming. </p>
<p/></font></font></div>
    </content>
    <updated>2020-05-29T21:04:11Z</updated>
    <published>2020-05-29T21:04:11Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="bottleneck"/>
    <category term="dataflow"/>
    <category term="names"/>
    <category term="Paris"/>
    <category term="streaming"/>
    <category term="von Neumann"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-06-02T07:20:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7736</id>
    <link href="https://windowsontheory.org/2020/05/29/liberation-from-grades/" rel="alternate" type="text/html"/>
    <title>Liberation from grades</title>
    <summary>This semester, like many other universities, Harvard switched to a pass/fail grade model. (In typical Harvard style, we give them different names – “Emergency Satisfactory” and “Emergency Unsatisfactory” – but that doesn’t matter much). One unexpected but happy consequence of this policy is that even though I already submitted the grades for my crypto course, […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This semester, like many other universities, Harvard switched to a pass/fail grade model. (In typical Harvard style, we give them different names – “Emergency Satisfactory” and “Emergency Unsatisfactory” –  but that doesn’t matter much).</p>



<p>One unexpected but happy consequence of this policy is that even though I already submitted the grades for my <a href="https://cs127.boazbarak.org/schedule/">crypto course</a>, I can now take the time and send students detailed feedback on their final projects. Typically,  both students and faculty tend to be focused on the “bottom line” of exams or papers – what is the final grade. The comments are viewed as of marginal importance and only serve to justify why points have been deducted.</p>



<p>Now that there is no grade, I am actually giving many more comments on the write ups, trying to focus on giving students feedback on writing and presentation that will be useful for them later on. I benefited immensely from the extensive comments on my writing that I received from my advisor Oded Goldreich. While I will never match Oded’s thoroughness and dedication, I try to at least provide some of this to my students (though unlike Oded, I use blue and not red ink, and also do not intersperse the comments with Hebrew curses for emphasis <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/> )</p>



<p/></div>
    </content>
    <updated>2020-05-29T16:53:57Z</updated>
    <published>2020-05-29T16:53:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-06-02T07:21:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=439</id>
    <link href="https://tcsplus.wordpress.com/2020/05/28/tcs-talk-wednesday-june-3-michael-p-kim-stanford-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, June 3 — Michael P. Kim, Stanford University</title>
    <summary>The next TCS+ talk (and penultimate of the season!) will take place this coming Wednesday, June 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Michael Kim from Stanford University will speak about “Learning from Outcomes:  Evidence-Based Rankings” (abstract below). You can reserve a spot as an individual […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk (and penultimate of the season!) will take place this coming Wednesday, June 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Michael Kim</strong> from Stanford University will speak about “<em>Learning from Outcomes:  Evidence-Based Rankings</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br/>
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: In this work, we address the task of ranking members of a population according to their qualifications based on a training set of binary outcome data. A natural approach for ranking is to reduce to prediction: first learn to predict individuals’ “probability” of success; then rank individuals in the order specified by the predictions. A concern with this approach is that such rankings may be vulnerable to manipulation. The rank of an individual depends not only on their own qualification but also on every other individuals’ qualifications, so small inaccuracies in prediction may result in a highly inaccurate and unfair induced ranking. We show how to obtain rankings that satisfy a number of desirable accuracy and fairness criteria, despite the coarseness of binary outcome data.<br/>
We develop two parallel definitions of evidence-based rankings. First, we study a semantic notion of <em>domination-compatibility</em>: if the training data suggest that members of a set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" title="S"/> are on-average more qualified than the members of <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" title="T"/>, then a ranking that favors <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" title="T"/> over <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" title="S"/> (i.e., where <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" title="T"/> dominates <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" title="S"/>) is blatantly inconsistent with the data, and likely to be discriminatory. Our definition asks for domination-compatibility, not just for a pair of sets (e.g., majority and minority populations), but rather for every pair of sets from a rich collection <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\mathcal{C}"/> of subpopulations. The second notion—evidence-consistency—aims at precluding even more general forms of discrimination: the ranking must be justified on the basis of consistency with the expectations for every set in the collection <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\mathcal{C}"/>. Somewhat surprisingly, while evidence-consistency is a strictly stronger notion than domination-compatibility when the collection <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\mathcal{C}"/> is predefined, the two notions are equivalent when the collection <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\mathcal{C}"/> may depend on the ranking itself. Finally, we show a tight connection between evidence-based rankings and multi-calibrated predictors [HKRR’18]. This connection establishes a way to reduce the task of ranking to prediction that ensures strong guarantees of fairness in the resulting ranking.<br/>
Joint work with Cynthia Dwork, Omer Reingold, Guy N. Rothblum, and Gal Yona. Appeared at FOCS 2019.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2020-05-28T23:18:00Z</updated>
    <published>2020-05-28T23:18:00Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-06-02T07:21:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5500</id>
    <link href="https://adamsheffer.wordpress.com/2020/05/28/polymath-reu/" rel="alternate" type="text/html"/>
    <title>Polymath REU</title>
    <summary>I am excited to announce a new program for undergraduates: The Polymath REU will run during the summer of 2020. Due to the pandemic, many students are stuck at home without a summer program. The aim of the Polymath REU program is to provide research opportunities for such students. The program consists of research projects […]</summary>
    <updated>2020-05-28T04:02:53Z</updated>
    <published>2020-05-28T04:02:53Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2020-06-02T07:21:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=749</id>
    <link href="https://emanueleviola.wordpress.com/2020/05/26/fast-stuff/" rel="alternate" type="text/html"/>
    <title>Fast stuff</title>
    <summary>5-second UFC knockout Bullet chess</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>5-second UFC <a href="https://www.youtube.com/watch?v=_rCTmFxQ4pM">knockout</a></p>



<p><a href="https://www.youtube.com/watch?v=gqyAc4VnObc">Bullet chess</a></p></div>
    </content>
    <updated>2020-05-26T12:37:20Z</updated>
    <published>2020-05-26T12:37:20Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2020-06-02T07:21:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4816</id>
    <link href="https://www.scottaaronson.com/blog/?p=4816" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4816#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4816" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Collapsing Leviathan</title>
    <summary xml:lang="en-US">I was seriously depressed for the last week, by noticeably more than my baseline amount for the new pandemic-ravaged world. The depression seems to have been triggered by two pieces of news: The US Food and Drug Administration—yes, the same FDA whose failure to approve covid tests in February infamously set the stage for the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>I was seriously depressed for the last week, by noticeably more than my baseline amount for the new pandemic-ravaged world.  The depression seems to have been triggered by two pieces of news:</p>



<ol><li>The US Food and Drug Administration—yes, the same FDA whose failure to approve covid tests in February infamously set the stage for the deaths of 100,000 Americans—has now <em>also</em> <a href="https://www.nytimes.com/2020/05/15/us/coronavirus-testing-seattle-bill-gates.html">banned</a> the Gates Foundation’s program for at-home covid testing.  This, it seems to me, is not the sort of thing that could happen in a still-functioning society, one where people valued their own and their neighbors’ physical survival, and viewed rules and regulations as merely instruments to that end.  It’s the sort of thing that one imagines in the waning years of a doomed empire, when no one pretends anymore that they can fix or improve the Leviathan; they’re all just scurrying to flee the Leviathan as it collapses with a thud.  More broadly, I <em>still</em> don’t think that the depth of America’s humiliation and downfall has sunk in to most Americans.  For me, it starts and ends with a single observation: <strong>where fifty years ago we landed humans on the moon, today we can no longer make or distribute paper masks, even when hundreds of thousands of lives depend on it.</strong>  Look, there are many countries, like Taiwan and New Zealand, that managed to protect both their economies <em>and</em> their vulnerable citizens’ lives, by crushing the virus early.  Then there are countries that waited, until they faced an excruciating choice between the two.  But here in the US, we’ve somehow achieved the worst of both worlds—triggering a second Great Depression while <em>also</em> utterly failing to control the virus.  Can we abandon the charade of treating this as a legible “policy choice,” to be debated in earnest thinkpieces?  To me, it just feels like the death-spasm of a collapsing Leviathan.</li><li>Something that, at first glance, might seem trivial by comparison, but isn’t: the University of California system—ignoring the <a href="https://senate.universityofcalifornia.edu/_files/reports/kkb-jn-standardized-testing.pdf">advice</a> of its own Academic Senate, and at the apparent insistence of its chancellor Janet Napolitano—will now <a href="https://timesofsandiego.com/education/2020/05/24/the-story-behind-university-of-californias-landmark-decision-to-ditch-the-sat/">permanently end</a> the use of the SAT and ACT in undergraduate admissions.  This is widely expected, probably correctly, to trigger a chain reaction, whereby one US university after the next will abandon standardized tests.  As a result, admissions to the top US universities—and hence, most chances for social advancement in the US—will henceforth be based <em>entirely</em> on shifting and nebulous criteria that rich, well-connected kids and their parents spend most of their lives figuring out, rather than merely <em>mostly</em> based on such criteria.  The last side door for smart noncomformist kids is now being slammed shut.  From now on, in the US, the <em>only</em> paths to success that clearly delineate their rules will be sports, gambling, reality TV, and the like.  In case it matters to anyone reading this, I feel certain that a 15-year-old me wouldn’t stand a chance in the emerging regime—any more than nerdy Jewish kids did in the USSR of the 1970s, or the US of the 1920s.  (As I’ve <a href="https://www.scottaaronson.com/blog/?p=2003">previously recounted</a> on this blog, the US’s “holistic” college admissions system, with its baffling-to-foreigners emphasis on “character,” “leadership,” “well-roundedness,” etc. rather than test scores, originated in a successful push a century ago by the presidents of Harvard, Princeton, and Yale to keep Jewish enrollments down.  Today the system fulfills precisely the same function, except against Asian-Americans rather than Jews.)  Ironically but predictably, the death of the SAT—i.e., of one of the most fearsome weapons against entrenched wealth and power ever devised—is being <em>celebrated</em> by the self-described champions of the underdog.  I have one question for those champions: do you not understand what your system will <em>actually</em> do to society’s underdogs?  Or do you understand perfectly well, and approve?</li></ol>



<p>To put it bluntly—since events like these leave no room for euphemism—a hundred thousand Americans are now dead from covid, and hundreds of thousands more are poised to die, because smart people are no longer in charge.  And the death of the SAT will help ensure that smart people will never be <em>back</em> in charge.  Obama might be remembered by history as America’s last smart-person-in-charge, its last competent technocrat—but one man couldn’t stop a tidal wave of stupid.</p>



<p>I know from experience what many will readers will say to all this: “instead of wallowing in gloom, Scott, why don’t you just make falsifiable predictions about the bad outcomes you expect from these developments, and then score yourself later?”</p>



<p>So here’s the thing about that.</p>



<p>Shortly after Trump was elected, I changed this blog’s background to black, as a small way to mourn the United States that I’d grown up thinking that I lived in, the one that had at least some ideals.  Today, with four years of hindsight, my thinking then feels <em>overly optimistic</em>: why plain black?  Why not, like, images of rotting corpses in a pit?</p>



<p>And yet, were I foolish enough to register predictions in 2016, I would’ve said that within one year, Trump’s staggering incompetence would <em>surely</em> cause some catastrophe or other to grip the country—a really obvious one, with mass death and even Trump’s beloved stock market cratering.</p>



<p>And then after a year, commenters would ridicule me, because none of that had happened.  After two years, they’d ridicule me again because it <em>still</em> hadn’t happened, and after three years they’d ridicule me a third time.</p>



<p>Now it’s happened.</p>



<p>America, we now know, is like the cartoon character who runs off a cliff: it dangled in midair for three years, defying physics, before it finally looked down.</p>



<p>Look, I’m a theoretical computer scientist.  By training, I deal in asymptotics, not in constant factors.  I don’t often make predictions with deadlines; when I do, I often regret it.  It’s a good thing that I became an academic rather than an investor!  For I’ve learned that the only “oracular power” I have is to make statements like:</p>



<blockquote class="wp-block-quote"><p>My eyes, my brain, and the pit of my stomach are all blaring at me that the asymptotics of this situation just took a sharp turn for the worse.  Sure, for an unknown length of time, noise and constant factors could mask the effects.  But eventually, either (1) society will need to reverse what it just did, or else (2) terrible effects will spring from it, or else (3) the entire universe no longer makes sense.</p></blockquote>



<p>When I’ve felt this way in the past, option (3) rarely turned out to be the right answer.</p>



<p>So, what can anyone say that will make me less depressed?  Thanks in advance!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (May 30): </span></strong>Woohoo!!  Avoiding yet another tragedy, after years of setbacks and struggles, it looks like today the US has finally launched humans into orbit, thereby recapitulating a technological achievement from 1961 that the US had already vastly surpassed by 1969.  I hereby retract the pessimism of this post.</p></div>
    </content>
    <updated>2020-05-26T12:12:41Z</updated>
    <published>2020-05-26T12:12:41Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Self-Referential"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-06-01T19:00:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3492</id>
    <link href="https://agtb.wordpress.com/2020/05/25/guide-to-virtual-ec-2020/" rel="alternate" type="text/html"/>
    <title>Guide to Virtual EC 2020</title>
    <summary>From the Virtual Transition Team for EC 2020: Due to concerns regarding the novel coronavirus COVID-19, the 2020 ACM Conference on Economics and Computation (EC 2020) will be held virtually.  This change of format offers exciting new opportunities. This guide can also be found on the EC 2020 webpage. Overview June 15 – 19: Mentoring […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="font-weight: 400;"><i>From the Virtual Transition Team for EC 2020:</i> Due to concerns regarding the novel coronavirus COVID-19, the 2020 ACM Conference on Economics and Computation (EC 2020) will be held virtually.  This change of format offers exciting new opportunities. This guide can also be found on the </span><a href="http://ec20.sigecom.org/participation/covid/"><span style="font-weight: 400;">EC 2020 webpage</span></a><span style="font-weight: 400;">.</span></p>
<h2><span style="font-weight: 400;">Overview</span></h2>
<p><b>June 15 – 19:</b><span style="font-weight: 400;"> Mentoring Workshop and Live Tutorial Pre-recording Sessions.</span><br/>
<b>June 22 – July 3:</b><span style="font-weight: 400;"> Live EC Paper Pre-recording Plenary Sessions.</span><br/>
<b>July 13:</b><span style="font-weight: 400;"> Tutorial Watch Parties, Business Meeting and Poster Session</span><br/>
<b>July 14 – 16:</b><span style="font-weight: 400;"> EC Conference (Paper Watch Parties, Paper Poster Sessions, and Plenaries).</span><br/>
<b>July 17 – 22:</b><span style="font-weight: 400;"> Workshops.</span></p>
<h2><span style="font-weight: 400;">Philosophy</span></h2>
<p><span style="font-weight: 400;">The planning committee has been hard at work revisioning EC 2020 as a virtual event.  The event aims to emphasize opportunities afforded by the virtual format with activities that are intractable in the physical format, but not to recreate aspects of the physical conference experience that are difficult virtually.  Some aspects of the event will look similar to a physical conference, while some will be quite different.  The desiderata for the selected format are:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Maximize exposure for EC papers.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Maximize interaction between community members.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Allow broad global accessibility.</span></li>
</ul>
<p><span style="font-weight: 400;">The organizational team studied the results of many virtual conferences that have already taken place (</span><a href="https://aamas2020.conference.auckland.ac.nz/"><span style="font-weight: 400;">AAMAS</span></a><span style="font-weight: 400;">, </span><a href="https://cacm.acm.org/blogs/blog-cacm/243882-the-asplos-2020-online-conference-experience/fulltext"><span style="font-weight: 400;">ASPLOS</span></a><span style="font-weight: 400;">, </span><a href="https://www.eurosys2020.org/"><span style="font-weight: 400;">EuroSys</span></a><span style="font-weight: 400;">, </span><a href="https://iccp2020.engr.wustl.edu/vsetup.html?fbclid=IwAR3ONLEftxPsTmW4b1oTyCjvAFWWOxWRWS4bV6kTox1yP8AUpgSAbkL76Qc"><span style="font-weight: 400;">ICCP</span></a><span style="font-weight: 400;">, </span><a href="https://medium.com/@iclr_conf/gone-virtual-lessons-from-iclr2020-1743ce6164a3"><span style="font-weight: 400;">ICLR</span></a><span style="font-weight: 400;">, </span><a href="https://www.daniellitt.com/blog/2020/4/20/wagon-lessons-learned"><span style="font-weight: 400;">WAGON</span></a><span style="font-weight: 400;">, among others, as well as the </span><a href="https://people.clarkson.edu/~jmatthew/acm/VirtualConferences_GuideToBestPractices_CURRENT.pdf"><span style="font-weight: 400;">ACM guidelines</span></a><span style="font-weight: 400;">).  Some general rules of thumb are (a) prerecording talks, (b) emphasizing posters, (c) expecting about four hours a day of participation, (d) prime time is 11am-3pm ET.  </span></p>
<p><span style="font-weight: 400;">The planning committee adopted a strategy that allows the minimal commitment level for authors and attendees of presenting and viewing 18 minute talks and attending activities only during the main EC week of July 13-17.  However, official EC events will run June 15 to July 24 and the planning committee highly encourages members of the community to enjoy a more relaxed pace where all programming is plenary.</span></p>
<h2><span style="font-weight: 400;">Registration</span></h2>
<p><span style="font-weight: 400;">Registration will be mandatory but complementary with ACM SIGecom membership ($10 Professional / $5 Student) which can easily be completed online.  Registration will open on June 1.</span></p>
<h2><span style="font-weight: 400;">Mentoring Workshop and Live Tutorial Pre-recording Sessions</span></h2>
<p><b>June 15-19:</b><span style="font-weight: 400;"> To give junior researchers plenty of time to prepare for the EC conference, the mentoring workshops and tutorial live pre-recording sessions will be held the week of June 15.  In addition to the usual activities, junior students will be paired with senior students who will share their EC itinerary and be available for discussions of papers and events in online chat throughout the duration of the EC events.  There will be three tutorials, each broken into four 45 minute segments and recorded over several days and with live audiences of EC participants.  Students will be able to work together on exercises in between sessions.</span></p>
<h2><span style="font-weight: 400;">Live Paper Pre-recording Plenary Sessions</span></h2>
<p><b>June 22 – July 3:</b><span style="font-weight: 400;">  EC papers will keep the usual 18-minute format.  The planning committee highly encourages authors to choose to pre-record their EC talks in live pre-recording sessions.  Each session comprises three papers and is followed by a virtual coffee break for discussion between the speakers, coauthors, and attendees.  Speakers and attendees are recommended to schedule for 2 hours.  These sessions are all plenary and will be scheduled for synergies in topic and preferred timing of the speakers.  These sessions begin at regular times:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Eastern/China 9:00, 13:00, 17:00, 21:00, 1:00, 5:00.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Central/Israel  8:00, 12:00, 16:00, 20:00, 24:00, 4:00.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Mountain/Europe 7:00, 11:00, 15:00, 19:00, 23:00, 3:00.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Pacific/London  6:00, 10:00, 14:00, 18:00, 22:00, 2:00.</span></li>
</ul>
<p><span style="font-weight: 400;">We expect about three sessions a day over the ten weekdays between June 22 and July 3.  (Pre-recording sessions are optional but highly encouraged for authors of EC papers.)</span></p>
<h2><span style="font-weight: 400;">EC Conference</span></h2>
<p><b>July 13-16: </b><span style="font-weight: 400;">July 13 is tutorial day.  It will include the EC business meeting and a contributed poster session for breaking results and results from other venues.  The main conference runs July 14-July 16 with programming from 9am-5pm Eastern.  This programming includes pre-recorded talk watch parties, poster discussion sessions for the papers, live plenary talks, and live highlights beyond EC.  The paper and poster discussion sessions will run in two-hour blocks as follows:</span></p>
<p><span style="font-weight: 400;">Hour 1: 3-5 parallel tracks of watch parties (3 papers each), with realtime chat with the authors.</span></p>
<p><span style="font-weight: 400;">Hour 2: plenary 1-minute lightning talks for all papers, breakout room for discussion for each paper with poster (a poster is 1-4 slides in PDF). (The lightning talks and poster discussion rooms are optional but highly encouraged for authors of the session’s papers.)</span></p>
<p><span style="font-weight: 400;">As these watch parties will be in parallel sessions and some may be at times that are hard for some members of the community to attend, the planning committee highly encourages participation in the live pre-recording sessions (June 22-July 3) and all talks will be available for individual viewing in advance of the conference. </span></p>
<h2><span style="font-weight: 400;">EC Workshops</span></h2>
<p><b>July 17-22:</b><span style="font-weight: 400;"> Three workshops will take place on the Friday of EC and the following week.  Details for these events are still being worked out by the organizers.</span></p>
<h2><span style="font-weight: 400;">Best Presentation by Student or Postdoctoral Research Award.</span></h2>
<p><span style="font-weight: 400;">The</span><a href="http://www.sigecom.org/award-presentation.html"><span style="font-weight: 400;"> Best Presentation by a Student or Postdoctoral Researcher Award</span></a><span style="font-weight: 400;"> is designed to encourage and acknowledge excellence in oral presentations by students and recent graduates. In addition to an honorarium, this year’s winner will be invited to present in a special session at WINE 2020 with expenses covered by SIGecom. </span><span style="font-weight: 400;">To be considered for the award, the presenter must participate in a live pre-recording session. </span><span style="font-weight: 400;"> This year will feature several “fun” categories for video presentations. Audience nominations are requested for creative and fun videos in the categories of: Best Video, Best Cameo by a Child or Pet, Best Special Effects, Best Soundtrack, or Best [Insert Your Nomination Here]. Submit nominations </span><a href="https://forms.gle/bfWEqDwseGHnciUP6"><span style="font-weight: 400;">here</span></a><span style="font-weight: 400;">.</span></p>
<h2><span style="font-weight: 400;">Virtual Transition Team</span></h2>
<p><span style="font-weight: 400;">Virtual aspects of the conference are being coordinated by a Virtual Transition Team appointed by the SIGecom Executive Committee, that in addition to the </span><a href="http://sigecom.org/officers.html"><span style="font-weight: 400;">Executive Committee</span></a><span style="font-weight: 400;"> and the </span><a href="http://ec20.sigecom.org/committees-acm/organizing-committee/"><span style="font-weight: 400;">EC 2020 organizing committee</span></a><span style="font-weight: 400;"> includes the following officers:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Virtual General Chair: Jason Hartline</span></li>
</ul>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Virtual Local Chair: Yannai Gonczarowski</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Virtual Global Outreach Chairs: Rediet Abebe and Eric Sodomka</span></li>
</ul>
<p><span style="font-weight: 400;">The team is looking forward to interacting with everyone in the community at an outstanding conference!</span></p></div>
    </content>
    <updated>2020-05-25T21:37:35Z</updated>
    <published>2020-05-25T21:37:35Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Jason Hartline</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2020-06-02T07:20:45Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1531592307894359428</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1531592307894359428/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/oldest-living-baseball-players-can-you.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1531592307894359428" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1531592307894359428" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/oldest-living-baseball-players-can-you.html" rel="alternate" type="text/html"/>
    <title>Oldest Living Baseball Players- can you estimate...</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">(The Baseball season is delayed or cancelled, so I post about baseball instead.)<br/>
<br/>
This post is going to ask a question that you could look up on the web. But what fun with that be?<br/>
<br/>
The following statements are true<br/>
<br/>
1) Don Larsen, a professional baseball player who played from 1953 to 1967, is still alive. He is 90 years old (or perhaps 90 years young---I don't know the state of his health).  He was born Aug 7, 1929. He is best know for pitching a perfect game in the World Series in 1956, pitching for the Yankees. He played for several other teams as well, via trades (this was before free agency).<br/>
(CORRECTION- I wrote this post a while back, and Don Larsen has died since then.)<br/>
<br/>
<br/>
2) Whitey Ford, a professional baseball player who played from 1950 to 1967, is still alive. He is 91 years old (or perhaps 91 years young---I don't know the state of his health).  He was born Oct 21,  1928. He had many great seasons and is in the hall of fame. He played for the New York Yankees and no other team.<br/>
<br/>
3) From 1900 (or so) until 1962 there were 16 professional baseball teams which had 25 people each. From 1962 until 1969 there were 20 teams which had 25 people each. There were also many minor league teams.<br/>
<br/>
4) The youngest ballplayers are usually around 20. The oldest around 35. These are not exact numbers<br/>
<br/>
SO here is my question: Try to estimate<br/>
<br/>
1) How many LIVING  retired major league baseball players are there now who are older than Don Larsen?<br/>
<br/>
2) How many LIVING retired major league baseball players are of an age between Don and Whitey?<br/>
<br/>
3) How  many LIVING retired major league baseball players are older than Whitey Ford?<br/>
<br/>
Give your REASONING for your answer.<br/>
<br/></div>
    </content>
    <updated>2020-05-25T16:34:00Z</updated>
    <published>2020-05-25T16:34:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-06-02T05:48:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17076</id>
    <link href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/" rel="alternate" type="text/html"/>
    <title>Proof of the Diagonal Lemma in Logic</title>
    <summary>Why is the proof so short yet so difficult? Saeed Salehi is a logician at the University of Tabriz in Iran. Three years ago he gave a presentation at a Moscow workshop on proofs of the diagonal lemma. Today I thought I would discuss the famous diagonal lemma. The lemma is related to Georg Cantor’s […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Why is the proof so short yet so difficult?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/saeed_salehi4/" rel="attachment wp-att-17078"><img alt="" class="alignright  wp-image-17078" src="https://rjlipton.files.wordpress.com/2020/05/saeed_salehi4.jpg?w=200" width="200"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Saeed Salehi is a logician at the University of Tabriz in Iran. Three years ago he gave a <a href="http://wrm17.mi.ras.ru/slides/Salehi.pdf">presentation</a> at a Moscow workshop on proofs of the diagonal lemma.</p>
<p>
Today I thought I would discuss the famous <a href="https://en.wikipedia.org/wiki/Diagonal_lemma">diagonal lemma</a>. </p>
<p>
The lemma is related to Georg Cantor’s famous diagonal argument yet is different. The logical version imposes requirements on when the argument applies, and requires that it be expressible within a formal system. </p>
<p>
The lemma underpins Kurt Gödel’s famous 1931 <a href="http://www.w-k-essler.de/pdfs/goedel.pdf">proof</a> that arithmetic is incomplete. However, Gödel did not state it as a lemma or proposition or theorem or anything else. Instead, he focused his attention on what we now call Gödel numbering. We consider this today as “obvious” but his paper’s title ended with “Part I”. And he had readied a “Part II” with over 100 pages of calculations should people question that his numbering scheme was expressible within the logic. </p>
<p>
Only after his proof was understood did people realize that one part, perhaps the trickiest part, could be abstracted into a powerful lemma. The tricky part is <em>not</em> the Gödel numbering. People granted that it can be brought within the logic once they saw enough of Gödel’s evidence, and so we may write <img alt="{\ulcorner \phi \urcorner}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Culcorner+%5Cphi+%5Curcorner%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ulcorner \phi \urcorner}"/> for the function giving the Gödel number of any formula <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> and use that in other formulas. The hard part is what one <em>does</em> with such expressions. </p>
<p>
This is what we will try to motivate.</p>
<p>
</p><p/><h2> Tracing the Lemma </h2><p/>
<p/><p>
Rudolf Carnap is often credited with the first formal statement, in 1934, for instance by Eliott Mendelson in his famous <a href="https://www.goodreads.com/book/show/250868.Introduction_to_Mathematical_Logic">textbook</a> on logic. Carnap was a <a href="https://en.wikipedia.org/wiki/Rudolf_Carnap">member</a> of the <a href="https://en.wikipedia.org/wiki/Vienna_Circle">Vienna Circle</a>, which Gödel frequented, and Carnap is <a href="http://texts.cdlib.org/view?docId=hb6h4nb3q7&amp;doc.view=frames&amp;chunk.id=div00004&amp;toc.depth=1&amp;toc.id=">considered</a> a giant among twentieth-century philosophers. He worked on sweeping grand problems of philosophy, including logical positivism and analysis of human language via syntax before semantics. Yet it strikes us with irony that his work on the lemma may be the best remembered.</p>
<p>
Who did the lemma first? Let’s leave that for others and move on to the mystery of how to prove the lemma once it is stated. I must say the lemma is easy to state, easy to remember, and has a short proof. But I believe that the proof is not easy to remember or even follow. </p>
<p>
Salehi’s <a href="http://wrm17.mi.ras.ru/slides/Salehi.pdf">presentation</a> quotes others’ opinions about the proof:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Sam Buss: “Its proof [is] quite simple but rather tricky and difficult to conceptualize.”</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> György Serény (we jump to Serény’s <a href="https://arxiv.org/pdf/math/0606425.pdf">paper</a>): “The proof of the lemma as it is presented in textbooks on logic is not self-evident to say the least.”</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Wayne Wasserman: “It is `Pulling a Rabbit Out of the Hat’—Typical Diagonal Lemma Proofs Beg the Question.”</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/hat/" rel="attachment wp-att-17079"><img alt="" class="aligncenter size-full wp-image-17079" src="https://rjlipton.files.wordpress.com/2020/05/hat.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
So I am not alone, and I thought it might be useful to try and unravel its proof. This exercise helped me and maybe it will help you.</p>
<p>
Here goes. </p>
<p>
</p><p/><h2> Stating the Lemma </h2><p/>
<p/><p>
Let <img alt="{S(w)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28w%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S(w)}"/> be a formula in Peano Arithmetic (<img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{PA}"/>). We claim that there is some sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++PA+%5Cvdash+%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). "/></p>
<p>Formally, </p>
<blockquote><p><b>Lemma 1</b> <em> Suppose that <img alt="{S(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S(x)}"/> is some formula in <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/>. Then there is a sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\phi}"/> so that 	</em></p><em>
<p align="center"><img alt="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++PA+%5Cvdash+%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
The beauty of this lemma is that it was used by Gödel and others to prove various powerful theorems. For example, the lemma quickly proves this result of Alfred Tarski:</p>
<blockquote><p><b>Theorem 2</b> <em> Suppose that <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/> is consistent. Then <i>truth</i> cannot be defined in <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/>. That is there is <b>no</b> formula <img alt="{Tr(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BTr%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{Tr(x)}"/> so that for all sentences <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\phi}"/> <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/> proves 	</em></p><em>
<p align="center"><img alt="\displaystyle  \phi \iff Tr(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff Tr(\ulcorner \phi \urcorner). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
The proof is this. Assume there is such a formula <img alt="{Tr(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BTr%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Tr(x)}"/>. Then use the diagonal lemma and get 	</p>
<p align="center"><img alt="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+%5Cneg+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner)."/></p>
<p>This shows that 	</p>
<p align="center"><img alt="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner) \iff Tr(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+%5Cneg+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29+%5Ciff+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner) \iff Tr(\ulcorner \phi \urcorner). "/></p>
<p>This is a contradiction. A short proof. </p>
<p>
</p><p/><h2> The Proof </h2><p/>
<p/><p>
The key is to define the function <img alt="{F(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(n)}"/> as follows: Suppose that <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is the Gödel number of a formula of the form <img alt="{A(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A(x)}"/> for some variable <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> then 	</p>
<p align="center"><img alt="\displaystyle  F(n) = \ulcorner A(\ulcorner A(x) \urcorner) \urcorner. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28n%29+%3D+%5Culcorner+A%28%5Culcorner+A%28x%29+%5Curcorner%29+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(n) = \ulcorner A(\ulcorner A(x) \urcorner) \urcorner. "/></p>
<p>If <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is not of this form then define <img alt="{F(n)=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(n)=0}"/>. This is a strange function, a clever function, but a perfectly fine function, It certainly maps numbers to numbers. It is certainly recursive, actually it is clearly computable in polynomial time for any reasonable Gödel numbering. Note: the function <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> does depend on the choice of the variable <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Thus, 	</p>
<p align="center"><img alt="\displaystyle  F(\ulcorner y=0 \urcorner) = \ulcorner (\ulcorner y=0 \urcorner)=0 \urcorner, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+y%3D0+%5Curcorner%29+%3D+%5Culcorner+%28%5Culcorner+y%3D0+%5Curcorner%29%3D0+%5Curcorner%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(\ulcorner y=0 \urcorner) = \ulcorner (\ulcorner y=0 \urcorner)=0 \urcorner, "/></p>
<p>and 	</p>
<p align="center"><img alt="\displaystyle  F(\ulcorner x=0 \urcorner) = \ulcorner (\ulcorner x=0 \urcorner)=0 \urcorner. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+x%3D0+%5Curcorner%29+%3D+%5Culcorner+%28%5Culcorner+x%3D0+%5Curcorner%29%3D0+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(\ulcorner x=0 \urcorner) = \ulcorner (\ulcorner x=0 \urcorner)=0 \urcorner. "/></p>
<p>	          Now we make two definitions:</p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++g%28w%29+%26%5Cequiv%26+S%28F%28w%29%29+%5C%5C++++++++%5Cphi+%26%5Cequiv%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} "/></p>
<p>
Now we compute just using the definitions of <img alt="{F, g, \phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%2C+g%2C+%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F, g, \phi}"/>:</p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)) \\           &amp;=&amp; S(\ulcorner g(\ulcorner g(x) \urcorner) \urcorner) \\               &amp;=&amp; S(\ulcorner \phi \urcorner). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++%5Cphi+%26%3D%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5C%5C+++++++++++++++++%26%3D%26+S%28F%28%5Culcorner+g%28x%29+%5Curcorner%29%29+%5C%5C+++++++++++%26%3D%26+S%28%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner%29+%5C%5C+++++++++++++++%26%3D%26+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)) \\           &amp;=&amp; S(\ulcorner g(\ulcorner g(x) \urcorner) \urcorner) \\               &amp;=&amp; S(\ulcorner \phi \urcorner). \end{array} "/></p>
<p>We are done.</p>
<p>
</p><p/><h2> But … </h2><p/>
<p/><p>
Where did this proof come from? Suppose that you forgot the proof but remember the statement of the lemma. I claim that we can then reconstruct the proof. </p>
<p>
First let’s ask: Where did the definition of the function <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> come from? Let’s see. Imagine we defined </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++g%28w%29+%26%5Cequiv%26+S%28F%28w%29%29+%5C%5C++++++++%5Cphi+%26%5Cequiv%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} "/></p>
<p>But left <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> undefined for now. Then</p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++%5Cphi+%26%3D%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5C%5C+++++++++++++++++%26%3D%26+S%28F%28%5Culcorner+g%28x%29+%5Curcorner%29%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)). \end{array} "/></p>
<p>But we want <img alt="{\phi = S(\ulcorner \phi \urcorner)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3D+S%28%5Culcorner+%5Cphi+%5Curcorner%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi = S(\ulcorner \phi \urcorner)}"/> that happens provided:</p>
<p align="center"><img alt="\displaystyle  \ulcorner g(\ulcorner g(x) \urcorner) \urcorner) = F(\ulcorner g(x) \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner%29+%3D+F%28%5Culcorner+g%28x%29+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \ulcorner g(\ulcorner g(x) \urcorner) \urcorner) = F(\ulcorner g(x) \urcorner). "/></p>
<p>This essentially gives the definition of the function <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. Pretty neat.</p>
<p>
</p><p/><h2> But but … </h2><p/>
<p/><p>
Okay where did the definition of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> and <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> come from? It is reasonable to define 	</p>
<p align="center"><img alt="\displaystyle  g(w) \equiv S(F(w)), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28w%29+%5Cequiv+S%28F%28w%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g(w) \equiv S(F(w)), "/></p>
<p>for some <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. We cannot change <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> but we can control the input to the formula <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, so let’s put a function there. Hence the definition for <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> is not unreasonable. </p>
<p>
Okay how about the definition of <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/>? Well we could argue that this is the magic step. If we are given this definition then <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> follows, by the above. I would argue that <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> is not completely surprising. The name of the lemma is after all the “diagonal” lemma. So defining <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> as the application of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> to itself is plausible.</p>
<p>
</p><p/><h2> Taking an Exam </h2><p/>
<p/><p>
Another way to think about the diagonal lemma is imagine you are taking an exam in logic. The first question is: </p>
<blockquote><p><b> </b> <em> Prove in <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/> that for any <img alt="{S(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S(x)}"/> there is a sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\phi}"/> so that 	</em></p><em>
<p align="center"><img alt="\displaystyle  \phi \iff S(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff S(\ulcorner \phi \urcorner). "/></p>
</em><p><em/>
</p></blockquote>
<p>You read the question again and think: “I wish I had studied harder, I should have not have checked Facebook last night. And then went out and <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/>” But you think let’s not panic, let’s think.</p>
<p>
Here is what you do. You say let me define 	</p>
<p align="center"><img alt="\displaystyle  g(x) = S(F(x)), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%29+%3D+S%28F%28x%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g(x) = S(F(x)), "/></p>
<p>for some <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. You recall there was a function that depends on <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, and changing the input from <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> to <img alt="{F(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(x)}"/> seems to be safe. Okay you say, now what? I need the definition of <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. Hmmm let me wait on that. I recall vaguely that <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> had a strange definition. I cannot recall it, so let me leave it for now.</p>
<p>
But you think: I need a sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/>. A sentence cannot have an unbound variable. So <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> cannot be <img alt="{g(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(x)}"/>. It could be <img alt="{g(m)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28m%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(m)}"/> for some <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/>. But what could <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> be? How about <img alt="{\ulcorner \phi \urcorner}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Culcorner+%5Cphi+%5Curcorner%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ulcorner \phi \urcorner}"/>. This makes 	</p>
<p align="center"><img alt="\displaystyle  \phi = g(\ulcorner g \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%3D+g%28%5Culcorner+g+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \phi = g(\ulcorner g \urcorner). "/></p>
<p>It is after all the diagonal lemma. Hmmm does this work. Let’s see if this works. Wait as above I get that <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> is now forced to satisfy 	</p>
<p align="center"><img alt="\displaystyle  F(\ulcorner g(x) \urcorner) = \ulcorner g(\ulcorner g(x) \urcorner) \urcorner. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+g%28x%29+%5Curcorner%29+%3D+%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(\ulcorner g(x) \urcorner) = \ulcorner g(\ulcorner g(x) \urcorner) \urcorner. "/></p>
<p>Great this works. I think this is the proof. Wonderful. Got the first question. </p>
<p>
Let’s look at the next exam question. Oh no <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Does this help? Does this unravel the mystery of the proof? Or is it still magic?</p>
<p/><p><br/>
[Fixed equation formatting]</p></font></font></div>
    </content>
    <updated>2020-05-25T15:55:22Z</updated>
    <published>2020-05-25T15:55:22Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="diagonal lemma"/>
    <category term="diagonalization"/>
    <category term="Godel"/>
    <category term="numbering"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-06-02T07:20:48Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/benchmarks/</id>
    <link href="https://gradientscience.org/benchmarks/" rel="alternate" type="text/html"/>
    <title>From ImageNet to Image Classification</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/2005.11295" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/ImageNetMultiLabel" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Data
</a>
<br/></p>

<p><i>In our <a href="https://arxiv.org/abs/2005.11295">new paper</a>, we explore how closely
the ImageNet benchmark aligns with the object recognition task it serves as a
proxy for. We find pervasive and systematic deviations of ImageNet annotations
from the ground truth, which can often be attributed to specific design choices
in the data collection pipeline. These issues indicate that ImageNet accuracy
alone might be insufficient to effectively gauge real model performance.  </i></p>

<h2 id="contextualizing-progress-on-benchmarks">Contextualizing Progress on Benchmarks</h2>

<p>Large-scale benchmarks are central to machine learning—they serve both
as concrete targets for model development, and as proxies for assessing model
performance on real-world tasks we actually care about. However, few benchmarks
are perfect, and so as our models get increasingly better at them, we must also
ask ourselves: <i>to what extent is performance on existing benchmarks
indicative of progress on the real-world tasks that motivate them?</i></p>

<p>In this post, we will explore this question of <i>benchmark-task alignment</i>
in the context of the popular
<a href="http://image-net.org/challenges/LSVRC/2012/">ImageNet object recognition dataset</a>.
Specifically, our goal is to understand how well the underlying ground truth is
captured by the dataset itself—this dataset is, after all, what we consider
to be the gold standard during model training and evaluation.</p>

<h3 id="a-sneak-peak-into-imagenet">A sneak peak into ImageNet</h3>
<p>The ImageNet dataset contains over a million images of objects from a thousand,
quite diverse classes. Like many other benchmarks of that scale, ImageNet was
not carefully curated by experts, but instead created via crowd-sourcing,
without perfect quality control. So what does ImageNet data look like? Here are
a few image-label pairs from the dataset:</p>

<div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/0.jpg"/>
      <div class="fake_label">missile</div>
      <div class="true_label">projectile</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/1.jpg"/>
      <div class="fake_label">stage</div>
      <div class="true_label">acoustic guitar</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/2.jpg"/>
      <div class="fake_label">monastery</div>
      <div class="true_label">church</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/3.jpg"/>
      <div class="fake_label">Norwich terrier</div>
      <div class="true_label">Norfolk terrier</div>
    </div>
</div>
<p><br/></p>

<p>These samples appear pretty reasonable…but are they? Actually, while these
are indeed <i>images</i> from the dataset, the labels shown above are
<i>not</i> their actual ImageNet labels!
<a href="https://gradientscience.org/" id="reveal">[Click to see the actual ImageNet labels]</a>
Still, even though not “correct” from the point of view of the ImageNet
dataset, these labels <i>do</i> correspond to actual ImageNet classes, and
appear plausible when you see them in isolation. This shows that for ImageNet
images, which capture objects in diverse real-world conditions, the ImageNet
label may not properly reflect the ground truth.</p>

<p>In our work, we dive into examining how this label misalignment actually
impacts ImageNet: how often do ImageNet labels deviate from the ground truth?
And how do shortcomings in these labels impact ImageNet-trained models?</p>

<h3 id="revisiting-the-imagenet-collection-pipeline">Revisiting the ImageNet collection pipeline</h3>
<p>Before going further, let’s take a look at how ImageNet was created. To build
such a large dataset, the creators of ImageNet had to leverage scalable methods
like automated data collection and crowd-sourcing. That is, they first selected
a set of object classes (using the <a href="https://wordnet.princeton.edu">WordNet</a>
hierarchy), and queried various search engines to obtain a pool of candidate
images. These candidate images were then verified by annotators on <a href="https://www.mturk.com/">Mechanical
Turk (MTurk)</a> using (what we will refer to as) the
<span class="sc">Contains</span> task: annotators were shown images retrieved
for a specific class
label, and were subsequently asked to select the ones that actually contain an
object of this class. Only images that multiple annotators validated ended up
in the final dataset.</p>

<div>
  <div class="stages block">
      <div class="stage rbutton block clicked" id="selection">Class Selection</div>
      <div class="stage rbutton block" id="retrieval">Image Retrieval</div>
      <div class="stage rbutton block" id="filtering">Label Validation</div>
  </div>
  <img id="stage_img" src="https://gradientscience.org/assets/multilabel/pipeline/selection.jpg"/>
</div>
<div class="footnote"> <strong>Imagenet collection pipeline:</strong> Click on a
stage at the top for an illustration. </div>

<p>While this is a natural approach to scalably annotate data (and, in fact, is
commonly used to create large-scale benchmarks—e.g.,
<a href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL VOC</a>,
<a href="http://cocodataset.org/#home">COCO</a>, <a href="http://places.csail.mit.edu/">places</a>),
it has an important caveat. Namely, this process has an inherent bias: the
annotation task itself is phrased as a leading question. ImageNet annotators
were not asked to provide an image label, but instead only to verify if a
<i>specific</i> label (predetermined by the image retrieval process) was
<i>contained</i> in an image. Annotators had no knowledge of what the other
classes in the dataset even were, or the granularity at which they were
required to make distinctions. In fact, they were explicitly instructed to
ignore clutter and obstructions.</p>

<p>Looking back at the ImageNet samples shown above, one can see how this setup
could lead to imperfect annotations. For instance, it is unclear if the average
annotator knows the differences between a “Norwich terrier” and a “Norfolk
terrier”, especially if they don’t even know that both of these (as well as 22
other terrier breeds) are valid ImageNet classes. Also, the
<span class="sc">Contains</span> task itself might be ill-suited for annotating
multi-object images—the answer to the <span class="sc">Contains</span> question would be yes for any
object in the image that corresponds to an ImageNet class. It is not unthinkable
that the same images could have made it into ImageNet under the labels “stage”
and “Norwich terrier” had they come up in the search results for those classes
instead.</p>

<p>Overall, this suggests that the labeling issues in ImageNet may go beyond just
occasional annotator mistakes—the <i>design</i> of the data collection
pipeline itself could have caused these labels to systematically deviate from
the ground truth.</p>

<h3 id="diagnosing-benchmark-task-misalignment">Diagnosing benchmark-task misalignment</h3>

<p>To characterize how wide-spread these deviations are, we first need to get a
better grasp of the ground truth for ImageNet data. In order to do this at
scale, we still need to rely on crowd-sourcing. However, in contrast to the
original label validation setup, we design a new annotation task based directly
on <i>image classification</i>. Namely, we present annotators with a set of
possible labels for a single image <i>simultaneously</i>. We then ask them to
assign one label to every object in the image, and identify what they believe
to be the main object. (Note that we intentionally ask for such fine-grained
image annotations since, as we saw before, a single label might be inherently
insufficient to capture the ground truth.)</p>

<p>Of course, we need to ensure that annotators can meaningfully perform this
task. To this end we  devise a way to narrow down the label choices they are
presented with (all thousand ImageNet classes would be nearly impossible for a
worker to choose between!). Specifically, for each image, we identify the most
relevant labels by pooling together the top-5 predictions of a diverse set of
ImageNet models and filtering them via the <span class="sc">Contains</span>
task. Note that, by doing so, we are effectively bootstrapping the existing
ImageNet labels by first using them to train models and then using model
predictions to get better annotation candidates.</p>

<p>This is what our resulting annotation task looks like:</p>

<p><img src="https://gradientscience.org/assets/multilabel/main_task.jpg"/></p>

<p>We aggregate the responses from multiple annotators to get per-image estimates
of the number of objects in the image (along with their corresponding labels),
as well as which object humans tend to view as the main one.</p>

<p>We collect such <a href="https://github.com/MadryLab/ImageNetMultiLabel">annotations for 10k images from the ImageNet validation
set</a>. With these more
fine-grained and accurate annotations in hand, we now examine where the
original ImageNet labels may fall short.</p>

<h3 id="multi-object-images">Multi-object images</h3>
<p>The simplest way in which ImageNet labels could deviate from the ground truth
is if the image contains multiple objects. So, the first thing we want to
understand is: how many ImageNet images contain objects from more than one
valid class?</p>

<p>It turns out: quite a few! Indeed, more than <b>20%</b> of the images contain
more than one ImageNet object. Examples:</p>

<div class="widget">
  <div class="choices_one">
    <span class="widgetheading">Inspect Image</span>
  </div>
  <div class="choices_img">
    <span class="widgetheading">Image</span>
  </div>
  <div class="choices_info block">
    <span class="widgetheading">Annotation</span>
  </div>
  <div class="choices_one" id="multi"> </div>
  <div class="choices_img widgetheading"> <img id="multi1"/> </div>
  <div class="choices_info block">
    <div class="choices_info_text" id="multiclass"> </div>
  </div>
</div>
<div style="clear: both;"/>
<div class="footnote"> <strong>Multi-object images:</strong> Choose an image on
the left to see the annotations we obtain for it.  </div>

<p>Looking at some of these images, it is clear that the problem is not just
natural image clutter but also the fact that certain objects are quite likely
to co-occur in the real-world—e.g., “table lamp” and “lamp shade”. This means
that choosing classes which in principle correspond to distinct objects (e.g.,
using WordNet) is not enough to guarantee that the corresponding images have
unambiguous labels. For example, see if you can guess the ImageNet label for
the samples below:</p>

<div class="widget">
  <span class="widgetheading" id="coclass">Chosen class pair</span>
  <div class="choices_one_full" id="co">
  <div class="show_labels rbutton block" id="cooc">Show Labels</div>
  </div>
  <div id="coimages" style="border-right: 10px white solid;"> </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Co-occurring objects:</strong> Select a class pair on the top and see if
you can identify which images correspond to each class (click on "Show Labels"
to reveal the answers).
</div>

<h4 id="model-performance-on-multi-object-images">Model performance on multi-object images</h4>

<p>So, how do models deal with images that contain multiple objects? To understand
this, we evaluate a number of models (from AlexNet to EfficientNet-B7), and
measure their accuracy (w.r.t. the ImageNet labels) on such images. We plot
these accuracies below (as a function of their full test accuracy):</p>

<canvas height="200" id="multi_drop" width="400"/>
<div class="footnote">
<strong>Performance on multi-object images:</strong>
Model accuracy variation as a function of the number of objects in the image.
Hover over each data point to see the corresponding model name.
</div>

<p>Across the board, in comparison to their performance on single-object images,
models suffer around a 10% accuracy drop on multi-object ones. At the same
time, this drop more-or-less disappears if we consider a model prediction to be
correct if it matches the label of <i>any</i> valid object in the image (see
the <a href="https://arxiv.org/abs/2005.11295">paper</a> for specifics).</p>

<div class="footnote">
    <strong>Aside:</strong> The original motivation of top-5 accuracy was
    exactly to accommodate such multi-label images. However, we find that, while
    it does largely account for these multi-object confusions, it also seems to
    overestimate accuracy on single-object images.
</div>

<p>Still, even though models seem to struggle with multi-object images, they
perform much better than chance (i.e., better than what one would get if they
were picking the label of an object in the image at random). This makes sense
when the image has a single prominent object that also matches the ImageNet
label. However, for a third of all multi-object images the ImageNet label does
not even match what annotators deem to be the main object in the image. Yet,
even in these cases, models still successfully predict the ImageNet label
(instead of what humans consider to be the right label for the image)!</p>

<canvas height="200" id="overfitting" width="400"/>
<div class="footnote">
<strong>Performance on images with annotator-label disagreement:</strong>
Model accuracy on images where annotators do not consider the ImageNet label to
be the main object. Baseline: accuracy of picking the label of a random
prominent object in the image.
</div>

<p>Here, models seem to base their predictions on biases in the dataset which
humans do not find salient. For instance, models get high accuracy on the class
“pickelhaube”, even though, pickelhaubes are usually present in images with
other, more salient objects, such as “military uniforms”, suggesting that
ImageNet models may be overly sensitive to the presence of distinctive objects
in the image. While exploiting such biases would improve ImageNet accuracy,
this strategy might not translate to improved performance on object recognition
in the wild. Here are a few examples that seem to exhibit a similar mismatch:</p>

<div class="widget">
  <div class="choices_one">
    <span class="widgetheading">Inspect Image</span>
  </div>
  <div class="choices_img">
    <span class="widgetheading">Image</span>
  </div>
  <div class="choices_info block">
    <span class="widgetheading">Annotation</span>
  </div>
  <div class="choices_one" id="main"> </div>
  <div class="choices_img widgetheading"> <img id="main1"/> </div>
  <div class="choices_info block">
    <div class="choices_info_text" id="mainclass"> </div>
  </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Annotator-label disagreement:</strong> Select an image on the left to
see the annotations we obtain for it. Notice that the ImageNet label is
different from the (annotator-selected) "main label".
</div>

<h3 id="biases-in-label-validation">Biases in label validation</h3>

<p>Let us now turn our attention to the ImageNet data filtering process. Recall
that each class in ImageNet was constructed by automatically retrieving many
images and filtering them (via the <span class="sc">Contains</span> task described above). How likely
were annotators to filter out mislabeled images under this setup?</p>

<p>To understand this, we replicate the original filtering process on the existing
ImageNet images. But this time, instead of only asking annotators to check if
the image is valid with respect to its ImageNet label (i.e., the search query),
we also try several other labels (each in isolation, with different sets of
annotators).</p>

<p>We find that annotators frequently deem an image to be valid for <i>many</i>
different labels—even when only one object is present. Typically, this occurs
when the image is ambiguous and lacks enough context (e.g. “seashore” or
“lakeshore”), or annotators are likely confused between different semantically
similar labels (e.g., “assault rifle” vs. “rifle”, dog breeds). It turns out
that this confusion, at least partly, stems from the one-sidedness of the
<span class="sc">Contains</span> task—i.e., asking annotators to ascertain the validity of a specific
label without them knowing about any other options. If instead we present
annotators with all the relevant labels simultaneously and ask them to choose
one (as we did in our annotation setup), this kind of label confusion is
alleviated: annotators select significantly fewer labels in total (see our
<a href="https://arxiv.org/abs/2005.11295">paper</a> for details). So, even putting
annotator’s expertise aside, the specific annotation task setup itself
drastically affects the quality of the resulting dataset labels.</p>

<div>
  <div class="dropdown">
    <div class="rates block" id="dropdownMenuButton">
      Choose annotator threshold
    </div>
    <div class="rates">
      <div class="block rbutton clicked">10%</div>
      <div class="block rbutton">30%</div>
      <div class="block rbutton">50%</div>
    </div>
  </div>
  <img id="filtering_plot" src="https://gradientscience.org/assets/multilabel/filtering/filtering_0.1.jpg"/>
</div>
<div class="footnote">
<strong>Number of valid labels:</strong> Distribution of labels annotators deem
valid (based on an agreement threshold) for single-object images in the <span class="sc">Contains</span>
task. Click the percentages on the top to change the annotator agreement
threshold.
</div>

<p>Going back to ImageNet, our findings give us reason to believe that annotators
may have had a rather limited ability to correct errors in labeling. Thus, in
certain cases, ImageNet labels were largely determined by the automated image
retrieval process—propagating any biases or mixups this process might
introduce to the final dataset.</p>

<p>In fact, we can actually see direct evidence of that in the ImageNet
dataset—there are pairs of classes that appear to be <i>inherently
ambiguous</i> (e.g., “laptop computer” and “notebook computer”) and neither
human annotators, nor models, can tell the corresponding images apart (see
below). If such class pairs actually overlap in terms of their ImageNet images,
it is unclear how models can learn to separate them without memorizing specific
validation examples.</p>

<div class="widget">
  <span class="widgetheading" id="ambclass">Chosen class pair</span>
  <div class="choices_one_full" id="am">
  <div class="show_labels rbutton block" id="amb">Show Labels</div>
  </div>
  <div id="ambimages" style="border-right: 10px white solid;"> </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Ambiguous class pairs:</strong> Select a class pair on the top and see if
you can identify which images correspond to each class (click on "Show Labels"
to reveal the answers).
</div>

<h3 id="beyond-test-accuracy-human-centric-model-evaluation">Beyond test accuracy: human-centric model evaluation</h3>

<p>Performance of ImageNet-trained models is typically judged based on their
ability to predict the dataset labels—yet, as we saw above, these labels may
not fully capture the ground truth. Hence, ImageNet accuracy may not reflect
properly model performance—for instance, measuring accuracy alone could
unfairly penalize models for certain correct predictions on  multi-object
images. So, how can we better assess model performance?</p>

<p>One approach is to measure model-human alignment directly—we present model
predictions to annotators and ask them to gauge their validity:</p>

<canvas height="200" id="limit" width="400"/>
<div class="footnote">
<strong>Human-based model evaluation:</strong> Fraction of annotators that
select a label as valid in the <span class="sc">Contains</span> task (i.e.,
selection frequency) for ImageNet labels and model predictions. Baseline:
(number of correct predictions) × (average selection frequency of the
ImageNet label).
</div>

<p>Surprisingly, we find that for state-of-the-art models, annotators actually
deem the prediction that models make to be valid about as often as the ImageNet
label (even when the two <i> do not</i> match). Thus, recent models may be
better at predicting the ground truth than their top-1 accuracy (w.r.t. the
ImageNet label) would indicate.</p>

<p>However, this does not imply that improving ImageNet accuracy is meaningless.
For instance, non-expert annotators may not be able to tell apart certain
fine-grained class differences (e.g., dog breeds) and for some of these images
the ImageNet label may actually match the ground truth. What it does indicate,
though, is that we are at a point where it may be hard to gauge if better
performance on ImageNet corresponds to actual progress or merely to exploiting
idiosyncrasies of the dataset.</p>

<p>For further experimental details and additional results (e.g., human confusion
matrices), take a look at <a href="https://arxiv.org/abs/2005.11295">our paper</a>!</p>

<h3 id="conclusions">Conclusions</h3>

<p>We took a closer look at how well ImageNet aligns with the real-world object
recognition task—even though ImageNet is used extensively, we rarely question
whether its labels actually reflect the ground truth. We saw that oftentimes
ImageNet labels do not fully capture image content—e.g., many images have
multiple (ImageNet) objects and there are classes that are inherently
ambiguous. As a result, models trained using these labels as ground truth end
up learning unintended biases and confusions.</p>

<p>Our analysis indicates that when creating datasets we must be aware of (and try
to mitigate) ways in which scalable data collection practices can skew the
corresponding annotations (see our
<a href="https://gradientscience.org/data_rep_bias">previous post</a> for another
example of such a skew). Finally, given that such imperfections in our datasets
could be inevitable, we also need to think about how to reliably assess model
performance in their presence.</p>





















<b/><br/><hr/><b/><br/><br/><br/><b/><br/><br/><div class="cooc_img block"><div class="image_label label_cooc"><br/></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg"/></div><div class="amb_img block"><div class="image_label label_amb"><br/></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg"/></div><div class="bias_img block"><div class="image_label label_bias"><br/></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg"/></div></div>
    </summary>
    <updated>2020-05-25T00:00:00Z</updated>
    <published>2020-05-25T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2020-06-01T22:31:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Vienna, TU Vienna, IST Austria, WU Vienna (apply by June 15, 2020)</title>
    <summary>The Vienna Graduate School on Computational Optimization (VGSCO) is a research and training program funded by the Austrian Science Funds (FWF). The VGSCO offers lecture series given by international experts in optimization related fields, organizes research seminars, retreats, soft skills courses, scientific workshops, and social events, provides travel grants, and supports research stays abroad. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Vienna Graduate School on Computational Optimization (VGSCO) is a research and training program funded by the Austrian Science Funds (FWF). The VGSCO offers lecture series given by international experts in optimization related fields, organizes research seminars, retreats, soft skills courses, scientific workshops, and social events, provides travel grants, and supports research stays abroad.</p>
<p>Website: <a href="http://vgsco.univie.ac.at/positions">http://vgsco.univie.ac.at/positions</a><br/>
Email: vgsco@univie.ac.at</p></div>
    </content>
    <updated>2020-05-24T08:48:14Z</updated>
    <published>2020-05-24T08:48:14Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-02T07:20:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/082</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/082" rel="alternate" type="text/html"/>
    <title>TR20-082 |  MaxSAT Resolution and Subcube Sums | 

	Yuval Filmus, 

	Meena Mahajan, 

	Gaurav Sood, 

	Marc Vinyals</title>
    <summary>We study the MaxRes rule in the context of certifying unsatisfiability. We show that it can be exponentially more powerful than tree-like resolution, and when augmented with weakening (the system MaxResW), p-simulates tree-like resolution. In devising a lower bound technique specific to MaxRes (and not merely inheriting lower bounds from Res), we define a new semialgebraic proof system called the SubCubeSums proof system. This system, which p-simulates MaxResW, is a special case of the Sherali-Adams proof system. In expressivity, it is the integral restriction of conical juntas studied in the contexts of communication complexity and extension complexity. We show that it is not simulated by Res. Using a proof technique qualitatively different from the lower bounds that MaxResW inherits from Res, we show that Tseitin contradictions on expander graphs are hard to refute in SubCubeSums. We also establish a lower bound technique via lifting: for formulas requiring large degree in SubCubeSums, their XOR-ification requires large size in SubCubeSums.</summary>
    <updated>2020-05-23T19:58:01Z</updated>
    <published>2020-05-23T19:58:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-02T07:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=437</id>
    <link href="https://tcsplus.wordpress.com/2020/05/22/tcs-talk-wednesday-may-27-rahul-ilango-mit/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 27 — Rahul Ilango, MIT</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 27th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Rahul Ilango from MIT will speak about “Is it (NP) hard to distinguish order from chaos?” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 27th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Rahul Ilango</strong> from MIT will speak about “<em>Is it (NP) hard to distinguish order from chaos?</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br/>
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: The Minimum Circuit Size Problem (MCSP) roughly asks what the “complexity” of a given string is. Informally, one can think of this as determining the degree of “computational order” a string has.</p>
<p>In the past several years, there has been a resurgence of interest in MCSP. A series of exciting results have begun unraveling what looks to be a fascinating story. This story already reveals deep connections between MCSP and a growing list of fields, including cryptography, learning theory, structural complexity theory, average-case complexity, and circuit complexity. As an example, Santhanam recently proved a conditional equivalence between the complexity of MCSP and the existence of one-way functions.</p>
<p>This talk is split into two parts. The first part is a broad introduction to MCSP, answering the following questions: What is this problem? Why is it interesting? What do we know so far, and where might the story go next? The second part discusses recent joint work with Bruno Loff and Igor Oliveira showing that the “multi-output version” of MCSP is NP-hard.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2020-05-22T22:37:13Z</updated>
    <published>2020-05-22T22:37:13Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-06-02T07:21:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17063</id>
    <link href="https://rjlipton.wordpress.com/2020/05/22/math-tells/" rel="alternate" type="text/html"/>
    <title>Math Tells</title>
    <summary>How to tell what part of math you are from Gerolamo Cardano is often credited with introducing the notion of complex numbers. In 1545, he wrote a book titled Ars Magna. He introduced us to numbers like in his quest to understand solutions to equations. Cardano was often short of money and gambled and played […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>How to tell what part of math you are from</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/unknown-141/" rel="attachment wp-att-17065"><img alt="" class="alignright size-medium wp-image-17065" height="160" src="https://rjlipton.files.wordpress.com/2020/05/unknown-1.jpeg?w=300&amp;h=160" width="300"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Gerolamo Cardano is often credited with introducing the notion of complex numbers. In 1545, he wrote a book titled <i>Ars Magna</i>. He introduced us to numbers like <img alt="{\sqrt{-5}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B-5%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{-5}}"/> in his quest to understand solutions to equations. Cardano was often short of money and gambled and played a certain board game to make money—see the second paragraph <a href="https://en.wikipedia.org/wiki/Gerolamo_Cardano#Mathematics">here</a>. </p>
<p>
Today, for amusement, Ken and I thought we’d talk about tells.</p>
<p>
What are tells? Wikipedia <a href="https://en.wikipedia.org/wiki/Tell_(poker)">says</a>: </p>
<blockquote><p><b> </b> <em> A tell in poker is a change in a player’s behavior or demeanor that is claimed by some to give clues to that player’s assessment of their hand. </em>
</p></blockquote>
<p/><p>
<a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/tell/" rel="attachment wp-att-17068"><img alt="" class="aligncenter size-medium wp-image-17068" height="300" src="https://rjlipton.files.wordpress.com/2020/05/tell.jpg?w=201&amp;h=300" width="201"/></a></p>
<p>
</p><p/><h2> Other Tells </h2><p/>
<p/><p>
Ken and I have been thinking of tells in a wider sense—when and whether one can declare inferences amid uncertain information. Historians face this all the time. So do biographers, at least when their subjects are no longer living. We would also like to make inferences in our current world, such as about the pandemic. The stakes can be higher than in poker. In poker, if your “tell” inference is wrong and you lose, you can play another hand—unless you went all in. With science and other academic areas the attitude must be that you’re all-in all the time.</p>
<p>
Cardano furnishes several instances. Wikipedia—which we regard as an equilibrium of opinions—says that Cardano </p>
<blockquote><p><b> </b> <em> acknowledged the existence of imaginary numbers … [but] did not understand their properties, [which were] described for the first time by his Italian contemporary Rafael Bombelli. </em>
</p></blockquote>
<p/><p>
This is a negative inference from how one of Cardano’s books stops short of treating imaginary numbers as objects that follow rules. </p>
<p>
There are also questions about whether Cardano can be considered “the father of probability” ahead of Blaise Pascal and Pierre de Fermat. Part of the problem is that Cardano’s own writings late in life recounted his first erroneous reasonings as well as final understanding in a Hamlet-like fashion. Wikipedia doubts whether he really knew the rule of multiplying probabilities of independent events, whereas the <a href="http://www.columbia.edu/~pg2113/index_files/Gorroochurn-Some Laws.pdf">essay</a> by Prakash Gorroochurn cited there convinces us that he did. Similar doubt extends to how much Cardano knew about the natural sciences, as correct inferences (such as mountains with seashell fossils having once been underwater) are mixed in with what we today regard as howlers.</p>
<p>
Every staging of Shakespeare’s <i>Hamlet</i> shows a book by Cardano—or does it? In Act II, scene 2, Polonius asks, “What do you read, my lord?”; to which Hamlet first replies “Words words words.” Pressed on the book’s topic, Hamlet perhaps references the section “Misery of Old Age” in Cardano’s 1543 book <i>De Consolatione</i> but what he <a href="https://www.sparknotes.com/nofear/shakespeare/hamlet/page_102/">says</a> is so elliptical it is hard to tell. The book also includes particular allusions between sleep and death that go into Hamlet’s soliloquy opening Act III. The book had been published in England in 1573 as <i>Cardan’s Comfort</i> under the aegis of the Earl of Oxford so it was well-known. Yet the writer Italo Calvino <a href="https://books.google.com/books?id=pQabBQAAQBAJ&amp;pg=PA77&amp;lpg=PA77&amp;dq=Hamlet+words+words+words+Cardano&amp;source=bl&amp;ots=mu1gcM6b8E&amp;sig=ACfU3U3W7te91qlDaIIC8EeHLM1GxpR2Dw&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwikkNjX1MfpAhWChHIEHQaWDZsQ6AEwCXoECAoQAQ#v=onepage&amp;q=Hamlet words words words Cardano&amp;f=false">held back</a> from the inference:</p>
<blockquote><p><b> </b> <em> To conclude from this that the book read by Hamlet is definitely Cardano, as is held by some scholars of Shakespeare’s sources, is perhaps unjustified. </em>
</p></blockquote>
<p/><p>
To be sure, there are some who believe Shakespeare’s main source was Oxford, in manuscripts if not flesh and blood. One reason we do not go there is that we do not see the wider community as having been able to establish reliable principles for judging what kinds of inferences are probably valid. We wonder if one could do an experiment of taking resolved cases, removing most of the information to take them down to the level of unresolved cases, and seeing what kinds of inferences from partial information would have worked.  That’s not our expertise, but within our expertise in math and CS, we wonder if a little experiment will be helpful.</p>
<p>
To set the idea, note that imaginary numbers are also called complex numbers. Yet the term complex numbers can mean other things. Besides numbers like <img alt="{2 + 3i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+%2B+3i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 + 3i}"/> it also can mean how hard it is to <a href="https://en.wikipedia.org/wiki/Integer_complexity">construct</a> a number. </p>
<blockquote><p><b> </b> <em> In number theory, the integer complexity of an integer is the smallest number of ones that can be used to represent it using ones and any number of additions, multiplications, and parentheses. It is always within a constant factor of the logarithm of the given integer. </em>
</p></blockquote>
<p/><p>
How easy is it to tell what kind of “complex” is meant if you only have partial information?  We don’t only mean scope-of-terminology issues; often a well-defined math object is used in multiple areas.  Let’s try an experiment.</p>
<p>
</p><p/><h2> Math Tells </h2><p/>
<p/><p>
Suppose you <s> walk in</s> log-in to a talk without any idea of the topic. If the speaker uses one of these terms can you tell what her talk might be about? Several have multiple meanings. What are some of them? A passing score is <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/></p>
<ol>
<p/><li>
She says let <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> be a <i>c.e.</i> set.<p/>
<p/></li><li>
She says let <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> be in <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega}"/>.<p/>
<p/></li><li>
She says by the König principle.<p/>
<p/></li><li>
She says <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> is a <i>prime</i>.<p/>
<p/></li><li>
She says <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is a <i>prime</i>.<p/>
<p/></li><li>
She says <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is <i>solvable</i>.<p/>
<p/></li><li>
She says let its <i>degree</i> be <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>.<p/>
<p/></li><li>
She says there is a <i>run</i>.<p/>
<p/></li><li>
She says it is <i>reducible</i>.<p/>
<p/></li><li>
She says it is <i>satisfiable</i>.<p/>
</li></ol>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What are your answers? Do you have some tells of your own?</p>
<p/></font></font></div>
    </content>
    <updated>2020-05-22T16:12:56Z</updated>
    <published>2020-05-22T16:12:56Z</published>
    <category term="History"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="poker"/>
    <category term="tells"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-06-02T07:20:47Z</updated>
    </source>
  </entry>
</feed>
