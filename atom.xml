<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-06-18T22:21:50Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4859</id>
    <link href="https://www.scottaaronson.com/blog/?p=4859" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4859#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4859" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Justice has no faction</title>
    <summary xml:lang="en-US">(1) To start with some rare good news: I was delighted that the US Supreme Court, in a 5-4 holding led by Chief Justice Roberts (!), struck down the Trump administration’s plan to end DACA (Deferred Action for Childhood Arrivals). Dismantling DACA would’ve been a first step toward deporting 700,000 overwhelmingly blameless and peaceful people from, in […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-group"><div class="wp-block-group__inner-container"/></div>



<p>(1) To start with some rare good news: I was delighted that the US Supreme Court, in a 5-4 <a href="https://www.supremecourt.gov/opinions/19pdf/18-587_5ifl.pdf">holding</a> led by Chief Justice Roberts (!), struck down the Trump administration’s plan to end DACA (Deferred Action for Childhood Arrivals). Dismantling DACA would’ve been a first step toward deporting 700,000 overwhelmingly blameless and peaceful people from, in many cases, the only homes they remember, for no particular reason other than to slake the resentment of Trump’s base. Better still was the majority’s argument: that when, by law, a federal agency has to supply a <em>reason</em> for a policy change (in this case, ending DACA), its reason can’t just be blatantly invented <em>post facto</em>.</p>



<p>To connect to my <a href="https://www.scottaaronson.com/blog/?p=4845">last post</a>: I hope this gives some evidence that, if Trump refuses to accept an electoral loss in November, and if it ends up in the Supreme Court as Bush v. Gore did, then Roberts might once again break from the Court’s other four rightists, in favor of the continued survival of the Republic.</p>



<p>(2) Along with Steven Pinker, Scott Alexander, Sam Altman, Jonathan Haidt, Robert Solovay, and others who might be known to this blog’s readership, I decided after reflection to sign a <a href="https://sites.google.com/view/petition-letter-stephen-hsu/home?authuser=0">petition</a> in support of Steve Hsu, a theoretical physicist turned genomics researcher, and the Senior Vice President for Research and Innovation at Michigan State University.</p>



<figure class="wp-block-image"><img alt="Information Processing: Hail to the Chief" src="https://1.bp.blogspot.com/-dVAprDWO_YI/UzGkunExlkI/AAAAAAAAEq4/MuApiqQsX_I/s1600/photo+(4).JPG"/>Hsu is the one on the right.</figure>



<p>Hsu now faces possible firing, because of a social media campaign apparently started by an MSU grad student and SneerClub poster named Kevin Bird.  What are the <a href="https://twitter.com/GradEmpUnion/status/1270829003130261504">charges</a>?  Hsu appeared in 2017 on an alt-right podcast (albeit, one that Noam Chomsky has also appeared on).  On Hsu’s own podcast, he <a href="https://manifoldlearning.com/episode-010-transcript/">interviewed Ron Unz</a>, who despite Jewish birth has become a <a href="https://www.unz.com/runz/american-pravda-holocaust-denial/">nutcase Holocaust denier</a>—yet somehow that topic never came up on the podcast.  Hsu said that, as a scientist, he doesn’t know whether group differences in average IQ have a genetic component, but our commitment to anti-racism should never hinge on questions of biology (a view also espoused by Peter Singer, perhaps the leading liberal moral philosopher of our time).  Hsu has championed genomics research that, in addition to medical uses, <em>might someday</em> help enable embryo screening for traits like IQ.  Finally, Hsu supports the continued use of standardized tests in university admissions (yes, that’s one of the listed charges).</p>



<p>Crucially, <strong>it doesn’t matter</strong> for present purposes if you disagree with many of Hsu’s views. The question is more like: <em>is agreement with Steven Pinker, Jonathan Haidt, and other mild-mannered, Obama-supporting thinkers featured in your local airport bookstore now a firing offense in academia?  And will those who affirm that it is, claim in the next breath to be oppressed, marginalized, the Rebel Alliance?</em></p>



<p>To be fair to the cancelers, I think they have two reasonable arguments in their favor.</p>



<p>The first that they’re “merely” asking for Hsu to step down as vice president, not for him to lose his tenured professorship in physics.  Only <em>professors</em>, say the activists, enjoy academic freedom; <em>administrators</em> need to uphold the values and public image of their university, as Larry Summers learned fifteen years ago.  (And besides, we might add, what intellectual iconoclast in their right mind would ever <em>become</em> a university VP, or want to stay one??)  I’d actually be fine with this if I had any confidence that it was going to end here.  But I don’t.  Given the now-enshrined standards—e.g., that professors hold positions of power, and that the powerful can oppress the powerless, or even do violence to them, just by expressing or entertaining thoughts outside an ever-shrinking range—why should Hsu trust any assurances that he’ll be left alone, if he <em>does</em> go back to being a physics professor?  If the SneerClubbers can cancel him, then how long until they cancel Pinker, or Haidt, or me?  (I <em>hope</em> the SneerClubbers enthusiastically embrace those ideas!  If they do, then no one ever again gets to call me paranoid about Red Guards behind every bush.)</p>



<p>The second reasonable argument is that, as far as I can tell, Hsu really did grant undeserved legitimacy to a Holocaust denier, via a friendly interview about other topics on his podcast.  I think it would help if, without ceding a word that he doesn’t believe, Hsu were now to denounce racism, Holocaust denial, and specifically Ron Unz’s flirtation with Holocaust denial in the strongest possible terms, and explain why he didn’t bring the topic up with his guest (e.g., did he not know Unz’s views?).</p></div>
    </content>
    <updated>2020-06-18T21:35:08Z</updated>
    <published>2020-06-18T21:35:08Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Obviously I'm Not Defending Aaronson"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-06-18T22:06:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-785371757594389601</id>
    <link href="http://processalgebra.blogspot.com/feeds/785371757594389601/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=785371757594389601" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/785371757594389601" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/785371757594389601" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/06/an-interview-with-hans-huttel-concur.html" rel="alternate" type="text/html"/>
    <title>An interview with Hans Hüttel, CONCUR Test-of-Time Award recipient</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US">This post is devoted to the fourth, and last, interview with the <a href="https://concur2020.forsyte.at/test-of-time/index.html">colleagues</a> who were selected for the first edition of the CONCUR  Test-of-Time Award. (See <a href="https://processalgebra.blogspot.com/2020/04/an-interview-with-davide-sangiorgi.html">here</a> for the interview with <a href="http://www.cs.unibo.it/~sangio/">Davide Sangiorgi</a>,  <a href="https://processalgebra.blogspot.com/2020/04/an-interview-with-nancy-lynch-and.html">here</a> for the interview with <a href="https://people.csail.mit.edu/lynch/">Nancy Lynch</a> and <a href="http://profs.sci.univr.it/~segala/">Roberto Segala</a>, and here for the interview with </span></span></span></span></span></span><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US"><a href="http://theory.stanford.edu/~rvg/">Rob van Glabbeek</a></span></span></span></span></span></span>.) In keeping with my previous interviews, I asked  <a href="http://people.cs.aau.dk/~hans/index-eng.html">Hans  Hüttel</a> (Aalborg University, Denmak) a few questions via email and you can find his answers below. </span></span></span></span></span></span><br/><br/><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US">Hans receives the award for his paper</span></span></span></span></span></span><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US"> "Bisimulation Equivalence is Decidable for all Context-Free Processes", which he wrote  with S</span></span></span><span><span style="font-size: x-small;"><span lang="da-DK">ø</span></span></span><span><span style="font-size: x-small;"><span lang="en-US">ren Christensen and Colin Stirling. </span></span></span></span></span></span></span></span></span></span></span></span><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US">S</span></span></span><span><span style="font-size: x-small;"><span lang="da-DK">ø</span></span></span><span><span style="font-size: x-small;"><span lang="en-US">ren moved to industry after finishing his PhD. Colin is now retired.  </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><br/><br/><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US">Luca: You receive one of the two CONCUR ToT Awards for the period 1990-1993 for the paper</span></span></span><span><span style="font-size: x-small;">  </span></span><span><span style="font-size: x-small;"><span lang="en-US">"Bisimulation Equivalence is Decidable for all Context-Free Processes" you published with S</span></span></span><span><span style="font-size: x-small;"><span lang="da-DK">ø</span></span></span><span><span style="font-size: x-small;"><span lang="en-US">ren Christensen and Colin Stirling at CONCUR 1992. Could you tell us briefly what the context for that work was and how it came about? </span></span></span></span></span></span></div><div align="justify" style="border: none; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">Hans: I was working on my PhD which was on the topic of decidability of behavioural equivalences for process calculi that allow processes to have infinite state spaces. Baeten, Bergstra and Klop had proved that strong bisimilarity is in fact decidable for the BPA calculus even though the class of trace languages for BPA is exactly the class of context-free languages. The result only held for normed BPA processes, that is, processes that could always terminate. Colin was my supervisor in Edinburgh, and he suggested that I try to find a simpler proof of that result (much can be said about the original proof, but simple it was not!). This turned out to be really interesting; I met Didier Caucal from IRISA who had found a nice, much shorter proof that relied on finite characterizations and I was able to use his notion of self-bisimilarity to prove that a tableau system for bisimilarity was sound and complete wrt. bisimilary. Jan Friso Groote, who was a PhD student at the CWI at the time, and myself proved that all other known equivalences are undecidable for normed BPA (and therefore </span></span></span><span><span style="font-size: x-small;"><span lang="da-DK"><i>a fortiori</i></span></span></span><span><span style="font-size: x-small;"><span lang="da-DK">also for full, unnormed BPA). But one problem that Colin and I was never able to solve was if the decidability result also held for the full BPA calculus.</span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">Søren arrived in Edinburgh in 1990 and we shared a flat there, in St Leonards Bank just across from Holyrood Park. We hung out with the same people in the LFCS, many of whom were clever Italians such as Davide Sangiorgi and clever quasi-Italians such as Marcelo Fiore. It is no surprise that Søren and I often discussed our work or that he, given that Colin was also his supervisor, moved on to study decidability problems as well, the only difference being that Søren was mostly interested in the BPP calculus that has parallel composition instead of sequential composition.</span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">When I left Edinburgh at the end of August of 1991, Colin and I had managed to make some progress on the decidability problem for unnormed BPA. We realised that a finite characterization of the maximal bisimulation </span></span></span><span><span style="font-size: x-small;"><span lang="da-DK"><i>á la</i></span></span></span><span><span style="font-size: x-small;"><span lang="da-DK">Caucal was the way head but the actual characterization escaped us. When I returned for my viva in December, Søren had found an important lemma on finite characterizations and everything fell into place. The decidability result relied on proving that bisimilarity is semi-decidable using the finite characterization; non-bisimilarity was already known to be semi-decidable. </span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US">Luca: Both S</span></span></span><span><span style="font-size: x-small;"><span lang="da-DK">ø</span></span></span><span><span style="font-size: x-small;"><span lang="en-US">ren Christensen and you wrote the award-winning paper as PhD students in Edinburgh. Could you tell us about the research environment in Edinburgh at that time, and the influence it had on you then and in the rest of your career? </span></span></span></span></span></span></div><div align="justify" style="border: none; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">Hans: The LFCS in Edinburgh was a great place to be at the time. Robin Milner was still around and busy working on the pi-calculus together with Davide Sangiorgi, who was his student at the time. Watching them at the blackboard was interesting, even though the rest of us often could not follow their discussions! Gordon Plotkin was there (and still is, fortunately). Rod Burstall was still active, and so was Mike Fourman. Plus many, many others. There was always someone to talk to, and it was perfectly legitimate to have an interest in basic research. Unlike the other professors, Colin had no background in maths – he was originally a philosopher! – and maybe that was why he was trying to avoid heavy mathematical lingo, trying to be simple and precise in his writing at the same time. I learned a lot from him, also in that respect.</span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">After Søren returned to Denmark, he left academia and got a job in the private sector. He still lives near Aarhus. Sadly we lost touch with each other, in all likelihood because our lives turned out so different. We got back in touch recently, when we were told about the reward and we look forward to finally meeting each other again. Colin retired recently, and I really hope to see him again, when I am able to travel to Scotland again some day.</span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: medium none; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US">Luca: How much of your later work has built on your award-winning papers? What follow-up results of yours are you most proud of and why?</span></span></span></span></span></span></div><div align="justify" style="border: none; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">Hans: I worked on decidability issues for another few years after that but there was no-one to talk to in Aalborg, or rather, there was no-one there that shared my interest in the area at the time. What is more, the open problems that remained were major challenges. Some were only solved much later (such as the decidability of bisimilarity for pushdown automata, a result due to Sénizergues, the proof later greatly simplified by Colin) or remain open (such as the decidability of weak bisimilarity for BPA). Eventually I drifted down the slippery slope and became interested in other, related topics, and focused somewhat more directly on program properties. The follow-up result that most directly relates to my paper with Søren and Colin is the result from 1994 that bisimilarity is also the only equivalence that is decidable for BPP. This is a result that I am also quite proud of. It grew out of discussions with Javier Esparza and Yoram Hirshfeld, when I was back in Edinburgh for a while in 1993. Ironically, there was a subtle flaw in the proof that Naoki Kobayashi discovered many years later. Naoki and one of his student found out how to repair the proof, and the three of us co-authored the journal version that came out many years later. The reason why Naoki became interested in BPP was that he was trying to use the calculus as a behavioural type system for a pi-calculus. As it happened, this was also the route that I had taken.</span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">Those of my later results that I am most proud of have to do with this area: My work on type systems for psi-calculus (CONCUR 2011 and later) and a session type system for bounded name use in the pi-calculus (Acta Informatica 2019). The latter paper also has a CONCUR connection; it grew out of attending a talk by Roland Meyer at CONCUR 2013 and wondering why they were not trying to characterize notions of name-boundedness in the pi-calculus by means of a type system. It turned out that Meyer et al. were not familiar with type systems at all. It took me an awful long time to work out a type-based characterization (using session types), as you can probably tell.</span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">As you can tell, I have not worked on bisimulation for quite some time. Not that there is anything wrong with bisimulation, of course, but if one wants results that are applicable for automated reasoning about program behaviour, decidability is important. One can then either choose to go for a less expressive model of computation (which is what researchers in the model checking community do) or keep the model of computation and go for sound, but incomplete characterizations of program properties (which is what researchers interested in type systems and related static analysis methods do). I ended up in the latter camp.</span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US">Luca: To your mind, what are the most interesting or unexpected uses in the literature of the notions and techniques you developed in "Bisimulation Equivalence is Decidable for all Context-Free Processes"? </span></span></span></span></span></span></div><div align="justify" style="border: none; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">Hans: BPA, BPP and similar process calculi can be used as the basis of behavioural type systems, and I am thrilled that my old research interests and my current research interests are so directly related. </span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">In 2018 I discovered that Vasco Vasconcelos and Peter Thiemann had devised a session type system for a functional programming language in which session types were not regular expressions (which is essentially what they are in the original work by Honda, Kubo and Vasconcelos) but BPA processes. Vasco and Peter knew that type equivalence should be decidable but they were not so familiar with our results from the early 1990s. At POPL 2019 I attended a talk by Andreia Mordido, one of Vasco’s collaborators, and she mentioned our paper from CONCUR 1992! Later that day, I ended up talking to Andreia and Vasco about my work on BPA from all those years ago.</span></span></span></span></span></span></div><div align="justify" style="border: none; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US">Luca: The last forty years have seen a huge amount of work on process algebra and process calculi. However, the emphasis on that field of research within concurrency theory seems to have diminished over the last few years, even in Europe. What advice would you give to a young researcher interested in working on process calculi today? </span></span></span></span></span></span></div><div align="justify" style="border: none; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">Hans: That they should still work on process calculi! There remains a lot of interesting work to be done. One reason why research topics drift in and out of focus is simply that researchers lose interest; it is hardly ever the case that a topic runs out of interesting research questions. Another reason is rather grim: Basic research is nowhere near as well-respected as it used to be. If you look at the funding schemes that we have today, only the very successful few can get funding for basic research; I am not among them and it does get frustrating quite often. Most topics these days deal with applied research. There is nothing wrong with applied research per se; some of what I do is on that side of things, but there has to be more to research than that. </span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: medium none; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US">Luca: What are the research topics that currently excite you the most?</span></span></span></span></span></span></div><div align="justify" style="border: none; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">Hans: I have slowly become more and more interested in programming language theory, and some of my current collaborators have taken a similar route, beginning in concurrency theory, drifting into the world of behavioural type systems and finally wanting to apply all of their skills to actual programming languages. Right now Mario Bravetti, Adrian Francalanza, António Ravara and myself are involved in working on a behavioural type system related to the Mungo tool for a subset of Java. I am fortunately enough to have had three exceptionally clever and productive MSc students involved as well, and this has been extremely helpful. Mungo is in many ways the work of Simon Gay and Ornela Dardha at Glasgow University, and they, too, began their careers working on process calculi.</span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: medium none; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="en-US">Luca: You have a keen interest in teaching at university level. Is there anything you think we should do better in conveying the beauty and usefulness of theoretical computer science in general, and concurrency theory in particular, to our students today? Do you have any thoughts you'd like to share with us on how to combine digital resources and in-person meetings in the teaching of subjects in the theory of computing?</span></span></span></span></span></span></div><div align="justify" style="border: none; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">Hans: Personally I think there is a tendency to present any academic topic – and in particular topics in the mathematical science, broadly speaking – in such a way that the definitions and theorems appear as if they fell from the heavens, fully formed. As any researcher will know, that is certainly </span></span></span><span><span style="font-size: x-small;"><span lang="da-DK"><i>not </i></span></span></span><span><span style="font-size: x-small;"><span lang="da-DK">how they came about. In this way, we focus a lot on what Imre Lakatos and Karl Popper called the </span></span></span><span><span style="font-size: x-small;"><span lang="da-DK"><i>context of justification. </i></span></span></span><span><span style="font-size: x-small;"><span lang="da-DK">A well-honed presentation of a theory may appear to be beautiful, but in my opinion the actual beauty is the one that one experiences when one has finally understood the theory and understands why the definitions and theorem turned out the way they did – that is, one must understand the </span></span></span><span><span style="font-size: x-small;"><span lang="da-DK"><i>context of discovery.</i></span></span></span><span><span style="font-size: x-small;"><span lang="da-DK">I think problem-based learning (which is something that we talk about a lot at Aalborg University and at Roskilde University) is the key here, because it puts the focus on student-centered active learning.</span></span></span></span></span></span></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><br/></div><div align="justify" style="border: none; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 100%; margin-bottom: 0in; padding: 0in; text-decoration: none;"><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK">I have lectured a lot over the years but since 2013 I have drifted away from traditional lectures towards flipped teaching in which I use pre-recorded podcasts (of my own making) that students can watch whenever they want; I then use the plenary sessions for activities that focus on active learning. I by far prefer having a dialogue with students that focuses on the problems that they encounter to the monologue style of a lecture. All my teaching activities are now flipped and I am happy to say that some of my colleagues are now thinking along similar lines. It is always good to have someone to talk to. </span></span></span></span></span></span><br/><span style="color: black;"><span><span style="background: transparent;"><span><span style="font-size: x-small;"><span lang="da-DK"> </span></span></span></span></span></span></div></div>
    </content>
    <updated>2020-06-18T21:29:00Z</updated>
    <published>2020-06-18T21:29:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-06-18T21:29:22Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5834988441622500007</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5834988441622500007/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/06/on-chain-letters-and-pandemics.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5834988441622500007" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5834988441622500007" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/06/on-chain-letters-and-pandemics.html" rel="alternate" type="text/html"/>
    <title>On Chain Letters and Pandemics</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div><i>Guest post by Varsha Dani.</i></div><div><br/></div><div>My 11-year-old child received a letter in the mail. "Send a book to the first person named," it said, "then move everyone's name up the list, add your own name and send copies of the letter to six friends. In a few weeks you will receive 36 books from all over the world!". Wow. When I first encountered chain letters in the mid eighties, it was postcards, but even then it hadn't taken me in. Since then I hadn't seen one of these in a long time, but I guess with a lot of people suddenly at home for extended periods, people crave both entertainment and a connection to others. </div><div><br/></div><div>What's wrong with chain letters? Well quite apart from the fact that they are illegal, even a child can comprehend that the number of books (or postcards or other gifts) received must equal the number sent, and that for every participant who does get a rich reward, there will be many who get nothing. </div><div><br/></div><div>But there is another kind of chain communication going around. It is an email, asking the recipient to send a poem or meditation to somebody, and later they will receive many communications of the same sort. How endearing. Poetry. Sweetness and Light. No get-rich-quick pyramid schemes here. What's wrong with that? </div><div><br/></div><div>Of course, it depends on what one means by "wrong". Maybe you like exchanging poetry with strangers. Maybe you don't find it onerous or wish that your spam filter would weed it out. But let's leave aside those issues and look at the math alone. You send the email to two friends, each of whom forwards it to two of their friends and so on. So the number of people the email reaches ostensibly doubles every step. Exponential growth. But in fact that is not what the graph of human connections looks like. Instead, what happens is that the sets of friends overlap, so that after a while the growth stops being exponential and tapers down. </div><div><br/></div><div>Where else have we seen something like that? Oh, right. The pandemic. The virus jumps from infected people to the people they meet, and from them to the people they meet and so on. Initially, that's exponential growth fof new cases, but after a while  it tapers off, forms a peak and then starts to decrease. Why? Because eventually there is overlap in the sets of people that each infected person is "trying" (unintentionally) to infect, and a newly infected person who got the virus from one or many previously infected people is still just one newly infected person.  </div><div><br/></div><div>So the chain letter spreads just like a virus. Indeed if one were to, somewhat fancifully, think of the chain letter as an independent entity whose goal is to self-replicate, then it looks even more like a virus, and, like a virus, it can only achieve its self-replication goal through the help of a host. But here's a way in which it is not like a virus. Once one has got the virus and recovered, one (hopefully) does not get it again. Not so the chain letter, of which one may get many copies over time! So maybe you will get some gifts or poetry, but you will likely also get more requests for them!</div><div><br/></div><div>So what's wrong with the poetry chain email? It depends on your perspective.  To those of you who are wistfully waiting for that Poem from a Stranger, I dedicate the following to you.</div><div><br/></div><hr/><div><br/></div><div><i>An open letter to my 2<sup>n</sup> dearest friends:</i></div><div><br/></div><div>A letter came for me today</div><div>It promised wondrous ends</div><div>If only I would forward it </div><div>To just two other friends.</div><div><br/></div><div>If they in turn should send it on</div><div>to two more that they know,</div><div>the goodwill that we're sending out</div><div>would grow and grow and grow.</div><div><br/></div><div>Is this as pleasant as it seems?</div><div>Alas, dear friends, it's not.</div><div>This exponential growth can lead</div><div>To quite a sticky spot.</div><div><br/></div><div>Friends of friends of friends of mine</div><div>May very well be linked</div><div>The further that the letters go.</div><div>These folks are not distinct!</div><div><br/></div><div>Ensuring there's no overlap</div><div>Is a logistic* pain.</div><div>As you will see, when you receive</div><div>That letter yet again. </div><div><br/></div><div>So while you're stuck at home this year</div><div>And pacing in your room.</div><div>Pick up the phone and make a call</div><div>Or see your friends on Zoom.</div><div><br/></div><div>Your real thoughts would make me smile.</div><div>Chain letters are a con.</div><div>Do everyone a favor and</div><div>Don't send that letter on!</div><div><br/></div><div>--------------</div><div>* Pun intended. <a href="https://en.wikipedia.org/wiki/Logistic_function#In_medicine:_modeling_of_a_pandemic">https://en.wikipedia.org/wiki/Logistic_function#In_medicine:_modeling_of_a_pandemic</a></div><div><br/></div><div><br/></div></div>
    </content>
    <updated>2020-06-18T13:57:00Z</updated>
    <published>2020-06-18T13:57:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-06-18T16:42:45Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/background/</id>
    <link href="https://gradientscience.org/background/" rel="alternate" type="text/html"/>
    <title>Noise or Signal&amp;#58; The Role of Backgrounds in Image Classification</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="http://arxiv.org/abs/2006.09994" style="float: left;">
<i class="fas fa-file-pdf"/>
    Read the paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/backgrounds_challenge" style="float: right;">
<i class="fab fa-github"/>
   Download the datasets
</a></p>

<p><em>We discuss our recent <a href="https://arxiv.org/abs/2006.09994">paper</a> 
on identifying the role of backgrounds in image classification. Along with our
results, we’re releasing our code and datasets <a href="https://github.com/MadryLab/backgrounds_challenge">as a benchmark</a>.</em></p>

<p>As we discussed in our <a href="https://gradientscience.org/benchmarks">last post</a>, quantitative
benchmarks are key drivers of progress across many computer vision tasks. 
On tasks like image classification, state of the art is often determined by models’
accuracies on standard datasets, such as CIFAR-10 and ImageNet. 
Still, model accuracy isn’t all that matters, as evidenced by investigations into
robustness (e.g., <a href="https://gradientscience.org/intro_adversarial">[1]</a>), 
reliability (e.g., <a href="https://arxiv.org/abs/1903.12261">[2]</a>), 
and out-of-distribution performance (e.g., <a href="https://arxiv.org/abs/1706.02690">[3]</a>). 
These properties are governed not only by models’ predictions on test data, but
also by the specific set of correlations models use, and by how these
correlations are combined to make predictions. 
For example, previous work has shown that model predictions can behave
unexpectedly due to reliance on correlations that we humans 
don’t rely on (e.g. <a href="https://arxiv.org/abs/1711.11561">[4]</a>,
<a href="https://arxiv.org/abs/1807.04200">[5]</a>,
<a href="https://arxiv.org/abs/1905.02175">[6]</a>, 
<a href="https://arxiv.org/abs/1811.00401">[7]</a>); or due to overusing even
human-recognizable correlations such as texture (e.g., 
<a href="https://arxiv.org/abs/1811.12231">[8]</a>,
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6306249/">[9]</a>) or color 
(e.g., <a href="https://journals.sagepub.com/doi/10.1068/p3376">[10]</a>). 
So it follows that if we want to understand these more complex properties of
machine learning models, we must first be able to characterize the correlations
that these models leverage.</p>

<p>Needless to say, a full characterization of the features and signals exploited
by deep neural networks is far beyond the scope of any single paper. In our
<a href="https://arxiv.org/abs/2006.09994">latest paper</a>, 
we thus take a deep dive into one specific kind of signal: <em>image backgrounds</em>.</p>

<p>Backgrounds are an established source of correlation between images and
their labels in object detection: ML models may use backgrounds in
classification (cf.
<a href="https://hal.archives-ouvertes.fr/hal-00171412/file/ZhangMarszalekLazebnikSchmid-IJCV07-ClassificationStudy.pdf">[11]</a>,
<a href="https://arxiv.org/abs/1602.04938">[12]</a>, 
<a href="https://arxiv.org/abs/1611.06596">[13]</a>,
<a href="https://arxiv.org/abs/1911.08731">[14]</a>), and even human
vision can make use of image context (cf.
<a href="http://people.csail.mit.edu/torralba/IJCVobj.pdf">[15]</a> and references). 
We thus want to understand better how current
state-of-the-art image classifiers rely on image backgrounds. 
Specifically, we investigate the extent of this reliance, its implications, and how models’ use of
backgrounds has evolved over time.</p>

<h2 id="a-new-dataset-or-seven">A new dataset (or seven)</h2>

<p>Our main tool for understanding how models use background signals is a set of
synthetic datasets that we refer to as ImageNet-9 (IN9). These datasets aim
to disentangle images’ foreground and background signals and thus enable us
to study their relative effects.</p>

<p>To generate ImageNet-9, we start by organizing a subset of ImageNet into nine
coarse-grained classes based on common ancestry in the <a href="https://wordnet.princeton.edu">WordNet hierarchy</a>: the
resulting “super-classes” are dog, bird, vehicle, reptile, carnivore, insect, 
instrument, primate, and fish. We call the 9-class dataset of unmodified images
Original.
We then use a combination of the bounding boxes
provided by ImageNet and the computer vision library <a href="https://opencv.org">OpenCV</a> to separate the
foreground and background in each imagewe deleted any images where this
process was unsuccessful (e.g., if there is no bounding box provided by ImageNet, or the bounding box takes up the entirety of the image).</p>

<p>Once we’ve separated the foreground and background signals, we introduce
 <em>seven</em> new datasets, falling into three categories:</p>

<ul>
  <li>Background-only datasets
    <ul>
      <li><strong>Only-BG-B</strong>: Black out the bounding boxes given by ImageNet annotations,
leaving only the background.</li>
      <li><strong>Only-BG-T</strong>: Take Only-BG-B and replace the blacked-out region with a
tiled version of the rest of the image (the background).</li>
      <li><strong>No-FG</strong>: Use OpenCV to extract the exact shape of the foreground, and
replace it with black.</li>
    </ul>
  </li>
  <li>Foreground-only datasets
    <ul>
      <li><strong>Only-FG</strong>: The exact complement of No-FG—rather than removing
the foreground, remove everything else.</li>
    </ul>
  </li>
  <li>Mixed datasets
    <ul>
      <li><strong>Mixed-Rand</strong>: For each image, overlay the foreground (extracted using
OpenCV) onto the background from a different random image (again,
extracted using OpenCV).</li>
      <li><strong>Mixed-Next</strong>: Assign each class a number from 1 to 9. For each image of
class $y$, add the background from a random image of class $y+1$ (or $1$, if
$y=9$).</li>
      <li><strong>Mixed-Same</strong>: For each image, add the background from a random image of
the same class.</li>
    </ul>
  </li>
</ul>

<div class="widget" style="display: flex;">
    <div class="choices_one" id="left_in9">
	<span class="widgetheading">Choose an Image</span>
    </div>
    <div class="selected_one" id="in9_selected"/>
</div>
<div style="clear: both;"/>
<div class="footnote">
        8 versions of the same image, with each version capturing a different
        combination of foreground and background signals.
</div>

<h2 id="putting-imagenet-9-to-work">Putting ImageNet-9 to work</h2>

<p>It turns out that these proposed ImageNet-9 datasets allow us to ask (and
answer) a variety of questions about the role of background signals in image
classification.</p>

<h3 id="1-is-background-signal-enough-for-classification">1. Is background signal enough for classification?</h3>

<p>Before we look at the behaviour of standard ML models, we first double-check
that background signals are exploitable in the first place—i.e., that models
can learn reasonably accurate classifiers for natural images while being trained
on backgrounds alone. 
We are not the first ones to consider this question (e.g.,
<a href="https://arxiv.org/abs/1611.06596">[13]</a>) but it serves as a useful sanity 
check and gives us a baseline to compare the rest of our experiments to.</p>

<p>We train models on the Only-BG-T, Only-BG-B, and No-FG datasets; the models
generalize reasonably well to both their corresponding test sets <em>and</em> the Original
test set (each model gets around 40-50% accuracy: a random classifier would get
11%). Thus, image backgroundsInterestingly, the No-FG model doesn't do
significantly better than the others, despite having access to the shape of the
foregrounds in the training set. <em>do</em> contain signal that
models can use to classify images.</p>

<p><img alt="The results of training models on background-only datasets and testing on the original." src="https://gradientscience.org/images/background/bg_only_train.png" style="width: 70%;"/></p>

<h3 id="2-do-models-actually-use-background-signals">2. Do models actually use background signals?</h3>

<p>So, backgrounds are indeed a useable signal for deep learning classifiers.
That’s not necessarily bad, or even different to humans: if you see an
occluded picture with an underwater background, you’d (hopefully) say that the
pictured object is more likely to be a fish than a dog.
On the other hand, humans are still able to call a dog a dog when it’s
underwater, i.e., a misleading/irrelevant background usually does not preclude us from
making the correct predictions. Is the same true for models?</p>

<p>To answer this question, we study model accuracies on the Mixed-Rand dataset,
where image backgrounds are randomized and thus provide no information 
about the correct label. 
Specifically, we compare model performance on Mixed-Rand and Mixed-Same: the
latter maintains the foreground-background correlation (since the background is
from the correct class), while controlling for artifactsSee
Appendix D of our paper for more details! from image
splicing process.</p>

<p>We denote the accuracy gap between Mixed-Same and Mixed-Rand as the BG-Gap,
i.e., the drop in model accuracy due to changing the class signal from the
background. The table below summarizes our observations: the BG-Gap is
13-22% and 6-14% for models trained on IN-9 and ImageNet, respectively,
suggesting that backgrounds often mislead state-of-the-art models even
when the correct foreground is present. (As we discuss in Appendix B of <a href="https://arxiv.org/abs/2006.09994">our
paper</a>, ImageNet-trained models do seem to be
more robust in this sense, but the reason for this robustness is hard to pin down.)</p>

<p><img alt="Evaluating pretrained models on Imagenet, Mixed-Rand, and Mixed-Same to compute the BG-Gap." src="https://gradientscience.org/images/background/table.png" style="width: 100%;"/></p>

<h3 id="3-ok-but-how-bad-can-it-get">3. Ok, but how bad can it get?</h3>

<p>The BG-Gap introduced in the previous experiment measures, in some sense,
models’ average robustness to misleading backgrounds. What does the worst case
look like? To diagnose just how large of an issue background over-reliance
can be, we search for the worst extracted background corresponding for each
extracted foreground. It turns out that a ResNet-50 model can be fooled on
87.5% of foregrounds by overlaying them on a corresponding “adversarial background.”</p>

<p>In fact, there also turn out to exist backgrounds that consistently affect the
prediction of the classifier <em>regardless</em> of what foreground is overlaid onto
them. The backgrounds below (extracted from insect images) fool our model into
predicting “insect” on up to 52\% of non-insect foregrounds:
<img alt="Adversarial Backgrounds" src="https://gradientscience.org/images/background/insect_result.png" style="width: 100%;"/></p>
<div class="footnote">
        The 5 most fooling backgrounds from the insect class, as well as the percent of non-insect foregrounds that they individually fool.
</div>

<p>Further, we can make the classifier predict “insect” on over 2/3 of the
foregrounds in the ImageNet-9 dataset, just by combining the foregrounds with
different insect backgrounds.</p>

<h3 id="4-what-is-the-effect-of-the-training-dataset">4. What is the effect of the training dataset?</h3>

<p>So far, all of the models that we’ve looked at have been trained on natural
data, i.e., either on the Original dataset from IN9, or on ImageNet itself.
We now want to test whether the previously-observed dependence on backgrounds
can be reduced (or removed altogether) by appropriately altering the training data.</p>

<p>To this end, we train models on Mixed-Rand, where background signals have been
decorrelated from class labels. 
We find these models perform only slightly better than
random on datasets with no foregrounds (e.g., a ResNet-50 trained on Mixed-Rand
achieves 15% accuracy on Only-BG-T and Only-BG-B).
They also perform better in the presence of misleading backgrounds:
training on Mixed-Rand improves accuracy on Mixed-Rand by 17%, and improves
accuracy on Mixed-NextRecall that Mixed-Next
images have foregrounds from class $y$ mixed with backgrounds from class $y+1$,
labeled as class $y$. by 22%. The model trained on
Mixed-Rand also has very little variation in accuracy across all five test sets
that contain the correct foreground (providing more evidence for its invariance
to other factors).</p>

<p><img alt="Training on Mixed-Rand and evaluating on other datasets" src="https://gradientscience.org/images/background/mixed_rand_results.png" style="width: 100%;"/></p>

<p>Qualitatively, the Mixed-Rand model also appears to place more relative
importance on foreground pixels than its original counterpart, as demonstrated
by the saliency maps below:</p>

<p><img alt="Saliency maps for models trained on original versus on Mixed-Rand" src="https://gradientscience.org/images/background/saliency_other.png" style="width: 100%;"/></p>

<h3 id="5-are-we-really-making-progress">5. Are we really making progress?</h3>

<p>We’ve now shown that models can exploit backgrounds, do exploit backgrounds, and
may actually do so to a fault. Considering that the development of these models
is driven by standard computer vision benchmarks, our results beg the question: to
what extent have improvements on ImageNet come with (or resulted from)
improvements in leveraging background correlations? And relatedly, how has
model robustness
to misleading background signals evolved over time?</p>

<p>As a first step towards answering these questions, we study the progress made by
ImageNet models on our proposed synthetic datasets. Below, we plot accuracy on
these datasets against ImageNet accuracy for a variety of different
network architectures:</p>

<p><img alt="ImageNet accuracy plotted against accuracy on synthetic datasets" src="https://gradientscience.org/images/background/in_vs_bg.png" style="width: 100%;"/></p>

<p>The plot indicates that accuracy increases on ImageNet generally correspond to
accuracy increases on all of the synthetic datasets that we consider. 
This includes the datasets that only contain background signals (Only-BG-T
in the graph above), which means that models <em>do</em> improve at extracting correlations
from image backgrounds. The fact that better models are also better at
classifying background-only images suggests that the use of background
signals might be inherent to the current training paradigm, and may
not disappearThough again, this might not be a bad
thing! on its own (i.e., without explicit regularization
or training).</p>

<p>Still, models’ <em>relative</em> improvement in accuracy across dataset variants is
promising—accuracy on background-only datasets is improving slower than
accuracy on datasets where the background is misleading, such as Mixed-Rand or
Mixed-Next. Another promising sign is that the
performance gap between the Mixed-Rand and Mixed-Same dataset (which we
previously referred to as the BG-Gap) trends towards closing, indicating that
models are not only better at using foreground features, but also more
robust to misleading background features.</p>

<p>Overall, our analysis reveals that better models in terms of ImageNet accuracy
are (a) increasingly capable of exploiting background correlations, but at the
same time (b) becoming more robust to changes in background, suggesting that
over-reliance on background features may not be necessary to maximize the
benchmark accuracy.</p>

<h3 id="conclusions">Conclusions</h3>

<p>In this post, we saw how computer vision models tend to over-rely on image
backgrounds in image classification. On one hand, our findings provide more
evidence that our models are not fully aligned with the human vision system. On the
other hand, we have shown that advances in computer vision models, such as new
architectures and training methods, have led to models that tend to use the
foreground more effectively and are more robust to misleading backgrounds.
We hope that the datasets and findings in this work provide a way to monitor
progress towards reliable, human-aligned machine learning.</p>

<p>For more detailed information about the <a href="https://github.com/MadryLab/backgrounds_challenge">datasets</a> we created, full
experimental results, and additional analysis, see <a href="https://gradientscience.org/background.pdf">our paper</a>!</p></div>
    </summary>
    <updated>2020-06-18T00:00:00Z</updated>
    <published>2020-06-18T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2020-06-18T19:20:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.10012</id>
    <link href="http://arxiv.org/abs/2006.10012" rel="alternate" type="text/html"/>
    <title>Robust Persistence Diagrams using Reproducing Kernels</title>
    <feedworld_mtime>1592438400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Siddharth Vishwanath, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fukumizu:Kenji.html">Kenji Fukumizu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuriki:Satoshi.html">Satoshi Kuriki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sriperumbudur:Bharath.html">Bharath Sriperumbudur</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.10012">PDF</a><br/><b>Abstract: </b>Persistent homology has become an important tool for extracting geometric and
topological features from data, whose multi-scale features are summarized in a
persistence diagram. From a statistical perspective, however, persistence
diagrams are very sensitive to perturbations in the input space. In this work,
we develop a framework for constructing robust persistence diagrams from
superlevel filtrations of robust density estimators constructed using
reproducing kernels. Using an analogue of the influence function on the space
of persistence diagrams, we establish the proposed framework to be less
sensitive to outliers. The robust persistence diagrams are shown to be
consistent estimators in bottleneck distance, with the convergence rate
controlled by the smoothness of the kernel. This, in turn, allows us to
construct uniform confidence bands in the space of persistence diagrams.
Finally, we demonstrate the superiority of the proposed approach on benchmark
datasets.
</p></div>
    </summary>
    <updated>2020-06-18T01:29:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09969</id>
    <link href="http://arxiv.org/abs/2006.09969" rel="alternate" type="text/html"/>
    <title>Playing Unique Games on Certified Small-Set Expanders</title>
    <feedworld_mtime>1592438400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bafna:Mitali.html">Mitali Bafna</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Barak:Boaz.html">Boaz Barak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kothari:Pravesh.html">Pravesh Kothari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schramm:Tselil.html">Tselil Schramm</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Steurer:David.html">David Steurer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09969">PDF</a><br/><b>Abstract: </b>We give an algorithm for solving unique games (UG) instances whose
constraints correspond to edges of graphs with a sum-of-squares (SoS)
small-set-expansion certificate. As corollaries, we obtain the first
polynomial-time algorithms for solving UG on the noisy hypercube and the short
code graphs. The prior best algorithm for such instances was the eigenvalue
enumeration algorithm of Arora, Barak, and Steurer (2010) which requires
quasi-polynomial time for the noisy hypercube and nearly-exponential time for
the short code graph. All of our results achieve an approximation of
$1-\epsilon$ vs $\delta$ for UG instances, where $\delta &gt; 0$ depends on the
expansion parameters of the graph but is independent of the alphabet size.
</p>
<p>Specifically, say that a regular graph $G=(V,E)$ is a $(\mu,\eta)$ small-set
expander (SSE) if for every subset $S \subseteq V$ with $|S| \leq \mu |V|$, the
edge-expansion of $S$ is at least $\eta$. We say that $G$ is a $d$-certified
$(\mu,\eta)$-SSE if there is a degree-d SoS certificate for this fact (based on
2 to 4 hypercontractivity). We prove that there is a $|V|^{f(d,\mu,\eta)}$ time
algorithm $A$ (based on the SoS hierarchy) such that for every $\eta&gt;0$ and
$d$-certified $(\mu, \eta)$-SSE $G$, if $I$ is a $1-\eta^2/100$ satisfiable
affine UG instance over $G$ then $A(I)$ is an assignment satisfying at least
some positive fraction $\delta = \delta(\mu,\eta)$ of $I$'s constraints. As a
corollary, we get a polynomial-time algorithm $A$ such that if $I$ is a
$1-\epsilon$ satisfiable instance over the $\alpha$-noisy hypercube or short
code graph, then $A(I)$ outputs an assignment satisfying an
$\exp(-O(\sqrt{\epsilon}/\alpha))$ fraction of the constraints. Our techniques
can be extended even to graphs that are not SSE, and in particular we obtain a
new efficient algorithm for solving UG instances over the Johnson graph.
</p></div>
    </summary>
    <updated>2020-06-18T01:20:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09956</id>
    <link href="http://arxiv.org/abs/2006.09956" rel="alternate" type="text/html"/>
    <title>Bad Projections of the PSD Cone</title>
    <feedworld_mtime>1592438400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Yuhan.html">Yuhan Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sturmfels:Bernd.html">Bernd Sturmfels</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09956">PDF</a><br/><b>Abstract: </b>The image of the cone of positive semidefinite matrices under a linear map is
a convex cone. Pataki characterized the set of linear maps for which that image
is not closed. The Zariski closure of this set is a hypersurface in the
Grassmannian. Its components are the coisotropic hypersurfaces of symmetric
determinantal varieties. We develop the convex algebraic geometry of such bad
projections, with focus on explicit computations.
</p></div>
    </summary>
    <updated>2020-06-18T01:28:37Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09912</id>
    <link href="http://arxiv.org/abs/2006.09912" rel="alternate" type="text/html"/>
    <title>Bute: A Bottom-Up Exact Solver for Treedepth (Submitted to PACE 2020 under username peaty)</title>
    <feedworld_mtime>1592438400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trimble:James.html">James Trimble</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09912">PDF</a><br/><b>Abstract: </b>This note introduces the exact solver Bute for the exact treedepth problem,
along with two variants of the solver. Each of these solvers computes an
elimination tree in a bottom-up fashion by finding sets of vertices that induce
subgraphs of small treedepth, then combining sets of vertices together with a
root vertex to produce larger sets. The algorithms make use of a trie data
structure to reduce the effort required to determine acceptable combinations of
subtrees. Bute-Plus and Bute-Plus-Plus add a heuristic presolve step, which can
quickly find a treedepth decomposition of optimal depth for many instances.
</p></div>
    </summary>
    <updated>2020-06-18T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09877</id>
    <link href="http://arxiv.org/abs/2006.09877" rel="alternate" type="text/html"/>
    <title>Twin-width II: small classes</title>
    <feedworld_mtime>1592438400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonnet:=Eacute=douard.html">Édouard Bonnet</a>, Colin Geniet, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Eun_Jung.html">Eun Jung Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thomass=eacute=:St=eacute=phan.html">Stéphan Thomassé</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Watrigant:R=eacute=mi.html">Rémi Watrigant</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09877">PDF</a><br/><b>Abstract: </b>The twin-width of a graph $G$ is the minimum integer $d$ such that $G$ has a
$d$-contraction sequence, that is, a sequence of $|V(G)|-1$ iterated vertex
identifications for which the overall maximum number of red edges incident to a
single vertex is at most $d$, where a red edge appears between two sets of
identified vertices if they are not homogeneous in $G$. We show that if a graph
admits a $d$-contraction sequence, then it also has a linear-arity tree of
$f(d)$-contractions, for some function $f$. First this permits to show that
every bounded twin-width class is small, i.e., has at most $n!c^n$ graphs
labeled by $[n]$, for some constant $c$. This unifies and extends the same
result for bounded treewidth graphs [Beineke and Pippert, JCT '69], proper
subclasses of permutations graphs [Marcus and Tardos, JCTA '04], and proper
minor-free classes [Norine et al., JCTB '06]. The second consequence is an
$O(\log n)$-adjacency labeling scheme for bounded twin-width graphs, confirming
several cases of the implicit graph conjecture. We then explore the "small
conjecture" that, conversely, every small hereditary class has bounded
twin-width. Inspired by sorting networks of logarithmic depth, we show that
$\log_{\Theta(\log \log d)}n$-subdivisions of $K_n$ (a small class when $d$ is
constant) have twin-width at most $d$. We obtain a rather sharp converse with a
surprisingly direct proof: the $\log_{d+1}n$-subdivision of $K_n$ has
twin-width at least $d$. Secondly graphs with bounded stack or queue number
(also small classes) have bounded twin-width. Thirdly we show that cubic
expanders obtained by iterated random 2-lifts from $K_4$~[Bilu and Linial,
Combinatorica '06] have bounded twin-width, too. We suggest a promising
connection between the small conjecture and group theory. Finally we define a
robust notion of sparse twin-width and discuss how it compares with other
sparse classes.
</p></div>
    </summary>
    <updated>2020-06-18T01:22:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09872</id>
    <link href="http://arxiv.org/abs/2006.09872" rel="alternate" type="text/html"/>
    <title>Scheduling a Proportionate Flow Shop of Batching Machines</title>
    <feedworld_mtime>1592438400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hertrich:Christoph.html">Christoph Hertrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei=szlig=:Christian.html">Christian Weiß</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ackermann:Heiner.html">Heiner Ackermann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heydrich:Sandy.html">Sandy Heydrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krumke:Sven_O=.html">Sven O. Krumke</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09872">PDF</a><br/><b>Abstract: </b>In this paper we study a proportionate flow shop of batching machines with
release dates and a fixed number $m \geq 2$ of machines. The scheduling problem
has so far barely received any attention in the literature, but recently its
importance has increased significantly, due to applications in the industrial
scaling of modern bio-medicine production processes. We show that for any fixed
number of machines, the makespan and the sum of completion times can be
minimized in polynomial time. Furthermore, we show that the obtained algorithm
can also be used to minimize the weighted total completion time, maximum
lateness, total tardiness and (weighted) number of late jobs in polynomial time
if all release dates are $0$. Previously, polynomial time algorithms have only
been known for two machines.
</p></div>
    </summary>
    <updated>2020-06-18T01:23:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09735</id>
    <link href="http://arxiv.org/abs/2006.09735" rel="alternate" type="text/html"/>
    <title>Efficient Statistics for Sparse Graphical Models from Truncated Samples</title>
    <feedworld_mtime>1592438400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhattacharyya:Arnab.html">Arnab Bhattacharyya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Desai:Rathin.html">Rathin Desai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nagarajan:Sai_Ganesh.html">Sai Ganesh Nagarajan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panageas:Ioannis.html">Ioannis Panageas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09735">PDF</a><br/><b>Abstract: </b>In this paper, we study high-dimensional estimation from truncated samples.
We focus on two fundamental and classical problems: (i) inference of sparse
Gaussian graphical models and (ii) support recovery of sparse linear models.
</p>
<p>(i) For Gaussian graphical models, suppose $d$-dimensional samples ${\bf x}$
are generated from a Gaussian $N(\mu,\Sigma)$ and observed only if they belong
to a subset $S \subseteq \mathbb{R}^d$. We show that ${\mu}$ and ${\Sigma}$ can
be estimated with error $\epsilon$ in the Frobenius norm, using
$\tilde{O}\left(\frac{\textrm{nz}({\Sigma}^{-1})}{\epsilon^2}\right)$ samples
from a truncated $\mathcal{N}({\mu},{\Sigma})$ and having access to a
membership oracle for $S$. The set $S$ is assumed to have non-trivial measure
under the unknown distribution but is otherwise arbitrary.
</p>
<p>(ii) For sparse linear regression, suppose samples $({\bf x},y)$ are
generated where $y = {\bf x}^\top{{\Omega}^*} + \mathcal{N}(0,1)$ and $({\bf
x}, y)$ is seen only if $y$ belongs to a truncation set $S \subseteq
\mathbb{R}$. We consider the case that ${\Omega}^*$ is sparse with a support
set of size $k$. Our main result is to establish precise conditions on the
problem dimension $d$, the support size $k$, the number of observations $n$,
and properties of the samples and the truncation that are sufficient to recover
the support of ${\Omega}^*$. Specifically, we show that under some mild
assumptions, only $O(k^2 \log d)$ samples are needed to estimate ${\Omega}^*$
in the $\ell_\infty$-norm up to a bounded error.
</p>
<p>For both problems, our estimator minimizes the sum of the finite population
negative log-likelihood function and an $\ell_1$-regularization term.
</p></div>
    </summary>
    <updated>2020-06-18T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09665</id>
    <link href="http://arxiv.org/abs/2006.09665" rel="alternate" type="text/html"/>
    <title>Caching with Time Windows and Delays</title>
    <feedworld_mtime>1592438400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Amit.html">Amit Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panigrahi:Debmalya.html">Debmalya Panigrahi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09665">PDF</a><br/><b>Abstract: </b>We consider two generalizations of the classical weighted paging problem that
incorporate the notion of delayed service of page requests. The first is the
(weighted) Paging with Time Windows (PageTW) problem, which is like the
classical weighted paging problem except that each page request only needs to
be served before a given deadline. This problem arises in many practical
applications of online caching, such as the "deadline" I/O scheduler in the
Linux kernel and video-on-demand streaming. The second, and more general,
problem is the (weighted) Paging with Delay (PageD) problem, where the delay in
serving a page request results in a penalty being assessed to the objective.
This problem generalizes the caching problem to allow delayed service, a line
of work that has recently gained traction in online algorithms (e.g., Emek et
al. STOC '16, Azar et al. STOC '17, Azar and Touitou FOCS '19).
</p>
<p>We give $O(\log k\log n)$-competitive algorithms for both the PageTW and
PageD problems on $n$ pages with a cache of size $k$. This significantly
improves on the previous best bounds of $O(k)$ for both problems (Azar et al.
STOC '17). We also consider the offline PageTW and PageD problems, for which we
give $O(1)$ approximation algorithms and prove APX-hardness. These are the
first results for the offline problems; even NP-hardness was not known before
our work. At the heart of our algorithms is a novel "hitting-set" LP relaxation
of the PageTW problem that overcomes the $\Omega(k)$ integrality gap of the
natural LP for the problem. To the best of our knowledge, this is the first
example of an LP-based algorithm for an online algorithm with delays/deadlines.
</p></div>
    </summary>
    <updated>2020-06-18T01:25:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09509</id>
    <link href="http://arxiv.org/abs/2006.09509" rel="alternate" type="text/html"/>
    <title>Online Algorithms for Weighted Paging with Predictions</title>
    <feedworld_mtime>1592438400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Zhihao.html">Zhihao Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panigrahi:Debmalya.html">Debmalya Panigrahi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Kevin.html">Kevin Sun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09509">PDF</a><br/><b>Abstract: </b>In this paper, we initiate the study of the weighted paging problem with
predictions. This continues the recent line of work in online algorithms with
predictions, particularly that of Lykouris and Vassilvitski (ICML 2018) and
Rohatgi (SODA 2020) on unweighted paging with predictions. We show that unlike
unweighted paging, neither a fixed lookahead nor knowledge of the next request
for every page is sufficient information for an algorithm to overcome existing
lower bounds in weighted paging. However, a combination of the two, which we
call the strong per request prediction (SPRP) model, suffices to give a
2-competitive algorithm. We also explore the question of gracefully degrading
algorithms with increasing prediction error, and give both upper and lower
bounds for a set of natural measures of prediction error.
</p></div>
    </summary>
    <updated>2020-06-18T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-6952358197668395658</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/6952358197668395658/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=6952358197668395658" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/6952358197668395658" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/6952358197668395658" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2020/06/algorithms-with-predictions-survey-and.html" rel="alternate" type="text/html"/>
    <title>Algorithms with Predictions:  Survey and Workshop</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">There's a whole new, interesting theory trend  -- Algorithms with Predictions.  The idea, spurred by advances in machine learning, is that you assume you have predictor that tells you something about your input.  For example, in caching, you might have a prediction of when the item you are currently accessing will be next accessed.  Of course, machine learning predictions aren't perfect.  Still, you'd like to use this prediction to improve your caching algorithm, but from the theory side, we'd like provable statements.  For example, you could say, if my prediction is THIS good (e.g., the error is bounded under some metric), then my caching performance will correspondingly be at least THIS good (e.g., performance bounded in some way).<br/><br/>If you haven't seen the burgeoning spread of this line of work and are interested, you're in luck.  First, Sergei Vassilvitskii and I have written a <a href="https://arxiv.org/abs/2006.09123">brief survey that's now on the arxiv</a>.  We had written it for a collection Tim Roughgarden is organizing on Beyond Worst-Case Analysis (that we thought we be out by now, and should be out from the publisher soon-ish), but we've gone ahead and put a version on the arxiv to make it available.  The area is moving fast, so there are already many new results --  we hope to update the "survey" with new material as the area grows.<br/><br/>Second, one of the <a href="https://www.mit.edu/~vakilian/stoc-workshop.html">STOC'20 Workshops will be on Algorithms with Predictions</a>.  It will be on Friday from 1-4pm, with speakers Tim Roughgarden, Edith Cohen,  Ravi Kumar, and me.  I'll be talking about some of my recent work  (in submission) on queues with predictions, and partitioned learned Bloom filters.  (Arxiv papers are <a href="https://arxiv.org/abs/1902.00732">here</a>, <a href="https://arxiv.org/abs/1905.12155">here</a>, and <a href="https://arxiv.org/abs/2006.03176">here</a>, but maybe you want to see the talk first.)  I'll also do a blog post on partitioned learned Bloom filters in the near future.</div>
    </content>
    <updated>2020-06-17T17:52:00Z</updated>
    <published>2020-06-17T17:52:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2020-06-17T17:53:04Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-2271116647104476479</id>
    <link href="http://processalgebra.blogspot.com/feeds/2271116647104476479/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=2271116647104476479" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/2271116647104476479" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/2271116647104476479" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/06/logic-mentoring-workshop-2020.html" rel="alternate" type="text/html"/>
    <title>Logic Mentoring Workshop 2020</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="_5pbx userContent _3576" id="js_k"><a href="https://www2.csc.liv.ac.uk/~lehtinen/">Karoliina Lehtinen</a> asked me to encourage young researchers of all ages to attend this year's edition of the Logic Mentoring Workshop. (See <a href="https://lmw.mpi-sws.org/">here</a> for information.) The <a href="https://lmw.mpi-sws.org/speakers.html">set of speakers</a> is top class, registration is  free and I am sure that attending the event would be beneficial  to  many. Spread the news!<br/><br/>FWIW, I gave a talk at the <a href="https://lics.siglog.org/lics17/lmw.html">2017 edition</a> of the event and thoroughly enjoyed it. Even though the event is targeted at students, from senior undergraduates to graduates, I feel that I always learn something new from attending this kind of workshops/talks. </div></div>
    </content>
    <updated>2020-06-17T16:10:00Z</updated>
    <published>2020-06-17T16:10:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-06-18T21:29:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09352</id>
    <link href="http://arxiv.org/abs/2006.09352" rel="alternate" type="text/html"/>
    <title>A One-Pass Private Sketch for Most Machine Learning Tasks</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coleman:Benjamin.html">Benjamin Coleman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shrivastava:Anshumali.html">Anshumali Shrivastava</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09352">PDF</a><br/><b>Abstract: </b>Differential privacy (DP) is a compelling privacy definition that explains
the privacy-utility tradeoff via formal, provable guarantees. Inspired by
recent progress toward general-purpose data release algorithms, we propose a
private sketch, or small summary of the dataset, that supports a multitude of
machine learning tasks including regression, classification, density
estimation, near-neighbor search, and more. Our sketch consists of randomized
contingency tables that are indexed with locality-sensitive hashing and
constructed with an efficient one-pass algorithm. We prove competitive error
bounds for DP kernel density estimation. Existing methods for DP kernel density
estimation scale poorly, often exponentially slower with an increase in
dimensions. In contrast, our sketch can quickly run on large, high-dimensional
datasets in a single pass. Exhaustive experiments show that our generic sketch
delivers a similar privacy-utility tradeoff when compared to existing DP
methods at a fraction of the computation cost. We expect that our sketch will
enable differential privacy in distributed, large-scale machine learning
settings.
</p></div>
    </summary>
    <updated>2020-06-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09221</id>
    <link href="http://arxiv.org/abs/2006.09221" rel="alternate" type="text/html"/>
    <title>Testing systems of real quadratic equations for approximate solutions</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Alexander Barvinok <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09221">PDF</a><br/><b>Abstract: </b>Consider systems of equations $q_i(x)=0$, where $q_i: {\Bbb R}^n
\longrightarrow {\Bbb R}$, $i=1, \ldots, m$, are quadratic forms. We want to be
able to tell efficiently systems with many non-trivial solutions or near
solutions $x \ne 0$ from systems that are far from having a solution. For that,
we pick a penalty function $F: {\Bbb R} \longrightarrow [0, 1]$ with $F(0)=1$
and $F(y) &lt; 1$ for $y \ne 0$ and compute the expectation of $F(q_1(x)) \cdots
F(q_m(x))$ for a random $x$ sampled from the standard Gaussian measure in
${\Bbb R}^n$. We choose $F(y)=y^{-2}\sin^2 y$ and show that the expectation can
be approximated within relative error $0&lt; \epsilon &lt; 1$ in quasi-polynomial
time $(m+n)^{O(\ln (m+n)-\ln \epsilon)}$, provided each form $q_i$ depends on
not more than $r$ real variables, has common variables with at most $r-1$ other
forms and satisfies $|q_i(x)| \leq \gamma \|x\|^2/r$, where $\gamma &gt;0$ is an
absolute constant.
</p></div>
    </summary>
    <updated>2020-06-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09197</id>
    <link href="http://arxiv.org/abs/2006.09197" rel="alternate" type="text/html"/>
    <title>Dense Non-Rigid Structure from Motion: A Manifold Viewpoint</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Suryansh.html">Suryansh Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gool:Luc_Van.html">Luc Van Gool</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oliveira:Carlos_E=_P=_de.html">Carlos E. P. de Oliveira</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cherian:Anoop.html">Anoop Cherian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dai:Yuchao.html">Yuchao Dai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Hongdong.html">Hongdong Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09197">PDF</a><br/><b>Abstract: </b>Non-Rigid Structure-from-Motion (NRSfM) problem aims to recover 3D geometry
of a deforming object from its 2D feature correspondences across multiple
frames. Classical approaches to this problem assume a small number of feature
points and, ignore the local non-linearities of the shape deformation, and
therefore, struggles to reliably model non-linear deformations. Furthermore,
available dense NRSfM algorithms are often hurdled by scalability,
computations, noisy measurements and, restricted to model just global
deformation. In this paper, we propose algorithms that can overcome these
limitations with the previous methods and, at the same time, can recover a
reliable dense 3D structure of a non-rigid object with higher accuracy.
Assuming that a deforming shape is composed of a union of local linear subspace
and, span a global low-rank space over multiple frames enables us to
efficiently model complex non-rigid deformations. To that end, each local
linear subspace is represented using Grassmannians and, the global 3D shape
across multiple frames is represented using a low-rank representation. We show
that our approach significantly improves accuracy, scalability, and robustness
against noise. Also, our representation naturally allows for simultaneous
reconstruction and clustering framework which in general is observed to be more
suitable for NRSfM problems. Our method currently achieves leading performance
on the standard benchmark datasets.
</p></div>
    </summary>
    <updated>2020-06-17T23:34:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09167</id>
    <link href="http://arxiv.org/abs/2006.09167" rel="alternate" type="text/html"/>
    <title>Heterogeneous Parallelization and Acceleration of Molecular Dynamics Simulations in GROMACS</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/P=aacute=ll:Szil=aacute=rd.html">Szilárd Páll</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhmurov:Artem.html">Artem Zhmurov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bauer:Paul.html">Paul Bauer</a>, Mark Abraham, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lundborg:Magnus.html">Magnus Lundborg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gray:Alan.html">Alan Gray</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hess:Berk.html">Berk Hess</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lindahl:Erik.html">Erik Lindahl</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09167">PDF</a><br/><b>Abstract: </b>The introduction of accelerator devices such as graphics processing units
(GPUs) has had profound impact on molecular dynamics simulations and has
enabled order-of-magnitude performance advances using commodity hardware. To
fully reap these benefits, it has been necessary to reformulate some of the
most fundamental algorithms, including the Verlet list, pair searching and
cut-offs. Here, we present the heterogeneous parallelization and acceleration
design of molecular dynamics implemented in the GROMACS codebase over the last
decade. The setup involves a general cluster-based approach to pair lists and
non-bonded pair interactions that utilizes both GPUs and CPU SIMD acceleration
efficiently, including the ability to load-balance tasks between CPUs and GPUs.
The algorithm work efficiency is tuned for each type of hardware, and to use
accelerators more efficiently we introduce dual pair lists with rolling pruning
updates. Combined with new direct GPU-GPU communication as well as GPU
integration, this enables excellent performance from single GPU simulations
through strong scaling across multiple GPUs and efficient multi-node
parallelization.
</p></div>
    </summary>
    <updated>2020-06-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09123</id>
    <link href="http://arxiv.org/abs/2006.09123" rel="alternate" type="text/html"/>
    <title>Algorithms with Predictions</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitzenmacher:Michael.html">Michael Mitzenmacher</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vassilvitskii:Sergei.html">Sergei Vassilvitskii</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09123">PDF</a><br/><b>Abstract: </b>We introduce algorithms that use predictions from machine learning applied to
the input to circumvent worst-case analysis. We aim for algorithms that have
near optimal performance when these predictions are good, but recover the
prediction-less worst case behavior when the predictions have large errors.
</p></div>
    </summary>
    <updated>2020-06-17T23:25:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.09085</id>
    <link href="http://arxiv.org/abs/2006.09085" rel="alternate" type="text/html"/>
    <title>MCRapper: Monte-Carlo Rademacher Averages for Poset Families and Approximate Pattern Mining</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pellegrina:Leonardo.html">Leonardo Pellegrina</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cousins:Cyrus.html">Cyrus Cousins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vandin:Fabio.html">Fabio Vandin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Riondato:Matteo.html">Matteo Riondato</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.09085">PDF</a><br/><b>Abstract: </b>We present MCRapper, an algorithm for efficient computation of Monte-Carlo
Empirical Rademacher Averages (MCERA) for families of functions exhibiting
poset (e.g., lattice) structure, such as those that arise in many pattern
mining tasks. The MCERA allows us to compute upper bounds to the maximum
deviation of sample means from their expectations, thus it can be used to find
both statistically-significant functions (i.e., patterns) when the available
data is seen as a sample from an unknown distribution, and approximations of
collections of high-expectation functions (e.g., frequent patterns) when the
available data is a small sample from a large dataset. This feature is a strong
improvement over previously proposed solutions that could only achieve one of
the two. MCRapper uses upper bounds to the discrepancy of the functions to
efficiently explore and prune the search space, a technique borrowed from
pattern mining itself. To show the practical use of MCRapper, we employ it to
develop an algorithm TFP-R for the task of True Frequent Pattern (TFP) mining.
TFP-R gives guarantees on the probability of including any false positives
(precision) and exhibits higher statistical power (recall) than existing
methods offering the same guarantees. We evaluate MCRapper and TFP-R and show
that they outperform the state-of-the-art for their respective tasks.
</p></div>
    </summary>
    <updated>2020-06-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.08958</id>
    <link href="http://arxiv.org/abs/2006.08958" rel="alternate" type="text/html"/>
    <title>On the Hardness of Problems Involving Negator Relationships in an Artificial Hormone System</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hutter:Eric.html">Eric Hutter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pacher:Mathias.html">Mathias Pacher</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brinkschulte:Uwe.html">Uwe Brinkschulte</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.08958">PDF</a><br/><b>Abstract: </b>The Artificial Hormone System (AHS) is a self-organizing middleware to
allocate tasks in a distributed system. We extended it by so-called negator
hormones to enable conditional task structures. However, this extension
increases the computational complexity of seemingly simple decision problems in
the system: In [1] and [2], we defined the problems Negator-Path and
Negator-Sat and proved their NP-completeness. In this supplementary report to
these papers, we show examples of Negator-Path and Negator-Sat, introduce the
novel problem Negator-Stability and explain why all of these problems involving
negators are hard to solve algorithmically.
</p></div>
    </summary>
    <updated>2020-06-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.08949</id>
    <link href="http://arxiv.org/abs/2006.08949" rel="alternate" type="text/html"/>
    <title>Utility-Based Graph Summarization: New and Improved</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hajiabadi:Mahdi.html">Mahdi Hajiabadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Jasbir.html">Jasbir Singh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Srinivasan:Venkatesh.html">Venkatesh Srinivasan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thomo:Alex.html">Alex Thomo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.08949">PDF</a><br/><b>Abstract: </b>A fundamental challenge in graph mining is the ever-increasing size of
datasets. Graph summarization aims to find a compact representation resulting
in faster algorithms and reduced storage needs. The flip side of graph
summarization is the loss of utility which diminishes its usability. The key
questions we address in this paper are: (1)How to summarize a graph without any
loss of utility? (2)How to summarize a graph with some loss of utility but
above a user-specified threshold? (3)How to query graph summaries without graph
reconstruction?} We also aim at making graph summarization available for the
masses by efficiently handling web-scale graphs using only a consumer-grade
machine. Previous works suffer from conceptual limitations and lack of
scalability. In this work, we make three key contributions. First, we present a
utility-driven graph summarization method, based on a clique and independent
set decomposition, that produces significant compression with zero loss of
utility. The compression provided is significantly better than state-of-the-art
in lossless graph summarization, while the runtime is two orders of magnitude
lower. Second, we present a highly scalable algorithm for the lossy case, which
foregoes the expensive iterative process that hampers previous work. Our
algorithm achieves this by combining a memory reduction technique and a novel
binary-search approach. In contrast to the competition, we are able to handle
web-scale graphs in a single machine without a performance impediment as the
utility threshold (and size of summary) decreases. Third, we show that our
graph summaries can be used as-is to answer several important classes of
queries, such as triangle enumeration, Pagerank, and shortest paths. This is in
contrast to other works that incrementally reconstruct the original graph for
answering queries, thus incurring additional time costs.
</p></div>
    </summary>
    <updated>2020-06-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.08926</id>
    <link href="http://arxiv.org/abs/2006.08926" rel="alternate" type="text/html"/>
    <title>Computing Igusa's local zeta function of univariates in deterministic polynomial-time</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dwivedi:Ashish.html">Ashish Dwivedi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saxena:Nitin.html">Nitin Saxena</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.08926">PDF</a><br/><b>Abstract: </b>Igusa's local zeta function $Z_{f,p}(s)$ is the generating function that
counts the number of integral roots, $N_{k}(f)$, of $f(\mathbf x) \bmod p^k$,
for all $k$. It is a famous result, in analytic number theory, that $Z_{f,p}$
is a rational function in $\mathbb{Q}(p^s)$. We give an elementary proof of
this fact for a univariate polynomial $f$. Our proof is constructive as it
gives a closed-form expression for the number of roots $N_{k}(f)$.
</p>
<p>Our proof, when combined with the recent root-counting algorithm of (Dwivedi,
Mittal, Saxena, CCC, 2019), yields the first deterministic poly($|f|, \log p$)
time algorithm to compute $Z_{f,p}(s)$. Previously, an algorithm was known only
in the case when $f$ completely splits over $\mathbb{Q}_p$; it required the
rational roots to use the concept of generating function of a tree
(Z\'u\~niga-Galindo, J.Int.Seq., 2003).
</p></div>
    </summary>
    <updated>2020-06-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.08886</id>
    <link href="http://arxiv.org/abs/2006.08886" rel="alternate" type="text/html"/>
    <title>Distinct distances in the complex plane</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sheffer:Adam.html">Adam Sheffer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zahl:Joshua.html">Joshua Zahl</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.08886">PDF</a><br/><b>Abstract: </b>We prove that if $P$ is a set of $n$ points in $\mathbb{C}^2$, then either
the points in $P$ determine $\Omega(n^{1-\epsilon})$ complex distances, or $P$
is contained in a line with slope $\pm i$. If the latter occurs then each pair
of points in $P$ have complex distance 0.
</p></div>
    </summary>
    <updated>2020-06-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.08731</id>
    <link href="http://arxiv.org/abs/2006.08731" rel="alternate" type="text/html"/>
    <title>Exact and Metaheuristic Approaches for the Production Leveling Problem</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vass:Johannes.html">Johannes Vass</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lackner:Marie=Louise.html">Marie-Louise Lackner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musliu:Nysret.html">Nysret Musliu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.08731">PDF</a><br/><b>Abstract: </b>In this paper we introduce a new problem in the field of production planning
which we call the Production Leveling Problem. The task is to assign orders to
production periods such that the load in each period and on each production
resource is balanced, capacity limits are not exceeded and the orders'
priorities are taken into account. Production Leveling is an important
intermediate step between long-term planning and the final scheduling of orders
within a production period, as it is responsible for selecting good subsets of
orders to be scheduled within each period.
</p>
<p>A formal model of the problem is proposed and NP-hardness is shown by
reduction from Bin Backing. As an exact method for solving moderately sized
instances we introduce a MIP formulation. For solving large problem instances,
metaheuristic local search is investigated. A greedy heuristic and two
neighborhood structures for local search are proposed, in order to apply them
using Variable Neighborhood Descent and Simulated Annealing. Regarding exact
techniques, the main question of research is, up to which size instances are
solvable within a fixed amount of time. For the metaheuristic approaches the
aim is to show that they produce near-optimal solutions for smaller instances,
but also scale well to very large instances.
</p>
<p>A set of realistic problem instances from an industrial partner is
contributed to the literature, as well as random instance generators. The
experimental evaluation conveys that the proposed MIP model works well for
instances with up to 250 orders. Out of the investigated metaheuristic
approaches, Simulated Annealing achieves the best results. It is shown to
produce solutions with less than 3% average optimality gap on small instances
and to scale well up to thousands of orders and dozens of periods and products.
The presented metaheuristic methods are already being used in the industry.
</p></div>
    </summary>
    <updated>2020-06-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.08668</id>
    <link href="http://arxiv.org/abs/2006.08668" rel="alternate" type="text/html"/>
    <title>Algorithmic Aspects of Temporal Betweenness</title>
    <feedworld_mtime>1592352000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bu=szlig=:Sebastian.html">Sebastian Buß</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Molter:Hendrik.html">Hendrik Molter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niedermeier:Rolf.html">Rolf Niedermeier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rymar:Maciej.html">Maciej Rymar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.08668">PDF</a><br/><b>Abstract: </b>The betweenness centrality of a graph vertex measures how often this vertex
is visited on shortest paths between other vertices of the graph. In the
analysis of many real-world graphs or networks, betweenness centrality of a
vertex is used as an indicator for its relative importance in the network. In
particular, it is among the most popular tools in social network analysis. In
recent years, a growing number of real-world networks is modeled as temporal
graphs, where we have a fixed set of vertices and there is a finite discrete
set of time steps and every edge might be present only at some time steps.
While shortest paths are straightforward to define in static graphs, temporal
paths can be considered "optimal" with respect to many different criteria,
including length, arrival time, and overall travel time (shortest, foremost,
and fastest paths). This leads to different concepts of temporal betweenness
centrality and we provide a systematic study of temporal betweenness variants
based on various concepts of optimal temporal paths. Computing the betweenness
centrality for vertices in a graph is closely related to counting the number of
optimal paths between vertex pairs. We show that counting foremost and fastest
paths is computationally intractable (#P-hard) and hence the computation of the
corresponding temporal betweenness values is intractable as well. For shortest
paths and two selected special cases of foremost paths, we devise
polynomial-time algorithms for temporal betweenness computation. Moreover, we
also explore the distinction between strict (ascending time labels) and
non-strict (non-descending time labels) time labels in temporal paths. In our
experiments with established real-world temporal networks, we demonstrate the
practical effectiveness of our algorithms, compare the various betweenness
concepts, and derive recommendations on their practical use.
</p></div>
    </summary>
    <updated>2020-06-17T23:29:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17204</id>
    <link href="https://rjlipton.wordpress.com/2020/06/16/pnp/" rel="alternate" type="text/html"/>
    <title>P&lt;NP</title>
    <summary>Some thoughts on P versus NP Norbert Blum is a computer science theorist at the University of Bonn, Germany. He has made important contributions to theory over his career. Another claim to fame is he was a student of Kurt Mehlhorn, indeed the third of Mehlhorn’s eighty-eight listed students. Today I wish to discuss a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Some thoughts on P versus NP</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/?attachment_id=17188" rel="attachment wp-att-17188"><img alt="" class="alignright size-full wp-image-17188" src="https://rjlipton.files.wordpress.com/2020/06/unknown.jpeg?w=600"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Norbert Blum is a computer science theorist at the University of Bonn, Germany. He has made important contributions to theory over his career. Another claim to fame is he was a student of Kurt Mehlhorn, indeed the <a href="https://www.genealogy.math.ndsu.nodak.edu/id.php?id=35475&amp;fChrono=1">third</a> of Mehlhorn’s eighty-eight listed students.</p>
<p>
Today I wish to discuss a new paper by Blum.</p>
<p>
No, it does not solve the P versus NP problem. The title of his paper is: <i>On the Approximation Method and the P versus NP Problem</i>. Its is available <a href="https://arxiv.org/pdf/1708.03486.pdf">here</a>.</p>
<p>
Blum. like most complexity theorists, believes that P is weaker than NP. This is usually stated as P<img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/>NP. The staff at GLL have the idea that we should state this as 	</p>
<p align="center"><img alt="\displaystyle  P &lt; NP. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P+%3C+NP.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  P &lt; NP. "/></p>
<p>This is clearer, more to the point, and logically what P<img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/>NP actually says. We will soon have T-shirts, mugs, and other stuff available in our web store at https:donotgotothisaddressplease.com. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/06/16/pnp/mug2-2/" rel="attachment wp-att-17207"><img alt="" class="aligncenter size-medium wp-image-17207" height="257" src="https://rjlipton.files.wordpress.com/2020/06/mug2-1.png?w=300&amp;h=257" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
</p><p/><h2> Three Years Ago </h2><p/>
<p/><p>
In 2017 Blum released a <a href="https://arxiv.org/abs/1708.03486">paper</a> that tried to prove P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP. It caused a sensation—it was discussed on the complexity blogs such as <a href="https://lucatrevisan.wordpress.com/2017/08/15/on-norbert-blums-claimed-proof-that-p-does-not-equal-np/">In theory</a> and <a href="https://www.scottaaronson.com/blog/?p=3409">Shtetl-Optimized</a>. And also at <a href="https://rjlipton.wordpress.com/2017/08/17/on-the-edge-of-eclipses-and-pnp/">GLL</a>. Blum’s paper got thousands of Twitter mentions. Unfortunately he had to retract it, since it is wrong: He said: </p>
<blockquote><p><b> </b> <em> The proof is wrong. I shall elaborate precisely what the mistake is. For doing this, I need some time. I shall put the explanation on my homepage </em>
</p></blockquote>
<p>Look at <a href="https://johncarlosbaez.wordpress.com/2017/08/15/norbert-blum-on-p-versus-np/">here</a> for more comments that were made after his paper was released. </p>
<p>
He did, months later in 2017, post a two-page retraction<br/>
<a href="http://theory.cs.uni-bonn.de/blum/PvsNP/mistake.pdf">here</a>. His original paper’s abstract: </p>
<blockquote><p><b> </b> <em> Berg and Ulfberg and Amano and Maruoka have used CNF-DNF-approximators to prove exponential lower bounds for the monotone network complexity of the clique function and of Andreev’s function. We show that these approximators can be used to prove the same lower bound for their non-monotone network complexity. This implies P not equal NP. </em>
</p></blockquote>
<p>This approach is what we will discuss.</p>
<p>
</p><p/><h2> Today </h2><p/>
<p/><p>
Blum’s new paper does not claim to prove P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP, but gives his thoughts on P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP. I think he has earned our attention. It must have been difficult to go from thinking you have solved <i>the problem</i> to retracting your paper. I have thought, privately, that I had solved some neat problems. Only later to discover that I was wrong. I cannot imagine how tough it was to do this in public. </p>
<p>
</p><p/><h2> The Idea </h2><p/>
<p/><p>
Blum’s work on proving lower bounds began with his dissertation under Mehlhorn, which included a 1985 <a href="https://www.sciencedirect.com/science/article/pii/0304397585900301">paper</a> on monotone network complexity for convolutions. Earlier in 1984 Blum <a href="https://reader.elsevier.com/reader/sd/pii/0304397583900294? which was improved token=1F67F05B416D89618750BC409200E17D536C46207555E1A9FD524B2592630E400FC2C597EA81F7190D2A747A4B82F28B">proved</a> a lower bound of order <img alt="{3n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3n}"/>. This stood for thirty years until in 2015 Magnus Find, Alexander Golovnev, Edward Hirsch, and Alexander Kulikov <a href="https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=919494">improved</a> it to order <img alt="{3.011n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3.011n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3.011n}"/>. A long way from super-polynomial lower bounds. See also a <a href="https://simons.berkeley.edu/sites/default/files/docs/3815/20151001gateelimination.pdf">talk</a> about this work. </p>
<p>
Blum’s new paper discusses an old approach to prove boolean circuit lower bounds. The methods he used in 1984 and those improved in 2015 do not seem to be on track to prove even non-linear circuit lower bounds. </p>
<p>
Let’s look at his comments at a high level. See his paper for details. </p>
<p>
Suppose that one has a boolean function <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/> that is monotone: recall this means that if <img alt="{f(x)=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)=1}"/>, then changing some input <img alt="{x_{k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{k}}"/> from <img alt="{0 \rightarrow 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%5Crightarrow+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0 \rightarrow 1}"/> does not change the value of <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/>. Then it is always possible to compute <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/> without using any negations: only and/or operations are needed. Sometimes one can prove that the number of such operations is super-linear, sometimes even super-polynomial. Even bounds in this restricted model can be deep.</p>
<p>
The idea that has tempted Blum and many other complexity theorists is: Can we extend the proofs for lower bounds without negations to ones with negations? One problem is there is a function <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/> so that the following are true: </p>
<ol>
<li>
The function is monotone; <p/>
</li><li>
The function can be computed in polynomial time; <p/>
</li><li>
Any monotone circuit for computing the function requires exponential size
</li></ol>
<p>This is the famous <a href="https://en.wikipedia.org/wiki/Tardos_function">Tardos function</a> due to Éva Tardos. The existence of this function sunk Blum’s original paper. And it makes life hard for this general program—this is an instance of what our previous <a href="https://rjlipton.wordpress.com/2020/06/13/proof-checking-not-line-by-line/">post</a> meant by a proof attempt running up against a fundamental law. Negations can help tremendously in computing a function. </p>
<p>
</p><p/><h2> Blum’s Paper </h2><p/>
<p/><p>
In his new <a href="https://arxiv.org/pdf/1708.03486.pdf">paper</a> he surveys boolean complexity ideas—especially those linked to monotone complexity. He begins by trying to argue that the largeness feature of the <a href="https://rjlipton.wordpress.com/2009/03/25/whos-afraid-of-natural-proofs/">natural proofs</a> barrier, which applies to combinatorial properties defined via sub-additive circuit complexity measures, does not constrain approximation complexity measures of the kind he envisions. He then proceeds to define <em>CNF-DNF approximators</em> and further what he calls <em>sunflower approximators</em>. He does enough development to highlight a missing piece of information about monomial representations of <em>non-</em>approximated pieces of the Boolean function one is trying to prove hard. He concludes that without this information, his methods cannot even prove super-<em>linear</em> size lower bounds on general circuits.</p>
<p>
He ends with this assessment: </p>
<blockquote><p><b> </b> <em> How to proceed the work with respect to the P versus NP problem? Currently, I am convinced that we are far away to prove a super-polynomial lower bound for the non-monotone complexity of any explicit Boolean function. On the other hand, the strongest barrier towards proving P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP could be that it holds P = NP. To ensure that the whole time spent for working on the P versus NP problem is not used to prove an impossible theorem, I would switch to the try to develop a polynomial algorithm for the solution of an NP-complete problem. </em>
</p></blockquote>
<p/><p>
Note, we have changed his P<img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/>NP to P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP. Ken and I agree with him on trying to work both on P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP and P=NP. However, see our comments below. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I applaud Blum for thinking about P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP. We need people to be fearless if it is ever going to be solved. However, I personally believe that his approach may be wrong:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> I am not as sure as he is that P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP. I do think that P=NP is possible, especially if algorithms are allowed to be <a href="https://en.wikipedia.org/wiki/Galactic_algorithm">galactic</a>. Recall these are algorithms that run in polynomial time, but in polynomials of astronomical degree.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Also I am not sure if the boolean approach to P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP is the right one. Suppose there is a <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> so that SAT has boolean circuits of size <img alt="{n^{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^{C}}"/> where 	</p>
<p align="center"><img alt="\displaystyle  C = 2^{2^{2^{10000}}}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%3D+2%5E%7B2%5E%7B2%5E%7B10000%7D%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C = 2^{2^{2^{10000}}}. "/></p>
<p>It still could be the case that P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP, since there may be no uniform algorithm for SAT.</p>
<p>
Restating the last point: I believe we should try to prove what is needed, and not any more. The approach to P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP based on boolean circuit complexity is trying to prove too much. A proof that SAT has super-polynomial circuits does imply more than P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP. A proof that SAT cannot be solved in time <img alt="o(n\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=o%28n%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="o(n\log n)"/> on a multitape Turing machine would imply much less than P &lt; NP, yet still be a breakthrough</p>
<p>
Be cheap, prove the least possible. </p>
<p/></font></font></div>
    </content>
    <updated>2020-06-16T20:40:41Z</updated>
    <published>2020-06-16T20:40:41Z</published>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-06-18T22:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/092</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/092" rel="alternate" type="text/html"/>
    <title>TR20-092 |  Computing Igusa&amp;#39;s local zeta function of univariates in deterministic polynomial-time | 

	Ashish Dwivedi, 

	Nitin Saxena</title>
    <summary>Igusa's local zeta function $Z_{f,p}(s)$ is the generating function that counts the number of integral roots, $N_{k}(f)$, of $f(\mathbf x) \bmod p^k$, for all $k$. It is a famous result, in analytic number theory, that $Z_{f,p}$ is a rational function in $\mathbb{Q}(p^s)$. We give an elementary proof of this fact for a univariate polynomial $f$. Our proof is constructive as it gives a closed-form expression for the number of roots $N_{k}(f)$. 

Our proof, when combined with the recent root-counting algorithm of (Dwivedi, Mittal, Saxena, CCC, 2019), yields the first deterministic poly($|f|, \log p$) time algorithm to compute $Z_{f,p}(s)$. Previously, an algorithm was known only in the case when $f$ completely splits over $\mathbb{Q_p}$; it required the rational roots to use the concept of generating function of a tree (Zuniga-Galindo, J.Int.Seq., 2003).</summary>
    <updated>2020-06-16T08:32:14Z</updated>
    <published>2020-06-16T08:32:14Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-18T22:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/06/15/postdoc-positions-in-tcs-at-university-of-copenhagen-apply-by-july-6-2020/</id>
    <link href="https://cstheory-jobs.org/2020/06/15/postdoc-positions-in-tcs-at-university-of-copenhagen-apply-by-july-6-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc positions in TCS at University of Copenhagen (apply by July 6, 2020)</title>
    <summary>The CS department at the University of Copenhagen invites applications for postdoc positions in TCS. The application deadline is July 6, 2020. See https://employment.ku.dk/faculty/?show=151975 for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to mthorup@di.ku.dk or jn@di.ku.dk. Website: https://employment.ku.dk/faculty/?show=151975 Email: jn@di.ku.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The CS department at the University of Copenhagen invites applications for postdoc positions in TCS. The application deadline is July 6, 2020. See <a href="https://employment.ku.dk/faculty/?show=151975">https://employment.ku.dk/faculty/?show=151975</a> for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to mthorup@di.ku.dk or jn@di.ku.dk.</p>
<p>Website: <a href="https://employment.ku.dk/faculty/?show=151975">https://employment.ku.dk/faculty/?show=151975</a><br/>
Email: jn@di.ku.dk</p></div>
    </content>
    <updated>2020-06-15T21:51:22Z</updated>
    <published>2020-06-15T21:51:22Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-18T22:20:57Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/06/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/06/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Graduata data structures online (), finally done and graded. Warning: dry voice-over-slides videos, and some mistakes, because I didn’t have time to put together anything more sophisticated or edit more carefully.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.ics.uci.edu/~eppstein/261/">Graduata data structures online</a> (<a href="https://mathstodon.xyz/@11011110/104266993520726615"/>), finally done and graded. Warning: dry voice-over-slides videos, and some mistakes, because I didn’t have time to put together anything more sophisticated or edit more carefully.</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2020/06/02/list-100-times-law-enforceme.html">Freedom of the press under attack: 100+ times law enforcement violently assaulted journalists in US at George Floyd protests</a> (<a href="https://mathstodon.xyz/@11011110/104276714124643888"/>). Of course this is only a small piece of an enormous pattern of awfulness by the current administration, law enforcement, and the prison-industrial complex, but it’s a piece that I think is important to document.</p>
  </li>
  <li>
    <p><a href="https://blog.archive.org/2020/06/01/four-commercial-publishers-filed-a-complaint-about-the-internet-archives-lending-of-digitized-books/">Four book publishing corporations claim that what libraries always have done (lending out copies of books they have purchased as physical objects) is illegal, because computer, and are suing the Internet Archive over it</a> (<a href="https://mathstodon.xyz/@11011110/104283895750093044"/>, <a href="https://www.theverge.com/2020/6/1/21277036/internet-archive-publishers-lawsuit-open-library-ebook-lending">via</a>, <a href="https://torrentfreak.com/publishers-sue-the-internet-archive-over-its-open-library-declare-it-a-pirate-site-200601/">via2</a>). One of them, Wiley, is also a major publisher of academic works. Perhaps that should give some of us pause in which journals we send our papers to and referee for.</p>
  </li>
  <li>
    <p><a href="https://observablehq.com/@otaviocv/moire-patterns-from-random-dots">Moiré patterns from random dots</a> (<a href="https://mathstodon.xyz/@11011110/104290097720006041"/>). Overlaying the same random dot pattern on a translated and rotated copy of itself shows concentric dots around the center of rotation, illustrating <a href="https://en.wikipedia.org/wiki/Chasles%27_theorem_(kinematics)">Chasles’ theorem</a> that every rigid transformation of the plane is a translation or rotation. The effect seems to have first been observed by Leon Glass in “<a href="https://doi.org/10.1038/223578a0">Moiré patterns from random dots</a>” (<em>Nature</em>, 1969).</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1007/978-3-030-48966-3_3">“The Micro-world of Cographs”, Alecu, Lozin, and de Werra, <em>IWOCA</em> 2020</a> (<a href="https://mathstodon.xyz/@11011110/104295850760016598"/>). <a href="https://en.wikipedia.org/wiki/Cograph">Cographs</a> have a simple structure, but there’s still an interesting hierarchy of subclasses of graphs within them restricting different parameters of graph complexity to be bounded. A typical result: Every cograph with large h-index must contain a large complete graph, balanced bipartite graph, or forest of many high-degree stars.</p>
  </li>
  <li>
    <p><a href="https://www.theguardian.com/technology/2020/jun/01/cutting-edge-japanese-paper-art-inspires-a-non-slip-shoe">Japanese scientists use kirigami to design a shoe sole with pop-up non-slip spikes</a>.</p>
  </li>
  <li>
    <p><a href="https://mamot.fr/@starifi/104246098809527372">Ombre et lumière</a>. Artwork in which random-looking blocks on a wall create a recognizable shadow in side-light.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/a/362569/440">Brian Hopkins answers his own 9-year-old question on the history of  Fibonacci numbers and compositions</a> (<a href="https://mathstodon.xyz/@11011110/104312192948087479"/>). The ancient Indians knew that compositions (ordered partitions of integers) into ’s and ’s are counted by Fibonacci numbers. For instance, there five ways of forming  as an ordered sum of ’s and ’s:     . Cayley knew that the compositions with all parts bigger than  have Fibonacci counts. But who first knew that compositions with all parts odd are also counted by Fibonacci? Hopkins suggests: de Morgan, 1846.</p>
  </li>
  <li>
    <p>Despite new US covid cases being more or less the same level (or worse) as the start of the lockdown in March, <a href="https://www.chronicle.com/article/Faculty-Want-a-Say-in-Whether/248951">some universities are telling their students that it’s safe to return to normal and at the same time telling their faculty that unless they’re close to retirement age and have additional medical conditions, they must teach face to face</a> (<a href="https://mathstodon.xyz/@11011110/104318055800990449"/>).</p>
  </li>
  <li>
    <p><a href="https://drericsilverman.wordpress.com/games/">A nice page of recent writings about abstract strategy games, mostly connection games</a> (<a href="https://mathstodon.xyz/@jsiehler/104241019767261031"/>).</p>
  </li>
  <li>
    <p><a href="https://news.mit.edu/2020/guided-by-open-access-principles-mit-ends-elsevier-negotiations-0611">MIT gives up on trying to get an equitable subscription deal from Elsevier, ends negotiations</a> (<a href="https://mathstodon.xyz/@11011110/104326335350221116"/>, <a href="https://news.ycombinator.com/item?id=23489068">via</a>).</p>
  </li>
  <li>
    <p><em><a href="https://drops.dagstuhl.de/opus/portals/lipics/index.php?semnr=16150">The Proceedings of the 17th Scandinavian Symposium and Workshops on Algorithm Theory (SWAT 2020)</a></em> (<a href="https://mathstodon.xyz/@11011110/104334978432657954"/>), newly published open-access through LIPIcs. Sadly, the conference will be online rather than in the Faroe Islands as originally planned. <em><a href="https://drops.dagstuhl.de/opus/portals/lipics/index.php?semnr=16149">The Proceedings of the 36th International Symposium on Computational Geometry (SoCG 2020)</a></em> is also now out.</p>
  </li>
  <li>
    <p><a href="https://www.win.tue.nl/~kbuchin/proj/ruler/art/">Illuminate</a> (<a href="https://mathstodon.xyz/@11011110/104340239573347579"/>). An online puzzle based on the art gallery theorem, part of the media exposition of this year’s Symposium on Computational Geometry. See also the <a href="https://doi.org/10.4230/LIPIcs.SoCG.2020.80">theoretical writeup</a>.</p>
  </li>
  <li>
    <p><a href="https://link.springer.com/book/10.1007/978-1-84800-070-4">Skiena’s <em>Algorithm Design Manual</em></a> (<a href="https://mathstodon.xyz/@11011110/104349420443124730"/>, <a href="https://news.ycombinator.com/item?id=23529759">via</a>, <a href="https://news.ycombinator.com/item?id=23055340">via2</a>, <a href="https://www.metafilter.com/187489/Free-Textbooks">see also</a>), one of 500 Springer textbooks still available for free download from the publisher.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-06-15T17:54:00Z</updated>
    <published>2020-06-15T17:54:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-06-16T00:55:22Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3523309889572389823</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3523309889572389823/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/06/presentations-of-diffie-helman-leave.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3523309889572389823" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3523309889572389823" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/06/presentations-of-diffie-helman-leave.html" rel="alternate" type="text/html"/>
    <title>Presentations of Diffie-Helman leave out how to find g</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">When I first taught Diffie Helman I read the following<br/>
1) Alice and Bob agree on p a prime and g a generator<br/>
2) Alice picks a, sends g^a to Bob, Bob picks b, sends g^b to Alice<br/>
3) Alice computes (g^b)^a and Bob computes (g^a)^b so they both have g^{ab}<br/>
<br/>
I knew how to find a prime- pick a number of length n (perhaps make sure the last digit is not even) and test for primality, if not then try again, you'll get one soon enough. I did not know how to find g. I had thought you<i> first </i>find p, and<i> then</i> given p you find g. I then figured out that you make actually pick  p to be a  <i>safe prime</i>, so q=(p-1)/2 is a prime, and then just pick random g and test them via computing  g^2  and g^q: if neither is 1 then g is a generator. You will find a generator soon enough.<br/>
<br/>
That was all fine. But how come my source didn't <i>say </i>how to find g.?You need to know that to run the algorithm. That was years ago. Then I wondered how common it is for an explanation to not say how to find g. So I Googled ``Diffie-Helman'' I only record those that had some technical content to them, and were not about other DH such as Elliptic Curves.<br/>
<br/>
0) <a href="http://www.cs.jhu.edu/~rubin/courses/sp03/papers/diffie.hellman.pdf">The Original DH paper</a> Page 34:<i> alpha is a fixed primitive element of GF(alpha)</i>. No mention of how to find either the prime q or the prim root alpha.<br/>
<br/>
1) <a href="https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange">Wikiepdia</a>: ... <i>protocol uses the mult group of integers mod p, where p is a prime and g is a prim</i> <i>root mod p</i>. NO mention of how they find p or g.<br/>
<br/>
2) <a href="https://mathworld.wolfram.com/Diffie-HellmanProtocol.html">Wolfram's MathWorld</a>:<i> They agree on two prime numbers g and p, where p is large and g is a prim root mod p. In practice it is good to choose p such that (p-1)/2 is also prime. </i>They mention (p-1)/2 but not for the reason I give. (There are algorithms for Discrete Log that do well if (p-1)/2 has many factors.)<br/>
<br/>
3) <a href="https://www.comparitech.com/blog/information-security/diffie-hellman-key-exchange/">Comparatech</a>: <i>Alice and Bob start out by deciding two numbers p and g.</i> No mention of how to find p or g.<br/>
<br/>
4) <a href="https://searchsecurity.techtarget.com/definition/Diffie-Hellman-key-exchange">Searchsecurity</a> Won't bother quoting, but more of the same, no mention of how to find p or g.<br/>
<br/>
5) <a href="https://doubleoctopus.com/security-wiki/encryption-and-cryptography/diffie-hellman-algorithm/">The Secret Security Wiki</a> <i>Alice and Bob agree on p and g</i>.<br/>
<br/>
6) <a href="https://www.sciencedirect.com/topics/computer-science/diffie-hellman">Science Direct</a> More of the same.<br/>
<br/>
7) <a href="https://www.math.ucla.edu/~baker/40/handouts/rev_DH/node1.html">Notes from a UCLA Crypto Course</a> YEAH! They say how to find g.<br/>
<br/>
8) <a href="https://brilliant.org/wiki/diffie-hellman-protocol/">Brilliant (yes that really is the name of this site)</a> Brilliant? Not brilliant enough to realize you need to say how to find p and g.<br/>
<br/>
9) <a href="https://wiki.openssl.org/index.php/Diffie_Hellman">OpenSSL</a> Hard to tell. Their intuitive explanation leaves it out, but they have details below and code that might have it.<br/>
<br/>
<br/>
I looked at a few more but it was the same story.<br/>
<br/>
This is NOT a RANT or even a complaint, but its a question:<br/>
<br/>
<b>Why do so few expositions of DH mention how to find p,g? You really need to do that if you really want to DO DH.</b><br/>
<b><br/></b>
Speculation<br/>
<br/>
1) Some of the above are for the laymen and hence can not get into that. But some are not.<br/>
<br/>
2) Some of them are for advanced audiences who would know how to do it. Even so, how to find the generator really needs to be mentioned.<br/>
<br/>
3) Goldilocks: Some papers are for the layman who would not notice the gap, and some papers are for the expert who can fill in the gap themselves, so no paper in between. I do not believe that.<br/>
<br/>
4) The oddest of the above is that the original paper did not say how to find g.<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2020-06-15T15:17:00Z</updated>
    <published>2020-06-15T15:17:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-06-18T16:42:45Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/06/14/infinite-threshold-graphs</id>
    <link href="https://11011110.github.io/blog/2020/06/14/infinite-threshold-graphs.html" rel="alternate" type="text/html"/>
    <title>Infinite threshold graphs, four different ways</title>
    <summary>One of the difficulties of extending results from finite graphs to infinite ones is that it is not always obvious how to extend the definitions. A single class of finite graphs may correspond, in the infinite graph world, to several different natural classes of infinite graphs. One of the ways this can happen is through orderings: if a class of graphs has a natural ordering on its vertices (say, through a construction in which graphs in this class are built up by adding one vertex at a time) then we might get several classes of infinite graphs with different ways of restricting or not restricting this vertex ordering.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>One of the difficulties of extending results from finite graphs to infinite ones is that it is not always obvious how to extend the definitions. A single class of finite graphs may correspond, in the infinite graph world, to several different natural classes of infinite graphs. One of the ways this can happen is through orderings: if a class of graphs has a natural ordering on its vertices (say, through a construction in which graphs in this class are built up by adding one vertex at a time) then we might get several classes of infinite graphs with different ways of restricting or not restricting this vertex ordering.</p>

<p style="text-align: center;"><img alt="Threshold graph" src="https://11011110.github.io/blog/assets/2020/threshold.svg"/></p>

<p>As an example of this phenomenon, consider the <a href="https://en.wikipedia.org/wiki/Threshold_graph">threshold graphs</a>, one of the simplest classes of finite graphs. An example is shown above. These can be defined in multiple equivalent ways:</p>

<ul>
  <li>
    <p>The finite threshold graphs are the graphs that can be built up by repeatedly adding either a universal vertex or an isolated vertex to a smaller graph. In the example, the vertices have been added in left-to-right order, with isolated vertices depicted in yellow and universal vertices in blue. This can be formalized in a way that extends to infinite graphs by saying that they are the graphs in which every nonempty induced subgraph contains either a universal vertex or an isolated vertex. Let’s call these graphs the “inductive threshold graphs”.</p>
  </li>
  <li>
    <p>We can also construct graphs in the reverse ordering, by repeatedly adding a vertex whose neighbors form a clique and whose non-neighbors form an independent set. More concisely, the vertex is both simplicial and cosimplicial. The example above can be constructed in this way by adding the vertices in right-to-left order, with the blue neighbors of each added vertex forming a clique and the yellow neighbors forming an independent set. This can be formalized in a way that extends to infinite graphs by requiring that every induced subgraph has a vertex that is simplicial and cosimplicial. Let’s call a graph with this property a “coinductive threshold graph”.</p>
  </li>
  <li>
    <p>The finite threshold graphs are the -free finite graphs, meaning that no four vertices form an induced subgraph that is a path, cycle, or perfect matching.</p>
  </li>
  <li>
    <p>The finite threshold graphs get their name from the following property: they are the graphs that we can generate by assigning weights in the interval  to the vertices and connecting two vertices by an edge whenever their sum of weights is at least one. Let’s call a graph with this property a “real threshold graph”.</p>
  </li>
</ul>

<p>All four of these properties are <a href="https://en.wikipedia.org/wiki/Hereditary_property">hereditary</a>: if a graph has the property, so do all its induced subgraphs. Because the three forbidden subgraphs , , and  have no universal or isolated vertex, have no simplicial and cosimplicial vertex, and have no valid weight assignment, the inductive threshold graphs, coinductive threshold graphs, and real threshold graphs are all subclasses of the -free graphs.</p>

<p>If a graph is -free, we can define a relation  on its vertices by saying that  if there is no vertex  with  forming an induced path or complement of a path. It follows from the nonexistence of the forbidden subgraphs that this is a total preorder, that every vertex  is universal or isolated among the vertices , and that every vertex  is simplicial and cosimplicial among the vertices . In the example above, two vertices are in the same equivalence class of the ordering if they are in a contiguous block of vertices with the same color, and otherwise their ordering according to  is the same as their left-to-right ordering. Because every finite total preorder has a minimal and a maximal element, every finite -free graph is an inductive threshold graph and a coinductive threshold graph.</p>

<p>However, these properties differ for infinite graphs. In an infinite inductive threshold graph, the total preorder must obey the <a href="https://en.wikipedia.org/wiki/Ascending_chain_condition">ascending chain condition</a> that there be no strictly-increasing infinite sequence of vertices, for the subgraph induced by the vertices of such a sequence would have no isolated or universal vertex. Conversely, if the order does obey the ascending chain condition, one could find an isolated or universal vertex in any subgraph by starting from an arbitrary vertex and repeatedly moving upwards in the order until getting stuck. So the inductive threshold graphs are exactly the ones whose order obeys the ascending chain condition. Similarly, the coinductive threshold graphs are exactly the ones whose order obeys the descending chain condition. But it is easy to construct orders that violate one or both of these conditions. A graph can only be a real threshold graph if the total order on the equivalence classes of its preorder can be embedded into , and again this is not true of all total orders.</p>

<p>One consequence of this difference between classes of infinite graphs is the construction of natural statements in the first-order <a href="https://en.wikipedia.org/wiki/Logic_of_graphs">logic of graphs</a> that are true for all finite graphs but untrue for some infinite graphs. For instance, the statements that a -free graph has an isolated or universal vertex, and that a -free graph has a simplicial and cosimplicial vertex, are both true of all finite graphs, but untrue of some infinite graphs.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104345188939710302">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-06-14T15:50:00Z</updated>
    <published>2020-06-14T15:50:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-06-16T00:55:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/091</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/091" rel="alternate" type="text/html"/>
    <title>TR20-091 |  Randomized polynomial-time equivalence between determinant and trace-IMM equivalence tests | 

	Janaky Murthy, 

	vineet nair, 

	Chandan Saha</title>
    <summary>Equivalence testing for a polynomial family $\{g_m\}_{m \in \mathbb{N}}$ over a field F is the following problem: Given black-box access to an $n$-variate polynomial $f(\mathbb{x})$, where $n$ is the number of variables in $g_m$ for some $m \in \mathbb{N}$, check if there exists an $A \in \text{GL}(n,\text{F})$ such that $f(\mathbb{x}) = g_m(A\mathbb{x})$. If yes, then output such an $A$. The complexity of equivalence testing has been studied for a number of important polynomial families, including the determinant (Det) and the family of iterated matrix multiplication polynomials. Two popular variants of the iterated matrix multiplication polynomial are: IMM$_{w,d}$ (the $(1,1)$ entry of the product of $d$ many $w\times w$  symbolic matrices) and Tr-IMM$_{w,d}$ (the trace of the product of $d$ many $w\times w$ symbolic matrices). The families - Det, IMM and Tr-IMM - are VBP-complete under $p$-projections, and so, in this sense, they have the same complexity. But, do they have the same equivalence testing complexity? We show that the answer is 'yes' for Det and Tr-IMM (modulo the use of randomness). 

The above result may appear a bit surprising as the complexity of equivalence testing for IMM and that for Det are quite different over rationals: a randomized polynomial-time equivalence testing for IMM over rationals is known [Kayal,Nair,Saha,Tavenas 2019], whereas [Garg,Gupta,Kayal,Saha 2019] showed that equivalence testing for Det over rationals is integer factoring hard (under randomized reductions and assuming GRH). To our knowledge, the complexity of equivalence testing for Tr-IMM was not known before this work. We show that, despite the syntactic similarity between IMM and Tr-IMM, equivalence testing for Tr-IMM and that for Det are randomized polynomial-time Turing reducible to each other over any field of characteristic zero or sufficiently large. The result is obtained by connecting the two problems via another well-studied problem in computer algebra, namely the full matrix algebra isomorphism problem (FMAI). In particular, we prove the following: 

1.Testing equivalence of polynomials to Tr-IMM$_{w,d}$, for $d\geq 3$ and $w\geq 2$, is randomized polynomial-time Turing reducible to testing equivalence of polynomials to Det$_w$, the determinant of the $w \times w$ matrix of formal variables. (Here, $d$ need not be a constant.)

2. FMAI is randomized polynomial-time Turing reducible to equivalence testing (in fact, to tensor isomorphism testing) for the family of matrix multiplication tensors $\{$Tr-IMM$_{w,3}\}_{w \in \mathbb{N}}$.

These results, in conjunction with the randomized poly-time reduction (shown in [GGKS19]) from determinant equivalence testing to FMAI, imply that the four problems - FMAI, equivalence testing for Tr-IMM and for Det, and the $3$-tensor isomorphism problem for the family of matrix multiplication tensors - are randomized poly-time equivalent under Turing reductions.</summary>
    <updated>2020-06-14T13:41:43Z</updated>
    <published>2020-06-14T13:41:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-18T22:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17191</id>
    <link href="https://rjlipton.wordpress.com/2020/06/13/proof-checking-not-line-by-line/" rel="alternate" type="text/html"/>
    <title>Proof Checking: Not Line by Line</title>
    <summary>Proofs and perpetual motion machines Leonardo da Vinci is, of course, famous for his paintings and drawings, but was also interested in inventions, and in various parts of science including mathematics and engineering. It is hard to imagine that he died over 500 years ago, given his continued impact on our world. He invented practical […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Proofs and perpetual motion machines</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2020/06/picture.png"><img alt="" class="alignright  wp-image-17193" height="150" src="https://rjlipton.files.wordpress.com/2020/06/picture.png?w=200&amp;h=150" width="200"/></a></p>
<p>
Leonardo da Vinci is, of course, famous for his paintings and drawings, but was also interested in inventions, and in various parts of science including mathematics and engineering. It is hard to imagine that he died over 500 years ago, given his continued impact on our world. He invented practical and impractical <a href="https://en.wikipedia.org/wiki/Leonardo_da_Vinci">inventions</a>: musical instruments, a mechanical knight, hydraulic pumps, reversible crank mechanisms, finned mortar shells, and a steam cannon.</p>
<p>
Today I wish to discuss proofs and perpetual motion machines.</p>
<p>
You might ask: <i>What do proofs and perpetual motion machines have in common?</i> Proofs refer to math proofs that claim to solve open problems like P <img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/> NP. Ken and I get such claims all time. I take a look at them, not because I think they are likely to be correct. Rather because I am interested in understanding how people think. </p>
<p>
I started to work on discussing such proofs when I realized that such “proofs” are related to claims about perpetual motion machines. Let’s see how.</p>
<p>
</p><p/><h2> Perpetual Motion Machines </h2><p/>
<p/><p>
A perpetual motion <a href="https://en.wikipedia.org/wiki/Perpetual_motion">machine</a> is a machine that operates indefinitely without an energy source. This kind of machine is impossible, as da Vinci knew already:</p>
<blockquote><p><b> </b> <em> Oh ye seekers after perpetual motion, how many vain chimeras have you pursued? Go and take your place with the alchemists. <br/>
—da Vinci, 1494 </em>
</p></blockquote>
<p/><p>
I like this statement about applying for US patents on such machines: </p>
<blockquote><p><b> </b> <em> Proposals for such inoperable machines have become so common that the United States Patent and Trademark Office (USPTO) has made an official policy of refusing to grant patents for perpetual motion machines without a working model. </em>
</p></blockquote>
<p/><p>
Here is a classic attempt at perpetual motion: The motion goes on “forever” since the right side floats up and the left side falls down. </p>
<p><a href="https://rjlipton.files.wordpress.com/2020/06/float.png"><img alt="" class="aligncenter size-full wp-image-17194" src="https://rjlipton.files.wordpress.com/2020/06/float.png?w=600"/></a></p>
<p>
The analogy of proofs and to perpetual motion machines is: The debunking such a machine is not done by looking carefully at each gear and lever to see why the machine fails to work. Rather is done like this: </p>
<blockquote><p><b> </b> <em> Your machine violates the fundamental laws of thermodynamics and is thus impossible. </em>
</p></blockquote>
<p>Candidate machines are not studied to find the exact flaw in their design. The force of fundamental laws allows a sweeping, simple, and powerful argument against them. There are similar ideas in checking a proof. Let’s take a look at them.</p>
<p>
</p><p/><h2> Proofs </h2><p/>
<p/><p>
Claims are made about proofs of open problems all the time. Often these are made for solutions to famous open problems, like P<img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/>NP or the Riemann Hypothesis (RH).</p>
<p>
Math proofs are used to try to get to the <i>truth</i>. As we said <a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/">before</a> proofs are only as good as the assumptions made and the rules invoked. The beauty of the proof concept is that arguments can be checked, even long and complex ones. If the assumptions and the rules are correct, then no matter how strange the conclusion is, it must be true.</p>
<p>
For <a href="https://math.stackexchange.com/questions/2949/which-one-result-in-mathematics-has-surprised-you-the-most">example</a>:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> The Riemann rearrangement <a href="https://en.wikipedia.org/wiki/Ri emann_series_theorem#Statement_of_the_theorem">theorem</a>. A sum 	</p>
<p align="center"><img alt="\displaystyle  a_{1} + a_{2} + a_{3} + \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a_%7B1%7D+%2B+a_%7B2%7D+%2B+a_%7B3%7D+%2B+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  a_{1} + a_{2} + a_{3} + \dots "/></p>
<p>that is conditionally convergent can be reordered to yield any number. Thus there is series 	</p>
<p align="center"><img alt="\displaystyle  b_{1} + b_{2} + b_{3} + \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++b_%7B1%7D+%2B+b_%7B2%7D+%2B+b_%7B3%7D+%2B+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  b_{1} + b_{2} + b_{3} + \dots "/></p>
<p>that sums conditionally to your favorite number <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M}"/> and yet the <img alt="{b_{1},b_{2},\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb_%7B1%7D%2Cb_%7B2%7D%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b_{1},b_{2},\dots}"/> is just a arrangement of the <img alt="{a_{1},a_{2},\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_%7B1%7D%2Ca_%7B2%7D%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_{1},a_{2},\dots}"/>. This says that addition is not commutative for infinite series.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Cover the largest triangle by two <a href="https://www2.stetson.edu/~efriedma/squcotri/">unit squares</a>: what is the best? The following shows that it is unexpected: </p>
<p/><p/>
<p><a href="https://rjlipton.files.wordpress.com/2020/06/cover.png"><img alt="" class="aligncenter size-full wp-image-17195" src="https://rjlipton.files.wordpress.com/2020/06/cover.png?w=600"/></a></p>
<p/><p><br/>
The point of a proof is that it is a series of small steps. If each step is correct, then the whole is correct. But in practice proofs are often checked in other ways.</p>
<p>
</p><p/><h2> Checking Proofs </h2><p/>
<p/><p>
The starting point for my thoughts—joined here with Ken’s—are these two issues:</p>
<ol>
<li>
A proof that <em>only</em> has many small steps but no global picture is hard to motivate. <p/>
</li><li>
A proof with complex logic at the high level is hard to understand.
</li></ol>
<p>
Note that a deep, hard theorem can still have straightforward logic. A famous <a href="https://en.wikipedia.org/wiki/Riemann_hypothesis#Littlewood's_theorem">theorem</a> of Littlewood has for its proof the structure:</p>
<ul>
<li>
Case the RH is false: Then <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/> <p/>
</li><li>
Case the RH is true: Then <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/>
</li></ul>
<p>
The RH-false case takes under a page. The benefit with this logic is that one gets to assume RH for the rest. The strategy for the famous proof by Andrew Wiles of Fermat’s Last Theorem (FLT)—incorporating the all-important fix by Richard Taylor—has this structure:</p>
<ul>
<li>
If <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> then <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y}"/>. <p/>
</li><li>
If not-<img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> then <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z}"/>. <p/>
</li><li>
<img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y}"/> implies FLT. <p/>
</li><li>
<img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z}"/> implies FLT.
</li></ul>
<p>
Wiles had done the last step long before but had put aside since he didn’t know how to get <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z}"/>. The key was framing <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> so that it enabled bridged the gap in his originally-announced proof while its negation enabled the older proof.</p>
<p>
Thus what we should seek are proofs with simple logic at the high level that breaks into cases or into sequential sub-goals so that the proof is a chain or relatively few of those goals. </p>
<p>
</p><p/><h2> Shapes and Barriers </h2><p/>
<p/><p>
This makes Ken and I think again about an old <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.73.7682">paper</a> by Juris Hartmanis with his students Richard Chang, Desh Ranjan, and Pankaj Rohatgi in the May 1990 <em>Bulletin of the EATCS</em> titled, “On IP=PSPACE and Theorems With Narrow Proofs.” Ken’s <a href="https://rjlipton.wordpress.com/2015/05/17/the-shapes-of-computations/">post</a> on it included this nice diagram of what the paper calls “shapes of proofs”:</p>
<p><a href="https://rjlipton.files.wordpress.com/2020/06/proofshapes.png"><img alt="" class="aligncenter  wp-image-17197" height="252" src="https://rjlipton.files.wordpress.com/2020/06/proofshapes.png?w=400&amp;h=252" width="400"/></a></p>
<p/><p><br/>
Ken’s thought now is that this taxonomy needs to be augmented with a proof shape corresponding to certain classes believed to be properly below polynomial time—classes within the <a href="https://en.wikipedia.org/wiki/NC_(complexity)">NC</a> hierarchy. Those proofs branch at the top into manageable-size subcases, and/or have a limited number of sequential stages, where each stage may be wide but is shallow in its chains of dependencies. Call this shape a “macro-tree.”</p>
<p>
The difference between the macro-tree shape and the sequential shapes pictured above is neatly captured by Ashley Ahlin on a <a href="http://www.math.wichita.edu/~pparker/classes/thms.htm">page</a> about “Reading Theorems”:</p>
<blockquote><p><b> </b> <em> Note that, in some ways, the easiest way to read a proof is to check that each step follows from the previous ones. This is a bit like following a game of chess by checking to see that each move was legal, or like running a spell-checker on an essay. It’s important, and necessary, but it’s not really the point. … The problem with this is that you are unlikely to remember anything about how to prove the theorem, if you’ve only read in this manner. Once you’re read a theorem and its proof, you can go back and ask some questions to help synthesize your understanding. </em>
</p></blockquote>
<p/><p>
The other high-level structure that a proof needs to make evident—before seeing it is reasonable to expend the effort to check it—is shaped by <em>barriers</em>. We have <a href="https://rjlipton.wordpress.com/2012/11/29/barriers-to-pnp-proofs/">touched</a> on <a href="https://rjlipton.wordpress.com/2013/03/13/no-go-theorems/">this</a> topic <a href="https://rjlipton.wordpress.com/2009/03/25/whos-afraid-of-natural-proofs/">several</a> <a href="https://rjlipton.wordpress.com/2019/04/18/a-reason-why-circuit-lower-bounds-are-hard/">times</a> but maybe have not stated it full on for P versus NP. A recent <a href="http://theory.stanford.edu/~liyang/teaching/projects/formal-barriers-to-proving-P-ne-NP.pdf">essay</a> for a course led by Li-Yang Tan at Stanford does so in just a few pages. A proof should state up front how it works around barriers, and this alone makes its strategy easier to follow.</p>
<p>
The idea of barriers extends outside P versus NP, of course. Peter Scholze seems to be invoking it in a <a href="https://www.math.columbia.edu/~woit/wordpress/?p=11709&amp;cpage=1#comment-235940">comment</a> two months ago in a <a href="https://www.math.columbia.edu/~woit/wordpress/?p=11709">post</a> by Peter Woit in April on the status of Shinichi Mochizuki’s claimed proof of the ABC conjecture:</p>
<blockquote><p><b> </b> <em> I may have not expressed this clearly enough in my manuscript with Stix, but there is just no way that anything like what Mochizuki does can work. … The reason it cannot work is a[nother] theorem of Mochizuki himself. … If the above claims [which are negated by the theorem] would have been true, I would see how Mochizuki’s strategy might have a nonzero chance of succeeding. … </em>
</p></blockquote>
<p/><p>
Thus what Ken and I conclude is that in order for a proof to be checkable <em>chunk by chunk</em>—not line by line—it needs to have:</p>
<ol>
<li>
A top-level decomposition into a relatively small number of components and stages—like legs in a sailing race—and <p/>
</li><li>
A demonstration of how the stages navigate around known barriers.
</li></ol>
<p>
Lack of a clear plan in the first already says the proof attempt cannot avoid being snagged on a barrier, as surely as natural laws prevent building a perpetual-motion machine.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Does this help in ascertaining what shape a proof that resolves the P versus NP problem must have?</p>
<p/></font></font></div>
    </content>
    <updated>2020-06-14T01:33:31Z</updated>
    <published>2020-06-14T01:33:31Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="P=NP"/>
    <category term="Proofs"/>
    <category term="Teaching"/>
    <category term="barriers"/>
    <category term="Leonardo da Vinci"/>
    <category term="perpetual motion"/>
    <category term="proof checking"/>
    <category term="shapes of proofs"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-06-18T22:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3504</id>
    <link href="https://agtb.wordpress.com/2020/06/14/submitting-papers-to-jet/" rel="alternate" type="text/html"/>
    <title>Suggestions for computer scientists submitting papers to JET</title>
    <summary>Guest post by Tilman Borgers (JET Lead Editor), Marciano Siniscalchi (JET Editor), and Jason Hartline (JET Associate Editor): The Journal of Economic Theory (JET) would like to encourage submissions from computer scientists. JET is a leading journal of the economic theory community, and has a broader readership among economists and covers a broader range of […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><i>Guest post by <a href="http://www-personal.umich.edu/~tborgers/">Tilman Borgers</a> (JET Lead Editor), <a href="https://faculty.wcas.northwestern.edu/~msi661/">Marciano Siniscalchi</a> (JET Editor), and <a href="https://sites.northwestern.edu/hartline/">Jason Hartline</a> (JET Associate Editor):</i></p>
<p>The <a href="https://www.journals.elsevier.com/journal-of-economic-theory">Journal of Economic Theory (JET)</a> would like to encourage submissions from computer scientists. JET is a leading journal of the economic theory community, and has a broader readership among economists and covers a broader range of topics than other theory journals. JET is also the first field journal of the economic theory community, having been founded more than 50 years ago. Many publications in JET in those 50 years have changed not just the direction of economic theory, but also the direction of economics overall.</p>
<p>The convergence of research interests of computer scientists and economic theorists has been a remarkable development, and JET would like to do more to help facilitate the exchange of ideas across fields. Therefore, we compile here some suggestions for computer scientists who are interested in submitting their work to JET.</p>
<p>The basic standard for a publication in JET is that the paper should be original, make a substantial contribution, and be of interest to a broad group of readers, and this group should include economic theorists. Of course, editors make subjective, and fallible, judgments when assessing whether a paper meets these criteria. Typically, the substantive contribution is a contribution to economic theory, i.e. to our understanding of models of markets, strategic games, mechanisms, etc. Papers may be computer-science centric in its contributions, but then these contributions should be on a topic of interest to economists. For example, new algorithmic results related to game theory or mechanism design may be of interest to JET, if it is the editors’ judgment that these algorithms will be of interest to economists. On the other hand, results on more applied computational problems, such as faster algorithms for winner determination in auctions, or for clearing prediction markets, may be out of scope for JET.</p>
<p>In terms of style, successful JET submissions include an introduction that is accessible to a broad theory audience, and that explains the motivation for the work, overviews the main results, and explains some key intuitions. The introduction, or a separate literature review section, should precisely situate the work relative to the most closely related research. The main body of the paper should explain the model rigorously, and state the results precisely. Proofs which are not very long, and which provide insight, are typically included in the main body of the paper, whereas other proofs are moved to an appendix. It is often useful to paraphrase results in words after stating them formally, and to give explanations of intuitions as well as explanations of proof structures. We encourage authors to make their work as simple as is possible without losing the main message.</p>
<p>There is no length limit per se, but published papers have rarely more than 40 pages, including appendix and references, in print. We value conciseness, and focus on a main theme throughout the paper. Minor results can be left out. On the other hand, we do provide authors with the space needed to be precise and clear.</p>
<p>One general recommendation for computer science authors in preparing manuscripts for economics journals is to have an economist colleague look over the paper before submitting.  This is a good way to identify inaccurate assumptions about readers’ knowledge, or omitted relationships to the prior literature in economics.  Such advice can also help better motivate the results of the paper from an economic perspective.</p>
<p>To be publishable, if an earlier version of a paper was published as an extended abstract in conference proceedings, then the journal version must make additional contributions beyond the conference version.  This additional contribution may include important conceptual aspects of economic interest that were omitted from the original extended abstract, proofs that were omitted from the extended abstract, and additional results that did not appear in the extended abstract. Authors should explain the differences between the conference version and the journal version in a cover letter. Papers that have previously appeared as one or two page abstracts in a conference volume do not need to distinguish themselves.  Mentioning these appearances in a cover letter would useful, however.</p></div>
    </content>
    <updated>2020-06-14T00:53:10Z</updated>
    <published>2020-06-14T00:53:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Jason Hartline</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2020-06-18T22:20:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/theory-jobs-2020</id>
    <link href="http://grigory.github.io/blog/theory-jobs-2020/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Theory Jobs 2020</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p>It’s been an unusually challenging year for both sides of the TCS job market with some unexpected obstacles and delays. Apologies for putting up the spreadsheet later than usual and congrats to both sides in each converged process!</p>

<p><a href="https://docs.google.com/spreadsheets/d/1kzq4xVyU1k5CUTrV0yjIgzqlcv8agZqN_jiVlbYJb9g/edit?usp=sharing">Here is a link</a> to a crowdsourced spreadsheet created to collect information about theory jobs this year. 
I put in a biased pseudorandom seed, please help populate and share!
Rules for the spreadsheet have been copied from previous years (with one substantial suggestion regarding senior hires based on one of my friends’ recommendation, see below) and all edits to the document are anonymized. Please, post a comment if you have any suggestions about the rules.</p>
<ul>
 <li>Separate sheets for faculty, industry and postdocs/visitors. </li>
 <li>People should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are absolutely sure have been offered and accepted. This is not the place for speculation and rumors. <b>New:</b> Please, be particularly careful when adding senior hires (people who already have an academic or industrial job) -- end dates of their current positions might be still in the future. </li>
 <li>You are welcome to add yourself, or people your department has hired. </li>
</ul>


  <p><a href="http://grigory.github.io/blog/theory-jobs-2020/">Theory Jobs 2020</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on June 14, 2020.</p></div>
    </content>
    <updated>2020-06-14T00:00:00Z</updated>
    <published>2020-06-14T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2020-06-15T04:23:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/06/12/phd-positions-at-international-max-planck-research-school-on-trustworthy-computing-apply-by-june-30-2020/</id>
    <link href="https://cstheory-jobs.org/2020/06/12/phd-positions-at-international-max-planck-research-school-on-trustworthy-computing-apply-by-june-30-2020/" rel="alternate" type="text/html"/>
    <title>PhD Positions at International Max Planck Research School on Trustworthy Computing (apply by June 30, 2020)</title>
    <summary>The International Max Planck Research School on Trustworthy Computing is a graduate program jointly run by the Max Planck Institutes for Informatics and Software Systems, Saarland University, and TU Kaiserslautern. It offers a fully funded PhD program that leads to a doctoral degree from one of the participating universities and is open to students with […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The International Max Planck Research School on Trustworthy Computing is a graduate program jointly run by the Max Planck Institutes for Informatics and Software Systems, Saarland University, and TU Kaiserslautern. It offers a fully funded PhD program that leads to a doctoral degree from one of the participating universities and is open to students with a degree in computer science or equivalent.</p>
<p>Website: <a href="https://www.imprs-trust.mpg.de">https://www.imprs-trust.mpg.de</a><br/>
Email: imprs@mpi-klsb.mpg.de</p></div>
    </content>
    <updated>2020-06-12T19:45:16Z</updated>
    <published>2020-06-12T19:45:16Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-18T22:20:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/06/12/logic-mentoring-workshop-2020/</id>
    <link href="https://cstheory-events.org/2020/06/12/logic-mentoring-workshop-2020/" rel="alternate" type="text/html"/>
    <title>Logic Mentoring Workshop 2020</title>
    <summary>July 6, 2020 Online http://lmw.mpi-sws.org/index.html The Logic Mentoring Workshop (LMW) will introduce young researchers to the technical and practical aspects of a career in logic research. It is targeted at students, from senior undergraduates to graduates, and will include talks and a panel session from leaders in the subject.</summary>
    <updated>2020-06-12T18:31:23Z</updated>
    <published>2020-06-12T18:31:23Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-06-18T22:21:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3496</id>
    <link href="https://agtb.wordpress.com/2020/06/12/virtual-ec-2020/" rel="alternate" type="text/html"/>
    <title>Virtual EC 2020</title>
    <summary>EC 2020 will be held virtually with events from June 15 to July 22 (details of virtual format).  Participation by members of related fields is strongly encouraged.   Since 1999 the ACM Special Interest Group on Economics and Computation (SIGecom) has sponsored the leading scientific conference on advances in theory, empirics, and applications at the interface […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>EC 2020 will be held virtually with events from June 15 to July 22 (<a href="http://ec20.sigecom.org/participation/covid/">details of virtual format</a>).  Participation by members of related fields is strongly encouraged.  </p>



<p>Since 1999 the ACM Special Interest Group on Economics and Computation (<a href="http://sigecom.org/">SIGecom</a>) has sponsored the leading scientific conference on advances in theory, empirics, and applications at the interface of economics and computation. The 21st ACM Conference on Economics and Computation (<a href="http://ec20.sigecom.org/">Virtual EC 2020</a>) will feature invited speakers, a highlight of papers from other conferences and journals, a technical program of submitted paper presentations and posters, workshops, and tutorials.  </p>



<p>Registration is mandatory (<a href="http://ec20.sigecom.org/participation/registration/">register here</a>) but complimentary with SIGecom membership of $10 ($5 for students).  Details on joining EC events will be emailed to registered participants.</p>



<p>An overview of the schedule:</p>



<p><strong>June 15 – 19:</strong> <a href="http://ec20.sigecom.org/program/mentoring-workshop">Mentoring Workshop</a> and <a href="http://ec20.sigecom.org/program/workshops-tutorials/">Live Tutorial Pre-recording Sessions</a>.<br/>
<strong>June 22 – July 3:</strong> <a href="http://ec20.sigecom.org/program/pre-recording/">Live EC Paper Pre-recording Plenary Sessions</a>.<br/>
<strong>July 13:</strong> <a href="http://ec20.sigecom.org/program/workshops-tutorials/">Tutorial Watch Parties</a>, Business Meeting, and <a href="http://ec20.sigecom.org/call-for-contributions-acm/posters/">Poster Session</a><br/>
<strong>July 14 – 16:</strong> <a href="http://ec20.sigecom.org/program/main/">EC Conference</a> (Paper Watch Parties, Paper Poster Sessions, and Plenaries).<br/>
<strong>July 17 – 22:</strong> <a href="http://ec20.sigecom.org/program/workshops-tutorials/">Workshops</a>.</p>



<p>Areas of interest include, but are not limited to:</p>



<p><strong>Design of economic mechanisms:</strong> algorithmic mechanism design; market design; matching; auctions; revenue maximization; pricing; fair division; computational social choice; privacy and ethics.</p>



<p><strong>Game theory:</strong> equilibrium computation; price of anarchy; learning in games.</p>



<p><strong>Information elicitation and generation:</strong> prediction markets; recommender, reputation and trust systems; social learning; data markets.</p>



<p><strong>Behavioral models:</strong> behavioral game theory and bounded rationality; decision theory; computational social science; agent-based modeling.</p>



<p><strong>Online systems:</strong> online advertising; electronic commerce; economics of cloud computing; social networks; crowdsourcing; ridesharing and transportation; labor markets; cryptocurrencies; industrial organization.</p>



<p><strong>Methodological developments:</strong> machine learning; econometrics; data mining.</p></div>
    </content>
    <updated>2020-06-12T16:09:54Z</updated>
    <published>2020-06-12T16:09:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Jason Hartline</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2020-06-18T22:20:51Z</updated>
    </source>
  </entry>
</feed>
