<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-05-25T21:21:49Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1531592307894359428</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1531592307894359428/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/oldest-living-baseball-players-can-you.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1531592307894359428" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1531592307894359428" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/oldest-living-baseball-players-can-you.html" rel="alternate" type="text/html"/>
    <title>Oldest Living Baseball Players- can you estimate...</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">(The Baseball season is delayed or cancelled, so I post about baseball instead.)<br/>
<br/>
This post is going to ask a question that you could look up on the web. But what fun with that be?<br/>
<br/>
The following statements are true<br/>
<br/>
1) Don Larsen, a professional baseball player who played from 1953 to 1967, is still alive. He is 90 years old (or perhaps 90 years young---I don't know the state of his health).  He was born Aug 7, 1929. He is best know for pitching a perfect game in the World Series in 1956, pitching for the Yankees. He played for several other teams as well, via trades (this was before free agency).<br/>
(CORRECTION- I wrote this post a while back, and Don Larsen has died since then.)<br/>
<br/>
<br/>
2) White Ford, a professional baseball player who played from 1950 to 1967, is still alive. He is 91 years old (or perhaps 91 years young---I don't know the state of his health).  He was born Oct 21,  1928. He had many great seasons and is in the hall of fame. He played for the New York Yankees and no other team.<br/>
<br/>
3) From 1900 (or so) until 1962 there were 16 professional baseball teams which had 25 people each. From 1962 until 1969 there were 20 teams which had 25 people each. There were also many minor league teams.<br/>
<br/>
4) The youngest ballplayers are usually around 20. The oldest around 35. These are not exact numbers<br/>
<br/>
SO here is my question: Try to estimate<br/>
<br/>
1) How many LIVING  retired major league baseball players are there now who are older than Don Larsen?<br/>
<br/>
2) How many LIVING retired major league baseball players are of an age between Don and Whitey?<br/>
<br/>
3) How  many LIVING retired major league baseball players are older than Whitey Ford?<br/>
<br/>
Give your REASONING for your answer.<br/>
<br/></div>
    </content>
    <updated>2020-05-25T16:34:00Z</updated>
    <published>2020-05-25T16:34:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-05-25T16:34:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17076</id>
    <link href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/" rel="alternate" type="text/html"/>
    <title>Proof of the Diagonal Lemma in Logic</title>
    <summary>Why is the proof so short yet so difficult? Saeed Salehi is a logician at the University of Tabriz in Iran. Three years ago he gave a presentation at a Moscow workshop on proofs of the diagonal lemma. Today I thought I would discuss the famous diagonal lemma. The lemma is related to Georg Cantor’s […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Why is the proof so short yet so difficult?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/saeed_salehi4/" rel="attachment wp-att-17078"><img alt="" class="alignright  wp-image-17078" src="https://rjlipton.files.wordpress.com/2020/05/saeed_salehi4.jpg?w=200" width="200"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Saeed Salehi is a logician at the University of Tabriz in Iran. Three years ago he gave a <a href="http://wrm17.mi.ras.ru/slides/Salehi.pdf">presentation</a> at a Moscow workshop on proofs of the diagonal lemma.</p>
<p>
Today I thought I would discuss the famous <a href="https://en.wikipedia.org/wiki/Diagonal_lemma">diagonal lemma</a>. </p>
<p>
The lemma is related to Georg Cantor’s famous diagonal argument yet is different. The logical version imposes requirements on when the argument applies, and requires that it be expressible within a formal system. </p>
<p>
The lemma underpins Kurt Gödel’s famous 1931 <a href="http://www.w-k-essler.de/pdfs/goedel.pdf">proof</a> that arithmetic is incomplete. However, Gödel did not state it as a lemma or proposition or theorem or anything else. Instead, he focused his attention on what we now call Gödel numbering. We consider this today as “obvious” but his paper’s title ended with “Part I”. And he had readied a “Part II” with over 100 pages of calculations should people question that his numbering scheme was expressible within the logic. </p>
<p>
Only after his proof was understood did people realize that one part, perhaps the trickiest part, could be abstracted into a powerful lemma. The tricky part is <em>not</em> the Gödel numbering. People granted that it can be brought within the logic once they saw enough of Gödel’s evidence, and so we may write <img alt="{\ulcorner \phi \urcorner}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Culcorner+%5Cphi+%5Curcorner%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ulcorner \phi \urcorner}"/> for the function giving the Gödel number of any formula <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> and use that in other formulas. The hard part is what one <em>does</em> with such expressions. </p>
<p>
This is what we will try to motivate.</p>
<p>
</p><p/><h2> Tracing the Lemma </h2><p/>
<p/><p>
Rudolf Carnap is often credited with the first formal statement, in 1934, for instance by Eliott Mendelson in his famous <a href="https://www.goodreads.com/book/show/250868.Introduction_to_Mathematical_Logic">textbook</a> on logic. Carnap was a <a href="https://en.wikipedia.org/wiki/Rudolf_Carnap">member</a> of the <a href="https://en.wikipedia.org/wiki/Vienna_Circle">Vienna Circle</a>, which Gödel frequented, and Carnap is <a href="http://texts.cdlib.org/view?docId=hb6h4nb3q7&amp;doc.view=frames&amp;chunk.id=div00004&amp;toc.depth=1&amp;toc.id=">considered</a> a giant among twentieth-century philosophers. He worked on sweeping grand problems of philosophy, including logical positivism and analysis of human language via syntax before semantics. Yet it strikes us with irony that his work on the lemma may be the best remembered.</p>
<p>
Who did the lemma first? Let’s leave that for others and move on to the mystery of how to prove the lemma once it is stated. I must say the lemma is easy to state, easy to remember, and has a short proof. But I believe that the proof is not easy to remember or even follow. </p>
<p>
Salehi’s <a href="http://wrm17.mi.ras.ru/slides/Salehi.pdf">presentation</a> quotes others’ opinions about the proof:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Sam Buss: “Its proof [is] quite simple but rather tricky and difficult to conceptualize.”</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> György Serény (we jump to Serény’s <a href="https://arxiv.org/pdf/math/0606425.pdf">paper</a>): “The proof of the lemma as it is presented in textbooks on logic is not self-evident to say the least.”</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Wayne Wasserman: “It is `Pulling a Rabbit Out of the Hat’—Typical Diagonal Lemma Proofs Beg the Question.”</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/hat/" rel="attachment wp-att-17079"><img alt="" class="aligncenter size-full wp-image-17079" src="https://rjlipton.files.wordpress.com/2020/05/hat.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
So I am not alone, and I thought it might be useful to try and unravel its proof. This exercise helped me and maybe it will help you.</p>
<p>
Here goes. </p>
<p>
</p><p/><h2> Stating the Lemma </h2><p/>
<p/><p>
Let <img alt="{S(w)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28w%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S(w)}"/> be a formula in Peano Arithmetic (<img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{PA}"/>). We claim that there is some sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++PA+%5Cvdash+%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). "/></p>
<p>Formally, </p>
<blockquote><p><b>Lemma 1</b> <em> Suppose that <img alt="{S(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S(x)}"/> is some formula in <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/>. Then there is a sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\phi}"/> so that 	</em></p><em>
<p align="center"><img alt="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++PA+%5Cvdash+%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
The beauty of this lemma is that it was used by Gödel and others to prove various powerful theorems. For example, the lemma quickly proves this result of Alfred Tarski:</p>
<blockquote><p><b>Theorem 2</b> <em> Suppose that <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/> is consistent. Then <i>truth</i> cannot be defined in <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/>. That is there is <b>no</b> formula <img alt="{Tr(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BTr%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{Tr(x)}"/> so that for all sentences <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\phi}"/> <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/> proves 	</em></p><em>
<p align="center"><img alt="\displaystyle  \phi \iff Tr(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff Tr(\ulcorner \phi \urcorner). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
The proof is this. Assume there is such a formula <img alt="{Tr(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BTr%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Tr(x)}"/>. Then use the diagonal lemma and get 	</p>
<p align="center"><img alt="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+%5Cneg+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner)."/></p>
<p>This shows that 	</p>
<p align="center"><img alt="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner) \iff Tr(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+%5Cneg+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29+%5Ciff+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner) \iff Tr(\ulcorner \phi \urcorner). "/></p>
<p>This is a contradiction. A short proof. </p>
<p>
</p><p/><h2> The Proof </h2><p/>
<p/><p>
The key is to define the function <img alt="{F(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(n)}"/> as follows: Suppose that <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is the Gödel number of a formula of the form <img alt="{A(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A(x)}"/> for some variable <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> then 	</p>
<p align="center"><img alt="\displaystyle  F(n) = \ulcorner A(\ulcorner A(x) \urcorner) \urcorner. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28n%29+%3D+%5Culcorner+A%28%5Culcorner+A%28x%29+%5Curcorner%29+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(n) = \ulcorner A(\ulcorner A(x) \urcorner) \urcorner. "/></p>
<p>If <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is not of this form then define <img alt="{F(n)=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(n)=0}"/>. This is a strange function, a clever function, but a perfectly fine function, It certainly maps numbers to numbers. It is certainly recursive, actually it is clearly computable in polynomial time for any reasonable Gödel numbering. Note: the function <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> does depend on the choice of the variable <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Thus, 	</p>
<p align="center"><img alt="\displaystyle  F(\ulcorner y=0 \urcorner) = \ulcorner (\ulcorner y=0 \urcorner)=0 \urcorner, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+y%3D0+%5Curcorner%29+%3D+%5Culcorner+%28%5Culcorner+y%3D0+%5Curcorner%29%3D0+%5Curcorner%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(\ulcorner y=0 \urcorner) = \ulcorner (\ulcorner y=0 \urcorner)=0 \urcorner, "/></p>
<p>and 	</p>
<p align="center"><img alt="\displaystyle  F(\ulcorner x=0 \urcorner) = \ulcorner (\ulcorner x=0 \urcorner)=0 \urcorner. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+x%3D0+%5Curcorner%29+%3D+%5Culcorner+%28%5Culcorner+x%3D0+%5Curcorner%29%3D0+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(\ulcorner x=0 \urcorner) = \ulcorner (\ulcorner x=0 \urcorner)=0 \urcorner. "/></p>
<p>	          Now we make two definitions:</p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++g%28w%29+%26%5Cequiv%26+S%28F%28w%29%29+%5C%5C++++++++%5Cphi+%26%5Cequiv%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} "/></p>
<p>
Now we compute just using the definitions of <img alt="{F, g, \phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%2C+g%2C+%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F, g, \phi}"/>:</p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)) \\           &amp;=&amp; S(\ulcorner g(\ulcorner g(x) \urcorner) \urcorner) \\               &amp;=&amp; S(\ulcorner \phi \urcorner). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++%5Cphi+%26%3D%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5C%5C+++++++++++++++++%26%3D%26+S%28F%28%5Culcorner+g%28x%29+%5Curcorner%29%29+%5C%5C+++++++++++%26%3D%26+S%28%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner%29+%5C%5C+++++++++++++++%26%3D%26+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)) \\           &amp;=&amp; S(\ulcorner g(\ulcorner g(x) \urcorner) \urcorner) \\               &amp;=&amp; S(\ulcorner \phi \urcorner). \end{array} "/></p>
<p>We are done.</p>
<p>
</p><p/><h2> But … </h2><p/>
<p/><p>
Where did this proof come from? Suppose that you forgot the proof but remember the statement of the lemma. I claim that we can then reconstruct the proof. </p>
<p>
First let’s ask: Where did the definition of the function <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> come from? Let’s see. Imagine we defined </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++g%28w%29+%26%5Cequiv%26+S%28F%28w%29%29+%5C%5C++++++++%5Cphi+%26%5Cequiv%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} "/></p>
<p>But left <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> undefined for now. Then</p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)). \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++%5Cphi+%26%3D%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5C%5C+++++++++++++++++%26%3D%26+S%28F%28%5Culcorner+g%28x%29+%5Curcorner%29%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)). \end{array} "/></p>
<p>But we want <img alt="{\phi = S(\ulcorner \phi \urcorner)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3D+S%28%5Culcorner+%5Cphi+%5Curcorner%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi = S(\ulcorner \phi \urcorner)}"/> that happens provided:</p>
<p align="center"><img alt="\displaystyle  \ulcorner g(\ulcorner g(x) \urcorner) \urcorner) = F(\ulcorner g(x) \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner%29+%3D+F%28%5Culcorner+g%28x%29+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \ulcorner g(\ulcorner g(x) \urcorner) \urcorner) = F(\ulcorner g(x) \urcorner). "/></p>
<p>This essentially gives the definition of the function <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. Pretty neat.</p>
<p>
</p><p/><h2> But but … </h2><p/>
<p/><p>
Okay where did the definition of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> and <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> come from? It is reasonable to define 	</p>
<p align="center"><img alt="\displaystyle  g(w) \equiv S(F(w)), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28w%29+%5Cequiv+S%28F%28w%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g(w) \equiv S(F(w)), "/></p>
<p>for some <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. We cannot change <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> but we can control the input to the formula <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, so let’s put a function there. Hence the definition for <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> is not unreasonable. </p>
<p>
Okay how about the definition of <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/>? Well we could argue that this is the magic step. If we are given this definition then <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> follows, by the above. I would argue that <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> is not completely surprising. The name of the lemma is after all the “diagonal” lemma. So defining <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> as the application of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> to itself is plausible.</p>
<p>
</p><p/><h2> Taking an Exam </h2><p/>
<p/><p>
Another way to think about the diagonal lemma is imagine you are taking an exam in logic. The first question is: </p>
<blockquote><p><b> </b> <em> Prove in <img alt="{PA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{PA}"/> that for any <img alt="{S(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S(x)}"/> there is a sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\phi}"/> so that 	</em></p><em>
<p align="center"><img alt="\displaystyle  \phi \iff S(\ulcorner \phi \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \phi \iff S(\ulcorner \phi \urcorner). "/></p>
</em><p><em/>
</p></blockquote>
<p>You read the question again and think: “I wish I had studied harder, I should have not have checked Facebook last night. And then went out and <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/>” But you think let’s not panic, let’s think.</p>
<p>
Here is what you do. You say let me define 	</p>
<p align="center"><img alt="\displaystyle  g(x) = S(F(x)), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%29+%3D+S%28F%28x%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g(x) = S(F(x)), "/></p>
<p>for some <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. You recall there was a function that depends on <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, and changing the input from <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> to <img alt="{F(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(x)}"/> seems to be safe. Okay you say, now what? I need the definition of <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. Hmmm let me wait on that. I recall vaguely that <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> had a strange definition. I cannot recall it, so let me leave it for now.</p>
<p>
But you think: I need a sentence <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/>. A sentence cannot have an unbound variable. So <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> cannot be <img alt="{g(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(x)}"/>. It could be <img alt="{g(m)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28m%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(m)}"/> for some <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/>. But what could <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> be? How about <img alt="{\ulcorner \phi \urcorner}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Culcorner+%5Cphi+%5Curcorner%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ulcorner \phi \urcorner}"/>. This makes 	</p>
<p align="center"><img alt="\displaystyle  \phi = g(\ulcorner g \urcorner). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%3D+g%28%5Culcorner+g+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \phi = g(\ulcorner g \urcorner). "/></p>
<p>It is after all the diagonal lemma. Hmmm does this work. Let’s see if this works. Wait as above I get that <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> is now forced to satisfy 	</p>
<p align="center"><img alt="\displaystyle  F(\ulcorner g(x) \urcorner) = \ulcorner g(\ulcorner g(x) \urcorner) \urcorner. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+g%28x%29+%5Curcorner%29+%3D+%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(\ulcorner g(x) \urcorner) = \ulcorner g(\ulcorner g(x) \urcorner) \urcorner. "/></p>
<p>Great this works. I think this is the proof. Wonderful. Got the first question. </p>
<p>
Let’s look at the next exam question. Oh no <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Does this help? Does this unravel the mystery of the proof? Or is it still magic?</p>
<p/><p><br/>
[Fixed equation formatting]</p></font></font></div>
    </content>
    <updated>2020-05-25T15:55:22Z</updated>
    <published>2020-05-25T15:55:22Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="diagonal lemma"/>
    <category term="diagonalization"/>
    <category term="Godel"/>
    <category term="numbering"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-05-25T21:20:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/benchmarks/</id>
    <link href="https://gradientscience.org/benchmarks/" rel="alternate" type="text/html"/>
    <title>From ImageNet to Image Classification</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/2005.11295" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/ImageNetMultiLabel" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Data
</a>
<br/></p>

<p><i>In our <a href="https://arxiv.org/abs/2005.11295">new paper</a>, we explore how closely
the ImageNet benchmark aligns with the object recognition task it serves as a
proxy for. We find pervasive and systematic deviations of ImageNet annotations
from the ground truth, which can often be attributed to specific design choices
in the data collection pipeline. These issues indicate that ImageNet accuracy
alone might be insufficient to effectively gauge real model performance.  </i></p>

<h2 id="contextualizing-progress-on-benchmarks">Contextualizing Progress on Benchmarks</h2>

<p>Large-scale benchmarks are central to machine learning—they serve both
as concrete targets for model development, and as proxies for assessing model
performance on real-world tasks we actually care about. However, few benchmarks
are perfect, and so as our models get increasingly better at them, we must also
ask ourselves: <i>to what extent is performance on existing benchmarks
indicative of progress on the real-world tasks that motivate them?</i></p>

<p>In this post, we will explore this question of <i>benchmark-task alignment</i>
in the context of the popular
<a href="http://image-net.org/challenges/LSVRC/2012/">ImageNet object recognition dataset</a>.
Specifically, our goal is to understand how well the underlying ground truth is
captured by the dataset itself—this dataset is, after all, what we consider
to be the gold standard during model training and evaluation.</p>

<h3 id="a-sneak-peak-into-imagenet">A sneak peak into ImageNet</h3>
<p>The ImageNet dataset contains over a million images of objects from a thousand,
quite diverse classes. Like many other benchmarks of that scale, ImageNet was
not carefully curated by experts, but instead created via crowd-sourcing,
without perfect quality control. So what does ImageNet data look like? Here are
a few image-label pairs from the dataset:</p>

<div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/0.jpg"/>
      <div class="fake_label">missile</div>
      <div class="true_label">projectile</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/1.jpg"/>
      <div class="fake_label">stage</div>
      <div class="true_label">acoustic guitar</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/2.jpg"/>
      <div class="fake_label">monastery</div>
      <div class="true_label">church</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/3.jpg"/>
      <div class="fake_label">Norwich terrier</div>
      <div class="true_label">Norfolk terrier</div>
    </div>
</div>
<p><br/></p>

<p>These samples appear pretty reasonable…but are they? Actually, while these
are indeed <i>images</i> from the dataset, the labels shown above are
<i>not</i> their actual ImageNet labels!
<a href="https://gradientscience.org/" id="reveal">[Click to see the actual ImageNet labels]</a>
Still, even though not “correct” from the point of view of the ImageNet
dataset, these labels <i>do</i> correspond to actual ImageNet classes, and
appear plausible when you see them in isolation. This shows that for ImageNet
images, which capture objects in diverse real-world conditions, the ImageNet
label may not properly reflect the ground truth.</p>

<p>In our work, we dive into examining how this label misalignment actually
impacts ImageNet: how often do ImageNet labels deviate from the ground truth?
And how do shortcomings in these labels impact ImageNet-trained models?</p>

<h3 id="revisiting-the-imagenet-collection-pipeline">Revisiting the ImageNet collection pipeline</h3>
<p>Before going further, let’s take a look at how ImageNet was created. To build
such a large dataset, the creators of ImageNet had to leverage scalable methods
like automated data collection and crowd-sourcing. That is, they first selected
a set of object classes (using the <a href="https://wordnet.princeton.edu">WordNet</a>
hierarchy), and queried various search engines to obtain a pool of candidate
images. These candidate images were then verified by annotators on <a href="https://www.mturk.com/">Mechanical
Turk (MTurk)</a> using (what we will refer to as) the
<span class="sc">Contains</span> task: annotators were shown images retrieved
for a specific class
label, and were subsequently asked to select the ones that actually contain an
object of this class. Only images that multiple annotators validated ended up
in the final dataset.</p>

<div>
  <div class="stages block">
      <div class="stage rbutton block clicked" id="selection">Class Selection</div>
      <div class="stage rbutton block" id="retrieval">Image Retrieval</div>
      <div class="stage rbutton block" id="filtering">Label Validation</div>
  </div>
  <img id="stage_img" src="https://gradientscience.org/assets/multilabel/pipeline/selection.jpg"/>
</div>
<div class="footnote"> <strong>Imagenet collection pipeline:</strong> Click on a
stage at the top for an illustration. </div>

<p>While this is a natural approach to scalably annotate data (and, in fact, is
commonly used to create large-scale benchmarks—e.g.,
<a href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL VOC</a>,
<a href="http://cocodataset.org/#home">COCO</a>, <a href="http://places.csail.mit.edu/">places</a>),
it has an important caveat. Namely, this process has an inherent bias: the
annotation task itself is phrased as a leading question. ImageNet annotators
were not asked to provide an image label, but instead only to verify if a
<i>specific</i> label (predetermined by the image retrieval process) was
<i>contained</i> in an image. Annotators had no knowledge of what the other
classes in the dataset even were, or the granularity at which they were
required to make distinctions. In fact, they were explicitly instructed to
ignore clutter and obstructions.</p>

<p>Looking back at the ImageNet samples shown above, one can see how this setup
could lead to imperfect annotations. For instance, it is unclear if the average
annotator knows the differences between a “Norwich terrier” and a “Norfolk
terrier”, especially if they don’t even know that both of these (as well as 22
other terrier breeds) are valid ImageNet classes. Also, the
<span class="sc">Contains</span> task itself might be ill-suited for annotating
multi-object images—the answer to the <span class="sc">Contains</span> question would be yes for any
object in the image that corresponds to an ImageNet class. It is not unthinkable
that the same images could have made it into ImageNet under the labels “stage”
and “Norwich terrier” had they come up in the search results for those classes
instead.</p>

<p>Overall, this suggests that the labeling issues in ImageNet may go beyond just
occasional annotator mistakes—the <i>design</i> of the data collection
pipeline itself could have caused these labels to systematically deviate from
the ground truth.</p>

<h3 id="diagnosing-benchmark-task-misalignment">Diagnosing benchmark-task misalignment</h3>

<p>To characterize how wide-spread these deviations are, we first need to get a
better grasp of the ground truth for ImageNet data. In order to do this at
scale, we still need to rely on crowd-sourcing. However, in contrast to the
original label validation setup, we design a new annotation task based directly
on <i>image classification</i>. Namely, we present annotators with a set of
possible labels for a single image <i>simultaneously</i>. We then ask them to
assign one label to every object in the image, and identify what they believe
to be the main object. (Note that we intentionally ask for such fine-grained
image annotations since, as we saw before, a single label might be inherently
insufficient to capture the ground truth.)</p>

<p>Of course, we need to ensure that annotators can meaningfully perform this
task. To this end we  devise a way to narrow down the label choices they are
presented with (all thousand ImageNet classes would be nearly impossible for a
worker to choose between!). Specifically, for each image, we identify the most
relevant labels by pooling together the top-5 predictions of a diverse set of
ImageNet models and filtering them via the <span class="sc">Contains</span>
task. Note that, by doing so, we are effectively bootstrapping the existing
ImageNet labels by first using them to train models and then using model
predictions to get better annotation candidates.</p>

<p>This is what our resulting annotation task looks like:</p>

<p><img src="https://gradientscience.org/assets/multilabel/main_task.jpg"/></p>

<p>We aggregate the responses from multiple annotators to get per-image estimates
of the number of objects in the image (along with their corresponding labels),
as well as which object humans tend to view as the main one.</p>

<p>We collect such <a href="https://github.com/MadryLab/ImageNetMultiLabel">annotations for 10k images from the ImageNet validation
set</a>. With these more
fine-grained and accurate annotations in hand, we now examine where the
original ImageNet labels may fall short.</p>

<h3 id="multi-object-images">Multi-object images</h3>
<p>The simplest way in which ImageNet labels could deviate from the ground truth
is if the image contains multiple objects. So, the first thing we want to
understand is: how many ImageNet images contain objects from more than one
valid class?</p>

<p>It turns out: quite a few! Indeed, more than <b>20%</b> of the images contain
more than one ImageNet object. Examples:</p>

<div class="widget">
  <div class="choices_one">
    <span class="widgetheading">Inspect Image</span>
  </div>
  <div class="choices_img">
    <span class="widgetheading">Image</span>
  </div>
  <div class="choices_info block">
    <span class="widgetheading">Annotation</span>
  </div>
  <div class="choices_one" id="multi"> </div>
  <div class="choices_img widgetheading"> <img id="multi1"/> </div>
  <div class="choices_info block">
    <div class="choices_info_text" id="multiclass"> </div>
  </div>
</div>
<div style="clear: both;"/>
<div class="footnote"> <strong>Multi-object images:</strong> Choose an image on
the left to see the annotations we obtain for it.  </div>

<p>Looking at some of these images, it is clear that the problem is not just
natural image clutter but also the fact that certain objects are quite likely
to co-occur in the real-world—e.g., “table lamp” and “lamp shade”. This means
that choosing classes which in principle correspond to distinct objects (e.g.,
using WordNet) is not enough to guarantee that the corresponding images have
unambiguous labels. For example, see if you can guess the ImageNet label for
the samples below:</p>

<div class="widget">
  <span class="widgetheading" id="coclass">Chosen class pair</span>
  <div class="choices_one_full" id="co">
  <div class="show_labels rbutton block" id="cooc">Show Labels</div>
  </div>
  <div id="coimages" style="border-right: 10px white solid;"> </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Co-occurring objects:</strong> Select a class pair on the top and see if
you can identify which images correspond to each class (click on "Show Labels"
to reveal the answers).
</div>

<h4 id="model-performance-on-multi-object-images">Model performance on multi-object images</h4>

<p>So, how do models deal with images that contain multiple objects? To understand
this, we evaluate a number of models (from AlexNet to EfficientNet-B7), and
measure their accuracy (w.r.t. the ImageNet labels) on such images. We plot
these accuracies below (as a function of their full test accuracy):</p>

<canvas height="200" id="multi_drop" width="400"/>
<div class="footnote">
<strong>Performance on multi-object images:</strong>
Model accuracy variation as a function of the number of objects in the image.
Hover over each data point to see the corresponding model name.
</div>

<p>Across the board, in comparison to their performance on single-object images,
models suffer around a 10% accuracy drop on multi-object ones. At the same
time, this drop more-or-less disappears if we consider a model prediction to be
correct if it matches the label of <i>any</i> valid object in the image (see
the <a href="https://arxiv.org/abs/2005.11295">paper</a> for specifics).</p>

<div class="footnote">
    <strong>Aside:</strong> The original motivation of top-5 accuracy was
    exactly to accommodate such multi-label images. However, we find that, while
    it does largely account for these multi-object confusions, it also seems to
    overestimate accuracy on single-object images.
</div>

<p>Still, even though models seem to struggle with multi-object images, they
perform much better than chance (i.e., better than what one would get if they
were picking the label of an object in the image at random). This makes sense
when the image has a single prominent object that also matches the ImageNet
label. However, for a third of all multi-object images the ImageNet label does
not even match what annotators deem to be the main object in the image. Yet,
even in these cases, models still successfully predict the ImageNet label
(instead of what humans consider to be the right label for the image)!</p>

<canvas height="200" id="overfitting" width="400"/>
<div class="footnote">
<strong>Performance on images with annotator-label disagreement:</strong>
Model accuracy on images where annotators do not consider the ImageNet label to
be the main object. Baseline: accuracy of picking the label of a random
prominent object in the image.
</div>

<p>Here, models seem to base their predictions on biases in the dataset which
humans do not find salient. For instance, models get high accuracy on the class
“pickelhaube”, even though, pickelhaubes are usually present in images with
other, more salient objects, such as “military uniforms”, suggesting that
ImageNet models may be overly sensitive to the presence of distinctive objects
in the image. While exploiting such biases would improve ImageNet accuracy,
this strategy might not translate to improved performance on object recognition
in the wild. Here are a few examples that seem to exhibit a similar mismatch:</p>

<div class="widget">
  <div class="choices_one">
    <span class="widgetheading">Inspect Image</span>
  </div>
  <div class="choices_img">
    <span class="widgetheading">Image</span>
  </div>
  <div class="choices_info block">
    <span class="widgetheading">Annotation</span>
  </div>
  <div class="choices_one" id="main"> </div>
  <div class="choices_img widgetheading"> <img id="main1"/> </div>
  <div class="choices_info block">
    <div class="choices_info_text" id="mainclass"> </div>
  </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Annotator-label disagreement:</strong> Select an image on the left to
see the annotations we obtain for it. Notice that the ImageNet label is
different from the (annotator-selected) "main label".
</div>

<h3 id="biases-in-label-validation">Biases in label validation</h3>

<p>Let us now turn our attention to the ImageNet data filtering process. Recall
that each class in ImageNet was constructed by automatically retrieving many
images and filtering them (via the <span class="sc">Contains</span> task described above). How likely
were annotators to filter out mislabeled images under this setup?</p>

<p>To understand this, we replicate the original filtering process on the existing
ImageNet images. But this time, instead of only asking annotators to check if
the image is valid with respect to its ImageNet label (i.e., the search query),
we also try several other labels (each in isolation, with different sets of
annotators).</p>

<p>We find that annotators frequently deem an image to be valid for <i>many</i>
different labels—even when only one object is present. Typically, this occurs
when the image is ambiguous and lacks enough context (e.g. “seashore” or
“lakeshore”), or annotators are likely confused between different semantically
similar labels (e.g., “assault rifle” vs. “rifle”, dog breeds). It turns out
that this confusion, at least partly, stems from the one-sidedness of the
<span class="sc">Contains</span> task—i.e., asking annotators to ascertain the validity of a specific
label without them knowing about any other options. If instead we present
annotators with all the relevant labels simultaneously and ask them to choose
one (as we did in our annotation setup), this kind of label confusion is
alleviated: annotators select significantly fewer labels in total (see our
<a href="https://arxiv.org/abs/2005.11295">paper</a> for details). So, even putting
annotator’s expertise aside, the specific annotation task setup itself
drastically affects the quality of the resulting dataset labels.</p>

<div>
  <div class="dropdown">
    <div class="rates block" id="dropdownMenuButton">
      Choose annotator threshold
    </div>
    <div class="rates">
      <div class="block rbutton clicked">10%</div>
      <div class="block rbutton">30%</div>
      <div class="block rbutton">50%</div>
    </div>
  </div>
  <img id="filtering_plot" src="https://gradientscience.org/assets/multilabel/filtering/filtering_0.1.jpg"/>
</div>
<div class="footnote">
<strong>Number of valid labels:</strong> Distribution of labels annotators deem
valid (based on an agreement threshold) for single-object images in the <span class="sc">Contains</span>
task. Click the percentages on the top to change the annotator agreement
threshold.
</div>

<p>Going back to ImageNet, our findings give us reason to believe that annotators
may have had a rather limited ability to correct errors in labeling. Thus, in
certain cases, ImageNet labels were largely determined by the automated image
retrieval process—propagating any biases or mixups this process might
introduce to the final dataset.</p>

<p>In fact, we can actually see direct evidence of that in the ImageNet
dataset—there are pairs of classes that appear to be <i>inherently
ambiguous</i> (e.g., “laptop computer” and “notebook computer”) and neither
human annotators, nor models, can tell the corresponding images apart (see
below). If such class pairs actually overlap in terms of their ImageNet images,
it is unclear how models can learn to separate them without memorizing specific
validation examples.</p>

<div class="widget">
  <span class="widgetheading" id="ambclass">Chosen class pair</span>
  <div class="choices_one_full" id="am">
  <div class="show_labels rbutton block" id="amb">Show Labels</div>
  </div>
  <div id="ambimages" style="border-right: 10px white solid;"> </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Ambiguous class pairs:</strong> Select a class pair on the top and see if
you can identify which images correspond to each class (click on "Show Labels"
to reveal the answers).
</div>

<h3 id="beyond-test-accuracy-human-centric-model-evaluation">Beyond test accuracy: human-centric model evaluation</h3>

<p>Performance of ImageNet-trained models is typically judged based on their
ability to predict the dataset labels—yet, as we saw above, these labels may
not fully capture the ground truth. Hence, ImageNet accuracy may not reflect
properly model performance—for instance, measuring accuracy alone could
unfairly penalize models for certain correct predictions on  multi-object
images. So, how can we better assess model performance?</p>

<p>One approach is to measure model-human alignment directly—we present model
predictions to annotators and ask them to gauge their validity:</p>

<canvas height="200" id="limit" width="400"/>
<div class="footnote">
<strong>Human-based model evaluation:</strong> Fraction of annotators that
select a label as valid in the <span class="sc">Contains</span> task (i.e.,
selection frequency) for ImageNet labels and model predictions. Baseline:
(number of correct predictions) × (average selection frequency of the
ImageNet label).
</div>

<p>Surprisingly, we find that for state-of-the-art models, annotators actually
deem the prediction that models make to be valid about as often as the ImageNet
label (even when the two <i> do not</i> match). Thus, recent models may be
better at predicting the ground truth than their top-1 accuracy (w.r.t. the
ImageNet label) would indicate.</p>

<p>However, this does not imply that improving ImageNet accuracy is meaningless.
For instance, non-expert annotators may not be able to tell apart certain
fine-grained class differences (e.g., dog breeds) and for some of these images
the ImageNet label may actually match the ground truth. What it does indicate,
though, is that we are at a point where it may be hard to gauge if better
performance on ImageNet corresponds to actual progress or merely to exploiting
idiosyncrasies of the dataset.</p>

<p>For further experimental details and additional results (e.g., human confusion
matrices), take a look at <a href="https://arxiv.org/abs/2005.11295">our paper</a>!</p>

<h3 id="conclusions">Conclusions</h3>

<p>We took a closer look at how well ImageNet aligns with the real-world object
recognition task—even though ImageNet is used extensively, we rarely question
whether its labels actually reflect the ground truth. We saw that oftentimes
ImageNet labels do not fully capture image content—e.g., many images have
multiple (ImageNet) objects and there are classes that are inherently
ambiguous. As a result, models trained using these labels as ground truth end
up learning unintended biases and confusions.</p>

<p>Our analysis indicates that when creating datasets we must be aware of (and try
to mitigate) ways in which scalable data collection practices can skew the
corresponding annotations (see our
<a href="https://gradientscience.org/data_rep_bias">previous post</a> for another
example of such a skew). Finally, given that such imperfections in our datasets
could be inevitable, we also need to think about how to reliably assess model
performance in their presence.</p>





















<b/><br/><hr/><b/><br/><br/><br/><b/><br/><br/><div class="cooc_img block"><div class="image_label label_cooc"><br/></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg"/></div><div class="amb_img block"><div class="image_label label_amb"><br/></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg"/></div><div class="bias_img block"><div class="image_label label_bias"><br/></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg"/></div></div>
    </summary>
    <updated>2020-05-25T00:00:00Z</updated>
    <published>2020-05-25T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2020-05-25T19:20:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11307</id>
    <link href="http://arxiv.org/abs/2005.11307" rel="alternate" type="text/html"/>
    <title>Algebraic Global Gadgetry for Surjective Constraint Satisfaction</title>
    <feedworld_mtime>1590364800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Hubie.html">Hubie Chen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11307">PDF</a><br/><b>Abstract: </b>The constraint satisfaction problem (CSP) on a finite relational structure B
is to decide, given a set of constraints on variables where the relations come
from B, whether or not there is a assignment to the variables satisfying all of
the constraints; the surjective CSP is the variant where one decides the
existence of a surjective satisfying assignment onto the universe of B.
</p>
<p>We present an algebraic framework for proving hardness results on surjective
CSPs; essentially, this framework computes global gadgetry that permits one to
present a reduction from a classical CSP to a surjective CSP. We show how to
derive a number of hardness results for surjective CSP in this framework,
including the hardness of surjective CSP on the reflexive 4-cycle and on the
no-rainbow 3-coloring relation. Our framework thus allows us to unify these
hardness results, and reveal common structure among them; we believe that our
hardness proof for the reflexive 4-cycle is more succinct than the original. In
our view, the framework also makes very transparent a way in which classical
CSPs can be reduced to surjective CSPs.
</p></div>
    </summary>
    <updated>2020-05-25T01:21:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11270</id>
    <link href="http://arxiv.org/abs/2005.11270" rel="alternate" type="text/html"/>
    <title>The Average-Case Time Complexity of Certifying the Restricted Isometry Property</title>
    <feedworld_mtime>1590364800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Yunzi.html">Yunzi Ding</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kunisky:Dmitriy.html">Dmitriy Kunisky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wein:Alexander_S=.html">Alexander S. Wein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bandeira:Afonso_S=.html">Afonso S. Bandeira</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11270">PDF</a><br/><b>Abstract: </b>In compressed sensing, the restricted isometry property (RIP) on $M \times N$
sensing matrices (where $M &lt; N$) guarantees efficient reconstruction of sparse
vectors. A matrix has the $(s,\delta)$-$\mathsf{RIP}$ property if behaves as a
$\delta$-approximate isometry on $s$-sparse vectors. It is well known that an
$M\times N$ matrix with i.i.d. $\mathcal{N}(0,1/M)$ entries is
$(s,\delta)$-$\mathsf{RIP}$ with high probability as long as $s\lesssim
\delta^2 M/\log N$. On the other hand, most prior works aiming to
deterministically construct $(s,\delta)$-$\mathsf{RIP}$ matrices have failed
when $s \gg \sqrt{M}$. An alternative way to find an RIP matrix could be to
draw a random gaussian matrix and certify that it is indeed RIP. However, there
is evidence that this certification task is computationally hard when $s \gg
\sqrt{M}$, both in the worst case and the average case.
</p>
<p>In this paper, we investigate the exact average-case time complexity of
certifying the RIP property for $M\times N$ matrices with i.i.d.
$\mathcal{N}(0,1/M)$ entries, in the "possible but hard" regime $\sqrt{M} \ll
s\lesssim M/\log N$, assuming that $M$ scales proportional to $N$. Based on
analysis of the low-degree likelihood ratio, we give rigorous evidence that
subexponential runtime $N^{\tilde\Omega(s^2/N)}$ is required, demonstrating a
smooth tradeoff between the maximum tolerated sparsity and the required
computational power. The lower bound is essentially tight, matching the runtime
of an existing algorithm due to Koiran and Zouzias. Our hardness result allows
$\delta$ to take any constant value in $(0,1)$, which captures the relevant
regime for compressed sensing. This improves upon the existing average-case
hardness result of Wang, Berthet, and Plan, which is limited to $\delta =
o(1)$.
</p></div>
    </summary>
    <updated>2020-05-25T01:20:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11232</id>
    <link href="http://arxiv.org/abs/2005.11232" rel="alternate" type="text/html"/>
    <title>More on zeros and approximation of the Ising partition function</title>
    <feedworld_mtime>1590364800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Alexander Barvinok <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11232">PDF</a><br/><b>Abstract: </b>We consider the problem of computing $\sum_x e^{f(x)}$, where $f(x)=\sum_{ij}
a_{ij} \xi_i \xi_j + \sum_i b_i \xi_i$ is a real-valued quadratic function and
$x=\left(\xi_1, \ldots, \xi_n\right)$ ranges over the Boolean cube $\{-1,
1\}^n$. We prove that for any $\delta &gt;0$, fixed in advance, the value of
$\sum_x e^{f(x)}$ can be approximated within relative error $0 &lt; \epsilon &lt; 1$
is quasi-polynomial $n^{O(\ln n - \ln \epsilon)}$ time, as long as $\sum_j
|a_{ij}| \leq 1-\delta$ for all $i$. We apply the method of polynomial
interpolation, for which we prove that $\sum_x e^{f(x)} \ne 0$ for complex
$a_{ij}$ and $b_i$ such that $\sum_j |\Re\thinspace a_{ij}| \leq 1-\delta$,
$\sum_j |\Im\thinspace a_{ij}| \leq \delta^2/10$ and $|\Im\thinspace b_i| \leq
\delta^2/10$ for all $i$, which is interpreted as the absence of a phase
transition in the Lee - Yang sense in the corresponding Ising model. The bounds
are asymptotically optimal. The novel feature of the bounds is that they
control the total interaction of each vertex but not every pairwise
interaction.
</p></div>
    </summary>
    <updated>2020-05-25T01:23:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11195</id>
    <link href="http://arxiv.org/abs/2005.11195" rel="alternate" type="text/html"/>
    <title>A Dynamic Tree Algorithm for On-demand Peer-to-peer Ride-sharing Matching</title>
    <feedworld_mtime>1590364800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yao:Rui.html">Rui Yao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bekhor:Shlomo.html">Shlomo Bekhor</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11195">PDF</a><br/><b>Abstract: </b>Innovative shared mobility services provide on-demand flexible mobility
options and have the potential to alleviate traffic congestion. These
attractive services are challenging from different perspectives. One major
challenge in such systems is to find suitable ride-sharing matchings between
drivers and passengers with respect to the system objective and constraints,
and to provide optimal pickup and drop-off sequence to the drivers. In this
paper, we develop an efficient dynamic tree algorithm to find the optimal
pickup and drop-off sequence. The algorithm finds an initial solution to the
problem, keeps track of previously explored feasible solutions, and reduces the
solution search space when considering new requests. In addition, an efficient
pre-processing procedure to select candidate passenger requests is proposed,
which further improves the algorithm performance. Numerical experiments are
conducted on a real size network to illustrate the efficiency of our algorithm.
Sensitivity analysis suggests that small vehicle capacities and loose excess
travel time constraints do not guarantee overall savings in vehicle kilometer
traveled.
</p></div>
    </summary>
    <updated>2020-05-25T01:23:05Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11188</id>
    <link href="http://arxiv.org/abs/2005.11188" rel="alternate" type="text/html"/>
    <title>Still Simpler Static Level Ancestors</title>
    <feedworld_mtime>1590364800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hagerup:Torben.html">Torben Hagerup</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11188">PDF</a><br/><b>Abstract: </b>A level-ancestor or LA query about a rooted tree $T$ takes as arguments a
node $v$ in $T$, of depth $d_v$, say, and an integer $d$ with $0\le d\le d_v$
and returns the ancestor of $v$ in $T$ of depth $d$. The static LA problem is
to process a given rooted tree $T$ so as to support efficient subsequent
processing of LA queries about $T$. All previous efficient solutions to the
static LA problem work by reducing a given instance of the problem to a smaller
instance of the same or a related problem, solved with a less efficient data
structure, and a collection of small micro-instances for which a different
solution is provided. We indicate the first efficient solution to the static LA
problem that works directly, without resorting to reductions or
micro-instances.
</p></div>
    </summary>
    <updated>2020-05-25T01:24:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11116</id>
    <link href="http://arxiv.org/abs/2005.11116" rel="alternate" type="text/html"/>
    <title>Optimal Lower Bounds for Matching and Vertex Cover in Dynamic Graph Streams</title>
    <feedworld_mtime>1590364800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dark:Jacques.html">Jacques Dark</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Konrad:Christian.html">Christian Konrad</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11116">PDF</a><br/><b>Abstract: </b>In this paper, we give simple optimal lower bounds on the one-way two-party
communication complexity of approximate Maximum Matching and Minimum Vertex
Cover with deletions. In our model, Alice holds a set of edges and sends a
single message to Bob. Bob holds a set of edge deletions, which form a subset
of Alice's edges, and needs to report a large matching or a small vertex cover
in the graph spanned by the edges that are not deleted. Our results imply
optimal space lower bounds for insertion-deletion streaming algorithms for
Maximum Matching and Minimum Vertex Cover.
</p>
<p>Previously, Assadi et al. [SODA 2016] gave an optimal space lower bound for
insertion-deletion streaming algorithms for Maximum Matching via the
simultaneous model of communication. Our lower bound is simpler and stronger in
several aspects: The lower bound of Assadi et al. only holds for algorithms
that (1) are able to process streams that contain a triple exponential number
of deletions in $n$, the number of vertices of the input graph; (2) are able to
process multi-graphs; and (3) never output edges that do not exist in the input
graph when the randomized algorithm errs. In contrast, our lower bound even
holds for algorithms that (1) rely on short ($O(n^2)$-length) input streams;
(2) are only able to process simple graphs; and (3) may output non-existing
edges when the algorithm errs.
</p></div>
    </summary>
    <updated>2020-05-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.11026</id>
    <link href="http://arxiv.org/abs/2005.11026" rel="alternate" type="text/html"/>
    <title>Target Location Problem for Multi-commodity Flow</title>
    <feedworld_mtime>1590364800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Xingwu.html">Xingwu Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pan:Zhida.html">Zhida Pan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Yuyi.html">Yuyi Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.11026">PDF</a><br/><b>Abstract: </b>Motivated by scheduling in Geo-distributed data analysis, we propose a target
location problem for multi-commodity flow (LoMuF for short). Given commodities
to be sent from their resources, LoMuF aims at locating their targets so that
the multi-commodity flow is optimized in some sense. LoMuF is a combination of
two fundamental problems, namely, the facility location problem and the network
flow problem. We study the hardness and algorithmic issues of the problem in
various settings. The findings lie in three aspects. First, a series of
NP-hardness and APX-hardness results are obtained, uncovering the inherent
difficulty in solving this problem. Second, we propose an approximation
algorithm for general undirected networks and an exact algorithm for undirected
trees, which naturally induce efficient approximation algorithms on directed
networks. Third, we observe separations between directed networks and
undirected ones, indicating that imposing direction on edges makes the problem
strictly harder. These results show the richness of the problem and pave the
way to further studies.
</p></div>
    </summary>
    <updated>2020-05-25T01:22:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10917</id>
    <link href="http://arxiv.org/abs/2005.10917" rel="alternate" type="text/html"/>
    <title>Succinct Trit-array Trie for Scalable Trajectory Similarity Search</title>
    <feedworld_mtime>1590364800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kanda:Shunsuke.html">Shunsuke Kanda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takeuchi:Koh.html">Koh Takeuchi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fujii:Keisuke.html">Keisuke Fujii</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tabei:Yasuo.html">Yasuo Tabei</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10917">PDF</a><br/><b>Abstract: </b>Massive datasets of spatial trajectories representing the mobility of a
diversity of moving objects are ubiquitous in research and industry. Similarity
search of a large collection of trajectories is indispensable for turning these
datasets into knowledge. Current methods for similarity search of trajectories
are inefficient in terms of search time and memory when applied to massive
datasets. In this paper, we address this problem by presenting a scalable
similarity search for Fr\'echet distance on trajectories, which we call
trajectory-indexing succinct trit-array trie (tSTAT). tSTAT achieves time and
memory efficiency by leveraging locality sensitive hashing (LSH) for Fr\'echet
distance and a trie data structure. We also present two novel techniques of
node reduction and a space-efficient representation for tries, which enable to
dramatically enhance a memory efficiency of tries. We experimentally test tSTAT
on its ability to retrieve similar trajectories for a query from large
collections of trajectories and show that tSTAT performs superiorly with
respect to search time and memory efficiency.
</p></div>
    </summary>
    <updated>2020-05-25T01:22:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10885</id>
    <link href="http://arxiv.org/abs/2005.10885" rel="alternate" type="text/html"/>
    <title>Algebraic Hardness versus Randomness in Low Characteristic</title>
    <feedworld_mtime>1590364800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Andrews:Robert.html">Robert Andrews</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10885">PDF</a><br/><b>Abstract: </b>We show that lower bounds for explicit constant-variate polynomials over
fields of characteristic $p &gt; 0$ are sufficient to derandomize polynomial
identity testing over fields of characteristic $p$. In this setting, existing
work on hardness-randomness tradeoffs for polynomial identity testing requires
either the characteristic to be sufficiently large or the notion of hardness to
be stronger than the standard syntactic notion of hardness used in algebraic
complexity. Our results make no restriction on the characteristic of the field
and use standard notions of hardness.
</p>
<p>We do this by combining the Kabanets-Impagliazzo generator with a white-box
procedure to take $p$-th roots of circuits computing a $p$-th power over fields
of characteristic $p$. When the number of variables appearing in the circuit is
bounded by some constant, this procedure turns out to be efficient, which
allows us to bypass difficulties related to factoring circuits in
characteristic $p$.
</p>
<p>We also combine the Kabanets-Impagliazzo generator with recent
"bootstrapping" results in polynomial identity testing to show that a
sufficiently-hard family of explicit constant-variate polynomials yields a
near-complete derandomization of polynomial identity testing. This result holds
over fields of both zero and positive characteristic and complements a recent
work of Guo, Kumar, Saptharishi, and Solomon, who obtained a slightly stronger
statement over fields of characteristic zero.
</p></div>
    </summary>
    <updated>2020-05-25T01:21:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10852</id>
    <link href="http://arxiv.org/abs/2005.10852" rel="alternate" type="text/html"/>
    <title>Online Coloring and a New Type of Adversary for Online Graph Problems</title>
    <feedworld_mtime>1590364800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yaqiao.html">Yaqiao Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narayan:Vishnu_V=.html">Vishnu V. Narayan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pankratov:Denis.html">Denis Pankratov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10852">PDF</a><br/><b>Abstract: </b>We introduce a new type of adversary for online graph problems. The new
adversary is parameterized by a single integer $\kappa$, which upper bounds the
number of connected components that the adversary can use at any time during
the presentation of the online graph $G$. We call this adversary "$\kappa$
components bounded", or $\kappa$-CB for short. On one hand, this adversary is
restricted compared to the classical adversary because of the $\kappa$-CB
constraint. On the other hand, we seek competitive ratios parameterized only by
$\kappa$ with no dependence on the input length $n$, thereby giving the new
adversary power to use arbitrarily large inputs.
</p>
<p>We study online coloring under the $\kappa$-CB adversary. We obtain finer
analysis of the existing algorithms $FirstFit$ and $CBIP$ by computing their
competitive ratios on trees and bipartite graphs under the new adversary.
Surprisingly, $FirstFit$ outperforms $CBIP$ on trees. When it comes to
bipartite graphs $FirstFit$ is no longer competitive under the new adversary,
while $CBIP$ uses at most $2\kappa$ colors. We also study several well known
classes of graphs, such as $3$-colorable, $C_k$-free, $d$-inductive, planar,
and bounded treewidth, with respect to online coloring under the $\kappa$-CB
adversary. We demonstrate that the extra adversarial power of unbounded input
length outweighs the restriction on the number of connected components leading
to non existence of competitive algorithms for these classes.
</p></div>
    </summary>
    <updated>2020-05-25T01:23:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10830</id>
    <link href="http://arxiv.org/abs/2005.10830" rel="alternate" type="text/html"/>
    <title>Chang's lemma via Pinsker's inequality</title>
    <feedworld_mtime>1590364800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hambardzumyan:Lianna.html">Lianna Hambardzumyan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yaqiao.html">Yaqiao Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10830">PDF</a><br/><b>Abstract: </b>Extending the idea in [Impagliazzo, R., Moore, C. and Russell, A., An
entropic proof of Chang's inequality. SIAM Journal on Discrete Mathematics,
28(1), pp.173-176.] we give a short information theoretic proof for Chang's
lemma that is based on Pinsker's inequality.
</p></div>
    </summary>
    <updated>2020-05-25T01:21:14Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Vienna, TU Vienna, IST Austria, WU Vienna (apply by June 15, 2020)</title>
    <summary>The Vienna Graduate School on Computational Optimization (VGSCO) is a research and training program funded by the Austrian Science Funds (FWF). The VGSCO offers lecture series given by international experts in optimization related fields, organizes research seminars, retreats, soft skills courses, scientific workshops, and social events, provides travel grants, and supports research stays abroad. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Vienna Graduate School on Computational Optimization (VGSCO) is a research and training program funded by the Austrian Science Funds (FWF). The VGSCO offers lecture series given by international experts in optimization related fields, organizes research seminars, retreats, soft skills courses, scientific workshops, and social events, provides travel grants, and supports research stays abroad.</p>
<p>Website: <a href="http://vgsco.univie.ac.at/positions">http://vgsco.univie.ac.at/positions</a><br/>
Email: vgsco@univie.ac.at</p></div>
    </content>
    <updated>2020-05-24T08:48:14Z</updated>
    <published>2020-05-24T08:48:14Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-25T21:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10801</id>
    <link href="http://arxiv.org/abs/2005.10801" rel="alternate" type="text/html"/>
    <title>Complexity Analysis Of Next-Generation VVC Encoding and Decoding</title>
    <feedworld_mtime>1590278400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pakdaman:Farhad.html">Farhad Pakdaman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adelimanesh:Mohammad_Ali.html">Mohammad Ali Adelimanesh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gabbouj:Moncef.html">Moncef Gabbouj</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hashemi:Mahmoud_Reza.html">Mahmoud Reza Hashemi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10801">PDF</a><br/><b>Abstract: </b>While the next generation video compression standard, Versatile Video Coding
(VVC), provides a superior compression efficiency, its computational complexity
dramatically increases. This paper thoroughly analyzes this complexity for both
encoder and decoder of VVC Test Model 6, by quantifying the complexity
break-down for each coding tool and measuring the complexity and memory
requirements for VVC encoding/decoding. These extensive analyses are performed
for six video sequences of 720p, 1080p, and 2160p, under Low-Delay (LD),
Random-Access (RA), and All-Intra (AI) conditions (a total of 320
encoding/decoding). Results indicate that the VVC encoder and decoder are 5x
and 1.5x more complex compared to HEVC in LD, and 31x and 1.8x in AI,
respectively. Detailed analysis of coding tools reveals that in LD on average,
motion estimation tools with 53%, transformation and quantization with 22%, and
entropy coding with 7% dominate the encoding complexity. In decoding, loop
filters with 30%, motion compensation with 20%, and entropy decoding with 16%,
are the most complex modules. Moreover, the required memory bandwidth for VVC
encoding/decoding are measured through memory profiling, which are 30x and 3x
of HEVC. The reported results and insights are a guide for future research and
implementations of energy-efficient VVC encoder/decoder.
</p></div>
    </summary>
    <updated>2020-05-24T22:21:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10800</id>
    <link href="http://arxiv.org/abs/2005.10800" rel="alternate" type="text/html"/>
    <title>New Approximation Algorithms for Maximum Asymmetric Traveling Salesman and Shortest Superstring</title>
    <feedworld_mtime>1590278400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paluch:Katarzyna.html">Katarzyna Paluch</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10800">PDF</a><br/><b>Abstract: </b>In the maximum asymmetric traveling salesman problem (Max ATSP) we are given
a complete directed graph with nonnegative weights on the edges and we wish to
compute a traveling salesman tour of maximum weight. In this paper we give a
fast combinatorial $\frac{7}{10}$-approximation algorithm for Max ATSP. It is
based on techniques of {\em eliminating} and {\em diluting} problematic
subgraphs with the aid of {\it half-edges} and a method of edge coloring. (A
{\it half-edge} of edge $(u,v)$ is informally speaking "either a head or a tail
of $(u,v)$".) A novel technique of {\em diluting} a problematic subgraph $S$
consists in a seeming reduction of its weight, which allows its better
handling.
</p>
<p>The current best approximation algorithms for Max ATSP, achieving the
approximation guarantee of $\frac 23$, are due to Kaplan, Lewenstein, Shafrir,
Sviridenko (2003) and Elbassioni, Paluch, van Zuylen (2012). Using a result by
Mucha, which states that an $\alpha$-approximation algorithm for Max ATSP
implies a $(2+\frac{11(1-\alpha)}{9-2\alpha})$-approximation algorithm for the
shortest superstring problem (SSP), we obtain also a $(2 \frac{33}{76} \approx
2,434)$-approximation algorithm for SSP, beating the previously best known
(having an approximation factor equal to $2 \frac{11}{23} \approx 2,4782$.)
</p></div>
    </summary>
    <updated>2020-05-24T22:35:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10749</id>
    <link href="http://arxiv.org/abs/2005.10749" rel="alternate" type="text/html"/>
    <title>Distributed Verifiers in PCP</title>
    <feedworld_mtime>1590278400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaladanki:Nagaganesh.html">Nagaganesh Jaladanki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Wilson.html">Wilson Wu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10749">PDF</a><br/><b>Abstract: </b>Traditional proof systems involve a resource-bounded verifier communicating
with a powerful (but untrusted) prover. Distributed verifier proof systems are
a new family of proof models that involve a network of verifier nodes
communicating with a single independent prover that has access to the complete
network structure of the verifiers. The prover is tasked with convincing all
verifiers of some global property of the network graph. In addition, each
individual verifier may be given some input string they will be required to
verify during the course of computation. Verifier nodes are allowed to exchange
messaged with nodes a constant distance away, and accept / reject the input
after some computation.
</p>
<p>Because individual nodes are limited to a local view, communication with the
prover is potentially necessary to prove global properties about the network
graph of nodes, which only the prover has access to. In this system of models,
the entire model accepts the input if and only if every individual node has
accepted.
</p>
<p>There are three models in the distributed verifier proof system family:
$\mathsf{LCP}$, $\mathsf{dIP}$, and our proposed $\mathsf{dPCP}$, with the
fundamental difference between these coming from the type of communication
established between the verifiers and the prover. In this paper, we will first
go over the past work in the $\mathsf{LCP}$ and $\mathsf{dIP}$ space before
showing properties and proofs in our $\mathsf{dPCP}$ system.
</p></div>
    </summary>
    <updated>2020-05-24T22:32:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10610</id>
    <link href="http://arxiv.org/abs/2005.10610" rel="alternate" type="text/html"/>
    <title>Combinatorial two-stage minmax regret problems under interval uncertainty</title>
    <feedworld_mtime>1590278400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goerigk:Marc.html">Marc Goerigk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kasperski:Adam.html">Adam Kasperski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zielinski:Pawel.html">Pawel Zielinski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10610">PDF</a><br/><b>Abstract: </b>In this paper a class of combinatorial optimization problems is discussed. It
is assumed that a feasible solution can be constructed in two stages. In the
first stage the objective function costs are known while in the second stage
they are uncertain and belong to an interval uncertainty set. In order to
choose a solution, the minmax regret criterion is used. Some general properties
of the problem are established and results for two particular problems, namely
the shortest path and the selection problem, are shown.
</p></div>
    </summary>
    <updated>2020-05-24T22:35:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10566</id>
    <link href="http://arxiv.org/abs/2005.10566" rel="alternate" type="text/html"/>
    <title>A Massively Parallel Algorithm for Minimum Weight Vertex Cover</title>
    <feedworld_mtime>1590278400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghaffari:Mohsen.html">Mohsen Ghaffari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Ce.html">Ce Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nilis:Daan.html">Daan Nilis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10566">PDF</a><br/><b>Abstract: </b>We present a massively parallel algorithm, with near-linear memory per
machine, that computes a $(2+\varepsilon)$-approximation of minimum-weight
vertex cover in $O(\log\log d)$ rounds, where $d$ is the average degree of the
input graph.
</p>
<p>Our result fills the key remaining gap in the state-of-the-art MPC algorithms
for vertex cover and matching problems; two classic optimization problems,
which are duals of each other. Concretely, a recent line of work---by Czumaj et
al. [STOC'18], Ghaffari et al. [PODC'18], Assadi et al. [SODA'19], and Gamlath
et al. [PODC'19]---provides $O(\log\log n)$ time algorithms for
$(1+\varepsilon)$-approximate maximum weight matching as well as for
$(2+\varepsilon)$-approximate minimum cardinality vertex cover. However, the
latter algorithm does not work for the general weighted case of vertex cover,
for which the best known algorithm remained at $O(\log n)$ time complexity.
</p></div>
    </summary>
    <updated>2020-05-24T22:34:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10506</id>
    <link href="http://arxiv.org/abs/2005.10506" rel="alternate" type="text/html"/>
    <title>Hardness of Modern Games</title>
    <feedworld_mtime>1590278400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Costa:Diogo_M=.html">Diogo M. Costa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Francisco:Alexandre_P=.html">Alexandre P. Francisco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Russo:Lu=iacute=s_M=_S=.html">Luís M. S. Russo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10506">PDF</a><br/><b>Abstract: </b>We consider the complexity properties of modern puzzle games, Hexiom, Cut the
Rope and Back to Bed. The complexity of games plays an important role in the
type of experience they provide to players. Back to Bed is shown to be
PSPACE-Hard and the first two are shown to be NP-Hard. These results give
further insight into the structure of these games and the resulting
constructions may be useful in further complexity studies.
</p></div>
    </summary>
    <updated>2020-05-24T22:33:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10427</id>
    <link href="http://arxiv.org/abs/2005.10427" rel="alternate" type="text/html"/>
    <title>Sparse Tensor Transpositions</title>
    <feedworld_mtime>1590278400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mueller:Suzanne.html">Suzanne Mueller</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahrens:Peter.html">Peter Ahrens</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chou:Stephen.html">Stephen Chou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kjolstad:Fredrik.html">Fredrik Kjolstad</a>, Saman Amarasinghe <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10427">PDF</a><br/><b>Abstract: </b>We present a new algorithm for transposing sparse tensors called Quesadilla.
The algorithm converts the sparse tensor data structure to a list of
coordinates and sorts it with a fast multi-pass radix algorithm that exploits
knowledge of the requested transposition and the tensors input partial
coordinate ordering to provably minimize the number of parallel partial sorting
passes. We evaluate both a serial and a parallel implementation of Quesadilla
on a set of 19 tensors from the FROSTT collection, a set of tensors taken from
scientific and data analytic applications. We compare Quesadilla and a
generalization, Top-2-sadilla to several state of the art approaches, including
the tensor transposition routine used in the SPLATT tensor factorization
library. In serial tests, Quesadilla was the best strategy for 60% of all
tensor and transposition combinations and improved over SPLATT by at least 19%
in half of the combinations. In parallel tests, at least one of Quesadilla or
Top-2-sadilla was the best strategy for 52% of all tensor and transposition
combinations.
</p></div>
    </summary>
    <updated>2020-05-24T22:34:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.10328</id>
    <link href="http://arxiv.org/abs/2005.10328" rel="alternate" type="text/html"/>
    <title>A Unifying Model for Locally Constrained Spanning Tree Problems</title>
    <feedworld_mtime>1590278400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Viana:Luiz_Alberto_do_Carmo.html">Luiz Alberto do Carmo Viana</a>, Manoel Campêlo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sau:Ignasi.html">Ignasi Sau</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Ana.html">Ana Silva</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10328">PDF</a><br/><b>Abstract: </b>Given a graph $G$ and a digraph $D$ whose vertices are the edges of $G$, we
investigate the problem of finding a spanning tree of $G$ that satisfies the
constraints imposed by $D$. The restrictions to add an edge in the tree depend
on its neighborhood in $D$. Here, we generalize previously investigated
problems by also considering as input functions $\ell$ and $u$ on $E(G)$ that
give a lower and an upper bound, respectively, on the number of constraints
that must be satisfied by each edge. The produced feasibility problem is
denoted by \texttt{G-DCST}, while the optimization problem is denoted by
\texttt{G-DCMST}. We show that \texttt{G-DCST} is NP-complete even under strong
assumptions on the structures of $G$ and $D$, as well as on functions $\ell$
and $u$. On the positive side, we prove two polynomial results, one for
\texttt{G-DCST} and another for \texttt{G-DCMST}, and also give a simple
exponential-time algorithm along with a proof that it is asymptotically optimal
under the \ETH. Finally, we prove that other previously studied constrained
spanning tree (\textsc{CST}) problems can be modeled within our framework,
namely, the \textsc{Conflict CST}, the \textsc{Forcing CS, the \textsc{At Least
One/All Dependency CST}, the \textsc{Maximum Degree CST}, the \textsc{Minimum
Degree CST}, and the \textsc{Fixed-Leaves Minimum Degree CST}.
</p></div>
    </summary>
    <updated>2020-05-24T22:32:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.09152</id>
    <link href="http://arxiv.org/abs/2005.09152" rel="alternate" type="text/html"/>
    <title>Lasso formulation of the shortest path problem</title>
    <feedworld_mtime>1590278400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Anqi.html">Anqi Dong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Taghvaei:Amirhossein.html">Amirhossein Taghvaei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Georgiou:Tryphon_T=.html">Tryphon T. Georgiou</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.09152">PDF</a><br/><b>Abstract: </b>The shortest path problem is formulated as an $l_1$-regularized regression
problem, known as lasso. Based on this formulation, a connection is established
between Dijkstra's shortest path algorithm and the least angle regression
(LARS) for the lasso problem. Specifically, the solution path of the lasso
problem, obtained by varying the regularization parameter from infinity to zero
(the regularization path), corresponds to shortest path trees that appear in
the bi-directional Dijkstra algorithm. Although Dijkstra's algorithm and the
LARS formulation provide exact solutions, they become impractical when the size
of the graph is exceedingly large. To overcome this issue, the alternating
direction method of multipliers (ADMM) is proposed to solve the lasso
formulation. The resulting algorithm produces good and fast approximations of
the shortest path by sacrificing exactness that may not be absolutely essential
in many applications. Numerical experiments are provided to illustrate the
performance of the proposed approach.
</p></div>
    </summary>
    <updated>2020-05-24T22:36:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/082</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/082" rel="alternate" type="text/html"/>
    <title>TR20-082 |  MaxSAT Resolution and Subcube Sums | 

	Yuval Filmus, 

	Meena Mahajan, 

	Gaurav Sood, 

	Marc Vinyals</title>
    <summary>We study the MaxRes rule in the context of certifying unsatisfiability. We show that it can be exponentially more powerful than tree-like resolution, and when augmented with weakening (the system MaxResW), p-simulates tree-like resolution. In devising a lower bound technique specific to MaxRes (and not merely inheriting lower bounds from Res), we define a new semialgebraic proof system called the SubCubeSums proof system. This system, which p-simulates MaxResW, is a special case of the Sherali-Adams proof system. In expressivity, it is the integral restriction of conical juntas studied in the contexts of communication complexity and extension complexity. We show that it is not simulated by Res. Using a proof technique qualitatively different from the lower bounds that MaxResW inherits from Res, we show that Tseitin contradictions on expander graphs are hard to refute in SubCubeSums. We also establish a lower bound technique via lifting: for formulas requiring large degree in SubCubeSums, their XOR-ification requires large size in SubCubeSums.</summary>
    <updated>2020-05-23T19:58:01Z</updated>
    <published>2020-05-23T19:58:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-25T21:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=437</id>
    <link href="https://tcsplus.wordpress.com/2020/05/22/tcs-talk-wednesday-may-27-rahul-ilango-mit/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 27 — Rahul Ilango, MIT</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 27th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Rahul Ilango from MIT will speak about “Is it (NP) hard to distinguish order from chaos?” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 27th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Rahul Ilango</strong> from MIT will speak about “<em>Is it (NP) hard to distinguish order from chaos?</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br/>
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: The Minimum Circuit Size Problem (MCSP) roughly asks what the “complexity” of a given string is. Informally, one can think of this as determining the degree of “computational order” a string has.</p>
<p>In the past several years, there has been a resurgence of interest in MCSP. A series of exciting results have begun unraveling what looks to be a fascinating story. This story already reveals deep connections between MCSP and a growing list of fields, including cryptography, learning theory, structural complexity theory, average-case complexity, and circuit complexity. As an example, Santhanam recently proved a conditional equivalence between the complexity of MCSP and the existence of one-way functions.</p>
<p>This talk is split into two parts. The first part is a broad introduction to MCSP, answering the following questions: What is this problem? Why is it interesting? What do we know so far, and where might the story go next? The second part discusses recent joint work with Bruno Loff and Igor Oliveira showing that the “multi-output version” of MCSP is NP-hard.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2020-05-22T22:37:13Z</updated>
    <published>2020-05-22T22:37:13Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-05-25T21:21:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17063</id>
    <link href="https://rjlipton.wordpress.com/2020/05/22/math-tells/" rel="alternate" type="text/html"/>
    <title>Math Tells</title>
    <summary>How to tell what part of math you are from Gerolamo Cardano is often credited with introducing the notion of complex numbers. In 1545, he wrote a book titled Ars Magna. He introduced us to numbers like in his quest to understand solutions to equations. Cardano was often short of money and gambled and played […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>How to tell what part of math you are from</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/unknown-141/" rel="attachment wp-att-17065"><img alt="" class="alignright size-medium wp-image-17065" height="160" src="https://rjlipton.files.wordpress.com/2020/05/unknown-1.jpeg?w=300&amp;h=160" width="300"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Gerolamo Cardano is often credited with introducing the notion of complex numbers. In 1545, he wrote a book titled <i>Ars Magna</i>. He introduced us to numbers like <img alt="{\sqrt{-5}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B-5%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{-5}}"/> in his quest to understand solutions to equations. Cardano was often short of money and gambled and played a certain board game to make money—see the second paragraph <a href="https://en.wikipedia.org/wiki/Gerolamo_Cardano#Mathematics">here</a>. </p>
<p>
Today, for amusement, Ken and I thought we’d talk about tells.</p>
<p>
What are tells? Wikipedia <a href="https://en.wikipedia.org/wiki/Tell_(poker)">says</a>: </p>
<blockquote><p><b> </b> <em> A tell in poker is a change in a player’s behavior or demeanor that is claimed by some to give clues to that player’s assessment of their hand. </em>
</p></blockquote>
<p/><p>
<a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/tell/" rel="attachment wp-att-17068"><img alt="" class="aligncenter size-medium wp-image-17068" height="300" src="https://rjlipton.files.wordpress.com/2020/05/tell.jpg?w=201&amp;h=300" width="201"/></a></p>
<p>
</p><p/><h2> Other Tells </h2><p/>
<p/><p>
Ken and I have been thinking of tells in a wider sense—when and whether one can declare inferences amid uncertain information. Historians face this all the time. So do biographers, at least when their subjects are no longer living. We would also like to make inferences in our current world, such as about the pandemic. The stakes can be higher than in poker. In poker, if your “tell” inference is wrong and you lose, you can play another hand—unless you went all in. With science and other academic areas the attitude must be that you’re all-in all the time.</p>
<p>
Cardano furnishes several instances. Wikipedia—which we regard as an equilibrium of opinions—says that Cardano </p>
<blockquote><p><b> </b> <em> acknowledged the existence of imaginary numbers … [but] did not understand their properties, [which were] described for the first time by his Italian contemporary Rafael Bombelli. </em>
</p></blockquote>
<p/><p>
This is a negative inference from how one of Cardano’s books stops short of treating imaginary numbers as objects that follow rules. </p>
<p>
There are also questions about whether Cardano can be considered “the father of probability” ahead of Blaise Pascal and Pierre de Fermat. Part of the problem is that Cardano’s own writings late in life recounted his first erroneous reasonings as well as final understanding in a Hamlet-like fashion. Wikipedia doubts whether he really knew the rule of multiplying probabilities of independent events, whereas the <a href="http://www.columbia.edu/~pg2113/index_files/Gorroochurn-Some Laws.pdf">essay</a> by Prakash Gorroochurn cited there convinces us that he did. Similar doubt extends to how much Cardano knew about the natural sciences, as correct inferences (such as mountains with seashell fossils having once been underwater) are mixed in with what we today regard as howlers.</p>
<p>
Every staging of Shakespeare’s <i>Hamlet</i> shows a book by Cardano—or does it? In Act II, scene 2, Polonius asks, “What do you read, my lord?”; to which Hamlet first replies “Words words words.” Pressed on the book’s topic, Hamlet perhaps references the section “Misery of Old Age” in Cardano’s 1543 book <i>De Consolatione</i> but what he <a href="https://www.sparknotes.com/nofear/shakespeare/hamlet/page_102/">says</a> is so elliptical it is hard to tell. The book also includes particular allusions between sleep and death that go into Hamlet’s soliloquy opening Act III. The book had been published in England in 1573 as <i>Cardan’s Comfort</i> under the aegis of the Earl of Oxford so it was well-known. Yet the writer Italo Calvino <a href="https://books.google.com/books?id=pQabBQAAQBAJ&amp;pg=PA77&amp;lpg=PA77&amp;dq=Hamlet+words+words+words+Cardano&amp;source=bl&amp;ots=mu1gcM6b8E&amp;sig=ACfU3U3W7te91qlDaIIC8EeHLM1GxpR2Dw&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwikkNjX1MfpAhWChHIEHQaWDZsQ6AEwCXoECAoQAQ#v=onepage&amp;q=Hamlet words words words Cardano&amp;f=false">held back</a> from the inference:</p>
<blockquote><p><b> </b> <em> To conclude from this that the book read by Hamlet is definitely Cardano, as is held by some scholars of Shakespeare’s sources, is perhaps unjustified. </em>
</p></blockquote>
<p/><p>
To be sure, there are some who believe Shakespeare’s main source was Oxford, in manuscripts if not flesh and blood. One reason we do not go there is that we do not see the wider community as having been able to establish reliable principles for judging what kinds of inferences are probably valid. We wonder if one could do an experiment of taking resolved cases, removing most of the information to take them down to the level of unresolved cases, and seeing what kinds of inferences from partial information would have worked.  That’s not our expertise, but within our expertise in math and CS, we wonder if a little experiment will be helpful.</p>
<p>
To set the idea, note that imaginary numbers are also called complex numbers. Yet the term complex numbers can mean other things. Besides numbers like <img alt="{2 + 3i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+%2B+3i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 + 3i}"/> it also can mean how hard it is to <a href="https://en.wikipedia.org/wiki/Integer_complexity">construct</a> a number. </p>
<blockquote><p><b> </b> <em> In number theory, the integer complexity of an integer is the smallest number of ones that can be used to represent it using ones and any number of additions, multiplications, and parentheses. It is always within a constant factor of the logarithm of the given integer. </em>
</p></blockquote>
<p/><p>
How easy is it to tell what kind of “complex” is meant if you only have partial information?  We don’t only mean scope-of-terminology issues; often a well-defined math object is used in multiple areas.  Let’s try an experiment.</p>
<p>
</p><p/><h2> Math Tells </h2><p/>
<p/><p>
Suppose you <s> walk in</s> log-in to a talk without any idea of the topic. If the speaker uses one of these terms can you tell what her talk might be about? Several have multiple meanings. What are some of them? A passing score is <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/></p>
<ol>
<p/><li>
She says let <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> be a <i>c.e.</i> set.<p/>
<p/></li><li>
She says let <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> be in <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega}"/>.<p/>
<p/></li><li>
She says by the König principle.<p/>
<p/></li><li>
She says <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> is a <i>prime</i>.<p/>
<p/></li><li>
She says <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is a <i>prime</i>.<p/>
<p/></li><li>
She says <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is <i>solvable</i>.<p/>
<p/></li><li>
She says let its <i>degree</i> be <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>.<p/>
<p/></li><li>
She says there is a <i>run</i>.<p/>
<p/></li><li>
She says it is <i>reducible</i>.<p/>
<p/></li><li>
She says it is <i>satisfiable</i>.<p/>
</li></ol>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What are your answers? Do you have some tells of your own?</p>
<p/></font></font></div>
    </content>
    <updated>2020-05-22T16:12:56Z</updated>
    <published>2020-05-22T16:12:56Z</published>
    <category term="History"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="poker"/>
    <category term="tells"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-05-25T21:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/081</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/081" rel="alternate" type="text/html"/>
    <title>TR20-081 |  Algebraic Hardness versus Randomness in Low Characteristic | 

	Robert Andrews</title>
    <summary>We show that lower bounds for explicit constant-variate polynomials over fields of characteristic $p &gt; 0$ are sufficient to derandomize polynomial identity testing over fields of characteristic $p$. In this setting, existing work on hardness-randomness tradeoffs for polynomial identity testing requires either the characteristic to be sufficiently large or the notion of hardness to be stronger than the standard syntactic notion of hardness used in algebraic complexity. Our results make no restriction on the characteristic of the field and use standard notions of hardness.

We do this by combining the Kabanets-Impagliazzo generator with a white-box procedure to take $p$-th roots of circuits computing a $p$-th power over fields of characteristic $p$. When the number of variables appearing in the circuit is bounded by some constant, this procedure turns out to be efficient, which allows us to bypass difficulties related to factoring circuits in characteristic $p$.

We also combine the Kabanets-Impagliazzo generator with recent ``bootstrapping'' results in polynomial identity testing to show that a sufficiently-hard family of explicit constant-variate polynomials yields a near-complete derandomization of polynomial identity testing. This result holds over fields of both zero and positive characteristic and complements a recent work of Guo, Kumar, Saptharishi, and Solomon, who obtained a slightly stronger statement over fields of characteristic zero.</summary>
    <updated>2020-05-21T23:45:11Z</updated>
    <published>2020-05-21T23:45:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-25T21:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/080</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/080" rel="alternate" type="text/html"/>
    <title>TR20-080 |  Continuous LWE | 

	Joan Bruna, 

	Oded Regev, 

	Min Jae Song, 

	Yi Tang</title>
    <summary>We introduce a continuous analogue of the Learning with Errors (LWE) problem, which we name CLWE. We give a polynomial-time quantum reduction from worst-case lattice problems to CLWE, showing that CLWE enjoys similar hardness guarantees to those of LWE. Alternatively, our result can also be seen as opening new avenues of (quantum) attacks on lattice problems. Our work resolves an open problem regarding the computational complexity of learning mixtures of Gaussians without separability assumptions (Diakonikolas 2016, Moitra 2018). As an additional motivation, (a slight variant of) CLWE was considered in the context of robust machine learning (Diakonikolas et al.~FOCS 2017), where hardness in the statistical query (SQ) model was shown; our work addresses the open question regarding its computational hardness (Bubeck et al.~ICML 2019).</summary>
    <updated>2020-05-21T14:39:43Z</updated>
    <published>2020-05-21T14:39:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-25T21:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/079</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/079" rel="alternate" type="text/html"/>
    <title>TR20-079 |  On Minimizing Regular Expressions Without Kleene Star | 

	Hermann Gruber , 

	Markus Holzer, 

	Simon Wolfsteiner</title>
    <summary>Finite languages lie at the heart of literally every regular expression. Therefore, we investigate the approximation complexity of minimizing regular expressions without Kleene star, or, equivalently, regular expressions describing finite languages. On the side of approximation hardness, given such an expression of size~$s$, we prove that it is impossible to approximate the minimum size required by an equivalent regular expression within a factor of  $O\left(\frac{s}{(\log s)^{2+\delta}}\right)$ if the running time is bounded by a quasipolynomial function depending on~$\delta$, for every $\delta&gt;0$, unless the exponential time hypothesis (ETH) fails. For approximation ratio~$O(s^{1-\delta})$, we prove an exponential time lower bound depending on~$\delta$, assuming ETH. The lower bounds apply for alphabets of constant size. On the algorithmic side, we show that the problem can be approximated in polynomial time within~$O(\frac{s\log\log s}{\log s})$, with~$s$ being the size of the given regular expression. For constant alphabet size, the bound improves to~$O(\frac{s}{\log s})$. Finally, we devise a familiy of superpolynomial approximation algorithms that attain the performance ratios of the lower bounds, while their running times are only slightly above those excluded by the ETH.</summary>
    <updated>2020-05-21T14:33:24Z</updated>
    <published>2020-05-21T14:33:24Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-25T21:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/078</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/078" rel="alternate" type="text/html"/>
    <title>TR20-078 |  The New Complexity Landscape around Circuit Minimization | 

	Eric Allender</title>
    <summary>We survey recent developments related to the Minimum Circuit Size Problem</summary>
    <updated>2020-05-21T14:15:55Z</updated>
    <published>2020-05-21T14:15:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-25T21:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4805</id>
    <link href="https://www.scottaaronson.com/blog/?p=4805" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4805#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4805" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Quantum Computing Lecture Notes 2.0</title>
    <summary xml:lang="en-US">Two years ago, I posted detailed lecture notes on this blog for my Intro to Quantum Information Science undergrad course at UT Austin. Today, with enormous thanks to UT PhD student Corey Ostrove, we’ve gotten the notes into a much better shape (for starters, they’re now in LaTeX). You can see the results here (7MB)—it’s […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Two years ago, I posted detailed <a href="https://www.scottaaronson.com/blog/?p=3943">lecture notes</a> on this blog for my Intro to Quantum Information Science undergrad course at UT Austin.  Today, with enormous thanks to UT PhD student Corey Ostrove, we’ve gotten the notes into a much better shape (for starters, they’re now in LaTeX).  You can <a href="https://www.scottaaronson.com/qclec.pdf">see the results here</a> (7MB)—it’s basically a 260-page introductory quantum computing textbook in beta form, covering similar material as many other introductory quantum computing textbooks, but in my style for those who like that.  It’s missing exercises, as well as material on quantum supremacy experiments, recent progress in hardware, etc., but that will be added in the next version if there’s enough interest.  Enjoy!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Unrelated Announcement:</span></strong> Bjorn Poonen at MIT pointed me to <a href="https://researchseminars.org/">researchseminars.org</a>, a great resource for finding out about technical talks that are being held online in the era of covid.  The developers recently added CS as a category, but so far there are very few CS talks listed.  Please help fix that!</p></div>
    </content>
    <updated>2020-05-20T21:14:56Z</updated>
    <published>2020-05-20T21:14:56Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-05-21T08:05:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1701</id>
    <link href="https://theorydish.blog/2020/05/19/incentive-compatible-sensitive-surveys/" rel="alternate" type="text/html"/>
    <title>Incentive Compatible Sensitive Surveys</title>
    <summary>Crucial decisions are increasingly being made by automated machine learning algorithms. These algorithms rely on data, and without high quality data, the resulting decisions may be inaccurate and/or unfair. In some cases, data is readily available: for example, location data passively collected by smartphones. In other cases, data may be difficult to obtain by automated means, and it is necessary to directly survey the population. However, individuals are not always motivated to take surveys if they receive no benefit. Offering a monetary reward may incentivize some individuals to participate, but there is a problem with this approach: what if an individual’s data is correlated with their willingness to take the survey? For concreteness, imagine that you are a health administrator trying to estimate the average weight in a population. This is a sensitive attribute that individuals may be reluctant to disclose, especially if their weight is not considered healthy. A generic survey may yield disproportionately more respondents with “healthy” weights, and thus may result an an inaccurate estimate (see, e.g, Shields et al., 2011). In this post, we discuss three papers which propose solutions to this problem through the lens of mechanism design. The idea is to carefully design payments [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1703" height="303" src="https://theorydish.files.wordpress.com/2020/05/survey.jpg?w=764" width="580"/></figure></div>

<p>Crucial decisions are increasingly being made by automated machine learning algorithms. These algorithms rely on data, and without high quality data, the resulting decisions may be inaccurate and/or unfair. In some cases, data is readily available: for example, location data passively collected by smartphones. In other cases, data may be difficult to obtain by automated means, and it is necessary to directly survey the population.</p>

<p>However, individuals are not always motivated to take surveys if they receive no benefit. Offering a monetary reward may incentivize some individuals to participate, but there is a problem with this approach: what if an individual’s data is correlated with their willingness to take the survey?</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1708" height="184" src="https://theorydish.files.wordpress.com/2020/05/correlation.png?w=459" width="457"/></figure></div>

<p>For concreteness, imagine that you are a health administrator trying to estimate the average weight in a population. This is a sensitive attribute that individuals may be reluctant to disclose, especially if their weight is not considered healthy. A generic survey may yield disproportionately more respondents with “healthy” weights, and thus may result an an inaccurate estimate (see, e.g, Shields et al., 2011).</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1707" height="234" src="https://theorydish.files.wordpress.com/2020/05/scale.jpeg?w=1024" width="408"/></figure></div>

<p>In this post, we discuss three papers which propose solutions to this problem through the lens of <em>mechanism design</em>. The idea is to carefully design payments so that we received an unbiased sample, leading to a hopefully accurate estimate.</p>

<h2><strong>Model</strong></h2>

<p>We use <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/> to denote agent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/>‘s data (e.g., her weight). We assume that each agent also has a personal cost <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/>, representing her level of reluctance to reveal her data. Agent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/> is willing to reveal <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/> if and only if she receives a payment of at least <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/>. Our goal is to allocate higher payments to agents with higher <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/>‘s, in order to get an unbiased sample. However, we also must obey an budget constraint: we cannot spend more than <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="B"/> total. The solution is to transact non-deterministically: with some probability, offer to purchase an agent’s data. Agents with higher costs will receive higher payments, but lower transaction probabilities.</p>

<p>We assume that agents are drawn at random independently from some distribution. Our crucial assumption is that we known the marginal distribution of agent costs, which we denote <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal{F}"/> (we will explore later what happens when this assumption is removed). However, we do not know the distribution of <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>, and that distribution can be arbitrarily correlated with <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal{F}"/>. As mentioned above, one might expect agents with less “desirable” <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>‘s may have higher costs, but one can imagine more complex correlations as well.</p>

<p>Our mechanisms consist of two parts: an <em>allocation rule</em> <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A"/>, and a <em>payment rule</em> <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P"/>. Given <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A"/> and <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P"/>, the mechanism works as follows:</p>

<ol><li>Ask each agent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/> to report <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/>. Let <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c"/> denote the actual reported cost.</li><li>With probability <img alt="A(c)" class="latex" src="https://s0.wp.com/latex.php?latex=A%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A(c)"/>, we purchase the agent’s data and pay her <img alt="P(c)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P(c)"/>. With probability <img alt="1 - A(c)" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+A%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="1 - A(c)"/>, we do not buy the data, and no payment is made.</li><li>At the end, use the data we learned to form an estimate of the population average of <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>. Let <img alt="\bar{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbar%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\bar{z}"/> denote our estimate.</li></ol>

<p>In this model, agent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/>‘s expected utility for reporting <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c"/> is <img alt="u_i(c) = A(c) (P(c) - c_i)" class="latex" src="https://s0.wp.com/latex.php?latex=u_i%28c%29+%3D+A%28c%29+%28P%28c%29+-+c_i%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="u_i(c) = A(c) (P(c) - c_i)"/>.</p>

<p>We have four main requirements:</p>

<ol><li><strong>Truthfulness. </strong>It should be in each agent’s best interest to truthfully report <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/>.</li><li><strong>Individual rationality. </strong>Agents should not receive negative utility if they are honest, i.e., we should have <img alt="P(c_i) \ge c_i" class="latex" src="https://s0.wp.com/latex.php?latex=P%28c_i%29+%5Cge+c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P(c_i) \ge c_i"/> for all <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/>.</li><li><strong>Budget constrained. </strong>Our total expected payment should not exceed <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="B"/>, i.e., <img alt="\mathbb{E}[\sum_i A(c_i) P(c_i)] \le B" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Csum_i+A%28c_i%29+P%28c_i%29%5D+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathbb{E}[\sum_i A(c_i) P(c_i)] \le B"/>.</li><li><strong>Unbiased. </strong>Our estimate isn’t consistently too high or too low. Specifically, the expected value of our estimate should be equal to the true average, i.e., <img alt="\mathbb{E}[\bar{z}] = \mathbb{E}[z_i]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Cbar%7Bz%7D%5D+%3D+%5Cmathbb%7BE%7D%5Bz_i%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathbb{E}[\bar{z}] = \mathbb{E}[z_i]"/>.</li></ol>

<p>Lack of bias doesn’t mean that our estimate is accurate, however. To this end, our primary goal is to <strong>minimize the variance</strong>, subject to the mechanism obeying the four above criteria. We evaluate variance via a worst-case framework: given a mechanism, we wish minimize the variance with respect to the worst-case distribution of agents for that mechanism. The idea is that the distribution of <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>‘s is not known to the mechanism, so we require it to perform well for all distributions.</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1729" height="203" src="https://theorydish.files.wordpress.com/2020/05/goal.jpg?w=970" width="438"/>Goal.</figure></div>

<p>When we refer to the “optimal” mechanism, we mean minimum variance, subject to being truthful, individually rational, budget constrained, and unbiased (henceforth TIBU).</p>

<p><strong>The Horvitz Thomspon Estimator</strong></p>

<p>Once we have learned the <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>‘s, how do we actual form an estimate of the mean? Luckily for us, this question has a simple answer. If we restrict ourselves to linear unbiased estimators, there is a unique way to do this, known as the <em>Horvitz-Thompson estimator</em>:</p>

<p><img alt="\bar{z} = \displaystyle\sum\limits_i \cfrac{z_i}{A(c_i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbar%7Bz%7D+%3D+%5Cdisplaystyle%5Csum%5Climits_i+%5Ccfrac%7Bz_i%7D%7BA%28c_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\bar{z} = \displaystyle\sum\limits_i \cfrac{z_i}{A(c_i)}"/></p>

<p>Thus our task is simply to choose <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A"/> and <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P"/>.</p>

<h2>Approach #1</h2>

<p>This model was first considered by Roth and Schoenebeck (2012). They are able to characterize a mechanism which is TIBU and has variance at most <img alt="1/n" class="latex" src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="1/n"/> more than the optimal variance, where <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n"/> is the number of agents. However, they do make the strong assumption that <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/> is either 0 or 1.</p>

<p>Their approach relies on <em>Take-It-Or-Leave-It</em> mechanisms. Such a mechanism is defined by a distribution $G$ over the positive real numbers, and works as follows:</p>

<ol><li>Each agent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/> reports a cost <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c"/>.</li><li>Sample a payment <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="p"/> from <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="G"/>.</li><li>If <img alt="p \ge c" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cge+c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="p \ge c"/>, buy the agent’s data with payment <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="p"/>. If <img alt="p &lt; c" class="latex" src="https://s0.wp.com/latex.php?latex=p+%3C+c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="p &lt; c"/>, do not buy the agent’s data.</li></ol>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" height="212" src="https://pbs.twimg.com/media/EDzLmHAWwAAEPkl.jpg" width="293"/></figure></div>

<p>This amounts to an allocation rule <img alt="A(c) = 1 - \text{Pr}[p\ge c]" class="latex" src="https://s0.wp.com/latex.php?latex=A%28c%29+%3D+1+-+%5Ctext%7BPr%7D%5Bp%5Cge+c%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A(c) = 1 - \text{Pr}[p\ge c]"/>, and a payment rule <img alt="P(c)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="P(c)"/> equal to the distribution <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="G"/> conditioned on being at least <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c"/>. The authors show that these mechanisms are fully general, i.e., any allocation and payment rule can be implemented by a Take-It-Or-Leave-It mechanism.</p>

<p>The proof of their main result is primarily based on using the calculus of variations to optimize over the space of distributions <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="G"/>. The paper contains some additional results, for example regarding an alternate model where we wish to minimize the budget, but that is outside the scope of this blog post.</p>

<h2>Approach #2</h2>

<p>Although the above result is a great step, it leaves room for improvement. First of all, the assumption that <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/> is binary is quite strong, and does not apply to our running example of body weight. Secondly, their mechanism does not quite achieve the optimal variance. Chen et al. (2018) remedy both of these concerns. That is, they allow <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/> to be any real number, and they characterize the TIBU mechanism with optimal variance. Their result also generalizes to more complex statistical estimates, not just the average <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>, and it holds for both continuous and discrete agent distributions.</p>

<p>The approach of Chen et al. (2018) is based on two primary ideas. First, they show that any monotone allocation rule (i.e., we are always less likely to purchase data from an agent with higher cost) can be implemented in a TIBU fashion by a unique payment rule. Thus we only need to identify the optimal allocation rule. (This is similar to the standard result from auction theory about implementable monotone allocation rules (Myerson 1981).)</p>

<p>The second idea is to view the problem as a zero-sum game between ourselves (the mechanism designer) and an adversary who chooses the distribution of agents. Given a distribution, we choose an allocation rule to minimize the variance, and given an allocation rule, the adversary chooses a distribution to maximize the variance.</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" height="297" src="https://i2.wp.com/marketbusinessnews.com/wp-content/uploads/2016/08/Zero-Sum-Game.jpg?fit=511%2C521&amp;ssl=1&amp;resize=1200%2C1223.4833659491" width="292"/></figure></div>

<p>The authors are able to solve for the equilibrium of this game and thus identify the TIBU mechanism with minimum possible variance.</p>

<h2>Approach #3</h2>

<p>Approach #2 gave us our desired result: a minimum variance mechanism subject to our four desired properties (TIBU), for any distribution of <img alt="z_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="z_i"/>‘s. But we are still making a very strong assumption: that we know the distribution of agent costs.</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1766" height="230" src="https://theorydish.files.wordpress.com/2020/05/prior.png?w=500" width="427"/></figure></div>

<p>Chen and Zheng (2019) do away with this assumption in a follow-up paper. They consider a model where the mechanism has no prior information on the distribution of costs (or on the distribution of data), and $n$ agents arrive one-by-one in a uniformly random order. Each agent reports a cost, and we decide whether to buy her data, and what to pay her. In order to price well, we need to learn the cost distribution, but we must do this while simultaneously making irrevocable purchasing decisions. The main result is a TIBU mechanism with variance at most a constant factor worse than optimal.</p>

<p>The authors note that after each step <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i"/>, the reported costs up to that point induce an empirical cost distribution <img alt="\mathcal{F}_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal{F}_i"/>. Using the results of Chen et al. (2018), we can determine the optimal mechanism for <img alt="\mathcal{F}_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\mathcal{F}_i"/>. The basic idea is to use that mechanism for the current step, learn a new agent cost <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/> (note that the agent reports <img alt="c_i" class="latex" src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_i"/> regardless of whether we purchase her data) and then update our empirical distribution accordingly. (The authors actually end up using an approximately optimal allocation rule, but the idea is the same.) The mechanism also uses more budget in the earlier rounds, to make up for the pricing being less accurate.</p>

<h2>Discussion</h2>

<p>In this post, we considered the problem of surveying a sensitive attribute where an agent’s data may be correlated with their willingness to participate. We discussed three different approaches, all of which rely of giving higher payments to agents with higher costs, in order to incentivize them to participate and to obtain an unbiased estimate. The final approach was able to give a truthful, individually rational, budget feasible, and unbiased mechanism with approximately optimal variance, without making any prior assumptions on the distribution of agents.</p>

<p>However, all three of the approaches assume that agents cannot lie about the data. This is reasonable for some attributes, such as a body weight, where an agent can be asked to step onto a physical scale. However, requiring participants come in person to a particular location will certainly lead to less engagement. Furthermore, for other sensitive attributes, there may not be a verifiable way to obtain the data. Future work could investigate alternative models where this assumption is not necessary. For example, perhaps agents do not maliciously lie, but rather are simply inaccurate at reporting their own attributes. For example, research has demonstrated that people consistently over-report height and under-report weight (e.g., Gorber et al., 2007). Could a mechanism learn the pattern of inaccuracy and compensate for that to still obtain an unbiased estimate?</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1775" height="243" src="https://theorydish.files.wordpress.com/2020/05/bias.png?w=1024" width="317"/></figure></div>

<h2><strong>References</strong></h2>

<ol><li>Yiling Chen, Nicole Immorlica, Brendan Lucier, Vasilis Syrgkanis, and Juba Ziani. “Optimal data acquisition for statistical estimation.” In <em>Proceedings of the 2018 ACM Conference on Economics and Computation</em>. 2018.</li><li>Yiling Chen and Shuran Zheng. “Prior-free data acquisition for accurate statistical estimation.” <em>Proceedings of the 2019 ACM Conference on Economics and Computation</em>. 2019.</li><li>Sarah Connor Gorber, Mark S. Tremplay, David Moher, and B. Gorber (2007). A comparison of direct vs. self‐report measures for assessing height, weight and body mass index: a systematic review. <em>Obesity reviews</em>, <em>8</em>(4), 307-326.</li><li>Roger Myerson. Optimal auction design. Mathematics of Operations Research, 6(1):58–73. 1981.</li><li>Aaron Roth and Grant Schoenebeck. “Conducting truthful surveys, cheaply.” <em>Proceedings of the 2012 ACM Conference on Electronic Commerce</em>. 2012.</li><li>Margot Shields, Sarah Connor Gorber, Ian Janssen, and Mark S. Tremblay. (2011). Bias in self-reported estimates of obesity in Canadian health surveys: an update on correction equations for adults. <em>Health Reports</em>, <em>22</em>(3), 35.</li></ol>

<p> </p></div>
    </content>
    <updated>2020-05-19T20:40:34Z</updated>
    <published>2020-05-19T20:40:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>bplaut</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-05-25T21:21:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1781</id>
    <link href="https://theorydish.blog/2020/05/19/nominations-for-tcs-women-rising-star-talks-at-stoc/" rel="alternate" type="text/html"/>
    <title>Nominations for TCS Women Rising Star talks at STOC</title>
    <summary>Directly from the organizers: ———– Dear colleagues, We invite you to nominate speakers for TCS Women Rising Star talks at STOC 2019, which are planned as part of our virtual TCS Women Spotlight Workshop. To be eligible, your nominee has to be a female or a minority researcher working in theoretical computer science (all topics represented at STOC are welcome) and has to be a graduating PhD student or a postdoc. You can make your nomination by filling this form by May 28th: https://forms.gle/R9nmit62ESA6V9vv6 STOC 2020 workshops will happen between June 23 and 25, with exact day/time TBD. You can see the list of speakers from last year here: https://sigact.org/tcswomen/2nd-tcs-women-meeting/tcs-women-2019/ Looking forward to your nominations and to seeing you at the our TCS Women Spotlight Workshop, Barna Saha, Virginia Vassilevska Williams, and Sofya Raskhodnikova</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>Directly from the organizers:</div>
<div/>
<div>———–</div>
<div/>
<div/>
<div>Dear colleagues,</div>
<div/>
<div>We invite you to nominate speakers for TCS Women Rising Star talks at STOC 2019, which are planned as part of our virtual TCS Women Spotlight Workshop. To be eligible, your nominee has to be a female or a minority researcher working in theoretical computer science (all topics represented at STOC are welcome) and has to be a graduating PhD student or a postdoc. You can make your nomination by filling this form by May 28th:</div>
<div/>
<div><a href="https://forms.gle/R9nmit62ESA6V9vv6" rel="noopener" target="_blank">https://forms.gle/R9nmit62ESA6V9vv6</a></div>
<div/>
<div>STOC 2020 workshops will happen between June 23 and 25, with exact day/time TBD.</div>
<div/>
<div>You can see the list of speakers from last year here:</div>
<div><a href="https://sigact.org/tcswomen/2nd-tcs-women-meeting/tcs-women-2019/" rel="noopener" target="_blank">https://sigact.org/tcswomen/2nd-tcs-women-meeting/tcs-women-2019/</a></div>
<div/>
<div>Looking forward to your nominations and to seeing you at the our TCS Women Spotlight Workshop,</div>
<div>Barna Saha, Virginia Vassilevska Williams, and Sofya Raskhodnikova</div></div>
    </content>
    <updated>2020-05-19T20:39:28Z</updated>
    <published>2020-05-19T20:39:28Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-05-25T21:21:21Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8975690027864850581</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8975690027864850581/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/obit-for-richard-dudley.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8975690027864850581" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8975690027864850581" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/obit-for-richard-dudley.html" rel="alternate" type="text/html"/>
    <title>Obit for Richard Dudley</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>
Richard M. (Dick) Dudley died on Jan. 19, 2020 (NOT from Coronavirus).You can find obituaries for him  <a href="http://news.mit.edu/2020/richard-dudley-mit-mathematics-professor-emeritus-dies-0218">here</a>, <a href="https://math.mit.edu/about/history/obituaries/dudley.php">here</a>, and <a href="http://www.ams.org/publicoutreach/in-memory/in-memory">here</a> and an interview with him from 2019  <a href="https://projecteuclid.org/euclid.ss/1555056041">here</a>.</div>
<div>
<br/></div>
<div>
<br/></div>
<div>
Professor Dudley worked in Probability and Statistics. His work is now</div>
<div>
being used in Machine Learning. Here is a guest-post-obit by</div>
<div>
David Marcus who had Prof. Dudley as his PhD Thesis Advisor.</div>
<div>
<br/></div>
<div>
-----------------------------------</div>
<div>
<br/></div>
<div>
Guest Blog Obit by David Marcus:</div>
<div>
<br/></div>
<div>
Dick was my thesis advisor at M.I.T. After I got my Ph.D. in 1983, I went</div>
<div>
to work in industry, so did not work closely with him, as some of his other</div>
<div>
students did. But, I enjoyed working with him very much in graduate school.</div>
<div>
<br/></div>
<div>
Dick was very precise. His lecture notes and articles (and later his books)</div>
<div>
said exactly what needed to be said and didn't waste words. In his classes,</div>
<div>
he always handed out complete lecture notes, thus letting you concentrate</div>
<div>
on the material rather than having to take a lot of notes.</div>
<div>
<br/></div>
<div>
Dick was very organized, but his office had piles of papers and journal</div>
<div>
articles everywhere. There is a picture <a href="http://news.mit.edu/2020/richard-dudley-mit-mathematics-professor-emeritus-dies-0218">here</a>.</div>
<div>
<br/></div>
<div>
Before Dick was my advisor, I took his probability course. My orals were</div>
<div>
going to be towards the end of the term, and I was going to use probability</div>
<div>
as one of my two minor areas. So, I spent a lot of time studying the</div>
<div>
material. Dick gave a final exam in the course. The final exam was unlike</div>
<div>
any other final exam I ever took: The exam listed twelve areas that had</div>
<div>
been covered in the course. The instructions said to pick ten and for each</div>
<div>
area give the main definitions and theorems and, if you had time, prove the</div>
<div>
theorems. Since I had been studying the material for my orals, I didn't</div>
<div>
have much trouble, but if I hadn't been studying it for my orals, it would</div>
<div>
have been quite a shock!(COMMENT FROM BILL: Sounds like a lazy way to make up an exam, though on this</div>
<div>
level of may it works. I know of a prof whose final was</div>
<div>
<br/></div>
<div>
Make up 4 good questions for the final. Now Solve them.</div>
<div>
<br/></div>
<div>
)</div>
<div>
<br/></div>
<div>
Once Dick became my advisor, Dick and I had a regular weekly meeting. I'd</div>
<div>
tell him what I'd figured out or what I'd found in a book or journal</div>
<div>
article over the last week and we'd discuss it and he'd make suggestions.</div>
<div>
At some point, I'd say I needed to think about it, and I'd leave. I never</div>
<div>
did find out how long these meetings were supposed to last because I was</div>
<div>
always the one to end them.(COMMENT FROM BILL: It's good someone ended them! Or else you might never</div>
<div>
had graduated :-) )</div>
<div>
<br/></div>
<div>
When I began working with Dick, he said he already had a full</div>
<div>
load of students, but he would see if he had something I could work on. The</div>
<div>
problem Dick came up with for me to work on was to construct a</div>
<div>
counterexample to a theorem that Dick had published. Dick knew his</div>
<div>
published proof was wrong, and had an idea of what a counterexample might</div>
<div>
look like, so suggested I might be able to prove it was a counterexample.</div>
<div>
In retrospect, this was perhaps a risky thesis problem for me since if the</div>
<div>
student gets stuck, the professor can spend time figuring out how to do it.</div>
<div>
But, in this case, presumably Dick had already put some effort into it</div>
<div>
without success. Regardless, with Dick's guidance, I was able to prove it,</div>
<div>
and soon after got my Ph.D.(COMMENT FROM BILL: Sounds risky since if Dick could not do it, maybe it's too hard.)</div>
<div>
<br/></div>
<div>
In 2003 there was a conference in honor of Dick's 65th birthday. All of his</div>
<div>
ex-students were invited, and many of them attended. There was a day of</div>
<div>
talks, and we all went out to dinner (Chinese food, if I recall correctly)</div>
<div>
in the evening. At dinner, I asked Dick if any of his other students had</div>
<div>
written a thesis that disproved one of his published theorems. He said I</div>
<div>
was the only one.(COMMENT FROM BILL: Really good that not only was he okay with you disproving</div>
<div>
his theorem, he encouraged you to!)</div>
<div>
<br/></div>
<div>
<br/></div></div>
    </content>
    <updated>2020-05-19T18:30:00Z</updated>
    <published>2020-05-19T18:30:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-05-25T16:34:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/077</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/077" rel="alternate" type="text/html"/>
    <title>TR20-077 |  Factorization of Polynomials Given by Arithmetic Branching Programs | 

	Amit Sinhababu, 

	Thomas Thierauf</title>
    <summary>Given a multivariate polynomial computed by an arithmetic branching program (ABP) of size $s$, we show that all its factors can be computed by arithmetic branching programs of size $\text{poly}(s)$. Kaltofen gave a similar result for polynomials computed by arithmetic circuits. The previously known best upper bound for ABP-factors was $\text{poly}(s^{\log s}) $.</summary>
    <updated>2020-05-19T13:47:54Z</updated>
    <published>2020-05-19T13:47:54Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-25T21:20:33Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/data_rep_bias/</id>
    <link href="https://gradientscience.org/data_rep_bias/" rel="alternate" type="text/html"/>
    <title>Identifying Statistical Bias in Dataset Replication</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/2005.09619" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/dataset-replication-analysis" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<br/></p>

<p><i> We discuss our <a href="https://arxiv.org/abs/2005.09619">paper</a>
on diagnosing bias in dataset replication 
studies. Zooming in on the <a href="https://arxiv.org/abs/1902.10811">ImageNet-v2</a>
reproduction effort, we explain the majority of the accuracy drop between
ImageNet and ImageNet-v2 (from 11.7% to 3.6%) after accounting
for bias in the data collection process. </i></p>

<h2 id="measuring-progress-in-supervised-learning">Measuring Progress in Supervised Learning</h2>
<p>In the last few years, researchers have made extraordinary progress on
increasing accuracy on vision tasks like those in the <a href="http://image-net.org">ImageNet</a>,
<a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>, and <a href="http://cocodataset.org/#home">COCO</a> datasets. Progress on these tasks is promising, but
comes with an important caveat: the test sets used to measure performance are
finite, fixed, and have been used and re-used by countless researchers over
several years.</p>

<p>There are (at least) two possible ways in which evaluating solely with test-set
accuracy could hinder our progress on the tasks researchers design benchmarks to
proxy (e.g. general image classification for ImageNet). The first of these
issues is adaptive overfitting: since each dataset has only one test
set to measure performance on, algorithmic progress on that (finite and fixed)
test set could be mistaken for algorithmic progress on the distribution from
which the test set was chosen from.</p>

<p>The second issue that could arise is
oversensitivity to irrelevant properties of the test distribution
arising from the dataset collection process; for example, the image encoding
algorithm used to save images.</p>

<p><em>How can we assess whether models are truly making progress on the tasks that
our benchmarks proxy</em>?</p>

<h3 id="dataset-replication">Dataset replication</h3>
<p>A promising approach to diagnosing the two above issues is <em>dataset
replication</em>, in which one mimics the original test set creation process as
closely as possible to make a new dataset. Then, existing models’ performance on
this newly created test set should identify any models that have adaptively
overfit to the original test set. Moreover, since every intricacy of a dataset
collection process cannot be mimicked exactly, natural variability in the
replication should help us unearth cases of algorithms’ oversensitivity to the
original dataset creation process.</p>

<p>The problem of replicating the original test set creation process is harder than it may initially appear. Particularly challenging is controlling for relevant covariates In experimental design, a <a href="https://www.theanalysisfactor.com/confusing-statistical-terms-5-covariate/">covariate</a>is a variable that is not the <a href="https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics">independent variable</a> (in our case the choice of dataset), but affects measurements of the <a href="https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics">dependent variable</a> (in our case the accuracy).&lt;p&gt;&lt;/p&gt; For example, suppose we wanted to replicate a study about the effect of a certain drug on an age-linked disease. After gathering subjects, we have to reweight or filter them so that the age distribution matches that of the original study, as otherwise the results of the studies are incomparable. This filtering/reweighting step is analogous to dataset replication, with participant age being the relevant covariate., or variables that we expect to have an impact on the outcome (i.e., model accuracy) we measure.</p>

<p>To match covariate distributions between the new reproduction and the original
dataset, we frame dataset replication as a two-step process. In step one, the
replicator collects candidate data using a data pipeline as similar as possible
to that used by the original dataset creators. Then, after approximating the
original pipeline, the dataset replicator should identify the relevant covariate
statistic(s), and choose candidates (via filtering or reweighting the collected
candidate data) so that the distributions of the statistic under the replicated
and original datasets are equal. We call this process statistic
matchingIn causal inference literature, statistic matching is often
referred to as enforcing covariate balance..</p>

<h3 id="imagenet-v2">ImageNet-v2</h3>
<p>In their recent work, <a href="https://arxiv.org/abs/1902.10811">Recht et al.</a> use
dataset replication to take a closer look at model performance on ImageNet, one
of the most widely used computer vision datasets. They first use an apparatus
similar to that of the original <a href="https://arxiv.org/abs/1409.0575">ImageNet
paper</a> to collect a large set of candidate
images and labels from the photo-sharing site <a href="https://www.flickr.com">Flickr</a>.</p>

<p>Then, in the second step, they identify and control for a covariate called the
“selection frequency.” Selection frequency is a measure of how frequently a
(time-limited) human decides that an image (with its candidate label) is
“correctly labeled.” We can get estimate selection frequencies by asking crowd
workers questions like:</p>

<p><br/>
<!-- PIPELINE DIAGRAM --></p>

  <img src="https://gradientscience.org/assets/idbias/selfreq.png"/>

<p><br/></p>

<p>Selection frequency is a very reasonable (and in some sense, the
“right”) covariate to control for, since the original ImageNet dataset 
was filtered using a similar measureTo construct ImageNet, the
authors scraped many candidate image-label pairs, and just as above, asked
crowd workers whether the image corresponded to the label---filtering was done
via a convincing-majority vote procedure, making the selection frequency a
relevant metric.</p>

<p><a href="https://arxiv.org/abs/1902.10811">Recht et al.</a> obtain empirical selection
frequency estimates for the ImageNet validation set and their collected
candidate pool by presentingThe figure above is simplified for
clarity—in reality, datapoints are presented to the crowd workers as
groups of 48 images, all corresponding to the same candidate label, and
annotators are asked to select the images in that group that truly correspond to
the
label. each (image, label) pair to 10 crowd workers.</p>

<p>By filtering on the selection frequency in various ways, the authors come up
with three new <code class="language-plaintext highlighter-rouge">ImageNet-v2</code> datasets. One of these, called
<code class="language-plaintext highlighter-rouge">MatchedFrequency</code>, is a “true” dataset replication in the sense that it tries
to control for the selecion frequency by filtering the candidate images and
labels so that the resulting selection frequency distribution matches that of
the original ImageNet validation set. 
Since our focus is on dataset
replication, from now on we’ll ignore the other datasets, and use
“MatchedFrequency” and “ImageNet-v2” interchangeably. For convenience, we’ll
also use “ImageNet” and “ImageNet-v1” interchangeably to refer to the original
ImageNet validation setSince the test set is not released, the
ImageNet validation set usually acts as a de facto test set, and the two terms
(validation and test) are used interchangeably to refer to the validation
set..</p>

<p>The key observation made by the ImageNet-v2 authors—and the one that we’ll
focus on in this post—is a <em>consistent</em> drop in accuracy that models suffer
when evaluated on ImageNet-v2 instead of ImageNet:</p>

<p><br/></p>
<canvas height="200" id="v1v2_orig" width="400"/>
<p><br/></p>

<p>Each dot in the scatter plot above represents a model—the $x$ coordinate is
the model’s ImageNet accuracy, and the $y$ coordinate is the model’s accuracy
when evaluated on ImageNet-v2. (You can mouse over the dots to see
model names!) Ideally, since the generating pipelines for the two datasets are
similar, and the relevant covariates were matched, one would expect all of the
models to fall on the dotted line $y = x$. Yet, across the examined models,
their accuracies dropped by an average of 11.7% between ImageNet and
ImageNet-v2.</p>

<h2 id="our-findings">Our Findings</h2>
<p>The significant accuracy drop above presents an empirical mystery given the
similarity in data pipeline between the two datasets. Why do classifiers perform
so poorly on ImageNet-v2?</p>

<p>In our <a href="https://arxiv.org/abs/2005.09619">work</a>, we identify an aspect of the dataset reproduction process
that might lead to a significant accuracy drop. The general phenomenon we
identify is that in dataset replication, even mean-zero noise in measurements of
the relevant covariate/control variable can result in significant bias in the
resulting matched dataset if not accounted for.</p>

<p>In the case of ImageNet-v2, we show how noisy readings of the selection
frequency statistic can result in bias in the ImageNet-v2 dataset towards lower
selection frequency and consequently, lower accuracyRecht et al. observed previously that changes in selection frequency affect model performance..</p>

<p>After accounting for this bias and thus controlling for selection
frequency, we estimate that the adjusted ImageNet to ImageNet-v2
accuracy gap is less than or equal to 3.6% (instead of the initially observed
11.7%).</p>

<p>In the next few sections, we’ll focus on ImageNet-v2, first trying to get a
clearer picture of the source of statistical bias in this dataset replication
effort, and then discussing ways to correct for it.</p>

<h3 id="identifying-the-bias">Identifying the bias</h3>

<p>Earlier, we decomposed the data replication process into two steps: (1)
replicating the pipeline, and (2) controlling for covariates via statistic
matching. 
To perform the latter of these two steps, one
first needs to find the distribution of the relevant statistic (e.g., selection
frequency, in the case of ImageNet) for both the original test set as well as
the newly collected data. 
Just as with any empirical statistic, however, we can’t 
read the true selection frequency $s(x)$ for any given image $x$. We can only sample from a binomial random variable,</p>

<p>\[
    \widehat{s}(x) = \frac{1}{n}\text{Binomial}(n, s(x)),
\]</p>

<p>to obtain a selection frequency estimate, i.e., where $n$
is the number of crowd workers used.</p>

<p>Now, even though the expected value of the measurement we get is indeed $s(x)$
(i.e., $\widehat{s}(x)$ is an unbiased estimator of $s(x)$), 
the reading itselfThis is not the same as the underlying selection frequency for an image; for example, a
image-label pair $x$ might have a true selection frequency of $s(x) = 0.645$,
meaning that on average a crowd annotator is 64.5% likely to say that the image
corresponds to the label. If we ask 10 crowd annotators to label the pair,
however, we never observe this 64.5% number. will be $k/n$ for some integer $k$ (with $n=10$, in the
case of the ImageNet-v2 replication process).</p>

<p>To see why this seemingly innocuous fact impacts the dataset replication
process, suppose we knew exactly what the distribution of true selection
frequency looked like for both the original ImageNet test set and the candidate
images collected through the ImageNet-v2 replication pipeline:</p>

<p><br/></p>
<canvas id="priorCtx"/>
<p><br/></p>

<p>Suppose now that we estimate a selection frequency of $\widehat{s}(x) = 0.7$ for
a given image $x$ (i.e., seven of the ten workers who were shown the image and
its candidate label, marked it as correctly labeled). The key observation here
is that given this empirical selection frequency, the most likely value for
$s(x)$ is not $\widehat{s}(x)$ (even though $\widehat{s}(x)$ is an unbiased
estimator of $s(x)$). Instead, the maximum likelihood estimate actually depends
on which dataset $x$ is from!</p>

<p>If $x$ was sourced from ImageNet, it’s more likely that $s(x) &gt; 0.7$ and
therefore that $\widehat{s}(x)$ is an underestimate, since most of the mass of
the ImageNet selection frequency distribution—see our (hypothetical) plot
above—is on $s(x) &gt; 0.7$. Conversely, if $x$ was a newly collected candidate
image, then most likely $s(x) &lt; 0.7$.</p>

<div class="footnote">
<strong>Remark:</strong> Mathematically, the phenomenon is that
$E[\widehat{s}(x)|s(x)] = s(x)$, but $E[s(x)|\widehat{s}(x)]$ is not equal to $s(x)$, and
instead depends on the distribution of $s(x)$ via Bayes'
ruleSpecifically, we have that for a given dataset,
$$p(s|\widehat{s}) = \frac{p_{data}(s) \cdot p_{binom}(\widehat{s}|s)} {\int
p(s') p_{binom}(\widehat{s}|s')\ ds'},$$ where $p_{data}$ is the density function
for selection frequencies under this dataset, and $p_{binom}$ is the binomial
probability mass function..
</div>

<p>The interactive graph below visualizes this intuition: by moving the
slider, you can adjust the “observed” selection frequency $\widehat{s}(x)$. The 
shaded curves then show what our belief about the corresponding true
selection frequency $s(x)$ looks like, depending on which pool the image
came from.</p>

<p><br/></p>
<canvas id="posteriorCtx"/>
<div class="slider">
    <label for="updateObs">Observed selection frequency <br/> (Current:
    <span id="updateObsVal"/>/20)</label>
    <div>
    <span class="inplabel">Hard (0/20)</span>
    <input id="updateObs" max="20" min="0" step="1" type="range" value="14"/>
    <span class="inplabel">Easy (20/20)</span>
    </div>
</div>
<p><br/></p>

<p>Thus, if the selection frequency distribution of ImageNet is skewed upwards
compared to that of the collected candidate images (which is likely, since
ImageNet was already filtered for qualityImageNet was
originally constructed by collecting candidate images in a way similar to the
ones described here, and then subsequently filtering for quality using a
"majority vote" based on selection frequency. when it was
constructed), matching the two data sources via observed selection frequency
will result in ImageNet images having systematically higher selection
frequencies than their new ImageNet-v2 counterparts. Also, the noiser our
reading of observed selection frequency are (i.e. the less annotators we use),
the more important the source distribution becomes, and so the greater the
effect of the bias. Indeed, if we were able to use infinite annotators for each
image, the bias would disappear, as we would have perfect readings of the
corresponding selection frequency. The only possible value for $s(x)$ would then
be $s(x) = \widehat{s}(x)$, making the shaded distributions above collapse
into a point mass at the green line.</p>

<p>The bias we are describing is summarized visually in the interactive graph below: by adjusting the
sliders, you can manipulate the distributions of “true” selection frequency for
ImageNet (red) and for the candidate data (black), as well as the number of
annotators used to estimate selection frequencies. The distribution of
ImageNet-v2 selection frequencies resulting from performing statistic matching
is shown in blue. Notice that as long as the candidate selection frequencies do
not come from the same distribution as the ImageNet selection frequencies,
the resulting ImageNet-v2 test set never matches the original test set
statistics. For the reasons we discussed earlier, having less annotators, or having a bigger gap between ImageNet and the candidate
distribution, also exacerbate the effect.</p>

<!-- CHART BELOW -->
<div class="chartholder">
<div class="chartdata">Mean IN/IN-v2 difference: <span id="gap"/>% </div>
<canvas height="200" id="myChart" width="400"/>
</div>
<div class="caption">
An interactive graph depicting the source of bias in the ImageNet-v2
generation process. Interact with the sliders below to change the
"easiness" (selection frequency) of the ImageNet-v1 and candidate image datasources,
and the number of annotators used to measure selection frequency (click the
legend to show/hide lines).
</div>
<div class="slider">
    <label for="updateV1">ImageNet selection frequency</label>
    <div>
    <span class="inplabel">Hard</span>
    <input id="updateV1" max="9" min="2" step="0.5" type="range" value="8"/>
    <span class="inplabel">Easy</span>
    </div>
</div>
<div class="slider">
    <label for="updateFlickr">Candidates selection frequency</label>
    <div>
        <span class="inplabel">Hard</span>
        <input id="updateFlickr" max="9" min="2" step="0.5" type="range" value="3"/> 
        <span class="inplabel">Easy</span>
    </div>
</div>
<div class="slider">
    <label for="updateNumWorkers">Number of workers</label>
    <div> 
    <span class="inplabel">1</span>
    <input id="updateNumWorkers" max="100" min="1" type="range"/> 
    <span class="inplabel">100</span>
    </div>
</div>
<div class="slider">
    <button> Reset to typical values </button>
    <button> Show / hide <em>s̑</em> distributions</button>
</div>
<!--- END CHART -->

<h2 id="quantifying-the-effects-of-bias">Quantifying the Effects of Bias</h2>

<p>The model above, paired with the fact that ImageNet (having already been
filtered for quality before) is likely much higher-quality than candidate images sourced from
Flickr, predicts that ImageNet-v2 images’ selection frequencies are
consistently lower than those of ImageNet images. To test this
theory, we set up another crowdsourced task that is extremely similar (but
not quite identicalWe implemented a few changes for quality control but kept
the task instructions and interface constant: the exact differences are outlined in Appendix
B.2 of our paper.) to the one used by the ImageNet-v2
creators, this time using 40 annotators per image (instead of 10) to estimate
its selection frequency. A histogram of the selection frequencies we 
observed in our experiment is shown below:</p>

<p><br/></p>

  <img src="https://gradientscience.org/assets/idbias/selfreq_histogram.png"/>

<p><br/></p>

<p>Even though the ImageNet-v2 creators report average selection
frequencies for ImageNet and ImageNet-v2 of 0.71 and 0.73 respectively, our
new experiments yield average selection frequencies of 0.85 and 0.81; note the change
in relative ordering (why are the selection frequencies
higher?We discuss this in depth in Appendix B.2 of our
paper. The task and instructions are the same, so we hypothesize that the
discrepancy boils down to either (a) data quality: we used <a href="https://blog.mturk.com/tutorial-managing-worker-cohorts-with-qualifications-e928cd30b173">worker
qualifications</a> to ensure annotator quality while the original experiment did
not—qualifications have been shown to reduce the share of low-quality or
inattentive crowd workers from 34% to 2% in other studies (e.g., the
study we reference found that 16%/0.4% of workers without/with qualifications reported
having had a <em>fatal</em> heart attack while watching television); or (b) data makeup:
workers are presented with grids of 48 images at a time in both experiments,
with grids containing a mix of ImageNet, ImageNet-v2, and candidate
images—but the exact proportions of this mix differ between the two
experiments.).</p>

<div class="footnote"><strong>Aside</strong>: Why did we need to run a new
crowdsourced study to observe this gap, instead of using the data already
collected for the ImageNet-v2 study? The answer is finite-sample reuse:
the selection frequencies collected in the original study are
precisely the ones used to filter the ImageNet-v2 dataset. So, by
construction, these selection frequency will match the selection
frequencies of the ones of ImageNet test set, regardless of whether
there is bias in the selection processTo draw a crude analogy here,
suppose that instead of matching image datasets we are matching piles of coins: 
Pile A is rigged $P(\text{heads})=1$, but Pile B is fair $P(\text{heads}) =
0.5$. We flip all the coins in both piles 10 times each---inevitably (if
there are enough total coins in Pile B), some of the Pile B coins will land
"heads" all 10 times, and will thus appear identical to the rigged Pile A
coins. Are they in fact identical? (After all, these coins match the Pile A
coins according to the "number of heads" statistic!) The answer is obviously
"no," but the key is that even though all the coins in Pile B are
fair (and flipping them another 10 times would reveal this), it's impossible to
conclude anything other than $P(\text{heads}) = 1$ solely from the
already-collected data on the selected coins.. If we're
careful about avoiding this finite-sample reuse (for example, by
re-performing the filtering process using half of the annotators and then
measuring selection frequencies with the other half) we can actually identify
bias in the original data—the process for doing so
is shown in Appendix C of <a href="https://arxiv.org/abs/2005.09619">our paper</a>.</div>

<h3 id="how-does-this-bias-affect-measured-accuracy">How does this bias affect measured accuracy?</h3>

<p>Our model and experiments suggest that matching empirical statistics from
different sources introduces bias into the dataset replication pipeline, and
that in the case of ImageNet-v2 this means that selection frequencies are
actually lower for the new ImageNet-v2 test set compared to the old one. Since
selection frequency is meant to roughly reflect data quality, and is known (as
found already by the ImageNet-v2 authors) to affect model accuracy, we expect the
downwards selection frequency bias in ImageNet-v2 to directly translate into a downwards
bias in model accuracy.</p>

<p>To test if this is really the case, we use a progressively increasing number of
annotators $n$ out of the 40 that we collected. For each $n$, we matchWe perform this matching via reweighting, rather than
filtering---more details are given in the next section. the
ImageNet-v2 observed selection frequencies (calculated using $n$ annotators) to
the ImageNet onesFor context, the ImageNet-v2 creators
matched their candidate pool to ImageNet with $n = 10$, and measure the
resulting model accuracies.
Our statistical model predicts that more annotators means less noise in the
observed selection frequencies, which in turn means less bias and higher model accuracies
in the original selection frequency used to create ImageNet-v2, and so we
should see the resulting model accuracies
increase. The data confirms this prediction: below we plot model accuracies on
ImageNet versus their adjusted accuracies on ImageNet-v2—using the slider
below the graph, you can vary the number of annotators used to make the
adjustment from zero (i.e. no matching, just raw ImageNet-v2 accuracies) to 40
(accuracies after statistic matching using all 40 annotations).</p>

<p><br/></p>
<div class="chartholder">
    <canvas id="v1v2_varying"/>
</div>
<div class="slider">
    <label for="updateNumWorkersAdjAcc">Number of annotators <br/> 
    (Current: <span id="updateNumWorkersAdjAccVal">0</span>)</label>
    <div> 
    <span class="inplabel">1</span>
    <input id="updateNumWorkersAdjAcc" max="40" min="0" step="5" type="range" value="0"/> 
    <span class="inplabel">40</span>
    </div>
</div>
<p><br/></p>

<p>After using 40 workers to control for selection frequency
between ImageNet and ImageNet-v2, we reduce the 11.7% gap that was originally observed
to a gap of 5.7%. This is already a significant reduction, but the trend of the
graph suggests that 5.7% is still an overestimate—the gap continues to
consistently shrink with each increase in number annotators. In the final part
of this post, we’ll use a technique from classical statistics to get
an even better estimate of the real, bias-adjusted gap between ImageNet and
ImageNet-v2 model accuracies.</p>

<h2 id="adjusting-for-bias-with-the-statistical-jackknife">Adjusting for Bias with the Statistical Jackknife</h2>

<p>The statistic matching that led to the previous graph was based on what we’ll call a
<em>selection frequency-adjusted accuracy</em> estimator, defined for a given
classifier $f$ as:</p>

<p>\[
    \text{Acc}(n) = \sum_{k=1}^n 
    E_{x\sim \text{ImageNet-v2}}\left(1[\text{$f$ is correct on }x] | \widehat{s}(x) = \frac{k}{n}\right)\cdot 
    P_{x\sim \text{ImageNet}}\left(\widehat{s}(x) = \frac{k}{n}\right)
\]</p>

<p>This estimator has a simple interpretation: it is equivalent to
(a) sampling an ImageNet-v1 image and observing its (empirical) selection frequency;
then (b) finding a random ImageNet-v2 image with the same (empirical) selection
frequency, and recording the classifier’s correctness on that input. So what we are
estimating here is what a model’s accuracy on ImageNet-v2 would be, if the
selection frequencies of ImageNet-v2 were distributed as in the ImageNet test
set. (Notice that if the ImageNet and ImageNet-v2 selection frequency
distributions already matched, then this estimator would be independent from $n$
and would evaluate to exactly model accuracy on ImageNet-v2.)</p>

<p>Now, the $\text{Acc}(n)$ estimator is subject to the same bias as dataset
replication itself, as it too ignores the discrepancy between the empirical
selection frequency $\widehat{s}(x)$ and the true selection frequency $s(x)$.
Since we’ve been talking about bias pretty abstractly in this post, it’s worth
noting that the $\text{Acc}(n)$ estimator ties everything back to the formal,
statistical definition of bias. Specifically, our main finding can be restated
(though maybe less intuitively) as “$\text{Acc}(n)$ is a downwards-biased
estimator of the true reweighted accuracy,” that is</p>

<p>\[
    E[\text{Acc}(n)] &lt; \lim_{n\rightarrow\infty} \text{Acc}(n). 
\]</p>

<p>So, from this perspective, our graph in the previous section can be viewed as just a plot of the value of $\text{Acc}(n)$ for every
classifier for various values of $n$. Also, the estimator behaves exactly how as predicted by
our model of the bias—as $n$ increases, $\widehat{s}(x)$ becomes a less
noisy estimator of $s(x)$, so the bias in the matching process decreases and
so $\text{Acc}(n)$ increases.</p>

<p>Now, what we really want to know is what $\text{Acc}(n)$ looks like as $n
\rightarrow \infty$, especially given that even when we use 40 annotators for
statistic matching the adjusted accuracy still improves.</p>

<p>In <a href="https://arxiv.org/abs/2005.09619">our paper</a> we present further techniques for tackling this problem.
These include making use of tools from empirical Bayes estimation techniques,
beta-binomial regression, and kernel density estimation. To keep things short
here, we’ll only discuss the simplest estimation method we use: one based on a
technique known as the statistical jackknife.</p>

<p>The jackknife dates back to the work of <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1015.9344&amp;rep=rep1&amp;type=pdf">Maurice Quenouille</a> and <a href="https://projecteuclid.org/euclid.aoms/1177706647">John
Tukey</a> in the 1950s, and provides a way to estimate the bias of anyTechnically, certain
mild assumptions, such as the estimator being (statistically) consistent and having
bias that is analytic in $1/n$, are needed. statistical estimator. In
short, the jackknife estimate of an $n$-sample estimator
$\Theta_n(X_1,\ldots,X_n)$ is given by</p>

<p>\[
    b_{jack}(\widehat{\theta}_n) 
    = (n-1)\cdot \left(\frac{1}{n}\sum_{i=1} 
    \widehat{\theta}_{n-1}^{(i)} - \widehat{\theta}_n 
    \right),
\]</p>

<p>\[
    \text{where}\qquad \widehat{\theta}_{n-1}^{(i)} =
    \widehat{\theta}(X_1,\ldots,X_{i-1},X_{i+1},\ldots,X_n) \text{ is the $i$th
    leave-one-out estimate.}
\]</p>

<section class="container">
<div>
<div class="checkboxdiv">
<input id="ac-1" name="accordion-1" type="checkbox"/>
<label for="ac-1"><span class="fas fa-chevron-right" id="titlespan"/>  How does the jackknife work? (click to expand)</label>
<article class="small">
A brief summary: consider our $n$-sample estimator $\Theta_n$, and define
$\Theta$ to be the true value of the estimator, i.e. $\lim_{n\rightarrow\infty}
\Theta_n$. In our case, $\Theta$ would be the adjusted accuracy of ImageNet-v2
if we had infinite workers. Now, suppose that the bias in $\Theta_n$ is on
the order of $1/n$, i.e.

\[
    E[\Theta_n] = \Theta + \frac{b(\Theta)}{n}
\]

Now, for a given n-sample estimate $\widehat{\Theta}_n$, we define a
<em>leave-one-our</em> estimator $\widehat{\Theta}_n^{(-i)}$ to be $\widehat{\Theta}_n$ 
computed with all but the $i$th datapoint. There are $n$ possible such leave-one-out 
estimators for a given $\widehat{\Theta}_n$. The main idea behind the jackknife
is to make the following two approximations:

\[
    \widehat{\Theta}_n \approx E[\Theta_n] = \Theta + \frac{b(\Theta)}{n}
\]
\[
    \frac{1}{n} \sum_{i=1} \widehat{\Theta}_n^{(-i)} \approx E[\Theta_{n-1}] = \Theta + \frac{b(\Theta)}{n-1}
\]

Using these two assumptions, one can solve for $b(\Theta)$ and end up with the
bias estimate given in the post.
</article>
</div>
</div>
</section>

<p>Averaging across all of the classifiers studied, the jackknife estimates the
bias in our estimator as about 1.0%, meaning that the bias-corrected gap
between ImageNet and ImageNet-v2 shrinks from 11.7% (without any correction) to
5.7% (using the 40 annotators we have to correct), to 4.7% (using this additional
jackknife bias correction). Moreover, as our paper discusses, this is almost
certainly still an overestimate—using more refined methods for bias estimation
reduces the gap to somewhere between 3.4% and 3.8% (with variation being across
different methods).</p>

<h2 id="summary-and-conclusions">Summary and Conclusions</h2>
<p>We find that noise—even if it is mean-zero—can result in bias in dataset reproductions if not
accounted for. Zooming in on the <code class="language-plaintext highlighter-rouge">ImageNet-v2</code> replication effort, we use
statistical modeling to find that majority of the observed accuracy drop can be
explained by differences in selection frequency distribution between the
original dataset and its replication. Our results suggest that statistic modeling
of the data collection process can be an important tool in data replication efforts.
For more details check out <a href="https://arxiv.org/abs/2005.09619">our paper</a>, where we discuss precise modeling
procedures, further experiments, and future directions.</p></div>
    </summary>
    <updated>2020-05-19T00:00:00Z</updated>
    <published>2020-05-19T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2020-05-25T19:20:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/05/18/siam-symposium-on-simplicity-in-algorithms/</id>
    <link href="https://cstheory-events.org/2020/05/18/siam-symposium-on-simplicity-in-algorithms/" rel="alternate" type="text/html"/>
    <title>SIAM Symposium on Simplicity in Algorithms</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">January 11-12, 2021 Westin Alexandria Old Town, Alexandria, Virginia, U.S. https://www.siam.org/conferences/cm/conference/sosa21 Submission deadline: August 12, 2020 Registration deadline: December 7, 2020 Symposium on Simplicity in Algorithms is a conference in theoretical computer science dedicated to advancing algorithms research by promoting simplicity and elegance in the design and analysis of algorithms. The benefits of simplicity are … <a class="more-link" href="https://cstheory-events.org/2020/05/18/siam-symposium-on-simplicity-in-algorithms/">Continue reading <span class="screen-reader-text">SIAM Symposium on Simplicity in Algorithms</span></a></div>
    </summary>
    <updated>2020-05-18T09:15:31Z</updated>
    <published>2020-05-18T09:15:31Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-05-25T21:21:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/076</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/076" rel="alternate" type="text/html"/>
    <title>TR20-076 |  The Round Complexity of Perfect MPC with Active Security and Optimal Resiliency | 

	Benny Applebaum, 

	Eliran Kachlon, 

	Arpita Patra</title>
    <summary>In STOC 1988, Ben-Or, Goldwasser, and Wigderson (BGW) established an important milestone in the fields of cryptography and distributed computing by showing that every functionality can be computed with perfect (information-theoretic and error-free) security at the presence of an active (aka Byzantine) rushing adversary that controls up to $n/3$ of the parties.

We study the round complexity of general secure multiparty computation in the BGW model. Our main result shows that every functionality can be realized in only four rounds of interaction, and that some functionalities cannot be computed in three rounds. This completely settles the round-complexity of perfect actively-secure optimally-resilient MPC, resolving a long line of research.

Our lower-bound is based on a novel round-reduction technique that allows us to lift existing three-round  lower-bounds for verifiable secret sharing to four-round lower-bounds for general MPC. To prove the upper-bound, we develop new round-efficient protocols for computing degree-2 functionalities over large fields, and establish the completeness of such functionalities. The latter result extends the recent completeness theorem of Applebaum, Brakerski and Tsabary (TCC 2018, Eurocrypt 2019) that was limited to the binary field.</summary>
    <updated>2020-05-17T16:55:29Z</updated>
    <published>2020-05-17T16:55:29Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-25T21:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7732</id>
    <link href="https://windowsontheory.org/2020/05/17/foundations-of-responsible-computing/" rel="alternate" type="text/html"/>
    <title>Foundations of responsible computing</title>
    <summary>[Hat tip: Aaron Roth] The inaugural conference on the foundations of responsible computing will take place in less than two weeks (June 1-2). Registration is FREE but you need to register by May 28. The program looks fantastic, and includes keynotes by Adrian Weller, Rakesh Vohra, Patricia Williams, and Jon Kleinberg, as well as a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>[Hat tip: Aaron Roth]</p>



<p>The inaugural <a href="https://responsiblecomputing.org/">conference on the foundations of responsible computing</a> will take place in less than two weeks (June 1-2). Registration is FREE but you need to register by May 28.</p>



<p/>



<p>The <a href="https://responsiblecomputing.org/program/">program</a> looks fantastic, and includes keynotes by <strong>Adrian Weller</strong>, <strong>Rakesh Vohra</strong>, <strong>Patricia Williams</strong>, and <strong>Jon Kleinberg</strong>,  as well as a set of (very interesting, judging by the titles) contributed talks.</p>



<p/>



<p/>



<p/></div>
    </content>
    <updated>2020-05-17T13:21:41Z</updated>
    <published>2020-05-17T13:21:41Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-05-25T21:21:03Z</updated>
    </source>
  </entry>
</feed>
