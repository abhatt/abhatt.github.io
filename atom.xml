<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-02-15T15:22:16Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=16854</id>
    <link href="https://gilkalai.wordpress.com/2019/02/15/henry-cohn-abhinav-kumar-stephen-d-miller-danylo-radchenko-and-maryna-viazovska-universal-optimality-of-the-e8-and-leech-lattices-and-interpolation-formulas/" rel="alternate" type="text/html"/>
    <title>Henry Cohn, Abhinav Kumar, Stephen D. Miller, Danylo Radchenko, and Maryna Viazovska: Universal optimality of the E8 and Leech lattices and interpolation formulas</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Henry Cohn A follow up paper on the tight bounds for sphere packings in eight and 24 dimensions. (Thanks, again, Steve, for letting me know.) For the 2016 breakthroughs see this post, this post of John Baez, this article by Erica Klarreich on … <a href="https://gilkalai.wordpress.com/2019/02/15/henry-cohn-abhinav-kumar-stephen-d-miller-danylo-radchenko-and-maryna-viazovska-universal-optimality-of-the-e8-and-leech-lattices-and-interpolation-formulas/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2019/02/hlc.jpg"><img alt="" class="alignnone size-full wp-image-16863" height="641" src="https://gilkalai.files.wordpress.com/2019/02/hlc.jpg?w=640&amp;h=641" width="640"/></a></p>
<p><span style="color: #ff0000;"><strong>Henry Cohn</strong></span></p>
<p>A follow up paper on the tight bounds for sphere packings in eight and 24 dimensions. (Thanks, again, Steve, for letting me know.)</p>
<p>For the 2016 breakthroughs see <a href="https://gilkalai.wordpress.com/2016/03/23/a-breakthrough-by-maryna-viazovska-lead-to-the-long-awaited-solutions-for-the-densest-packing-problem-in-dimensions-8-and-24/">this post</a>,<a href="https://golem.ph.utexas.edu/category/2016/03/e8_is_the_best.html"> this post of John Baez</a>, this <a href="https://www.quantamagazine.org/20160330-sphere-packing-solved-in-higher-dimensions/">article by Erica Klarreich on Quanta Magazine, </a>and a Notices AMS article by  Henry Cohn   <a href="http://www.ams.org/publications/journals/notices/201702/rnoti-p102.pdf" rel="nofollow">A conceptual breakthrough in sphere packing</a>. See also, Henry Cohn’s 2010 paper <a href="https://arxiv.org/abs/1003.3053">Order and disorder in energy minimization</a>, and <a href="https://www.youtube.com/watch?v=8y-uqcyRZ1M">Maryna Viazovska’s ICM 2018 videotaped lecture.</a></p>
<h3>Henry Cohn, Abhinav Kumar, Stephen D. Miller, Danylo Radchenko, and Maryna Viazovska: <a href="https://arxiv.org/abs/1902.05438">Universal optimality of the E8 and Leech lattices and interpolation formulas</a></h3>
<p><strong>Abstract: </strong>We prove that the <img alt="E_8" class="latex" src="https://s0.wp.com/latex.php?latex=E_8&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_8"/> root lattice and the Leech lattice are universally optimal among point configurations in Euclidean spaces of dimensions 8 and 24, respectively. In other words, they minimize energy for every potential function that is a completely monotonic function of squared distance (for example, inverse power laws or Gaussians), which is a strong form of robustness not previously known for any configuration in more than one dimension. This theorem implies their recently shown optimality as sphere packings, and broadly generalizes it to allow for long-range interactions.</p>
<p>The proof uses sharp linear programming bounds for energy. To construct the optimal auxiliary functions used to attain these bounds, we prove a new interpolation theorem, which is of independent interest. It reconstructs a radial Schwartz function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f"/> from the values and radial derivatives of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f"/> and its Fourier transform <img alt="\hat f" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat+f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\hat f"/> at the radii √2π for integers <em>n ≥ 1</em> in <img alt="R^8" class="latex" src="https://s0.wp.com/latex.php?latex=R%5E8&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R^8"/> and <em>n ≥ 2</em> in <img alt="R^{24}" class="latex" src="https://s0.wp.com/latex.php?latex=R%5E%7B24%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R^{24}"/>. To prove this theorem, we construct an interpolation basis using integral transforms of quasimodular forms, generalizing Viazovska’s work on sphere packing and placing it in the context of a more conceptual theory.</p></div>
    </content>
    <updated>2019-02-15T07:17:39Z</updated>
    <published>2019-02-15T07:17:39Z</published>
    <category term="Algebra and Number Theory"/>
    <category term="Combinatorics"/>
    <category term="Geometry"/>
    <category term="Abhinav Kumar"/>
    <category term="Danylo Radchenko"/>
    <category term="Henry Cohn"/>
    <category term="Maryna Viazovska"/>
    <category term="Sphere packing"/>
    <category term="sphere packing in 24 dimensions"/>
    <category term="sphere packing in eight dimensions"/>
    <category term="Stephen D. Miller"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-02-15T15:20:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=336</id>
    <link href="https://tcsplus.wordpress.com/2019/02/14/tcs-talk-wednesday-february-20th-sepehr-assadi-princeton/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, February 20th, Sepehr Assadi, Princeton</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, February 20th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Sepehr Assadi from Princeton University will speak about “A Simple Sublinear-Time Algorithm for Counting Arbitrary Subgraphs via Edge Sampling” (abstract below). Please make sure you reserve a spot […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, February 20th at<br/>
1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European<br/>
Time, 18:00 UTC). <strong>Sepehr Assadi</strong> from Princeton University will speak about “<em>A Simple Sublinear-Time Algorithm for Counting Arbitrary Subgraphs via Edge Sampling</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: In the subgraph counting problem, we are given a (large) graph <img alt="G(V, E)" class="latex" src="https://s0.wp.com/latex.php?latex=G%28V%2C+E%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="G(V, E)"/> and a (small) graph <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/> (e.g., a triangle), and the goal is to estimate the number of occurrences of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/> in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=fff&amp;fg=444444&amp;s=0" title="G"/>. Our focus in this talk is on designing sublinear-time algorithms for approximately computing number of occurrences of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/> in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=fff&amp;fg=444444&amp;s=0" title="G"/> in the setting where the algorithm is given query access to <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=fff&amp;fg=444444&amp;s=0" title="G"/>. This problem has been studied in several recent work which primarily focused on specific families of graphs H such as triangles, cliques, and stars. However, not much is known about approximate counting of arbitrary graphs <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/> in the literature. This is in sharp contrast to the closely related subgraph enumeration problem in which the goal is to list all copies of the subgraph <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/> in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=fff&amp;fg=444444&amp;s=0" title="G"/>. The AGM bound shows that the maximum number of occurrences of any arbitrary subgraph <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/> in a graph <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=fff&amp;fg=444444&amp;s=0" title="G"/> with <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=fff&amp;fg=444444&amp;s=0" title="m"/> edges is <img alt="O(m^{p(H)})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28m%5E%7Bp%28H%29%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(m^{p(H)})"/>, where <img alt="p(H)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28H%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="p(H)"/> is the fractional edge cover number of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/>, and enumeration algorithms with matching runtime are known for every <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/>.</p>
<p>In this talk, we bridge this gap between the subgraph counting and subgraph enumeration problems and present a simple sublinear-time algorithm that estimates the number of occurrences of any arbitrary graph <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/> in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=fff&amp;fg=444444&amp;s=0" title="G"/>, denoted by <img alt="\#H" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%23H&amp;bg=fff&amp;fg=444444&amp;s=0" title="\#H"/>, to within a <img alt="(1 \pm \varepsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=%281+%5Cpm+%5Cvarepsilon%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="(1 \pm \varepsilon)"/>-approximation factor with high probability in <img alt="O(m^{p(H)} /\#H)\cdot \text{poly}(\log n,1/\varepsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28m%5E%7Bp%28H%29%7D+%2F%5C%23H%29%5Ccdot+%5Ctext%7Bpoly%7D%28%5Clog+n%2C1%2F%5Cvarepsilon%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(m^{p(H)} /\#H)\cdot \text{poly}(\log n,1/\varepsilon)"/> time. Our algorithm is allowed the standard set of queries for general graphs, namely degree queries, pair queries and neighbor queries, plus an additional edge-sample query that returns an edge chosen uniformly at random. The performance of our algorithm matches those of Eden et al. [FOCS 2015, STOC 2018] for counting triangles and cliques and extend them to all choices of subgraph <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/> under the additional assumption of edge-sample queries. Our results are also applicable to the more general problem of database join size estimation problem and for this slightly more general problem achieve optimal bounds for every choice of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0" title="H"/>.</p>
<p>Joint work with Michael Kapralov and Sanjeev Khanna.</p></blockquote></div>
    </content>
    <updated>2019-02-15T00:03:08Z</updated>
    <published>2019-02-15T00:03:08Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-02-15T15:21:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.05529</id>
    <link href="http://arxiv.org/abs/1902.05529" rel="alternate" type="text/html"/>
    <title>Parameterized Fine-Grained Reductions</title>
    <feedworld_mtime>1550188800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Elli Anastasiadi, Antonis Antonopoulos, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pagourtzis:Aris.html">Aris Pagourtzis</a>, Stavros Petsalakis <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.05529">PDF</a><br/><b>Abstract: </b>During recent years the field of fine-grained complexity has bloomed to
produce a plethora of results, with both applied and theoretical impact on the
computer science community. The cornerstone of the framework is the notion of
fine-grained reductions, which correlate the exact complexities of problems
such that improvements in their running times or hardness results are carried
over. We provide a parameterized viewpoint of these reductions (PFGR) in order
to further analyze the structure of improvable problems and set the foundations
of a unified methodology for extending algorithmic results. In this context, we
define a class of problems (FPI) that admit fixed-parameter improvements on
their running time. As an application of this framework we present a truly
sub-quadratic fixed-parameter algorithm for the orthogonal vectors problem.
Finally, we provide a circuit characterization for FPI to further solidify the
notion of improvement.
</p></div>
    </summary>
    <updated>2019-02-15T02:20:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.05487</id>
    <link href="http://arxiv.org/abs/1902.05487" rel="alternate" type="text/html"/>
    <title>Complexity-Theoretic Aspects of Expanding Cellular Automata</title>
    <feedworld_mtime>1550188800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Modanese:Augusto.html">Augusto Modanese</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.05487">PDF</a><br/><b>Abstract: </b>The expanding cellular automata (XCA) variant of cellular automata is
investigated and characterized from a complexity-theoretical standpoint. The
respective polynomial-time complexity class is shown to coincide with
${\le_{tt}^p}(\textbf{NP})$, that is, the class of decision problems
polynomial-time truth-table reducible to problems in $\textbf{NP}$. Corollaries
on select XCA variants are proven: XCAs with multiple accept and reject states
are shown to be polynomial-time equivalent to the original XCA model.
Meanwhile, XCAs with diverse acceptance behavior are classified in terms of
${\le_{tt}^p}(\textbf{NP})$ and the Turing machine polynomial-time class
$\textbf{P}$.
</p></div>
    </summary>
    <updated>2019-02-15T02:21:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.05479</id>
    <link href="http://arxiv.org/abs/1902.05479" rel="alternate" type="text/html"/>
    <title>Which is the least complex explanation? Abduction and complexity</title>
    <feedworld_mtime>1550188800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soler=Toscano:Fernando.html">Fernando Soler-Toscano</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.05479">PDF</a><br/><b>Abstract: </b>It may happen that for a certain abductive problem there are several possible
explanations, not all of them mutually compatible. What explanation is selected
and which criteria are used to select it? This is the well-known problem of the
selection of abductive hypotheses. Are there criteria that can help us to
select the simplest explanation in a broad spectrum of abductive problems? To
give an (affirmative) answer to this question we will move to a field in
theoretical computer science: Algorithmic Information Theory (AIT). The
algorithmic complexity measure K(s) can be used to determine which is the best
theory within those explaining a set of observations. We introduce an
application of K(s) to the selection of the best abductive explanation, in the
context of dynamic epistemic logic (DEL).
</p></div>
    </summary>
    <updated>2019-02-15T02:20:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.05432</id>
    <link href="http://arxiv.org/abs/1902.05432" rel="alternate" type="text/html"/>
    <title>Search and Rescue in the Face of Uncertain Threats</title>
    <feedworld_mtime>1550188800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lidbetter:Thomas.html">Thomas Lidbetter</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.05432">PDF</a><br/><b>Abstract: </b>We consider a search problem in which one or more targets must be rescued by
a search party, or Searcher. The targets may be survivors of some natural
disaster, or prisoners held by an adversary. The targets are hidden among a
finite set of locations, but when a location is searched, there is a known
probability that the search will come to an end, perhaps because the Searcher
becomes trapped herself, or is captured by the adversary. If this happens
before all the targets have been recovered, then the rescue attempt is deemed a
failure. The objective is to find the search that maximizes the probability of
recovering all the targets. We present and solve a game theoretic model for
this problem, by placing it in a more general framework that encompasses
another game previously introduced by the author. We also consider an extension
to the game in which the targets are hidden on the vertices of a graph. In the
case that there is only one target, we give a solution of the game played on a
tree.
</p></div>
    </summary>
    <updated>2019-02-15T02:24:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.05224</id>
    <link href="http://arxiv.org/abs/1902.05224" rel="alternate" type="text/html"/>
    <title>Conversion from RLBWT to LZ77</title>
    <feedworld_mtime>1550188800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nishimoto:Takaaki.html">Takaaki Nishimoto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tabei:Yasuo.html">Yasuo Tabei</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.05224">PDF</a><br/><b>Abstract: </b>Converting a compressed format of a string into another compressed format
without an explicit decompression is one of the central research topics in
string processing. We discuss the problem of converting the run-length
Burrows-Wheeler Transform (RLBWT) of a string to Lempel-Ziv 77 (LZ77) phrases
of the reversed string. The first results with Policriti and Prezza's
conversion algorithm [Algorithmica 2018] were $O(n \log r)$ time and $O(r)$
working space for length of the string $n$, number of runs $r$ in the RLBWT,
and number of LZ77 phrases $z$. Recent results with Kempa's conversion
algorithm [SODA 2019] are $O(n / \log n + r \log^{9} n + z \log^{9} n)$ time
and $O(n / \log_{\sigma} n + r \log^{8} n)$ working space for the alphabet size
$\sigma$ of the RLBWT. In this paper, we present a new conversion algorithm by
improving Policriti and Prezza's conversion algorithm where dynamic data
structures for general purpose are used. We argue that these dynamic data
structures can be replaced and present new data structures for faster
conversion. The time and working space of our conversion algorithm with new
data structures are $O(n \min \{ \log \log n, \sqrt{\frac{\log r}{\log\log r}}
\})$ and $O(r)$, respectively.
</p></div>
    </summary>
    <updated>2019-02-15T02:22:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.05166</id>
    <link href="http://arxiv.org/abs/1902.05166" rel="alternate" type="text/html"/>
    <title>Space-Efficient Data Structures for Lattices</title>
    <feedworld_mtime>1550188800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munro:J=_Ian.html">J. Ian Munro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sandlund:Bryce.html">Bryce Sandlund</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sinnamon:Corwin.html">Corwin Sinnamon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.05166">PDF</a><br/><b>Abstract: </b>A lattice is a partially-ordered set in which every pair of elements has a
unique meet (greatest lower bound) and join (least upper bound). We present new
data structures for lattices that are simple, efficient, and nearly optimal in
terms of space complexity.
</p>
<p>Our first data structure can answer partial order queries in constant time
and find the meet or join of two elements in $O(n^{3/4})$ time, where $n$ is
the number of elements in the lattice. It occupies $O(n^{3/2}\log n)$ bits of
space, which is only a $\Theta(\log n)$ factor from the $\Theta(n^{3/2})$-bit
lower bound for storing lattices. The preprocessing time is $O(n^2)$.
</p>
<p>This structure admits a simple space-time tradeoff so that, for any $c \in
[\frac{1}{2}, 1]$, the data structure supports meet and join queries in
$O(n^{1-c/2})$ time, occupies $O(n^{1+c}\log n)$ bits of space, and can be
constructed in $O(n^2 + n^{1+3c/2})$ time.
</p>
<p>Our second data structure uses $O(n^{3/2}\log n)$ bits of space and supports
meet and join in $O(d \frac{\log n}{\log d})$ time, where $d$ is the maximum
degree of any element in the transitive reduction graph of the lattice. This
structure is much faster for lattices with low-degree elements.
</p>
<p>This paper also identifies an error in a long-standing solution to the
problem of representing lattices. We discuss the issue with this previous work.
</p></div>
    </summary>
    <updated>2019-02-15T02:22:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.05134</id>
    <link href="http://arxiv.org/abs/1902.05134" rel="alternate" type="text/html"/>
    <title>Efficient Continuous Multi-Query Processing over Graph Streams</title>
    <feedworld_mtime>1550188800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zervakis:Lefteris.html">Lefteris Zervakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Setty:Vinay.html">Vinay Setty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tryfonopoulos:Christos.html">Christos Tryfonopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hose:Katja.html">Katja Hose</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.05134">PDF</a><br/><b>Abstract: </b>Graphs are ubiquitous and ever-present data structures that have a wide range
of applications involving social networks, knowledge bases and biological
interactions. The evolution of a graph in such scenarios can yield important
insights about the nature and activities of the underlying network, which can
then be utilized for applications such as news dissemination, network
monitoring, and content curation. Capturing the continuous evolution of a graph
can be achieved by long-standing sub-graph queries. Although, for many
applications this can only be achieved by a set of queries, state-of-the-art
approaches focus on a single query scenario. In this paper, we therefore
introduce the notion of continuous multi-query processing over graph streams
and discuss its application to a number of use cases. To this end, we designed
and developed a novel algorithmic solution for efficient multi-query evaluation
against a stream of graph updates and experimentally demonstrated its
applicability. Our results against two baseline approaches using real-world, as
well as synthetic datasets, confirm a two orders of magnitude improvement of
the proposed solution.
</p></div>
    </summary>
    <updated>2019-02-15T02:21:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.05101</id>
    <link href="http://arxiv.org/abs/1902.05101" rel="alternate" type="text/html"/>
    <title>Reconstructing Trees from Traces</title>
    <feedworld_mtime>1550188800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Davies:Sami.html">Sami Davies</a>, Miklos Z. Racz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rashtchian:Cyrus.html">Cyrus Rashtchian</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.05101">PDF</a><br/><b>Abstract: </b>We study the problem of learning a node-labeled tree given independent traces
from an appropriately defined deletion channel. This problem, tree trace
reconstruction, generalizes string trace reconstruction, which corresponds to
the tree being a path. For many classes of trees, including complete trees and
spiders, we provide algorithms that reconstruct the labels using only a
polynomial number of traces. This exhibits a stark contrast to known results on
string trace reconstruction, which require exponentially many traces, and where
a central open problem is to determine whether a polynomial number of traces
suffice. Our techniques combine novel combinatorial and complex analytic
methods.
</p></div>
    </summary>
    <updated>2019-02-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.05070</id>
    <link href="http://arxiv.org/abs/1902.05070" rel="alternate" type="text/html"/>
    <title>Optimization problems with low SWaP tactical Computing</title>
    <feedworld_mtime>1550188800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Im:Mee_Seong.html">Mee Seong Im</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dasari:Venkat_R=.html">Venkat R. Dasari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beshaj:Lubjana.html">Lubjana Beshaj</a>, Dale Shires <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.05070">PDF</a><br/><b>Abstract: </b>In a resource-constrained, contested environment, computing resources need to
be aware of possible size, weight, and power (SWaP) restrictions. SWaP-aware
computational efficiency depends upon optimization of computational resources
and intelligent time versus efficiency tradeoffs in decision making. In this
paper we address the complexity of various optimization strategies related to
low SWaP computing. Due to these restrictions, only a small subset of less
complicated and fast computable algorithms can be used for tactical, adaptive
computing.
</p></div>
    </summary>
    <updated>2019-02-15T02:20:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5151224614999297675</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5151224614999297675/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/02/the-iphonification-of-everything.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5151224614999297675" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5151224614999297675" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/02/the-iphonification-of-everything.html" rel="alternate" type="text/html"/>
    <title>The iPhonification of Everything</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">So you've got an iPhone XS in Space Grey. Congrats, so do 20 million other people. Maybe you have different cases but otherwise the hardware in all these phones are virtually identical. Yet you can tell with a glance that this is your phone. You can personalize apps and other elements of the home screen. It's your calendar and email and music.<br/>
<br/>
What? You've dropped your phone over Niagara falls. Luckily you've backed up your data. So you go back to Apple and buy another Space Grey iPhone XS and restore your data. Physically it's a completely different phone but for all practical purposes it's though you still had the original phone. Your phone is not defined by the device but the data that resides on it.<br/>
<br/>
It's not just phones. I can log into Google on anyone's Chrome browser and it will feel like my machine.<br/>
<br/>
Now we've all heard about a future world where nobody owns cars and we get driven around in self-driving Ubers, Lyfts and Waymos. One argument against this world is that people feel connected to their cars and unwilling to commute in some generic vehicle. But one can also imagine the car knows who you are, knows how you like your music, your lighting, how you adjust your seats even how your car drives. It becomes your car. Maybe even has electronic bumper stickers that change to support your political party.<br/>
<br/>
You can imagine the same for hotel rooms, your office, maybe even your apartment. It won't replicate your dog (or will it?) but as we get define more by our data than our things, do our things matter at all?</div>
    </content>
    <updated>2019-02-14T12:52:00Z</updated>
    <published>2019-02-14T12:52:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>https://plus.google.com/101693130490639305932</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>https://plus.google.com/101693130490639305932</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-02-14T12:58:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.05027</id>
    <link href="http://arxiv.org/abs/1902.05027" rel="alternate" type="text/html"/>
    <title>Proximity Queries for Absolutely Continuous Parametric Curves</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lakshmanan:Arun.html">Arun Lakshmanan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Patterson:Andrew.html">Andrew Patterson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cichella:Venanzio.html">Venanzio Cichella</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hovakimyan:Naira.html">Naira Hovakimyan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.05027">PDF</a><br/><b>Abstract: </b>In motion planning problems for autonomous robots, such as self-driving cars,
the robot must ensure that its planned path is not in close proximity to
obstacles in the environment. However, the problem of evaluating the proximity
is generally non-convex and serves as a significant computational bottleneck
for motion planning algorithms. In this paper, we present methods for a general
class of absolutely continuous parametric curves to compute: (i) the minimum
separating distance, (ii) tolerance verification, and (iii) collision
detection. Our methods efficiently compute bounds on obstacle proximity by
bounding the curve in a convex region. This bound is based on an upper bound on
the curve arc length that can be expressed in closed form for a useful class of
parametric curves including curves with trigonometric or polynomial bases. We
demonstrate the computational efficiency and accuracy of our approach through
numerical simulations of several proximity problems.
</p></div>
    </summary>
    <updated>2019-02-14T23:46:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04960</id>
    <link href="http://arxiv.org/abs/1902.04960" rel="alternate" type="text/html"/>
    <title>Counting Answers to Existential Questions</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dell:Holger.html">Holger Dell</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roth:Marc.html">Marc Roth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wellnitz:Philip.html">Philip Wellnitz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04960">PDF</a><br/><b>Abstract: </b>Conjunctive queries select and are expected to return certain tuples from a
relational database. We study the potentially easier problem of counting all
selected tuples, rather than enumerating them. In particular, we are interested
in the problem's parameterized and data complexity, where the query is
considered to be small or fixed, and the database is considered to be large. We
identify two structural parameters for conjunctive queries that capture their
inherent complexity: The dominating star size and the linked matching number.
If the dominating star size of a conjunctive query is large, then we show that
counting solution tuples to the query is at least as hard as counting
dominating sets, which yields a fine-grained complexity lower bound under the
Strong Exponential Time Hypothesis as well as a #W[2]-hardness result.
Moreover, if the linked matching number of a conjunctive query is large, then
we show that the structure of the query is so rich that arbitrary queries up to
a certain size can be encoded into it; this essentially establishes
#A[2]-completeness. Using ideas stemming from Lov\'asz, we lift complexity
results from the class of conjunctive queries to arbitrary existential or
universal formulas that might contain inequalities and negations on constraints
over the free variables. As a consequence, we obtain a complexity
classification that generalizes previous results of Chen, Durand, and Mengel
(ToCS 2015; ICDT 2015; PODS 2016) for conjunctive queries and of Curticapean
and Marx (FOCS 2014) for the subgraph counting problem. Our proof also relies
on graph minors, and we show a strengthening of the Excluded-Grid-Theorem which
might be of independent interest: If the linked matching number is large, then
not only can we find a large grid somewhere in the graph, but we can find a
large grid whose diagonal has disjoint paths leading into an assumed
node-well-linked set.
</p></div>
    </summary>
    <updated>2019-02-14T23:23:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04919</id>
    <link href="http://arxiv.org/abs/1902.04919" rel="alternate" type="text/html"/>
    <title>New Results on Directed Edge Dominating Set</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Belmonte:R=eacute=my.html">Rémy Belmonte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanaka:Tesshu.html">Tesshu Hanaka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Katsikarelis:Ioannis.html">Ioannis Katsikarelis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Eun_Jung.html">Eun Jung Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lampis:Michael.html">Michael Lampis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04919">PDF</a><br/><b>Abstract: </b>We study a family of generalizations of Edge Dominating Set on directed
graphs called Directed $(p,q)$-Edge Dominating Set. In this problem an arc
$(u,v)$ is said to dominate itself, as well as all arcs which are at distance
at most $q$ from $v$, or at distance at most $p$ to $u$.
</p>
<p>First, we give significantly improved FPT algorithms for the two most
important cases of the problem, $(0,1)$-dEDS and $(1,1)$-dEDS (that correspond
to versions of Dominating Set on line graphs), as well as polynomial kernels.
We also improve the best-known approximation for these cases from logarithmic
to constant. In addition, we show that $(p,q)$-dEDS is FPT parameterized by
$p+q+tw$, but W-hard parameterized by $tw$ (even if the size of the optimal is
added as a second parameter), where $tw$ is the treewidth of the underlying
graph of the input.
</p>
<p>We then go on to focus on the complexity of the problem on tournaments. Here,
we provide a complete classification for every possible fixed value of $p,q$,
which shows that the problem exhibits a surprising behavior, including cases
which are in P; cases which are solvable in quasi-polynomial time but not in P;
and a single case $(p=q=1)$ which is NP-hard (under randomized reductions) and
cannot be solved in sub-exponential time, under standard assumptions.
</p></div>
    </summary>
    <updated>2019-02-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04828</id>
    <link href="http://arxiv.org/abs/1902.04828" rel="alternate" type="text/html"/>
    <title>On the achromatic number of signed graphs</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lajou:Dimitri.html">Dimitri Lajou</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04828">PDF</a><br/><b>Abstract: </b>In this paper, we generalize the concept of complete coloring and achromatic
number to 2-edge-colored graphs and signed graphs. We give some useful
relationships between different possible definitions of such achromatic numbers
and prove that computing any of them is NP-complete.
</p></div>
    </summary>
    <updated>2019-02-14T23:21:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04805</id>
    <link href="http://arxiv.org/abs/1902.04805" rel="alternate" type="text/html"/>
    <title>Task-based Augmented Contour Trees with Fibonacci Heaps</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gueunet:Charles.html">Charles Gueunet</a>, P. Fortin, J Jomier, J Tierny <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04805">PDF</a><br/><b>Abstract: </b>This paper presents a new algorithm for the fast, shared memory, multi-core
computation of augmented contour trees on triangulations. In contrast to most
existing parallel algorithms our technique computes augmented trees, enabling
the full extent of contour tree based applications including data segmentation.
Our approach completely revisits the traditional, sequential contour tree
algorithm to re-formulate all the steps of the computation as a set of
independent local tasks. This includes a new computation procedure based on
Fibonacci heaps for the join and split trees, two intermediate data structures
used to compute the contour tree, whose constructions are efficiently carried
out concurrently thanks to the dynamic scheduling of task parallelism. We also
introduce a new parallel algorithm for the combination of these two trees into
the output global contour tree. Overall, this results in superior time
performance in practice, both in sequential and in parallel thanks to the
OpenMP task runtime. We report performance numbers that compare our approach to
reference sequential and multi-threaded implementations for the computation of
augmented merge and contour trees. These experiments demonstrate the run-time
efficiency of our approach and its scalability on common workstations. We
demonstrate the utility of our approach in data segmentation applications.
</p></div>
    </summary>
    <updated>2019-02-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04785</id>
    <link href="http://arxiv.org/abs/1902.04785" rel="alternate" type="text/html"/>
    <title>Constructing Antidictionaries in Output-Sensitive Space</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ayad:Lorraine_A=_K=.html">Lorraine A. K. Ayad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Badkobeh:Golnaz.html">Golnaz Badkobeh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fici:Gabriele.html">Gabriele Fici</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/H=eacute=liou:Alice.html">Alice Héliou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pissis:Solon_P=.html">Solon P. Pissis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04785">PDF</a><br/><b>Abstract: </b>A word $x$ that is absent from a word $y$ is called minimal if all its proper
factors occur in $y$. Given a collection of $k$ words $y_1,y_2,\ldots,y_k$ over
an alphabet $\Sigma$, we are asked to compute the set
$\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{k}}$ of minimal absent words of length at
most $\ell$ of word $y=y_1\#y_2\#\ldots\#y_k$, $\#\notin\Sigma$. In data
compression, this corresponds to computing the antidictionary of $k$ documents.
In bioinformatics, it corresponds to computing words that are absent from a
genome of $k$ chromosomes. This computation generally requires $\Omega(n)$
space for $n=|y|$ using any of the plenty available $\mathcal{O}(n)$-time
algorithms. This is because an $\Omega(n)$-sized text index is constructed over
$y$ which can be impractical for large $n$. We do the identical computation
incrementally using output-sensitive space. This goal is reasonable when
$||\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{N}}||=o(n)$, for all $N\in[1,k]$. For
instance, in the human genome, $n \approx 3\times 10^9$ but
$||\mathrm{M}^{12}_{y_{1}\#\ldots\#y_{k}}|| \approx 10^6$. We consider a
constant-sized alphabet for stating our results. We show that all
$\mathrm{M}^{\ell}_{y_{1}},\ldots,\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{k}}$ can
be computed in
$\mathcal{O}(kn+\sum^{k}_{N=1}||\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{N}}||)$
total time using $\mathcal{O}(\mathrm{MaxIn}+\mathrm{MaxOut})$ space, where
$\mathrm{MaxIn}$ is the length of the longest word in $\{y_1,\ldots,y_{k}\}$
and
$\mathrm{MaxOut}=\max\{||\mathrm{M}^{\ell}_{y_{1}\#\ldots\#y_{N}}||:N\in[1,k]\}$.
Proof-of-concept experimental results are also provided confirming our
theoretical findings and justifying our contribution.
</p></div>
    </summary>
    <updated>2019-02-14T23:43:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04764</id>
    <link href="http://arxiv.org/abs/1902.04764" rel="alternate" type="text/html"/>
    <title>Explicit lower bounds on strong simulation of quantum circuits in terms of $T$-gate count</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Cupjin.html">Cupjin Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Newman:Michael.html">Michael Newman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Szegedy:Mario.html">Mario Szegedy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04764">PDF</a><br/><b>Abstract: </b>We investigate Clifford+$T$ quantum circuits with a small number of
$T$-gates. Using the sparsification lemma, we identify time complexity lower
bounds in terms of $T$-gate count below which a strong simulator would improve
on the state-of-the-art $3$-SAT solving.
</p></div>
    </summary>
    <updated>2019-02-14T23:21:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04741</id>
    <link href="http://arxiv.org/abs/1902.04741" rel="alternate" type="text/html"/>
    <title>Learning and Generalization for Matching Problems</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Alon.html">Alon Cohen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hassidim:Avinatan.html">Avinatan Hassidim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaplan:Haim.html">Haim Kaplan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mansour:Yishay.html">Yishay Mansour</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moran:Shay.html">Shay Moran</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04741">PDF</a><br/><b>Abstract: </b>We study a classic algorithmic problem through the lens of statistical
learning. That is, we consider a matching problem where the input graph is
sampled from some distribution. This distribution is unknown to the algorithm;
however, an additional graph which is sampled from the same distribution is
given during a training phase (preprocessing). More specifically, the
algorithmic problem is to match $k$ out of $n$ items that arrive online to $d$
categories ($d\ll k \ll n$). Our goal is to design a two-stage online algorithm
that retains a small subset of items in the first stage which contains an
offline matching of maximum weight. We then compute this optimal matching in a
second stage. The added statistical component is that before the online
matching process begins, our algorithms learn from a training set consisting of
another matching instance drawn from the same unknown distribution. Using this
training set, we learn a policy that we apply during the online matching
process. We consider a class of online policies that we term \emph{thresholds
policies}. For this class, we derive uniform convergence results both for the
number of retained items and the value of the optimal matching. We show that
the number of retained items and the value of the offline optimal matching
deviate from their expectation by $O(\sqrt{k})$. This requires usage of
less-standard concentration inequalities (standard ones give deviations of
$O(\sqrt{n})$). Furthermore, we design an algorithm that outputs the optimal
offline solution with high probability while retaining only $O(k\log \log n)$
items in expectation.
</p></div>
    </summary>
    <updated>2019-02-14T23:31:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04740</id>
    <link href="http://arxiv.org/abs/1902.04740" rel="alternate" type="text/html"/>
    <title>CSPs with Global Modular Constraints: Algorithms and Hardness via Polynomial Representations</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brakensiek:Joshua.html">Joshua Brakensiek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gopi:Sivakanth.html">Sivakanth Gopi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guruswami:Venkatesan.html">Venkatesan Guruswami</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04740">PDF</a><br/><b>Abstract: </b>We study the complexity of Boolean constraint satisfaction problems (CSPs)
when the assignment must have Hamming weight in some congruence class modulo M,
for various choices of the modulus M. Due to the known classification of
tractable Boolean CSPs, this mainly reduces to the study of three cases: 2-SAT,
HORN-SAT, and LIN-2 (linear equations mod 2). We classify the moduli M for
which these respective problems are polynomial time solvable, and when they are
not (assuming the ETH). Our study reveals that this modular constraint lends a
surprising richness to these classic, well-studied problems, with interesting
broader connections to complexity theory and coding theory. The HORN-SAT case
is connected to the covering complexity of polynomials representing the NAND
function mod M. The LIN-2 case is tied to the sparsity of polynomials
representing the OR function mod M, which in turn has connections to modular
weight distribution properties of linear codes and locally decodable codes. In
both cases, the analysis of our algorithm as well as the hardness reduction
rely on these polynomial representations, highlighting an interesting algebraic
common ground between hard cases for our algorithms and the gadgets which show
hardness. These new complexity measures of polynomial representations merit
further study.
</p>
<p>The inspiration for our study comes from a recent work by N\"agele, Sudakov,
and Zenklusen on submodular minimization with a global congruence constraint.
Our algorithm for HORN-SAT has strong similarities to their algorithm, and in
particular identical kind of set systems arise in both cases. Our connection to
polynomial representations leads to a simpler analysis of such set systems, and
also sheds light on (but does not resolve) the complexity of submodular
minimization with a congruency requirement modulo a composite M.
</p></div>
    </summary>
    <updated>2019-02-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04738</id>
    <link href="http://arxiv.org/abs/1902.04738" rel="alternate" type="text/html"/>
    <title>Mesh: Compacting Memory Management for C/C++ Applications</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Powers:Bobby.html">Bobby Powers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tench:David.html">David Tench</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berger:Emery_D=.html">Emery D. Berger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McGregor:Andrew.html">Andrew McGregor</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04738">PDF</a><br/><b>Abstract: </b>Programs written in C/C++ can suffer from serious memory fragmentation,
leading to low utilization of memory, degraded performance, and application
failure due to memory exhaustion. This paper introduces Mesh, a plug-in
replacement for malloc that, for the first time, eliminates fragmentation in
unmodified C/C++ applications. Mesh combines novel randomized algorithms with
widely-supported virtual memory operations to provably reduce fragmentation,
breaking the classical Robson bounds with high probability. Mesh generally
matches the runtime performance of state-of-the-art memory allocators while
reducing memory consumption; in particular, it reduces the memory of
consumption of Firefox by 16% and Redis by 39%.
</p></div>
    </summary>
    <updated>2019-02-14T23:29:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04735</id>
    <link href="http://arxiv.org/abs/1902.04735" rel="alternate" type="text/html"/>
    <title>Computing the Yolk in Spatial Voting Games without Computing Median Lines</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gudmundsson:Joachim.html">Joachim Gudmundsson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wong:Sampson.html">Sampson Wong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04735">PDF</a><br/><b>Abstract: </b>The yolk is an important concept in spatial voting games as it generalises
the equilibrium and provides bounds on the uncovered set. We present
near-linear time algorithms for computing the yolk in the spatial voting model
in the plane. To the best of our knowledge our algorithm is the first algorithm
that does not require precomputing the median lines and hence able to break the
existing $O(n^{4/3})$ bound which equals the known upper bound on the number of
median lines. We avoid this requirement by using Megiddo's parametric search,
which is a powerful framework that could lead to faster algorithms for many
other spatial voting problems.
</p></div>
    </summary>
    <updated>2019-02-14T23:45:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04728</id>
    <link href="http://arxiv.org/abs/1902.04728" rel="alternate" type="text/html"/>
    <title>Learning Ising Models with Independent Failures</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goel:Surbhi.html">Surbhi Goel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kane:Daniel_M=.html">Daniel M. Kane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klivans:Adam_R=.html">Adam R. Klivans</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04728">PDF</a><br/><b>Abstract: </b>We give the first efficient algorithm for learning the structure of an Ising
model that tolerates independent failures; that is, each entry of the observed
sample is missing with some unknown probability p. Our algorithm matches the
essentially optimal runtime and sample complexity bounds of recent work for
learning Ising models due to Klivans and Meka (2017).
</p>
<p>We devise a novel unbiased estimator for the gradient of the Interaction
Screening Objective (ISO) due to Vuffray et al. (2016) and apply a stochastic
multiplicative gradient descent algorithm to minimize this objective. Solutions
to this minimization recover the neighborhood information of the underlying
Ising model on a node by node basis.
</p></div>
    </summary>
    <updated>2019-02-14T23:28:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04703</id>
    <link href="http://arxiv.org/abs/1902.04703" rel="alternate" type="text/html"/>
    <title>Assessing Solution Quality of 3SAT on a Quantum Annealing Platform</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gabor:Thomas.html">Thomas Gabor</a>, Sebastian Zielinski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feld:Sebastian.html">Sebastian Feld</a>, Christoph Roch, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seidel:Christian.html">Christian Seidel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neukart:Florian.html">Florian Neukart</a>, Isabella Galter, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mauerer:Wolfgang.html">Wolfgang Mauerer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Linnhoff=Popien:Claudia.html">Claudia Linnhoff-Popien</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04703">PDF</a><br/><b>Abstract: </b>When solving propositional logic satisfiability (specifically 3SAT) using
quantum annealing, we analyze the effect the difficulty of different instances
of the problem has on the quality of the answer returned by the quantum
annealer. A high-quality response from the annealer in this case is defined by
a high percentage of correct solutions among the returned answers. We show that
the phase transition regarding the computational complexity of the problem,
which is well-known to occur for 3SAT on classical machines (where it causes a
detrimental increase in runtime), persists in some form (but possibly to a
lesser extent) for quantum annealing.
</p></div>
    </summary>
    <updated>2019-02-14T23:22:08Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04629</id>
    <link href="http://arxiv.org/abs/1902.04629" rel="alternate" type="text/html"/>
    <title>Crowdsourced PAC Learning under Classification Noise</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heinecke:Shelby.html">Shelby Heinecke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reyzin:Lev.html">Lev Reyzin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04629">PDF</a><br/><b>Abstract: </b>In this paper, we analyze PAC learnability from labels produced by
crowdsourcing. In our setting, unlabeled examples are drawn from a distribution
and labels are crowdsourced from workers who operate under classification
noise, each with their own noise parameter. We develop an end-to-end
crowdsourced PAC learning algorithm that takes unlabeled data points as input
and outputs a trained classifier. Our three-step algorithm incorporates
majority voting, pure-exploration bandits, and noisy-PAC learning. We prove
several guarantees on the number of tasks labeled by workers for PAC learning
in this setting and show that our algorithm improves upon the baseline by
reducing the total number of tasks given to workers. We demonstrate the
robustness of our algorithm by exploring its application to additional
realistic crowdsourcing settings.
</p></div>
    </summary>
    <updated>2019-02-14T23:39:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.04569</id>
    <link href="http://arxiv.org/abs/1902.04569" rel="alternate" type="text/html"/>
    <title>Computational Complexity and the Nature of Quantum Mechanics</title>
    <feedworld_mtime>1550102400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Benavoli:Alessio.html">Alessio Benavoli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Facchini:Alessandro.html">Alessandro Facchini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zaffalon:Marco.html">Marco Zaffalon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.04569">PDF</a><br/><b>Abstract: </b>Quantum theory (QT) has been confirmed by numerous experiments, yet we still
cannot fully grasp the meaning of the theory. As a consequence, the quantum
world appears to us paradoxical. Here we shed new light on QT by having it
follow from two main postulates (i) the theory should be logically consistent;
(ii) inferences in the theory should be computable in polynomial time. The
first postulate is what we require to each well-founded mathematical theory.
The computation postulate defines the physical component of the theory. We show
that the computation postulate is the only true divide between QT, seen as a
generalised theory of probability, and classical probability. All quantum
paradoxes, and entanglement in particular, arise from the clash of trying to
reconcile a computationally intractable, somewhat idealised, theory (classical
physics) with a computationally tractable theory (QT) or, in other words, from
regarding physics as fundamental rather than computation.
</p></div>
    </summary>
    <updated>2019-02-14T23:28:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4121</id>
    <link href="https://www.scottaaronson.com/blog/?p=4121" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4121#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4121" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Four updates</title>
    <summary xml:lang="en-US">A few weeks ago, I was at QIP’2019 in Boulder, CO. This week I was at SQuInT’2019 in Albuquerque, NM. There were lots of amazing talks—feel free to ask in the comments section. There’s an interview with me at the website “GigaOm,” conducted by Byron Reese and entitled Quantum Computing: Capabilities and Limits. I didn’t […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>A few weeks ago, I was at <a href="http://jila.colorado.edu/qip2019/">QIP’2019</a> in Boulder, CO.  This week I was at <a href="http://physics.unm.edu/SQuInT/2019/index.php">SQuInT’2019</a> in Albuquerque, NM.  There were lots of amazing talks—feel free to ask in the comments section.</p>



<p>There’s an interview with me at the website “GigaOm,” conducted by Byron Reese and entitled <a href="https://gigaom.com/2019/01/17/quantum-computing-capabilities-and-limits-an-interview-with-scott-aaronson/">Quantum Computing: Capabilities and Limits</a>.  I didn’t proofread the transcript and it has some errors in it, but hopefully the meaning comes through.  In other interview news, if you were interested in my podcast with Adam Ford in Melbourne but don’t like YouTube, Adam has helpfully prepared transcripts of the two longest segments: <a href="http://www.scifuture.org/the-ghost-in-the-quantum-turing-machine-scott-aaronson/">The Ghost in the Quantum Turing Machine</a> and <a href="http://www.scifuture.org/the-winding-road-to-quantum-supremacy-scott-aaronson/">The Winding Road to Quantum Supremacy</a>.</p>



<p>The <em>New York Times</em> ran an article entitled <a href="https://www.nytimes.com/2019/01/24/technology/computer-science-courses-college.html">The Hard Part of Computer Science? Getting Into Class</a>, about the surge in computer science majors all over the US, and the shortage of professors to teach them.  The article’s go-to example of a university where this is happening is UT Austin, and there’s extensive commentary from my department chair, Don Fussell.</p>



<p>The <a href="http://acm-stoc.org/stoc2019/STOC%202019%20accepted%20papers.html">STOC’2019 accepted papers list</a> is finally out.  Lots of cool stuff!</p></div>
    </content>
    <updated>2019-02-13T04:43:23Z</updated>
    <published>2019-02-13T04:43:23Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-02-13T04:44:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5768393804446188224</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5768393804446188224/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/02/i-think-ze-was-confused-in-favor-of.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5768393804446188224" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5768393804446188224" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/02/i-think-ze-was-confused-in-favor-of.html" rel="alternate" type="text/html"/>
    <title>I think ze was confused  -- in favor of genderless pronouns</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">You've probably heard the following:<br/>
<br/>
<br/>
               At first I didn't want to get an X but now that I have it, I can't imagine life without one.<br/>
<br/>
X could be telegraph, radio, TV, color TV, VCR, CD player, streaming, Netflix, Amazon prime, an uber account, Washer and Dryer, Car Phones (remember those), Cell Phones. If you go back in history  wrist watches or sun dials (or wrist-sun-dials!).<br/>
<br/>
This has happened to me recently though not with an object. I read an article someplace saying that ze can be used instead of he or she. It was referring to nonbinaries (using `they' never quite sounded right) but actually it would be great if this was a general genderless pronoun. I am not making a political statement here (although I doubt I have any readers who are against genderless pronouns).<br/>
<br/>
Once I came across the term ze I found places to use it and now I can't imagine not using it.<br/>
<br/>
In a recent article I wrote I needed to say that someone was probably confused, but I did not know their gender. I used<br/>
<br/>
                                                         Ze was probably confused<br/>
<br/>
which is much better than<br/>
<br/>
                                                         S/he was probably confused<br/>
<br/>
                                                         He or she was probably confused<br/>
<br/>
                                                         The student was probably confused<br/>
<br/>
                                                         They were probably confused.<br/>
<br/>
Note that the first two leave out nonbinaries.<br/>
<br/>
0) In the article I put in a footnote saying what ze meant. In the future I may not have to.<br/>
<br/>
1) Will ze catch on? This blog post is an attempt to hasten the practice.<br/>
<br/>
2) Is there a term for his/her that is non-gendered? If not then maybe zer.<br/>
<br/>
3) Will there be political pushback on this usage? If its phrased as a way to include nonbinaries than unfortunately yes. If its phrased as above as when you don't know the gender, what do you do, then no.<br/>
<br/>
4) Is <i> nonbinary </i>the correct term? If not then please politely correct me in the comments.<br/>
<br/>
5) Has Ms replaced Miss and Mrs?<br/>
<br/>
I have used the term ze several times since then- often when I get email from a student such that I can't tell from the first name what their gender is, and I need to forward the email, such as<br/>
<br/>
                   Ze wants to take honors discrete math but does not have the prerequisite, but<br/>
                   since ze placed in the top five in the math olympiad, we'll let zer take it.<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-02-11T19:43:00Z</updated>
    <published>2019-02-11T19:43:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>https://plus.google.com/101693130490639305932</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-02-14T12:58:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/02/11/parameterized-approximation-algorithms-workshop-paaw-2019/</id>
    <link href="https://cstheory-events.org/2019/02/11/parameterized-approximation-algorithms-workshop-paaw-2019/" rel="alternate" type="text/html"/>
    <title>Parameterized Approximation Algorithms Workshop (PAAW) 2019</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 8, 2019 Patras, Greece https://sites.google.com/site/aefeldmann/parameterized-approximation-algorithms-workshop-paaw-2019 Submission deadline: April 26, 2019 Registration deadline: April 30, 2019 The 2019 edition of the Parameterized Approximation Algorithms Workshop (PAAW) will take place as a satellite workshop of ICALP 2019 in Patras, Greece, on Monday July 8th 2019. — Topics of interest — – Parameterized approximation algorithms – Lossy … <a class="more-link" href="https://cstheory-events.org/2019/02/11/parameterized-approximation-algorithms-workshop-paaw-2019/">Continue reading <span class="screen-reader-text">Parameterized Approximation Algorithms Workshop (PAAW) 2019</span></a></div>
    </summary>
    <updated>2019-02-11T15:31:13Z</updated>
    <published>2019-02-11T15:31:13Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-02-15T15:21:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://bit-player.org/?p=2137</id>
    <link href="http://bit-player.org/2019/divisive-factorials" rel="alternate" type="text/html"/>
    <link href="http://bit-player.org/2019/divisive-factorials#comments" rel="replies" type="text/html"/>
    <link href="http://bit-player.org/2019/divisive-factorials/feed/atom" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Divisive factorials!</title>
    <summary type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml">The other day I was derailed by this tweet from Fermat’s Library: The moment I saw it, I had to stop in my tracks, grab a scratch pad, and check out the formula. The result made sense in a rough-and-ready … <a href="http://bit-player.org/2019/divisive-factorials">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>The other day I was derailed by this tweet from Fermat’s Library:</p>
<p><img alt="Inverse factorial tweet" border="0" class="aligncenter" height="385" src="http://bit-player.org/wp-content/uploads/2019/02/inverse-factorial-tweet.png" width="592"/></p>
<p class="undent">The moment I saw it, I had to stop in my tracks, grab a scratch pad, and check out the formula. The result made sense in a rough-and-ready sort of way. Since the multiplicative version of \(n!\) goes to infinity as \(n\) increases, the “divisive” version should go to zero. And \(\frac{n^2}{n!}\) does exactly that; the polynomial function \(n^2\) grows slower than the exponential function \(n!\) for large enough \(n\):</p>
<p>\[\frac{1}{1}, \frac{4}{2}, \frac{9}{6}, \frac{16}{24}, \frac{25}{120}, \frac{36}{720}, \frac{49}{5040}, \frac{64}{40320}, \frac{81}{362880}, \frac{100}{3628800}.\]</p>
<p class="undent">But why does the quotient take the particular form \(\frac{n^2}{n!}\)? Where does the \(n^2\) come from?</p>
<p>To answer that question, I had to revisit the long-ago trauma of learning to divide fractions, but I pushed through the pain. Proceeding from left to right through the formula in the tweet, we first get \(\frac{n}{n-1}\). Then, dividing that quantity by \(n-2\) yields</p>
<p>\[\cfrac{\frac{n}{n-1}}{n-2} = \frac{n}{(n-1)(n-2)}.\]</p>
<p class="undent">Continuing in the same way, we ultimately arrive at:</p>
<p>\[n \mathbin{/} (n-1) \mathbin{/} (n-2) \mathbin{/} (n-3) \mathbin{/} \cdots \mathbin{/} 1 = \frac{n}{(n-1) (n-2) (n-3) \cdots 1} = \frac{n}{(n-1)!}\]</p>
<p class="undent">To recover the tweet’s stated result of \(\frac{n^2}{n!}\), just multiply numerator and denominator by \(n\). (To my taste, however, \(\frac{n}{(n-1)!}\) is the more perspicuous expression.)</p>
<hr/>
<p>I am a card-carrying factorial fanboy. You can keep your fancy Fibonaccis; <em>this</em> is my favorite function. Every time I try out a new programming language, my first exercise is to write a few routines for calculating factorials. Over the years I have pondered several variations on the theme, such as replacing \(\times\) with \(+\) in the definition (which produces triangular numbers). But I don’t think I’ve ever before considered substituting \(\mathbin{/}\) for \(\times\). It’s messy. Because multiplication is commutative and associative, you can define \(n!\) simply as the product of all the integers from \(1\) through \(n\), without worrying about the order of the operations. With division, order can’t be ignored. In general, \(x \mathbin{/} y \ne y \mathbin{/}x\), and \((x \mathbin{/} y) \mathbin{/} z \ne x \mathbin{/} (y \mathbin{/} z)\).</p>
<p>The Fermat’s Library tweet puts the factors in descending order: \(n, n-1, n-2, \ldots, 1\). The most obvious alternative is the ascending sequence \(1, 2, 3, \ldots, n\). What happens if we define the divisive factorial as \(1 \mathbin{/} 2 \mathbin{/}  3 \mathbin{/} \cdots \mathbin{/} n\)? Another visit to the schoolroom algorithm for dividing fractions yields this simple answer:</p>
<p>\[1 \mathbin{/} 2 \mathbin{/}  3 \mathbin{/} \cdots \mathbin{/} n = \frac{1}{2 \times 3 \times 4 \times \cdots \times n} = \frac{1}{n!}.\]</p>
<p class="undent">In other words, when we repeatedly divide while counting up from \(1\) to \(n\), the final quotient is the reciprocal of \(n!\).  (I wish I could put an exclamation point at the end of that sentence!) If you’re looking for a canonical answer to the question, “What do you get if you divide instead of multiplying in \(n!\)?” I would argue that \(\frac{1}{n!}\) is a better candidate than \(\frac{n}{(n - 1)!}\). Why not embrace the symmetry between \(n!\) and its inverse?</p>
<p>Of course there are many other ways to arrange the <em>n</em> integers in the set \(\{1 \ldots n\}\). How many ways? As it happens, \(n!\) of them! Thus it would seem there are \(n!\) distinct ways to define the divisive \(n!\) function. However, looking at the answers for the two permutations discussed above suggests there’s a simpler pattern at work. Whatever element of the sequence happens to come first winds up in the numerator of a big fraction, and the denominator is the product of all the other elements. As a result, there are really only \(n\) different outcomes—assuming we stick to performing the division operations from left to right. For any integer \(k\) between \(1\) and \(n\), putting \(k\) at the head of the queue creates a divisive \(n!\) equal to \(k\) divided by all the other factors. We can write this out as:</p>
<p>\[\cfrac{k}{\frac{n!}{k}}, \text{ which can be rearranged as } \frac{k^2}{n!}.\]</p>
<p class="undent">And thus we also solve the minor mystery of how \(\frac{n}{(n-1)!}\) became \(\frac{n^2}{n!}\) in the tweet.</p>
<p>It’s worth noting that all of these functions converge to zero as \(n\) goes to infinity. Asymptotically speaking, \(\frac{1^2}{n!}, \frac{2^2}{n!}, \ldots, \frac{n^2}{n!}\) are all alike.</p>
<hr/>
<p>Ta dah! Mission accomplished. Problem solved. Done and dusted. Now we know everything there is to know about divisive factorials, right?</p>
<p>Well, maybe there’s one more question. What does the computer say? If you take your favorite factorial algorithm, and do as the tweet suggests, replacing any appearance of the \(\times\) (or <code>*</code>) operator with <code>/</code>, what happens? Which of the \(n\) variants of divisive \(n!\) does the program produce?</p>
<p>Here’s <em>my</em> favorite algorithm for computing factorials, in the form of a <a href="https://julialang.org/">Julia</a> program:</p>
<pre class="language-julia"><code>function mul!(n)
    if n == 1
        return 1
    else
        return n * mul!(n - 1)
    end
end
</code></pre>
<p>This is the algorithm that has introduced generations of nerds to the concept of recursion. In narrative form it says: If \(n\) is \(1\), then \(mul!(n)\) is \(1\). Otherwise, evaluate the function \(mul!(n-1)\), then multiply the result by \(n\). You might ask what happens if \(n\) is zero or negative. You might ask, but please don’t. For present purposes, \(n \in \mathbb{N}\).Starting with any positive \(n\), the sequence of recursive calls must eventually bottom out with \(n = 1\).</p>
<p>The function can be written more tersely using Julia’s one-liner style of definition:.</p>
<pre class="language-julia"><code>mul!(n)  =  n == 1 ? 1 : n * mul!(n - 1)</code></pre>
<p>The right side of the assignment statement is a conditional expression, or ternary operator, which has the form <code>a ? b : c</code>. Here <code>a</code> is a boolean test clause, which must return a value of either <code>true</code> or <code>false</code>. If <code>a</code> is <code>true</code>, clause <code>b</code> is evaluated, and the result becomes the value of the entire expression. Otherwise clause <code>c</code> is evaluated.</p>
<p>Just to be sure I’ve got this right, here are the first 10 factorials, as calculated by this program:</p>
<pre class="language-julia"><code>[mul!(n) for n in 1:10]
10-element Array{Int64,1}:
       1
       2
       6
      24
     120
     720
    5040
   40320
  362880
 3628800</code></pre>
<p class="indent">Now let’s edit that definition and convert the single occurence of <code>*</code> to a <code>/</code>, leaving everything else (except the name of the function) unchanged.</p>
<pre class="language-julia"><code>div!(n)  =  n == 1 ? 1 : n / div!(n - 1)</code></pre>
<p>And here’s what comes back when we run the program for values of \(n\) from \(1\) through \(20\):</p>
<pre class="language-julia"><code>[div!(n) for n in 1:20]
20-element Array{Real,1}:
 1                 
 2.0               
 1.5               
 2.6666666666666665
 1.875             
 3.2               
 2.1875            
 3.657142857142857 
 2.4609375         
 4.063492063492063 
 2.70703125        
 4.432900432900433 
 2.9326171875      
 4.773892773892774 
 3.14208984375     
 5.092152292152292 
 3.338470458984375 
 5.391690662278897 
 3.523941040039063 
 5.675463855030418 </code></pre>
<p>Huh? That sure doesn’t look like it’s converging to zero—not as \(\frac{1}{n!}\) or as \(\frac{n}{n - 1}\). As a matter of fact, it doesn’t look like it’s going to converge at all. The graph below suggests the sequence is made up of two alternating components, both of which appear to be slowly growing toward infinity as well as diverging from one another.</p>
<p><img alt="Div" border="0" class="aligncenter" height="" src="http://bit-player.org/wp-content/uploads/2019/02/div.svg" width=""/></p>
<p class="indent">In trying to make sense of what we’re seeing here, it helps to change the output type of the <code>div!</code> function. Instead of applying the division operator <code>/</code>, which returns the quotient as a floating-point number, we can substitute the  <code>//</code> operator, which returns an exact rational quotient, reduced to lowest terms.</p>
<pre class="language-julia"><code>div!(n)  =  n == 1 ? 1 : n // div!(n - 1)</code></pre>
<p class="undent">Here’s the sequence of values for <code>n in 1:20</code>:</p>
<pre class="language-julia"><code>20-element Array{Real,1}:
       1      
      2//1    
      3//2    
      8//3    
     15//8    
     16//5    
     35//16   
    128//35   
    315//128  
    256//63   
    693//256  
   1024//231  
   3003//1024 
   2048//429  
   6435//2048 
  32768//6435 
 109395//32768
  65536//12155
 230945//65536
 262144//46189 </code></pre>
<p>The list is full of curious patterns. It’s a double helix, with even numbers and odd numbers zigzagging in complementary strands. The even numbers are not just even; they are all powers of \(2\). Also, they appear in pairs—first in the numerator, then in the denominator—and their sequence is nondecreasing. But there are gaps; not all powers of \(2\) are present. The odd strand looks even more complicated, with various small prime factors flitting in and out of the numbers. (The primes <em>have</em> to be small—smaller than \(n\), anyway.)</p>
<p>This outcome took me by surprise. I had really expected to see a much tamer sequence, like those I worked out with pencil and paper. All those jagged, jitterbuggy ups and downs made no sense. Nor did the overall trend of unbounded growth in the ratio. How could you keep dividing and dividing, and wind up with bigger and bigger numbers?</p>
<p>At this point you may want to pause before reading on, and try to work out your own theory of where these zigzag numbers are coming from. If you need a hint, you can get a strong one—almost a spoiler—by looking up the sequence of numerators or the sequence of denominators in the <a href="http://oeis.org">Online Encyclopedia of Integer Sequences</a>.</p>
<hr/>
<p>Here’s another hint. A small edit to the <code>div!</code> program completely transforms the output. Just flip the final clause, changing <code>n // div!(n - 1)</code> into <code>div!(n - 1) // n</code>. </p>
<pre class="language-julia"><code>div!(n)  =  n == 1 ? 1 : div!(n - 1) // n</code></pre>
<p class="undent">Now the results look like this:</p>
<pre class="language-julia"><code>10-element Array{Real,1}:
  1                    
 1//2                  
 1//6                  
 1//24                 
 1//120                
 1//720                
 1//5040               
 1//40320              
 1//362880             
 1//3628800</code></pre>
<p>This is the inverse factorial function we’ve already seen, the series of quotients generated when you march left to right through an ascending sequence of divisors \(1 \mathbin{/} 2 \mathbin{/}  3 \mathbin{/} \cdots \mathbin{/} n\). </p>
<p>It’s no surprise that flipping the final clause in the procedure alters the outcome. After all, we know that division is not commutative or associative. What’s not so easy to see is why the sequence of quotients generated by the original program takes that weird zigzag form. What mechanism is giving rise to those paired powers of 2 and the alternation of odd and even?</p>
<p>I have found that it’s easier to explain what’s going on in the zigzag sequence when I describe an iterative version of the procedure, rather than the recursive one. (This is an embarrassing admission for someone who has argued that recursive definitions are easier to reason about, but there you have it.) Here’s the program:</p>
<pre class="language-julia"><code>function div!_iter(n)
    q = 1
    for i in 1:n
        q = i // q
    end
    return q
end</code></pre>
<p>I submit that this looping procedure is operationally identical to the recursive function, in the sense that if <code>div!(n)</code> and <code>div!_iter(n)</code> both return a result for some positive integer <code>n</code>, it will always be the same result. Here’s my evidence: </p>
<pre class="language-julia"><code>[div!(n) for n in 1:20]    [div!_iter(n) for n in 1:20]
            1                         1//1    
           2//1                       2//1    
           3//2                       3//2    
           8//3                       8//3    
          15//8                      15//8    
          16//5                      16//5    
          35//16                     35//16   
         128//35                    128//35   
         315//128                   315//128  
         256//63                    256//63   
         693//256                   693//256  
        1024//231                  1024//231  
        3003//1024                 3003//1024 
        2048//429                  2048//429  
        6435//2048                 6435//2048 
       32768//6435                32768//6435 
      109395//32768              109395//32768
       65536//12155               65536//12155
      230945//65536              230945//65536
      262144//46189              262144//46189</code></pre>
<p>To understand the process that gives rise to these numbers, consider the successive values of the variables \(i\) and \(q\) each time the loop is executed. Initially, \(i\) and \(q\) are both set to \(1\); hence, after the first passage through the loop, the statement <code>q = i // q</code> gives \(q\) the value \(\frac{1}{1}\). Next time around, \(i = 2\) and \(q = \frac{1}{1}\), so \(q\)’s new value is \(\frac{2}{1}\). On the third iteration, \(i = 3\) and \(q = \frac{2}{1}\), yielding \(\frac{i}{q} \rightarrow \frac{3}{2}\). If this is still confusing, try thinking of \(\frac{i}{q}\) as \(i \times \frac{1}{q}\). The crucial observation is that on every passage through the loop, \(q\) is inverted, becoming \(\frac{1}{q}\).</p>
<p>If you unwind these operations, and look at the multiplications and divisions that go into each element of the series, a pattern emerges:</p>
<p>\[\frac{1}{1}, \quad \frac{2}{1}, \quad \frac{1 \cdot 3}{2}, \quad \frac{2 \cdot 4}{1 \cdot 3}, \quad \frac{1 \cdot 3 \cdot 5}{2 \cdot 4} \quad \frac{2 \cdot 4 \cdot 6}{1 \cdot 3 \cdot 5}\]</p>
<p class="undent">The general form is:</p>
<p>\[\frac{1 \cdot 3 \cdot 5 \cdot \cdots \cdot n}{2 \cdot 4 \cdot \cdots \cdot (n-1)} \quad (\text{odd } n) \qquad  \frac{2 \cdot 4 \cdot 6 \cdot \cdots \cdot n}{1 \cdot 3 \cdot 5 \cdot \cdots \cdot (n-1)} \quad (\text{even } n).<br/>
\]</p>
<hr/>
<p>The functions \(1 \cdot 3 \cdot 5 \cdot \cdots \cdot n\) for odd \(n\) and \(2 \cdot 4 \cdot 6 \cdot \cdots \cdot n\) for even \(n\) have a name! They are known as double factorials, with the notation \(n!!\). Terrible terminology, no? Better to have named them “semi-factorials.” And if I didn’t know better, I would read \(n!!\) as “the factorial of the factorial.” The double factorial of <em>n</em> is defined as the product of <em>n</em> and all smaller positive integers of the same parity. Thus our peculiar sequence of zigzag quotients is simply \(\frac{n!!}{(n-1)!!}\).</p>
<p>A <a href="https://www.tandfonline.com/doi/abs/10.4169/math.mag.85.3.177">2012 article</a> by Henry W. Gould and Jocelyn Quaintance (behind a paywall, regrettably) surveys the applications of double factorials. They turn up more often than you might guess. In the middle of the 17th century John Wallis came up with this identity:</p>
<p>\[\frac{\pi}{2} = \frac{2 \cdot 2 \cdot 4 \cdot 4 \cdot 6 \cdot 6 \cdots}{1 \cdot 3 \cdot 3 \cdot 5 \cdot 5 \cdot 7 \cdots} = \lim_{n \rightarrow \infty} \frac{((2n)!!)^2}{(2n + 1)!!(2n - 1)!!}\]</p>
<p class="undent">An even weirder series, involving the cube of a quotient of double factorials, sums to \(\frac{2}{\pi}\). That one was discovered by (who else?) Srinivasa Ramanujan.</p>
<p>Gould and Quaintance also discuss the double factorial counterpart of binomial coefficients. The standard binomial coefficient is defined as:</p>
<p>\[\binom{n}{k} = \frac{n!}{k! (n-k)!}.\]</p>
<p class="undent">The double version is:</p>
<p>\[\left(\!\binom{n}{k}\!\right) = \frac{n!!}{k!! (n-k)!!}.\]</p>
<p class="undent">Note that our zigzag numbers fit this description and therefore qualify as double factorial binomial coefficients. Specifically, they are the numbers:</p>
<p>\[\left(\!\binom{n}{1}\!\right) = \left(\!\binom{n}{n - 1}\!\right) = \frac{n!!}{1!! (n-1)!!}.\]</p>
<p class="undent">The regular binomial \(\binom{n}{1}\) is not very interesting; it is simply equal to \(n\). But the doubled version \(\left(\!\binom{n}{1}\!\right)\), as we’ve seen, dances a livelier jig. And, unlike the single binomial, it is not always an integer. (The only integer values are \(1\) and \(2\).)</p>
<p>Seeing the zigzag numbers as ratios of double factorials explains quite a few of their properties, starting with the alternation of evens and odds. We can also see why all the even numbers in the sequence are powers of 2. Consider the case of \(n = 6\). The numerator of this fraction is \(2 \cdot 4 \cdot 6 = 48\), which acquires a factor of \(3\) from the \(6\). But the denominator is \(1 \cdot 3 \cdot 5 = 15\). The \(3\)s above and below cancel, leaving \(\frac{16}{5}\). Such cancelations will happen in every case. Whenever an odd factor \(m\) enters the even sequence, it must do so in the form \(2 \cdot m\), but at that point \(m\) itself must already be present in the odd sequence.</p>
<hr/>
<p>Is the sequence of zigzag numbers a reasonable answer to the question, “What happens when you divide instead of multiply in \(n!\)?” Or is the computer program that generates them just a buggy algorithm? My personal judgment is that \(\frac{1}{n!}\) is a more intuitive answer, but \(\frac{n!!}{(n - 1)!!}\) is more interesting.</p>
<p>Furthermore, the mere existence of the zigzag sequence broadens our horizons. As noted above, if you insist that the division algorithm must always chug along the list of \(n\) factors in order, at each stop dividing the number on the left by the number on the right, then there are only \(n\) possible outcomes, and they all look much alike. But the zigzag solution suggests wilder possibilities. We can formulate the task as follows. Take the set of factors \(\{1 \dots n\}\), select a subset, and invert all the elements of that subset; now multiply all the factors, both the inverted and the upright ones. If the inverted subset is empty, the result is the ordinary factorial \(n!\). If <em>all</em> of the factors are inverted, we get the inverse \(\frac{1}{n!}\). And if every second factor is inverted, starting with \(n - 1\), the result is an element of the zigzag sequence.</p>
<p>These are only a few among the many possible choices; in total there are \(2^n\) subsets of \(n\) items. For example, you might invert every number that is prime or a power of a prime \((2, 3, 4, 5, 7, 8, 9, 11, \dots)\). For small \(n\), the result jumps around but remains consistently less than \(1\):</p>
<p><img alt="Prime powers" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2019/02/prime-powers.svg" width=""/></p>
<p class="undent">If I were to continue this plot to larger \(n\), however, it would take off for the stratosphere. Prime powers get sparse farther out on the number line.</p>
<hr/>
<p>Here’s a question. We’ve seen factorial variants that go to zero as \(n\) goes to infinity, such as \(1/n!\). We’ve seen other variants grow without bound as \(n\) increases, including \(n!\) itself, and the zigzag numbers. Are there any versions of the factorial process that converge to a finite bound other than zero?</p>
<p>My first thought was this algorithm:</p>
<pre class="language-julia"><code>function greedy_balance(n)
    q = 1
    while n &gt; 0
        q = q &gt; 1 ? q /= n : q *= n
        n -= 1
    end
    return q
end</code></pre>
<p class="undent">We loop through the integers from \(n\) down to \(1\), calculating the running product/quotient \(q\) as we go. At each step, if the current value of \(q\) is greater than \(1\), we divide by the next factor; otherwise, we multiply. This scheme implements a kind of feedback control or target-seeking behavior. If \(q\) gets too large, we reduce it; too small and we increase it. I conjectured that as \(n\) goes to infinity, \(q\) would settle into an ever-narrower range of values near \(1\).</p>
<p>Running the experiment gave me another surprise:</p>
<p><img alt="Greedy balance linear" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2019/02/greedy_balance_linear.svg" width=""/></p>
<p class="undent">That sawtooth wave is not quite what I expected. One minor peculiarity is that the curve is not symmetric around \(1\); the excursions above have higher amplitude than those below. But this distortion is more visual than mathematical. Because \(q\) is a ratio, the distance from \(1\) to \(10\) is the same as the distance from \(1\) to \(\frac{1}{10}\), but it doesn’t look that way on a linear scale. The remedy is to plot the log of the ratio:</p>
<p><img alt="Greedy balance" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2019/02/greedy_balance.svg" width=""/></p>
<p>Now the graph is symmetric, or at least approximately so, centered on \(0\), which is the logarithm of \(1\). But a larger mystery remains. The sawtooth waveform is very regular, with a period of \(4\), and it shows no obvious signs of shrinking toward the expected limiting value of \(\log q = 0\). Numerical evidence suggests that as \(n\) goes to infinity the peaks of this curve converge on a value just above \(q = \frac{5}{3}\), and the troughs approach a value just below \(q = \frac{3}{5}\). (The corresponding base-\(10\) logarithms are roughly \(\pm0.222\). I have not worked out why this should be so. Perhaps someone will explain it to me.</p>
<p>The failure of this greedy algorithm doesn’t mean we can’t find a divisive factorial that converges to \(q = 1\). If we work with the logarithms of the factors, this procedure becomes an instance of a well-known compu­tational problem called the number partitioning problem. You are given a set of real numbers and asked to divide it into two sets whose sums are equal, or as close to equal as possible. It’s a certifiably hard problem, but it has also been called (<a href="http://bit-player.org/bph-publications/AmSci-2002-03-Hayes-NPP.pdf">PDF</a>) “the easiest hard problem.”For any given \(n\), we might find that inverting some other subset of the factors gives a better approximation to \(n! = 1\). For small \(n\), we can solve the problem by brute force: Just look at all \(2^n\) subsets and pick the best one.</p>
<p>I have computed the optimal partitionings up to \(n = 30\), where there are a billion possibilities to choose from.</p>
<p><img alt="Optimum balance graph" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2019/02/optimum_balance_graph.svg" width=""/></p>
<p class="undent">The graph is clearly flatlining. You could use the same method to force convergence to any other value between \(0\) and \(n!\).</p>
<p>And thus we have yet another answer to the question in the tweet that launched this adventure. What happens when you divide instead of multiply in n!? Anything you want.</p></div>
    </content>
    <updated>2019-02-10T08:48:33Z</updated>
    <published>2019-02-10T08:48:33Z</published>
    <category scheme="http://bit-player.org" term="computing"/>
    <category scheme="http://bit-player.org" term="mathematics"/>
    <author>
      <name>Brian Hayes</name>
      <uri>http://bit-player.org</uri>
    </author>
    <source>
      <id>http://bit-player.org/feed/atom</id>
      <link href="http://bit-player.org" rel="alternate" type="text/html"/>
      <link href="http://bit-player.org/feed/atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">An amateur's outlook on computation and mathematics</subtitle>
      <title xml:lang="en-US">bit-player</title>
      <updated>2019-02-10T20:48:43Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra</id>
    <link href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html" rel="alternate" type="text/html"/>
    <title>Big convex polyhedra in grids</title>
    <summary>I recently wrote here about big convex polygons in grids, a problem for which we know very precise answers. This naturally raises the question: what about higher dimensions? How many vertices can be part of a convex polyhedron in an grid, or more generally a convex polytope in a -dimensional grid of side length ? Here we do still know some pretty good answers, at least up to constant factors in spaces of constant dimension.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I recently wrote here about <a href="https://11011110.github.io/blog/2018/09/05/big-convex-polygons.html">big convex polygons in grids</a>, a problem for which we know very precise answers. This naturally raises the question: what about higher dimensions? How many vertices can be part of a convex polyhedron in an  grid, or more generally a convex polytope in a -dimensional grid of side length ? Here we do still know some pretty good answers, at least up to constant factors in spaces of constant dimension.</p>

<p>The problem is included in a 2008 survey by Imre Bárány,<sup id="fnref:bar"><a class="footnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fn:bar">1</a></sup> according to whom the maximum number of vertices is</p>



<p>For instance, in three dimensional  grids the maximum number of vertices is .</p>

<p>One way to find polyhedra with this many vertices is to take the convex hull of the points in a ball,<sup id="fnref:bl"><a class="footnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fn:bl">2</a></sup> <sup id="fnref:bd"><a class="footnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fn:bd">3</a></sup> or in scaled copies of any fixed smooth convex body. Another way, which should generate polyhedra with a somewhat less irregular appearance and (up to constant factors) the same number of vertices, is to take the <a href="https://en.wikipedia.org/wiki/Minkowski_addition">Minkowski sum</a> of all line segments (up to scaling and translation) that will fit into a smaller grid, of side length . For instance, the <a href="https://en.wikipedia.org/wiki/Truncated_rhombicuboctahedron">truncated rhombicuboctahedron</a> below<sup id="fnref:ruen"><a class="footnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fn:ruen">4</a></sup> is the Minkowski sum of all the line segments that fit into a unit cube. Its 96 vertices lie in a  grid. In general, this method produces a <a href="https://en.wikipedia.org/wiki/Zonohedron">zonohedron</a> whose complexity can be analyzed in terms of a -dimensional arrangement of  hyperplanes. As long as this arrangement is not too degenerate (which it appears not to be, but I haven’t worked out the details carefully) this should give
a number of vertices within a constant factor of the number coming from the convex hull construction.</p>

<p style="text-align: center;"><img alt="Truncated rhombicuboctahedron" src="https://11011110.github.io/blog/assets/2019/truncated-rhombicuboctahedron2.png"/></p>

<p>A matching upper bound comes from a 1963 paper by G. K. Andrews,<sup id="fnref:and"><a class="footnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fn:and">5</a></sup> and Bárány writes that although several more proofs have been published none of them is easy. I’m not sure whether the difficulty is in getting the exact bound or in the fact that Andrews and the later proofs allow more general shapes with volume  that don’t fit into a grid, but it’s not hard to get close to the right bound simply by counting the number of possible facets of a given volume . By using <a href="https://en.wikipedia.org/wiki/Lenstra%E2%80%93Lenstra%E2%80%93Lov%C3%A1sz_lattice_basis_reduction_algorithm">lattice basis reduction</a> the integer vectors in the hyperplane through any facet have a nearly-orthogonal basis whose product of lengths is proportional to . By considering how this product of lengths can be broken down into factors of different scales, and counting how many integer vectors of those lengths exist, it follows that the number of possible facets of volume  is . Combining this with the  surface area of a grid polytope gives the correct upper bound on the number of vertices up to a polylog factor.</p>

<p>What about when the dimension is not constant? An easy construction for high dimensions is to take all points with a fixed distance  from the grid center. There are  possible values for the distance, so this construction produces a convex polytope with  vertices. It comes from a 1946 paper by Behrend, who uses this idea to find <a href="https://en.wikipedia.org/wiki/Salem%E2%80%93Spencer_set">dense sets of integers with no arithmetic progressions</a>.<sup id="fnref:beh"><a class="footnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fn:beh">6</a></sup>
It is never worse to use the convex hull of the ball than the points on a sphere,
and a celebrated paper by Elkin from 2011 (in the appendix of the published version) gives another proof of the  bound for convex hulls of balls (for ) in which the constant factor of the  is universal, not depending on . So when  is singly exponential in ,  becomes constant and the convex hull technique produces  vertices, improving Behrend’s construction for progression-free sets by the same  factor.<sup id="fnref:elk"><a class="footnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fn:elk">7</a></sup></p>

<div class="footnotes">
  <ol>
    <li id="fn:bar">
      <p>Bárány, Imre (2008), “Extremal problems for convex lattice polytopes: a survey”, <em>Surveys on Discrete and Computational Geometry</em>, Contemporary Mathematics 453, Amer. Math. Soc., pp. 87–103, <a href="https://doi.org/10.1090/conm/453/08796">doi:10.1090/conm/453/08796</a>, <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=2405678">MR2405678</a>. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fnref:bar">↩</a></p>
    </li>
    <li id="fn:bl">
      <p>Bárány, Imre and Larman, David (1998), “The convex hull of the integer points in a large ball”, <em>Math. Ann.</em> 312 (1), pp. 167–181, <a href="https://doi.org/10.1007/s002080050217">doi:10.1007/s002080050217</a>, <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=1645957">MR1645957</a>. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fnref:bl">↩</a></p>
    </li>
    <li id="fn:bd">
      <p>Balog, Antal and Deshouillers, Jean-Marc (1999), “On some convex lattice polytopes”. <em>Number Theory in Progress</em>, Vol. 2 (Zakopane-Kościelisko, 1997), de Gruyter, pp. 591–606, <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=1689533">MR1689533</a>. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fnref:bd">↩</a></p>
    </li>
    <li id="fn:ruen">
      <p>Ruen, Tom (2014), “Truncated rhombicuboctahedron”, CC-BY-SA 4.0, <a href="https://commons.wikimedia.org/wiki/File:Truncated_rhombicuboctahedron2.png">File:Truncated rhombicuboctahedron2.png</a> on Wikimedia commons. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fnref:ruen">↩</a></p>
    </li>
    <li id="fn:and">
      <p>Andrews, George E. (1963), “A lower bound for the volume of strictly convex bodies with many boundary lattice points”, <em>Trans. Amer. Math. Soc.</em> 106, pp. 270–279, <a href="https://doi.org/10.2307/1993769">doi:10.2307/1993769</a>, <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=0143105">MR0143105</a>. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fnref:and">↩</a></p>
    </li>
    <li id="fn:beh">
      <p>Behrend, F. A. (1946), “On sets of integers which contain no three terms in arithmetical progression”, <em>Proc. Nat. Acad. Sci.</em> 32 (12), pp. 331–332, <a href="https://doi.org/10.1073/pnas.32.12.331">doi:10.1073/pnas.32.12.331</a>, <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=0018694">MR0018694</a>. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fnref:beh">↩</a></p>
    </li>
    <li id="fn:elk">
      <p>Elkin, Michael (2011), “An improved construction of progression-free sets”, <em>Israel J. Math.</em> 184, pp. 93–128, <a href="https://arxiv.org/abs/0801.4310">arXiv:0801.4310</a>, <a href="https://doi.org/10.1007%2Fs11856-011-0061-1">doi:10.1007/s11856-011-0061-1</a>, <a href="https://www.ams.org/mathscinet-getitem?mr=2823971">MR2823971</a>. The paragraph describing Elkin’s results was updated from an earlier more tentative version in the original post. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/02/09/big-convex-polyhedra.html#fnref:elk">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/101564963348879092">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-02-09T15:07:00Z</updated>
    <published>2019-02-09T15:07:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-02-11T18:35:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1087</id>
    <link href="https://ptreview.sublinear.info/?p=1087" rel="alternate" type="text/html"/>
    <title>News for January 2019</title>
    <summary>Minimax Testing of Identity to a Reference Ergodic Markov Chain, by Geoffrey Wolfer and Aryeh Kontorovich (arXiv). This work studies distributional identity testing on Markov chains from a single trajectory, as recently introduced by Daskalakis, Dikkala, and Gravin: we wish to test whether a Markov chain is equal to some reference chain, or far from […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Minimax Testing of Identity to a Reference Ergodic Markov Chain</strong>, by Geoffrey Wolfer and Aryeh Kontorovich (<a href="https://arxiv.org/abs/1902.00080">arXiv</a>). This work studies distributional identity testing on Markov chains from a single trajectory, as recently introduced by <a href="https://arxiv.org/abs/1704.06850">Daskalakis, Dikkala, and Gravin</a>: we wish to test whether a Markov chain is equal to some reference chain, or far from it. This improves on previous work by considering a stronger distance measure than before, and showing that the sample complexity only depends on properties of the reference chain (which we are trying to test identity to). It additionally proves instance-by-instance bounds (where the sample complexity depends on properties of the specific chain we wish to test identity to).</p>



<p><strong>Almost Optimal Distribution-free Junta Testing</strong>, by Nader H. Bshouty (<a href="https://arxiv.org/abs/1901.00717">arXiv</a>). This paper provides a \(\tilde O(k/\varepsilon)\)-query algorithm with two-sided error for testing if a Boolean function is a \(k\)-junta (that is, its value depends only on \(k\) of its variables) in the distribution-free model (where distance is measured with respect to an unknown distribution from which we can sample). This complexity is a quadratic improvement over the \(\tilde O(k^2)/\varepsilon\)-query algorithm of <a href="https://arxiv.org/abs/1802.04859">Chen, Liu, Servedio, Sheng, and Xie</a>. This complexity is also near-optimal, as shown in a lower bound by Saglam (which we covered back in <a href="https://ptreview.sublinear.info/?p=1030">August</a>).</p>



<p><strong>Exponentially Faster Massively Parallel Maximal Matching</strong>, by Soheil Behnezhad, MohammadTaghi Hajiaghayi, and David G. Harris (<a href="https://arxiv.org/abs/1901.03744">arXiv</a>). The authors consider maximal matching in the Massively Parallel Computation (MPC) model. They show that one can compute a maximal matching in \(O(\log \log \Delta)\)-rounds, with \(O(n)\) space per machine. This is an exponential improvement over the previous works, which required either \(\Omega(\log n)\) rounds or \(n^{1 + \Omega(1)}\) space per machine. Corollaries of their result include approximation algorithms for vertex cover, maximum matching, and weighted maximum matching. </p></div>
    </content>
    <updated>2019-02-08T18:30:54Z</updated>
    <published>2019-02-08T18:30:54Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2019-02-14T23:49:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3379</id>
    <link href="https://agtb.wordpress.com/2019/02/08/acm-sigecom-elections/" rel="alternate" type="text/html"/>
    <title>ACM SIGecom Elections</title>
    <summary>The following message just went out to ACM SIGecom members: ———- Forwarded message ——— From: Monique Chang &lt;chang@hq.acm.org&gt; Date: Mon, Feb 4, 2019 at 2:20 PM Subject: 2019 ACM SIGecom Election: Candidate Slate Announcement To: &lt;SIG-ELECTION-ANNOUNCEMENT@listserv.acm.org&gt; Dear ACM SIGecom Member, The ACM SIGecom Nominating Committee has proposed the following candidates for the 2019 ACM SIGecom election. Chair […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="font-weight: 400;">The following message just went out to ACM SIGecom members:</span></p>
<blockquote><p>———- Forwarded message ———<br/>
From: <strong>Monique Chang</strong> &lt;<a href="mailto:chang@hq.acm.org">chang@hq.acm.org</a>&gt;<br/>
Date: Mon, Feb 4, 2019 at 2:20 PM<br/>
Subject: 2019 ACM SIGecom Election: Candidate Slate Announcement<br/>
To: &lt;<a href="mailto:SIG-ELECTION-ANNOUNCEMENT@listserv.acm.org">SIG-ELECTION-ANNOUNCEMENT@listserv.acm.org</a>&gt;</p>
<p>Dear ACM SIGecom Member,</p>
<p>The ACM SIGecom Nominating Committee has proposed the following candidates for the 2019 ACM SIGecom election.</p>
<p><strong><u>Chair</u></strong><br/>
(Running Unopposed)</p>
<p>Nicole Immorlica</p>
<p><strong><u>Vice-Chair</u></strong></p>
<p>Scott Kominers</p>
<p>Ariel Procaccia</p>
<p><strong><u>Secretary-Treasurer</u></strong></p>
<p>Hu Fu</p>
<p>Katrina Ligett</p>
<p>In accordance with the ACM SIG Bylaws, additional candidates may be placed on the ballot by petition. All candidates must be ACM Professional Members, as well as members of the SIG. Anyone interested in petitioning must inform ACM Headquarters, Pat Ryan (<a href="mailto:ryanp@hq.acm.org">ryanp@hq.acm.org</a>), and SIGecom’s Secretary-Treasurer, Jenn Wortman Vaughan (<a href="mailto:jenn@microsoft.com">jenn@microsoft.com</a>), of their intent to petition by <strong>15 March 2019</strong>. Petitions must be submitted to ACM Headquarters for verification by <strong>2 April 2019</strong>.</p>
<p>Monique Chang</p>
<p>ACM SIG Elections Coordinator</p>
<p>Office of Policy and Administration</p></blockquote>
<p><span style="font-weight: 400;">Three things for members of our community to note:</span></p>
<ol>
<li style="font-weight: 400;">It’s important vote (once the link goes out; note that the current email is just an announcement and an invitation for additional candidates to petition to be included on the ballot). The SIG leadership is very important for the ongoing direction of our organization. Your vote makes a difference, because our elections are often decided by small margins.</li>
</ol>
<ol start="2">
<li style="font-weight: 400;">If you didn’t get this email, you’re likely not registered as a member of our SIG. Membership costs only $5 for students and $10 for others; AFAIK, you don’t have to be an ACM member to be a SIG member. Our number of members is an important signal to the ACM about the strength of our community (which is why we have set our fees so low). Votes like this one are also restricted to members! If your membership has lapsed, or if you’ve never taken the plunge, this might be a good occasion to do so, by clicking on the link below:</li>
</ol>
<p style="font-weight: 400;"><a href="https://www.acm.org/special-interest-groups/sigs/sigecom">https://www.acm.org/special-interest-groups/sigs/sigecom</a></p>
<ol start="3">
<li style="font-weight: 400;">Thanks to our nominations chair, David Parkes, who put together the slate of candidates just listed, and also to all of the candidates who agreed to serve. Our community is really lucky to have such a strong and deep pool of volunteers, and this is one more example. Indeed, in advance, I’d particularly like to thank those candidates who *don’t* win, whoever they turn out to be: it’s thankless to stick one’s neck out for an election only to see someone else get chosen (often by a small margin; see #1), but your willingness to serve is much appreciated.</li>
</ol></div>
    </content>
    <updated>2019-02-08T02:44:19Z</updated>
    <published>2019-02-08T02:44:19Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Kevin Leyton-Brown</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-02-15T15:20:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/017</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/017" rel="alternate" type="text/html"/>
    <title>TR19-017 |  Fourier bounds and pseudorandom generators for product tests | 

	Chin Ho Lee</title>
    <summary>We study the Fourier spectrum of functions $f\colon \{0,1\}^{mk} \to \{-1,0,1\}$ which can be written as a product of $k$ Boolean functions $f_i$ on disjoint $m$-bit inputs.  We prove that for every positive integer $d$,
\[
  \sum_{S \subseteq [mk]: |S|=d} |\hat{f_S}| = O(m)^d .
\]
Our upper bound is tight up to a constant factor in the $O(\cdot)$.  Our proof builds on a new "level-$d$ inequality" that bounds above $\sum_{|S|=d} \hat{f_S}^2$ for any $[0,1]$-valued function $f$ in terms of its expectation, which may be of independent interest.

As a result, we construct pseudorandom generators for such functions with seed length $\tilde O(m + \log(k/\varepsilon))$, which is optimal up to polynomial factors in $\log m$, $\log\log k$ and $\log\log(1/\varepsilon)$.  Our generator in particular works for the well-studied class of combinatorial rectangles, where in addition we allow the bits to be read in any order.  Even for this special case, previous generators have an extra $\tilde O(\log(1/\varepsilon))$ factor in their seed lengths. 

Using Schur-convexity, we also extend our results to functions $f_i$ whose range is $[-1,1]$.</summary>
    <updated>2019-02-07T14:59:22Z</updated>
    <published>2019-02-07T14:59:22Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-02-15T15:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/016</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/016" rel="alternate" type="text/html"/>
    <title>TR19-016 |  The hardest halfspace | 

	Alexander A. Sherstov</title>
    <summary>We study the approximation of halfspaces $h:\{0,1\}^n\to\{0,1\}$ in the infinity norm by polynomials and rational functions of any given degree.  Our main result is an explicit construction of the "hardest" halfspace, for which we prove polynomial and rational approximation lower bounds that match the trivial upper bounds achievable for all halfspaces.  This completes a lengthy line of work started by Myhill and Kautz (1961).

As an application, we construct a communication problem with essentially the largest possible gap, of $n$ versus $2^{-\Omega(n)},$ between the sign-rank and discrepancy. Equivalently, our problem exhibits a gap of $\log n$ versus $\Omega(n)$ between the communication complexity with unbounded versus weakly unbounded error, improving quadratically on previous constructions and completing a line of work started by Babai, Frankl, and Simon (FOCS 1986). Our results further generalize to the $k$-party number-on-the-forehead model, where we obtain an explicit separation of $\log n$ versus $\Omega(n/4^{n})$ for communication with unbounded versus weakly unbounded error. This gap is a quadratic improvement on previous work and matches the state of the art for number-on-the-forehead lower bounds.</summary>
    <updated>2019-02-07T14:57:13Z</updated>
    <published>2019-02-07T14:57:13Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-02-15T15:20:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-971154734717743655</id>
    <link href="https://blog.computationalcomplexity.org/feeds/971154734717743655/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/02/an-immerman-szelepcsenyi-story.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/971154734717743655" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/971154734717743655" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/02/an-immerman-szelepcsenyi-story.html" rel="alternate" type="text/html"/>
    <title>An Immerman-Szelepcsényi Story</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">As a grad student in the late 80's I had the opportunity to witness many great and often surprising theorems in computational complexity. Let me tell you about one of them, the Immerman-Szelepcsényi result that <a href="https://blog.computationalcomplexity.org/2003/06/foundations-of-complexity-lesson-19.html">nondeterministic space is closed under complement</a>. I wish I had the original emails for this story but instead I'm working from memory and apologies if I get some of the details wrong. I'm expanding from a <a href="https://blog.computationalcomplexity.org/2002/08/last-spring-i-saw-copenhagen-great.html">short version</a> from the early days of this blog.<br/>
<br/>
I started my graduate work at UC Berkeley in 1985 and then moved to MIT in the summer of '86, following my advisor Michael Sipser. In the summer of 1987, Neil Immerman, then at Yale, proved his famous result building on his work in <a href="https://en.wikipedia.org/wiki/Descriptive_complexity_theory">descriptive complexity</a> In those days you didn't email papers, he made copies and sent them by US postal mail to several major researchers in complexity including Sipser. But Sipser was away for the summer, I believe in Russia, and the paper sat in his office.<br/>
<br/>
Immerman also sent the paper to a Berkeley professor, probably Manuel Blum, who gave it to one of his students who decided to speak about the result in a student-led seminar. I forgot who was the student, maybe Moni Naor. I was still on the Berkeley email list so I got the talk announcement and went into complexity ecstasy over the news. I asked Moni (or whomever was giving the talk) if he could tell me details and he sent me a nice write-up of the proof. Given the importance of the result, I sent the proof write-up out to the MIT theory email list.<br/>
<br/>
Guess who was on the MIT theory list? Neil Immerman. Neil wrote back with his own explanation of the proof. Neil explained how it came out of descriptive complexity but as a pure write-up of a proof of the theorem, Moni did an excellent job.<br/>
<br/>
We found out about Robert Szelepcsényi when his paper showed up a few months later in the Bulletin of the European Association for Theoretical Computer Science. Szelepcsényi came to the problem from formal languages, whether context-sensitive languages (nondeterministic linear space) was closed under complement. Szelepcsényi, an undergrad in Slovakia at the time, heard about the problem in a class he took. Szelepcsényi's proof was very similar to Immerman. Szelepcsényi's paper took longer to get to US researchers but likely was proven and written about the same time as Immerman.<br/>
<br/>
Even though both papers were <a href="https://doi.org/10.1137/0217058">published</a> <a href="https://doi.org/10.1007/BF00299636">separately</a> we refer to the result as Immerman-Szelepcsényi and is now just some old important theorem you see in introductory theory classes.</div>
    </content>
    <updated>2019-02-07T12:47:00Z</updated>
    <published>2019-02-07T12:47:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>https://plus.google.com/101693130490639305932</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>https://plus.google.com/101693130490639305932</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-02-14T12:58:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/015</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/015" rel="alternate" type="text/html"/>
    <title>TR19-015 |  QMA Lower Bounds for Approximate Counting | 

	William Kretschmer</title>
    <summary>We prove a query complexity lower bound for $QMA$ protocols that solve approximate counting: estimating the size of a set given a membership oracle. This gives rise to an oracle $A$ such that $SBP^A \not\subset QMA^A$, resolving an open problem of Aaronson [2]. Our proof uses the polynomial method to derive a lower bound for the $SBQP$ query complexity of the $AND$ of two approximate counting instances. We use Laurent polynomials as a tool in our proof, showing that the "Laurent polynomial method" can be useful even for problems involving ordinary polynomials.</summary>
    <updated>2019-02-07T08:16:08Z</updated>
    <published>2019-02-07T08:16:08Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-02-15T15:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15634</id>
    <link href="https://rjlipton.wordpress.com/2019/02/06/an-old-but-cool-result/" rel="alternate" type="text/html"/>
    <title>An Old But Cool Result</title>
    <summary>Solving a type of Fermat Equation Leo Moser was a mathematician who worked on a very varied set of problems. He for example raised a question about “worms,” and invented a notation for huge numbers. Today I want to talk about one of his results with a very short proof. No, it is not about […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p/><p>
<font color="#0044cc"><br/>
<em>Solving a type of Fermat Equation</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/02/06/an-old-but-cool-result/unknown-117/" rel="attachment wp-att-15639"><img alt="" class="alignright size-full wp-image-15639" src="https://rjlipton.files.wordpress.com/2019/02/unknown.jpeg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"/></td>
</tr>
</tbody>
</table>
<p>
Leo Moser was a <a href="https://en.wikipedia.org/wiki/Leo_Moser">mathematician</a> who worked on a very varied set of problems. He for example raised a question about “worms,” and invented a notation for huge <a href="https://en.wikipedia.org/wiki/Steinhaus-Moser_notation">numbers</a>.</p>
<p>
Today I want to talk about one of his results with a very short proof.</p>
<p>
No, it is not about worms. That is a <a href="https://en.wikipedia.org/wiki/Moser%27s_worm_problem">question</a> in discrete geometry that is still open I believe: “What is the region of smallest area which can accommodate every planar arc of length one?” The region must be able to hold the arc inside but the curve can be moved and rotated to allow it to fit. A disk of diameter <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> works and has area about <img alt="{ 0.78}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+0.78%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ 0.78}"/>. It is possible to do much better and get around <img alt="{ 0.27}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+0.27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ 0.27}"/>. </p>
<p><a href="https://rjlipton.wordpress.com/2019/02/06/an-old-but-cool-result/worm3/" rel="attachment wp-att-15636"><img alt="" class="aligncenter size-medium wp-image-15636" height="143" src="https://rjlipton.files.wordpress.com/2019/02/worm3.png?w=300&amp;h=143" width="300"/></a></p>
<p>See this <a href="https://www.nada.kth.se/~johanh/snakes.pdf">paper</a> for some additional details.</p>
<p>
No, it is not about a conjecture of Paul Erdős See <a href="https://arxiv.org/pdf/1011.2956.pdf">this</a> for a great paper on this result: </p>
<blockquote><p><b>Theorem 1</b> <em> Suppose that 	</em></p><em>
<p align="center"><img alt="\displaystyle  1^{k} + 2^{k} + \cdots + (m-1)^{k} = m^{k}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%5E%7Bk%7D+%2B+2%5E%7Bk%7D+%2B+%5Ccdots+%2B+%28m-1%29%5E%7Bk%7D+%3D+m%5E%7Bk%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  1^{k} + 2^{k} + \cdots + (m-1)^{k} = m^{k}. "/></p>
<p>Then any <img alt="{(m,k)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28m%2Ck%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{(m,k)}"/> solution in integers with <img alt="{k \ge 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cge+2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{k \ge 2}"/> must have 	</p>
<p align="center"><img alt="\displaystyle  m &gt; 10^{10^{6}}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++m+%3E+10%5E%7B10%5E%7B6%7D%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  m &gt; 10^{10^{6}}. "/></p>
</em><p><em/>
</p></blockquote>
<p>Erdős conjectured there are no solutions at all. It is easy to check that for <img alt="{k=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k=1}"/> the unique solution is a bit smaller: 	</p>
<p align="center"><img alt="\displaystyle  1 + 2 = 3." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+%2B+2+%3D+3.&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  1 + 2 = 3."/></p>
<p>
</p><p/><h2> The Result </h2><p/>
<p/><p>
Yes, it is about the solution to a natural family of Diophantine equations. This result of Moser comes from an old paper of his. The result can be found on the wonderful blog called <a href="https://www.cut-the-knot.org/arithmetic/algebra/TwoParameterFermat.shtml">cut-the-knot</a> written by Alexander Bogomolny.</p>
<p>
The question considered by Moser is simple to state: </p>
<blockquote><p><b> </b> <em> Consider the equation over the integers <img alt="{x^{a} + y^{b} = z^{c}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Ba%7D+%2B+y%5E%7Bb%7D+%3D+z%5E%7Bc%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x^{a} + y^{b} = z^{c}}"/> where <img alt="{a, b, c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%2C+b%2C+c%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{a, b, c}"/> are fixed values that are relatively prime. Show that there are infinitely many integer solutions. </em>
</p></blockquote>
<p/><p>
The surprise, to me, is that this equation always has integer solutions. I thought about it for a bit and had no idea how to even start.</p>
<p>
The solution is as follows. The initial insight is that the restriction on the exponents implies that there are integers <img alt="{m, n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%2C+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m, n}"/> so that <img alt="{abm + 1 = cn}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Babm+%2B+1+%3D+cn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{abm + 1 = cn}"/>. </p>
<p>
Wait a minute. We must be careful by what we mean by “the values of <img alt="{a,b,c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a,b,c}"/> are relatively prime.” We need more than the greatest common divisor (GCD) of <img alt="{a,b,c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a,b,c}"/> is <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. We need that <img alt="{ab}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bab%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ab}"/> and <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> are relatively prime. Note that <img alt="{6,10,15}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B6%2C10%2C15%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{6,10,15}"/> have GCD equal to <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> but no matter which of the triple is “<img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/>” we cannot find the needed <img alt="{m,n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%2Cn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m,n}"/>: 	</p>
<p align="center"><img alt="\displaystyle  (6\cdot 10,15) &gt; 1, (10\cdot 15, 6)&gt;1, (15,6 \cdot 10) &gt; 1. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%286%5Ccdot+10%2C15%29+%3E+1%2C+%2810%5Ccdot+15%2C+6%29%3E1%2C+%2815%2C6+%5Ccdot+10%29+%3E+1.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (6\cdot 10,15) &gt; 1, (10\cdot 15, 6)&gt;1, (15,6 \cdot 10) &gt; 1. "/></p>
<p>I thank Subrahmanyam Kalyanasundaram for catching this.</p>
<p>
The next idea is not to look for a single set of solutions but rather to find a parametrized solution. That is try to find expressions for <img alt="{x,y,z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y,z}"/> that depend on some variables <img alt="{u,v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%2Cv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u,v}"/> so that for all <img alt="{u,v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%2Cv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u,v}"/> the equation is satisfied. </p>
<p>
Then set 	</p>
<p align="center"><img alt="\displaystyle  x = u^{bm}(u^{abm} + v^{abm})^{bm}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+u%5E%7Bbm%7D%28u%5E%7Babm%7D+%2B+v%5E%7Babm%7D%29%5E%7Bbm%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x = u^{bm}(u^{abm} + v^{abm})^{bm}. "/></p>
<p align="center"><img alt="\displaystyle  y = v^{am}(u^{abm} + v^{abm})^{am}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y+%3D+v%5E%7Bam%7D%28u%5E%7Babm%7D+%2B+v%5E%7Babm%7D%29%5E%7Bam%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y = v^{am}(u^{abm} + v^{abm})^{am}. "/></p>
<p>Note as <img alt="{u,v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%2Cv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u,v}"/> vary over integers the values of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> vary over integers too. The claim is that this is a parameterization of the equation. Let’s see why. We need to figure out what <img alt="{x^{a} + y^{b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Ba%7D+%2B+y%5E%7Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^{a} + y^{b}}"/> is equal to. It looks a bit nasty but it is not. Let <img alt="{W = u^{abm} + v^{abm}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW+%3D+u%5E%7Babm%7D+%2B+v%5E%7Babm%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W = u^{abm} + v^{abm}}"/>. Then 	</p>
<p align="center"><img alt="\displaystyle  x = u^{bm}W^{bm}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+u%5E%7Bbm%7DW%5E%7Bbm%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x = u^{bm}W^{bm}. "/></p>
<p align="center"><img alt="\displaystyle  y = v^{am}W^{am}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y+%3D+v%5E%7Bam%7DW%5E%7Bam%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y = v^{am}W^{am}. "/></p>
<p>So <img alt="{x^{a} + y^{b} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Ba%7D+%2B+y%5E%7Bb%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^{a} + y^{b} }"/> is 	</p>
<p align="center"><img alt="\displaystyle  u^{abm} W^{abm} + v^{abm}W^{abm}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++u%5E%7Babm%7D+W%5E%7Babm%7D+%2B+v%5E%7Babm%7DW%5E%7Babm%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  u^{abm} W^{abm} + v^{abm}W^{abm}. "/></p>
<p>Which magically is 	</p>
<p align="center"><img alt="\displaystyle  W^{abm}(u^{abm} + v^{abm}) = W^{abm+1}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W%5E%7Babm%7D%28u%5E%7Babm%7D+%2B+v%5E%7Babm%7D%29+%3D+W%5E%7Babm%2B1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W^{abm}(u^{abm} + v^{abm}) = W^{abm+1}. "/></p>
<p>Thus setting 	</p>
<p align="center"><img alt="\displaystyle  z = W^{n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++z+%3D+W%5E%7Bn%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  z = W^{n} "/></p>
<p>implies that 	</p>
<p align="center"><img alt="\displaystyle  x^{a} + y^{b} = W^{abm+1} = W^{cn} = (W^{n})^{c} = z^{c}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7Ba%7D+%2B+y%5E%7Bb%7D+%3D+W%5E%7Babm%2B1%7D+%3D+W%5E%7Bcn%7D+%3D+%28W%5E%7Bn%7D%29%5E%7Bc%7D+%3D+z%5E%7Bc%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^{a} + y^{b} = W^{abm+1} = W^{cn} = (W^{n})^{c} = z^{c}. "/></p>
<p>Very neat. By the way we do need to note that as <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> run through integers the values of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> and <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> vary enough to get an infinite number of solutions. A simple growth argument shows that this is true.</p>
<p>
The key trick was to <b>not</b> use a standard idea and apply the binomial theorem and expand 	</p>
<p align="center"><img alt="\displaystyle  (u^{abm} + v^{abm})^{bm}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28u%5E%7Babm%7D+%2B+v%5E%7Babm%7D%29%5E%7Bbm%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (u^{abm} + v^{abm})^{bm}. "/></p>
<p>My algebra DNA suggests that expanding such an expression is often a good idea. Here it would lead to a mess. This is a case where using the binomial expansion does not work.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I really like Moser’s clever solution to the diophantine equation 	</p>
<p align="center"><img alt="\displaystyle  x^{a} + y^{b} = z^{c}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7Ba%7D+%2B+y%5E%7Bb%7D+%3D+z%5E%7Bc%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^{a} + y^{b} = z^{c}. "/></p>
<p>Note that it must fail when <img alt="{a=b=c=p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%3Db%3Dc%3Dp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a=b=c=p}"/> for <img alt="{p&gt;2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%3E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p&gt;2}"/> by the famous solution to the original Fermat equation. </p></font></font></div>
    </content>
    <updated>2019-02-06T12:57:11Z</updated>
    <published>2019-02-06T12:57:11Z</published>
    <category term="Oldies"/>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Diophantine"/>
    <category term="Fermat"/>
    <category term="worm problem"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-02-15T15:20:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1062</id>
    <link href="http://corner.mimuw.edu.pl/?p=1062" rel="alternate" type="text/html"/>
    <title>HALG 2019 - Call For Submissions of Short Contributed Presentations</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The HALG 2019 conference seeks submissions for contributed presentations. Each presentation is expected to consist of a poster and a short talk (an invitation to the poster). There will be no conference proceedings, hence presenting work already published at a … <a href="http://corner.mimuw.edu.pl/?p=1062">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The HALG 2019 conference seeks submissions for contributed presentations. Each presentation is expected to consist of a poster and a short talk (an invitation to the poster). There will be no conference proceedings, hence presenting work already published at a different venue or journal (or to be submitted there) is welcome.</p>
<p>If you would like to present your results at HALG 2019, please submit their details the abstract of the talk or the contribution of the poster via EasyChair: <a href="https://easychair.org/conferences/?conf=halg2019" rel="noopener noreferrer" target="_blank">https://easychair.org/conferences/?conf=halg2019</a></p>
<p>The abstract should include (when relevant) information where the results have been published/accepted (e.g., conference), and where they are publicly available (e.g., arXiv). All submissions will be reviewed by the program committee, giving priority to new work not formally published yet, and to papers published in 2018 or later.</p>
<p>Submissions deadline: March 15th, 2019.<br/>
Late submissions will be accepted subject to space constraints.</p></div>
    </content>
    <updated>2019-02-06T11:41:10Z</updated>
    <published>2019-02-06T11:41:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2019-02-14T23:48:41Z</updated>
    </source>
  </entry>
</feed>
