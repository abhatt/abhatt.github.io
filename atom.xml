<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-05-01T10:22:04Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=3460</id>
    <link href="https://francisbach.com/gradient-flows/" rel="alternate" type="text/html"/>
    <title>Effortless optimization through gradient flows</title>
    <summary>Optimization algorithms often rely on simple intuitive principles, but their analysis quickly leads to a lot of algebra, where the original idea is not transparent. In last month post, Adrien Taylor explained how convergence proofs could be automated. This month, I will show how proof sketches can be obtained easily for algorithms based on gradient...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Optimization algorithms often rely on simple intuitive principles, but their analysis quickly leads to a lot of algebra, where the original idea is not transparent. In last month <a href="https://francisbach.com/computer-aided-analyses/">post</a>, <a href="https://www.di.ens.fr/~ataylor/">Adrien Taylor</a> explained how convergence proofs could be automated. This month, I will show how proof sketches can be obtained easily for algorithms based on gradient descent. This will be done using vanishing step-sizes that lead to <em>gradient flows</em>.</p>



<h2>Gradient as local information</h2>



<p class="justify-text">The intuitive principle behind gradient descent is the quest for <em>local</em> descent. We thus need to characterize the local behavior of the function we aim to optimize. This is what gradients are for.</p>



<p class="justify-text">In this blog post, I will consider minimizing a function \(f\) over \(\mathbb{R}^d\). Assuming \(f\) is differentiable, a first order Taylor expansion of \(f\) around a point \(x\) leads to $$f(x+\delta) = f(x) + \nabla f(x) ^\top \delta + o(\| \delta\|),$$ for any norm \(\| \cdot \|\) on \(\mathbb{R}^d\), where \(\nabla f(x) \in \mathbb{R}^d\) is  the gradient of \(f\) at \(x\), composed of partial derivatives of \(f\). Therefore, around \(x\), \(f\) is approximately affine.</p>



<p class="justify-text">Since we have a local affine approximation around \(x\), we can look for the direction of steepest descent, that is, the unit norm vector \(u \in \mathbb{R}^d\) such that \(f\) decays the most along \(u\), that is such that $$  u^\top \nabla f(x)$$ is minimized. This steepest descent direction depends on the choice of norm (assuming that the gradient is not zero at \(x\)).</p>



<p class="justify-text">For the \(\ell_2\)-norm, then minimizing \(u^\top \nabla f(x)\) such that \(\|u\|_2 = 1\), leads to $$ \displaystyle u = \ – \frac{\nabla f(x)}{ \| \nabla f(x) \|_2},$$ that is the steepest descent is along the negative gradient (see an illustration below). In this blog post I will only focus on this steepest descent direction. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3542" height="149" src="https://francisbach.com/wp-content/uploads/2020/04/gradient_contours-1024x358.png" width="428"/>Function \(f\) represented through its contour lines for values 1, 2, 3, 4 and 5. Negative gradient \(– \nabla f(x)\) as the steepest descent direction at point \(x\), which is orthogonal to the contour lines.</figure></div>



<p class="justify-text">As as side note, for the \(\ell_1\)-norm, then minimizing \(u^\top \nabla f(x)\) such that \(\|u\|_1 = 1\), leads to $$u \in\  – \arg\max_{ v \in \{-e_1,\, e_1,\, -e_2,\, e_2,\dots,\, -e_d,\, e_d \}} v^\top \nabla f(x),$$ where \(e_i\) is the \(i\)-th canonical basis vector of \(\mathbb{R}^d\). Here the steepest descent is along a coordinate axis (along the positive or negative side), and this leads to various forms of <a href="https://en.wikipedia.org/wiki/Coordinate_descent">coordinate descent</a> (this will probably be a topic for another post). </p>



<p class="justify-text">Given that the negative gradient leads to the steepest descent direction (for the Euclidean norm), it is natural to use this as a direction for an iterative algorithm, an idea that dates back to <a href="https://en.wikipedia.org/wiki/Augustin-Louis_Cauchy">Cauchy</a> in 1847 [<a href="http://gallica.bnf.fr/ark:/12148/bpt6k90190w/f406">1</a>] (see the nice summary by Claude Le Maréchal [<a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf">2</a>]).</p>



<h2>From gradient descent to gradient flows</h2>



<p class="justify-text"><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient descent</a> is the most classical iterative algorithm to minimize differentiable functions. It takes the form $$x_{n+1} = x_{n} \, – \gamma \nabla f(x_{n})$$ at iteration \(n\), where \(\gamma &gt; 0 \) is a step-size.  </p>



<p class="justify-text">Gradient descent comes in many flavors, steepest, stochastic, pre-conditioned, conjugate, proximal, projected, accelerated, etc. There are lots of papers and books [e.g., 3, 4, 5] analyzing it in various settings.</p>



<p class="justify-text">In this post, to simplify its analysis and setting the stage for later posts, I will present the gradient flow, which is essentially the limit of gradient descent when the step-size \(\gamma\) tends to zero.</p>



<p class="justify-text">More precisely, this is obtained by considering that our iterates \(x_n\) are sampled at each multiple of \(\gamma\), from a function \(X: \mathbb{R}_+ \to \mathbb{R}^d\), as $$x_n = X(n\gamma).$$ We can then use a piecewise affine interpolation to define a function defined on all points. We then have for \(t = n\gamma\), $$X(t + \gamma) = x_{n+1} =x_{n} \, – \gamma \nabla f(x_{n}) = X(t)\, – \gamma \nabla f(X(t)).$$ Dividing by \(\gamma\), we get $$ \frac{1}{\gamma} \big[ X(t + \gamma) \, – X(t) \big] = \, – \nabla f(X(t)).$$</p>



<p class="justify-text">When \(\gamma\) tends to zero (and with simple additional regularity assumptions), the left hand side tends to the derivative of \(X\) at \(t\), and thus the function \(X\) tends to the solution of the following <a href="https://en.wikipedia.org/wiki/Ordinary_differential_equation">ordinary differential equation</a> $$ \dot{X}(t) = \ – \nabla f (X(t)).$$ See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-3479" height="271" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow.gif" width="348"/>Gradient descent (with piece-wise affine interpolation between iterates) vs. gradient flow on the same time scale for a logistic regression problem.</figure></div>



<p class="justify-text">Studying the gradient flow in lieu of the gradient descent recursions comes with pros and cons.</p>



<p class="justify-text"><strong>Simplified analyses</strong>. The gradient flow has no step-size, so all the traditional annoying issues regarding the choice of step-size, with line-search, constant, decreasing or with a weird schedule are unnecessary. Moreover, the use of differential calculus makes proving properties really simple (see examples below). We can thus focus on the essence of the algorithm rather than on technicalities.</p>



<p class="justify-text"><strong>From (continuous) flow to actual (discrete) algorithms</strong>. A flow cannot be run on a computer at is is a continuous-time object. The traditional discretization is the <a href="https://en.wikipedia.org/wiki/Euler_method">Euler method</a>, that exactly replaces the flow by a piecewise-affine interpolation of the gradient descent iterates, where as shown above, we see \(x_n\) as \(X(n\gamma)\), where \(\gamma\) is the time increment between two samples. Four interesting observations:</p>



<ul class="justify-text"><li><em>No direct proof transfer</em> : While Euler discretization always provides an algorithm, the generic convergence proofs do not allow to transfer immediately continuous-time proofs to convergence results for the discrete analysis. A key difficulty is to set-up the step-size \(\gamma\). However, the analysis can often be mimicked, i.e., similar <a href="https://en.wikipedia.org/wiki/Lyapunov_function">Lyapunov functions</a> can be used (see examples below).</li><li><em>Proximal algorithms</em> : Faced with non-continuous gradient functions, the <em>forward</em> version of Euler discretization \(x_{n+1} = x_{n} – \gamma \nabla f(x_{n})\) can be replaced by the <em>backward</em> version $$x_{n+1} = x_{n} \, –  \gamma \nabla f(x_{n+1}),$$ which is only implicit as it can be solved by minimizing $$ f(x) + \frac{1}{2\gamma}\|x-x_{n}\|_2^2,$$ thus leading to the <a href="https://fr.wikipedia.org/wiki/Algorithme_proximal_(optimisation)">proximal point algorithm</a>. Forward-backward schemes can also be recovered when \(f\) is the sum of a smooth and a non-smooth term.</li><li><em>Stochastic gradient descent</em> : There are two ways to deal with stochastic gradient descent, leading to two very different continuous limits. Adding independent and identically distributed (for simplicity) zero-mean noise \(\varepsilon_n\) to the gradient leads to the recursion $$x_{n+1} = x_{n} – \gamma \big[ \nabla f(x_{n}) + \varepsilon_n\big] = x_{n}\, – \gamma \nabla f(x_{n}) \,- \gamma \varepsilon_n,$$ where the noise is multiplied by the step-size \(\gamma\). Surprisingly, taking the limit when \(\gamma\) tends to zero leads to the deterministic gradient flow equation. A more detailed argument is presented at the end of post, but the main hand-waving reason is that the noise contribution vanishes because it is multiplied by the step-size. Note that this limiting behavior is consistent with a convergence to a minimizer of \(f\).</li><li><em>Convergence to a Langevin diffusion</em> : When instead the noise is added with magnitude proportional to the square root \(\sqrt{2 \gamma}\) of the step-size (which is asymptotically larger than \(\gamma\)), when \(\gamma\) tends to zero, and if the covariance of the noise is identity, we converge to a <a href="https://en.wikipedia.org/wiki/Diffusion_process">diffusion process</a> which is the solution of a <a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">stochastic differential equation</a>: $$ dX(t) = \ – \nabla f(X(t)) + \sqrt{2} dB(t),$$ where \(B\) is a standard <a href="https://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a>. Moreover, as \(t\) tends to infinity, \(X(t)\) happens to tend in distribution to a random variable with density proportional to \(\exp( – f(x) )\). See more details at the end of the post and in [6]. The difference in behavior is illustrated below.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-3527" height="318" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow_SGD-3.gif" width="359"/> Comparison of flow and diffusion, for the same small \(\gamma\). The flow is deterministic and converges to a stationary point of \(f\) (here the global minimum), while the diffusion is stochastic and converges to a distribution (which is typically not a point mass)</figure></div>



<h2>Properties of gradient flows</h2>



<p class="justify-text">The gradient flow $$ \dot{X}(t) = \ – \nabla f (X(t)) $$ is well-defined for a wide variety of conditions on the function \(f\). The most classical ones are <a href="https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem">Lipschitz-continuity</a> or semi-convexity [<a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">7</a>].</p>



<p class="justify-text">The most obvious property is that the function decreases along the flow; in other words, \(f(X(t))\) is decreasing, which is a simple consequence of $$ \frac{d}{dt} f(X(t)) =  \nabla f(X(t))^\top \frac{dX(t)}{dt} =\  – \| \nabla f (X(t) )\|_2^2 \leqslant 0.$$</p>



<p class="justify-text">If \(f\) is bounded from below, then \(f(X(t))\) will always converge (as a non-increasing function which is bounded from below, see <a href="https://en.wikipedia.org/wiki/Monotone_convergence_theorem">here</a>). However, in general, \(X(t)\) may not always converge without any further assumptions, e.g., it may oscillate forever. This is however rare and there are a variety of sufficient conditions for convergence of gradient flows, that date back to Lojasiewicz [8], and are based on “Lojasiewicz inequalities” that state that for \(y\) and \(x\) close enough, \(|f(x) – f(y)|^{1-\theta} \leqslant C \| \nabla f(x)\|\) for some \(C &gt; 0 \) and \(\theta \in (0,1)\). These are satisfied for “sub-analytical functions”, that include most functions one can imagine [<a href="https://www.sciencedirect.com/sdfe/reader/pii/S0022247X05006864/pdf">9</a>].</p>



<p class="justify-text">Once \(X(t)\) converges to some \(X(\infty) \in \mathbb{R}^d\), assuming \(\nabla f\) is continuous, we must have \(\nabla f(X(\infty))=0\), that is, \(X(\infty)\) is a stationary point of \(f\). Among all stationary points (that can be local minima, local maxima, or saddle-points), the one to which \(X(t)\) converges to depends on \(X(0)\).</p>



<p class="justify-text">Given any stationary point, one can look at the set of initializations that lead to it. Typically, only local minima are stable, that is, the attraction basins of other stationary points has typically zero Lebesgue measure (see, e.g., [<a href="http://www.jmlr.org/proceedings/papers/v49/lee16.pdf">10</a>]). See examples below. </p>



<p class="justify-text">We start with a simple function defined on the two-dimensional plane, with several local minima and saddle-points.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3490" height="401" src="https://francisbach.com/wp-content/uploads/2020/04/plot_non_convex-1.png" width="511"/>Various gradient flows trajectories, starting from green points and ending in black points. Note the proximity of the three top starting points, all ending in different local minima. See the motion below.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-3492" height="500" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow_noncvx-1.gif" width="519"/>Various gradient flows trajectories, in motion! All flows share the same time scale. Some seem “slower” than others (because the gradient norm is small).</figure></div>



<p class="justify-text">Before moving on, I cannot resist presenting a “real” two-dimensional example that probably all skiers, hikers, and cyclists with some form of mathematical abilities have thought of, the topographic map. Here is an example below:</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3751" height="458" src="https://francisbach.com/wp-content/uploads/2020/04/glandon_croix_de_fer-1-1024x1024.jpg" width="458"/>Extract from <a href="https://www.geoportail.gouv.fr/">IGN</a> topographic map, around <a href="https://en.wikipedia.org/wiki/Col_du_Glandon">Col du Glandon</a> and <a href="https://en.wikipedia.org/wiki/Col_de_la_Croix_de_Fer">Col de la Croix de Fer</a> (French Alps).</figure></div>



<p class="justify-text">Given the topographic map, how would gradient descent or gradient flow perform? Clearly, this corresponds to a non convex function, but it is quite well-behaved, as following water flows will typically lead to sea level. I chose two starting points famous to cyclists, <a href="https://en.wikipedia.org/wiki/Col_du_Glandon">Col du Glandon</a> and <a href="https://en.wikipedia.org/wiki/Col_de_la_Croix_de_Fer">Col de la Croix de Fer</a>, and ran gradient descent with a small step-size (to approximate the gradient flow), without noise (left) and with noise (right), on the topographic map (thanks to <a href="http://recherche.ign.fr/labos/matis/cv.php?nom=Landrieu">Loïc Landrieu</a> for the data extraction).</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-3753" src="https://francisbach.com/wp-content/uploads/2020/04/flows_final_square_small-1024x394.png"/></figure>



<p class="justify-text">Without noise, the descent from la Croix de Fer ends up getting stuck quickly in a local minimum, while the one from Glandon goes down to the valley, but then is not able to follow the almost flat slope. When noise is added, the two flows go a bit lower, highlighting the benefits of noise to escape local minima.</p>



<h2>Gradient flows for optimization and machine learning</h2>



<p class="justify-text">There are (at least) two key questions in optimization and machine learning related to gradient flows: </p>



<ul class="justify-text"><li>When can we have global guarantees for convergence? That is, can we make sure that we choose an initialization point well enough to get the the global optimum <em>without knowing where the global optimum is</em>. A key difficulty is that the volume of the attraction basin of the global optimum can be made arbitrarily small, even for infinitely differentiable functions (imagine a function equal to zero everywhere except on a small ball where it is negative).</li><li>How fast can we get there? “there” can be a stationary point or a global optimum. This is an important question as mere convergence in the limit may be arbitrarily slow [<a href="https://papers.nips.cc/paper/6707-gradient-descent-can-take-exponential-time-to-escape-saddle-points.pdf">11</a>].</li></ul>



<p class="justify-text">An important class of function is <a href="https://en.wikipedia.org/wiki/Convex_function">convex functions</a>, where everything works out very well. We will study them below. Other functions will be studied in future posts.</p>



<h2>Convex functions</h2>



<p class="justify-text">We now assume that the function \(f\) is <a href="https://en.wikipedia.org/wiki/Convex_function">convex</a> and differentiable. Within machine learning, this corresponds to objective functions encountered for supervised learning which are based on empirical risk minimization with a prediction function which is linearly parameterized, such as <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>.</p>



<p class="justify-text">There are various definitions of convexity, which are based on global properties (the function is always “below its chords”, or it is always “above its tangents”) or local properties (the Hessian is always positive semi-definite). The one which we need here is to be above its tangents, that is, for any \(x, y \in \mathbb{R}^d\), $$f(x) \geqslant f(y)  + \nabla f(y)^\top ( x \, – y).$$ Applying this to any stationary point \(y\) such that \(\nabla f(y)=0\) shows that for all \(x\), \(f(x) \geqslant f(y)\), that is, \(y\) is a global minimizer of \(f\). This is the classical benefit of convexity: no need to worry about local minima.</p>



<p class="justify-text">Another property we will need is the Lojasiewicz inequality, which is in particular satisfied when \(f\) is \(\mu\)-<a href="https://en.wikipedia.org/wiki/Convex_function#Strongly_convex_functions">strongly convex</a> (that is, \(f – \frac{1}{2} \| \cdot \|_2^2\) is convex): $$ f(x) \ – f(x_\ast) \leqslant \frac{1}{2 \mu} \| \nabla f (x)\|^2$$ for any minimizer \(x_\ast\) of \(f\) and any \(x\). This property allows to go from a bound on the gradient norm to a bound on function values.</p>



<p class="justify-text">We then obtain the convergence rate <em>in one line</em> as follows (see more details in [<a href="http://papers.nips.cc/paper/6711-integration-methods-and-optimization-algorithms.pdf">12</a>]): $$ \frac{d}{dt} \big[ f(X(t))\ – f(x_\ast) \big] =\  \nabla f(X(t))^\top \dot{X}(t) =  \ – \| \nabla f(X(t))\|_2^2 \leqslant \ – 2\mu  \big[ f(X(t)) \ – f(x_\ast) \big]$$ using the Lojasiewicz inequality above, leading to by simple integration of the derivative of \(\log \big[ f(X(t)) \ – f(x_\ast) \big]\): $$f(X(t)) \ – f(x_\ast) \leqslant \exp( – 2\mu t ) \big[ f(X(0))\  – f(x_\ast) \big], $$ that is, the convergence is exponential and the characteristic time is proportional to \(1/\mu\).</p>



<p class="justify-text">The gradient flow gives the main insight (exponential convergence); and applying the result above to \(t = \gamma n\), we seem to recover the traditional rate proportional to \(\exp( – \gamma \mu n)\); HOWEVER, this is only true asymptotically for \(\gamma\) tending to zero, and proving a result for gradient descent requires extra steps to deal with having a constant step-size. This requires typically \(\gamma \leqslant 1/L\), where \(L\) is the smoothness constant of \(f\), and the simplest proof happens to use the same structure (see [<a href="https://arxiv.org/pdf/1608.04636">13</a>] and references therein, as well as [<a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=zvmmf&amp;paperid=7813&amp;what=fullt&amp;option_lang=eng">14</a>]).</p>



<p class="justify-text">Without strong convexity, we have, using the tangent property at \(X(t)\) and \(x_\ast\): $$ \frac{d}{dt}\big[   \| X(t)\ – x_\ast \|^2 \big] = \ –   2 ( X(t) \ – x_\ast )^\top \nabla f(X(t)) \leqslant \ – 2 \big[ f(X(t)) \ – f(x_\ast) \big],$$  leading to, by integrating from \(0\) to \(t\), and using the monotonicity of \(f(X(t))\): $$  f(X(t)) \ – f(x_\ast) \leqslant \frac{1}{t} \int_0^t \big[ f(X(u)) \ – f(x_\ast) \big] du \leqslant \frac{1}{2t} \| X(0) \ – x_\ast \|^2 \ – \frac{1}{2t} \| X(t) \ – x_\ast \|^2.$$ We recover the usual rates in \(O(1/n)\), with \(t = \gamma n\), with the same caveat as above (the step-size needs to be bounded).</p>



<h2>Conclusion</h2>



<p class="justify-text">In this blog post, I covered the basic aspects of gradient flows, in particular their relationships with various forms of gradient descent, and their use in obtaining simple convergence justifications. Next months, I will cover extensions of the analyses above, in particular in terms of (1) acceleration for convex functions, where several flows and discretizations are interesting beyond the gradient flow and Euler method [12, 15], and (2) another class of functions which includes non-convex functions as encountered when learning with neural networks [16].</p>



<h2>References</h2>



<p class="justify-text">[1] Augustin Louis Cauchy. <a href="http://gallica.bnf.fr/ark:/12148/bpt6k90190w/f406">Méthode générale pour la résolution des systèmes d’équations simultanées</a>. Compte Rendu à l’Académie des Sciences, 25:536–538, 1847.<br/>[2] Claude Lemaréchal. <a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf">Cauchy and the Gradient Method</a>. <em>Documenta Mathematica</em>, <a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/vol-ismp.html">Extra Volume: Optimization Stories</a>, 251–254, 2012.<br/>[3] Yurii Nesterov. <em>Introductory lectures on convex optimization: A basic course</em> (Vol. 87). Springer Science &amp; Business Media, 2013.<br/>[4] Dimitri P. Bertsekas, <em>Nonlinear programming</em>. Athena Scientific, 1999.<br/>[5] Jorge Nocedal and Stephen Wright. <em>Numerical optimization</em>. Springer Science &amp; Business Media, 2006.<br/>[6] Arnak S. Dalalyan. <a href="https://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/rssb.12183">Theoretical guarantees for approximate sampling from smooth and log‐concave densities</a>. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 79(3), 651-676, 2017.<br/>[7] Filippo Santambrogio. <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">{Euclidean, metric, and Wasserstein} gradient flows: an overview</a>. <em>Bulletin of Mathematical Sciences</em>, <em>7</em>(1), 87-154, 2017.<br/>[8] Stanislaw Lojasiewicz. Sur les trajectoires du gradient d’une fonction analytique. <em>Seminari di Geometria</em>, 1983:115–117, 1982.<br/>[9] Jérôme Bolte, Aris Daniilidis, and Adrian Lewis. <a href="https://www.sciencedirect.com/sdfe/reader/pii/S0022247X05006864/pdf">A nonsmooth Morse–Sard theorem for subanalytic functions</a>. <em>Journal of Mathematical Analysis and Applications</em>, 321(2):729–740, 2006.<br/>[10] Jason D. Lee, Max Simchowitz, Michael I. Jordan, Benjamin Recht. <a href="http://www.jmlr.org/proceedings/papers/v49/lee16.pdf">Gradient descent only converges to minimizers</a>. <em>Conference on learning theory</em>, 1246-1257, 2016.<br/>[11] Simon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Barnabas Poczos, Aarti Singh. <a href="https://papers.nips.cc/paper/6707-gradient-descent-can-take-exponential-time-to-escape-saddle-points.pdf">Gradient descent can take exponential time to escape saddle points</a>. <em>Advances in neural information processing systems</em>, 1067-1077, 2017.<br/>[12] Damien Scieur, Vincent Roulet, Francis Bach, Alexandre d’Aspremont,. <a href="http://papers.nips.cc/paper/6711-integration-methods-and-optimization-algorithms.pdf">Integration methods and optimization algorithms</a>. <em>Advances in Neural Information Processing Systems</em>, 1109-1118, 2017.<br/>[13] Hamed Karimi, Julie Nutini, Mark Schmidt. <a href="https://arxiv.org/pdf/1608.04636">Linear convergence of gradient and proximal-gradient methods under the Polyak-Lojasiewicz condition</a>. <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 795-811, 2016.<br/>[14] Boris T. Polyak. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=zvmmf&amp;paperid=7813&amp;what=fullt&amp;option_lang=eng">Gradient methods for minimizing functionals</a>. <em>Zh. Vychisl. Mat. Mat. Fiz.</em>, 3(4):643–653, 1963. <br/>[15] Weijie Su, Stephen Boyd, Emmanuel J. Candès. <a href="http://www.jmlr.org/papers/volume17/15-084/15-084.pdf">A differential equation for modeling Nesterov’s accelerated gradient method: theory and insights</a>. <em>Journal of Machine Learning Research</em>, 17(1), 5312-5354, 2017.<br/>[16] Lénaïc Chizat, Francis Bach. <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">On the global convergence of gradient descent for over-parameterized models using optimal transport</a>. <em>Advances in Neural Information Processing Systems</em>, 3036-3046, 2018.</p>



<h2>Limits of stochastic gradient descent for vanishing step-sizes</h2>



<p class="justify-text"><strong>Convergence to gradient flow. </strong>We consider fixed times \(t = n \gamma \) and \(s = m \gamma\), and we let \(\gamma\) tend to zero, with thus \(m\) and \(n\) tending to infinity. Starting from the recursion $$x_{n+1} = x_{n}\, – \gamma \nabla f(x_{n})\  – \gamma \varepsilon_n,$$ we get the following by applying it \(m\) times: $$X(t+s) \ – X(t) = x_{n+m}-x_n = \ – \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\  – \gamma \sum_{k=0}^{m-1} \varepsilon_{k+n}.$$ The term \(\displaystyle \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\) converges to \(\displaystyle \int_{t}^{t+s}\!\!\! \nabla f(X(u)) du\), while the term \(\gamma \sum_{k=0}^{m-1} \varepsilon_{k+n}\) has zero expectation and variance equal to \(\gamma^2 m = \gamma s \) times the variance of each \(\varepsilon_{k+n}\), and thus it tends to zero (since \(\gamma\) tends to zero). Thus, in the limit, $$X(t+s)\  – X(t) = \ – \int_{t}^{t+s} \!\!\! \nabla f(X(u)) du,$$ which is equivalent to the gradient flow equation.</p>



<p class="justify-text"><strong>Convergence to diffusion.</strong> We consider the recursion $$x_{n+1} = x_{n}\,  – \gamma \nabla f(x_{n}) + \sqrt{2\gamma} \varepsilon_n.$$ With the same argument as above, we now get $$X(t+s) \ – X(t) = x_{n+m}-x_n =\ – \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\ – \sqrt{2\gamma} \sum_{k=0}^{m-1} \varepsilon_{k+n}.$$ Now the second term has zero mean but a variance proportional to \(2s\) (<em>which does not go to zero when \(\gamma\) goes to zero</em>). We can then use when \(m\) tends to infinity the <a href="https://en.wikipedia.org/wiki/Wiener_process#Wiener_process_as_a_limit_of_random_walk">limit of the sum of independent variables as a Wiener process</a>, to get $$X(t+s)\ – X(t) =\  – \int_{t}^{t+s} \!\!\! \nabla f(X(u)) du + \sqrt{2} \big[ B(t+s)-B(t) \big].$$ The <a href="https://en.wikipedia.org/wiki/It%C3%B4_diffusion#Invariant_measures">limiting distribution</a> of \(X(t)\) happens to be the so-called <a href="https://en.wikipedia.org/wiki/Gibbs_measure">Gibbs</a> distribution, with density \(\exp(-f(x))\) (the factor of \(\sqrt{2}\) was added to avoid an extra constant factor in the Gibbs distribution). More on this in a future post.</p></div>
    </content>
    <updated>2020-05-01T05:15:04Z</updated>
    <published>2020-05-01T05:15:04Z</published>
    <category term="Machine learning"/>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-05-01T10:22:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=428</id>
    <link href="https://tcsplus.wordpress.com/2020/04/30/tcs-talk-wednesday-may-6-nathan-klein-university-of-washington/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 6 — Nathan Klein, University of Washington</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Nathan Klein from the University of Washington will speak about “An improved approximation algorithm for TSP in the half integral case” (abstract below). You can reserve a spot […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Nathan Klein</strong> from the University of Washington will speak about “<em>An improved approximation algorithm for TSP in the half integral case” (abstract below).</em></p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br/>
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a>suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: A classic result from Christofides in the 70s tells us that a fast algorithm for the traveling salesperson problem (TSP) exists which returns a solution at most 3/2 times worse than the optimal. Since then, however, no better approximation algorithm has been found. In this talk, I will give an overview of research towards the goal of beating 3/2 and will present the first sub-3/2 approximation algorithm for the special case of “half integral” TSP instances. These instances have received significant attention in part due to a conjecture from Schalekamp, Williamson and van Zuylen that they attain the integrality gap of the subtour polytope. If this conjecture is true, our work shows that the integrality gap of the polytope is bounded away from 3/2, giving hope for an improved approximation for the general case. This presentation is of joint work with Anna Karlin and Shayan Oveis Gharan.</p></blockquote></div>
    </content>
    <updated>2020-05-01T03:10:05Z</updated>
    <published>2020-05-01T03:10:05Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-05-01T10:21:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.15009</id>
    <link href="http://arxiv.org/abs/2004.15009" rel="alternate" type="text/html"/>
    <title>From communication complexity to an entanglement spread area law in the ground state of gapped local Hamiltonians</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anshu:Anurag.html">Anurag Anshu</a>, Aram W. Harrow, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soleimanifar:Mehdi.html">Mehdi Soleimanifar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.15009">PDF</a><br/><b>Abstract: </b>In this work, we make a connection between two seemingly different problems.
The first problem involves characterizing the properties of entanglement in the
ground state of gapped local Hamiltonians, which is a central topic in quantum
many-body physics. The second problem is on the quantum communication
complexity of testing bipartite states with EPR assistance, a well-known
question in quantum information theory. We construct a communication protocol
for testing (or measuring) the ground state and use its communication
complexity to reveal a new structural property for the ground state
entanglement. This property, known as the entanglement spread, roughly measures
the ratio between the largest and the smallest Schmidt coefficients across a
cut in the ground state. Our main result shows that gapped ground states
possess limited entanglement spread across any cut, exhibiting an "area law"
behavior. Our result quite generally applies to any interaction graph with an
improved bound for the special case of lattices. This entanglement spread area
law includes interaction graphs constructed in [Aharonov et al., FOCS'14] that
violate a generalized area law for the entanglement entropy. Our construction
also provides evidence for a conjecture in physics by Li and Haldane on the
entanglement spectrum of lattice Hamiltonians [Li and Haldane, PRL'08]. On the
technical side, we use recent advances in Hamiltonian simulation algorithms
along with quantum phase estimation to give a new construction for an
approximate ground space projector (AGSP) over arbitrary interaction graphs.
</p></div>
    </summary>
    <updated>2020-05-01T01:20:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14995</id>
    <link href="http://arxiv.org/abs/2004.14995" rel="alternate" type="text/html"/>
    <title>Using Decision Diagrams to Compactly Represent the State Space for Explicit Model Checking</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zheng:Hao.html">Hao Zheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Price:Andrew.html">Andrew Price</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Myers:Chris.html">Chris Myers</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14995">PDF</a><br/><b>Abstract: </b>The enormous number of states reachable during explicit model checking is the
main bottleneck for scalability. This paper presents approaches of using
decision diagrams to represent very large state space compactly and
efficiently. This is possible for asynchronous systems as two system states
connected by a transition often share many same local portions. Using decision
diagrams can significantly reduce memory demand by not using memory to store
the redundant information among different states. This paper considers
multi-value decision diagrams for this purpose. Additionally, a technique to
reduce the runtime overhead of using these diagrams is also described.
Experimental results and comparison with the state compression method as
implemented in the model checker SPIN show that the approaches presented in
this paper are memory efficient for storing large state space with acceptable
runtime overhead.
</p></div>
    </summary>
    <updated>2020-05-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14931</id>
    <link href="http://arxiv.org/abs/2004.14931" rel="alternate" type="text/html"/>
    <title>The Complexity of Dynamic Data Race Prediction</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mathur:Umang.html">Umang Mathur</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pavlogiannis:Andreas.html">Andreas Pavlogiannis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Viswanathan:Mahesh.html">Mahesh Viswanathan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14931">PDF</a><br/><b>Abstract: </b>Writing concurrent programs is notoriously hard due to scheduling
non-determinism. The most common concurrency bugs are data races, which are
accesses to a shared resource that can be executed concurrently. Dynamic
data-race prediction is the most standard technique for detecting data races:
given an observed, data-race-free trace $t$, the task is to determine whether
$t$ can be reordered to a trace $t^*$ that exposes a data-race. Although the
problem has received significant practical attention for over three decades,
its complexity has remained elusive. In this work, we address this lacuna,
identifying sources of intractability and conditions under which the problem is
efficiently solvable. Given a trace $t$ of size $n$ over $k$ threads, our main
results are as follows.
</p>
<p>First, we establish a general $O(k\cdot n^{2\cdot (k-1)})$ upper-bound, as
well as an $O(n^k)$ upper-bound when certain parameters of $t$ are constant. In
addition, we show that the problem is NP-hard and even W[1]-hard parameterized
by $k$, and thus unlikely to be fixed-parameter tractable. Second, we study the
problem over acyclic communication topologies, such as server-clients
hierarchies. We establish an $O(k^2\cdot d\cdot n^2\cdot \log n)$ upper-bound,
where $d$ is the number of shared variables accessed in $t$. In addition, we
show that even for traces with $k=2$ threads, the problem has no
$O(n^{2-\epsilon})$ algorithm under Orthogonal Vectors. Since any trace with 2
threads defines an acyclic topology, our upper-bound for this case is optimal
wrt polynomial improvements for up to moderate values of $k$ and $d$. Finally,
we study a distance-bounded version of the problem, where the task is to expose
a data race by a witness trace that is similar to $t$. We develop an algorithm
that works in $O(n)$ time when certain parameters of $t$ are constant.
</p></div>
    </summary>
    <updated>2020-05-01T01:20:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14891</id>
    <link href="http://arxiv.org/abs/2004.14891" rel="alternate" type="text/html"/>
    <title>Fully-Dynamic Coresets</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henzinger:Monika.html">Monika Henzinger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kale:Sagar.html">Sagar Kale</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14891">PDF</a><br/><b>Abstract: </b>With input sizes becoming massive, coresets---small yet representative
summary of the input---are relevant more than ever. A weighted set $C_w$ that
is a subset of the input is an $\varepsilon$-coreset if the cost of any
feasible solution $S$ with respect to $C_w$ is within $[1 {\pm} \varepsilon]$
of the cost of $S$ with respect to the original input. We give a very general
technique to compute coresets in the fully-dynamic setting where input points
can be added or deleted. Given a static $\varepsilon$-coreset algorithm that
runs in time $t(n, \varepsilon, \lambda)$ and computes a coreset of size $s(n,
\varepsilon, \lambda)$, where $n$ is the number of input points and $1
{-}\lambda$ is the success probability, we give a fully-dynamic algorithm that
computes an $\varepsilon$-coreset with worst-case update time $O((\log n) \cdot
t(s(n, \varepsilon/\log n, \lambda/n), \varepsilon/\log n, \lambda/n) )$ (this
bound is stated informally), where the success probability is $1{-}\lambda$.
Our technique is a fully-dynamic analog of the merge-and-reduce technique that
applies to insertion-only setting. Although our space usage is $O(n)$, we work
in the presence of an adaptive adversary.
</p>
<p>As a consequence, we get fully-dynamic $\varepsilon$-coreset algorithms for
$k$-median and $k$-means with worst-case update time
$O(\varepsilon^{-2}k^2\log^5 n \log^3 k)$ and coreset size
$O(\varepsilon^{-2}k\log n \log^2 k)$ ignoring $\log \log n$ and
$\log(1/\varepsilon)$ factors and assuming that $\varepsilon, \lambda =
\Omega(1/$poly$(n))$ (very weak assumptions made to make bounds easy to parse).
These are the first fully-dynamic algorithms for $k$-median and $k$-means with
worst-case update times $O($poly$(k, \log n, \varepsilon^{-1}))$. The best
previous bound for both problems was amortized $O(n\log n)$ by Cohen-Addad et
al. via randomized $O(1)$-coresets in $O(n)$ space.
</p></div>
    </summary>
    <updated>2020-05-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14789</id>
    <link href="http://arxiv.org/abs/2004.14789" rel="alternate" type="text/html"/>
    <title>Twin-width I: tractable FO model checking</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonnet:=Eacute=douard.html">Édouard Bonnet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Eun_Jung.html">Eun Jung Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thomass=eacute=:St=eacute=phan.html">Stéphan Thomassé</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Watrigant:R=eacute=mi.html">Rémi Watrigant</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14789">PDF</a><br/><b>Abstract: </b>Inspired by a width invariant defined on permutations by Guillemot and Marx
[SODA '14], we introduce the notion of twin-width on graphs and on matrices.
Proper minor-closed classes, bounded rank-width graphs, map graphs, $K_t$-free
unit $d$-dimensional ball graphs, posets with antichains of bounded size, and
proper subclasses of dimension-2 posets all have bounded twin-width. On all
these classes (except map graphs without geometric embedding) we show how to
compute in polynomial time a sequence of $d$-contractions, witness that the
twin-width is at most $d$. We show that FO model checking, that is deciding if
a given first-order formula $\phi$ evaluates to true for a given binary
structure $G$ on a domain $D$, is FPT in $|\phi|$ on classes of bounded
twin-width, provided the witness is given. More precisely, being given a
$d$-contraction sequence for $G$, our algorithm runs in time $f(d,|\phi|) \cdot
|D|$ where $f$ is a computable but non-elementary function. We also prove that
bounded twin-width is preserved by FO interpretations and transductions
(allowing operations such as squaring or complementing a graph). This unifies
and significantly extends the knowledge on fixed-parameter tractability of FO
model checking on non-monotone classes, such as the FPT algorithm on
bounded-width posets by Gajarsk\'y et al. [FOCS '15].
</p></div>
    </summary>
    <updated>2020-05-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14724</id>
    <link href="http://arxiv.org/abs/2004.14724" rel="alternate" type="text/html"/>
    <title>Learning Bayesian Networks Under Sparsity Constraints: A Parameterized Complexity Analysis</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gr=uuml=ttemeier:Niels.html">Niels Grüttemeier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Komusiewicz:Christian.html">Christian Komusiewicz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14724">PDF</a><br/><b>Abstract: </b>We study the problem of learning the structure of an optimal Bayesian network
$D$ when additional constraints are posed on the DAG $D$ or on its moralized
graph. More precisely, we consider the constraint that the moralized graph can
be transformed to a graph from a sparse graph class $\Pi$ by at most $k$ vertex
deletions. We show that for $\Pi$ being the graphs with maximum degree $1$, an
optimal network can be computed in polynomial time when $k$ is constant,
extending previous work that gave an algorithm with such a running time for
$\Pi$ being the class of edgeless graphs [Korhonen &amp; Parviainen, NIPS 2015]. We
then show that further extensions or improvements are presumably impossible.
For example, we show that when $\Pi$ is the set of graphs with maximum degree
$2$ or when $\Pi$ is the set of graphs in which each component has size at most
three, then learning an optimal network is NP-hard even if $k=0$. Finally, we
show that learning an optimal network with at most $k$ edges in the moralized
graph presumably has no $f(k)\cdot |I|^{\mathcal{O}(1)}$-time algorithm and
that, in contrast, an optimal network with at most $k$ arcs in the DAG $D$ can
be computed in $2^{\mathcal{O}(k)}\cdot
</p>
<p>|I|^{\mathcal{O}(1)}$ time where $|I|$ is the total input size.
</p></div>
    </summary>
    <updated>2020-05-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14715</id>
    <link href="http://arxiv.org/abs/2004.14715" rel="alternate" type="text/html"/>
    <title>Randomized Two-Valued Bounded Delay Online Buffer Management</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=uuml=rr:Christoph.html">Christoph Dürr</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamali:Shahin.html">Shahin Kamali</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14715">PDF</a><br/><b>Abstract: </b>In the bounded delay buffer management problem unit size packets arrive
online to be sent over a network link. The objective is to maximize the total
weight of packets sent before their deadline. In this paper we are interested
in the two-valued variant of the problem, where every packet has either low (1)
or high priority weight ($\alpha$ &gt; 1). We show that its randomized competitive
ratio against an oblivious adversary is 1 + ($\alpha$ -- 1)/($\alpha$ 2 +
$\alpha$).
</p></div>
    </summary>
    <updated>2020-05-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14692</id>
    <link href="http://arxiv.org/abs/2004.14692" rel="alternate" type="text/html"/>
    <title>Sparse Hashing for Scalable Approximate Model Counting: Theory and Practice</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meel:Kuldeep_S=.html">Kuldeep S. Meel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akshay:S=.html">S. Akshay</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14692">PDF</a><br/><b>Abstract: </b>Given a CNF formula F on n variables, the problem of model counting or #SAT
is to compute the number of satisfying assignments of F . Model counting is a
fundamental but hard problem in computer science with varied applications.
Recent years have witnessed a surge of effort towards developing efficient
algorithmic techniques that combine the classical 2-universal hashing with the
remarkable progress in SAT solving over the past decade. These techniques
augment the CNF formula F with random XOR constraints and invoke an NP oracle
repeatedly on the resultant CNF-XOR formulas. In practice, calls to the NP
oracle calls are replaced a SAT solver whose runtime performance is adversely
affected by size of XOR constraints. The standard construction of 2-universal
hash functions chooses every variable with probability p = 1/2 leading to XOR
constraints of size n/2 in expectation. Consequently, the challenge is to
design sparse hash functions where variables can be chosen with smaller
probability and lead to smaller sized XOR constraints.
</p>
<p>In this paper, we address this challenge from theoretical and practical
perspectives. First, we formalize a relaxation of universal hashing, called
concentrated hashing and establish a novel and beautiful connection between
concentration measures of these hash functions and isoperimetric inequalities
on boolean hypercubes. This allows us to obtain (log m) tight bounds on
variance and dispersion index and show that p = O( log(m)/m ) suffices for
design of sparse hash functions from {0, 1}^n to {0, 1}^m. We then use sparse
hash functions belonging to this concentrated hash family to develop new
approximate counting algorithms. A comprehensive experimental evaluation of our
algorithm on 1893 benchmarks demonstrates that usage of sparse hash functions
can lead to significant speedups.
</p></div>
    </summary>
    <updated>2020-05-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14650</id>
    <link href="http://arxiv.org/abs/2004.14650" rel="alternate" type="text/html"/>
    <title>Weakly Submodular Function Maximization Using Local Submodularity Ratio</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Santiago:Richard.html">Richard Santiago</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoshida:Yuichi.html">Yuichi Yoshida</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14650">PDF</a><br/><b>Abstract: </b>Weak submodularity is a natural relaxation of the diminishing return
property, which is equivalent to submodularity. Weak submodularity has been
used to show that many (monotone) functions that arise in practice can be
efficiently maximized with provable guarantees. In this work we introduce two
natural generalizations of weak submodularity for non-monotone functions. We
show that an efficient randomized greedy algorithm has provable approximation
guarantees for maximizing these functions subject to a cardinality constraint.
We then provide a more refined analysis that takes into account that the weak
submodularity parameter may change (sometimes improving) throughout the
execution of the algorithm. This leads to improved approximation guarantees in
some settings. We provide applications of our results for monotone and
non-monotone maximization problems.
</p></div>
    </summary>
    <updated>2020-05-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14632</id>
    <link href="http://arxiv.org/abs/2004.14632" rel="alternate" type="text/html"/>
    <title>Geometric group testing</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berendsohn:Benjamin_Aram.html">Benjamin Aram Berendsohn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kozma:L=aacute=szl=oacute=.html">László Kozma</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14632">PDF</a><br/><b>Abstract: </b>Group testing is concerned with identifying $t$ defective items in a set of
$m$ items, where each test reports whether a specific subset of items contains
at least one defective. In non-adaptive group testing, the subsets to be tested
are fixed in advance. By testing multiple items at once, the required number of
tests can be made much smaller than $m$. In fact, for $t \in \mathcal{O}(1)$,
the optimal number of (non-adaptive) tests is known to be $\Theta(\log{m})$.
</p>
<p>In this paper, we consider the problem of non-adaptive group testing in a
geometric setting, where the items are points in $d$-dimensional Euclidean
space and the tests are axis-parallel boxes (hyperrectangles). We present upper
and lower bounds on the required number of tests under this geometric
constraint. In contrast to the general, combinatorial case, the bounds in our
geometric setting are polynomial in $m$. For instance, our results imply that
identifying a defective pair in a set of $m$ points in the plane always
requires $\Omega(m^{4/7})$ rectangle-tests, and there exist configurations of
$m$ points for which $\mathcal{O}(m^{2/3})$ rectangle-tests are sufficient,
whereas to identify a single defective point in the plane, $\Theta(m^{1/2})$
tests are always necessary and sometimes sufficient.
</p></div>
    </summary>
    <updated>2020-05-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14622</id>
    <link href="http://arxiv.org/abs/2004.14622" rel="alternate" type="text/html"/>
    <title>The Canny-Emiris conjecture for the sparse resultant</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Carlos D'Andrea, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jeronimo:Gabriela.html">Gabriela Jeronimo</a>, Martin Sombra <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14622">PDF</a><br/><b>Abstract: </b>We present a product formula for the initial parts of the sparse resultant
associated to an arbitrary family of supports, generalizing a previous result
by Sturmfels. This allows to compute the homogeneities and degrees of the
sparse resultant, and its evaluation at systems of Laurent polynomials with
smaller supports. We obtain a similar product formula for some of the initial
parts of the principal minors of the Sylvester-type square matrix associated to
a mixed subdivision of a polytope. Applying these results, we prove that the
sparse resultant can be computed as the quotient of the determinant of such a
square matrix by a certain principal minor, under suitable hypothesis. This
generalizes the classical Macaulay formula for the homogeneous resultant, and
confirms a conjecture of Canny and~Emiris.
</p></div>
    </summary>
    <updated>2020-05-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14574</id>
    <link href="http://arxiv.org/abs/2004.14574" rel="alternate" type="text/html"/>
    <title>On Solving Cycle Problems with Branch-and-Cut: Extending Shrinking and Exact Subcycle Elimination Separation Algorithms</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobeaga:Gorka.html">Gorka Kobeaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Merino:Mar=iacute=a.html">María Merino</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lozano:Jose_A=.html">Jose A. Lozano</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14574">PDF</a><br/><b>Abstract: </b>In this paper, we extend techniques developed in the context of the
Travelling Salesperson Problem for cycle problems. Particularly, we study the
shrinking of support graphs and the exact algorithms from subcycle elimination
separation problems. The efficient application of the considered techniques has
proved to be essential in the Travelling Salesperson Problem when solving large
size problems by Branch-and-Cut, and this has been the motivation behind this
work. Regarding the shrinking of support graphs, we prove the validity of the
Padberg-Rinaldi general shrinking rules and the Crowder-Padberg subcycle-safe
shrinking rules. Concerning the subcycle separation problems, we extend two
exact separation algorithms, the Dynamic Hong and the Extended
Padberg-Gr\"otschel algorithms, which are shown to be superior to the ones used
so far in the literature of cycle problems.
</p>
<p>The proposed techniques are empirically tested in 24 subcycle elimination
problem instances generated by solving the Orienteering Problem (involving up
to 15112 vertices) with Branch-and-Cut. The experiments suggest the relevance
of the proposed techniques for cycle problems. The obtained average speedup for
the subcycle separation problems in the Orienteering Problem when the proposed
techniques are used together is around 50 times in medium-sized instances and
around 250 times in large-sized instances.
</p></div>
    </summary>
    <updated>2020-05-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14473</id>
    <link href="http://arxiv.org/abs/2004.14473" rel="alternate" type="text/html"/>
    <title>Arc Routing with Time-Dependent Travel Times and Paths</title>
    <feedworld_mtime>1588291200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vidal:Thibaut.html">Thibaut Vidal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martinelli:Rafael.html">Rafael Martinelli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pham:Tuan_Anh.html">Tuan Anh Pham</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/H=agrave=:Minh_Ho=agrave=ng.html">Minh Hoàng Hà</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14473">PDF</a><br/><b>Abstract: </b>Vehicle routing algorithms usually reformulate the road network into a
complete graph in which each arc represents the shortest path between two
locations. Studies on time-dependent routing followed this model and therefore
defined the speed functions on the complete graph. We argue that this model is
often inadequate, in particular for arc routing problems involving services on
edges of a road network. To fill this gap, we formally define the
time-dependent capacitated arc routing problem (TDCARP), with travel and
service speed functions given directly at the network level. Under these
assumptions, the quickest path between locations can change over time, leading
to a complex problem that challenges the capabilities of current solution
methods. We introduce effective algorithms for preprocessing quickest paths in
a closed form, efficient data structures for travel time queries during routing
optimization, as well as heuristic and exact solution approaches for the
TDCARP. Our heuristic uses the hybrid genetic search principle with tailored
solution-decoding algorithms and lower bounds for filtering moves. Our
branch-and-price algorithm exploits dedicated pricing routines, heuristic
dominance rules and completion bounds to find optimal solutions for problem
counting up to 75 services. Based on these algorithms, we measure the benefits
of time-dependent routing optimization for different levels of travel-speed
data accuracy.
</p></div>
    </summary>
    <updated>2020-05-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/04/30/linkage</id>
    <link href="https://11011110.github.io/blog/2020/04/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Goodbye Insta (). I don’t have an Instagram account. I don’t want to join yet another closed system for capturing my data and sending it to a corporation. But there were a few people whose Instagram accounts I would check out semi-regularly. No longer. Now Instagram won’t show me any posts not-logged-in. If you’re going to fence yourself off from the Internet, then you’re fencing yourself off from me. If you think this is going to encourage me to make an account, the opposite is true.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p>Goodbye Insta (<a href="https://mathstodon.xyz/@11011110/104009624456399772"/>). I don’t have an Instagram account. I don’t want to join yet another closed system for capturing my data and sending it to a corporation. But there were a few people whose Instagram accounts I would check out semi-regularly. No longer. Now Instagram won’t show me any posts not-logged-in. If you’re going to fence yourself off from the Internet, then you’re fencing yourself off from me. If you think this is going to encourage me to make an account, the opposite is true.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.07630">Four pages are indeed necessary for planar graphs</a> (<a href="https://mathstodon.xyz/@11011110/104016197893535979"/>). At STOC 1986, Yannakakis proved that planar graphs have 4-page <a href="https://en.wikipedia.org/wiki/Book_embedding">book embeddings</a>, and announced an example requiring 4 pages, but never published the example. Finally now Bekos et al. have provided detailed constructions for planar graphs requiring 4 pages. Still lost in limbo: Unger’s claim from 1992 that testing 3-page embeddability with fixed vertex ordering is polynomial.</p>
  </li>
  <li>
    <p><a href="https://www.latimes.com/california/story/2020-04-16/uc-reeling-under-staggering-coronavirus-costs-the-worst-impacts-all-at-once">Universities are starting to see the costs of the lockdown</a> (<a href="https://mathstodon.xyz/@11011110/104023169399231809"/>), in lost revenue from students and medical centers and extra expenses from the transition to remote learning — the linked story is on the University of California, but other universities are likely in similar or worse shape. So far my campus has not announced any specific cuts but colleagues predict that a hiring freeze, at least, is likely to come.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.13777">Subgraph densities in a surface</a> (<a href="https://mathstodon.xyz/@11011110/104035284995793475"/>). In 1993 I published a paper “<a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-JGT-93.pdf">Connectivity, graph minors, and subgraph multiplicity</a>” showing that a planar graph  can have at most linearly many copies in larger planar graphs if and only if  is 3-connected. I thought it was long-forgotten, but now Huynh, Joret and Wood have generalized it in two ways: other surfaces than the plane, and other exponents than one in the number of copies.</p>
  </li>
  <li>
    <p><a href="https://github.com/microsoft/STL/pull/724">Fix boyer_moore_searcher with the Rytter correction</a> (<a href="https://mathstodon.xyz/@11011110/104040417854587667"/>, <a href="https://news.ycombinator.com/item?id=22895932">via</a>). A 40-year-old bugfix to an even older linear-time string matching algorithm finally makes it to production code, with an admonishment that this should have been mentioned in more recent explanations of the algorithm such as <a href="https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string-search_algorithm">Wikipedia’s</a> (since added).</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2020/04/19/to-cheer-you-up-in-difficult-times-ii-mysterious-matching-news-by-gal-beniamini-naom-nisan-vijay-vazirani-and-thorben-trobst/">To cheer you up in difficult times II: Mysterious matching news</a> (<a href="https://mathstodon.xyz/@11011110/104046659532923035"/>). Gil Kalai blogs about two recent papers: one by Gal Beniamini and Noam Nisan on <a href="https://arxiv.org/abs/2001.07642">a polynomial that is one for bipartite graphs with perfect matchings and zero otherwise</a> and one by UCI colleagues Vijay Vazirani and Thorben Tröbst <a href="https://arxiv.org/abs/2003.08917">extending it to test whether a subgraph of a weighted complete bipartite graph contains a minimum-weight perfect matching</a>.</p>
  </li>
  <li>
    <p><a href="https://inkscape.org/release/inkscape-1.0rc1/">Inkscape 1.0 (release candidate) now runs natively on OS X</a> (<a href="https://mathstodon.xyz/@11011110/104052350948366314"/>, <a href="https://news.ycombinator.com/item?id=22855357">via</a>). I still haven’t tried it, and the discussion suggests it still needs some tuning. But it seems pretty popular in free-software circles, and it’s good to know that there are free vector drawing programs out there that can compete with the (expensive) one I use, Adobe Illustrator.</p>
  </li>
  <li>
    <p>Antoine Chambert-Loir <a href="https://mathstodon.xyz/@antoinechambertloir/104018917625130123">asks for an explanation of the Schläfli graph’s Hamiltonicity</a>, or more specifically how <a href="https://en.wikipedia.org/wiki/Schl%C3%A4fli_graph">the Wikipedia article’s</a> infobox illustration can be related to more standard constructions of the graph.</p>
  </li>
  <li>
    <p><a href="https://aperiodical.com/2020/04/the-big-lock-down-math-off-match-5/">The eggbox puzzle</a> (<a href="https://mathstodon.xyz/@11011110/104063021972727214"/>). <em>The Aperiodical</em> has been posting “Big Lock-Down Math-Off” posts, double-headers with a vote for which “made you say Aha! the loudest”. I chose this one with James Munro’s eggbox puzzle because the illustrations made me say Aha! This state space is a <a href="https://en.wikipedia.org/wiki/Median_graph">median graph</a>!</p>

    <p>To find the median of three -egg placements, put the th egg in the median of its three positions. In fact, it’s a distributive lattice where the meet of two placements is to move each egg to the rightmost of its two positions and the join is to move each egg to the leftmost of the two. Of course that doesn’t help much in solving the actual puzzle…</p>
  </li>
  <li>
    <p><a href="https://listserv.umd.edu/cgi-bin/wa?A2=ind2004&amp;L=CAST10&amp;P=3191">Another MDPI journal editorial board resigns</a> (<a href="https://mathstodon.xyz/@11011110/104068456940158118"/>; 
“another” because of the 2018 <em>Nutrients</em> mass resignation). The topic appears to be related to chemical engineering. The resigning editor-in-chief describes the reason: “MDPI has stated that it will not modify the current plan for rapid, quota-driven growth, while the Editorial Board will not compromise its overarching goals of publication quality and scholarly contribution.”</p>
  </li>
  <li>
    <p>I had been using Wunderlist for to-do lists of review/submit deadlines, a shared family grocery list, etc but <a href="https://11011110.github.io/blog/2020/03/15/stay-home-linkage.html">as I posted earlier</a>, it is being strangled by new owner Microsoft to push you to their other thing. So after comparing other cross-platform shareable to-do-list apps I chose <a href="https://todoist.com/">todoist</a> because of its similar workflow and <a href="https://todoist.com/import/wunderlist">Wunderlist import feature</a> (<a href="https://mathstodon.xyz/@11011110/104074471851344995"/>). Close second was Tom Hull’s suggestion, Trello. Not as close: Microsoft.</p>
  </li>
  <li>
    <p><a href="http://statnet.org/COVID-JustOneFriend/">Can’t I please just visit one friend?</a> (<a href="https://mathstodon.xyz/@11011110/104085917246234853"/>) Or, how graph drawing helps us understand the importance of maintaining strict isolation.</p>
  </li>
  <li>
    <p><a href="https://www.icann.org/news/blog/icann-board-withholds-consent-for-a-change-of-control-of-the-public-interest-registry-pir">ICANN blocks .org sale</a> (<a href="https://mathstodon.xyz/@11011110/104091255847507444"/>, <a href="https://news.ycombinator.com/item?id=23038637">via</a>). This is very good news, and follows onto their <a href="https://www.theregister.co.uk/2020/04/17/icann_california_org_sale_delay/">delay of the sale a couple of weeks ago after the intervention of the California attorney general</a>.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-04-30T21:37:00Z</updated>
    <published>2020-04-30T21:37:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-01T05:01:21Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-2580044341116607459</id>
    <link href="http://processalgebra.blogspot.com/feeds/2580044341116607459/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=2580044341116607459" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/2580044341116607459" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/2580044341116607459" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/04/an-interview-with-rob-van-glabbeek.html" rel="alternate" type="text/html"/>
    <title>An interview with Rob van Glabbeek, CONCUR Test-of-Time Award recipient</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post is devoted to the third interview with the <a href="https://concur2020.forsyte.at/test-of-time/index.html">colleagues</a> who were selected for the first edition of the CONCUR  Test-of-Time Award. (See <a href="https://processalgebra.blogspot.com/2020/04/an-interview-with-davide-sangiorgi.html">here</a> for the interview with <a href="http://www.cs.unibo.it/~sangio/">Davide Sangiorgi</a>, and <a href="https://processalgebra.blogspot.com/2020/04/an-interview-with-nancy-lynch-and.html">here</a> for the interview with <a href="https://people.csail.mit.edu/lynch/">Nancy Lynch</a> and <a href="http://profs.sci.univr.it/~segala/">Roberto Segala</a>.) I asked <a href="http://theory.stanford.edu/~rvg/">Rob van Glabbeek</a> (Data61, CSIRO, Sydney, Australia) a few questions via email and I am happy share his answers with readers of this blog below. Rob's words brought me back to the time when the CONCUR conference started and our field of concurrency theory was roughly a decade old. I hope that his interesting account and opinions will inspire young researchers in concurrency theory of all ages.<br/><br/>Luca: You receive one of the two CONCUR ToT Awards for the period  1990-1993 for your companion papers on "The Linear Time-Branching Time  Spectrum", published at CONCUR 1990 and 1993. Could you tell us briefly  what spurred you to  embark in the encyclopaedic study of process semantics you developed in  those two papers and what were your main sources of inspiration?<br/><br/>Rob: My PhD supervisors, Jan Willem Klop and Jan Bergstra, were examining<br/>bisimulation semantics, but also failures semantics, in collaboration<br/>with Ernst-Ruediger Olderog, and at some point, together with Jos Baeten,<br/>wrote a paper on ready-trace semantics to deal with a priority operator.<br/>I had a need to see these activities, and also those of Tony Hoare and<br/>Robin Milner, as revealing different aspects of the same greater whole,<br/>rather than as isolated research efforts. This led naturally to an<br/>abstraction of their work in which the crucial difference was the semantic<br/>equivalence employed. Ordering these semantics in a lattice was never<br/>optional for me, but necessary to see the larger picture.  It may be in<br/>my nature to relate different solutions to similar problems I encounter<br/>by placing them in lattices.<br/><span class="im"/> <br/>Luca: Did you imagine at the time that the papers on "The Linear Time-Branching Time Spectrum" would have so much impact? <br/><div><br/></div><div>Rob: I didn't think much about impact in those days. I wrote the papers<br/>because I felt the need to do so, and given that, could just as well<br/>publish them. Had I made an estimate, I would have guessed that my<br/>papers would have impact, as I imagined that things that I find<br/>necessary to know would interest some others too. But I would have<br/>underestimated the amount of impact, in part because I had grossly<br/>underestimated the size of the research community eventually working<br/>on related matters.<span class="im"><br/></span></div><div><br/></div><div>Luca: In your papers, you present characterizations of process semantics in  relational form, via testing scenarios, via modal logics and in terms of  (conditional) equational axiomatizations. Of all those  characterizations, which ones do you think have been most useful in  future work? To your mind, what are the most  interesting or unexpected uses in the  literature of the notions and techniques you developed in the "The  Linear Time-Branching Time Spectrum"? </div><div> </div><div>Rob: I think that all these characterizations together shed the light on<br/>process semantics that makes them understandable and useful.<br/>Relational characterizations, i.e., of (bi)simulations, are often the<br/>most useful to grasp what kind of relation one has, and this is essential<br/>for any use. But modal characterizations are intimately linked with<br/>model checking, which is one of the more successful verification tools.<br/>Equational characterizations also find practical use in verification,<br/>e.g., in normalization. Yet, they are most useful in illustrating<br/>the essential difference between process semantics, and thus form a guide<br/>in choosing the right one for an application. To me, the testing scenarios<br/>are the most useful in weighing how realistic certain semantic<br/>identifications are in certain application contexts.</div><div><span class="im"/> </div><div>Luca: How much of your later work has  built on your award-winning papers? What follow-up results of yours are  you most proud of and why?</div><div> </div><div>Rob: About one third, I think.</div><div><ul><li>My work on action refinement, together with Ursula Goltz, extends theclassification of process semantics in a direction orthogonal to the linear time - branching time spectrum. I am proud of this work mostly because, for a distinct class of applications, action refinement is great criterion to tell which semantics equivalences one ought to prefer over others. These process semantics have also been used in later work on distributability with Ursula and with Jens-Wolfhard Schicke Uffmann. Distributability tells us which distributed systems can be cast as sequential systems cooperating asynchronously, and which fundamentally cannot.</li><li>I wrote several papers on Structural Operational Semantics, many jointly with Wan Fokkink, aiming to obtain congruence results for semantic equivalences. These works carefully employed the modal characterizations from my spectrum papers. I consider this work important because congruence properties are essential in verification, as we need compositionality to avoid state explosions. </li><li>I used a version of failure simulation semantics from my second spectrum paper in work with Yuxin Deng, Matthew Hennessy, Carroll Morgan and Chenyi Zhang, characterizing may- and must testing equivalences for probabilistic nondeterministic processes. Probability and nondeterminism interact in many applications, and I think that may- and must testing are the right tools to single out the essence of their semantics.</li></ul></div><div><span class="im"><br/></span> </div><div>Luca: If I remember correctly, your master thesis was in pure mathematics. Why  did you decide to move to a PhD in computer science? What was it like  to work as a PhD student at CWI in the late 1980s? How did you start  your collaborations with Jos Baeten and fellow PhD student Frits  Vaandrager, amongst others? </div><div> </div><div>Rob: I was interested in mathematical logic already since high school,<br/>after winning a book on the foundations of logic set theory in a<br/>mathematics Olympiad.  I found the material in my logic course,<br/>thought at the University of Leiden by Jan Bergstra, fascinating, and<br/>when doing a related exam at CWI, where Jan had moved to at that<br/>time, and he asked me to do a PhD on related subjects, I say no reason<br/>not to. But he had to convince me first that my work would not involve<br/>any computer science, as my student advisor at The University of<br/>Leiden, Professor Claas, had told us that this was not a real<br/>science. Fortunately, Jan had no problem passing that hurdle.<br/><br/>I was the third member of the team of Jan Bergstra and Jan Willem<br/>Klop, after Jos Baeten had been recruited a few months earlier, but in<br/>a more senior position. Frits arrived a month later than me. As we<br/>were exploring similar subjects, and shared an office, it was natural<br/>to collaborate. As these collaborations worked out very well, they<br/>became quite strong.<br/><br/>CWI was a marvelous environment in the late 1980s. Our group gradually<br/>grew beyond a dozen members, but there remained a much greater<br/>cohesion then I have seen anywhere else. We had weekly process<br/>algebra meetings, where each of us shared our recent ideas with others<br/>in the group. At those meetings, if one member presented a proof, the<br/>entire audience followed it step by step, contributing meaningfully<br/>where needed, and not a single error went unnoticed.</div><div><span class="im"><br/></span> </div><div>Luca: Already as PhD student, you took up leadership roles within the  community. For instance, I remember attending a workshop on "Combining  Compositionality and Concurrency" <span>in March 1988,</span> which you  co-organized with Ursula Goltz and Ernst-Ruediger Olderog. Would you  recommend to a PhD student that he/she become involved in that kind of  activities early on in their career? Do you think that doing so had a  positive effect on your future career?  </div><div> </div><div>Rob: Generally I recommend this. Although I co-organized this workshop<br/>because the circumstances were right for it, and the need great; not<br/>because I felt it necessary to organize things.  My advice to PhD<br/>students would not be specifically to add a leadership component to<br/>their CV regardless of the circumstances, but instead do take such an<br/>opportunity when the time is right, and not be shy because of lack of<br/>experience.</div><div> </div><div>This workshop definitively had a positive effect on my future career.<br/>When finishing my PhD I inquired at Stanford and MIT for a possible<br/>post-doc position, and the good impressions I had left at this<br/>workshop were a factor in getting offers from both institutes.<br/>In fact, Vaughan Pratt, who was deeply impressed with work I had<br/>presented at this workshop, convinced me to apply straight away for an<br/>assistant professorship, and this led to my job at Stanford.   </div><div><span class="im"><br/></span> </div><div>Luca: I still have vivid memories of your talk at the first CONCUR conference  in 1990 in Amsterdam, where you introduced notions of black-box  experiments on processes using an actual box and beer-glass mats. You  have attended and contributed to many of the editions of the conference  since then. To your mind, how has the focus of CONCUR changed since its  first edition in 1990? </div><div> </div><div>Rob: I think the focus on foundations has shifted somewhat to more applied<br/>topics, and the scope of "concurrency theory" has broadened significantly.<br/>Still, many topics from the earlier CONCURs are still celebrated today.</div><div><span class="im"><br/></span> </div><div>Luca: The last forty years have seen a huge amount of work on process algebra  and process calculi. However, the emphasis on that field of research  within concurrency theory seems to have diminished over the last few  years, even in Europe. What advice would you give to a young researcher  interested in working  on process calculi today? </div><div> </div><div>Rob: Until 10 years ago, many people I met in industry voiced the opinion<br/>that formals methods in general, and process algebra in particular,<br/>has been tried in the previous century, and had been found to not to<br/>scale to tackle practical problems. But more recently this opinion is<br/>on the way out, in part because it became clear that the kind of<br/>problems solved by formal methods are much more pressing then originally<br/>believed, and in part because many of the applications that were<br/>deemed to hard in the past, are now being addressed quite adequately.<br/>For these reasons, I think work on process calculi is still a good bet<br/>for a PhD study, although a real-world application motivating the<br/>theory studied is perhaps harder needed than in the past, and toy<br/>examples just to illustrate the theory are not always enough.</div><div><span class="im"><br/></span> </div><div>Luca: What are the research topics that currently excite you the most?</div><div> </div><div>Rob: I am very excited about showing liveness properties, in Lamport's<br/>sense, telling that a modeled system eventually achieves a desirable outcome.<br/>And even more in creating process algebraic verification frameworks<br/>that are in principle able to do this. I have come to believe that<br/>traditional approaches popular in process algebra and temporal logic<br/>can only deal with liveness properties when making fairness<br/>assumptions that are too strong and lead to unwarranted conclusions.<br/>Together with Peter Hoefner I have been advocating a weaker concept of<br/>fairness, that we call justness, that is much more suitable as a basis<br/>for formulating liveness properties. Embedding this concept into the<br/>conceptual foundations of process algebra and concurrency theory, in<br/>such a way that it can be effectively used in verification, is for a me<br/>a challenging task, involving many exciting open questions.<br/><br/>A vaguely related subject that excites me since the end of last year,<br/>is the extension of standard process algebra with a time-out operator,<br/>while still abstaining from quantifying time. I believe that such an<br/>extension reaches exactly the right level of expressiveness necessary<br/>for many applications.<br/><br/>Both topics also relate with the impossibility of correctly expressing<br/>expressing mutual exclusion in standard process algebras, a discovery<br/>that called forth many philosophical questions, such as under which<br/>assumptions on our hardware is mutual exclusion, when following the<br/>formal definitions of Dijkstra and others, even theoretically implementable.</div><div> </div></div>
    </content>
    <updated>2020-04-30T21:16:00Z</updated>
    <published>2020-04-30T21:16:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-04-30T21:16:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14318</id>
    <link href="http://arxiv.org/abs/2004.14318" rel="alternate" type="text/html"/>
    <title>The Dual Polynomial of Bipartite Perfect Matching</title>
    <feedworld_mtime>1588204800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beniamini:Gal.html">Gal Beniamini</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14318">PDF</a><br/><b>Abstract: </b>We obtain a description of the Boolean dual function of the Bipartite Perfect
Matching decision problem, as a multilinear polynomial over the Reals. We show
that in this polynomial, both the number of monomials and the magnitude of
their coefficients are at most exponential in $\mathcal{O}(n \log n)$. As an
application, we obtain a new upper bound of $\mathcal{O}(n^{1.5} \sqrt{\log
n})$ on the approximate degree of the bipartite perfect matching function,
improving the previous best known bound of $\mathcal{O}(n^{1.75})$. We deduce
that, beyond a $\mathcal{O}(\sqrt{\log n})$ factor, the polynomial method
cannot be used to improve the lower bound on the bounded-error quantum query
complexity of bipartite perfect matching.
</p></div>
    </summary>
    <updated>2020-04-30T23:21:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-04-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14304</id>
    <link href="http://arxiv.org/abs/2004.14304" rel="alternate" type="text/html"/>
    <title>Bipartite Stochastic Matching: Online, Random Order, and I.I.D. Models</title>
    <feedworld_mtime>1588204800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Borodin:Allan.html">Allan Borodin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/MacRury:Calum.html">Calum MacRury</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rakheja:Akash.html">Akash Rakheja</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14304">PDF</a><br/><b>Abstract: </b>Within the context of stochastic probing with commitment, we consider the
stochastic rewards problem, that is, the one sided online bipartite matching
problem where edges adjacent to an online node must be probed to determine if
they exist, based on known edge probabilities. If a probed edge exists, it must
be used in the matching (if possible). We consider the competitiveness of
online algorithms in four different models. Namely, we consider the following
stochastic reward problems: when the stochastic graph is not known and the
order of online arrivals is determined uniformly at random (i.e., the ROM
model); when the stochastic graph is known to the algorithm and the order of
arrival of online vertices is determined adversarially; when the stochastic
graph is known and the order of arrival of online vertices is determined
uniformly at random; and finally when the online vertices are generated i.i.d.
from a known distribution on the vertices of a known stochastic (type) graph.
In all these settings, the algorithm does not have any control on the ordering
of the online nodes. We study these models under various assumptions on the
patience (number of given probes) of the online vertices, and whether edges or
offline vertices are weighted in determining the stochastic reward. Our main
contribution is to establish new and improved competitive bounds in these four
models.
</p>
<p>In all settings, there is one ideal benchmark (the optimal offline probing
algorithm) relative to which the competitive ratio is defined. For establishing
competitive ratios we introduce a new LP relaxation which upper bounds the
performance of the ideal benchmark.
</p></div>
    </summary>
    <updated>2020-04-30T22:30:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14286</id>
    <link href="http://arxiv.org/abs/2004.14286" rel="alternate" type="text/html"/>
    <title>A Relative Theory of Interleavings</title>
    <feedworld_mtime>1588204800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Botnan:Magnus_Bakke.html">Magnus Bakke Botnan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Curry:Justin.html">Justin Curry</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munch:Elizabeth.html">Elizabeth Munch</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14286">PDF</a><br/><b>Abstract: </b>The interleaving distance, although originally developed for persistent
homology, has been generalized to measure the distance between functors modeled
on many posets or even small categories. Existing theories require that such a
poset have a superlinear family of translations or a similar structure.
However, many posets of interest to topological data analysis, such as zig-zag
posets and the face relation poset of a cell-complex, do not admit interesting
translations, and consequently don't admit a nice theory of interleavings. In
this paper we show how one can side-step this limitation by providing a general
theory where one maps to a poset that does admit interesting translations, such
as the lattice of down sets, and then defines interleavings relative to this
map. Part of our theory includes a rigorous notion of discretization or
"pixelization" of poset modules, which in turn we use for interleaving
inference. We provide an approximation condition that in the setting of
lattices gives rise to two possible pixelizations, both of which are guaranteed
to be close in the interleaving distance. Finally, we conclude by considering
interleaving inference for cosheaves over a metric space and give an explicit
description of interleavings over a grid structure on Euclidean space.
</p></div>
    </summary>
    <updated>2020-04-30T22:32:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14147</id>
    <link href="http://arxiv.org/abs/2004.14147" rel="alternate" type="text/html"/>
    <title>On the Existence of Algebraically Natural Proofs</title>
    <feedworld_mtime>1588204800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chatterjee:Prerona.html">Prerona Chatterjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Mrinal.html">Mrinal Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramya:C=.html">C. Ramya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saptharishi:Ramprasad.html">Ramprasad Saptharishi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tengse:Anamay.html">Anamay Tengse</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14147">PDF</a><br/><b>Abstract: </b>For every constant $c &gt; 0$, we show that there is a family $\{P_{N, c}\}$ of
polynomials whose degree and algebraic circuit complexity are polynomially
bounded in the number of variables, that satisfies the following properties:
</p>
<p>$\bullet$ For every family $\{f_n\}$ of polynomials in VP, where $f_n$ is an
$n$ variate polynomial of degree at most $n^c$ with bounded integer
coefficients and for $N = \binom{n^c + n}{n}$, $P_{N,c}$ \emph{vanishes} on the
coefficient vector of $f_n$.
</p>
<p>$\bullet$ There exists a family $\{h_n\}$ of polynomials where $h_n$ is an
$n$ variate polynomial of degree at most $n^c$ with bounded integer
coefficients such that for $N = \binom{n^c + n}{n}$, $P_{N,c}$ \emph{does not
vanish} on the coefficient vector of $h_n$.
</p>
<p>In other words, there are efficiently computable defining equations for
polynomials in VP that have small integer coefficients. In fact, we also prove
an analogous statement for the seemingly larger class VNP. Thus, in this
setting of polynomials with small integer coefficients, this provides evidence
\emph{against} a natural proof like barrier for proving algebraic circuit lower
bounds, a framework for which was proposed in the works of Forbes, Shpilka and
Volk (2018), and Grochow, Kumar, Saks and Saraf (2017).
</p>
<p>Our proofs are elementary and rely on the existence of (non-explicit) hitting
sets for VP (and VNP) to show that there are efficiently constructible, low
degree defining equations for these classes and also extend to finite fields of
small size.
</p></div>
    </summary>
    <updated>2020-04-30T23:21:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-04-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14102</id>
    <link href="http://arxiv.org/abs/2004.14102" rel="alternate" type="text/html"/>
    <title>Dense Steiner problems: Approximation algorithms and inapproximability</title>
    <feedworld_mtime>1588204800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karpinski:Marek.html">Marek Karpinski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lewandowski:Mateusz.html">Mateusz Lewandowski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meesum:Syed_Mohammad.html">Syed Mohammad Meesum</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mnich:Matthias.html">Matthias Mnich</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14102">PDF</a><br/><b>Abstract: </b>The Steiner Tree problem is a classical problem in combinatorial
optimization: the goal is to connect a set $T$ of terminals in a graph $G$ by a
tree of minimum size. Karpinski and Zelikovsky (1996) studied the
$\delta$-dense version of {\sc Steiner Tree}, where each terminal has at least
$\delta |V(G)\setminus T|$ neighbours outside $T$, for a fixed $\delta &gt; 0$.
They gave a PTAS for this problem.
</p>
<p>We study a generalization of pairwise $\delta$-dense {\sc Steiner Forest},
which asks for a minimum-size forest in $G$ in which the nodes in each terminal
set $T_1,\dots,T_k$ are connected, and every terminal in $T_i$ has at least
$\delta |T_j|$ neighbours in $T_j$, and at least $\delta|S|$ nodes in $S =
V(G)\setminus (T_1\cup\dots\cup T_k)$, for each $i, j$ in $\{1,\dots, k\}$ with
$i\neq j$. Our first result is a polynomial-time approximation scheme for all
$\delta &gt; 1/2$. Then, we show a $(\frac{13}{12}+\varepsilon)$-approximation
algorithm for $\delta = 1/2$ and any $\varepsilon &gt; 0$. We also consider the
$\delta$-dense Group Steiner Tree problem as defined by Hauptmann and show that
the problem is $\mathsf{APX}$-hard.
</p></div>
    </summary>
    <updated>2020-04-30T22:24:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.14064</id>
    <link href="http://arxiv.org/abs/2004.14064" rel="alternate" type="text/html"/>
    <title>Quantum and approximation algorithms for maximum witnesses of Boolean matrix products</title>
    <feedworld_mtime>1588204800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Mirosław Kowaluk, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lingas:Andrzej.html">Andrzej Lingas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14064">PDF</a><br/><b>Abstract: </b>The problem of finding maximum (or minimum) witnesses of the Boolean product
of two Boolean matrices (MW for short) has a number of important applications,
in particular the all-pairs lowest common ancestor (LCA) problem in directed
acyclic graphs (dags). The best known upper time-bound on the MW problem for
n\times n Boolean matrices of the form O(n^{2.575}) has not been substantially
improved since 2006. In order to obtain faster algorithms for this problem, we
study quantum algorithms for MW and approximation algorithms for MW (in the
standard computational model). Some of our quantum algorithms are input or
output sensitive. Our fastest quantum algorithm for the MW problem, and
consequently for the related problems, runs in time
\tilde{O}(n^{2+\lambda/2})=\tilde{O}(n^{2.434}), where \lambda satisfies the
equation \omega(1, \lambda, 1) = 1 + 1.5 \, \lambda and \omega(1, \lambda, 1)
is the exponent of the multiplication of an n \times n^{\lambda}$ matrix by an
n^{\lambda} \times n matrix. Next, we consider a relaxed version of the MW
problem (in the standard model) asking for reporting a witness of bounded rank
(the maximum witness has rank 1) for each non-zero entry of the matrix product.
By reducing the relaxed problem to the so called k-witness problem, we provide
an algorithm that reports for each non-zero entry C[i,j] of the product matrix
C a witness of rank O(\lceil W_C(i,j)/k\rceil ), where W_C(i,j) is the number
of witnesses for C[i,j], with high probability. The algorithm runs in
\tilde{O}(n^{\omega}k^{0.4653} +n^2k) time, where \omega=\omega(1,1,1).
</p></div>
    </summary>
    <updated>2020-04-30T22:22:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.13978</id>
    <link href="http://arxiv.org/abs/2004.13978" rel="alternate" type="text/html"/>
    <title>Planted Models for the Densest $k$-Subgraph Problem</title>
    <feedworld_mtime>1588204800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Yash Khanna, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Louis:Anand.html">Anand Louis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.13978">PDF</a><br/><b>Abstract: </b>Given an undirected graph $G$, the Densest $k$-subgraph problem (DkS) asks to
compute a set $S \subset V$ of cardinality $\left\lvert S\right\rvert \leq k$
such that the weight of edges inside $S$ is maximized. This is a fundamental
NP-hard problem whose approximability, inspite of many decades of research, is
yet to be settled. The current best known approximation algorithm due to
Bhaskara et al. (2010) computes a $\mathcal{O}\left({n^{1/4 +
\epsilon}}\right)$ approximation in time
$n^{\mathcal{O}\left(1/\epsilon\right)}$.
</p>
<p>We ask what are some "easier" instances of this problem? We propose some
natural semi-random models of instances with a planted dense subgraph, and
study approximation algorithms for computing the densest subgraph in them.
These models are inspired by the semi-random models of instances studied for
various other graph problems such as the independent set problem, graph
partitioning problems etc. For a large range of parameters of these models, we
get significantly better approximation factors for the Densest $k$-subgraph
problem. Moreover, our algorithm recovers a large part of the planted solution.
</p></div>
    </summary>
    <updated>2020-04-30T22:29:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.13933</id>
    <link href="http://arxiv.org/abs/2004.13933" rel="alternate" type="text/html"/>
    <title>On the complexity of Winner Verification and Candidate Winner for Multiwinner Voting Rules</title>
    <feedworld_mtime>1588204800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sonar:Chinmay.html">Chinmay Sonar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dey:Palash.html">Palash Dey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Misra:Neeldhara.html">Neeldhara Misra</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.13933">PDF</a><br/><b>Abstract: </b>The Chamberlin-Courant and Monroe rules are fundamental and well-studied
rules in the literature of multi-winner elections. The problem of determining
if there exists a committee of size k that has a Chamberlin-Courant
(respectively, Monroe) score of at most r is known to be NP-complete. We
consider the following natural problems in this setting: a) given a committee S
of size k as input, is it an optimal k-sized committee, and b) given a
candidate c and a committee size k, does there exist an optimal k-sized
committee that contains c?
</p>
<p>In this work, we resolve the complexity of both problems for the
Chamberlin-Courant and Monroe voting rules in the settings of rankings as well
as approval ballots. We show that verifying if a given committee is optimal is
coNP-complete whilst the latter problem is complete for $\Theta_{2}^{P}$. We
also demonstrate efficient algorithms for the second problem when the input
consists of single-peaked rankings. Our contribution fills an essential gap in
the literature for these important multi-winner rules.
</p></div>
    </summary>
    <updated>2020-04-30T22:29:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.13891</id>
    <link href="http://arxiv.org/abs/2004.13891" rel="alternate" type="text/html"/>
    <title>Hierarchy-Based Algorithms for Minimizing Makespan under Precedence and Communication Constraints</title>
    <feedworld_mtime>1588204800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kulkarni:Janardhan.html">Janardhan Kulkarni</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Shi.html">Shi Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tarnawski:Jakub.html">Jakub Tarnawski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Ye:Minwei.html">Minwei Ye</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.13891">PDF</a><br/><b>Abstract: </b>We consider the classic problem of scheduling jobs with precedence
constraints on a set of identical machines to minimize the makespan objective
function. Understanding the exact approximability of the problem when the
number of machines is a constant is a well-known question in scheduling theory.
Indeed, an outstanding open problem from the classic book of Garey and Johnson
asks whether this problem is NP-hard even in the case of 3 machines and
unit-length jobs. In a recent breakthrough, Levey and Rothvoss gave a
$(1+\epsilon)$-approximation algorithm, which runs in nearly quasi-polynomial
time, for the case when job have unit lengths. However, a substantially more
difficult case where jobs have arbitrary processing lengths has remained open.
</p>
<p>We make progress on this more general problem. We show that there exists a
$(1+\epsilon)$-approximation algorithm (with similar running time as that of
Levey and Rothvoss) for the non-migratory setting: when every job has to be
scheduled entirely on a single machine, but within a machine the job need not
be scheduled during consecutive time steps. Further, we also show that our
algorithmic framework generalizes to another classic scenario where, along with
the precedence constraints, the jobs also have communication delay constraints.
Both of these fundamental problems are highly relevant to the practice of
datacenter scheduling.
</p></div>
    </summary>
    <updated>2020-04-30T22:31:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.13881</id>
    <link href="http://arxiv.org/abs/2004.13881" rel="alternate" type="text/html"/>
    <title>A Stochastic Team Formation Approach for Collaborative Mobile Crowdsourcing</title>
    <feedworld_mtime>1588204800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hamrouni:Aymen.html">Aymen Hamrouni</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghazzai:Hakim.html">Hakim Ghazzai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alelyani:Turki.html">Turki Alelyani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Massoud:Yehia.html">Yehia Massoud</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.13881">PDF</a><br/><b>Abstract: </b>Mobile Crowdsourcing (MCS) is the generalized act of outsourcing sensing
tasks, traditionally performed by employees or contractors, to a large group of
smart-phone users by means of an open call. With the increasing complexity of
the crowdsourcing applications, requesters find it essential to harness the
power of collaboration among the workers by forming teams of skilled workers
satisfying their complex tasks' requirements. This type of MCS is called
Collaborative MCS (CMCS). Previous CMCS approaches have mainly focused only on
the aspect of team skills maximization. Other team formation studies on social
networks (SNs) have only focused on social relationship maximization. In this
paper, we present a hybrid approach where requesters are able to hire a team
that, not only has the required expertise, but also is socially connected and
can accomplish tasks collaboratively. Because team formation in CMCS is proven
to be NP-hard, we develop a stochastic algorithm that exploit workers knowledge
about their SN neighbors and asks a designated leader to recruit a suitable
team. The proposed algorithm is inspired from the optimal stopping strategies
and uses the odds-algorithm to compute its output. Experimental results show
that, compared to the benchmark exponential optimal solution, the proposed
approach reduces computation time and produces reasonable performance results.
</p></div>
    </summary>
    <updated>2020-04-30T23:20:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-04-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.13748</id>
    <link href="http://arxiv.org/abs/2004.13748" rel="alternate" type="text/html"/>
    <title>Learning Polynomials of Few Relevant Dimensions</title>
    <feedworld_mtime>1588204800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Sitan.html">Sitan Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meka:Raghu.html">Raghu Meka</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.13748">PDF</a><br/><b>Abstract: </b>Polynomial regression is a basic primitive in learning and statistics. In its
most basic form the goal is to fit a degree $d$ polynomial to a response
variable $y$ in terms of an $n$-dimensional input vector $x$. This is extremely
well-studied with many applications and has sample and runtime complexity
$\Theta(n^d)$. Can one achieve better runtime if the intrinsic dimension of the
data is much smaller than the ambient dimension $n$? Concretely, we are given
samples $(x,y)$ where $y$ is a degree at most $d$ polynomial in an unknown
$r$-dimensional projection (the relevant dimensions) of $x$. This can be seen
both as a generalization of phase retrieval and as a special case of learning
multi-index models where the link function is an unknown low-degree polynomial.
Note that without distributional assumptions, this is at least as hard as junta
learning.
</p>
<p>In this work we consider the important case where the covariates are
Gaussian. We give an algorithm that learns the polynomial within accuracy
$\epsilon$ with sample complexity that is roughly $N = O_{r,d}(n
\log^2(1/\epsilon) (\log n)^d)$ and runtime $O_{r,d}(N n^2)$. Prior to our
work, no such results were known even for the case of $r=1$. We introduce a new
filtered PCA approach to get a warm start for the true subspace and use
geodesic SGD to boost to arbitrary accuracy; our techniques may be of
independent interest, especially for problems dealing with subspace recovery or
analyzing SGD on manifolds.
</p></div>
    </summary>
    <updated>2020-04-30T22:30:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7481748881751465174</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7481748881751465174/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/a-guest-blog-on-pandemics-affect-on.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7481748881751465174" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7481748881751465174" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/a-guest-blog-on-pandemics-affect-on.html" rel="alternate" type="text/html"/>
    <title>A Guest Blog on the Pandemic's affect on disability students</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>I asked my Grad Ramsey Theory class to email me about whatever thoughts they have on the pandemic that they want to share with the world, with the intend of making some of them into a blog post. I thought there would be several short thoughts for one post. And I may still do that post. But I got a FANTASTIC long answer from one Emily Mae Kaplitz. Normally I would ask to shorten or edit a guest post, but I didn't do that here since that might make it less authentic.</div><div><br/></div><div>Here is Emily Kaplitz's email (with her enthusiastic permission)</div><div><br/></div><div>--------------------------------------------------------------------------------------</div><div><br/></div><div>Ok so this might be super ranty, (It definitely is.) but I think it is super important to bring up in a blog post written by an academic that will be probably read by other academics. </div><div><br/></div><div>The students that are being most affected by this pandemic with online learning are disability students. As a disability student, we carefully cultivate the way that we learn best based off of years of trial and error. This is harder than anything else, we have to face in our lifetime. Most of the time disability students are left on the back burner and that statement is so much more prevalent right now. My friends brother is autistic. He is struggling so much right now because he is at home. Disability students learn what environment works best for them and at home is usually not the best place. We have to split our lives into different boxes that each have different tools to help us get our brains to focus and work well when we need them too. Disability students will rely on everything being planned out, so that they can succeed. Teachers and professors cannot understand the stress and strain that having to work at home puts on the student. Every time I go to another school, it is a struggle to figure out what new thing I need to add into the mix and what old thing I need to throw away. It's exhausting, but when I go from one school to another I at least know that the basics are the same. I sit in a classroom, the professors lecturer, and then I do work at home that is assigned to me. Changing to online changes that dynamic so much. A professor cannot see when a student is visibly struggling with a topic because we'll all behind computers. A neurotypical person might ask, "well why don't you just ask a question? Why don't you just let the professor know that you don't understand". Let me answer that simply. If all your life you've been silenced because of something that you cannot control, is your first reaction to speak out or to stay silent. It is so hard for disability students to ask a question after we've been labeled the dumb kid. Every time we ask a question, we always have the thought of: is this going to make me sound stupid. We've worked so hard to eliminate that word from our vocabulary and from others who will throw that word back at us. Disability students are being left in the hands of their parents and teachers/professors who do not understand us and our needs even if they try to or want to. It is so hard for us to explain what our normal is because we don't live your normal and therefore don't know the difference. Many disability students have their confidence slashed the moment they enter a classroom and realize that they are not like the other kids. Even more so because they don't understand why they aren't. Disability students are one of the most hard-working individuals when we have a cheerleader to cheer us on because it's hard. It's harder than anything anyone has to do. Because no one listens to you when you are stupid and no one cares for you if you're not easy to care for unless they are given a specific reason to. Fighting a losing battle every day is awful. Now imagine all of your weapons that you have carefully crafted over the years have been taken away and you are left defenseless. While we have things like ADS that are supposed to help support us, it's not enough. Just like putting a Band-Aid on an open infected wound will not be enough. Now more than ever we need to learn from this as academics. We need to learn that helping disability students does not only help disability students. It helps all students because all students learn differently. All students if given the chance can excel at any field that we put them in. We just have to figure out the best way to get that student to shine. That is one of the reasons why I am a PhD student right now. I saw in the tutoring center at my undergrad how many students came to me with so much frustration about something they are doing in class. Both students with disabilities and without. These students are constantly apologizing because they don't understand something. In one session by just changing the way that we talk about a subject the student was able to get it in less time than the professor taught it. I've had students come to me after an exam and tell me that the only reason they got the grade that they did was because in their head was my voice coaching them on a subject. We are not teaching optimally. We are teaching the way that it has been done for years and years and years and that is not the best way to teach. It might be the best way to teach the strongest links but really the link that matters the most is the weakest link that will snap under pressure because you can't pull a tractor with a broken link. Disability students think differently. Imagine how many impossible problems we can solve when we have people that think differently. But that's just my two cents as a disability student who is struggling and sees other disability students struggling every day. And really just wants to help all students succeed.</div><div><br/></div><div>I blame any misspellings, grammar errors, and run on sentences on my speech to text and text to speech. This was a long email and if we were on tumblr, I would post a potato at the end. Since we aren't, I will leave this email with this. Thank you for taking the time to read this rant. Even if you don't include this in your blog post, I believe one person reading this has made the difference.</div><div><br/></div><div>Thanks!</div><div><br/></div><div>Emily Mae Kaplitz</div></div>
    </content>
    <updated>2020-04-29T13:44:00Z</updated>
    <published>2020-04-29T13:44:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-04-30T19:55:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/063</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/063" rel="alternate" type="text/html"/>
    <title>TR20-063 |  On the Existence of Algebraically Natural Proofs | 

	Prerona Chatterjee, 

	Mrinal Kumar, 

	C Ramya, 

	Ramprasad Saptharishi, 

	Anamay Tengse</title>
    <summary>For every constant c &gt; 0, we show that there is a family {P_{N,c}} of polynomials whose degree and algebraic circuit complexity are polynomially bounded in the number of variables, and that satisfies the following properties:
* For every family {f_n} of polynomials in VP, where f_n is an n variate polynomial of degree at most n^c with bounded integer coefficients and for N = \binom{n^c + n}{n}, P_{N,c} -vanishes- on the coefficient vector of f_n.
* There exists a family {h_n} of polynomials where h_n is an n variate polynomial of degree at most n^c with bounded integer coefficients such that for N = \binom{n^c + n}{n}, P_{N,c} -does not vanish- on the coefficient vector of h_n.

In other words, there are efficiently computable defining equations for polynomials in VP that have small integer coefficients.
In fact, we also prove an analogous statement for the seemingly larger class VNP. Thus, in this setting of polynomials with small integer coefficients, this provides evidence -against- a natural proof like barrier for proving algebraic circuit lower bounds, a framework for which was proposed in the works of Forbes, Shpilka and Volk (2018), and Grochow, Kumar, Saks and Saraf (2017).

Our proofs are elementary and rely on the existence of (non-explicit) hitting sets for VP (and VNP) to show that there are efficiently constructible, low degree defining equations for these classes, and also extend to finite fields of small size.</summary>
    <updated>2020-04-29T13:29:05Z</updated>
    <published>2020-04-29T13:29:05Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-01T10:20:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/062</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/062" rel="alternate" type="text/html"/>
    <title>TR20-062 |  Testing Data Binnings | 

	Clement Canonne, 

	Karl  Wimmer</title>
    <summary>Motivated by the question of data quantization and "binning," we revisit the problem of identity testing of discrete probability distributions. Identity testing (a.k.a. one-sample testing), a fundamental and by now well-understood problem in distribution testing, asks, given a reference distribution (model) $\mathbf{q}$ and samples from an unknown distribution $\mathbf{p}$, both over $[n]=\{1,2,\dots,n\}$, whether $\mathbf{p}$ equals $\mathbf{q}$, or is significantly different from it.

In this paper, we introduce the related question of identity up to binning, where the reference distribution $\mathbf{q}$ is over $k \ll n$ elements: the question is then whether there exists a suitable binning of the domain $[n]$ into $k$ intervals such that, once "binned," $\mathbf{p}$ is equal to $\mathbf{q}$. We provide nearly tight upper and lower bounds on the sample complexity of this new question, showing both a quantitative and qualitative difference with the vanilla identity testing one, and answering an open question of Canonne (2019). Finally, we discuss several extensions and related research directions.</summary>
    <updated>2020-04-29T03:23:33Z</updated>
    <published>2020-04-29T03:23:33Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-01T10:20:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/04/28/cartesian-triangle-centers</id>
    <link href="https://11011110.github.io/blog/2020/04/28/cartesian-triangle-centers.html" rel="alternate" type="text/html"/>
    <title>Cartesian triangle centers</title>
    <summary>The usual definition of a Euclidean triangle center is a point defined from a triangle in a way that is equivariant under similarity transformations, meaning that applying the transformation to a triangle and then constructing its center produces the same point as constructing the center first and then transforming it. These include familiar points defined from triangles like the orthocenter (where perpendiculars to the sides meet), incenter (the center of the inscribed circle), and circumcenter (the center of the circumscribed circle). What I have in mind in this post is a similarly-defined concept, but with a different group of transformations than similarity: the combination of any two separate linear transformations to the two coordinate axes of the Cartesian plane, or transformations that swap the (transformed) axes. Another way of describing these transformations is that they preserve axis-parallel lines and relative area. They also preserve other lines and parallelism of lines.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The usual definition of a Euclidean <a href="https://en.wikipedia.org/wiki/Triangle_center">triangle center</a> is a point defined from a triangle in a way that is <a href="https://en.wikipedia.org/wiki/Equivariant_map">equivariant</a> under <a href="https://en.wikipedia.org/wiki/Similarity_(geometry)">similarity transformations</a>, meaning that applying the transformation to a triangle and then constructing its center produces the same point as constructing the center first and then transforming it. These include familiar points defined from triangles like the <a href="https://en.wikipedia.org/wiki/Orthocenter">orthocenter</a> (where perpendiculars to the sides meet), <a href="https://en.wikipedia.org/wiki/Incenter">incenter</a> (the center of the inscribed circle), and <a href="https://en.wikipedia.org/wiki/Circumcenter">circumcenter</a> (the center of the circumscribed circle). What I have in mind in this post is a similarly-defined concept, but with a different group of transformations than similarity: the combination of any two separate linear transformations to the two coordinate axes of the <a href="https://en.wikipedia.org/wiki/Cartesian_plane">Cartesian plane</a>, or transformations that swap the (transformed) axes. Another way of describing these transformations is that they preserve axis-parallel lines and relative area. They also preserve other lines and parallelism of lines.</p>

<p>One of the standard Euclidean triangle centers turns out to be a Cartesian center as well: the centroid has as its coordinates the average of the three triangle vertex coordinates, and this coordinatewise calculation (independent of the ordering of the three vertices) shows it to be equivariant under Cartesian transformations. It is also the point where the three lines through each vertex and opposite side midpoint meet.</p>

<p style="text-align: center;"><img alt="Centroid of a triangle" src="https://11011110.github.io/blog/assets/2020/centroid.svg"/></p>

<p>Similarly, we can find a Cartesian triangle center from the coordinatewise median of the three vertices.</p>

<p style="text-align: center;"><img alt="Median of a triangle" src="https://11011110.github.io/blog/assets/2020/median.svg"/></p>

<p>The center of the axis-parallel bounding box is also a Cartesian triangle center, again computed coordinatewise as the average of the minimum and maximum coordinates.</p>

<p style="text-align: center;"><img alt="Bounding-box center of a triangle" src="https://11011110.github.io/blog/assets/2020/bbcenter.svg"/></p>

<p>But a Cartesian triangle center does not have to be determined coordinatewise in this way. As an example, denote the three triangle vertices by  and consider the point  with the property that the axis-parallel bounding boxes of the three line segments  all have equal area. For two points  and , the locus of points for which the two bounding boxes have equal area is a line through the other two corners of the bounding box of segment . (To see this, consider a linear transformation that takes this bounding box to a square, and apply symmetry.) The three lines defined in this way from the three pairs of vertices of a non-degenerate triangle always meet in a point, the center we are seeking. (This follows from the shared equality of the three rectangles, but it is also an instance of the dual of <a href="https://en.wikipedia.org/wiki/Pappus%27s_hexagon_theorem">Pappus’s theorem</a>.) And this point is equivariant under Cartesian transformations because these transformations preserve the ratios of areas of bounding rectangles.</p>

<p style="text-align: center;"><img alt="Point of equal bounding boxes of a triangle" src="https://11011110.github.io/blog/assets/2020/equalbb.svg"/></p>

<p>You can tell it’s not a coordinatewise center because, in the figure above, the triangle vertices have equally spaced -coordinates, from which it follows that all coordinatewise centers lie on the horizontal line with median -coordinate, but this center doesn’t.</p>

<p>In general, the geometry of the plane with Cartesian instead of Euclidean transformations does not seem to have been very thoroughly explored. These examples show, I think, that there are interesting things to find there.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104079992540248342">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-04-28T21:24:00Z</updated>
    <published>2020-04-28T21:24:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-01T05:01:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4369</id>
    <link href="https://lucatrevisan.wordpress.com/2020/04/28/phase-2-ready-or-not-here-it-comes/" rel="alternate" type="text/html"/>
    <title>Phase 2, ready or not, here it comes</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Yesterday the Italian prime minister announced the timeline of the loosening of the lockdown. Some manufacturing restarted yesterday, and some customer-facing businesses will gradually reopen between next Monday and the beginning of June. Since the start of the lockdown 51 … <a href="https://lucatrevisan.wordpress.com/2020/04/28/phase-2-ready-or-not-here-it-comes/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Yesterday the Italian prime minister announced the timeline of the loosening of the lockdown. Some manufacturing restarted yesterday, and some customer-facing businesses will gradually reopen between next Monday and the beginning of June.</p>
<p><span id="more-4369"/></p>
<p>Since the start of the lockdown 51 days ago, it was clear that the condition to “reopen” was to have in place a “test-trace-isolate” plan to find infected people as soon as possible after their contagion, trace their contacts, and isolate them. For this, one needs an infrastructure for large-scale testing with quick turnaround, a well-staffed agency to do manual tracing or an app to do it automatically, and facilities to isolate people who are infected and not in need of hospitalization.</p>
<p>None of this has been done. There hasn’t been a sufficient ramp-up of testing capacity; as far as I know no additional people have been hired and trained for manual contact-tracing; there is an app for digital contact-tracing, but the plan to adopt it appears to have been shelved; if people test positive and are well they are asked to stay home, potentially with their family members, who are free to leave as they please.</p>
<p>All our eggs are in the basket of social distancing. The humor site Kotiomkin posted “Basically, phase 2 will rely on everybody’s common sense. We are fucked”.</p>
<p>The problem is that social distancing requires a common-sense avoidance of close contacts with other people, and the government can give guidelines on how to achieve it, but eventually it has to rely on everybody’s sense of responsibility. Unfortunately, the national mood around regulations is to immediately look for loopholes. </p>
<p>For example the initial lockdown measures stipulated that one could leave home to go buy groceries. Then when the police would stop people tens of miles away from their home, people would say “I drove here to buy groceries”, “but we are thirty miles away from where you live”, “yes but here the groceries are better”. So a subsequent amendment stipulated that one could buy groceries only in the town of residence, making it hard for people living next to a town border, for whom the closest grocery store was across the town line. In fact, every time I have encountered a crazy Italian law or regulation, and asked around for the likely reason it was instituted, it was usually to close a loophole in a previous regulation, which in turn had been put in place to close a loophole in a third regulation, and basically it’s loophole-closing all the way down to <a href="https://en.wikipedia.org/wiki/Roman_law">Roman law</a>.</p>
<p>I think that, from this point on, the story of the Italian covid-19 epidemic will not show the future of the rest of the Western world, but will evolve in its own timeline. Meanwhile, there are a couple of lessons that are still relevant, particularly in the comparison between Lombardy and NYC, which continue to track each other remarkably well.</p>
<p><img alt="2020-05-28-nyc-daily" class="alignnone size-full wp-image-4375" src="https://lucatrevisan.files.wordpress.com/2020/04/2020-05-28-nyc-daily.png?w=584"/><img alt="2020-05-28-nyc" class="alignnone size-full wp-image-4376" src="https://lucatrevisan.files.wordpress.com/2020/04/2020-05-28-nyc.png?w=584"/></p>
<p>One is that, at one point, it was decided to move older people with mild cases of covid-19 from hospital to nursing homes, to open up beds in hospitals. Since the personnel of nursing homes are not trained in the safety procedures for infectious diseases, and since nursing homes host older, frail people who are the highest-risk category for this illness, the result was a huge number of deaths in these nursing homes. Now nursing homes in New York are <a href="https://www.nytimes.com/2020/04/24/us/nursing-homes-coronavirus.html">being asked to take covid-19 patients from hospitals</a>.</p>
<p>The other is that an analysis of all-cause mortality shows a spike in March such that the difference between the typical March all-cause mortality and the March 2020 all-cause mortality is much bigger than the number of confirmed covid-19 deaths. Now the same phenomenon is being <a href="https://www.nytimes.com/interactive/2020/04/27/upshot/coronavirus-deaths-new-york-city.html"> observed in New York City</a>, with numbers similar to Lombardy’s. Notably, the baseline all-case mortality rate in Lombardy is much higher than in New York City (because  population growth has stalled and the demographic skews older), so while the absolute number of additional deaths is similar, the relative increase is a much more dramatic 6x in NYC versus roughly 4x in Lombardy. </p></div>
    </content>
    <updated>2020-04-28T16:40:10Z</updated>
    <published>2020-04-28T16:40:10Z</published>
    <category term="Milan"/>
    <category term="New York"/>
    <category term="covid-19"/>
    <category term="phase 2"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-05-01T10:20:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/061</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/061" rel="alternate" type="text/html"/>
    <title>TR20-061 |  Tree-depth and the Formula Complexity of Subgraph Isomorphism | 

	Benjamin Rossman, 

	Deepanshu Kush</title>
    <summary>For a fixed "pattern" graph $G$, the $\textit{colored}$ $G\textit{-subgraph isomorphism problem}$ (denoted $\mathrm{SUB}(G)$) asks, given an $n$-vertex graph $H$ and a coloring $V(H) \to V(G)$, whether $H$ contains a properly colored copy of $G$. The complexity of this problem is tied to parameterized versions of $\mathit{P}$ ${=}?$ $\mathit{NP}$ and $\mathit{L}$ ${=}?$ $\mathit{NL}$, among other questions. An overarching goal is to understand the complexity of $\mathrm{SUB}(G)$, under different computational models, in terms of natural invariants of the pattern graph $G$.

In this paper, we establish a close relationship between the $\textit{formula complexity}$ of $\mathrm{SUB}$ and an invariant known as $\textit{tree-depth}$ (denoted $\mathrm{td}(G)$). $\mathrm{SUB}(G)$ is known to be solvable by monotone $\mathit{AC^0}$ formulas of size $O(n^{\mathrm{td}(G)})$. Our main result is an $n^{\tilde\Omega(\mathrm{td}(G)^{1/3})}$ lower bound for formulas that are monotone $\textit{or}$ have sub-logarithmic depth. This complements a lower bound of Li, Razborov and Rossman (SICOMP 2017) relating tree-width and $\mathit{AC^0}$ circuit size. As a corollary, it implies a stronger homomorphism preservation theorem for first-order logic on finite structures (Rossman, ITCS 2017).

The technical core of this result is an $n^{\Omega(k)}$ lower bound in the special case where $G$ is a complete binary tree of height $k$, which we establish using the $\textit{pathset framework}$ introduced in (Rossman, SICOMP 2018). (The lower bound for general patterns follows via a recent excluded-minor characterization of tree-depth (Czerwi\'nski et al, arXiv:1904.13077).) Additional results of this paper extend the pathset framework and improve upon both, the best known upper and lower bounds on the average-case formula size of $\mathrm{SUB}(G)$ when $G$ is a path.</summary>
    <updated>2020-04-28T11:49:13Z</updated>
    <published>2020-04-28T11:49:13Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-01T10:20:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/04/28/tenure-track-assistant-associate-or-full-professor-theory-of-computation-at-university-of-groningen-the-netherlands-apply-by-may-5-2020/</id>
    <link href="https://cstheory-jobs.org/2020/04/28/tenure-track-assistant-associate-or-full-professor-theory-of-computation-at-university-of-groningen-the-netherlands-apply-by-may-5-2020/" rel="alternate" type="text/html"/>
    <title>Tenure Track Assistant, Associate or Full Professor Theory of Computation at University of Groningen, the Netherlands  (apply by May 5, 2020)</title>
    <summary>The University of Groningen seeks for an outward looking researcher in Computer Science who will perform research on theory of computation, broadly construed, in relation to new (neuromorphic) computing systems and architectures. We offer a challenging position in a unique world-class research environment, where close collaborations between research groups with different expertise are encouraged. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The University of Groningen seeks for an outward looking researcher in Computer Science who will perform research on theory of computation, broadly construed, in relation to new (neuromorphic) computing systems and architectures. We offer a challenging position in a unique world-class research environment, where close collaborations between research groups with different expertise are encouraged.</p>
<p>Website: <a href="https://www.rug.nl/about-ug/work-with-us/job-opportunities/?details=00347-02S0007KLP">https://www.rug.nl/about-ug/work-with-us/job-opportunities/?details=00347-02S0007KLP</a><br/>
Email: b.noheda@rug.nl, j.b.t.m.roerdink@rug.nl and/or j.h.m.van.der.velde@rug.nl</p></div>
    </content>
    <updated>2020-04-28T08:50:14Z</updated>
    <published>2020-04-28T08:50:14Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-01T10:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7716</id>
    <link href="https://windowsontheory.org/2020/04/27/lessons-from-covid-19-what-works-online-and-what-doesnt/" rel="alternate" type="text/html"/>
    <title>Lessons from COVID-19: What works online and what doesn’t</title>
    <summary>(I am now on Twitter , so you can follow this blog there too if you prefer it. –Boaz) Between Zoom meetings and deadlines, I thought I’d jot down a few of my impressions so far on what lessons we can draw from this period on how well research and education can work online. I’ve […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>(I am now on <a href="https://twitter.com/boazbaraktcs">Twitter</a> ,  so you can follow this blog there too if you prefer it. –Boaz)</p>



<p>Between Zoom meetings and deadlines, I thought I’d jot down a few of my impressions so far on what lessons we can draw from this period on how well research and education can work online.  I’ve had a few surprises in both directions – things that worked better than I would have expected, and aspects that were more problematic than I realized. These are personal impressions – please do comment on your own experiences.</p>



<p>As a rule of thumb, the interactions that most successfully replicate online are those that are relatively short and focused (an hour or so – e.g., a focused research meeting, seminar talk, or a lecture in a course).  Other interactions (e.g., faculty meetings) are also fairly easily to port online, perhaps because the original wasn’t that great to begin with.</p>



<p>The things that are harder to replicate are sustained interactions over longer periods. These include more extended and less directed research collaborations, informal workshops, as well as support for students outside lectures in education.</p>



<p><strong>Works well: Research seminars</strong></p>



<p>I’ve been pleasantly surprised by how effective research seminars such as our <a href="https://mltheory.org/">machine learning theory seminar</a> are over Zoom. In particular these were no less interactive than physical seminars – in fact people are offten <em>more</em> comfortable asking questions on chat than they would during in-person seminars. I hope such seminars become common practice even after this period ends- flying a speaker across the country or the world to give an hour talk doesn’t makes much sense given that there is a perfectly satisfactory alternative. </p>



<p><strong>Works well: Lectures</strong></p>



<p>This term I am teaching <a href="https://cs127.boazbarak.org/schedule/">cryptography</a>, and online lectures on Zoom have gone surprisingly well (after  working out some <a href="https://windowsontheory.org/2020/03/26/technology-for-theory-covid-19-edition/">technical issues</a>). Students participate on chat and ask questions, and seem to be following the lecture quite well. The important caveat is that lectures only work well for the students that attend and can follow them. For students who need extra support, it’s become much harder to access it.  It’s also much easier for students to (literally) “fall off the screen” and fall behind in a course, which brings me to the next point.</p>



<p><strong>Works less well: Support outside lectures</strong></p>



<p>Lectures are just one component of a course. Most of students’ learning occurs outside the classroom, where students meet together and work on problem sets, or discuss course material. These interactions between students (both related and unrelated to course) are where much of their intellectual growth happens. </p>



<p>All these interactions are greatly diminished online, and I did not yet see a good alternative. I’ve seen reduced attendance in office hours and sections, and reports are that students find it much harder to have the sort of chance discussions and opportunities to find study partners that they value so much.  If anything, this experience had made me <em>less</em> positive about the possibility of online education replacing physical colleges (though there are interesting <a href="https://en.wikipedia.org/wiki/Minerva_Schools_at_KGI">hybrid models</a>, where the students are co-located but lecturers are online).</p>



<p><strong>Works less well: unstructured research collaborations</strong></p>



<p>A focused meeting reporting on results or deciding on work allocation works pretty well over Zoom. So far it seems that extended brainstorming meetings, such as talking to someone over several hours in a coffeeshop, are much harder to replicate. In particular, a good part of such meetings is often spent with people staring in silence into their notebooks. As I <a href="https://twitter.com/boazbaraktcs/status/1253330145789673473">wrote</a>,  mutual silence seems to be very hard to do over Zoom.</p>



<p>Generally, informal week-long workshops, where much time is devoted to unstructured discussions, are ones that are most important to hold in person, and are hard (or maybe impossible) to replicate online. I have still not attended an online conference, but I suspect that these aspects of the conference would also be the ones hardest to replicate.</p>



<p><strong>Works well: faculty meetings</strong></p>



<p>I’ve always found it hard to bring a laptop to a faculty meeting and get work done, while listening with one ear to what’s going on. This is so much easier over Zoom <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/></p></div>
    </content>
    <updated>2020-04-27T21:01:00Z</updated>
    <published>2020-04-27T21:01:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-05-01T10:21:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1676</id>
    <link href="https://theorydish.blog/2020/04/27/whats-your-story/" rel="alternate" type="text/html"/>
    <title>What’s Your Story?</title>
    <summary>Last quarter, I taught a course on research methods in TOC, which gave me an opportunity to think through many aspects of research. I was promoting a human-centric perspective on research: how to facilitate better research by addressing the conditions needed for an individual researcher and groups of researchers to succeed. As science is a communal effort, the communication of science is critical, and thus one of the topics we covered is oral presentations. There are plenty of resources about research talks, and mostly they emphasize form over matter. How many words in a slide? How many slides in a talk? how to and how not to use font colors? How to and how not to use animation? and so on. While all of these are important, I find that the failing of many research talks is on a much more basic level. Think back to a research talk you heard recently, or to one you heard a few months ago. You may remember how you felt and what you thought of the talk but what do you remember of this talk in terms of content? Most of us will find that we don’t remember much, I rarely do. Yet [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Last quarter, I taught <a href="https://omereingold.wordpress.com/cs-353-the-practice-of-theory-research/">a course on research methods</a> in TOC, which gave me an opportunity to think through many aspects of research. I was promoting a human-centric perspective on research: how to facilitate better research by addressing the conditions needed for an individual researcher and groups of researchers to succeed. As science is a communal effort, the communication of science is critical, and thus one of the topics we covered is oral presentations.</p>
<p>There are plenty of resources about research talks, and mostly they emphasize form over matter. How many words in a slide? How many slides in a talk? how to and how not to use font colors? How to and how not to use animation? and so on. While all of these are important, I find that the failing of many research talks is on a much more basic level.</p>
<p>Think back to a research talk you heard recently, or to one you heard a few months ago. You may remember how you felt and what you thought of the talk but what do you remember of this talk in terms of content? Most of us will find that we don’t remember much, I rarely do. Yet in our presentations, we often follow a research-paper-like mold and squeeze in many little details that are somehow important to us, forgetting that they will all vanish in our audience’s memory soon after (or completely missed in the first place). Giving a talk (writing a paper, writing a blog post etc.) is about communication: who is your audience? what are the limitation of the medium? what is the message you want to convey? Since so little stays with the audience long term, it makes sense to make sure that this little will be what seems most important for you to convey.</p>
<p>The idea I am promoting here is not new, and there are various techniques towards this goal. One (which I think Oded Goldreich shared with me), is to think of audience’s attention as a limited currency. Whenever you share a big idea you spend a big token and other ideas cost a smaller token. Imagine you have one or two big tokens and a few smaller tokens. <a href="https://www.youtube.com/watch?v=5OFAhBw0OXs">Another approach</a>, emphasizes the notion of <strong>a premise</strong>. The idea promoted here is that a talk needs a premise and this should be the title of the talk. Furthermore, every slide needs a premise and it should be the title of the slide. A premise is a main idea and is a complete sentence. It is not unusual to find a slide titled “Analysis” or “Efficiency” but neither of these is a premise. “Problem X has an efficient algorithm” could be. The talk’s premise could help you distill what you want the audience to take out of your talk. It also helps shape the talk, as everything that doesn’t serve the premise shouldn’t be there. Note that each paper can provoke many different premises and thus many different talks.</p>
<p>Here I want to play with a different idea, that I find intriguing, even if it may seem a bit extreme. It will not be controversial that a good talk (and paper) tells a story. After all, humans understand and remember narratives. But could we take inspiration from the form of storytelling in fiction writing? A vast literature, classifies different kinds of stories and explores their templates (see for example <a href="http://storybistro.com/7-story-frameworks/">this short discussion</a>).  Can we find analogues to these types in scientific research talks?</p>
<p>The type of story that is easiest to relate to is the <strong>Quest/Hero’s Journey</strong> (think Lord of the Rings). These have several distinct ingredients: a call to adventure, tests, allies, enemies, ordeal, reward, victorious return. Some research talks that follow this template do it well and preserve a sense of suspense and excitement, others seem like a long list of problems and the tricks that the work uses to handle them.</p>
<p>I believe that many other story templates can find analogues is research talks as well. Here are my initial attempts:</p>
<ul>
<li><strong>Coming of age</strong> stories – this area of research previously only had naive ideas but this works brings significant depth.</li>
<li><strong>The Under<span style="color: #000000;">dog</span></strong><span style="color: #000000;"> (think David and Goliath): a modest technique that concurred a great challenge.</span></li>
<li><strong>Rags to Riches</strong> (think the Ugly Duckling): an area or technique that were not successful prove powerful.
<ul>
<li>Similarly: <strong>Rebirth</strong> (reinvention, renewal).</li>
</ul>
</li>
<li><strong>Comedy</strong> (or the Clarity Tale) – conceptual works shedding a new perspective.</li>
<li><strong>Tragedy</strong> (or the Cautionary Tale) – Some impossibility results come to mind (couldn’t we view Arrow’s impossibility theorem as being tragic?)</li>
<li><strong>Redemption stories</strong>: the field so far has missed the point, was misleading or harmful, but this work makes amends.</li>
</ul>
<p>Can you suggest papers and a story type that could fit them?</p></div>
    </content>
    <updated>2020-04-27T16:26:33Z</updated>
    <published>2020-04-27T16:26:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-05-01T10:21:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/060</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/060" rel="alternate" type="text/html"/>
    <title>TR20-060 |  Leakage-Resilient Extractors and Secret-Sharing against Bounded Collusion Protocols | 

	Eshan Chattopadhyay, 

	Xin Li, 

	Vipul Goyal, 

	Jesse Goodman</title>
    <summary>In a recent work, Kumar, Meka, and Sahai (FOCS 2019) introduced the notion of bounded collusion protocols (BCPs), in which $N$ parties wish to compute some joint function $f:(\{0,1\}^n)^N\to\{0,1\}$ using a public blackboard, but such that only $p$ parties may collude at a time. This generalizes well studied models in multiparty communication complexity, such as the number-in-hand (NIH) and number-on-forehead (NOF) models, which are just endpoints on this rich spectrum. We construct explicit hard functions against this spectrum, and achieve a tradeoff between collusion and complexity. Using this, we obtain improved leakage-resilient secret sharing schemes against bounded collusion protocols. 

Our main tool in obtaining hard functions against BCPs are explicit constructions of leakage resilient extractors against BCPs for a wide range of parameters. Kumar et al. (FOCS 2019) studied  such extractors and called them cylinder intersection extractors. In fact, such extractors directly yield correlation bounds against BCPs. We focus on the following setting: the input to the extractor consists of $N$ independent sources of length $n$, and the leakage function Leak $:(\{0,1\}^n)^N\to\{0,1\}^\mu\in\mathcal{F}$ is a BCP with some collusion bound $p$ and leakage (output length) $\mu$. While our extractor constructions are very general, we highlight some interesting parameter settings:

1. In the case when the input sources are uniform, and $p=0.99N$ parties collude, our extractor can handle $n^{\Omega(1)}$ bits of leakage, regardless of the dependence between $N,n$. The best NOF lower bound (i.e., $p=N-1$) on the other hand requires $N&lt;\log n$ even to handle $1$ bit of leakage.

2. Next, we show that for the same setting as above, we can drop the entropy requirement to $k=$ polylog $n$, while still handling polynomial leakage for $p=0.99N$.  This resolves an open question about cylinder intersection extractors raised by Kumar et al. (FOCS 2019), and we find an application of such low entropy extractors in a new type of secret sharing.

We also provide an explicit compiler that transforms any function with high NOF (distributional) communication complexity into a leakage-resilient extractor that can handle polylogarithmic entropy and substantially more leakage against BCPs. Thus any improvement of NOF lower bounds will immediately yield better leakage-resilient extractors.

Using our extractors against BCPs, we obtain improved $N$-out-of-$N$ leakage-resilient secret sharing schemes. The previous best scheme from Kumar et al. (FOCS 2019) required share size to grow exponentially in the collusion bound, and thus cannot efficiently handle $p=\omega(\log N)$. Our schemes have no dependence of this form, and can thus handle collusion size $p=0.99N$.</summary>
    <updated>2020-04-27T06:12:53Z</updated>
    <published>2020-04-27T06:12:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-01T10:20:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/059</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/059" rel="alternate" type="text/html"/>
    <title>TR20-059 |  Pr-ZSUBEXP is not contained in Pr-RP | 

	Gonen Krak, 

	Noam Parzanchevski, 

	Amnon Ta-Shma</title>
    <summary>We unconditionally prove there exists a promise problem in promise ZSUBEXP that cannot be solved in promise RP. 
The proof technique builds upon Kabanets' easy witness method [Kab01] as implemented by Impagliazzo et. al [IKW02], with a separate diagonalization carried out on each of the two alternatives in the win-win argument. We remark that even though the easy witness method is a key component in many celebrated results in derandomization, we are not aware of any previous unconditional separation like the one we show.

We remark that the result relativizes. We could not prove a similar result for total functions, nor for functions in ZTime(T(n)) for T(n) below a half-exponential function (i.e., T such that T(T(n)) &lt; 2^n).</summary>
    <updated>2020-04-27T05:27:15Z</updated>
    <published>2020-04-27T05:27:15Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-01T10:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/058</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/058" rel="alternate" type="text/html"/>
    <title>TR20-058 |  Interactive Proofs for Verifying Machine Learning | 

	Jonathan Shafer, 

	Amir Yehudayoff, 

	Shafi Goldwasser, 

	Guy Rothblum</title>
    <summary>We consider the following question: using a source of labeled data and interaction with an untrusted prover, what is the complexity of verifying that a given hypothesis is "approximately correct"? We study interactive proof systems for PAC verification, where a verifier that interacts with a prover is required to accept good hypotheses, and reject bad hypotheses. Both the verifier and the prover are efficient and have access to data samples from an unknown distribution. We are interested in cases where the verifier can use significantly less data than is required for (agnostic) PAC learning, or use a substantially cheaper data source (e.g., using only random samples for verification, even though learning requires membership queries). We believe that today, when data and data-driven algorithms are quickly gaining prominence, the question of verifying purported outcomes of data analyses is very well-motivated.
		
We show three main results. First, we prove that for a specific hypothesis class, verification is significantly cheaper than learning in terms of the number of random samples required, even if the verifier engages with the prover only in a single-round (NP-like) protocol. Moreover, for this class we prove that single-round verification is also significantly cheaper than testing closeness to the class. Second, for the broad class of Fourier-sparse boolean functions, we show a multi-round (IP-like) verification protocol, where the prover uses membership queries, and the verifier is able to assess the result while only using random samples. Third, we show that verification is not always more efficient. Namely, we show a class of functions where verification requires as many samples as learning does, up to a logarithmic factor.</summary>
    <updated>2020-04-27T01:42:43Z</updated>
    <published>2020-04-27T01:42:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-01T10:20:32Z</updated>
    </source>
  </entry>
</feed>
