<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2022-07-29T00:21:45Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.13638</id>
    <link href="http://arxiv.org/abs/2207.13638" rel="alternate" type="text/html"/>
    <title>A Simple and Elegant Mathematical Formulation for the Acyclic DAG Partitioning Problem</title>
    <feedworld_mtime>1658966400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>M. Yusuf Özkaya, Ümit V. Çatalyürek <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.13638">PDF</a><br/><b>Abstract: </b>This work addresses the NP-Hard problem of acyclic directed acyclic graph
(DAG) partitioning problem. The acyclic partitioning problem is defined as
partitioning the vertex set of a given directed acyclic graph into disjoint and
collectively exhaustive subsets (parts). Parts are to be assigned such that the
total sum of the vertex weights within each part satisfies a common upper bound
and the total sum of the edge costs that connect nodes across different parts
is minimized. Additionally, the quotient graph, i.e., the induced graph where
all nodes that are assigned to the same part are contracted to a single node
and edges of those are replaced with cumulative edges towards other nodes, is
also a directed acyclic graph. That is, the quotient graph itself is also a
graph that contains no cycles. Many computational and real-life applications
such as in computational task scheduling, RTL simulations, scheduling of
rail-rail transshipment tasks and Very Large Scale Integration (VLSI) design
make use of acyclic DAG partitioning. We address the need for a simple and
elegant mathematical formulation for the acyclic DAG partitioning problem that
enables easier understanding, communication, implementation, and
experimentation on the problem.
</p></div>
    </summary>
    <updated>2022-07-28T22:37:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-07-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.13396</id>
    <link href="http://arxiv.org/abs/2207.13396" rel="alternate" type="text/html"/>
    <title>A Geometric Approach to Passive Localisation</title>
    <feedworld_mtime>1658966400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Triommatis:Theofilos.html">Theofilos Triommatis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potapov:Igor.html">Igor Potapov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rees:Gareth.html">Gareth Rees</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ralph:Jason_F=.html">Jason F. Ralph</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.13396">PDF</a><br/><b>Abstract: </b>In this paper, we present a geometric framework for the passive localisation
of static emitters. The objective is to localise the position of the emitters
in a given area by centralised coordination of mobile passive sensors. This
framework uses only the geometry of the problem to minimise the maximal bounds
of the emitters' locations without using a belief or probability distribution.
This geometric approach provides effective boundaries on the emitters'
position. It can also be useful in evaluating different decision-making
strategies for coordinating mobile passive sensors and complementing
statistical methods during the initialisation process. The effectiveness of the
geometric approach is shown by designing and evaluating a greedy
decision-making strategy, where a sensor selects its future position by
minimising the maximum uncertainty on its next measurement using one of the
global objective functions. Finally, we analyse and discuss the emergent
behaviour and robustness of the proposed algorithms.
</p></div>
    </summary>
    <updated>2022-07-28T22:42:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-07-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.13312</id>
    <link href="http://arxiv.org/abs/2207.13312" rel="alternate" type="text/html"/>
    <title>Searching for Regularity in Bounded Functions</title>
    <feedworld_mtime>1658966400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iyer:Siddharth.html">Siddharth Iyer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Whitmeyer:Michael.html">Michael Whitmeyer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.13312">PDF</a><br/><b>Abstract: </b>Given a function $f:\mathbb{F}_2^n \to [-1,1]$, this work seeks to find a
large affine subspace $\mathcal{U}$ such that $f$, when restricted to
$\mathcal{U}$, has small nontrivial Fourier coefficients.
</p>
<p>We show that for any function $f:\mathbb{F}_2^n \to [-1,1]$ with Fourier
degree $d$, there exists an affine subspace of dimension at least $
\tilde\Omega(n^{1/d!}k^{-2})$, wherein all of $f$'s nontrivial Fourier
coefficients become smaller than $ 2^{-k}$. To complement this result, we show
the existence of degree $d$ functions with coefficients larger than $2^{-d\log
n}$ when restricted to any affine subspace of dimension larger than
$\Omega(dn^{1/(d-1)})$. In addition, we give explicit examples of functions
with analogous but weaker properties.
</p>
<p>Along the way, we provide multiple characterizations of the Fourier
coefficients of functions restricted to subspaces of $\mathbb{F}_2^n$ that may
be useful in other contexts. Finally, we highlight applications and connections
of our results to parity kill number and affine dispersers/extractors.
</p></div>
    </summary>
    <updated>2022-07-28T22:37:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-07-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.13281</id>
    <link href="http://arxiv.org/abs/2207.13281" rel="alternate" type="text/html"/>
    <title>Cubic Goldreich-Levin</title>
    <feedworld_mtime>1658966400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Dain.html">Dain Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Anqi.html">Anqi Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tidor:Jonathan.html">Jonathan Tidor</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.13281">PDF</a><br/><b>Abstract: </b>In this paper, we give a cubic Goldreich-Levin algorithm which makes
polynomially-many queries to a function $f \colon \mathbb F_p^n \to \mathbb C$
and produces a decomposition of $f$ as a sum of cubic phases and a small error
term. This is a natural higher-order generalization of the classical
Goldreich-Levin algorithm. The classical (linear) Goldreich-Levin algorithm has
wide-ranging applications in learning theory, coding theory and the
construction of pseudorandom generators in cryptography, as well as being
closely related to Fourier analysis. Higher-order Goldreich-Levin algorithms on
the other hand involve central problems in higher-order Fourier analysis,
namely the inverse theory of the Gowers $U^k$ norms, which are well-studied in
additive combinatorics. The only known result in this direction prior to this
work is the quadratic Goldreich-Levin theorem, proved by Tulsiani and Wolf in
2011. The main step of their result involves an algorithmic version of the
$U^3$ inverse theorem. More complications appear in the inverse theory of the
$U^4$ and higher norms. Our cubic Goldreich-Levin algorithm is based on
algorithmizing recent work by Gowers and Mili\'cevi\'c who proved new
quantitative bounds for the $U^4$ inverse theorem.
</p>
<p>Our cubic Goldreich-Levin algorithm is constructed from two main tools: an
algorithmic $U^4$ inverse theorem and an arithmetic decomposition result in the
style of the Frieze-Kannan graph regularity lemma. As one application of our
main theorem we solve the problem of self-correction for cubic Reed-Muller
codes beyond the list decoding radius. Additionally we give a purely
combinatorial result: an improvement of the quantitative bounds on the $U^4$
inverse theorem.
</p></div>
    </summary>
    <updated>2022-07-28T22:37:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-07-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.13237</id>
    <link href="http://arxiv.org/abs/2207.13237" rel="alternate" type="text/html"/>
    <title>Rule 30: Solving the Chaos</title>
    <feedworld_mtime>1658966400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Das:Mayukhmali.html">Mayukhmali Das</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.13237">PDF</a><br/><b>Abstract: </b>This paper provides an analytical solution to the Wolfram Alpha Rule 30
Problem 1. In this paper we discuss whether the central column of the Rule 30
structure is purely random and aperiodic.
</p></div>
    </summary>
    <updated>2022-07-28T22:42:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-07-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.13121</id>
    <link href="http://arxiv.org/abs/2207.13121" rel="alternate" type="text/html"/>
    <title>Approximation Algorithms for Scheduling under Non-Uniform Machine-Dependent Delays</title>
    <feedworld_mtime>1658966400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rajaraman:Rajmohan.html">Rajmohan Rajaraman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stalfa:David.html">David Stalfa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Sheng.html">Sheng Yang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.13121">PDF</a><br/><b>Abstract: </b>We consider the problem of scheduling precedence-constrained jobs on
heterogenous machines in the presence of non-uniform machine-dependent
communication delays. We are given as input a set of $n$ unit size
precedence-constrained jobs, and a set of $m$ related machines each with size
$m_i$ (machine $i$ can execute at most $m_i$ jobs at any time). Each machine
$i$ also has an associated in-delay $\rho^{\mathrm{in}}_i$ and out-delay
$\rho^{\mathrm{out}}_i$. For any job $v$, machine $i$, and time $t$, we say
that $v$ is available to machine $i$ at time $t$ if $v$ is completed on $i$
before time $t$ or on any machine $j$ before time $t - (\rho^{\mathrm{in}}_i +
\rho^{\mathrm{out}}_j)$. If job $v$ is scheduled at time $t$ on machine $i$,
then all of its predecessors must be available to $i$ by time $t$. The
objective is to construct a schedule that minimizes makespan, which is the
maximum completion time over all jobs.
</p>
<p>We consider schedules which allow duplication of jobs as well as schedules
which do not. When duplication is allowed, we provide an asymptotic
$\mathrm{polylog}(n)$-approximation algorithm; it is a true approximation if
the makespan also accounts for the time to communicate the jobs to the machines
and for the time to communicate the results out. For no-duplication schedules,
we also obtain an asymptotic $\mathrm{polylog}(n)$-approximation via a
reduction to the case with duplication, and a true
$\mathrm{polylog}(n)$-approximation for symmetric delays ($\rho^{\text{in}}_{i}
= \rho^{\mathrm{out}}_i$ for all machines $i$). These results represent the
first polylogarithmic approximation algorithms for scheduling with non-uniform
communication delays.
</p></div>
    </summary>
    <updated>2022-07-28T22:38:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-07-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6599</id>
    <link href="https://scottaaronson.blog/?p=6599" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6599#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6599" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">On black holes, holography, the Quantum Extended Church-Turing Thesis, fully homomorphic encryption, and brain uploading</title>
    <summary xml:lang="en-US">I promise you: this post is going to tell a scientifically coherent story that involves all five topics listed in the title. Not one can be omitted. My story starts with a Zoom talk that the one and only Lenny Susskind delivered for the Simons Institute for Theory of Computing back in May. There followed […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><figure class="wp-block-image size-large"><img alt="" src="https://www.scottaaronson.com/alice.jpg"/></figure>



<p>I promise you: this post is going to tell a scientifically coherent story that involves <em>all five topics </em>listed in the title.  Not one can be omitted.</p>



<p>My story starts with a <a href="https://www.youtube.com/watch?v=1CpzigpEJnU">Zoom talk</a> that the one and only <a href="https://en.wikipedia.org/wiki/Leonard_Susskind">Lenny Susskind</a> delivered for the Simons Institute for Theory of Computing back in May.  There followed a panel discussion involving Lenny, Edward Witten, Geoffrey Penington, Umesh Vazirani, and your humble shtetlmaster.</p>



<p>Lenny’s talk led up to a gedankenexperiment involving an observer, Alice, who bravely jumps into a specially-prepared black hole, in order to see the answer to a certain computational problem in her final seconds before being ripped to shreds near the singularity.  Drawing on earlier work by <a href="https://arxiv.org/abs/1910.14646">Bouland, Fefferman, and Vazirani</a>, Lenny speculated that the computational problem could be exponentially hard even for a (standard) quantum computer.  Despite this, Lenny repeatedly insisted—indeed, he asked me again to stress here—that he was <em>not</em> claiming to violate the <a href="https://quantumcomputing.stackexchange.com/questions/6088/what-precisely-is-the-quantum-extended-church-turing-thesis">Quantum Extended Church-Turing Thesis</a> (QECTT), the statement that all of nature can be efficiently simulated by a standard quantum computer.  Instead, he was simply investigating how the QECTT needs to be <em>formulated</em> in order to be a true statement.</p>



<p>I didn’t understand this, to put it mildly.  If what Lenny was saying was right—i.e., if the infalling observer could see the answer to a computational problem not in <a href="https://en.wikipedia.org/wiki/BQP">BQP</a>, or Bounded-Error Quantum Polynomial-Time—then why <em>shouldn’t</em> we call that a violation of the QECTT?  Just like we call Shor’s quantum factoring algorithm a likely violation of the <em>classical</em> Extended Church-Turing Thesis, the thesis saying that nature can be efficiently simulated by a classical computer?  Granted, you don’t have to <em>die</em> in order to run Shor’s algorithm, as you do to run Lenny’s experiment.  But why should such implementation details matter from the lofty heights of computational complexity?</p>



<p>Alas, not only did Lenny never answer that in a way that made sense to me, he kept trying to shift the focus from real, physical black holes to “silicon spheres” made of qubits, which would be programmed to <em>simulate</em> the process of Alice jumping into the black hole (in a dual boundary description).  Say what?  Granting that Lenny’s silicon spheres, being quantum computers under another name, could clearly be simulated in BQP, wouldn’t this still leave the question about the computational powers of observers who jump into <em>actual</em> black holes—i.e., the question that we presumably cared about in the first place?</p>



<p>Confusing me even further, Witten seemed almost dismissive of the idea that Lenny’s gedankenexperiment raised any new issue for the QECTT—that is, any issue that wouldn’t already have been present in a universe without gravity.  But as to Witten’s reasons, the most I understood from his remarks was that he was worried about various “engineering” issues with implementing Lenny’s gedankenexperiment, involving gravitational backreaction and the like.  Ed Witten, now suddenly the practical guy!  I couldn’t even isolate the crux of disagreement between Susskind and Witten, since after all, they <em>agreed</em> (bizarrely, from my perspective) that the QECTT wasn’t violated.  Why wasn’t it?</p>



<p>Anyway, shortly afterward I <a href="https://scottaaronson.blog/?p=6457">attended the 28th Solvay Conference in Brussels</a>, where one of the central benefits I got—besides seeing friends after a long COVID absence and eating some amazing chocolate mousse—was a <em>dramatically</em> clearer understanding of the issues in Lenny’s gedankenexperiment.  I owe this improved understanding to conversations with many people at Solvay, but above all Daniel Gottesman and Daniel Harlow.  Lenny himself wasn’t there, other than in spirit, but I ran the Daniels’ picture by him afterwards and he assented to all of its essentials.</p>



<p>The Daniels’ picture is what I want to explain in this post.  Needless to say, I take sole responsibility for any errors in my presentation, as I <em>also</em> take sole responsibility for not understanding (or rather: not doing the work to translate into terms that I understood) what Susskind and Witten had said to me before.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>The first thing you need to understand about Lenny’s gedankenexperiment is that it takes place entirely in the context of <a href="https://en.wikipedia.org/wiki/AdS/CFT_correspondence">AdS/CFT</a>: the famous holographic duality between two types of physical theories that look wildly different.  Here AdS stands for <em>anti-de-Sitter</em>: a quantum theory of gravity describing a D-dimensional universe with a negative cosmological constant (i.e. hyperbolic geometry), one where black holes can form and evaporate and so forth.  Meanwhile, CFT stands for <em>conformal field theory</em>: a quantum field theory, with no apparent gravity (and hence no black holes), that lives on the (D-1)-dimensional boundary of the D-dimensional AdS space.  The staggering claim of AdS/CFT is that every physical question about the AdS bulk can be translated into an equivalent question about the CFT boundary, and vice versa, with a one-to-one mapping from states to states and observables to observables.  So in that sense, they’re actually the <em>same</em> theory, just viewed in two radically different ways.  AdS/CFT originally came out of string theory, but then <a href="https://www.math.columbia.edu/~woit/wordpress/?p=12981">notoriously</a> “swallowed its parent,” to the point where nowadays, if you go to what are still called “string theory” meetings, you’re liable to hear <em>vastly</em> more discussion of AdS/CFT than of actual strings.</p>



<p>Thankfully, the story I want to tell won’t depend on fine details of how AdS/CFT works.  Nevertheless, <em>you can’t just ignore the AdS/CFT part</em> as some technicality, in order to get on with the vivid tale of Alice jumping into a black hole, hoping to learn the answer to a beyond-BQP computational problem in her final seconds of existence.  The reason you can’t ignore it is that the whole beyond-BQP computational problem we’ll be talking about, involves the <em>translation</em> (or “dictionary”) between the AdS bulk and the CFT boundary.  If you like, then, it’s actually the chasm between bulk and boundary that plays the starring role in this story.  The more familiar chasm <em>within</em> the bulk, between the interior of a black hole and its exterior (the two separated by an <a href="https://en.wikipedia.org/wiki/Event_horizon">event horizon</a>), plays only a subsidiary role: that of causing the AdS/CFT dictionary to become exponentially complex, as far as anyone can tell.</p>



<p>Pause for a minute.  Previously I led you to believe that we’d be talking about an actual observer Alice, jumping into an actual physical black hole, and whether Alice could see the answer to a problem that’s intractable even for quantum computers in her last moments before hitting the singularity, and if so whether we should take that to refute the Quantum Extended Church-Turing Thesis.  What I’m saying now is so wildly at variance with that picture, that it had to be repeated to me about 10 times before I understood it.  Once I did understand, I then had to repeat it to <em>others</em> about 10 times before <em>they</em> understood.  And I don’t care if people ridicule me for that admission—how slow Scott and his friends must be, compared to string theorists!—because my only goal right now is to get <em>you</em> to understand it.</p>



<p>To say it again: Lenny has <em>not</em> proposed a way for Alice to surpass the complexity-theoretic power of quantum computers, even for a brief moment, by crossing the event horizon of a black hole.  If <em>that</em> was Alice’s goal when she jumped into the black hole, then alas, she probably sacrificed her life for nothing!  As far as anyone knows, Alice’s experiences, even after crossing the event horizon, ought to continue to be described extremely well by general relativity and quantum field theory (at least until she nears the singularity and dies), and therefore ought to be simulatable in BQP.  Granted, we don’t actually <em>know</em> this—you can call it an open problem if you like—but it seems like a reasonable guess.</p>



<p>In that case, though, what beyond-BQP problem was Lenny talking about, and what does it have to do with black holes?  Building on the <a href="https://arxiv.org/abs/1910.14646">Bouland-Fefferman-Vazirani paper</a>, Lenny was interested in a class of problems of the following form: Alice is given as input a pure quantum state |ψ⟩, which encodes a boundary CFT state, which is dual to an AdS bulk universe that contains a black hole.  Alice’s goal is, by examining |ψ⟩, to learn something about what’s <em>inside</em> the black hole.  For example: does the black hole interior contain “shockwaves,” and if so how many and what kind?  Does it contain a wormhole, connecting it to a different black hole in another universe?  If so, what’s the volume of that wormhole?  (Not the first question <em>I</em> would ask either, but bear with me.)</p>



<p>Now, when I say Alice is “given” the state |ψ⟩, this could mean several things: she could just be physically given a collection of <em>n</em> qubits.  Or, she could be given a gigantic table of 2<sup><em>n</em></sup> amplitudes.  Or, as a third possibility, she could be given a description of a quantum circuit that prepares |ψ⟩, say from the all-0 initial state |0<sup><em>n</em></sup>⟩.  Each of these possibilities leads to a different complexity-theoretic picture, and the differences are extremely interesting to <em>me</em>, so that’s what I mostly focused on in my remarks in the panel discussion after Lenny’s talk.  But it won’t matter much for the story I want to tell in this post.</p>



<p>However |ψ⟩ is given to Alice, the prediction of AdS/CFT is that |ψ⟩ encodes everything there is to know about the AdS bulk, <em>including</em> whatever is inside the black hole—but, and this is crucial, the information about what’s inside the black hole will be <em>pseudorandomly scrambled</em>.  In other words, it works like this: whatever simple thing you’d like to know about parts of the bulk that <em>aren’t</em> hidden behind event horizons—is there a star over here? some gravitational lensing over there? etc.—it seems that you could not only learn it by measuring |ψ⟩, but learn it <em>in polynomial time</em>, the dictionary between bulk and boundary being computationally efficient in that case.  (As with almost everything else in this subject, even <em>that</em> hasn’t been rigorously proven, though my postdoc Jason Pollack and I made <a href="https://www.scottaaronson.com/talks/discretebulk.pdf">some progress</a> this past spring by proving a piece of it.)  On the other hand, as soon as you want to know what’s <em>inside</em> an event horizon, the fact that there are no probes that an “observer at infinity” could apply to find out, seems to translate into the requisite measurements on |ψ⟩ being exponentially complex to apply.  (Technically, you’d have to measure an ensemble of poly(<em>n</em>) identical copies of |ψ⟩, but I’ll ignore that in what follows.)</p>



<p>In more detail, the relevant part of |ψ⟩ turns into a pseudorandom, scrambled mess: a mess that it’s plausible that no polynomial-size quantum circuit could even distinguish from the maximally mixed state.  So, while in principle the information is all there in |ψ⟩, getting it out seems as hard as various well-known problems in symmetric-key cryptography, if not literally NP-hard.  This is <em>way</em> beyond what we expect even a quantum computer to be able to do efficiently: indeed, after 30 years of quantum algorithms research, the best quantum speedup we know for this sort of task is typically just the quadratic speedup from Grover’s algorithm.</p>



<p>So now you understand why there was some hope that Alice, by jumping into a black hole, could solve a problem that’s exponentially hard for quantum computers!  Namely because, once she’s inside the black hole, she can just<em> see</em> the shockwaves, or the volume of the wormhole, or whatever, and no longer faces the exponentially hard task of decoding that information from |ψ⟩.  It’s as if the black hole has <em>solved the problem for her</em>, by physically instantiating the otherwise exponentially complex transformation between the bulk and boundary descriptions of |ψ⟩.</p>



<p>Having now gotten your hopes up, the next step in the story is to destroy them.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Here’s the fundamental problem: |ψ⟩ does <em>not</em> represent the CFT dual of a bulk universe that contains the black hole with the shockwaves or whatever, <em>and that also contains Alice herself, floating outside the black hole, and being given |ψ⟩ as an input.</em>  Indeed, it’s unclear what the latter state would even mean: how do we get around the circularity in its definition?  How do we avoid an infinite regress, where |ψ⟩ would have to encode a copy of |ψ⟩ which would have to encode a copy of … and so on forever?  Furthermore, who created this |ψ⟩ to give to Alice?  We don’t normally imagine that an “input state” contains a complete description of the body and brain of the person whose job it is to learn the output.</p>



<p>By contrast, a scenario that we <em>can</em> define without circularity is this: Alice is given (via physical qubits, a giant table of amplitudes, an obfuscated quantum circuit, or whatever) a pure quantum state |ψ⟩, which represents the CFT dual of a hypothetical universe containing a black hole.  Alice wants to learn what shockwaves or wormholes are inside the black hole, a problem plausibly conjectured not to have any ordinary polynomial-size quantum circuit that takes copies of |ψ⟩ as input.  To “solve” the problem, Alice sets into motion the following sequence of events:</p>



<ol><li>Alice scans and uploads her own brain into a quantum computer, presumably destroying the original meat brain in the process!  The QC represents Alice, who now exists only virtually, via a state |φ⟩.</li><li>The QC performs entangling operations on |φ⟩ and |ψ⟩, which correspond to inserting Alice into the bulk of the universe described by |ψ⟩, and then having her fall into the black hole.</li><li>Now in simulated form, “Alice” (or so we assume, depending on our philosophical position) has the subjective experience of falling into the black hole and observing what’s inside.  <em>Success!</em>  Given |ψ⟩ as input, we’ve now caused “Alice” (for some definition of “Alice”) to have observed the answer to the beyond-BQP computational problem.</li></ol>



<p>In the panel discussion, I now model Susskind as having proposed scenario 1-3, Witten as going along with 1-2 but rejecting 3 or not wanting to discuss it, and me as having made valid points about the computational complexity of simulating Alice’s experience in 1-3, yet while being radically mistaken about what the scenario <em>was</em> (I still thought an actual black hole was involved).</p>



<p>An obvious question is whether, having learned the answer, “Alice” can now get the answer back out to the “real, original” world.  Alas, the expectation is that this would require exponential time.  Why?  Because otherwise, this whole process would’ve constituted a subexponential-time algorithm for distinguishing random from pseudorandom states using an “ordinary” quantum computer!  Which is conjectured not to exist.</p>



<p>And what about Alice herself?  In polynomial time, could she return from “the Matrix,” back to a real-world biological body?  Sure she could, in principle—if, for example, the entire quantum computation were run in reverse.  But notice that reversing the computation would <em>also</em> make Alice forget the answer to the problem!  Which is not at all a coincidence: if the problem is outside BQP, then in general, Alice can know the answer <em>only</em> while she’s “inside the Matrix.”</p>



<p>Now that hopefully everything is crystal-clear and we’re all on the same page, what can we say about this scenario?  In particular: <em>should</em> it cause us to reject or modify the QECTT itself?</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Daniel Gottesman, I thought, offered a brilliant reductio ad absurdum of the view that the simulated black hole scenario should count as a refutation of the QECTT.  Well, he didn’t <em>call</em> it a “reductio,” but I will.</p>



<p>For the reductio, let’s forget not only about quantum gravity but even about quantum mechanics itself, and go all the way back to classical computer science.  A <a href="https://en.wikipedia.org/wiki/Homomorphic_encryption">fully homomorphic encryption scheme</a>, the <a href="https://crypto.stanford.edu/craig/craig-thesis.pdf">first example</a> of which was discovered by Craig Gentry 15 years ago, lets you do arbitrary computations on encrypted data without ever needing to decrypt it.  It has both an encryption key, for encrypting the original plaintext data, and a separate decryption key, for decrypting the final answer.</p>



<p>Now suppose Alice has some homomorphically encrypted top-secret emails, which she’d like to read.  She has the encryption key (which is public), but not the decryption key.</p>



<p><em>If</em> the homomorphic encryption scheme is secure against quantum computers—as the schemes discovered by Gentry and later researchers currently appear to be—and <em>if</em> the QECTT is true, then Alice’s goal is obviously infeasible: decrypting the data will take her exponential time.</p>



<p>Now, however, a classical version of Lenny comes along, and explains to Alice that she simply needs to do the following:</p>



<ol><li>Upload her own brain state into a classical computer, destroying the “meat” version in the process (who needed it?).</li><li>Using the known encryption key, homomorphically encrypt a computer program that simulates (and thereby, we presume, <em>enacts</em>) Alice’s consciousness.</li><li>Using the homomorphically encrypted Alice-brain, <em>together with</em> the homomorphically encrypted input data, do the homomorphic computations that simulate the process of Alice’s brain reading the top-secret emails.</li></ol>



<p>The claim would now be that, <em>inside the homomorphic encryption</em>, the simulated Alice has the subjective experience of reading the emails in the clear.  Aha, therefore she “broke” the homomorphic encryption scheme!  Therefore, assuming that the scheme was secure even against quantum computers, the QECTT must be false!</p>



<p>According to Gottesman, this is almost perfectly analogous to Lenny’s black hole scenario.  In particular, they share the property that “encryption is easy but decryption is hard.”   Once she’s uploaded her brain, Alice can efficiently <em>enter</em> the homomorphically encrypted world to see the solution to a hard problem, just like she can efficiently <em>enter</em> the black hole world to do the same.  In both cases, however, getting <em>back</em> to her normal world with the answer would then take Alice exponential time.  Note that in the latter case, the difficulty is <em>not</em> so much about “escaping from a black hole,” as it is about inverting the AdS/CFT dictionary.</p>



<p>Going further, we can regard the AdS/CFT dictionary for regions behind event horizons as, itself, an <em>example</em> of a fully homomorphic encryption scheme—in this case, of course, one where the ciphertexts are quantum states.  This strikes me as potentially an important insight about AdS/CFT itself, even if that wasn’t Gottesman’s intention.  It complements many other recent connections between AdS/CFT and theoretical computer science, including the <a href="https://arxiv.org/abs/1411.7041">view of AdS/CFT as a quantum error-correcting code</a>, and the <a href="https://arxiv.org/pdf/1604.00354">connection between AdS/CFT and the Max-Flow/Min-Cut Theorem</a> (see also my <a href="https://www.scottaaronson.com/talks/discretebulk.pdf">talk</a> about my work with Jason Pollack).</p>



<p>So where’s the reductio?  Well, when it’s put so starkly, I suspect that not many would regard Gottesman’s classical homomorphic encryption scenario as a “real” challenge to the QECTT.  Or rather, people might say: yes, this raises fascinating questions for the philosophy of mind, but at any rate, we’re no longer talking about <em>physics</em>.  Unlike with (say) quantum computing, no new <em>physical</em> phenomenon is being brought to light that lets an otherwise intractable computational problem be solved.  Instead, it’s all about the <em>user herself</em>, about Alice, and which physical systems get to count as instantiating her.</p>



<p>It’s like, imagine Alice at the computer store, weighing which laptop to buy.  Besides weight, battery life, and price, she definitely does care about processing power.  She might even consider a quantum computer, if one is available.  Maybe even a computer with a black hole, wormhole, or closed timelike curve inside: as long as it gives the answers she wants, what does she care about the innards?  But a computer whose normal functioning would (pessimistically) kill her or (optimistically) radically change her own nature, trapping her in a simulated universe that she can escape only by forgetting the computer’s output?  Yeah, I don’t envy the computer salesman.</p>



<p>Anyway, if we’re going to say this about the homomorphic encryption scenario, then shouldn’t we say the same about the simulated black hole scenario?  Again, from an “external” perspective, all that’s happening is a giant BQP computation.  Anything beyond BQP that we consider to be happening, depends on adopting the standpoint of an observer who “jumps into the homomorphic encryption on the CFT boundary”—at which point, it would seem, we’re no longer talking about physics but about philosophy of mind.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>So, that was the story!  I promised you that it would integrally involve black holes, holography, the Quantum Extended Church-Turing Thesis, fully homomorphic encryption, <em>and</em> brain uploading, and I hope to have delivered on my promise.</p>



<p>Of course, while this blog post has forever cleared up all philosophical confusions about AdS/CFT and the Quantum Extended Church-Turing Thesis, many questions of a more technical nature remain.  For example: what about the original scenario?  <em>can</em> we argue that the experiences of bulk observers can be simulated in BQP, even when those observers jump into black holes?  Also, what can we say about the complexity class of problems to which the simulated Alice can learn the answers?  Could she even solve NP-complete problems in polynomial time this way, or at least invert one-way functions?  More broadly, what’s the power of “BQP with an oracle for applying the AdS/CFT dictionary”—once or multiple times, in one direction or both directions?</p>



<p>Lenny himself described his gedankenexperiment as exploring the power of a new complexity class that he called “JI/poly,” where the JI stands for “Jumping In” (to a black hole, that is).  The nomenclature is transparently ridiculous—“/poly” means “with polynomial-size advice,” which we’re <em>not</em> talking about here—and I’ve argued in this post that the “JI” is rather misleading as well.  If Alice is “jumping” anywhere, it’s not into a black hole <em>per se</em>, but into a quantum computer that simulates a CFT that’s dual to a bulk universe containing a black hole.</p>



<p>In a broader sense, though, to contemplate these questions at all is clearly to “jump in” to … <em>something</em>.  It’s old hat by now that one can start in physics and end up in philosophy: what else is the quantum measurement problem, or the Boltzmann brain problem, or anthropic cosmological puzzles like whether (all else equal) we’re a hundred times as likely to find ourselves in a universe with a hundred times as many observers?  More recently, it’s also become commonplace that one can start in physics and end in computational complexity theory: quantum computing itself is the example par excellence, but over the past decade, the <a href="https://arxiv.org/abs/1301.4504">Harlow-Hayden argument</a> about decoding Hawking radiation and the <a href="https://arxiv.org/abs/1509.07876">complexity = action proposal</a> have made clear that it can happen even in quantum gravity.</p>



<p>Lenny’s new gedankenexperiment, however, is the first case I’ve seen where you start out in physics, and end up embroiled in some of the hardest questions of philosophy of mind and computational complexity theory <em>simultaneously</em>.</p></div>
    </content>
    <updated>2022-07-27T22:54:34Z</updated>
    <published>2022-07-27T22:54:34Z</published>
    <category scheme="https://scottaaronson.blog" term="Complexity"/>
    <category scheme="https://scottaaronson.blog" term="Metaphysical Spouting"/>
    <category scheme="https://scottaaronson.blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-07-28T17:22:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/07/27/midsphere-facts-fallacies</id>
    <link href="https://11011110.github.io/blog/2022/07/27/midsphere-facts-fallacies.html" rel="alternate" type="text/html"/>
    <title>Midsphere facts and fallacies</title>
    <summary>Some three-dimensional convex polyhedra have a midsphere, a sphere tangent to all of their edges. More strongly, every three-dimensional convex polyhedron has a combinatorially equivalent form, called a “canonical polyhedron”, that has a midsphere. The literature on midspheres is a little sparse and missing some key facts and counterexamples, so I thought I would collect some of them here.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Some three-dimensional convex polyhedra have a <a href="https://en.wikipedia.org/wiki/Midsphere">midsphere</a>, a sphere tangent to all of their edges. More strongly, every three-dimensional convex polyhedron has a combinatorially equivalent form, called a “canonical polyhedron”, that has a midsphere. The literature on midspheres is a little sparse and missing some key facts and counterexamples, so I thought I would collect some of them here.</p>

<p style="text-align: center;"><img alt="A canonical polyhedron and its midsphere" src="https://11011110.github.io/blog/assets/2017/midsphere.png"/></p>

<p>The midsphere, when it exists, has a center equidistant from all the edges, and all of the lines through edges. The set of points equidistant from two <a href="https://en.wikipedia.org/wiki/Skew_lines">skew lines</a> is a curved surface, a <a href="https://en.wikipedia.org/wiki/Paraboloid">hyperbolic paraboloid</a>, but when two lines cross each other the set of points equidistant from both lines is instead the union of two planes, perpendicular to each other and to the plane of the crossing lines, through the crossing point of the two lines. For the midsphere of a polyhedron, and two edges incident on a face of the polyhedron, only one of these two planes is relevant, the one that bisects the interior angle of the face. The center of the midsphere (if it exists) can be found at the point where all of these face-angle-bisecting planes meet.</p>

<p>A convenient class of examples of polyhedra with midspheres is given by “Crelle’s tetrahedra”, the tetrahedra with midspheres. For every such tetrahedron, one can find four spheres, centered at the tetrahedron vertices, tangent in pairs at the points where the midsphere touches the edges. The tetrahedron’s six edge lengths are sums of radii of two of these four spheres. Conversely, for every four pairwise-externally-tangent spheres whose centers are not coplanar, the convex hull of the centers is one of Crelle’s tetrahedra. The radii of the four spheres can be varied continuously, keeping them in contact, although not every quadruple of radii works: the <a href="https://en.wikipedia.org/wiki/Cayley%E2%80%93Menger_determinant">Cayley–Menger determinant</a> of the six sums of pairs of radii must be positive. For instance, if three radii are equal and the fourth is much smaller, the three large spheres will be too far apart for the fourth sphere to touch all three of them and the determinant will be negative. Because Crelle’s tetrahedra can be parameterized by four radii, they form a four-dimensional subspace of the six-dimensional space of all tetrahedra (as parameterized by edge lengths).</p>

<p>Some sources either state that the midsphere touches the polyhedron edges at their midpoints, or define a midsphere to be a sphere through all the edge midpoints. But even when a polyhedron has both a midsphere and a sphere through all of its edge midpoints, these two spheres might not be the same as each other. An example is given by the right pyramids over equilateral triangles, parameterized by the quadruples of radii \((1,1,1,x)\) for \(x\ne 1\). These pyramids have an equilateral-triangle base, which both spheres touch at the edge midpoints. But the three edges of length \(1+x\) meeting at the apex of the pyramid are touched by the midsphere at unit distance from the base, rather than at their midpoints.</p>

<p>Anthony Pugh’s 1976 book <em>Polyhedra: A Visual Approach</em> <a href="https://books.google.com/books?id=IDDxpYQTR7kC&amp;pg=PA4">claims that only the Platonic solids have all three of an insphere, midsphere, and circumsphere</a>, but this is not true. Crelle’s tetrahedra are counterexamples. Even with the midpoint-based alternative definition of midspheres, the pyramids are counterexamples.</p>

<p>For a long time the Wikipedia article on midspheres stated that the midsphere gets its name from being between the insphere and circumsphere. But it is not always between them. For a very tall pyramid (with large \(x\)), the circumsphere passes very close to the plane of the equilateral triangle at the base of the pyramid, and the midsphere pokes out past the circumsphere below the base. I briefly thought that the midsphere might be the same as the smallest sphere that touches or contains all of the edges of a polyhedron. In general, even when a midsphere does not exist, this smallest sphere can be found in linear time, as an instance of <a href="https://arxiv.org/abs/cs.CG/0412046">quasiconvex programming</a>. But for very flat pyramids (\(x\) close to its minimum value), the smallest touching sphere has the incircle of the base as its equator, while the midsphere is larger. The same flat pyramids have an insphere that touches their isosceles-triangle faces at points close to the apex of the pyramid, outside the midsphere, so their insphere pokes out of the midsphere.</p>

<p>For most polyhedra having an insphere and a midsphere, or a midsphere and a circumsphere, these spheres are not concentric. They are concentric for symmetric polyhedra, such as the Platonic solids, but those are not the only cases. For instance, when a polyhedron with concentric midsphere and circumsphere, like a regular icosahedron, is sliced by the plane through a cycle of its edges, the result continues to have the same concentric midsphere and circumsphere. The <a href="https://en.wikipedia.org/wiki/Metabidiminished_icosahedron">metabidiminished icosahedron</a> below is an example. The polar dual of a polyhedron with concentric midsphere and circumsphere instead has concentric insphere and midsphere.</p>

<p style="text-align: center;"><img alt="Metabidiminished icosahedron, CC-BY-SA image by AndrewKepert, November 1, 2004, from https://commons.wikimedia.org/wiki/File:Metabidiminished_icosahedron.png" src="https://11011110.github.io/blog/assets/2022/metabidiminished.png"/></p>

<p>It may possibly be the case that the inradius is always less than the midradius (when both exist) and that the midradius is always less than the circumradius (when both exist). I don’t have a proof, but I also don’t have a counterexample.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/108722887534301591">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2022-07-27T20:28:00Z</updated>
    <published>2022-07-27T20:28:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-07-28T03:41:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/109</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/109" rel="alternate" type="text/html"/>
    <title>TR22-109 |  Searching for Regularity in Bounded Functions | 

	Siddharth Iyer, 

	Michael Whitmeyer</title>
    <summary>Given a function $f:\mathbb F_2^n \to [-1,1]$, this work seeks to find a large affine subspace $\mathcal U$ such that $f$, when restricted to $\mathcal U$, has small nontrivial Fourier coefficients.

We show that for any function $f:\mathbb F_2^n \to [-1,1]$ with Fourier degree $d$, there exists an affine subspace of dimension at least $ \tilde\Omega(n^{1/d!}k^{-2})$, wherein all of $f$'s nontrivial Fourier coefficients become smaller than $ 2^{-k}$. 
To complement this result, we show the existence of degree $d$ functions with coefficients larger than $2^{-d\log n}$ when restricted to any affine subspace of dimension larger than $\Omega(dn^{1/(d-1)})$. In addition, we give explicit examples of functions with analogous but weaker properties.

Along the way, we provide multiple characterizations of the Fourier coefficients of functions restricted to subspaces of $\mathbb F_2^n$ that may be useful in other contexts. Finally, we highlight applications and connections of our results to parity kill number and affine dispersers/extractors.</summary>
    <updated>2022-07-27T12:58:49Z</updated>
    <published>2022-07-27T12:58:49Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-29T00:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=629</id>
    <link href="https://tcsplus.wordpress.com/2022/07/26/tcs-we-want-your-feedback/" rel="alternate" type="text/html"/>
    <title>TCS+: We want YOU(R feedback)</title>
    <summary>For the new season of TCS+, the organizers’ team has been discussing a range of suggestions to improve our general format, and adapt it to the all-Zoom, COVID/post-COVID world. We would love your feedback on some of these ideas — if you could take a few minutes to answer the following survey (4-5 questions), this […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>For the new season of TCS+, the organizers’ team has been discussing a range of suggestions to improve our general format, and adapt it to the all-Zoom, COVID/post-COVID world. We would love your feedback on some of these ideas — if you could take a few minutes to answer the following survey (4-5 questions), this would be greatly appreciated!</p>



<p class="has-text-align-center"><img alt="&#x1F4CB;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4cb.png" style="height: 1em;"/> <a href="https://docs.google.com/forms/d/e/1FAIpQLSeNhfAeGsbCWHfZpoH3b2A3_pb1iD2h_H4YTgsiomhKNldmZA/viewform">Link to the survey</a> </p></div>
    </content>
    <updated>2022-07-27T02:00:47Z</updated>
    <published>2022-07-27T02:00:47Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2022-07-29T00:21:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/07/26/two-2-year-postdocs-at-university-of-oxford-apply-by-september-12-2022/</id>
    <link href="https://cstheory-jobs.org/2022/07/26/two-2-year-postdocs-at-university-of-oxford-apply-by-september-12-2022/" rel="alternate" type="text/html"/>
    <title>Two 2-year postdocs at University of Oxford (apply by September 12, 2022)</title>
    <summary>Two postdoc positions are available at Oxford as part of the UKRI Consolidator Grant “New Approaches to Approximability of Satisfiable Problems” led by Standa Zivny. Strong candidates with any background in maths or theoretical computer science will be considered. Background in approximation algorithms, universal algebra, category theory, combinatorics, or topology would be particularly useful. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Two postdoc positions are available at Oxford as part of the UKRI Consolidator Grant “New Approaches to Approximability of Satisfiable Problems” led by Standa Zivny. Strong candidates with any background in maths or theoretical computer science will be considered. Background in approximation algorithms, universal algebra, category theory, combinatorics, or topology would be particularly useful.</p>
<p>Website: <a href="http://www.cs.ox.ac.uk/news/2075-full.html">http://www.cs.ox.ac.uk/news/2075-full.html</a><br/>
Email: standa.zivny@cs.ox.ac.uk</p></div>
    </content>
    <updated>2022-07-26T13:34:18Z</updated>
    <published>2022-07-26T13:34:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-07-29T00:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/07/26/assistant-professor-3-posts-at-heriot-watt-university-uk-apply-by-august-15-2022/</id>
    <link href="https://cstheory-jobs.org/2022/07/26/assistant-professor-3-posts-at-heriot-watt-university-uk-apply-by-august-15-2022/" rel="alternate" type="text/html"/>
    <title>Assistant Professor (3 posts) at Heriot-Watt University, UK (apply by August 15, 2022)</title>
    <summary>We are looking for three assistant professors (T&amp;R) in Computer Science. The vacancy is not restricted to applicants working in any specific area, but the research of the successful candidate will fall into at least one of three themes the department organises itself around: interactive systems, intelligent systems, and rigorous systems. Website: https://enzj.fa.em3.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX/job/2282/?utm_medium=jobshare Email: j.hage@hw.ac.uk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are looking for three assistant professors (T&amp;R) in Computer Science. The vacancy is not restricted to applicants working in any specific area, but the research of the successful candidate will fall into at least one of three themes the department organises itself around: interactive systems, intelligent systems, and rigorous systems.</p>
<p>Website: <a href="https://enzj.fa.em3.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX/job/2282/?utm_medium=jobshare">https://enzj.fa.em3.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX/job/2282/?utm_medium=jobshare</a><br/>
Email: j.hage@hw.ac.uk</p></div>
    </content>
    <updated>2022-07-26T08:26:18Z</updated>
    <published>2022-07-26T08:26:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-07-29T00:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=6741</id>
    <link href="https://francisbach.com/rethinking-sgd-noise/" rel="alternate" type="text/html"/>
    <title>Rethinking SGD’s noise</title>
    <summary>It seemed a bit unfair to devote a blog to machine learning (ML) without talking about its current core algorithm: stochastic gradient descent (SGD). Indeed, SGD has become, year after year, the basic foundation of many algorithms used for large-scale ML problems. However, the history of stochastic approximation is much older than that of ML:...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="has-text-align-justify">It seemed a bit unfair to devote a blog to machine learning (ML) without talking about its current core algorithm: stochastic gradient descent (SGD). Indeed, SGD has become, year after year, the basic foundation of many algorithms used for large-scale ML problems.  However, the history of stochastic approximation is much older than that of ML: its first study by Robbins and Monro [<a href="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full" rel="noreferrer noopener" target="_blank">1</a>] dates back to 1951. Their aim was to find the zeros of a function that can only be accessed through noisy measurements; and this set-up was applied (and studied!) later on for many problems [<a href="https://link.springer.com/book/9783540606994" rel="noreferrer noopener" target="_blank">2</a>]. Stochastic gradient descent, in its most general definition, is simply the application of the Robbins and Monro’s procedure to find the zeros of the <em>gradient</em> of a function \(f\).</p>



<p class="has-text-align-justify">In this post, I will try to show that the instance of SGD used to solve modern ML problems carries rich particularities. In particular, we will put emphasis on the difference between the under and overparametrised regimes. To provide prototypical representations of these, we will try to cast the typical dynamics of each regime as a known stochastic process.</p>



<h2 class="has-text-align-left" id="fun-and-useful-facts-and-common-mistakes">General set-up</h2>



<p class="has-text-align-justify justify-text"><strong>Stochastic approximation.</strong> At each time \(t \in \mathbb{N}\) of the procedure, let us suppose that we only have access to an unbiased estimate of the gradient, \(\nabla f_t\), of a function \(f\) we want to minimise. More formally this means that given the information of the first \(t-1\) steps, denoted by \(\mathcal{F}_{t-1}\), the estimate we get at time \(t\) is centred around the true gradient: \(\displaystyle \mathbb{E}\left[ \nabla f_t (\theta_{t-1}) | \mathcal{F}_{t-1} \right] = \nabla f (\theta_{t-1})\). Then, the SGD iterates with step-sizes \((\gamma_t)_{t \in \mathbb{N}}\), and initialised at \(\theta_{t=0} = \theta_0\), write $$\theta_t = \theta_{t-1} – \gamma_t \nabla f_t (\theta_{t-1}).$$ To put the emphasis on the noise induced by the noisy estimates of the true gradient, we prefer sometimes to rephrase the recursion in term of the conditional zero-mean noise sequence \(\varepsilon_t = \nabla f – \nabla f_t\). $$\theta_t = \theta_{t-1} – \gamma_t \nabla f (\theta_{t-1}) + \gamma_t\varepsilon_t(\theta_{t-1}).$$ As we will see later, to analyse this discrete time stochastic dynamics, we crucially need to understand the behaviour of \((\varepsilon_t)_{t \in \mathbb{N}}\).</p>



<p class="has-text-align-justify"><strong>Supervised learning reformulation</strong>. This general recursion can be applied to many settings and fits particularly well to the supervised learning framework. In this case, the function to minimise is the empirical risk $$\mathcal{R}_n(\theta)=\mathbb{E}_{(X,Y)\sim\widehat{\rho}_n}\left[\ell((X,Y), \theta)\right] = \frac{1}{n}\sum_{i=1}^n \ell((x_i,y_i), \theta),$$ where \(\widehat{\rho}_n:= \frac{1}{n}\sum_{i=1}^n \delta_{(x_i,y_i)}\) is the empirical measure associated to the samples \((x_i, y_i)_{1 \leqslant i \leqslant n}\). We can derive an unbiased estimate \(\nabla_\theta \ell((x_{i_t},y_{i_t}), \theta)\) of its gradient where \((i_t)_t\) is the sequence of uniformly sampled indices over \(\{1,…,n\}\). The recursion reads: $$\theta_t = \theta_{t-1} – \gamma_t \nabla_\theta \ell((x_{i_t},y_{i_t}), \theta_{t-1}).$$ However, one of the real power of SGD is that it can be seen as a direct <em>stochastic</em> gradient method to optimise the true risk [<a href="https://proceedings.neurips.cc/paper/2007/hash/0d3180d672e08b4c5312dcdafdf6ef36-Abstract.html" rel="noreferrer noopener" target="_blank">3</a>]. Indeed, recall that the true risk is \(\mathcal{R}(\theta) = \mathbb{E}_\rho\left[\ell((X,Y), \theta)\right]\) and consider an input/output sample pair \((x_i,y_i)\) drawn from \(\rho\). Now, \(\ell((x_i,y_i), \theta)\) is an unbiased estimate of the true risk \(\mathcal{R}(\theta)\) such that \(\nabla_\theta \ell((x_i,y_i), \theta)\) is an unbiased estimate of its  gradient. Hence, if we denote \(\mathcal{F}_t = \sigma((x_i,y_i), \ i\leqslant t)\), then the stochastic gradient descent optimises \(\mathcal{R}\) as long as new points \((x,y)\) are added in the data set  $$\theta_t = \theta_{t-1} – \gamma_t \nabla_\theta \ell((x_t,y_t), \theta_{t-1}).$$ This reveals the real strength of SGD against other types of gradient descent algorithm: beyond its low computational cost, as long as we use <em>unseen data</em>, SGD optimises <em>directly</em> <em>the true risk</em> although it is an <em>a priori</em> unknown function. As a consequence, the SGD algorithm, when using only fresh samples, cannot overfit the dataset and does not need any regularisation.</p>



<h2 id="matrix-functions">The noise of SGD in practical ML setting</h2>



<p class="has-text-align-justify">The story outlined in the previous paragraph has been very popular to explain the success of SGD. However, nowadays, the number of samples is usually way smaller than the number of iterations performed, i.e. \(t \gg n\): several <em>epochs</em> are made over the dataset and the fact that SGD is a stochastic approximation algorithm that minimises directly the true risk doesn’t hold any longer. Hence, from now on, let us set the <strong>data to \(n\) input/output pairs</strong> \((x_i,y_i)_{1 \leqslant i \leqslant n}\). </p>



<p class="has-text-align-justify">First, let us briefly present certain crucial properties of SGD in this setting (see also the nice paper [<a href="https://arxiv.org/pdf/2105.01650.pdf" rel="noreferrer noopener" target="_blank">8</a>] for more details). We take a parameterised family of predictors \(\{x \mapsto h(\theta,x), \ \text{for } \theta \in \Theta\}\), and assume that \(\ell\) is the square-loss. The empirical risk classically writes \(\mathcal{R}_n(\theta)= \frac{1}{2n}\sum_{i=1}^n (h(\theta,x_i)\, – y_i)^2\) and the SGD recursion can be rewritten as $$ \theta_t = \theta_{t-1} – \gamma \nabla \mathcal{R}_n (\theta_{t-1}) + \gamma \varepsilon_t(\theta_{t-1}), $$ where \(\varepsilon_t(\theta)\) is the noise term at time \(t\), which writes $$ \varepsilon_t(\theta):= \frac{1}{n} \sum_{j = 1}^n r_j(\theta) \nabla_\theta h(\theta, x_j)\, – r_{i_t}(\theta) \nabla_\theta h(\theta, x_{i_t}), $$ where for all \(i \in \{1,…,n \}\), we define the \(i\)-th residual \(r_i(\theta) = h(\theta,x_i)\, – y_i\). Note that while the expression written requires that the loss is the square-loss, the conclusions of this section extend to different losses (such as the logistic).</p>



<p class="has-text-align-justify"><strong>Multiplicative noise and additive noise.</strong> Let us call \(\theta^*\) any vector belonging to the set of critical points of the loss \(\mathcal{R}_n\), that is \(\nabla \mathcal{R}_n(\theta^*) = 0\) (note that, in the convex case, it is exactly the set of global minima of the loss). We say that we are in the <strong>“overparametrised” regime</strong> if there exists at least one interpolator which fits the data set, i.e. if there exists \(\theta^*\) such that \(R_n( \theta^*)=0\). Otherwise, we say that we are in the <strong>“underparametrised” regime</strong>. We now decompose the noise as the sum of two different noises, that will contribute differently to the dynamics: $$\varepsilon_t(\theta) = \underbrace{\varepsilon_t(\theta)\, – \varepsilon_t(\theta^*) }_{\text{Multiplicative noise}} +  \underbrace{\varepsilon_t(\theta^*)}_{\text{Additive noise}}. $$ If \(\varepsilon_t\) is \(L\)-Lipshitz continuous, which can be verified for instance if we use the squared loss, the term \(\varepsilon_t(\theta)\, – \varepsilon_t(\theta^*) \) converges to \(0\) when \(\theta\) approaches \(\theta^*\). This motivates the “multiplicative” denomination of this term. On the contrary, remark that the additive noise does not depend on the state of the dynamics, i.e. it is independent of \(\theta_t\). Notably, in the overparametrised regime, taking \(\theta^*\) as an interpolator, we see that \(\varepsilon_t(\theta^*)=0\): there is no additive noise in this case! This is part of the explanation of the very different optimisation dynamics observed in the underparametrised and overparametrised regimes [<a href="https://proceedings.mlr.press/v89/vaswani19a.html" rel="noreferrer noopener" target="_blank">14</a>, <a href="https://proceedings.mlr.press/v80/ma18a.html" rel="noreferrer noopener" target="_blank">15</a>].</p>



<p class="has-text-align-justify">As an illustration of these two distinct regimes, consider a least-squares problem: $$ \mathcal{R}_n(\theta)= \frac{1}{2n}\sum_{i=1}^n \big(\langle \theta,x_i\rangle\, – y_i\big)^2.$$ In the overparametrised setting, which typically requires that \(d \geqslant n\), where \(d\) is the dimension of the inputs, it is possible to interpolate the dataset and the noise vanishes at optimum. On the opposite, in any underparametrised setting, the global minimum of the loss function is strictly positive, and the noise’s variance is always lower-bounded: the presence of additive noise totally changes the nature of the dynamics as shown by the following pictures.</p>



<div class="wp-container-3 wp-block-columns">
<div class="wp-container-1 wp-block-column is-vertically-aligned-top"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img alt="" class="wp-image-7442" height="480" src="https://francisbach.com/wp-content/uploads/2022/07/SGD_first-1.gif" width="720"/></figure></div></div>



<div class="wp-container-2 wp-block-column is-vertically-aligned-center"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img alt="" class="wp-image-7441" height="480" src="https://francisbach.com/wp-content/uploads/2022/07/SGD_noise_scale-1.gif" width="720"/></figure></div></div>
</div>



<div class="wp-container-5 is-vertical wp-block-group">
<div class="wp-container-4 is-vertical wp-block-group">
<p class="has-text-align-justify"><span style="text-decoration: underline;">Left:</span> Iterates of SGD for underparametrised (blue) and overparametrised (orange) regimes. The ellipses represent different level sets of \(\mathcal{R}_n\). Inputs \((x_i)_{i\leqslant n}\) are the same in the two models: centred Gaussian with anisotropic covariance (\(n=100,\ d = 2\)). In the underparametrised setting, the outputs have been generated randomly, and there is no interpolator of the dataset, whereas in the overparametrised problem, we generated the outputs according to \(Y = X \theta^* \), where \(\theta^*\) is the unique global minimum of the first setup and hereby of the second as well.</p>



<p class="has-text-align-justify"><span style="text-decoration: underline;">Right:</span> The \(\ell_2\)-norm of the SGD noise in \(\mathrm{log}\)-scale along iterations in the underparametrised (blue) and overparametrised (orange) regimes. In the underparametrised regime, the noise stays constant after \(\sim 75\) iterations, whereas the intensity of the noise goes to \(0\) linearly over the iterates.</p>
</div>



<p/>



<p/>
</div>



<p class="has-text-align-justify"><strong>Noise geometry.</strong> Another important fact to stress concerning the noise \(\varepsilon_t(\theta)\) is that, at state \(\theta\), it belongs to a specific linear space: \(\text{span}\{\nabla_\theta h(\theta, x_1), …, \nabla_\theta h(\theta, x_n) \}\). To provide an intuition on how restrictive it can be, consider once again our favourite linear model: in this case, \(\nabla_\theta h(\theta, x_i)=x_i\) and \(\varepsilon_t(\theta)\) belongs to \(\text{span}\{x_1, …, x_n \}\), which is a space of dimension at most \(n\). In the overparametrised case, this is a strict subspace of \(\mathbb{R}^d\): noise is degenerate in \(d-n\) directions! On the contrary, in the underparametrised setting, as \(d \leqslant n\) there is no specificity at this level.</p>



<div class="wp-container-12 wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-container-11 wp-block-columns">
<div class="wp-container-10 wp-block-column">
<div class="wp-container-9 wp-block-columns">
<div class="wp-container-8 wp-block-column">
<div class="wp-container-7 wp-block-columns are-vertically-aligned-top">
<div class="wp-container-6 wp-block-column is-vertically-aligned-top"><div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-7090" height="297" src="https://francisbach.com/wp-content/uploads/2022/07/Geometric_bias-1024x691.png" width="442"/>3D picture showing the geometric bias of SGD: in the least-squares setting, the iterates (red lines) stay in the affine space \(\theta_0 + \text{span}(x_i)\). The contours represent different levels set of the loss highlighting the translational invariance along the null space \(\text{Ker}(X)\).</figure></div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>



<h2 id="positivity-of-lowner-matrices">SGD as a Markov chain</h2>



<p class="has-text-align-justify">In this section, we develop a different point of view on the general SGD algorithm, that also provides helpful intuition: we try to understand its dynamics through a Markov chain interpretation.</p>



<p class="has-text-align-justify">The first key to understand the behaviour of SGD is that, with constant step-sizes \(\gamma_t = \gamma\), the iterates define an homogeneous <a href="https://en.wikipedia.org/wiki/Markov_chain" rel="noreferrer noopener" target="_blank">Markov chain</a> [<a href="https://link.springer.com/book/10.1007/978-1-4471-3267-7" rel="noreferrer noopener" target="_blank">4</a>]. Let us recall the SGD recursion to keep it next to us.$$ \theta_t = \theta_{t-1} – \gamma\nabla \mathcal{R}_n (\theta_{t-1}) + \gamma\varepsilon_t(\theta_{t-1}). $$ This point of view induces a natural question: <em>does the distribution of the iterates converge to some limit we can characterise?</em> This will again depend on the regime: under or overparametrised. Roughly, for the former, with a constant step size \(\gamma\), the distribution will converge to a stationary distribution with a strictly positive variance (e.g. a Gaussian centred around \(\theta^*\) at scale \(\gamma^{1/2}\) [<a href="https://epubs.siam.org/doi/10.1137/0324039" rel="noreferrer noopener" target="_blank">5</a>]). Less understood is the fact that, for the latter, it will converge to a Dirac distribution \(\delta_{ \theta^*}\), with \(\theta^*\) being one <strong>specific interpolator</strong>, that will depend on the precise structure of the algorithm  (initialisation, set-size, architecture of the model… more details on this will be discussed in a future blog post)!</p>



<p class="has-text-align-justify"><strong>The underparametrised case.</strong> Recall that in this case, the additive noise is nondegenerate. Hence, the gradient and noise parts of the dynamics cannot cancel simultaneously and under some mild assumption on \(\mathcal{R}_n\), the dynamics equilibrates after a certain time: formally this means that the distribution of the iterates \((\theta_t)_{t \geqslant 0}\) converges to an invariant distribution \(\pi^\gamma\). Hence the question: <em>how far is \(\pi^\gamma\) from \(\delta_{\theta^*}\)</em>? Giving a general and precise answer to this question is not trivial, and more importantly, it depends heavily on the multiplicative part of the noise. For now, let us stick with some common modelling to study the recursion: we make the assumption that the noise does not depend on the current iterate \(\theta\) (multiplicative part is \(0\)). In this case, it has been shown that the iterates of SGD have Gaussian fluctuations around \(\theta^*\) at scale \(\gamma^{1/2}\) [<a href="https://epubs.siam.org/doi/10.1137/0324039" rel="noreferrer noopener" target="_blank">5</a>]. From this point of view, the smaller \(\gamma\), the closer to \(\theta^*\) we get and this justifies a known variance reduction technique: decaying step-sizes. Another technique to reduce the variance is to resort to averaging. Once again, to explain this, ergodic theorems for Markov chains [<a href="https://link.springer.com/book/10.1007/978-1-4471-3267-7" rel="noreferrer noopener" target="_blank">4</a>] give us an important insight: the time-mean \(\bar{\theta}_t = \frac{1}{t+1} \sum_{i = 0}^t \theta_i\) converges to \(\bar{\theta}_\gamma = \mathbb{E}_{\pi^\gamma} [\theta]\) almost surely. Furthermore, some magic happens with the quadratic case, as we can show that \(\bar{\theta}_\gamma = \theta^*\), which implies,  as a direct application of the strong law of large numbers, that \(\bar{\theta}_t\) converges almost surely to \(\theta^*\), and proves that averaging the iterates is relevant [<a href="https://papers.nips.cc/paper/2011/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html" rel="noreferrer noopener" target="_blank">6</a>].</p>



<div class="wp-container-15 wp-block-columns">
<div class="wp-container-13 wp-block-column is-vertically-aligned-top"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img alt="" class="wp-image-7446" height="480" src="https://francisbach.com/wp-content/uploads/2022/07/Large_gamma_comparison-1.gif" width="720"/></figure></div></div>



<div class="wp-container-14 wp-block-column"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img alt="" class="wp-image-7448" height="480" src="https://francisbach.com/wp-content/uploads/2022/07/Medium_gamma_comparison-1.gif" width="720"/></figure></div></div>
</div>



<div class="wp-container-18 wp-block-columns">
<div class="wp-container-16 wp-block-column is-vertically-aligned-top"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img alt="" class="wp-image-7447" height="480" src="https://francisbach.com/wp-content/uploads/2022/07/Small_gamma_comparison-4.gif" width="720"/></figure></div></div>



<div class="wp-container-17 wp-block-column">
<p class="has-text-align-justify">Here is a series of plots corresponding to different step-sizes \(\gamma\) showing the dynamics of plain SGD (blue), averaged SGD (red) and step-sizes decay at \(1/\sqrt{t}\) rate (orange)  in an underparametrised setting (\(n=100\), \(d=2\)). Plain SGD is always faster that the other two methods but the larger the \(\gamma\) the larger the variance at optimum is.</p>
</div>
</div>



<p class="has-text-align-justify">Note also that the same analysis can be done in the convex, yet nonquadratic case, indeed, in <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-3/Bridging-the-gap-between-constant-step-size-stochastic-gradient-descent/10.1214/19-AOS1850.full" rel="noreferrer noopener" target="_blank">[7]</a> the authors showed that the order of magnitude of the distance between \(\bar{\theta}_\gamma\) and \({\theta}^*\) is \(\gamma^2\).</p>



<p class="has-text-align-justify"><strong>The overparametrised case.</strong> We have seen in the previous section that both the scale of noise and the geometry are affected in this case. The effect of the geometry is a little bit difficult to handle at a general level and we will detail its role in specific examples (see below and in the next blog post to come). Concerning the scale, we have seen that the noise vanishes at a global optimum and that the closer to a global optimum, the smaller the intensity of the noise is. Hence, global optima are fixed points of the dynamics, and under technical assumptions (e.g. local attractiveness), it can be shown the dynamics eventually converge to one of them [<a href="https://arxiv.org/pdf/2105.01650.pdf" rel="noreferrer noopener" target="_blank">8</a>, <a href="https://proceedings.neurips.cc/paper/2021/hash/b4a0e0fbaa9f16d8947c49f4e610b549-Abstract.html" rel="noreferrer noopener" target="_blank">9</a>]. In this case, the nature of the dynamics is very different from the previous one: the distribution of the iterates \((\theta_t)_{t \geqslant 0}\) converges to \(\delta_{\theta^*}\) where \(\theta^*\) is a random variable in the set of interpolators! Remarkably, no variance reduction technique are required for convergence towards a global minimum!</p>



<h2 id="characterizing-all-matrix-monotone-functions">The stochastic gradient flow and the least-squares example</h2>



<p class="has-text-align-justify justify-text"><strong>Continuous time model of SGD.</strong> Continuous time counterparts of (discrete) numerical optimisation methods and Markov chains are well-worn subjects in applied mathematics and have found applications in machine learning in particular. Indeed, in recent years, gradient descent has been actively studied through <a href="https://francisbach.com/gradient-flows/" rel="noreferrer noopener" target="_blank">gradient flows</a>, which have led to convergence results for neural networks discussed in previous <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">blog</a> <a href="https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/" rel="noreferrer noopener" target="_blank">posts</a>. However, due to its stochasticity, SGD cannot be properly modelled as a deterministic flow. An interesting and natural approach is to consider stochastic differential equations (SDEs) $$ d \theta_t = b(t,\theta_t) d t + \sigma(t,\theta_t)d B_t, $$ where \((B_t)_{t \geqslant 0}\) is a standard Brownian motion [<a href="https://link.springer.com/book/10.1007/978-3-642-14394-6" rel="noreferrer noopener" target="_blank">10</a>]. In order to accurately represent the SGD dynamics, the drift term \(b\) and the noise \(\sigma\) should meet the following requirements:</p>



<div class="wp-container-19 is-vertical wp-block-group">
<p class="has-text-align-justify justify-text">(i) The drift term \(b\) should match the negative full gradient: \(b=-\nabla \mathcal{R}_n\).</p>



<p>(ii) The noise covariance \(\sigma\sigma^\top(t,\theta)\) should match \(\gamma\mathbb{E}[ \varepsilon_t(\theta_t) \varepsilon_t(\theta_t)^\top | \theta_t = \theta ]\).</p>



<p>(iii) The noise at state \(\theta\) should belong to \(\text{span}\{\nabla_\theta h(\theta, x_1), …, \nabla_\theta h(\theta, x_n) \}\).</p>



<p>\({\color{white} f} \)</p>
</div>



<p class="has-text-align-justify">Points (i) and (ii) imply that the SGD recursion corresponds in fact to the Euler-Maruyama discretisation with step-size \(\gamma\) of the SDE [<a href="https://jmlr.org/papers/v20/17-526.html" rel="noreferrer noopener" target="_blank">11</a>]. Hence, the SDE and the discrete models match perfectly for infinitesimal step-sizes (up to first order terms in \(\gamma\)) and the model is said to be <em>consistent</em>. Point (iii) states a geometrical property of the SGD noise and is of crucial importance for a proper modelisation.</p>



<p class="has-text-align-justify"><strong>The least-squares model.</strong> Consider once again our favourite least-squares model $$\mathcal{R}_n(\theta)= \frac{1}{2n}\sum_{i=1}^n (\langle \theta,x_i\rangle\, – y_i)^2,$$ and let \(X:= [x_1, …, x_n]^\top \in \mathbb{R}^{n \times d}\) be the data matrix and \(y = [y_1, …, y_n]^\top \in \mathbb{R}^n\) the vector of outputs. Let us explicitly write the SDE model in this case: the drift corresponds to the gradient \(b(\theta)=- \frac{1}{n}X^\top (X\theta – y) \) and it is simple linear algebra to calculate the covariance of the noise (see also [<a href="https://proceedings.mlr.press/v119/ali20a.html" rel="noreferrer noopener" target="_blank">19</a>, Remark 4])$$ \sigma \sigma^\top(\theta) =  \frac{\gamma}{n} X^\top \left(\textrm{diag}\left[ \big(\langle \theta, x_i\rangle\, – y_i\big)^2 \right]_{i=1}^n – \frac{1}{n}  \left[ \big(\langle \theta, x_i\rangle\, – y_i\big)\big(\langle \theta, x_j\rangle\, – y_j\big) \right]_{i,j=1}^n  \right) X. $$ To simplify a bit this analysis let us neglect the residual term of order \(1/n^2\) and assume that the residuals \(\langle \theta, x_i\rangle – y_i\), are approximately equal across all the samples of the data set: \(\textrm{diag}\left[ \left(\langle \theta, x_i\rangle\, – y_i\right)^2 \right]_{i=1}^n \simeq \frac{1}{n}\|X \theta\, – y\|^2 I_n \). Then the SDE model writes: $$d \theta_t =\, – \frac{1}{n}X^\top (X\theta_t\, – y) d t + \frac{\sqrt{\gamma}}{n} \| X\theta_t -y \| X^\top d B_t. $$ As stressed before, notice that there is an important difference between the under and overparametrised regimes: in the former, the noise spans all \(\mathbb{R}^d\) and as there exists \(\sigma &gt; 0\) such that \(\| X\theta_t -y \| \geqslant \sigma \), the noise is non degenerate in every direction. Instead, in the overparametrised regime, the noise vanishes at a global optimum \(\theta^*\) and is degenerate in the directions of \([\mathrm{range} (X^\top)]^\perp = \mathrm{Ker} (X)\). As also noticed in [<a href="https://proceedings.mlr.press/v119/ali20a.html" rel="noreferrer noopener" target="_blank">19</a>], this will result in important differences in term of the processes’ behaviour! In the rest of the post, let us denote \(\theta^{\text{LS}}:= X^\dagger y\), where \(X^\dagger\) is the pseudo inverse of \(X\). It corresponds to the ordinary least-square estimator in the underparametrised regime and the projection of \(0\) into \(\mathrm{Ker} (X)\) in the overparametrised setting.</p>



<p class="has-text-align-justify"><strong>The underparametrised regime and the multivariate <a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" rel="noreferrer noopener" target="_blank">Ornstein–Uhlenbeck process</a>.</strong> As written before, let us consider a model where the residuals scale like the best possible fit, i.e.  \(\frac{1}{\sqrt{n}}\| X\theta_t -y \| \simeq \sigma \). The dynamics reads: $$ d \theta_t =\, – \frac{1}{n}X^\top (X\theta_t\, – y) d t + \sqrt{\frac{\gamma}{n}} \sigma  X^\top d B_t. $$ This can be viewed as a <em>multivariate <a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" rel="noreferrer noopener" target="_blank">Ornstein–Uhlenbeck process</a></em> for which we can show the following convergence result: the law \(\pi_t\) of \(\theta_t\) converges to \(\pi^\gamma\), a Gaussian distribution of mean \(\theta^{\text{LS}}\) and covariance \(\frac{\gamma \sigma^2}{2} I_d\): $$\lim_{t \to \infty} \pi_t =\mathcal{N} \left( \theta^{\text{LS}}, \frac{\gamma \sigma^2}{2} I_d \right).$$ From this, as for the Markov chain interpretation, we can nicely interpret the need to decrease step-sizes to \(0\) in order to concentrate to \(\theta^{\text{LS}}\). Similarly, note that the ergodic theorem gives you automatically that the time average of \(\theta_t\) goes to \(\theta^{\text{LS}}\) as $$\qquad \qquad \qquad \lim_{t \to \infty} \frac{1}{t}\int_0^t \theta_s ds = \mathbb{E}[\pi^\gamma]=\theta^{\text{LS}}, \qquad \text{almost surely}.$$ Besides its nice interpretation as a Ornstein-Uhlenbeck process, we illustrate below the predictive power of the SDE model to understand the SGD dynamics.</p>



<div class="wp-container-22 wp-block-columns">
<div class="wp-container-20 wp-block-column is-vertically-aligned-top"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img alt="" class="wp-image-7449" height="480" src="https://francisbach.com/wp-content/uploads/2022/07/SGD_SDE_under-1.gif" width="720"/></figure></div></div>



<div class="wp-container-21 wp-block-column"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img alt="" class="wp-image-7444" height="480" src="https://francisbach.com/wp-content/uploads/2022/07/hist-2.gif" width="720"/></figure></div></div>
</div>



<div class="wp-container-25 wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-container-24 wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-container-23 is-vertical wp-block-group">
<p class="has-text-align-justify"><span style="text-decoration: underline;">Left</span>: \(10\) realisations of SGD in blue-like colours (corresponding to \(10\) different random sampling sequences), next to \(10\) realisations of the SDE model in red-like colours  (corresponding to \(10\) different realisations of the Brownian motion). </p>



<p class="has-text-align-justify"><span style="text-decoration: underline;">Right</span>: A histogram of snap-shots of the SGD dynamics (marginal along the first coordinate) at increasing time intervals that show the convergence (by ergodicity) to the predicted Gaussian \(\mathcal{N}(\theta^{\text{LS}}, \gamma \sigma^2/2)\) ! </p>
</div>
</div></div>
</div></div>



<p/>



<p class="has-text-align-justify justify-text"><strong>The overparametrised regime and the <a href="https://en.wikipedia.org/wiki/Geometric_Brownian_motion" rel="noreferrer noopener" target="_blank">geometric Brownian motion</a></strong>. We take back our SDE model, initialise it at \(\theta_0 = 0\), but now we are extra careful that the noise cancels at any interpolator \(\theta^*\). We will show that despite being a random process, the dynamics with constant \(\gamma &gt; 0\) converges almost surely to \(\theta^{\text{LS}}\). To do so let us consider the evolution of the  squared difference \(\eta_t = \|\theta_t – \theta^{\text{LS}}\|^2\). Thanks to <a href="https://en.wikipedia.org/wiki/It%C3%B4_calculus" rel="noreferrer noopener" target="_blank">Itô calculus</a> (chain rule for stochastic processes), $$\begin{aligned} d \eta_t &amp;= 2 \langle d \theta_t, \theta_t – \theta^{\text{LS}} \rangle + \frac{\gamma}{n^2} \mathrm{tr} (X^\top X) \| X (\theta_t – \theta^{\text{LS}}) \|^2   dt \\ &amp;=  2\left( \gamma \mathrm{tr} \left(n^{-1}X^\top X\right) – 2 \right) \mathcal{R}_n(\theta_t) dt + 4\sqrt{\gamma}\mathcal{R}_n(\theta_t) dW_t, \end{aligned} $$ where \((W_t)_{t \geqslant 0}\) is a one dimensional Brownian motion (see details of this calculation at the end of the post). To simplify the calculation set the step-size such that \(\gamma \mathrm{tr} (n^{-1}X^\top X) = 1\) (note that we retrieve here the usual step-size for least-squares SGD [<a href="https://proceedings.mlr.press/v38/defossez15.html" rel="noreferrer noopener" target="_blank">16</a>]). Noting \(\mu_t = \frac{2\mathcal{R}_n(\theta_t)}{\eta_t}\) and \(\nu_t = \frac{4\sqrt{\gamma}\mathcal{R}_n(\theta_t)}{\eta_t}\), the dynamics of \((\eta_t)_{t \geqslant 0}\) follows $$d  \eta_t =  -\mu_t  \eta_t dt + \nu_t \eta_t dW_t, $$ Furthermore, as \(\theta_t – \theta^{\text{LS}} \in \mathrm{span} (X^\top)\) , we have \(\mu\cdot \eta_t\leqslant 2 \mathcal{R}_n(\theta_t) \leqslant L\cdot \eta_t\), where \(\mu\) and \(L\) stand respectively for the minimum and maximum eigenvalues of \(\frac{1}{n}XX^\top\). Hence, the behaviour of \((\eta_t)_{t \geqslant 0}\) is approximately the same as that of the solution to the following SDE: $$d  S_t =  -\mu  S_t dt + \nu S_t dW_t, $$ for \(\nu \simeq 4 L \sqrt{\gamma}\). That is, the dynamics of \((\|\theta_t – \theta^{\text{LS}}\|^2)_{t \geqslant 0}\) is approximately the one of a <a href="https://en.wikipedia.org/wiki/Geometric_Brownian_motion" rel="noreferrer noopener" target="_blank">Geometric Brownian Motion</a> (GBO), a.k.a. the process underlying the <a href="https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model" rel="noreferrer noopener" target="_blank">Black-Scholes model</a>! Hence, to understand SGD in the overparametrised regime, one “simply” need to study the properties of the GBO. It is known that in fact there is an explicit solution of this SDE $$ S_t =  S_0 \exp\left( -\left(\mu + \frac{\nu^2}{2}\right) t  + \nu W_t\right). $$ Hence, for all \(t&gt;0\), \(S_t\) is distributed according to a log-normal random variable with mean \(\mathbb{E}(S_t) = S_0 e^{- \mu t}\) and variance \(\text{var}(S_t) = S_0^2 e^{- 2\mu t} (e^{\nu^2 t} – 1)\). Furthermore, it is a nice application to the <a href="https://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm" rel="noreferrer noopener" target="_blank">law of the iterated logarithm</a> for the Brownian motion to show that  $$ \qquad \qquad \qquad \lim_{t \to \infty} S_t = 0, \qquad \text{almost surely!} $$ The almost sure convergence of \((\theta_t)_{t \geqslant 0}\) to \(\theta^{\text{LS}}\) is in stark contrast with the limiting behaviour we have seen in the underparametrised setting. Remarkably, the process does not require any variance reduction to converge to a specific point. Note also that among all the possible interpolators the process could have converged to, the process in fact goes almost surely to \(\theta^{\text{LS}}\). This selection of the minimum norm interpolator [<a href="https://openreview.net/forum?id=Sy8gdB9xx" rel="noreferrer noopener" target="_blank">12</a>], known as the implicit bias of the process  will be the core of a next blog post (for more complex architectures).</p>



<p>As in the underparametrised case, below are two plots showing the typical behaviour of the SGD dynamics as a GBO.</p>



<div class="wp-container-28 wp-block-columns">
<div class="wp-container-26 wp-block-column is-vertically-aligned-top"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img alt="" class="wp-image-7445" height="480" src="https://francisbach.com/wp-content/uploads/2022/07/SGD_SDE_over-1.gif" width="720"/></figure></div></div>



<div class="wp-container-27 wp-block-column"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img alt="" class="wp-image-7443" height="480" src="https://francisbach.com/wp-content/uploads/2022/07/Geometric_brownian-1.gif" width="720"/></figure></div></div>
</div>



<div class="wp-container-29 is-vertical wp-block-group">
<p><span style="text-decoration: underline;">Left</span>: \(10\) realisations of SGD in blue-like colours (corresponding to \(10\) different random sampling sequences), next to \(10\) realisations of the SDE model in red-like colours (corresponding to \(10\) different realisations of the Brownian motion). </p>



<p><span style="text-decoration: underline;">Right</span>: The exponential convergence of  \(\eta_t = \|\theta_t – \theta^{\text{LS}}\|^2\) to \(0\) for the \(10\) realisations of SGD and the SDE (in log-scale).</p>



<p/>



<p/>
</div>



<h2 id="conclusion">Conclusion</h2>



<p class="has-text-align-justify">In this blog post, I have tried to show that despite the old legacy inherited from the stochastic approximation literature, the particularities of the noise of the stochastic gradient descent in machine learning contexts necessitate a fresh look.  This specificity strikes the most in modern overparametrised models as the noise can be largely degenerate, both (i) in direction (which I called <em>noise geometry</em>), and (ii) in scale, as its variance is dominated by the loss and hence vanishes at a global optimum. In fact I have tried to show that SDE models can be a relevant framework to shed light on typical behaviours of stochastic learning dynamics: for overparametrised architectures, they differentiate from the <a href="https://en.wikipedia.org/wiki/Langevin_dynamics" rel="noreferrer noopener" target="_blank">overdamped Langevin diffusion model</a>, and act as multiplicative noise dynamics like the geometric brownian motion.</p>



<p class="has-text-align-justify">Besides, as it is believed that stochasticity plays a role in the generalisation performances of optimisation algorithms [<a href="https://arxiv.org/pdf/1609.04836.pdf" rel="noreferrer noopener" target="_blank">13</a>], there is a need to consider its action precisely. Yet, grasping its overall nature appears to be particularly hard as it mixes properties from the model’s architecture, the data’s distribution and the loss. In a following post, I will try to analyse some simple non-convex models where we can show that the SGD-noise drives the dynamics to possibly “good for generalisation” regions [<a href="https://proceedings.neurips.cc/paper/2021/hash/f4661398cb1a3abd3ffe58600bf11322-Abstract.html" rel="noreferrer noopener" target="_blank">17</a>,<a href="https://proceedings.mlr.press/v178/vivien22a.html" rel="noreferrer noopener" target="_blank">18</a>].<br/><br/><strong>Acknowledgements</strong>. I would like to thank Scott Pesme and Etienne Boursier for fruitful discussions, and especially <a href="https://open.spotify.com/track/1r6EJWSoh9Kzn6L38BhYS6?si=b352952e9bac4420" rel="noreferrer noopener" target="_blank">Nicolas Le Hir</a> for proofreading this blog post and making good clarifying suggestions.</p>



<h2 id="references">References</h2>



<div class="wp-container-40 is-vertical wp-block-group">
<div class="wp-container-39 is-vertical wp-block-group">
<div class="wp-container-38 is-vertical wp-block-group">
<div class="wp-container-37 wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-container-36 is-vertical wp-block-group">
<div class="wp-container-35 is-vertical wp-block-group">
<div class="wp-container-34 is-vertical wp-block-group">
<div class="wp-container-33 is-vertical wp-block-group">
<div class="wp-container-32 is-vertical wp-block-group">
<div class="wp-container-31 is-vertical wp-block-group">
<div class="wp-container-30 is-vertical wp-block-group">
<p class="justify-text">[1] Herbert Robbins and Sutton Monro. <a href="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full" rel="noreferrer noopener" target="_blank">A Stochastic Approximation Method</a>. The Annals of Mathematical Statistics, 22 (3): 400 – 407, September, 1951.<br/>[2] Marie Duflo. <a href="https://link.springer.com/book/9783540606994" rel="noreferrer noopener" target="_blank">Algorithmes stochastiques</a>. Volume 23 of Mathématiques &amp; Applications (Berlin) [Mathematics &amp; Applications]. Springer-Verlag, Berlin, 1996<br/>[3] Léon Bottou and Olivier Bousquet. <a href="https://proceedings.neurips.cc/paper/2007/hash/0d3180d672e08b4c5312dcdafdf6ef36-Abstract.html" rel="noreferrer noopener" target="_blank">The Tradeoffs of Large Scale Learning</a>, In Advances in Neural Information Processing Systems, vol.20, 161-168, 2008.<br/>[4] Sean P.Meyn and Richard L. Tweedie. <a href="https://link.springer.com/book/10.1007/978-1-4471-3267-7" rel="noreferrer noopener" target="_blank">Markov Chains and Stochastic Stability.</a> Springer, London, 1993.<br/>[5] Georg Ch. Pflug. <a href="https://epubs.siam.org/doi/10.1137/0324039" rel="noreferrer noopener" target="_blank">Stochastic minimization with constant step-size: asymptotic laws</a>. SIAM Journal on Control and Optimization, 24(4):655–666, 1986.<br/>[6] Francis Bach and Eric Moulines. <a href="https://papers.nips.cc/paper/2011/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html" rel="noreferrer noopener" target="_blank">Non-asymptotic analysis of stochastic approximation algorithms for machine learning.</a> In Advances in Neural Information Processing Systems (NIPS), 2011.<br/>[7] Aymeric Dieuleveut, Alain Durmus and Francis Bach. <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-3/Bridging-the-gap-between-constant-step-size-stochastic-gradient-descent/10.1214/19-AOS1850.full" rel="noreferrer noopener" target="_blank">Bridging the gap between constant step size stochastic gradient descent and Markov chains.</a> The Annals of Statistics, 48 (3): 1348 – 1382, June 2020.<br/>[8] Stephan Wojtowytsch. <a href="https://arxiv.org/pdf/2105.01650.pdf" rel="noreferrer noopener" target="_blank">Stochastic gradient descent with noise of machine learning type.</a> Part I: Discrete time analysis Technical Report, arXiv-2105.01650, 2021.<br/>[9] Aditya Varre, Loucas Pillaud-Vivien and Nicolas Flammarion. <a href="https://proceedings.neurips.cc/paper/2021/hash/b4a0e0fbaa9f16d8947c49f4e610b549-Abstract.html" rel="noreferrer noopener" target="_blank">Last iterate convergence of SGD for Least-Squares in the Interpolation regime.</a> Advances in Neural Information Processing Systems 34, 21581-21591, 2021.<br/>[10] Bernt Øksendal. <a href="https://link.springer.com/content/pdf/10.1007/978-3-642-14394-6_5.pdf" rel="noreferrer noopener" target="_blank">Stochastic Differential Equations: An Introduction with Applications</a>, 6th edition. Springer, New York, 2003.<br/>[11] Qianxiao Li, Cheng Tai, and Weinan E. <a href="https://jmlr.org/papers/v20/17-526.html" rel="noreferrer noopener" target="_blank">Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations.</a> Journal of Machine Learning Research, 20(40):1–47, 2019.<br/>[12] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. <a href="https://openreview.net/forum?id=Sy8gdB9xx" rel="noreferrer noopener" target="_blank">Understanding deep learning requires rethinking generalization</a>. In International Conference on Learning Representations, 2017.<br/>[13] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. <a href="https://arxiv.org/pdf/1609.04836.pdf" rel="noreferrer noopener" target="_blank">On large-batch training for deep learning: Generalization gap and sharp minima.</a> In International Conference on Learning Representations, 2017.[8] Stephan Wojtowytsch. <a href="https://arxiv.org/pdf/2105.01650.pdf" rel="noreferrer noopener" target="_blank">Stochastic gradient descent with noise of machine learning type.</a> Part I: Discrete time analysis Technical Report, arXiv-2105.01650, 2021.<br/>[14] Sharan Vaswani, Francis Bach and Mark Schmidt. <a href="https://proceedings.mlr.press/v89/vaswani19a.html" rel="noreferrer noopener" target="_blank">Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron</a>. Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, PMLR 89:1195-1204, 2019.<br/>[15] Siyuan Ma, Raef Bassily and Mikhail Belkin. <a href="https://proceedings.mlr.press/v80/ma18a.html" rel="noreferrer noopener" target="_blank">The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning</a>. Proceedings of the 35th International Conference on Machine Learning, PMLR 80:3325-3334, 2018.<br/>[16] Alexandre Defossez and Francis Bach. <a href="https://proceedings.mlr.press/v38/defossez15.html" rel="noreferrer noopener" target="_blank">Averaged Least-Mean-Squares: Bias-Variance Trade-offs and Optimal Sampling Distributions.</a> Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, PMLR 38:205-213, 2015.<br/>[17] Scott Pesme, Loucas Pillaud-Vivien and Nicolas Flammarion. <a href="https://proceedings.neurips.cc/paper/2021/hash/f4661398cb1a3abd3ffe58600bf11322-Abstract.html" rel="noreferrer noopener" target="_blank">Implicit bias of SGD for diagonal linear networks: a provable benefit of stochasticity.</a> In Advances in Neural Information Processing Systems 34, 29218-29230, 2021.<br/>[18] Loucas Pillaud Vivien, Julien Reygner and Nicolas Flammarion. <a href="https://proceedings.mlr.press/v178/vivien22a.html" rel="noreferrer noopener" target="_blank">Label noise (stochastic) gradient descent implicitly solves the Lasso for quadratic parametrisation.</a> Proceedings of Thirty Fifth Conference on Learning Theory, PMLR 178:2127-2159, 2022.<br/>[19] Alnur Ali, Edgar Dobriban and Ryan Tibshirani. <a href="https://proceedings.mlr.press/v119/ali20a.html" rel="noreferrer noopener" target="_blank">The Implicit Regularization of Stochastic Gradient Flow for Least Squares</a>. Proceedings of the 37th International Conference on Machine Learning, PMLR 119:233-244, 2020.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>



<p class="justify-text"/>



<h2>Details on the evolution equation of \(\eta_t = \|\theta_t – \theta^{\text{LS}}\|^2\).</h2>



<p/>



<p class="has-text-align-justify">Recall that the evolution of the iterates is given by the SDE: $$ d \theta_t =\, – \frac{1}{n}X^\top (X\theta_t\, – y) d t + \frac{\sqrt{\gamma}}{n} \| X\theta_t -y \| X^\top d B_t. $$ Then by <a href="https://en.wikipedia.org/wiki/It%C3%B4_calculus" rel="noreferrer noopener" target="_blank">Itô calculus</a>, we have $$\begin{aligned} d \eta_t &amp;= 2 \langle d \theta_t, \theta_t – \theta^{\text{LS}} \rangle + \frac{\gamma}{n^2} \mathrm{tr} (X^\top X) \| X (\theta_t – \theta^{\text{LS}}) \|^2   dt \\ &amp;= -\frac{2}{n} \langle X^\top (X\theta_t\, – y), \theta_t – \theta^{\text{LS}} \rangle dt + 2 \sqrt{\frac{2\gamma}{n}} \sqrt{\mathcal{R}_n(\theta_t)} \langle X^\top d B_t, \theta_t – \theta^{\text{LS}} \rangle \\ &amp; \hspace{6.5cm}+ 2\gamma \mathrm{tr} (n^{-1}X^\top X) \mathcal{R}_n(\theta_t)   dt. \end{aligned}$$ Since,  \(\theta^{\text{LS}}\) is an interpolator, i.e \(X \theta^{\text{LS}} = y\), the first term corresponds to $$ -\frac{2}{n}\langle X^\top (X\theta_t\, – y), \theta_t – \theta^{\text{LS}} \rangle = -\frac{2}{n}\langle X(\theta_t – \theta^{\text{LS}}), X(\theta_t – \theta^{\text{LS}}) \rangle = -4 \mathcal{R}_n(\theta_t).$$ For the second term, note that \(\langle X^\top d B_t, \theta_t – \theta^{\text{LS}} \rangle = \langle d B_t, X(\theta_t – \theta^{\text{LS}}) \rangle\), and by <a href="https://almostsuremath.com/2010/04/13/levys-characterization-of-brownian-motion/" rel="noreferrer noopener" target="_blank">Lévy’s Characterization of Brownian Motion</a>, the local martingale $$ W_t = \int_0^t \frac{\langle d B_s, X(\theta_s – \theta^{\text{LS}})\rangle}{\|X(\theta_s – \theta^{\text{LS}})\|} $$ is in fact a one dimensional Brownian motion. Hence, $$ \langle X^\top d B_t, \theta_t – \theta^{\text{LS}} \rangle = \|X(\theta_s – \theta^{\text{LS}})\| dW_t = \sqrt{2 n \mathcal{R}_n(\theta_t)}dW_t,$$ and finally, $$ d \eta_t = -4 \mathcal{R}_n(\theta_t) dt + 4 \sqrt{\gamma} \mathcal{R}_n(\theta_t) d W_t + 2 \gamma \mathrm{tr} (n^{-1}X^\top X) \mathcal{R}_n(\theta_t)   dt,$$ which gives the claimed result of the main text.</p></div>
    </content>
    <updated>2022-07-25T13:32:15Z</updated>
    <published>2022-07-25T13:32:15Z</published>
    <category term="Machine learning"/>
    <category term="Optimization"/>
    <author>
      <name>Loucas Pillaud-Vivien</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2022-07-29T00:21:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6792824547495697148</id>
    <link href="http://blog.computationalcomplexity.org/feeds/6792824547495697148/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/100-best-number-theory-books-of-all.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6792824547495697148" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6792824547495697148" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/100-best-number-theory-books-of-all.html" rel="alternate" type="text/html"/>
    <title>100 Best Number Theory books of all Time---except many are not on Number Theory</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I was recently emailed this link:<div><br/></div><div><a href="https://bookauthority.org/books/best-number-theory-books">100 Best Number Theory books of all Time</a><br/></div><div><br/></div><div>That sounds like a good list to have!  But then I looked at it. </div><div><br/></div><div>The issue IS NOT that the books on it are not good. I suspect they all are.</div><div><br/>The issue IS that many of the books on the list are not on Number Theory.</div><div><br/></div><div>DEFINITLY NOT:</div><div><br/></div><div>A Mathematicians Apology by Hardy</div><div><br/></div><div>The Universe Speaks in Numbers by Farmelo (looks like Physics)</div><div><br/></div><div>Category theory in Context by Riehl</div><div><br/></div><div>A First Course in Mathematical logic and set theory by O'Leary</div><div><br/>Astronomical Algorithms by Meeus (Algorithms for Astronomy)</div><div><br/></div><div>Pocket Book of Integrals and Math Formulas by Tallardia</div><div><br/></div><div>Entropy and Diversity by Leinster</div><div><br/></div><div>BORDERLINE:</div><div><br/>Too many to name, so I will name categories (Not the type Riehl talks about)</div><div><br/></div><div>Logic books. Here <i>Number Theory </i> seems to mean <i>Peano Arithmetic</i> and they are looking at what you can and can't prove in it. </div><div><br/></div><div>Combinatorics books:  Indeed, sometimes it is hard to draw the line between Combinatorics and Number Theory, but I still would not put a book on <i>Analytic Combinatorics o</i>n a list of top books in Number Theory. </div><div><br/></div><div>Discrete Math textbooks: Yes, most discrete math textbooks have some elementary number theory in them, but that does not make them number theory books.</div><div><br/></div><div>Abstract Algebra, Discrete Harmonic Analysis, other hard math books: These have theorems in Number Theory as an Application.  But they are not books on number theory. </div><div><br/>WHAT OF ALL THIS? </div><div><br/></div><div>Lists like this often have several problems</div><div><br/></div><div>1) The actual object of study is not well defined.</div><div><br/></div><div>2) The criteria for being good is not well defined.</div><div><br/></div><div>3) The list is just one person's opinion. If I think the best math-novelty song of all time is William-Rowan-Hamilton (see  <a href="https://www.youtube.com/watch?v=SZXHoWwBcDc">here</a>) and the worse one is the Bolzano--Weierstrass rap (see <a href="https://www.youtube.com/watch?v=dfO18klwKHg&amp;t=8s">here</a>) that's just my opinion. Even if I was the leading authority on Math Novelty songs and had the largest collection in the world, its still just my opinion. (Another contender for worst math song of all time is <a href="https://www.youtube.com/watch?v=4xQFlsK7jKg&amp;t=10s">here</a>.)</div><div><br/></div><div>4) Who is the audience for such lists? For the Number Theory Books is the audience ugrad math majors? grad math majors? Number Theorists? This needs to be well defined.</div><div><br/></div><div>5) The list may tell more about the people making the list then the intrinsic qualify of the objects. This is more common in, say, the ranking of presidents. My favorite is Jimmy Carter since he is the only one with the guts to be sworn in by his nickname Jimmy, unlike  Bill Clinton (sworn in as William Jefferson Clinton- a name only used by his mother when she was mad at him) or Joe Biden (sworn in as Joseph Robinette Biden which I doubt even his mother ever used). My opinion may seem silly, but it reflects my bias towards nicknames, just as the people who rank presidents use their bias. </div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div></div>
    </content>
    <updated>2022-07-24T22:46:00Z</updated>
    <published>2022-07-24T22:46:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-07-28T17:47:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/108</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/108" rel="alternate" type="text/html"/>
    <title>TR22-108 |  Hardness Self-Amplification from Feasible Hard-Core Sets | 

	Shuichi Hirahara, 

	Nobutaka Shimizu</title>
    <summary>We consider the question of hardness self-amplification: Given a Boolean function $f$ that is hard to compute on a $o(1)$-fraction of inputs drawn from some distribution, can we prove that $f$ is hard to compute on a $(\frac{1}{2} - o(1))$-fraction of inputs drawn from the same distribution? We prove hardness self-amplification results for natural distributional problems studied in fine-grained average-case complexity, such as the problem of counting the number of the triangles modulo 2 in a random tripartite graph and the online vector-matrix-vector multiplication problem over $\mathbb{F}_2$. More generally, we show that any problem that can be decomposed into ``computationally disjoint'' subsets of inputs admits hardness self-amplification. This is proved by generalizing the security proof of the Nisan--Wigderson pseudorandom generator, in which case nearly disjoint subsets of inputs are considered.
    
At the core of our proof techniques is a new notion of feasible hard-core set, which generalizes Impagliazzo's hard-core set [Impagliazzo, FOCS'95]. We show that any weak average-case hard function $f$ has a feasible hard-core set $H$: any small $H$-oracle circuit (that is allowed to make queries $q$ to $H$ if $f(q)$ can be computed without the oracle) fails to compute $f$ on a $(\frac{1}{2} - o(1))$-fraction of inputs in $H$.</summary>
    <updated>2022-07-24T03:28:50Z</updated>
    <published>2022-07-24T03:28:50Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-29T00:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=2216</id>
    <link href="https://toc4fairness.org/cacm-formalizing-fairness/" rel="alternate" type="text/html"/>
    <title>CACM – Formalizing Fairness</title>
    <summary>There is a new article in CACM, by Mariana Krakovsky, who wrote a really nice introduction to the work within TOC on Algorithmic Fairness. It does not (and cannot) get ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>There is a <a href="https://cacm.acm.org/magazines/2022/8/262911-formalizing-fairness/fulltext">new article in CACM</a>, by <a href="https://marinakrakovsky.com/">Mariana Krakovsky</a>, who wrote a really nice introduction to the work within TOC on Algorithmic Fairness. It does not (and cannot) get too deep into anything but I think it is quite well crafted.</p></div>
    </content>
    <updated>2022-07-23T02:35:50Z</updated>
    <published>2022-07-23T02:35:50Z</published>
    <category term="Blog"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i0.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2022-07-29T00:21:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6593</id>
    <link href="https://scottaaronson.blog/?p=6593" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6593#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6593" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">More AI debate between me and Steven Pinker!</title>
    <summary xml:lang="en-US">Several people have complained that Shtetl-Optimized has become too focused on the niche topic of “people being mean to Scott Aaronson on the Internet.” In one sense, this criticism is deeply unfair—did I decide that a shockingly motivated and sophisticated troll should attack me all week, in many cases impersonating fellow academics to do so? […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Several people have complained that <em>Shtetl-Optimized</em> has become too focused on the niche topic of “people being mean to Scott Aaronson on the Internet.”  In one sense, this criticism is deeply unfair—did I <em>decide</em> that a shockingly motivated and sophisticated troll should attack me all week, in many cases impersonating fellow academics to do so?  Has such a thing happened to <em>you</em>?  Did I <em>choose</em> a personality that forces me to respond when it happens?</p>



<p>In another sense, the criticism is of course completely, 100% justified.  That’s why I’m happy and grateful to have formed the <a href="https://scottaaronson.blog/?p=6576">SOCG (Shtetl-Optimized Committee of Guardians)</a>, whose purpose is to prevent a recurrence, thereby letting me get back to your regularly scheduled programming.</p>



<p>On that note, I hope the complainers will be satisfied with more exclusive-to-<em>Shtetl-Optimized</em> content from one of the world’s greatest living public intellectuals: the Johnstone Family Professor of Psychology at Harvard University, Steven Pinker.</p>



<p>Last month, you’ll recall, Steve and I <a href="https://scottaaronson.blog/?p=6524">debated the implications of scaling AI models</a> such as GPT-3 and DALL-E.  A main crux of disagreement turned out to be whether there’s any coherent concept of “superintelligence.”  I gave a qualified “yes” (I can’t provide <em>necessary and sufficient conditions</em> for it, nor do I know <em>when</em> AI will achieve it if ever, but there are certainly things an AI could do that would cause me to say it <em>was</em> achieved).  Steve, by contrast, gave a strong “no.”</p>



<p>My friend (and previous <em>Shtetl-Optimized</em> guest blogger) Sarah Constantin then wrote a <a href="https://sarahconstantin.substack.com/p/is-human-intelligence-simple-part-75c">thoughtful response to Steve</a>, taking a different tack than I had.  Sarah emphasized that Steve himself is on record defending the statistical validity of <a href="https://en.wikipedia.org/wiki/G_factor_(psychometrics)">Spearman’s <em>g</em></a>: the “general factor of human intelligence,” which accounts for a large fraction of the variation in humans’ performance across nearly every intelligence test ever devised, and which is <em>also</em> found to correlate with cortical thickness and other physiological traits.  Is it so unreasonable, then, to suppose that <em>g</em> is measuring <em>something</em> of abstract significance, such that it would continue to make sense when extrapolated, not to godlike infinity, but at any rate, well beyond the maximum that happens to have been seen in humans?</p>



<p>I relayed Sarah’s question to Steve.  (As it happens, the same question was also discussed at length in, e.g., <a href="https://www.vetta.org/documents/Machine_Super_Intelligence.pdf">Shane Legg’s 2008 PhD thesis</a>; Legg then went on to cofound <a href="https://en.wikipedia.org/wiki/DeepMind">DeepMind</a>.)  Steve was then gracious enough to write the following answer, and to give me permission to post it here.  I’ll also share my reply to him.  There’s some further back-and-forth between me and Steve that I’ll save for the comments section to kick things off there.  Everyone is warmly welcomed to join: just remember to stay on topic, be respectful, and <strong>click the link in your verification email!</strong></p>



<p>Without further ado:</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2>Comments on General, Artificial, and Super-Intelligence</h2>



<p><em>by Steven Pinker</em></p>



<p>While I defend the existence and utility of IQ and its principal component, general intelligence or <em>g</em>,  in the study of individual differences, I think it’s completely irrelevant to AI, AI scaling, and AI safety. It’s a measure of differences among humans within the restricted range they occupy, developed more than a century ago. It’s a statistical construct with no theoretical foundation, and it has tenuous connections to any mechanistic understanding of cognition other than as an omnibus measure of processing efficiency (speed of neural transmission, amount of neural tissue, and so on). It exists as a coherent variable only because performance scores on subtests like vocabulary, digit string memorization, and factual knowledge intercorrelate, yielding a statistical principal component, probably a global measure of neural fitness.</p>



<p>In that regard, it’s like a <em>Consumer Reports </em>global rating of cars, or overall score in the pentathlon. It would not be surprising that a car with a more powerful engine also had a better suspension and sound system, or that better swimmers are also, on average, better fencers and shooters. But this tells us precisely nothing about how engines or human bodies work. And imagining an extrapolation to a supervehicle or a superathlete is an exercise in fantasy but not a means to develop new technologies.</p>



<p>Indeed, if “superintelligence” consists of sky-high IQ scores, it’s been here since the 1970s! A few lines of code could recall digit strings or match digits to symbols orders of magnitude better than any human, and old-fashioned AI programs could also trounce us in multiple-choice vocabulary tests, geometric shape extrapolation (“progressive matrices”), analogies, and other IQ test components. None of this will help drive autonomous vehicles, discover cures for cancer, and so on.</p>



<p>As for recent breakthroughs in AI which may or may not surpass humans (the original prompt for this exchange); What is the IQ of GPT-3, or DALL-E, or AlphaGo? The question makes no sense!</p>



<p>So, to answer your question: yes, general intelligence in the psychometrician’s sense is not something that can be usefully extrapolated. And it’s “one-dimensional” only in the sense that a single statistical principal component can always be extracted from a set of intercorrelated variables.</p>



<p>One more point relevant to the general drift of the comments. My statement that “superintelligence” is incoherent is not a semantic quibble that the word is meaningless, and it’s not a pre-emptive strategy of Moving the True Scottish Goalposts. Sure, you could <em>define</em> “superintelligence,” just as you can define “miracle” or “perpetual motion machine” or “square circle.” And you could even recognize it if you ever saw it. But that does not make it coherent in the sense of being physically realizable.</p>



<p>If you’ll forgive me one more analogy, I think “superintelligence” is like “superpower.” Anyone can define “superpower” as “flight, superhuman strength, X-ray vision, heat vision, cold breath, super-speed, enhance hearing, and nigh-invulnerability.” Anyone could imagine it, and recognize it when he or she sees it. But that does not mean that there exists a highly advanced physiology called “superpower” that is possessed by refugees from Krypton!  It does not mean that anabolic steroids, because they increase speed and strength, can be “scaled” to yield superpowers. And a skeptic who makes these points is not quibbling over the meaning of the word <em>superpower, </em>nor would he or she balk at applying the word upon meeting a real-life Superman. Their point is that we almost certainly will never, in fact, meet a real-life Superman. That’s because he’s defined by human imagination, not by an understanding of how things work. We will, of course, encounter machines that are faster than humans, and that see X-rays, that fly, and so on, each exploiting the relevant technology, but “superpower” would be an utterly useless way of understanding them.</p>



<p>To bring it back to productive discussions of AI: there’s plenty of room to analyze the capabilities and limitations of particular intelligent algorithms and data structures—search, pattern-matching, error back-propagation, scripts, multilayer perceptrons, structure-mapping, hidden Markov models, and so on. But melting all these mechanisms into a global variable called “intelligence,” understanding it via turn-of-the-20<sup>th</sup>-century school tests, and mentally extrapolating it with a comic-book prefix, is, in my view, not a productive way of dealing with the challenges of AI.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2>Scott’s Response</h2>



<p>I wanted to drill down on the following passage:</p>



<blockquote class="wp-block-quote"><p>Sure, you could define “superintelligence,” just as you can define “miracle” or “perpetual motion machine” or “square circle.” And you could even recognize it if you ever saw it.  But that does not make it coherent in the sense of being physically realizable.</p></blockquote>



<p>The way I use the word “coherent,” it basically <em>means</em> “we could recognize it if we saw it.”  Clearly, then, there’s a sharp difference between this and “physically realizable,” although any physically-realizable empirical behavior must be coherent.  Thus, “miracle” and “perpetual motion machine” are both coherent but presumably not physically realizable.  “Square circle,” by contrast, is not even coherent.</p>



<p>You now seem to be saying that “superintelligence,” like “miracle” or “perpetuum mobile,” is coherent (in the “we could recognize it if we saw it” sense) but not physically realizable.  If so, then that’s a big departure from what I understood you to be saying before!  I thought you were saying that we couldn’t even recognize it.</p>



<p>If you do agree that there’s a quality that we could recognize as “superintelligence” if we saw it—and I don’t mean mere memory or calculation speed, but, let’s say, “the quality of being to John von Neumann in understanding and insight as von Neumann was to an average person”—and if the debate is merely over the physical realizability of that, then the arena shifts back to human evolution.  As you know far better than me, the human brain was limited in scale by the width of the birth canal, the need to be mobile, and severe limitations on energy.  And it wasn’t optimized for understanding algebraic number theory or anything else with no survival value in the ancestral environment.  So why should we think it’s gotten anywhere near the limits of what’s physically realizable in our world?</p>



<p>Not only does the concept of “superpowers” seem coherent to me, but from the perspective of someone a few centuries ago, we arguably <em>have</em> superpowers—the ability to summon any of several billion people onto a handheld video screen at a moment’s notice, etc. etc.  You’d probably reply that AI should be thought of the same way: just more tools that will enhance our capabilities, like airplanes or smartphones, not some terrifying science-fiction fantasy.</p>



<p>What I keep saying is this: we have the luxury of regarding airplanes and smartphones as “mere tools” <em>only</em> because there remain so many clear examples of tasks we can do that our devices can’t.  What happens when the devices can do <em>everything</em> important that we can do, much better than we can?  Provided we’re physicalists, I don’t see how we reject such a scenario as “not physically realizable.”  So then, are you making an empirical prediction that this scenario, although both coherent and physically realizable, won’t come to pass for thousands of years?  Are you saying that it might come to pass much sooner, like maybe this century, but even if so we shouldn’t worry, since a tool that can do everything important better than we can do it is still just a tool?</p></div>
    </content>
    <updated>2022-07-22T01:46:29Z</updated>
    <published>2022-07-22T01:46:29Z</published>
    <category scheme="https://scottaaronson.blog" term="Metaphysical Spouting"/>
    <category scheme="https://scottaaronson.blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-07-28T17:22:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/07/21/flipping-until-lost</id>
    <link href="https://11011110.github.io/blog/2022/07/21/flipping-until-lost.html" rel="alternate" type="text/html"/>
    <title>Flipping until you are lost</title>
    <summary>Start with any triangulation of a convex polygon, and then repeatedly choose a random diagonal and flip it, replacing the two triangles it borders with two different triangles. Eventually, these random flips will cause your triangulation to be nearly equally likely to be any of the possible triangulations of the polygon. But how long is “eventually”? My student Daniel Frishberg has a new answer, in our preprint “Improved mixing for the convex polygon triangulation flip walk” (arXiv:2207.09972).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Start with any triangulation of a convex polygon, and then repeatedly choose a random diagonal and flip it, replacing the two triangles it borders with two different triangles. Eventually, these random flips will cause your triangulation to be nearly equally likely to be any of the possible triangulations of the polygon. But how long is “eventually”? My student Daniel Frishberg has a new answer, in our preprint “Improved mixing for the convex polygon triangulation flip walk” (<a href="https://arxiv.org/abs/2207.09972">arXiv:2207.09972</a>).</p>

<p style="text-align: center;"><img alt="Flip graph of a hexagon" src="https://11011110.github.io/blog/assets/2006/fg6.png"/></p>

<p>The triangulations of an \(n\)-gon, and the flips between them, form the vertices and edges of an \((n-3)\)-dimensional convex polytope called the <a href="https://en.wikipedia.org/wiki/Associahedron">associahedron</a>; the example above, showing the flips of a hexagon, is from <a href="https://11011110.github.io/blog/2006/10/13/another-gratuitously-nonplanar-drawing.html">an older post</a>. It’s difficult to convey all the symmetries of the four-dimensional associahedron (representing the triangulations of a heptagon) in a planar drawing, but here it is as a graph, at least:</p>

<p style="text-align: center;"><img alt="Flip graph of a heptagon" src="https://11011110.github.io/blog/assets/2022/heptagon-flips.svg"/></p>

<p>The number of steps until a random walk converges to near its <a href="https://en.wikipedia.org/wiki/Stable_distribution">stable distribution</a> is called its <a href="https://en.wikipedia.org/wiki/Markov_chain_mixing_time">mixing time</a>. So another way of stating the results in the preprint is that we provide a new bound on the mixing time of associahedra. <a href="https://tetali.math.gatech.edu/PUBLIS/mt.pdf">A 1997 paper of McShine and Tetali</a> shows that this mixing time is \(O(n^5\log n)\), and we improve the exponent slightly, to \(O(n^{4.75})\). Which doesn’t sound like much, but it’s the first improvement in 25 years!</p>

<p>It’s part of a line of research Daniel has been following on mixing times, <a href="https://en.wikipedia.org/wiki/Treewidth">treewidth</a>,  and <a href="https://en.wikipedia.org/wiki/Expander_graph">expansion</a> of large state spaces, including <a href="https://11011110.github.io/blog/2020/05/03/hanoi-vs-sierpinski.html">Hanoi graphs</a> and <a href="https://11011110.github.io/blog/2021/11/14/random-independent-sets.html">independent sets in bounded-treewidth graphs</a>. Like those other papers, it exploits known connections between mixing time, treewidth, expansion, and <a href="https://en.wikipedia.org/wiki/Multi-commodity_flow_problem">multi-commodity flow</a> to formulate the problem as one of finding a system of paths between every pair of vertices in the associahedron in such a way that each edge of the associahedron is used only for a small fraction of these paths. The construction of these paths exploits a recursive decomposition of the associahedra into products of smaller associahedra, obtained by cutting out a central triangle of any triangulated polygon and using associahedra to describe the triangulations of the remaining pieces.</p>

<p>The same paper also includes a weaker bound on the <a href="https://11011110.github.io/blog/2006/10/08/happy-endings-for.html">triangulations of rectangular grids of points</a>. The convex polygons have a <a href="https://en.wikipedia.org/wiki/Catalan_number">Catalan number</a> of triangulations, but for grids, we can’t say as precisely how many triangulations there are. Whatever this number \(N\) is, we show that these state spaces have treewidth \(N^{1-o(1)}\), by using a fixed choice of diagonals to partition the grid into smaller grids, triangulating each smaller grid independently, and showing that there are many of these partitioned triangulations and that their product structure gives them high expansion. But this result is still somewhat unsatisfactory because it doesn’t consider all triangulations and because the \(N^{o(1)}\) part of the bound is not polynomial in the grid size. Do triangulations of grids have polynomial mixing time?</p>

<p>(<a href="https://mathstodon.xyz/@11011110/108689366093473745">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2022-07-21T22:23:00Z</updated>
    <published>2022-07-21T22:23:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-07-28T03:41:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/107</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/107" rel="alternate" type="text/html"/>
    <title>TR22-107 |  Fractional certificates for bounded functions | 

	Shachar Lovett, 

	Jiapeng Zhang</title>
    <summary>A folklore conjecture in quantum computing is that the acceptance probability of a quantum query algorithm can be approximated by a classical decision tree, with only a polynomial increase in the number of queries. Motivated by this conjecture, Aaronson and Ambainis (Theory of Computing, 2014) conjectured that this should hold more generally for any bounded function computed by a low degree polynomial.

In this work we prove two new results towards establishing this conjecture: first, that any such polynomial has a small fractional certificate complexity; and second, that many inputs have a small sensitive block. We also give two new conjectures that, if true, would imply the Aaronson and Ambainis conjecture given our results. 

On the technical side, many classical techniques used in the analysis of Boolean functions seem to fail when applied to bounded functions. Here, we develop a new technique, based on a mix of combinatorics, analysis and geometry, and which in part extends a recent technique of Knop et al. (STOC 2021) to bounded functions.</summary>
    <updated>2022-07-21T20:21:03Z</updated>
    <published>2022-07-21T20:21:03Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-29T00:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/106</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/106" rel="alternate" type="text/html"/>
    <title>TR22-106 |  On Hardness of Testing Equivalence to Sparse Polynomials Under Shifts | 

	Suryajith Chillara, 

	Coral Grichener, 

	Amir Shpilka</title>
    <summary>We say that two given polynomials $f, g \in R[x_1, \ldots, x_n]$, over a ring $R$, are equivalent under shifts if there exists a vector $(a_1, \ldots, a_n)\in R^n$ such that $f(x_1+a_1, \ldots, x_n+a_n) = g(x_1, \ldots, x_n)$. This is a special variant of the polynomial projection problem in Algebraic Complexity Theory.  Grigoriev and Karpinski (FOCS 1990), Lakshman and Saunders (SIAM J. Computing, 1995), and Grigoriev and Lakshman (ISSAC 1995) studied the problem of testing polynomial equivalence of a given polynomial to "any" $t$-sparse polynomial, over the rational numbers, and gave exponential time algorithms. In this paper, we provide  hardness results for this problem.
  
Formally, for a ring $R$, let $\mathrm{SparseShift}_R$ be the following decision problem -- Given a polynomial $P(X)$, is there a vector $a$ such that $P(X+a)$ contains fewer monomials than $P(X)$. We show that $\mathrm{SparseShift}_R$ is at least as hard as checking if a given system of polynomial equations over $R[x_1,\ldots, x_n]$ has a solution (Hilbert's Nullstellensatz).  

As a consequence of this reduction, we get the following results.
1. $\mathrm{SparseShift}_\mathbb{Z}$ is undecidable.
2. For any ring $R$ (which is not a field) such that $\mathrm{HN}_R$ is $\mathrm{NP}_R$-complete over the Blum-Shub-Smale model of computation, $\mathrm{SparseShift}_{R}$ is also $\mathrm{NP}_{R}$-complete.  In particular, $\mathrm{SparseShift}_{\mathbb{Z}}$ is also $\mathrm{NP}_{\mathbb{Z}}$-complete.


We also study the gap version of the $\mathrm{SparseShift}_R$ and show the following.  
1. For every function $\beta:\mathbb{N}\to\mathbb{R}_+$ such that $\beta\in o(1)$, $N^\beta$-gap-$\mathrm{SparseShift}_\mathbb{Z}$ is also undecidable (where $N$ is the input length).
2. For $R=\mathbb{F}_p, \mathbb{Q}, \mathbb{R}$ or $\mathbb{Z}_q$ and for every $\beta&gt;1$  the $\beta$-gap-$\mathrm{SparseShift}_R$ problem is $\mathrm{NP}$-hard. Furthermore, there exists a constant $\alpha&gt;1$ such that for every $d = O(1)$ in the sparse representation model, and for every $d\leq n^{O(1)}$ in the arithmetic circuit model, the $\alpha^d$-gap-$\mathrm{SparseShift}_R$  problem is  $\mathrm{NP}$-hard when given polynomials of degree at most $d$, in $O(nd)$ many variables, as input.</summary>
    <updated>2022-07-21T19:41:40Z</updated>
    <published>2022-07-21T19:41:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-29T00:20:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-798583255902437127</id>
    <link href="http://blog.computationalcomplexity.org/feeds/798583255902437127/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/what-is-known-about-that-sequence.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/798583255902437127" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/798583255902437127" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/what-is-known-about-that-sequence.html" rel="alternate" type="text/html"/>
    <title>What is known about that sequence</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> In my last post I wrote:</p><p><br/></p><p>---------------------------</p><div>Consider the recurrence</div><div><br/></div><div><br/></div><div>a_1=1</div><div><br/></div><div>for all n\ge 2, a_n = a_{n-1} + a_{n/2}.</div><div><br/></div><div>For which M does this recurrence have infinitely many n such that a_n \equiv  0 mod M?</div><div><br/></div><div><br/></div><div>I have written an open problems column on this for SIGACT News which also says</div><div>what is known (or at least what I know is known).  It will appear in the next issue.</div><div><br/></div><div>I will post that open problems column here on my next post.</div><div><br/>Until then  I would like you to work on it, untainted by what I know. </div><div>----------------------------------------</div><div><br/>I will now say what is known and point to the open problems column, co-authored with Emily Kaplitz and Erik Metz. </div><div><br/></div><div>If  M=2 or M=3 or M=5 or M=7 then there are infinitely many n such that a_n \equiv 0 mod M</div><div><br/>If M\equiv 0 mod 4 then there are no n such that a_n \equiv 0 mod M</div><div><br/></div><div>Empirical evidence suggests that</div><div><br/></div><div>If M \not\equiv 0 mod 4 then there are infinitely many n such that a_n\equiv 0 mod M</div><div><br/></div><div>That is our conjecture. Any progress would be good- for example proving it for M=9. M=11 might be easier since 11 is prime. </div><div><br/></div><div>The article that I submitted is <a href="https://www.cs.umd.edu/~gasarch/open/cseq.pdf">HERE</a></div></div>
    </content>
    <updated>2022-07-21T02:30:00Z</updated>
    <published>2022-07-21T02:30:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-07-28T17:47:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6576</id>
    <link href="https://scottaaronson.blog/?p=6576" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6576#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6576" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">A low-tech solution</title>
    <summary xml:lang="en-US">Thanks so much to everyone who offered help and support as this blog’s comment section endured the weirdest, most motivated and sophisticated troll attack in its 17-year history. For a week, a parade of self-assured commenters showed up to demand that I explain and defend my personal hygiene, private thoughts, sexual preferences, and behavior around […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Thanks so much to everyone who offered help and support as this blog’s comment section endured the weirdest, most motivated and sophisticated troll attack in its 17-year history.  For a week, a parade of self-assured commenters showed up to demand that I explain and defend my personal hygiene, private thoughts, sexual preferences, and behavior around female students (and, absurdly, to cajole me into taking my family on a specific Disney cruise ship).  In many cases, the troll or trolls <em>appropriated the names and email addresses of real academics</em>, imitating them so convincingly that those academics’ closest colleagues told me they were confident it was really them.  And when some trolls finally “outed” themselves, I had no way to know whether that was just another chapter in the trolling campaign.  It was enough to precipitate an epistemic crisis, where one actively doubts the authenticity of just about <em>every</em> piece of text.</p>



<p>The irony isn’t lost on me that I’ve endured this just as I’m <a href="https://scottaaronson.blog/?p=6484">starting my year-long gig at OpenAI</a>, to think, among other things, about the potential avenues for misuse of Large Language Models like GPT-3, and what theoretical computer science could contribute to mitigating them.  To say this episode has given me a more vivid understanding of the risks would be an understatement.</p>



<p><strong><em>But why didn’t I just block and ignore the trolls immediately?  Why did I bother engaging?</em></strong>  </p>



<p>At least a hundred people asked some variant of this question, and the answer is this.  For most of my professional life, this blog has been my forum, where anyone in the world could show up to raise any issue they wanted, as if we were tunic-wearing philosophers in the Athenian agora.  I prided myself on my refusal to take the coward’s way out and ignore anything—even, <em>especially</em>, severe personal criticism.  I’d witnessed how Jon Stewart, let’s say, would night after night completely eviscerate George W. Bush, his policies and worldview and way of speaking and justifications and lies, and then Bush would just continue the next day, totally oblivious, never deigning to rebut any of it.  And it became a core part of my identity that I’d never be like that.  If anyone on earth had a narrative of me where I was an arrogant bigot, a clueless idiot, etc., I’d confront that narrative head-on and refute it—or if I couldn’t, I’d reinvent my whole life.  What I’d <em>never</em> do is suffer anyone’s monstrous caricature of me to strut around the Internet unchallenged, as if conceding that only my academic prestige or tenure or power, rather than a reasoned rebuttal, could protect me from the harsh truths that the caricature revealed.</p>



<p>Over the years, of course, I carved out some exceptions: P=NP provers and quantum mechanics deniers enraged that I’d dismissed their world-changing insights.  Raving antisemites.  <em>Their</em> caricatures of me had no legs in any community I cared about.  But if an attack carried the implied backing of the whole modern social-justice movement, of thousands of angry grad students on Twitter, of <em>Slate</em> and <em>Salon</em> and <em>New York Times</em> writers and Wikipedia editors and university DEI offices, then the coward’s way out was closed.  The monstrous caricature then loomed directly over me; I could either parry his attacks or die.</p>



<p>With this stance, you might say, the astounding part is not that this blog’s “agora” model eventually broke down, but rather that it survived for so long!  I started blogging in October 2005.  It took until July 2022 for me to endure a full-scale “social/emotional denial of service attack” (not counting the comment-171 affair).  Now that I have, though, it’s obvious even to me that the old way is no longer tenable.</p>



<p>So what’s the solution?  Some of you liked the idea of requiring registration with real email addresses—but alas, when I tried to implement that, I found that WordPress’s registration system is a mess and I couldn’t see how to make it work.  Others liked the idea of moving to Substack, but others actively hated it, and in any case, even if I moved, I’d <em>still</em> have to figure out a comment policy!  Still others liked the idea of an army of volunteer moderators.  At least ten people volunteered themselves.</p>



<p>On reflection, the following strikes me as most directly addressing the actual problem.  I’m hereby establishing the <strong>Shtetl-Optimized Committee of Guardians</strong>, or SOCG (same acronym as the <a href="https://cse.buffalo.edu/socg21/socg.html">computational geometry conference</a> <img alt="&#x1F642;" class="wp-smiley" src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f642.png" style="height: 1em;"/> ).  If you’re interested in joining, shoot me an email, or leave a comment on this post with your (real!) email address.  I’ll accept members only if I know them in real life, personally or by reputation, or if they have an honorable history on this blog.</p>



<p>For now, the SOCG’s only job is this: whenever I get a comment that gives me a feeling of unease—because, e.g., it seems trollish or nasty or insincere, it asks a too-personal question, or it challenges me to rebut a hostile caricature of myself—I’ll email the comment to the SOCG and ask what to do.  I commit to respecting the verdict of those SOCG members who respond, whenever a clear verdict exists.  The verdict could be, e.g., “this seems fine,” “if you won’t be able to resist responding then don’t let this appear,” or “email the commenter first to confirm their identity.”  And if I simply need reassurance that the commenter’s view of me is false, I’ll seek it from the SOCG before I seek it from the whole world.</p>



<p>Here’s what SOCG members can expect in return: I continue pouring my heart into this subscription-free, ad-free blog, and I credit you for making it possible—publicly if you’re comfortable with your name being listed, privately if not.  I buy you a fancy lunch or dinner if we’re ever in the same town.</p>



<p>Eventually, we might move to a model where the SOCG members can log in to WordPress and directly moderate comments themselves.  But let’s try it this way first and see if it works.</p></div>
    </content>
    <updated>2022-07-19T18:27:35Z</updated>
    <published>2022-07-19T18:27:35Z</published>
    <category scheme="https://scottaaronson.blog" term="Announcements"/>
    <category scheme="https://scottaaronson.blog" term="Self-Referential"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-07-28T17:22:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4813311592001744524</id>
    <link href="http://blog.computationalcomplexity.org/feeds/4813311592001744524/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/an-open-question-about-sequence-mod-m.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4813311592001744524" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4813311592001744524" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/an-open-question-about-sequence-mod-m.html" rel="alternate" type="text/html"/>
    <title>An open question about a sequence mod M.</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In this post n/2 means floor{n/2}<div><br/></div><div>Consider the recurrence</div><div><br/></div><div><br/></div><div>a_1=1</div><div><br/></div><div>for all n\ge 2, a_n = a_{n-1} + a_{n/2}.</div><div><br/></div><div>For which M does this recurrence have infinitely many n such that a_n \equiv  0 mod M?</div><div><br/></div><div><br/></div><div>I have written an open problems column on this for SIGACT News which also says</div><div>what is known (or at least what I know is known).  It will appear in the next issue.</div><div><br/></div><div>I will post that open problems column here on my next post.</div><div><br/>Until then  I would like you to work on it, untainted by what I know. </div><div><br/></div><div>ADDED LATER: I have now posted the sequel which includes a pointer to the open problems column. To save you time, I post it <a href="https://www.cs.umd.edu/~gasarch/open/cseq.pdf">here</a> as well.</div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div></div>
    </content>
    <updated>2022-07-19T02:52:00Z</updated>
    <published>2022-07-19T02:52:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-07-28T17:47:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/105</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/105" rel="alternate" type="text/html"/>
    <title>TR22-105 |  On vanishing sums of roots of unity in polynomial calculus and sum-of-squares | 

	Ilario Bonacina, 

	Nicola Galesi, 

	Massimo Lauria</title>
    <summary>Vanishing sums of roots of unity can be seen as a natural generalization of knapsack from Boolean variables to variables taking values over the roots of unity. We show that these sums are hard to prove for polynomial calculus and for sum-of-squares, both in terms of degree and size.</summary>
    <updated>2022-07-18T13:04:40Z</updated>
    <published>2022-07-18T13:04:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-29T00:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/104</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/104" rel="alternate" type="text/html"/>
    <title>TR22-104 |  On One-Sided Testing Affine Subspaces | 

	Nader Bshouty</title>
    <summary>We study the query complexity of one-sided $\epsilon$-testing the class of Boolean functions $f:F^n\to \{0,1\}$ that describe affine subspaces and Boolean functions that describe axis-parallel affine subspaces, where $F$ is any finite field. We give a polynomial-time $\epsilon$-testers that ask $\tilde O(1/\epsilon)$ queries. This improves the query complexity $\tilde O(|F|/\epsilon)$ in~[16]. 

We then show that any one-sided $\epsilon$-tester with proximity parameter $\epsilon&lt;1/|F|^d$ for the class of Boolean functions that describe $(n-d)$-dimensional affine subspaces and Boolean functions that describe axis-parallel $(n-d)$-dimensional affine subspaces must make at least
$\Omega(1/\epsilon+|F|^{d-1}\log n)$ and $\Omega(1/\epsilon+|F|^{d-1}n)$ queries, respectively.
This improves the lower bound $\Omega(\log n/\log\log n)$  that is proved in~[16] for $F=GF(2)$. We also give  testers for those classes with query complexity that almost match the lower bounds.</summary>
    <updated>2022-07-18T12:43:42Z</updated>
    <published>2022-07-18T12:43:42Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-29T00:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=20239</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/07/18/complexity-2022/" rel="alternate" type="text/html"/>
    <title>Complexity 2022</title>
    <summary>Weaving patterns of proof and the accepted papers for this week’s conference her bio page Karen Donde is the Chair of Complexity 2022, which is being held this month in Knoxville, Tennessee. This is not the same as the Computational Complexity 2022 (CCC22) conference, which is being held in-person at the University of Pennsylvania this […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Weaving patterns of proof and the accepted papers for this week’s conference</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/07/18/complexity-2022/karendonde/" rel="attachment wp-att-20241"><img alt="" class="alignright wp-image-20241" height="159" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/07/KarenDonde.jpg?resize=151%2C159&amp;ssl=1" width="151"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://karendondehandwovens.com/page/1-Statement-Bio.html">her bio page</a></font></td>
</tr>
</tbody>
</table>
<p>
Karen Donde is the Chair of <a href="https://complexityexhibition.org/">Complexity 2022</a>, which is being held this month in Knoxville, Tennessee. This is not the same as the <a href="https://computationalcomplexity.org/Archive/2022/fullsite/">Computational Complexity 2022</a> (CCC22) conference, which is being held <b>in-person</b> at the University of Pennsylvania this <b>Wednesday, July 20</b>, through <b>Saturday, July 23</b>. The Knoxville event is not about computer science, nor dynamical nor biological complexity. It is about the art of weaving complex patterns in textiles by hand.</p>
<p>
Today we collect pointers to the papers at CCC22 after saying something separate about weaving and proofs.</p>
<p>
Unlike CCC22, the Knoxville exhibition is also <a href="https://complexityexhibition.org/all-works/">open online</a>. Here is a detail from the Complex Weaver first prize <a href="https://complexityexhibition.org/melanie-olde-morphology-i/">winner</a>, an example of three-dimensional weaving:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/07/18/complexity-2022/complexweavingprize/" rel="attachment wp-att-20242"><img alt="" class="aligncenter wp-image-20242" height="270" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/07/ComplexWeavingPrize.jpg?resize=250%2C270&amp;ssl=1" width="250"/></a></p>
<p>
Like CCC22, the Knoxville event has a program committee. It consists of <a href="http://www.juliehedges.co.uk/">Julie Hedges</a>, <a href="https://www.spadystudios.com/">Robyn Spady</a>, and <a href="https://www.bettyvera.com/">Betty Vera</a>. Also like CCC22, it has a steering committee. Besides Donde, the committee consists of <a href="https://www.etsy.com/shop/MargepyeTextiles?ref=profile_header">Margaret Dugger</a>, <a href="https://www.pinterest.com/dyen2weave/">Diane Smith</a>, <a href="https://wovenful.com/an-interview-with-sarah-fortin/">Sarah Fortin</a>, <a href="https://pikespeakweavers.org/member-galleries/nggallery/member-galleries/susan-bowman">Susan Bowman</a>, <a href="https://newworldtextiles.com/about/">Eileen Hallman</a>, <a href="https://www.woodlandsgallerync.com/artists/pat-brown">Pat Brown</a>, <a href="https://www.facebook.com/katiedoanhandweaver/">Katie Doan</a>, <a href="https://www.weavingschool.com/Geri.html">Geri Forkner</a>, <a href="https://www.linkedin.com/in/cathy-mccarthy-42347828/">Cathy McCarthy</a>, <a href="http://www.spinningforth.com/perso/perso.html">Ruth MacGregor</a>, and <a href="https://weavingspace.co.uk/#About-Weaving-Space">Cally Booker</a>. The gender imbalance is more extreme than for CCC22 or what we <a href="https://rjlipton.wpcomstaging.com/2021/11/13/popl-2022-et-tu-brute/">noted</a> last fall for POPL22 (also see end of <a href="https://rjlipton.wpcomstaging.com/2021/11/24/best-to-dean-mynatt/">this</a>). Oh well.</p>
<p>
</p><p/><h2> Weaving Into Theory </h2><p/>
<p/><p>
Karen Donde also writes a <a href="https://karen63615.wixsite.com/karendondeblog">blog</a>, <em>Speaking of Weaving</em>. The blog has numerous technical how-to articles. In some places they verge on mathematical theory. </p>
<p>
There are more express connections between mathematics and weaving. The mathematics teacher <a href="http://www.patrickhonner.com/about.html">Patrick Honner</a> has a <a href="https://mrhonner.com/weaving">page</a> of posts on weaving. He was featured in an <a href="https://naturalmath.com/2012/07/weaving-mathematics/">article</a> “Weaving your way through mathematics” by the mathematics educator Maria Droujkova. See also a <a href="https://www.youtube.com/watch?v=Breul3cnW9s">video</a> on weaving and the mathematics of <a href="https://en.wikipedia.org/wiki/Spirograph">Spirograph</a> patterns.</p>
<p>
On the computing theory side, connections to cellular automata are shown in a 2017 <a href="https://www.semanticscholar.org/paper/The-Complexity-of-Braids,-Cables,-and-Weaves-with-Holden/5b83e108623548c8b073a1e96b00d027eefb197e">paper</a> by Joshua Holden. There is also a recent <a href="https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445750">paper</a> from CMU’s Human-Computer Interaction Institute on “Enabling Personal Computational Handweaving with a Low-Cost Jacquard Loom.” </p>
<p>
</p><p/><h2> Weaving Into Proofs </h2><p/>
<p/><p>
Our association to weaving was really motivated, however, by an <a href="https://www.quantamagazine.org/how-do-mathematicians-know-their-proofs-are-correct-20220713/">article</a> last Wednesday by Steven Strogatz, a Cornell mathematician and extraordinary popularizer whom I (Ken) have known since we were undergraduates together at Princeton. The article interviews the Harvard mathematician Melanie Matchett Wood and is titled, “How Do Mathematicians Know Their Proofs Are Correct?”</p>
<p>
We have written about proofs and the social issue of verification <a href="https://rjlipton.wpcomstaging.com/2022/06/13/sorting-and-proving/">several</a> <a href="https://rjlipton.wpcomstaging.com/2022/05/23/hilberts-lost-problem/">times</a> <a href="https://rjlipton.wpcomstaging.com/2020/12/10/the-future-of-mathematics/">recently</a>. But this article goes into a more particular topic that we tried to get at in a <a href="https://rjlipton.wpcomstaging.com/2014/09/09/a-challenge-from-dyson/">series</a> of <a href="https://rjlipton.wpcomstaging.com/2014/06/06/is-this-a-proof-2/">posts</a> in <a href="https://rjlipton.wpcomstaging.com/2014/10/11/more-on-testing-dysons-conjecture/">2014</a>. This is about whether probabilistic modeling—not the <a href="https://en.wikipedia.org/wiki/Probabilistic_method">Probabilistic Method</a> which is airtight—can give confidence in conjectures that is tantamount to proof.</p>
<p>
Strogatz’s interview leads off with a reference to a 2019 <a href="https://www.quantamagazine.org/where-proof-evidence-and-imagination-intersect-in-math-20190314/">article</a> for <em>Quanta</em> titled, “Where Proof, Evidence, and Imagination Intersect.” That article is by the same Patrick Honner whom we just mentioned for weaving, and gives caveats of how bias and unrecognized implicit constraints can creep into models, so as to invalidate them. </p>
<p>
Wood begins with basic “coinflip” random models of primes—such as mentioned in the above-listed posts—and fixes on a bias-revealing model that we also covered <a href="https://rjlipton.wpcomstaging.com/2016/03/26/bias-in-the-primes/">here</a>. She then describes how they incrementally build rules for adjusting coin weights to compensate for biases introduced by small-number cases: </p>
<blockquote><p><b> </b> <em> “So the model is something that starts with this coin-flipping model, but then it’s modified by all these other rules, and all the other things that we know about the primes. And once you put all of those things that we do know into the model, you then ask [it] well, do you see, infinitely often, coins coming up prime just 2 apart? And the model tells you, oh, yes, we do see that. In fact, we see it at this very particular rate we can give you a formula for. And then … you see that the model gives you a very accurate prediction for the number of pairs of twin primes you’ll find as you go along. And so then you think, you know, maybe this model knows what it’s talking about.” </em>
</p></blockquote>
<p/><p>
In response to Strogatz noting that the accuracy must be judged by long computer runs, Wood is quick to emphasize that the rules given to the model are determined manually:</p>
<blockquote><p><b> </b> <em> “But for building this model and coming up with the formula that the model gives. You know, that’s done by hand, essentially, by mathematicians thinking about the model and figuring out with it. … [A]t some point, the computer stops. You know, there’s only so much computing power. But that formula that you would get, that the model would give you, that you could prove is true, again, about this model coin-flipping situation, that formula will keep going. You can put bigger and bigger numbers into that formula, much bigger than your computer could ever, ever compute with.” </em>
</p></blockquote>
<p>
</p><p/><h2> Proof of the Loom? </h2><p/>
<p/><p>
Wood goes on to describe <em>universality</em> in probability theory as signifying “that there are certain kinds of machines that if you put in a lot of random inputs, you can predict the output.” She gives as a bellwether example how the Central Limit Theorem creates such a universal machine for the bell curve. Regardless of an unknown distribution <em>D</em>, if you take means of samples from <em>D</em>, then the bell curve gives progressively—and provably—more accurate predictions of your outputs. Strogatz catches the warp and asks whether “somehow we’re getting the idea of universality to show up in number theory? Or am I dreaming?” Her peroration is:</p>
<blockquote><p><b> </b> <em> “[W]hat my collaborators and I work on is trying to make that kind of dream a reality so that, that some of these puzzling questions about numbers that we don’t know the answer to, maybe we could understand that there are patterns that come out, like a bell curve, like a normal distribution, that we can prove came out of the machine even if we don’t know what mysteries were put in.” </em>
</p></blockquote>
<p/><p>
So she is building a machine that takes rules as input—like Jacquard cards for a loom—and produces patterns that are analyzable. We use the computational success of the machine to judge how well universality has taken hold—as theoretically it must—and generate proofs from formulas based on the <em>a-priori</em> predicted outputs using the rules input thus far. </p>
<p>
This is like building a loom for weaving proofs—where, however, there is still the question of confidence in how well the patterns obtained match reality. Such doubt notwithstanding, the process may also augment non-linear ways of evaluating claimed proofs of the kind we discussed <a href="https://rjlipton.wpcomstaging.com/2020/06/13/proof-checking-not-line-by-line/">here</a> and recently debated <a href="https://rjlipton.wpcomstaging.com/2022/04/10/discussion-about-proving-again/">here</a>.</p>
<p>
</p><p/><h2> The Papers </h2><p/>
<p/><p>
The proofs in the accepted papers were, to be sure, evaluated by the standard expert social process. Here they are, lifted from the conference’s own program <a href="https://computationalcomplexity.org/Archive/2022/program.php">page</a>. Clicking on the time of the talk gives a pointer to the paper. </p>
<p>
<b>Wednesday, July 20</b></p>
<p>
<a href="https://arxiv.org/pdf/2205.10749.pdf">9:00</a> “Vanishing Spaces of Random Sets and Applications to Reed-Muller Codes.”<br/>
Siddharth Bhandari, Prahladh Harsha, Ramprasad Saptharishi, Srikanth Srinivasan</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16572/pdf/LIPIcs-CCC-2022-10.pdf">9:30</a> “New Near-Linear Time Decodable Codes Closer to the GV Bound.”<br/>
Guy Blanc and Dean Doron</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16576/pdf/LIPIcs-CCC-2022-14.pdf">10:00</a> “The plane test is a local tester for Multiplicity Codes.”<br/>
Dan Karliner, Roie Salama and Amnon Ta-Shma</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/025/">13:30</a> “Efficient Low-Space Simulations From the Failure of the Weak Pigeonhole Principle” (co-winner Best student paper).<br/>
Oliver Korten</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/023/">14:00</a> “Nisan-Wigderson generators in Proof Complexity: New lower bounds.”<br/>
Erfan Khaniki</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16577/pdf/LIPIcs-CCC-2022-15.pdf">14:30</a> “Pseudorandom Generators, Resolution and Heavy Width.”<br/>
Dmitry Sokolov</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16565/pdf/LIPIcs-CCC-2022-3.pdf">15:30</a> “Hitting Sets for Regular Branching Programs.”<br/>
Andrej Bogdanov, William Hoza, Gautam Prakriya and Edward Pyne</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/021/">16:00</a> “Improved Pseudorandom Generators for <img alt="{AC^0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAC%5E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Circuits” (co-winner Best student paper).<br/>
Xin Lyu</p>
<p>
<a href="https://arxiv.org/abs/2103.14134">16:40</a> “Random restrictions and PRGs for PTFs in Gaussian Space.”<br/>
Zander Kelley and Raghu Meka</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/024/">17:00</a> “Pseudorandomness of Expander Random Walks for Symmetric Functions and Permutation Branching Programs.”<br/>
Louis Golowich and Salil Vadhan</p>
<p>
<b>Thursday, July 21</b></p>
<p>
<a href="https://arxiv.org/abs/2111.02999">9:00</a> “Quantum search-to-decision reductions and the state synthesis problem.”<br/>
Sandy Irani, Anand Natarajan, Chinmay Nirkhe, Sujit Rao and Henry Yuen</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16590/pdf/LIPIcs-CCC-2022-28.pdf">9:30</a> “Influence in Completely Bounded Block-multilinear Forms and Classical Simulation of Quantum Algorithms.”<br/>
Nikhil Bansal, Makrand Sinha and Ronald de Wolf</p>
<p>
<a href="https://arxiv.org/abs/2111.10409">10:00</a> “The Acrobatics of BQP” (winner – Best paper award).<br/>
Scott Aaronson, Devon Ingram and William Kretschmer</p>
<p>
<a href="https://eprint.iacr.org/2021/513">13:30</a> “On One-way Functions from NP-Complete Problems.”<br/>
Yanyi Liu and Rafael Pass</p>
<p>
<a href="https://nanashima.github.io">14:00</a> “Finding Errorless Pessiland in Error-Prone Heuristica.”<br/>
Shuichi Hirahara and Mikito Nanashima</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/084/">14:30</a> “Characterizing Derandomization Through Fine-Grained Hardness of Levin-Kolmogorov Complexity.”<br/>
Yanyi Liu and Rafael Pass</p>
<p>
<a href="https://www.researchgate.net/publication/356891307_Almost_Polynomial_Factor_Inapproximability_for_Parameterized_k-Clique">15:30</a> “Almost Polynomial Factor Inapproximability for Parameterized k-Clique.”<br/>
Karthik C. S. and Subhash Khot</p>
<p>
<a href="https://arxiv.org/abs/2106.12710">16:00</a> “Certifying solution geometry in random CSPs: counts, clusters and balance.”<br/>
Jun-Ting Hsieh, Sidhanth Mohanty and Jeff Xu</p>
<p>
<a href="https://arxiv.org/abs/2203.03705">16:40</a> “High-Dimensional Expanders from Chevalley Groups.”<br/>
Ryan O’Donnell and Kevin Pratt</p>
<p>
<a href="https://arxiv.org/abs/2205.02374">17:10</a> “The composition complexity of majority.”<br/>
Victor Lecomte, Prasanna Ramakrishnan and Li-Yang Tan</p>
<p>
<b>Friday, July 22</b></p>
<p>
<a href="https://arxiv.org/abs/2004.14318">9:00</a> “The Approximate Degree of Bipartite Perfect Matching.”<br/>
Gal Beniamini</p>
<p>
<a href="https://arxiv.org/abs/2205.06249">9:30</a> “Optimal-Degree Polynomial Approximations for Exponentials and Gaussian Kernel Density Estimation.”<br/>
Amol Aggarwal and Josh Alman</p>
<p>
<a href="https://arxiv.org/abs/2108.13578">10:00</a> “<img alt="{\ell_p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-Spread and Restricted Isometry Properties of Sparse Random Matrices.”<br/>
Venkatesan Guruswami, Peter Manohar and Jonathan Mosheiff</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/074/">13:30</a> “On Randomized Reductions to the Random Strings.”<br/>
Michael Saks and Rahul Santhanam</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/086/">14:00</a> “Extremely Efficient Constructions of Hash Functions, with Applications to Hardness Magnification and PRFs.”<br/>
Lijie Chen, Jiatu Li and Tianqi Yang</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16575/pdf/LIPIcs-CCC-2022-13.pdf">14:30</a> “A better-than-3log(n) depth lower bound for De Morgan formulas with restrictions on top gates.”<br/>
Ivan Mihajlin and Anastasia Sofronova</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/072/">15:30</a> “Probabilistic Kolmogorov Complexity with Applications to Average-Case Complexity.”<br/>
Halley Goldberg, Valentine Kabanets, Zhenjian Lu and Igor C. Oliveira</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16588/pdf/LIPIcs-CCC-2022-26.pdf">16:00</a> “Symmetry of Information from Meta-Complexity.”<br/>
Shuichi Hirahara</p>
<p>
<a href="https://arxiv.org/abs/2201.08895">16:30</a> “On the Satisfaction Probability of k-CNF Formulas.”<br/>
Till Tantau</p>
<p>
<b>Saturday, July 23</b></p>
<p>
<a href="https://arxiv.org/abs/2202.09883">9:00</a> “On Efficient Noncommutative Polynomial Factorization via Higman Linearization.”<br/>
Vikraman Arvind and Pushkar Joglekar</p>
<p>
<a href="https://www.researchgate.net/publication/360332836_Improved_Low-Depth_Set-Multilinear_Circuit_Lower_Bounds">9:30</a> “Improved Low-Depth Set-Multilinear Circuit Lower Bounds.”<br/>
Deepanshu Kush and Shubhangi Saraf</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16594/pdf/LIPIcs-CCC-2022-32.pdf">10:15</a> “On the Partial Derivative Method Applied to Lopsided Set-Multilinear Polynomials.”<br/>
Nutan Limaye, Srikanth Srinivasan and Sebastien Tavenas</p>
<p>
<a href="https://arxiv.org/abs/2205.15168">10:45</a> “Subrank and Optimal Reduction of Scalar Multiplications to Generic Tensors.”<br/>
Harm Derksen, Visu Makam and Jeroen Zuiddam</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/026/">11:00</a> “Trading Time and Space in Catalytic Branching Programs.”<br/>
Ian Mertz and James Cook</p>
<p>
<a href="https://arxiv.org/abs/2201.10997">12:30</a> “Linear Branching Programs and Directional Affine Extractors.”<br/>
Svyatoslav Gryaznov, Pavel Pudlak and Navid Talebanfard</p>
<p>
<a href="https://www.cs.mcgill.ca/~robere/research.html">14:00</a> “Further collapses in TFNP.”<br/>
Mika Goos, Alexandros Hollender, Siddhartha Jain, Gilbert Maystre, William Pires, Robert Robere and Ran Tao</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16592/pdf/LIPIcs-CCC-2022-30.pdf">14:30</a> “Interactive Oracle Proofs of Proximity to Algebraic Geometry Codes.”<br/>
Sarah Bordage, Mathieu Lhotel, Jade Nardi and Hugues Randriam</p>
<p>
<a href="https://eprint.iacr.org/2022/168">15:00</a> “Hardness of Approximation for Stochastic Problems via Interactive Oracle Proofs.”<br/>
Gal Arnon, Alessandro Chiesa and Eylon Yogev</p>
<p>
We congratulate all the authors of the accepted papers.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Can we really build mathematical looms for helping us generate proofs at high level?</p></font></font></div>
    </content>
    <updated>2022-07-18T05:34:57Z</updated>
    <published>2022-07-18T05:34:57Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="primes"/>
    <category term="Proofs"/>
    <category term="CCC22"/>
    <category term="Computational Complexity Conference"/>
    <category term="computer proofs"/>
    <category term="Karen Donde"/>
    <category term="Melanie Matchett Wood"/>
    <category term="Patrick Honner"/>
    <category term="probabilistic proofs"/>
    <category term="Steven Strogatz"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-07-29T00:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/07/17/full-professorships-and-tenure-track-professorships-at-ruhr-university-bochum-germany-apply-by-july-29-2022/</id>
    <link href="https://cstheory-jobs.org/2022/07/17/full-professorships-and-tenure-track-professorships-at-ruhr-university-bochum-germany-apply-by-july-29-2022/" rel="alternate" type="text/html"/>
    <title>Full Professorships and Tenure-Track Professorships at Ruhr-University Bochum, Germany (apply by July 29, 2022)</title>
    <summary>We invite outstanding applicants from all areas of computer science, including the Foundations of Computer Science. Salary and working conditions are internationally very competitive and come with civil servant status. Full professorships are chair positions with administrative staff. We offer dual-career &amp; relocation support and a family-friendly environment. Knowledge of German is not required. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We invite outstanding applicants from all areas of computer science, including the Foundations of Computer Science. Salary and working conditions are internationally very competitive and come with civil servant status. Full professorships are chair positions with administrative staff. We offer dual-career &amp; relocation support and a family-friendly environment. Knowledge of German is not required.</p>
<p>Website: <a href="https://informatik.rub.de/en/news/openings-professorships-in-computer-science-w3-and-w2-tenure-track-w3/">https://informatik.rub.de/en/news/openings-professorships-in-computer-science-w3-and-w2-tenure-track-w3/</a><br/>
Email: career@casa.rub.de</p></div>
    </content>
    <updated>2022-07-17T10:15:42Z</updated>
    <published>2022-07-17T10:15:42Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-07-29T00:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/103</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/103" rel="alternate" type="text/html"/>
    <title>TR22-103 |  Almost Chor--Goldreich Sources and Adversarial Random Walks | 

	Dean Doron, 

	Dana Moshkovitz, 

	Justin Oh, 

	David Zuckerman</title>
    <summary>A Chor--Goldreich (CG) source [CG88] is a sequence of random variables  $X = X_1 \circ \ldots \circ X_t$, each $X_i \sim \{0,1 \{^d$, such that each $X_i$ has $\delta d$ min-entropy for some constant $\delta &gt; 0$, even conditioned on any fixing of $X_1 \circ \ldots \circ X_{i-1}$. We typically think of $d$ as constant. We extend this notion in several ways, and most notably allow each $X_i$ to be only $\gamma$-close to having $\delta d$ min entropy.

Studying such almost CG sources allows us to achieve pseudorandomness results which were not known to hold even for standard CG sources, and even for the weaker model of Santha--Vazirani sources [SV86]. We construct a deterministic condenser that on input $X$, outputs a distribution which is close to having constant entropy gap, namely a distribution $Z \sim \{0,1 \}^m$ for $m \approx \delta dt$ with min-entropy $m-O(1)$. 

Our new primitive readily implies fast simulation results:

*	We can simulate $\mathbf{BPP}$ using almost CG sources with constant multiplicative slowdown.
*	When the randomized algorithm has small failure probability, we can simulate it using almost CG sources with no multiplicative slowdown. This result extends to randomized protocols as well, and any setting in which we cannot simply cycle over all seeds, and a ``one-shot'' simulation is needed.

Moreover, our framework is flexible enough to work even when the $X_i$-s only have Shannon entropy rather than min-entropy, and in some cases, even when a few $X_i$-s are completely damaged.

Our main technical contribution is a novel analysis of random walks which may be of independent interest. We analyze walks with adversarially correlated steps, each step being entropy-deficient, on good enough lossless expanders. We prove that such walks (or certain interleaved walks on two expanders), starting from a fixed vertex and walking according to $X_1\circ \ldots \circ X_t$, accumulate most of the entropy in $X$.</summary>
    <updated>2022-07-15T19:42:52Z</updated>
    <published>2022-07-15T19:42:52Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-29T00:20:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/07/15/linkage</id>
    <link href="https://11011110.github.io/blog/2022/07/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Creating weaving patterns from subdivision schemes (\(\mathbb{M}\)), new paper by Lipschütz, Reitebuch, Skrodzi, and Polthier, and explanatory thread by Skrodzi.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://doi.org/10.1080/17513472.2022.2069417">Creating weaving patterns from subdivision schemes</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@msmathcomputer/108561058326179122">\(\mathbb{M}\)</a>),</span> new paper by Lipschütz, Reitebuch, Skrodzi, and Polthier, and explanatory thread by Skrodzi.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@peterrowlett/108578769727211199">How would you make a sphere from three circles?</a>, asks Peter Rowlett after his son said he could do it.</p>
  </li>
  <li>
    <p><a href="http://blog.computationalcomplexity.org/2022/06/a-gadget-for-3-colorings.html">Counting 3-colorings</a> and <a href="https://blog.computationalcomplexity.org/2022/06/counting-number-of-3-colorings-of-graph">follow-up post</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108584298969286802">\(\mathbb{M}\)</a>).</span> Like all “natural” \(\mathsf{NP}\)-complete problems (and many easier problems), the 3-coloring problem should have a \(\#\mathsf{P}\)-complete counting version, but the gadgets needed to prove it are a little subtle and tracking down the history of proof of this result took some effort.</p>
  </li>
  <li>
    <p><a href="https://youtu.be/tH6vLXMaCwQ">Polyhedra in which all but one edge have a right angle</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@henryseg/108584342158901821">\(\mathbb{M}\)</a>),</span> 3d-printed based on a construction used by Sydler to study Dehn invariants. Achieving this property leads to surprisingly complicated polyhedra.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/tag/2022-fields-and-abacus-medals/">The 2022 Fields medals</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@mathcination/108595654085200673">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://11011110.github.io/blog/2020/07/31/linkage.html">Two years ago</a> I linked to <a href="https://cp4space.wordpress.com/2020/07/25/rational-dodecahedron-inscribed-in-unit-sphere/">a post by Adam Goucher</a>, solving <a href="https://mathoverflow.net/q/234212/440">an old MathOverflow question</a> by showing that it is possible to find a dodecahedron, combinatorially equivalent to a regular one, with rational coordinates, inscribed in a unit sphere. But <a href="https://cp4space.hatsya.com/2022/06/20/infinitely-many-rational-dodecahedra/">now there are infinitely many</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108602499475969576">\(\mathbb{M}\)</a>)!</span> Some messy algebra, and then some work with elliptic curve group operations, eventually simplifies down to a parametric family with a dodecahedron for each integer right triangle.</p>
  </li>
  <li>
    <p>For integer \(A\), a grid of  \(n\) points has roughly \(n^2\sigma(A)/A\) area-\(A\) triangles, where \(\sigma\) is the sum of divisors <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108606089910227326">\(\mathbb{M}\)</a>);</span> see <a href="https://users.renyi.hu/~p_erdos/1971-20.pdf">Erdős &amp; Purdy 1971</a> who used non-square grids and factorial \(A\) to find points with \(\Omega(n\log\log n)\) unit-area triangles. So how big is \(\sigma(A)/A\)? <a href="https://en.wikipedia.org/wiki/Divisor_function#Robin's_theorem">It depends on the Riemann hypothesis!</a> If RH is true, at most \(e^\gamma\log\log A\) for \(A&gt;5040\). If not, slightly larger infinitely often.</p>
  </li>
  <li>
    <p><a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-ICGT-22.pdf">Slides from my talk on “Graphs in Nature”</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108611451435162895">\(\mathbb{M}\)</a>)</span> at the International Colloquium on Graph Theory and Combinatorics in Montpellier, France.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Prince_Rupert%27s_cube">Prince Rupert’s cube</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108623867716491741">\(\mathbb{M}\)</a>):</span> a cube can fit through a square hole drilled through another cube its size, or even slightly smaller. Now a Good Article on Wikipedia. I’ve been wondering: is it possible to make Prince Rupert’s Borromean rings, by drilling square holes into three unit cubes, each simultaneously passing through the hole in the next one?</p>
  </li>
  <li>
    <p>On the CSTheory stackexchange, Alexey Milovanov asks for updates on the (as far as I know still unknown) complexity of an old problem, <a href="https://cstheory.stackexchange.com/q/51680/95">finding shortest addition chains</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108627574398080196">\(\mathbb{M}\)</a>).</span> This highlights something I love about editing Wikipedia: if you take the effort to track down a repeated error in the literature, and <a href="https://en.wikipedia.org/wiki/Special:Diff/206806087">document it properly in the right Wikipedia article</a>, then maybe 14 years later the correction rather than the error can be common knowledge.</p>
  </li>
  <li>
    <p><a href="https://www.maa.org/press/maa-reviews/pop-up-geometry">The MAA reviews Joe O’Rourke’s new book, <em>Pop-Up Geometry: The Mathematics Behind Pop-Up Cards</em></a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108632717277132574">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2207.04923">Killing a vortex</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108637362908341033">\(\mathbb{M}\)</a>),</span> by Thilikos and Wiederrecht. Robertson and Seymour’s structural decomposition of minor-closed graph families glues together surface-embedded graphs, a few arbitrarily-connected “apex” vertices, and “vortices”, bounded-pathwidth graphs attached to faces. For graph matching, vortices are problematic. This new preprint describes the families that don’t need them and shows that they are exactly the ones whose matchings can be counted quickly.</p>
  </li>
  <li>
    <p>Scott Aaronson, quantum complexity theorist and debunker of quantum hype on <a href="https://scottaaronson.blog/">his blog</a>, is also a <a href="https://scottaaronson.blog/?p=6552">target of trolls who have pushed him to back down from his free-speech principles and restrict comments</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108641124975905102">\(\mathbb{M}\)</a>).</span> <a href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/">Boaz Barak gives some support</a>. Via Boaz I re-found Scott’s 2005 “<a href="https://arxiv.org/abs/quant-ph/0502072">NP-complete Problems and Physical Reality</a>” debunking soap bubble and rubber band solvers for hard optimization problems. Worth a re-read!</p>
  </li>
  <li>
    <p>I tend to pick technologically better solutions over popular ones, despite popularity’s importance for long-term viability <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108649887665081093">\(\mathbb{M}\)</a>).</span> Which is why I just switched from a gas range to induction. It has all the responsiveness of gas (vs the glacial response of conventional electric), is more efficient, has a smaller carbon footprint, fewer noxious emissions, etc. These are still uncommon in Southern California, but new laws require electric appliances for new construction, and I hope that with familiarity they will become better liked as well.</p>
  </li>
  <li>
    <p><a href="https://www.surgehq.ai//blog/30-percent-of-googles-reddit-emotions-dataset-is-mislabeled">Seriously bad data in Google’s GoEmotions dataset</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108653832818847808">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=32090389">via</a>), some 58K reddit comments categorized by affect. Opinions in the post and comments vary on why the categorization was so inaccurate, including lack of context, farming it out to poorly-paid workers in countries less likely to be familiar with the specific idioms used in the comments, or maybe just that it’s a hard problem.</p>
  </li>
</ul></div>
    </content>
    <updated>2022-07-15T16:57:00Z</updated>
    <published>2022-07-15T16:57:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-07-28T03:41:14Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2022/07/15/imp-reg-htf-cnn/</id>
    <link href="http://offconvex.github.io/2022/07/15/imp-reg-htf-cnn/" rel="alternate" type="text/html"/>
    <title>Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Networks</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The ability of large neural networks to generalize is commonly believed to stem from an implicit regularization — a tendency of gradient-based optimization towards predictors of low complexity.
A lot of effort has gone into theoretically formalizing this intuition.
Tackling modern neural networks head-on can be quite difficult, so existing analyses often focus on simplified models as stepping stones.
Among these, matrix and tensor factorizations have attracted significant attention due to their correspondence to linear neural networks and certain shallow non-linear convolutional networks, respectively. 
Specifically, they were shown to exhibit an implicit tendency towards low matrix and tensor ranks, respectively.</p>

<p>This post overviews a recent <a href="https://arxiv.org/abs/2201.11729">ICML 2022 paper</a> with <a href="https://asafmaman101.github.io/">Asaf Maman</a> and <a href="http://www.cohennadav.com/">Nadav Cohen</a>, in which we draw closer to practical deep learning by analyzing <em>hierarchical tensor factorization</em>, a model equivalent to certain <em>deep non-linear</em> convolutional networks. 
We find that, analogously to matrix and tensor factorizations, the implicit regularization in hierarchical tensor factorization strives to lower a notion of rank (called hierarchical tensor rank).
This turns out to have surprising implications on the origin of locality in convolutional networks, inspiring a practical method (explicit regularization scheme) for improving their performance on tasks with long-range dependencies.</p>

<h2 id="background-matrix-and-tensor-factorizations">Background: Matrix and Tensor Factorizations</h2>

<p>To put our work into context, let us briefly go over existing dynamical characterizations of implicit regularization in matrix and tensor factorizations.
In both cases they suggest an incremental learning process that leads to low rank solutions (for respective notions of rank). We will then see how these characterizations transfer to the considerably richer hierarchical tensor factorization.</p>

<h3 id="matrix-factorization-incremental-matrix-rank-learning">Matrix factorization: Incremental matrix rank learning</h3>
<p><em>Matrix factorization</em> is arguably the most extensively studied model in the context of implicit regularization. 
Indeed, it was already discussed in four previous posts (<a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">1</a>, <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">2</a>, <a href="http://www.offconvex.org/2019/07/10/trajectories-linear-nets/">3</a>, <a href="http://www.offconvex.org/2019/06/03/trajectories/">4</a>), but for completeness we will present it once more. 
Consider the task of minimizing a loss $\mathcal{L}_M : \mathbb{R}^{D, D’} \to \mathbb{R}$ over matrices, e.g. $\mathcal{L}_M$ can be a matrix completion loss — mean squared error over observed entries from some ground truth matrix. 
Matrix factorization refers to parameterizing the solution $W_M \in \mathbb{R}^{D, D’}$ as a product of $L$ matrices, and minimizing the resulting objective using <em>gradient descent (GD)</em>:</p>
<div style="text-align: center;">
\[
    \min\nolimits_{W^{(1)}, \ldots, W^{(L)}} \mathcal{L}_M \big ( W_M \big ) := \mathcal{L}_M \big ( W^{(1)} \cdots W^{(L)} \big ) ~.
\]
</div>
<p>Essentially, matrix factorization amounts to applying a linear neural network (fully connected neural network with no non-linearity) for minimizing $\mathcal{L}_M$. 
We can explicitly constrain the matrix rank of $W_M$ by limiting the shared dimensions of the weight matrices $\{ W^{(l)} \}_l$. However, from an implicit regularization standpoint, the most interesting case is where rank is unconstrained. 
In this case there is no explicit regularization, and the kind of solution we get is determined implicitly by the parameterization and the optimization algorithm.</p>

<p>Although it was initially conjectured that GD (with small initialization and step size) over matrix factorization minimizes a norm (see the seminal work of <a href="https://arxiv.org/abs/1705.09280">Gunasekar et al. 2017</a>), recent evidence points towards an implicit matrix rank minimization (see <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a>; <a href="https://arxiv.org/abs/1904.13262">Gidel et al. 2019</a>; <a href="https://arxiv.org/abs/2005.06398">Razin &amp; Cohen 2020</a>; <a href="https://arxiv.org/abs/2011.13772">Chou et al. 2020</a>; <a href="https://arxiv.org/abs/2012.09839">Li et al. 2021</a>).
In particular, <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a> characterized the dynamics of $W_M$’s singular values throughout optimization:</p>

<blockquote>
  <p><strong>Theorem (informal; <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a>):</strong>
Gradient flow (GD with infinitesimal step size) over matrix factorization initialized near zero leads the $r$’th singular value of $W_M$, denoted $\sigma_M^{(r)} (t)$, to evolve by:
[ 
    \color{brown}{\frac{d}{dt} \sigma_M^{(r)} (t) \propto \sigma_M^{(r)} (t)^{2 - 2/L}} ~.
]</p>
</blockquote>

<p>As can be seen from the theorem above, singular values evolve at a rate proportional to their size exponentiated by $2 - 2 / L$. This means that they are subject to a momentum-like effect, by which they move slower when small and faster when large. 
When initializing near the origin (as commonly done in practice), we therefore expect singular values to progress slowly at first, and then, upon reaching a certain threshold, to quickly rise until convergence. 
<strong>These dynamics create an incremental learning process that promotes solutions with few large singular values and many small ones, i.e. low matrix rank solutions</strong>.
In their paper, <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a> support this qualitative explanation through theoretical illustrations and empirical evaluations. 
For example, the following plot reproduces one of their experiments:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/mf_dyn_blog.png" style="width: 380px; padding-bottom: 0px; padding-top: 0px;"/>
<br/>
<i><b>Figure 1:</b> 
Dynamics of singular values during GD over matrix factorization <br/> — incremental learning leads to low matrix rank.
</i>
</div>
<p><br/>
We note that the incremental matrix rank learning phenomenon was later on used to prove exact matrix rank minimization, under certain technical conditions (<a href="https://arxiv.org/abs/2012.09839">Li et al. 2021</a>).</p>

<h3 id="tensor-factorization-incremental-tensor-rank-learning">Tensor factorization: Incremental tensor rank learning</h3>

<p>Despite the significant interest in matrix factorization, as a theoretical surrogate for deep learning its practical relevance is rather limited. 
It corresponds to linear neural networks, and thus misses non-linearity — a crucial aspect of modern neural networks.
As was mentioned in a <a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">previous post</a>, by moving from matrix (two-dimensional array) to tensor (multi-dimensional array) factorizations it is possible to address this limitation.</p>

<p>A classical scheme for factorizing tensors, named CANDECOMP/PARAFAC (CP), parameterizes a tensor as a sum of outer products (for more details on this scheme, see <a href="http://www.kolda.net/publication/TensorReview.pdf">this excellent survey</a>).
Given a loss $\mathcal{L}_T : \mathbb{R}^{D_1, \ldots, D_N} \to \mathbb{R}$ over $N$-dimensional tensors, e.g. $\mathcal{L}_T$ can be a tensor completion loss, we simply refer by <em>tensor factorization</em> to parameterizing the solution $\mathcal{W}_T \in \mathbb{R}^{D_1, \ldots, D_N}$ as a CP factorization, and minimizing the resulting objective via GD:</p>
<div style="text-align: center;">
\[
    \min\nolimits_{ \{ \mathbf{w}_r^n \}_{r , n} } \mathcal{L}_T \big ( \mathcal{W}_T \big ) := \mathcal{L}_T \big (  {\textstyle \sum}_{r = 1}^R \mathbf{w}_r^1 \otimes \cdots \otimes \mathbf{w}_r^N \big) ~.
\]
</div>
<p>Each term $\mathbf{w}_r^{(1)} \otimes \cdots \otimes \mathbf{w}_r^{(N)}$ in the sum is called a <em>component</em>, and $\otimes$ stands for outer product.
The concept of rank naturally extends from matrices to tensors.
For a given tensor $\mathcal{W}$, its <em>tensor rank</em> is defined to be the minimal number of components (i.e. of outer product summands) $R$ required for CP parameterization to express it.
Note that we can explicitly constrain the tensor rank of $\mathcal{W}_T$ by limiting the number of components $R$.
But, since our interest lies in implicit regularization, we consider the case where $R$ is large enough for any tensor to be expressed.</p>

<p>Similarly to how matrix factorization captures linear neural networks, tensor factorization is equivalent to certain <em>shallow non-linear</em> convolutional networks (with multiplicative non-linearity).
This equivalence was discussed in a couple of previous posts (<a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">1</a>, <a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">2</a>), for the exact details behind it feel free to check out the preliminaries section of <a href="https://arxiv.org/abs/2201.11729">our paper</a> and references therein.
The bottom line is that tensor factorization takes us one step closer to practical neural networks.</p>

<p>Motivated by the incremental learning dynamics in matrix factorization, in a <a href="https://arxiv.org/abs/2102.09972">previous paper</a> (see accompanying <a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">blog post</a>) we analyzed the behavior of component norms during optimization of tensor factorization:</p>

<blockquote>
  <p><strong>Theorem (informal; <a href="https://arxiv.org/abs/2102.09972">Razin et al. 2021</a>):</strong>
Gradient flow over tensor factorization initialized near zero leads the $r$’th component norm, $\sigma_T^{(r)} (t) := || \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) ||$, to evolve by:
[ 
    \color{brown}{\frac{d}{dt} \sigma_T^{(r)} (t) \propto \sigma_T^{(r)} (t)^{2 - 2/N}} ~.
]</p>
</blockquote>

<p>The dynamics of component norms in tensor factorization are structurally identical to those of singular values in matrix factorization.
Accordingly, we get a momentum-like effect that attenuates the movement of small component norms and accelerates that of large ones.
This suggests that, <strong>in analogy with matrix factorization, when initializing near zero components tend to be learned incrementally, resulting in a bias towards low tensor rank</strong>.
The following plot empirically demonstrates this phenomenon:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/tf_dyn_blog.png" style="width: 380px; padding-bottom: 0px; padding-top: 0px;"/>
<br/>
<i><b>Figure 2:</b> 
Dynamics of component norms during GD over tensor factorization <br/> — incremental learning leads to low tensor rank.
</i>
</div>
<p><br/>
Continuing with the analogy to matrix factorization, the incremental tensor rank learning phenomenon formed the basis for proving exact tensor rank minimization, under certain technical conditions (<a href="https://arxiv.org/abs/2102.09972">Razin et al. 2021</a>).</p>

<h2 id="hierarchical-tensor-factorization">Hierarchical Tensor Factorization</h2>

<p>Tensor factorization took us beyond linear predictors, yet it still lacks a critical feature of modern neural networks — depth (recall that it corresponds to <em>shallow</em> non-linear convolutional networks).
A natural extension that accounts for both non-linearity and depth is <em>hierarchical tensor factorization</em> — our protagonist — which corresponds to certain <em>deep</em> non-linear convolutional networks (with multiplicative non-linearity).
This equivalence is actually not new, and has facilitated numerous analyses of expressive power in deep learning (see <a href="https://arxiv.org/abs/1705.02302">this survey</a> for a high-level overview).</p>

<p>As opposed to tensor factorization, which is a simple construct dating back to at least the early 20’th century (<a href="https://onlinelibrary.wiley.com/doi/10.1002/sapm192761164">Hitchcock 1927</a>), hierarchical tensor factorization was formally introduced only recently (<a href="https://link.springer.com/article/10.1007/s00041-009-9094-9">Hackbusch &amp; Kuhn 2009</a>), and is much more elaborate.
Its exact definition is rather technical (the interested reader can find it in <a href="https://arxiv.org/abs/2201.11729">our paper</a>).
For our current purpose it suffices to know that a hierarchical tensor factorization consists of multiple local tensor factorizations, whose components we call the <em>local components</em> of the hierarchical factorization.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/tf_htf_cnn_blog.png" style="width: 900px; padding-bottom: 15px; padding-top: 10px;"/>
<br/>
<i><b>Figure 3:</b> 
Tensor factorization, which is a sum of components (outer products), <br/> corresponds to a shallow non-linear convolutional neural network (CNN).
<br/> Hierarchical tensor factorization, which consists of multiple local tensor <br/> factorizations, corresponds to a deep non-linear CNN.
</i>
</div>
<p><br/>
In contrast to matrices, which have a single standard definition for rank, tensors posses several different definitions for rank.
Hierarchical tensor factorizations induce their own such notion, known as <em>hierarchical tensor rank</em>.
Basically, if a tensor can be represented through hierarchical tensor factorization with few local components, then it has low hierarchical tensor rank.
This stands in direct analogy with tensor rank, which is low if the tensor can be represented through tensor factorization with few components.</p>

<p>Seeing that the implicit regularization in matrix and tensor factorizations leads to low matrix and tensor ranks, respectively, in <a href="https://arxiv.org/abs/2201.11729">our paper</a> we investigated whether the implicit regularization in hierarchical tensor factorization leads to low hierarchical tensor rank. 
That is, whether GD (with small initialization and step size) over hierarchical tensor factorization learns solutions that can be represented with few local components.
Turns out it does.</p>

<h2 id="dynamical-analysis-incremental-hierarchical-tensor-rank-learning">Dynamical Analysis: Incremental Hierarchical Tensor Rank Learning</h2>

<p>At the heart of our analysis is the following dynamical characterization for local component norms during optimization of hierarchical tensor factorization:</p>

<blockquote>
  <p><strong>Theorem (informal):</strong>
Gradient flow over hierarchical tensor factorization initialized near zero leads the $r$’th local component norm in a local tensor factorization, denoted $\sigma_H^{(r)} (t)$, to evolve by:
[ 
    \color{brown}{\frac{d}{dt} \sigma_H^{(r)} (t) \propto \sigma_H^{(r)} (t)^{2 - 2/K}} ~,
]
where $K$ is the number of axes of the local tensor factorization.</p>
</blockquote>

<p>This should really feel like deja vu, as these <strong>dynamics are structurally identical to those of singular values in matrix factorization and component norms in tensor factorization!</strong>
Again, we have a momentum-like effect, by which local component norms move slower when small and faster when large.
As a result, <strong>when initializing near zero local components tend to be learned incrementally, yielding a bias towards low hierarchical tensor rank</strong>.
In <a href="https://arxiv.org/abs/2201.11729">the paper</a> we provide theoretical and empirical demonstrations of this phenomenon.
For example, the following plot shows the evolution of local component norms at some local tensor factorization under GD:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/htf_dyn_blog.png" style="width: 380px; padding-bottom: 0px; padding-top: 0px;"/>
<br/>
<i><b>Figure 4:</b> 
Dynamics of local component norms during GD over hierarchical <br/> tensor factorization — incremental learning leads to low hierarchical tensor rank.
</i>
</div>
<p><br/></p>

<h2 id="practical-implication-countering-locality-in-convolutional-networks-via-explicit-regularization">Practical Implication: Countering Locality in Convolutional Networks via Explicit Regularization</h2>

<p>We saw that in hierarchical tensor factorization GD leads to solutions of low hierarchical tensor rank.
But what does this even mean for the associated convolutional networks?</p>

<p>Hierarchical tensor rank is known (<a href="https://arxiv.org/abs/1605.06743">Cohen &amp; Shashua 2017</a>) to measure the strength of long-range dependencies modeled by a network.
In the context of image classification, e.g., it quantifies how well we take into account dependencies between distant patches of pixels.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/local_vs_non_local_dep.png" style="width: 430px; padding-bottom: 8px; padding-top: 8px;"/>
<br/>
<i><b>Figure 5:</b> 
Illustration of short-range (local) vs. long-range dependencies in image data.
</i>
</div>
<p><br/>
<strong>The implicit regularization towards low hierarchical tensor rank in hierarchical tensor factorization therefore translates to an implicit regularization towards <em>locality</em> in the corresponding convolutional networks</strong>.
At first this may not seem surprising, since convolutional networks typically struggle or completely fail to learn tasks entailing long-range dependencies.
However, conventional wisdom attributes this failure to expressive properties (i.e. to an inability of convolutional networks to realize functions modeling long-range dependencies), suggesting that addressing the problem requires modifying the architecture.
Our analysis, on the other hand, reveals that implicit regularization also plays a role: it is not just a matter of expressive power, the optimization algorithm is implicitly pushing towards local solutions.
Inspired by this observation, we asked:</p>

<blockquote>
  <p><strong>Question:</strong>
Is it possible to improve the performance of modern convolutional networks on long-range tasks via explicit regularization (without modifying their architecture)?</p>
</blockquote>

<p>To explore this prospect, <strong>we designed explicit regularization that counteracts locality by promoting high hierarchical tensor rank (i.e. long-range dependencies)</strong>.
Then, through a series of controlled experiments, <strong>we confirmed that it can greatly improve the performance of modern convolutional networks (e.g. ResNets) on long-range tasks</strong>.</p>

<p>For example, the following plot displays test accuracies achieved by a ResNet on an image classification benchmark, in which it is possible to control the spatial range of dependencies required to model.
When increasing the range of dependencies, the test accuracy obtained by an unregularized network substantially deteriorates, reaching performance no better than random guessing.
As evident from the plot, our regularization closes the gap between short- and long-range tasks, significantly boosting generalization on the latter.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/pathfinder_resnet18_with_reg_blog.png" style="width: 430px; padding-bottom: 0px; padding-top: 0px;"/>
<br/>
<i><b>Figure 6:</b> 
Specialized explicit regularization promoting high hierarchical tensor rank (i.e. long-range dependencies between image regions) can counter the locality of convolutional networks, significantly improving their performance on long-range tasks.
</i>
</div>
<p><br/></p>

<h2 id="concluding-thoughts">Concluding Thoughts</h2>
<p>Looking forward, there are two main takeaways from our work:</p>

<ol>
  <li>
    <p>Across three different neural network types (equivalent to matrix, tensor, and hierarchical tensor factorizations), we have an architecture-dependant notion of rank that is implicitly lowered. Moreover, the underlying mechanism for this implicit regularization is identical in all cases. This leads us to believe that implicit regularization towards low rank may be a general phenomenon. If true, finding notions of rank lowered for different architectures can facilitate an understanding of generalization in deep learning.</p>
  </li>
  <li>
    <p>Our findings imply that the tendency of modern convolutional networks towards locality may largely be due to implicit regularization, and not an inherent limitation of expressive power as often believed. More broadly, they showcase that deep learning architectures considered suboptimal for certain tasks can be greatly improved through a right choice of explicit regularization. 
Theoretical understanding of implicit regularization may be key to discovering such regularizers.</p>
  </li>
</ol>

<p><strong><em><a href="https://noamrazin.github.io/">Noam Razin</a></em></strong></p></div>
    </summary>
    <updated>2022-07-15T09:00:00Z</updated>
    <published>2022-07-15T09:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2022-07-28T22:44:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/102</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/102" rel="alternate" type="text/html"/>
    <title>TR22-102 |  Range Avoidance for Low-depth Circuits and Connections to Pseudorandomness | 

	Xiuhan Wang, 

	Venkatesan Guruswami, 

	Xin Lyu</title>
    <summary>In the range avoidance problem, the input is a multi-output Boolean circuit with more outputs than inputs, and the goal is to find a string outside its range (which is guaranteed to exist). We show that well-known explicit construction questions such as finding binary linear codes achieving the Gilbert-Varshamov bound or list-decoding capacity, and constructing rigid matrices, reduce to the range avoidance problem of log-depth circuits, and by a further recent reduction [Ren, Santhanam, and Wang, ECCC 2022] to $NC^0_4$ circuits where each output depends on at most $4$ input bits. 

On the algorithmic side, we show that range avoidance for $NC^0_2$ circuits can be solved in polynomial time. We identify a general condition relating to correlation with low-degree parities that implies that any almost pairwise independent set has some string that avoids the range of every circuit in the class. We apply this to $NC^0$ circuits, and to small width CNF/DNF and general De Morgan formulae (via a connection to approximate-degree), yielding non-trivial small hitting sets for range avoidance in these cases.</summary>
    <updated>2022-07-15T07:11:28Z</updated>
    <published>2022-07-15T07:11:28Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-29T00:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/07/15/postdoc-at-university-of-cologne-apply-by-august-31-2022/</id>
    <link href="https://cstheory-jobs.org/2022/07/15/postdoc-at-university-of-cologne-apply-by-august-31-2022/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Cologne (apply by August 31, 2022)</title>
    <summary>A postdoc position in theoretical computer sciece, algorithms and data structures or algorithmic data analysis is available in the Algorithmic Data Analysis group led by Prof. Dr. Christian Sohler in the department of mathematics/computer science at University of Cologne. Website: https://jobportal.uni-koeln.de/ausschreibung/renderFile/893?propertyName=flyer Email: sohler@cs.uni-koeln.de</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A postdoc position in theoretical computer sciece, algorithms and data structures or algorithmic data analysis is available in the Algorithmic Data Analysis group led by Prof. Dr. Christian Sohler in the department of mathematics/computer science at University of Cologne.</p>
<p>Website: <a href="https://jobportal.uni-koeln.de/ausschreibung/renderFile/893?propertyName=flyer">https://jobportal.uni-koeln.de/ausschreibung/renderFile/893?propertyName=flyer</a><br/>
Email: sohler@cs.uni-koeln.de</p></div>
    </content>
    <updated>2022-07-15T06:10:56Z</updated>
    <published>2022-07-15T06:10:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-07-29T00:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/101</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/101" rel="alternate" type="text/html"/>
    <title>TR22-101 |  A Near-Cubic Lower Bound for 3-Query Locally Decodable Codes from Semirandom CSP Refutation | 

	Omar Alrabiah, 

	Pravesh Kothari, 

	Venkatesan Guruswami, 

	Peter Manohar</title>
    <summary>A code $C \colon \{0,1\}^k \to \{0,1\}^n$ is a $q$-locally decodable code ($q$-LDC) if one can recover any chosen bit $b_i$ of the message $b \in \{0,1\}^k$ with good confidence by randomly querying the encoding $x = C(b)$ on at most $q$ coordinates. Existing constructions of $2$-LDCs achieve $n = \exp(O(k))$, and lower bounds show that this is in fact tight. However, when $q = 3$, far less is known: the best constructions achieve $n = \exp(k^{o(1)})$, while the best known results only show a quadratic lower bound $n \geq \widetilde{\Omega}(k^2)$ on the blocklength.

In this paper, we prove a near-cubic lower bound of $n \geq \widetilde{\Omega}(k^3)$ on the blocklength of $3$-query LDCs. This improves on the best known prior works by a polynomial factor in $k$. Our proof relies on a new connection between LDCs and refuting constraint satisfaction problems with limited randomness. Our quantitative improvement builds on the new techniques for refuting semirandom instances of CSPs developed in [GKM22] and, in particular, relies on bounding the $(\infty \to 1)$-norm of appropriate Kikuchi matrices.</summary>
    <updated>2022-07-14T21:14:52Z</updated>
    <published>2022-07-14T21:14:52Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-29T00:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/100</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/100" rel="alternate" type="text/html"/>
    <title>TR22-100 |  Streaming complexity of CSPs with randomly ordered constraints | 

	Santhoshini Velusamy, 

	Noah Singer, 

	Raghuvansh Saxena, 

	Madhu Sudan</title>
    <summary>We initiate a study of the streaming complexity of constraint satisfaction problems (CSPs) when the constraints arrive in a random order. We show that there exists a CSP, namely Max-DICUT, for which random ordering makes a provable difference. Whereas a $4/9 \approx 0.445$ approximation of DICUT requires $\Omega(\sqrt{n})$ space with adversarial ordering, we show that with random ordering of constraints there exists a $0.48$-approximation algorithm that only needs $O(\log n)$ space. We also give new algorithms for Max-DICUT in variants of the adversarial ordering setting. Specifically, we give a two-pass  $O(\log n)$ space $0.48$-approximation algorithm for general graphs and a single-pass $\tilde{O}(\sqrt{n})$ space $0.48$-approximation algorithm for bounded degree graphs.
    
    On the negative side, we prove that CSPs where the satisfying assignments of the constraints support a one-wise independent distribution require $\Omega(\sqrt{n})$-space for any non-trivial approximation, even when the constraints are randomly ordered. This was previously known only for adversarially ordered constraints. Extending the results to randomly ordered constraints requires switching the hard instances from a union of random matchings to simple Erdos-Renyi random (hyper)graphs and extending tools that can perform Fourier analysis on such instances. 
    
    The only CSP to have been considered previously with random ordering is Max-CUT where the ordering is not known to change the approximability. Specifically, it is known to be as hard to approximate with random ordering as with adversarial ordering, for $o(\sqrt{n})$ space algorithms. Our results show a richer variety of possibilities and motivate further study of CSPs with randomly ordered constraints.</summary>
    <updated>2022-07-14T18:39:48Z</updated>
    <published>2022-07-14T18:39:48Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-29T00:20:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8443</id>
    <link href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/" rel="alternate" type="text/html"/>
    <title>My friend, Scott Aaronson</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is a photo of my book shelf at the office. Ever since joining Harvard, I have been ordering copies of Quantum Computing Since Democritus on a regular basis. I often hand them out to bright students, curious about science, whom I want to expose to the beautiful connections between computer science, math, physics, and … <a class="more-link" href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/">Continue reading <span class="screen-reader-text">My friend, Scott Aaronson</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><a href="https://windowsontheory.files.wordpress.com/2022/07/image.png"><img alt="" class="wp-image-8446" height="643" src="https://windowsontheory.files.wordpress.com/2022/07/image.png?w=750" width="471"/></a></figure></div>


<p>This is a photo of my book shelf at the office. Ever since joining Harvard, I have been ordering copies of <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing Since Democritus</a> on a regular basis. I often hand them out to bright students, curious about science, whom I want to expose to the beautiful connections between computer science, math, physics, and even philosophy. Scott has been one of the great popularizers of our field even before he started blogging in 2005. His surveys and blog posts provide some of the best introductions to our field. For example, when investigating  <a href="http://quant-ph/0502072">P vs NP and physical reality</a>, Scott actually <a href="https://scottaaronson.blog/?p=266">went out and verified</a> that nature indeed cannot solve an NP-complete problem via finding the globally minimal energy configuration of soap bubbles.  Through his blog, popular writing, and research, Scott has done more than anyone else to introduce new people of all backgrounds to theoretical computer science. </p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="408" src="https://lh6.googleusercontent.com/JRN6bxodVkY1JiHSWOEx4QXtvmH3twDk-O5jGgsio1IM92aUUxYZ87i7T0qU8bfrCnVbBWIt1OV4soNgnhoMpTHvnJBIVk8tAPO_oK2_WLzHbrGfWNpRDQyazey-CG9V0YLDDZ2r45pwOKsSF4PZaQ" width="543"/></figure></div>


<p>One of Scott’s endearing qualities is his openness to all people. While many of us would ignore a random email or anonymous blog comment, Scott would patiently explain for the millionth time why quantum computers can’t solve NP-hard problems by “trying all solutions in parallel” or why Bell’s Inequality does indeed rule out hidden-variable theories of nature. Alas, the same openness also results in him sometimes giving too much attention and caring far too much about the opinions of Internet “trolls” that are not worthy of his time. </p>



<p>While Scott has always attracted some vitriol, recently this has taken to a <a href="https://scottaaronson.blog/?p=6546">new level</a>, with commenters attacking his integrity, his speech mannerisms, even his T-shirt choice/frequency, and worst of all, his family, with misogynistic attacks on his wife and xenophobic and ableist attacks on neurodivergent researchers.</p>



<p>None of these people have made a fraction of the contributions of Scott not just to science, but also to broadening the diversity of computer science, and other causes including <a href="https://scottaaronson.blog/?p=6411">assisting women dealing with Texas’ restrictive abortion laws</a>. (As full disclosure, one of the causes <a href="https://scottaaronson.blog/?p=6256">Scott helped raise money</a> for is <a href="https://www.addiscoder.com/">AddisCoder</a> and <a href="https://jamcoder.org.jm/">JamCoders</a> of which I am a board member. I just came back from a <a href="https://twitter.com/boazbaraktcs/status/1545765613167067136?s=20&amp;t=53HLUIjmCwBsl1oYhEQXhw">week teaching in Jamaica</a>, the students were amazing and are so thankful for the chance to participate in this program; they couldn’t care less how often Scott changes his shirt.)</p>



<p>I am grateful that Scott is a member of our scientific community and proud to call him my friend. Does this mean that I agree with all his positions? Absolutely not. I tend to be on his left on many issues  (though am probably more conservative when it comes to oracle-based complexity..). Are there people he’s friendly with whom I even more strongly disagree with, and whose views I might even find repugnant? Probably. But it doesn’t matter, all of us are connected via 6 degrees of separation. If we start to “recursively cancel” every one that is somehow connected to someone we find odious, then we would not be able to talk to anyone.</p>



<p>I hope that Scott is not disheartened by these attacks, and continues to contribute for many years to CS research and education, outreach, and humanity at large.</p></div>
    </content>
    <updated>2022-07-13T16:07:23Z</updated>
    <published>2022-07-13T16:07:23Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2022-07-29T00:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6552</id>
    <link href="https://scottaaronson.blog/?p=6552" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6552#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6552" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Choosing a new comment policy</title>
    <summary xml:lang="en-US">Update (July 13): I was honored to read this post by my friend Boaz Barak. Update (July 14): By now, comments on this post allegedly from four CS professors — namely, Josh Alman, Aloni Cohen, Rana Hanocka, and Anna Farzindar — as well as from the graduate student “BA,” have been unmasked as from impersonator(s). […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Update (July 13):</strong> I was honored to read <a href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/">this post</a> by my friend Boaz Barak.</p>



<p><strong>Update (July 14):</strong> By now, comments on this post allegedly from four CS professors  — namely, Josh Alman, Aloni Cohen, Rana Hanocka, and Anna Farzindar — as well as from the graduate student “BA,” <em>have been unmasked as from impersonator(s)</em>.</p>



<p>I’ve been the target of a motivated attack-troll (or multiple trolls, but I now believe just one) who knows about the CS community. This might be the single weirdest thing that’s happened to me in 17 years of blogging, surpassing even the legendary <a href="https://scottaaronson.blog/?p=277">Ricoh printer episode</a> of 2007.  It obviously underscores the need for a new, stricter comment policy, which is what this whole post was about.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Yesterday and today, both my work and my enjoyment of the James Webb images were interrupted by an anonymous troll, who used the <em>Shtetl-Optimized</em> comment section to heap <a href="https://scottaaronson.blog/?p=6546#comment-1941041">libelous abuse</a> on me—derailing an anodyne quantum computing discussion to opine at length about how I’m a disgusting creep who surely, probably, maybe has lewd thoughts about his female students.  Unwisely or not, I allowed it all to appear, and replied to all of it.  I had a few reasons: I wanted to prove that I’m now strong enough to withstand bullying that might once have driven me to suicide.  I wanted, frankly, many readers to come to my defense (thanks to those who did!).  I at least wanted readers to <em>see</em> firsthand what I now regularly deal with: the emotional price of maintaining this blog.  Most of all, I wanted my feminist, social-justice-supporting readers to either explicitly endorse or (hopefully) explicitly repudiate the unambiguous harassment that was now being gleefully committed in their name.</p>



<p>Then, though, the same commenter upped the ante further, by heaping misogynistic abuse on my wife <a href="https://www.cs.utexas.edu/~danama/">Dana</a>—while <em>still</em>, ludicrously and incongruously, cloaking themselves in the rhetoric of social justice.  Yes: apparently the woke, feminist thing to do is now to rate female computer scientists on their looks.</p>



<p>Let me be blunt: I cannot continue to write <em>Shtetl-Optimized</em> while dealing with regular harassment of me and my family.  At the same time, I’m also determined not to “surrender to the terrorists.”  So, I’m weighing the following options:</p>



<ul><li>Close comments except to commenters who provide a real identity—e.g., a full real name, a matching email address, a website.</li><li>Move to Substack, and then allow only commenters who’ve signed up.</li><li>Hire someone to pre-screen comments for me, and delete ones that are abusive or harassing (to me or others) before I even see them.  (Any volunteers??)</li><li>Make the comment sections for readers only, eliminating any expectation that I’ll participate.</li></ul>



<p>One thing that’s clear is that the status quo will not continue.  I can’t “just delete” harassing or abusive comments, because the trolls have gotten too good at triggering me, and they will continue to weaponize my openness and my ethic of responding to all possible arguments against me.</p>



<p>So, regular readers: what do you prefer?</p></div>
    </content>
    <updated>2022-07-12T20:47:43Z</updated>
    <published>2022-07-12T20:47:43Z</published>
    <category scheme="https://scottaaronson.blog" term="Announcements"/>
    <category scheme="https://scottaaronson.blog" term="Obviously I'm Not Defending Aaronson"/>
    <category scheme="https://scottaaronson.blog" term="Self-Referential"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-07-28T17:22:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3528238240939023029</id>
    <link href="http://blog.computationalcomplexity.org/feeds/3528238240939023029/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/review-of-engines-of-cognition-essays.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/3528238240939023029" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/3528238240939023029" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/review-of-engines-of-cognition-essays.html" rel="alternate" type="text/html"/>
    <title>Review of The Engines of Cognition: Essays From the LessWrong Forum/Meta question about posts</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> A while back I reviewed<i> A Map that Reflects the Territory</i> which is a collection of essays posted on the lesswrong forum. My review is <a href="https://www.cs.umd.edu/~gasarch/bookrev/FRED/lesswrong.pdf">here</a>. I posted it to both this blog and to the lesswrong forum. In both cases I posted a link to it. My post to lesswrong is <a href="https://www.lesswrong.com/posts/JXTEDFCC5r4dW2tta/review-of-a-map-that-reflects-the-territory">here</a></p><p>On the lesswrong post many of the comments, plus some private emails, told me NO BILL- don't post a link, post it directly as text. It was not clear how to do that, but I got it done with help.</p><p>On complexity blog nobody commented that this was a problem. Then again, nobody commented at all, so its not clear what to make of that. </p><p>So</p><p>Meta Question: Is posting a link worse than posting direct text? Note that the book review was 12 pages long and looked great in LaTeX. </p><p>Meta Question: Why did lesswrong care about the format but complexityblog did not (Probably answer: omplexityblog readers did not care at all, whereas Lesswrong cared about what I though about Lesswrong)</p><p>Another Question, not Meta. One of the comments was (I paraphrase)</p><p><i>When I open a pdf file I expected to see something in the style of an academic paper. This is written in very much chatty, free-flowing blog post style with jokes like calling neologisms ``newords'', so the whole think felt more off-kilter than was intended. The style of writing would prob work better as an HTML blog post (which could then be posted directly as a Lesswrong post here instead of hosted elsewhere and linked.)</i></p><p>I think its interesting that the format of an article telegraphs (in this case incorrectly) what type of article it will be. Is this a common problem?  I have had the experience of reading a real academic paper and being surprised that some joke or cultural-reference is in it, though I do not object to this. </p><p>Another comment and question</p><p><i>I was surprised the post only had 11 karma when I saw it (William had send me an advance copy and I'd really liked reading it) but when I saw that it was a link post, I understood why.</i></p><p>I find this hilarious- they have some way the posts are rated!  For one, Lance told me very early on to never worry about comments, and I don't. Second, it reminds me of the Black Mirror episode <a href="https://en.wikipedia.org/wiki/Nosedive_(Black_Mirror)">Nosedive</a>.</p><p>ANYWAY, I have reviewed another collection of essays for less wrong, this one called <i>The Engines of</i> <i>Cognition. </i>I am posting it here as a link: <a href="https://www.cs.umd.edu/~gasarch/bookrev/FRED/lesswrong2.pdf">here</a>  and I will post it on lesswrong as full text (with help) in a few days. </p><p>I am posting it so I can get comments before I submit it to the SIGACT News book review column. But this is odd since I think this blog has more readers than SIGACT news has subscribers, so perhaps THIS is its real debut, not that. And of course the lesswrong forum is a place where more will read it since its about them. </p><p>So- I appreciate comments to make it a better review!</p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2022-07-12T01:22:00Z</updated>
    <published>2022-07-12T01:22:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-07-28T17:47:06Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/data-transfer/</id>
    <link href="https://gradientscience.org/data-transfer/" rel="alternate" type="text/html"/>
    <title>A Data-Based Perspective on Transfer Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><!-- <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> -->
<!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"> -->









<!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css"> -->
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script> -->
<!-- <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script> -->


<!-- <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css"> -->
<!-- chart.js -->


<p><a class="bbutton" href="https://arxiv.org/abs/2207.05739" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/data-transfer" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<br/>
<i>
In our latest paper, we present a framework for pinpointing the impact of the source datasets in transfer learning. Our framework enables us to improve transfer learning performance by removing source datapoints that are detrimental for a specific target task. It also unlocks various other capabilities, such as debugging transfer learning failures, automatically identifying granular subpopulations in the target dataset, and detecting data leakage between source and target datasets. 
</i></p>

<p>Transfer learning is a widely utilized technique for adapting a model trained on a source dataset to improve performance on a downstream target task. Used in applications ranging from <a href="https://arxiv.org/abs/2101.06871">radiology</a>, <a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w13/papers/Kim_End-To-End_Ego_Lane_CVPR_2017_paper.pdf">autonomous driving</a>, to <a href="https://arxiv.org/abs/1510.00098">satellite imagery analysis</a>, the transfer learning paradigm also fuels the recent emergence of large vision and language models trained on enormous amounts of data, such as <a href="https://openai.com/blog/clip/">CLIP</a> or <a href="https://openai.com/blog/gpt-3-apps/">GPT-3</a>.</p>

<p>Why is transfer learning so effective though? And, in particular, what drives transfer learning performance? Definitely, much depends on the properties of the source model, i.e., the model trained on the source dataset. For example, recent works highlight the impact of the model’s <a href="https://arxiv.org/abs/2101.06871">architecture</a>, <a href="https://arxiv.org/abs/1805.08974">accuracy</a>, <a href="https://arxiv.org/abs/2007.08489">adversarial vulnerability</a>, and <a href="https://arxiv.org/abs/1905.05901">training procedure</a>.</p>

<p>But, in addition to the source model, it is hard to not expect the source dataset to play a major role as well. Indeed, several works have shown that increasing the size of the dataset usually <a href="https://arxiv.org/abs/1912.11370">boosts transfer learning performance</a>. <a href="https://arxiv.org/abs/1811.07056">Others</a> have found that limiting the source dataset to images that are relevant to the target task can help as well. So: what is the exact role of the source dataset in transfer learning?</p>

<p>In our most <a href="https://arxiv.org/abs/2207.05739">recent paper</a>, we present a framework for pinpointing the impact of the source dataset on the downstream predictions in transfer learning. This framework draws inspiration from techniques such as <a href="https://arxiv.org/abs/2008.03703">influence functions</a> and <a href="https://arxiv.org/abs/2202.00622">datamodels</a> and it enables us, in particular, to automatically identify source datapoints that—positively or negatively—impact transfer learning performance.</p>

<p>We’ll now walk through how we calculate the influence of the source dataset in transfer learning, and then demonstrate how our framework can be used to:</p>

<ul type="a">
  <li>Boost transfer learning performance by removing detrimental source datapoints;</li>
  <li>Automatically extract granular subpopulations in the target dataset by projecting the  class hierarchy of the source dataset onto it; and</li>
  <li>Surface pathologies such as source-target data leakage and misleading or mislabelled source datapoints.</li>
</ul>

<h2 id="computing-the-influence-of-the-source-dataset">Computing the Influence of the Source Dataset</h2>

<p>How to pinpoint the relationship between the source dataset’s composition and the model’s downstream predictions? We build here on the <a href="https://arxiv.org/abs/2008.03703">influence functions</a> and <a href="https://arxiv.org/abs/2202.00622">datamodels</a> methodology (check out our <a href="https://gradientscience.org/datamodels-1/">previous post</a> for a deeper dive into these) to study the counterfactual effect of removing source datapoints on the target model’s predictions. However, unlike in the standard supervised setting in which the focus is on individual datapoints, here we focus on removing entire classes. This is motivated by the fact that we expect the removal of entire classes to have a more measurable impact on the features learned by the source model (and thus the resulting model’s predictions).</p>

<p>So, at a high level, we first train a large number of models on random subsets of classes in the source dataset, and fine-tune them on the target task. Then we compute the influence of a source class on a target example by simply measuring the average difference in the model’s performance on a given target example when the class was included versus excluded from the source dataset. A positive influence value thus means that including the class improved the model’s performance on that example, while a negative value indicates that the class was detrimental to the correctness of the corresponding model’s prediction.</p>

<h2 id="identifying-the-most-influential-classes-of-the-source-dataset">Identifying the Most Influential Classes of the Source Dataset</h2>
<p>Now that we’ve calculated these influences, how can we use them to study transfer learning? First, let’s take a look at the ranking of the most positively and negatively influencing classes for a variety of target tasks. We first look at the influences from different ImageNet classes to the entire CIFAR-10 test set:</p>

<p><img alt="Most influential" src="https://gradientscience.org/assets/data-transfer/images/most_influencing_classes.png" style="width: 100%; margin-left: 0; margin-right: 0;"/>
<!-- <div class="footnote">
TODO
</div> --></p>

<p>Note that in the most positive source classes tend to semantically overlap with the classes in the target dataset. For example, tailed frog and sorrel horse have the most positive influence for the CIFAR-10 dataset, which contains the classes frog and horse.</p>

<p>Also, the plot above suggests that there is a number of source classes, such as bookshop or jigsaw puzzle, whose inclusion actually hurts the overall transfer learning performance. So what happens if we indeed remove these classes from the source dataset? Well, as one might hope, they do boost the transfer learning performance on a variety of downstream image classification:</p>

<p><img alt="Counterfactual main" src="https://gradientscience.org/assets/data-transfer/images/main_counterfactual_exp.png" style="width: 100%; margin-left: 0; margin-right: 0;"/></p>
<div class="footnote">
Target task accuracies after removing the most positively or negatively influential ImageNet classes from the source dataset.
</div>

<p>In fact, we can get an accuracy boost of nearly 2.5% on CIFAR-10 (as compared to what one gets from pre-training with the full ImageNet dataset).</p>

<h2 id="leveraging-influences-to-study-transfer-learning">Leveraging Influences to Study Transfer Learning</h2>

<p>Above, we used our framework to pinpoint the most influential—be it positively or negatively— classes for transfer learning. Here, we’ll discuss how our framework provides us with a broader set of tools for studying transfer learning, including: (1) debugging transfer learning failures, (2) automatically extracting granular subpopulations in the target dataset, and (3) detecting data leakage between source and target datasets.</p>

<h3 id="1--debugging-the-failures-of-the-transferred-model">(1)  Debugging the failures of the transferred model</h3>
<p>Suppose our transferred model wrongly predicts the dog image displayed in the figure below—it labels it as a horse. Can we use our framework to understand why our model is making this mistake? Yes! The influences we computed enable us to identify the source class sorrel horse as one having a strong (and the strongest) negative influence on our image of interest. This suggests that the features learned by the source model due to the presence of this class might be the culprit here. Indeed, once we remove the sorrel horse class from the source dataset, our model now makes the correct prediction on our dog image much more frequently (with respect to the randomness of the training procedure).</p>

<div id="debug_examples_widget" style="overflow: auto; text-align: center;"/>
<div class="footnote">
Identifying highly negatively influencing source classes can explain why our transfer learning model made a mistake (middle). Once we remove the most negatively influencing class from the source dataset, the model predicts the correct label more frequently (right). Click through the thumbnails on the left to see more examples!
</div>

<!-- <div class="footnote">
TODO
</div> -->

<h3 id="2-automatically-extracting-granular-subpopulations-in-the-target-dataset">(2) Automatically extracting granular subpopulations in the target dataset</h3>
<p>Imagine you want to find all the images of ostriches in the CIFAR-10 dataset. However, the problem is that CIFAR-10 does not contain any subpopulation annotations that could help with this task and having to manually look for ostriches among all the images in the bird class is not a very appealing alternative. Our framework allows us to do something much more scalable!</p>

<p>Indeed, as we already observed, the most positively influencing source classes usually semantically overlap with the images in the target dataset they influence the most. In fact, this goes further: the target images which are most influenced by a given source class tend to share relevant salient features too. So, to identify our ostrich subpopulation in CIFAR-10, we just need to look at all the images that are most influenced by the source class “ostrich”! Below we display some of the CIFAR-10 images identified in this way.</p>

<div id="subpop_examples_widget" style="overflow: auto; text-align: center;"/>
<div class="footnote">
The CIFAR-10 images which are most positively influenced by a particular ImageNet class. Click through the thumbnails on the left to see more examples!
</div>

<h3 id="3-detecting-data-leakage-and-misleading-source-dataset-examples">(3) Detecting data-leakage and misleading source dataset examples</h3>
<p>Thus far, we have focused on the role of classes in the source dataset in transfer learning. But we can also compute the influences of specific source examples on the transferred model’s predictions. This turns out to enable us to isolate, in particular, instances of data leakage and misleading examples in the source dataset.</p>

<p>Indeed, below, we display ImageNet training examples that are highly influential on CIFAR-10 test examples. The source images that have a highly positive influence are often identical copies of images from the target task (just at a higher resolution)—a clear example of data leakage. On the other hand, images that have a high negative influence tend to be the ones that are misleading, mislabeled, or otherwise surprising.  For example, the presence of the (amusing) ImageNet image of a flying lawnmower (see below) hurts the performance on a CIFAR-10 image of a regular (but similarly shaped) airplane.</p>

<p><img alt="Detect leakage" src="https://gradientscience.org/assets/data-transfer/images/detect_leakage.png" style="width: 100%; margin-left: 0; margin-right: 0;"/>
<!-- <div class="footnote">
TODO
</div> --></p>

<h3 id="conclusions">Conclusions</h3>
<p>In this post, we described a new framework for pinpointing the impact of the source dataset in transfer learning. Our framework allows one to improve the transfer learning performance on a range of downstream tasks by identifying and removing source datapoints that are detrimental. Furthermore, by using our framework one can automatically extract granular subpopulations in the target dataset by leveraging the fine-grained class hierarchy of the source dataset, better understand how the errors of the model on the downstream task are rooted in the source dataset, and detect potential data leakage from the source to the downstream dataset. We believe our framework provides a new perspective on transfer learning by highlighting the role of the source dataset in the transfer learning pipeline and gives us a toolkit for performing a fine-grained analysis of it.</p></div>
    </summary>
    <updated>2022-07-12T00:00:00Z</updated>
    <published>2022-07-12T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2022-07-28T22:44:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/07/11/postdoc-position-in-computer-science-logic-at-university-of-sheffield-apply-by-july-20-2022/</id>
    <link href="https://cstheory-jobs.org/2022/07/11/postdoc-position-in-computer-science-logic-at-university-of-sheffield-apply-by-july-20-2022/" rel="alternate" type="text/html"/>
    <title>PostDoc position in Computer Science Logic at University of Sheffield (apply by July 20, 2022)</title>
    <summary>All candidates interested in working in logics and complexity theory utilising numerical features and real valued data are encouraged to apply. The project topics range from logical foundations of probabilistic data and complexity theory utilising real numbers to logical approach to quantum information theory utilising the newly discovered connections to probabilistic team semantics. Website: https://www.jobs.ac.uk/job/CRB003/research-associate-in-computer-science-logic-x2-positions […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>All candidates interested in working in logics and complexity theory utilising numerical features and real valued data are encouraged to apply.</p>
<p>The project topics range from logical foundations of probabilistic data and complexity theory utilising real numbers to logical approach to quantum information theory utilising the newly discovered connections to probabilistic team semantics.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CRB003/research-associate-in-computer-science-logic-x2-positions">https://www.jobs.ac.uk/job/CRB003/research-associate-in-computer-science-logic-x2-positions</a><br/>
Email: j.t.virtema@sheffield.ac.uk</p></div>
    </content>
    <updated>2022-07-11T15:43:44Z</updated>
    <published>2022-07-11T15:43:44Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-07-29T00:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/07/11/associate-professor-or-tenure-track-assistant-professor-at-technical-university-of-denmark-apply-by-october-1-2022/</id>
    <link href="https://cstheory-jobs.org/2022/07/11/associate-professor-or-tenure-track-assistant-professor-at-technical-university-of-denmark-apply-by-october-1-2022/" rel="alternate" type="text/html"/>
    <title>Associate Professor or Tenure Track Assistant Professor at Technical University of Denmark (apply by October 1, 2022)</title>
    <summary>DTU Compute’s section for Algorithms, Logic, and Graphs (AlgoLoG) invites applications for our next assistant or associate professor within logic and logic-based artificial intelligence or algorithms and data structures. The AlgoLoG section focuses on research in the foundations of computer science and discrete mathematics and application in industry. Website: https://www.dtu.dk/english/about/job-and-career/vacant-positions/job?id=77ec4bc1-f834-4d49-96da-e263a7abb56c Email: phbi@dtu.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>DTU Compute’s section for Algorithms, Logic, and Graphs (AlgoLoG) invites applications for our next assistant or associate professor within logic and logic-based artificial intelligence or algorithms and data structures. The AlgoLoG section focuses on research in the foundations of computer science and discrete mathematics and application in industry.</p>
<p>Website: <a href="https://www.dtu.dk/english/about/job-and-career/vacant-positions/job?id=77ec4bc1-f834-4d49-96da-e263a7abb56c">https://www.dtu.dk/english/about/job-and-career/vacant-positions/job?id=77ec4bc1-f834-4d49-96da-e263a7abb56c</a><br/>
Email: phbi@dtu.dk</p></div>
    </content>
    <updated>2022-07-11T11:34:26Z</updated>
    <published>2022-07-11T11:34:26Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-07-29T00:20:38Z</updated>
    </source>
  </entry>
</feed>
