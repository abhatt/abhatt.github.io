<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-09-08T00:22:25Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1108</id>
    <link href="http://corner.mimuw.edu.pl/?p=1108" rel="alternate" type="text/html"/>
    <title>IGAFIT Algorithmic Colloquium</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">We are excited to announce a new online seminar - IGAFIT Algorithmic Colloquium. This new event aims to integrate the European algorithmic community and keep it connected during the times of the pandemic. This online seminar will take place biweekly on … <a href="http://corner.mimuw.edu.pl/?p=1108">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are excited to announce a new online seminar - IGAFIT Algorithmic Colloquium. This new event aims to integrate the European algorithmic community and keep it connected during the times of the pandemic. This online seminar will take place biweekly on Thursday at 14:00 CET, with the talks lasting for 45 minutes. Each talk will be followed by a networking and discussion session on topics related to the talk. We cordially invite all participants to this session. The meeting will be run on <a href="https://www.airmeet.com/e/55923fa0-eee9-11ea-8530-b3eab1e75816" rel="noreferrer noopener" target="_blank">Airmeet</a>. More details on the event can be found on <a href="http://igafit.mimuw.edu.pl/?page_id=483786" rel="noreferrer noopener" target="_blank">IGAFIT web page</a>.</p>



<p>The first talk will be held on the 1st of October 2020.</p>



<p>October 1, 2020<br/>Vera Traub, University of Bonn<br/>Title: An improved approximation algorithm for ATSP<br/>Abstract: In a recent breakthrough, Svensson, Tarnawski, and Végh gave the first constant-factor approximation algorithm for the asymmetric traveling salesman problem (ATSP). In this work we revisit their algorithm. While following their overall framework, we improve on each part of it.</p>



<p>Svensson, Tarnawski, and Végh perform several steps of reducing ATSP to more and more structured instances. We avoid one of their reduction steps (to irreducible instances) and thus obtain a simpler and much better reduction to vertebrate pairs. Moreover, we show that a slight variant of their algorithm for vertebrate pairs has a much smaller approximation ratio.</p>



<p>Overall we improve the approximation ratio from 506 to 22 + ε for any ε &gt; 0. We also improve the upper bound on the integrality ratio of the standard LP relaxation from 319 to 22.</p>



<p>This is joint work with Jens Vygen.</p>



<p>Other upcoming talks include:</p>



<p>October 15, 2020<br/>Thatchaphol Saranurak, Toyota Technological Institute at Chicago<br/>Title: An almost-linear time deterministic algorithm for expander decomposition</p>



<p>October 29, 2020<br/>Nathan Klein, University of Bonn<br/>Title: A (Slightly) Improved Approximation Algorithm for Metric TSP</p>



<p>For more details please contact the Organization Committee:<br/>Nikhil Bansal<br/>Artur Czumaj<br/>Andreas Feldmann<br/>Adi Rosén<br/>Eva Rotenberg<br/>Piotr Sankowski<br/>Christian Sohler <br/></p></div>
    </content>
    <updated>2020-09-07T20:33:10Z</updated>
    <published>2020-09-07T20:33:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2020-09-07T23:23:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=2727</id>
    <link href="https://francisbach.com/integration-by-parts-randomized-smoothing-score-functions/" rel="alternate" type="text/html"/>
    <title>The many faces of integration by parts – II : Randomized smoothing and score functions</title>
    <summary>This month I will follow-up on last month blog post and look at another application of integration by parts, which is central to many interesting algorithms in machine learning, optimization and statistics. In this post, I will consider extensions in higher dimensions, where we take integrals on a subset of \(\mathbb{R}^d\), and focus primarily on...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">This month I will follow-up on last month blog post and look at another application of integration by parts, which is central to many interesting algorithms in machine learning, optimization and statistics. In this post, I will consider extensions in higher dimensions, where we take integrals on a subset of \(\mathbb{R}^d\), and focus primarily on property of the so-called “score function” of a density \(p: \mathbb{R}^d \to \mathbb{R}\), namely the gradient of its logarithm: $$\nabla  \log  p(z)  = \frac{1}{p(z)} \nabla p(z) \in \mathbb{R}^d,$$ or, done coordinate by coordinate, $$ \big(\nabla \log p(z)\big)_i = \frac{\partial [ \log p]}{\partial z_i}(z) = \frac{1}{p(z)} \frac{\partial  p }{\partial z_i}(z) .$$ Note here that we take derivatives with respect to \(z\) and not with respect to some hypothetical external parameter, which is often the case in statistics (see <a href="https://en.wikipedia.org/wiki/Score_(statistics)">here</a>).</p>



<p class="justify-text">As I will show below, this quantity comes up in many different areas, most often used with integration by parts. After a short review on integration by parts and its applications to score functions, I will present four quite diverse applications, to (1) optimization and randomized smoothing, (2) differentiable perturbed optimizers, (3) learning single-index models in statistics, and (4) score matching for density estimation.</p>



<h2>Integration by parts in multiple dimensions</h2>



<p class="justify-text">I will focus only on situations where we have some random variable \(Z\) defined on \(\mathbb{R}^d\), with differentiable strictly positive density \(p(\cdot)\) with respect to the Lebesgue measure (I could also consider bounded supports, but then I would need to use the <a href="https://en.wikipedia.org/wiki/Divergence_theorem">divergence theorem</a>). I will consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), and my goal is to provide an expression of \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] \in \mathbb{R}^d\) using the gradient of \(f\).</p>



<p class="justify-text">Assuming that \(f(z) p(z)\) goes to zero when \(\| z\| \to +\infty\), we have: $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big]  = \int_{\mathbb{R}^d} f(z)\Big( \frac{1}{p(z)} \nabla p (z) \Big) p(z) dz = \int_{\mathbb{R}^d}  f (z) \nabla p(z) dz .$$ We can then use integration by parts (together with the zero limit at infinity), to get $$\int_{\mathbb{R}^d} f (z) \nabla p(z) dz = \ – \int_{\mathbb{R}^d} p (z) \nabla f(z) dz.$$ This leads to $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\  – \mathbb{E} \big[ \nabla f(Z) \big]. \tag{1}$$ In other words, expectations of the gradient of \(f\) can be obtained through expectations of \(f\) times the negative of the score function.  </p>



<p class="justify-text">Note that Eq. (1) can be used in the two possible directions: to estimate the right hand side (expectation of gradients) when the score function is known, and vice-versa to estimate expectations (as a simple example, when \(f\) is constant equal to one, we get the traditional identity \(\mathbb{E} \big[ \nabla \log p(Z) \big] = 0\)).</p>



<p class="justify-text"><strong>Gaussian distribution.</strong> Assuming that \(p(z) = \frac{1}{(2\pi \sigma^2)^{d/2}} \exp\big( – \frac{1}{2 \sigma^2}\|  z – \mu\|_2^2 \big)\), that is, \(Z\) is normally distributed with mean vector \(\mu \in \mathbb{R}^d\) and covariance matrix \(\sigma^2 I\), we get a particularly simple expression $$\frac{1}{\sigma^2} \mathbb{E} \big[ f(Z) (Z-\mu)  \big] =  \mathbb{E} \big[ \nabla f(Z) \big],$$ which is often referred to as <a href="https://en.wikipedia.org/wiki/Stein%27s_lemma">Stein’s lemma</a> (see for example an application to <a href="https://en.wikipedia.org/wiki/Stein%27s_unbiased_risk_estimate">Stein’s unbiased risk estimation</a>).</p>



<p class="justify-text"><strong>Vector extension.</strong> If now \(f\) has values in \(\mathbb{R}^d\), still with the product \(f(z) p(z)\) going to zero when \(\| z\| \to +\infty\), we get $$\mathbb{E} \big[ f(Z)^\top \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla \!\cdot \! f(Z) \big], \tag{2}$$ where \(\nabla\! \cdot \! f\) is the <a href="https://en.wikipedia.org/wiki/Divergence">divergence</a> of \(f\) defined as \(\displaystyle \nabla\! \cdot\! f(z) = \sum_{i=1}^d \frac{\partial f}{\partial z_i}(z)\). </p>



<h2>Optimization and randomized smoothing</h2>



<p class="justify-text">We consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), which is  non-differentiable everywhere. There are several ways of <em>smoothing</em> it. A very traditional way is to convolve it with a smooth function. In our context, this corresponds to considering $$f_\varepsilon(x) = \mathbb{E} f(x+ \varepsilon Z) = \int_{\mathbb{R}^d} f(x+\varepsilon z) p(z) dz,$$ where \(z\) is a random variable with strictly positive sufficiently differentiable density, and \(\varepsilon \) is a positive parameter. Typically, if \(f\) is Lipschitz-continuous, \(| f – f_\varepsilon|\) is uniformly bounded by a constant times \(\varepsilon\).</p>



<p class="justify-text">Let us now assume that we can take gradients within the integral, leading to: $$\nabla f_\varepsilon(x) = \int_{\mathbb{R}^d}   \nabla f(x+\varepsilon z) p(z) dz = \mathbb{E} \big[  \nabla f(x+\varepsilon z) \big].$$ This derivation is problematic as the whole goal is to apply this to functions \(f\) which are not everywhere differentiable, so the gradient \(\nabla f\) is not always defined. It turns out that when \(p\) is sufficiently differentiable, integration by parts exactly provides an expression which does not imply the gradient of \(f\).</p>



<p class="justify-text">Indeed, still imagining that \(f\) is differentiable, we can apply Eq. (1) to the function \(z \mapsto \frac{1}{\varepsilon} f(x+\varepsilon z)\), whose gradient is the function \(z \mapsto \nabla f(x+\varepsilon z)\), and get $$\nabla f_\varepsilon(x) = \ – \frac{1}{\varepsilon} \int_{\mathbb{R}^d} f(x+\varepsilon z) \nabla p(z) dz = \frac{1}{\varepsilon} \mathbb{E} \big[ – f(x+\varepsilon Z) \nabla \log p(Z)\big].$$ These computations can easily be made rigorous and we obtain an expression of the gradient of \(f_\varepsilon\) without invoking the gradient of \(f\) (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>, <a href="https://arxiv.org/pdf/2002.08676">14</a>] for details).</p>



<p class="justify-text">Moreover, if \(p\) is a differentiable function, we can expect the expectation in the right hand side of the equation above to be bounded, and therefore the function \(f_\varepsilon\) has gradients bounded by \(\frac{1}{\varepsilon}\).</p>



<p class="justify-text">This can be used within (typically convex) optimization in two ways:</p>



<ul class="justify-text"><li><strong>Zero-th order optimization</strong>: if our goal is to minimize the function \(f\), which is non-smooth, and for which we only have access to function values (so-called “zero-th order oracle), then we can obtain an unbiased stochastic gradient of the smoothed version \(f_\varepsilon\) as \(– f(x+\varepsilon z) \nabla \log p(z)\) where \(z\) is sampled from \(p\). The variance of the stochastic gradient grows with \(1/\varepsilon\) and the bias due to the use of \(f_\varepsilon\) instead of \(f\) is proportional to \(\varepsilon\). There is thus a sweet spot for the choice of \(\varepsilon\), with many variations; see, e.g., [<a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">5</a>, <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">6</a>, <a href="https://arxiv.org/pdf/cs/0408007">7</a>]. </li><li><strong>Randomized smoothing with acceleration</strong> [<a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">8</a>, <a href="https://arxiv.org/pdf/1204.0665">9</a>]: Here the goal is to follow the “Nesterov smoothing” idea [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">10</a>] and minimize a non-smooth function \(f\) using accelerated gradient descent on the smoothed version \(f_\varepsilon\), but this time with a stochastic gradient. Stochastic versions of Nesterov accelerations are then needed; this is useful when a full deterministic smoothing of \(f\) is too costly, see [<a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">11</a>, <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">12</a>] for details.</li></ul>



<p class="justify-text"><strong>Example.</strong> We consider minimizing a quadratic function in two dimensions, and we compare below plain gradient descent, stochastic gradient descent (left) and zero-th order optimization where we take a step towards the direction \(– f(x+\varepsilon Z) \nabla \log p(Z)\) for a standard normal \(Z\). We compare stochastic zero-th order optimization to plain stochastic gradient descent (SGD) below: SGD is a first-order method requiring access to stochastic gradients with a variance that is bounded, while zero-th order optimization only requires function values, but with significantly higher variance and thus requiring more iterations to converge.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4633" height="254" src="https://francisbach.com/wp-content/uploads/2020/09/paths_video_zeroth_order.gif" width="556"/>Left: gradient descent (GD) and stochastic gradient descent (SGD). Right: zero-th order optimization. All with constant step-sizes.</figure></div>



<h2>Differentiable perturbed optimizers</h2>



<p class="justify-text">The randomized smoothing technique can be used in a different context with applications to differentiable programming. We now assume that the function \(f\) can be written as the <a href="https://en.wikipedia.org/wiki/Support_function">support function</a> of a polytope \(\mathcal{C}\), that is, for all \(u \in \mathbb{R}^d\), $$f(u) = \max_{y \in \mathcal{C}} u^\top y,$$ where \(\mathcal{C}\) is the convex hull of a finite family \((y_i)_{i \in I}\). </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4598" height="292" src="https://francisbach.com/wp-content/uploads/2020/09/polytope_intro-1024x842.png" width="357"/>Polytope \(C\), convex hull of 8 vectors in \(\mathbb{R}^2\).</figure></div>



<p class="justify-text">Typically, the family is very large (e.g., \(|I|\) is exponential in \(d\)), but a polynomial-time algorithm exists for computing an arg-max \(y^\ast(u)\) above. Classical examples, from simpler to more interesting, are:</p>



<ul class="justify-text"><li><strong>Simplex</strong>: \(\mathcal{C}\) is the set of vectors with non-negative components that sum to one, and is the convex hull of canonical basis vectors. Then \(f\) is the maximum function, and there are many classical ways of smoothing it (see link with the <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> below).</li><li><strong>Hypercube</strong>: \(\mathcal{C} = [0,1]^n\), which is the convex hull of all vectors in \(\{0,1\}^n\). The maximization of linear functions can then be done independently for each bit.</li><li><strong>Permutation matrices</strong>: \(\mathcal{C}\) is then the <a href="https://en.wikipedia.org/wiki/Birkhoff_polytope">Birkhoff polytope</a>, the convex hull of all <a href="https://en.wikipedia.org/wiki/Permutation_matrix">permutation matrices</a> (square matrices with elements in \(\{0,1\}\), and with exactly a single \(1\) in each row and column). Maximizing linear functions is the classical <a href="https://en.wikipedia.org/wiki/Assignment_problem">linear assignment problem</a>.</li><li><strong>Shortest paths</strong>: given a graph, a path is a sequence of vertices which are connected to each other in the graph. They can classically be represented as a vector of of 0’s and 1’s indicating the edges which are followed by the paths. Minimizing linear functions is then equivalent to <a href="https://en.wikipedia.org/wiki/Shortest_path_problem">shortest path</a> problems.</li></ul>



<p class="justify-text">In many supervised applications, the vector \(u\) is as a function of some input \(x\) and some parameter vector \(\theta\). In order to learn the pararameter \(\theta\) from data, one needs to be able to differentiate with respect to \(\theta\), and this is typically done through the chain rule by differentiating \(y^\ast(u)\) with respect to \(u\). There come two immediate obstacles: (1) the element \(y^\ast(u)\) is not even well-defined when the arg-max is not unique, which is not a real problem because this can only be the case for a set of \(u\)’s with zero Lebesgue measure; and (2) the function \(y^\ast(u)\) is locally constant for most \(u\)’s, that is, the gradient is equal to zero almost everywhere. Thus, in the context of differentiable programming, this is non informative and essentially useless.</p>



<p class="justify-text">Randomized smoothing provides a simple and generic way to define an approximation which is differentiable and with informative gradient everywhere (there are others, such as adding a strongly convex regularizer \(\psi(y)\), and maximizing \(u^\top y\  – \psi(y)\) instead, see [<a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">20</a>] for details. See also [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>]).</p>



<p class="justify-text">In order to obtain a differentiable function through randomized smoothing, we can consider \(y^\ast(u + \varepsilon z)\), for a random \(z\), which is an instance of the more general “perturb-and-MAP” paradigm [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242">21</a>, <a href="https://icml.cc/Conferences/2012/papers/528.pdf">22</a>].</p>



<p class="justify-text">Since \(y^\ast(u)\) is a subgradient of \(f\) at \(u\) and \(f_\varepsilon(u) = \int_{\mathbb{R}^d} f(u+\varepsilon z) p(z) dz\), by swapping integration (with respect to \(z\)) and differentiation (with respect to \(u\)), we have the following identities: $$ \mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big] = \nabla f_\varepsilon(u),$$ that is, the expectation of the perturbed arg-max is the gradient of the smoothed function \(f_\varepsilon\). I will use the notation \(y^\ast_\varepsilon(u) =\mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big]\) to denote this gradient; see an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4545" height="322" src="https://francisbach.com/wp-content/uploads/2020/08/polytope-1024x671.png" width="491"/>Polytope \(\mathcal{C}\), with a direction \(u\), the non-perturbed maximizer \(y^\ast(u)\), a perturbed direction \(u+\varepsilon Z\) and the perturbed maximizer \(y^\ast(u+\varepsilon Z)\). The areas of the red circles are proportional to the probability of selecting the corresponding extreme point after the perturbation. The expected perturbed maximizer \(y^\ast_\varepsilon(u)\) is in the interior of \(\mathcal{C}\).</figure></div>



<p class="justify-text">In a joint work with Quentin Berthet, Mathieu Blondel, Oliver Teboul, Marco Cuturi, and Jean-Philippe Vert [<a href="http://arxiv.org/pdf/2002.08676(opens in a new tab)">14</a>], we detail theoretical and practical properties of \(y^\ast_\varepsilon(u)\), in particular:</p>



<ul class="justify-text"><li>Estimation: \(y^\ast_\varepsilon(u)\) can be estimated by replacing the expectation by empirical averages.</li><li>Differentiability: if \(Z\) has a strictly positive density over \(\mathbb{R}^d\), then the function \(y^\ast_\varepsilon\) is infinitely differentiable, with simple expression of  the Jacobian, obtained by integration by parts (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>] for details).</li><li>The <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> is the simplest instance of such a smoothing technique, with \(\mathcal{C}\) being the simplex, and \(Z\) having independent Gumbel distributions. The function \(f_\varepsilon\) is then a “<a href="https://en.wikipedia.org/wiki/LogSumExp">log-sum-exp</a>” function.</li></ul>



<p class="justify-text"><strong>Illustration</strong>. Following [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>], this can be applied to learn the travel costs in graphs based on features. The vectors \(y_i\) represent shortest path between the top-left and bottom-right corners, with costs corresponding to the terrain type. See [<a href="https://arxiv.org/pdf/2002.08676">14</a>] for details on the learning procedure. Here I just want to highlight the effect of varying the amount of smoothing characterized by \(\varepsilon\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4605" height="225" src="https://francisbach.com/wp-content/uploads/2020/09/paths-1024x518.png" width="446"/>Left: Warcraft terrain. Right: Cost associated to each terrain type.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full"><img alt="" class="wp-image-4624" height="288" src="https://francisbach.com/wp-content/uploads/2020/09/anim_smoothed.gif" width="432"/>Shortest paths \(y^\ast_\varepsilon(u)\), from \(\varepsilon=0\) (no smoothing) to \(\varepsilon=2\). From an essentially single shortest path, as smoothing increases, we obtain a mixture of two potential paths, before having many extreme points.</figure></div>



<h2>Learning single-index models</h2>



<p class="justify-text">Given a random vector \((X,Y) \in \mathbb{R}^d \times \mathbb{R}\), we assume that \(Y = f(X) + \varepsilon\), where \(f(x) = \sigma(w^\top x)\) for some unknown function \(\sigma: \mathbb{R} \to \mathbb{R}\) and \(w \in \mathbb{R}^d\), with \(\varepsilon\) a zero-mean noise independent from \(X\).  Given some observations \((x_1,y_1), \dots, (x_n,y_n)\) in \(\mathbb{R}^d \times \mathbb{R}\), the goal is to estimate the direction \(w \in \mathbb{R}^d\). This model is referred to as single-index regression models in the statistics literature [<a href="https://www.jstor.org/stable/pdf/1913713.pdf">1</a>, <a href="https://www.jstor.org/stable/pdf/3035585.pdf">2</a>]</p>



<p class="justify-text">One possibility if \(\sigma\) was known would be to perform least-squares estimation and minimize with respect to \(w\) $$ \frac{1}{2n} \sum_{i=1}^n \big( y_i\  – \sigma(w^\top x_i) \big)^2, $$ which is a non-convex optimization problem in general. When \(\sigma\) is unknown, one could imagine adding the estimation of \(\sigma\) into the optimization, making it even more complicated.</p>



<p class="justify-text">Score functions provide an elegant solution that leads to the “average derivative method” (ADE) [<a href="https://www.jstor.org/stable/pdf/1914309.pdf">3</a>], which I will now describe. We consider \(p\) the density of \(X\). We then have, using Eq. (1): $$ \mathbb{E} \big[ Y \nabla \log p(X) \big] =\mathbb{E} \big[ f(X) \nabla \log p(X) \big] = \ – \mathbb{E} \big[ \nabla f(X)  \big] =\ –  \Big( \mathbb{E} \big[ \sigma'(w^\top X) \big] \Big) w, $$ which is proportional to \(w\). When replacing the expectation by an empirical mean, this provides a way to estimate \(w\) (up to a constant factor) without even knowing the function \(\sigma\), but assuming the density of \(X\) is known so that the score function is available.</p>



<p class="justify-text"><strong>Extensions.</strong> The ADE method can be extended in a number of ways to deal with more complex situations. Here are some examples below:</p>



<ul class="justify-text"><li><em>Multiple index models</em>: if the response/output \(Y\) is instead assumed of the form $$ Y = f(X) + \varepsilon =  \sigma(W^\top x) + \varepsilon, $$ where \(W \in \mathbb{R}^{d \times k}\) is a matrix with \(k\) columns, we obtained a “multiple index model”, for which a similar technique seems to apply since now \(\nabla f(x) = W \nabla  \sigma(W^\top x) \in \mathbb{R}^d\), and thus, for the assumed model \(\mathbb{E} \big[ Y \nabla \log p(X) \big]\) is in the linear span of the columns of \(W\); this is not enough for recovering the entire subspace if \(k&gt;1\) because we have only a single element of the span. There are two solutions for this. The first one is is to condition on some values of \(Y\) being in some set \(\mathcal{Y}\), where one can show that \(\mathbb{E} \big[ Y \nabla \log p(X) | Y \in \mathcal{Y} \big]\) is also in the desired subspace; thus, with several sets \(\mathcal{Y}\), one can generate several elements, and after \(k\) of these, one can expect to estimate the full \(k\)-dimensional subspace. The idea of conditioning on \(Y\) is called <a href="https://en.wikipedia.org/wiki/Sliced_inverse_regression">sliced inverse regression</a> [<a href="https://www.jstor.org/stable/pdf/2290563.pdf">15</a>], and the application to score function can be found in [<a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>]. The second one is to consider higher-order moments and derivatives of the score functions, that is, using integration by parts twice! (see [<a href="https://arxiv.org/pdf/1412.2863">17</a>, <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">18</a>, <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>] for details).</li><li><em>Neural networks</em>: when the function \(\sigma\) is the sum of functions that depends on single variables, multiple-index models are exactly one-hidden-layer neural networks. Similar techniques can be used for deep networks with more than a single hidden layer (see [<a href="https://arxiv.org/pdf/1506.08473">19</a>]).</li></ul>



<p class="justify-text"><strong>Moment matching vs. empirical risk minimization. </strong>In all cases mentioned above, the use of score functions can be seen as an instance of the <a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">method of moments</a>: we assume a specific model for the data, derive identities satisfied by expectations of some functions under the model, and use these identities to identify a parameter vector. In the situations above, direct empirical risk minimization would lead to a potentially hard optimization problem. However, moment matching techniques rely heavily on the model being well-specified, which is often not the case in practice, while empirical risk minimization techniques try to fit the data as much as the model allows, and is thus typically more robust to model misspecification.</p>



<h2>Score matching for density estimation</h2>



<p class="justify-text">We consider the problem of density estimation. That is, given some observations \(x_1,\dots,x_n \in \mathbb{R}^d\) sampled independently and identically distributed from some distribution with density \(p\), we want to estimate \(p\) from the data. Given a model \(q_\theta \) with some parameters \(\theta\), the most standard method is maximum likelihood estimation, which corresponds to the following optimization problem: $$\max_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n \log q_\theta(x_i).$$ It requires <em>normalized</em> densities that is, \(\int_{\mathbb{R}^d} q_\theta(x) dx = 1\), and dealing with normalized densities often requires to explicitly normalize them and thus to compute integrals, which is difficult when the underlying dimension \(d\) gets large.</p>



<p class="justify-text">Score matching is a recent method proposed by Aapo Hyvärinen [4] based on score functions. The simple (yet powerful) idea is to perform least-squares estimation on the score functions. That is, in the population case, the goal is to minimize $$\mathbb{E} \big\| \nabla \log p(X) \ – \nabla  \log q_\theta(X) \big\|_2^2 = \int_{\mathbb{R}^d} \big\| \nabla \log p(x)\  – \nabla  \log q_\theta(x) \big\|_2^2 p(x) dx.$$ Apparently, this expectation does not lead to an estimation procedure where \(p(x)\) is replaced by the empirical distribution of the data because of the presence of \(\nabla \log p(x)\). Integration by parts will solve this.</p>



<p class="justify-text">We can expand \(\mathbb{E} \big\| \nabla \log p(X) \ – \nabla \log q_\theta(X) \big\|_2^2\) as  $$ \mathbb{E} \big\| \nabla \log p(X) \|_2^2 + \mathbb{E} \big\|\nabla \log q_\theta(X) \big\|_2^2 – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big]. $$ The first term is independent of \(q_\theta\) so it does not count when minimizing. The second term is an expectation with respect to \(p(\cdot)\) so it can be replaced by the empirical mean. The third term can be dealt with with integration by parts, that is Eq. (2), leading to: $$ – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \nabla \cdot \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \Delta \log q_\theta(X) \big],$$ where \(\Delta\) is the <a href="https://en.wikipedia.org/wiki/Laplace_operator">Laplacian</a>.</p>



<p class="justify-text">We now have an expectation with respect to the data distribution \(p\), and we can replace the expectation with an empirical average to estimate the parameter \(\theta\) from data \(x_1,\dots,x_n\). We then use the cost function $$\frac{1}{n} \sum_{i=1}^n \big\|\nabla \log q_\theta(x_i) \big\|_2^2 + \frac{2}{n} \sum_{i=1}^n \Delta \log q_\theta(x_i), $$ which is linear in \(\log q_\theta\). Hence, when the unnormalized log-density is linearly parameterized, which is common, we obtain a quadratic problem. This procedure has a number of attractive properties, in particular consistency [<a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">4</a>], but the key benefit is to allow estimation without requiring normalizing constants.</p>



<h2>Conclusion</h2>



<p class="justify-text">Overall, the simple identity from Eq. (1), that is, \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla f(Z) \big]\), has many applications in diverse somewhat unrelated areas of machine learning, optimization and statistics. There are of course many other uses of integration by parts within this field. Feel free to add your preferred one as comment.</p>



<p class="justify-text">It has been a while since the last post on polynomial magic. I will revive the thread next month. I let you guess which polynomials will be the stars of my next blog post.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Quentin Berthet for producing the video of shortest paths, proofreading this blog post, and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] James L. Powell, James H. Stock, Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1913713.pdf">Semiparametric estimation of index coefficients</a>. <em>Econometrica: Journal of the Econometric Society</em>. 57(6):1403-1430, 1989.<br/>[2] Wolfgang Hardle, Peter Hall, Hidehiko Ichimura. <a href="https://www.jstor.org/stable/pdf/3035585.pdf">Optimal smoothing in single-index models</a>. <em>Annals of Statistics</em>. 21(1): 157-178(1993): 157-178.<br/>[3] Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1914309.pdf">Consistent Estimation of Scaled Coefficients</a>. <em>Econometrica</em>, 54(6):1461-1481, 1986.<br/>[4] Aapo Hyvärinen. <a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Estimation of non-normalized statistical models by score matching</a>. <em>Journal of Machine Learning Research</em>, <em>6</em>(Apr), 695-709, 2005.<br/>[5] Yurii Nesterov. <a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">Random gradient-free minimization of convex functions</a>. Technical report, Université Catholique de Louvain (CORE), 2011.<br/>[6] Boris T. Polyak and Alexander B. Tsybakov. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">Optimal order of accuracy of search algorithms in stochastic optimization</a>. <em>Problemy Peredachi Informatsii</em>, 26(2):45–53, 1990.<br/>[7]  Abraham D. Flaxman, Adam Tauman Kalai, H. Brendan McMahan. <a href="https://arxiv.org/pdf/cs/0408007">Online convex optimization in the bandit setting: gradient descent without a gradient</a>. In Proc. Symposium on Discrete algorithms (SODA), 2005.<br/>[8] John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. <a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">Randomized Smoothing for Stochastic Optimization</a>. SIAM Journal on Optimization, 22(2), 674–701, 2012.<br/>[9] Alexandre d’Aspremont, Nourredine El Karoui, <a href="https://www.di.ens.fr/~aspremon/stochsmooth.html">A Stochastic Smoothing Algorithm for Semidefinite Programming.</a> <em>SIAM Journal on Optimization</em>, 24(3): 1138-1177, 2014.<br/>[10] Yurii Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions</a>. Mathematical Programming, 103(1):127–152, 2005.<br/>[11] Lin Xiao. <a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a>. <em>Journal of Machine Learning Research</em>, 11(88): 2543−2596, 2010.<br/>[12] Guanghui Lan. <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">An optimal method for stochastic composite optimization</a>. <em>Mathematical Programming</em>, 133(1):365–397, 2012.<br/>[13] Tamir Hazan, George Papandreou, and Daniel Tarlow. <a href="https://mitpress.mit.edu/books/perturbations-optimization-and-statistics">Perturbation, Optimization, and Statistics</a>. MIT Press, 2016.<br/>[14] Quentin Berthet, Matthieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach, <a href="https://arxiv.org/pdf/2002.08676">Learning with differentiable perturbed optimizers</a>. Technical report arXiv 2002.08676, 2020.<br/>[15] Ker-Chau Li. <a href="https://www.jstor.org/stable/pdf/2290563.pdf">Sliced inverse regression for dimension reduction</a>. <em>Journal of the American Statistical Association</em>, <em>86</em>(414), 316-327, 1991.<br/>[16] Dmitry Babichev and Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">Slice inverse regression with score functions</a>. <em>Electronic Journal of Statistics</em>, 12(1):1507-1543, 2018.<br/>[17] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1412.2863">Score function features for discriminative learning: Matrix and tensor framework</a>. Technical report arXiv:1412.2863, 2014.<br/>[18] David R. Brillinger. <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">A generalized linear model with “Gaussian” regressor variables</a>.  <em>Selected Works of David Brillinger</em>, 589-606, 2012.<br/>[19] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1506.08473">Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods</a>.  Technical report arXiv:1506.08473, 2015.<br/>[20] Vlad Niculae, André F. T. Martins, Mathieu Blondel, and Claire Cardie. <a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">SparseMAP: Differentiable sparse structured inference</a>. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 2017.<br/>[21] George Papandreou and Alan L. Yuille.<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242"> Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models</a>. <em>International Conference on Computer Vision</em>, 2011.<br/>[22] Tamir Hazan and Tommi Jaakkola. <a href="https://icml.cc/Conferences/2012/papers/528.pdf">On the partition function and random maximum a-posteriori perturbations</a>. <em>Proceedings of the International Conference on International Conference on Machine Learning (ICML),</em> 2012.<br/>[23] Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. <a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">Perturbation techniques in online learning and optimization</a>. <em>Perturbations, Optimization, and Statistics</em>, 233-264, 2016.<br/>[24] Marin Vlastelica, Anselm Paulus, Vít Musil, Georg Martius, Michal Rolínek. <a href="https://openreview.net/pdf?id=BkevoJSYPB">Differentiation of Blackbox Combinatorial Solvers</a>. <em>International Conference on Learning Representations</em>. 2019.</p></div>
    </content>
    <updated>2020-09-07T19:06:36Z</updated>
    <published>2020-09-07T19:06:36Z</published>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-09-08T00:22:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/132</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/132" rel="alternate" type="text/html"/>
    <title>TR20-132 |  Towards Stronger Counterexamples to the Log-Approximate-Rank Conjecture | 

	Arkadev Chattopadhyay, 

	Ankit Garg, 

	Suhail Sherif</title>
    <summary>We give improved separations for the query complexity analogue of the log-approximate-rank conjecture i.e. we show that there are a plethora of total Boolean functions on $n$ input bits, each of which has approximate Fourier sparsity at most $O(n^3)$ and randomized parity decision tree complexity $\Theta(n)$. This improves upon the recent work of Chattopadhyay, Mande and Sherif (JACM '20) both qualitatively (in terms of designing a large number of examples) and quantitatively (improving the gap from quartic to cubic). We leave open the problem of proving a randomized communication complexity lower bound for XOR compositions of our examples. A linear lower bound would lead to new and improved refutations of the log-approximate-rank conjecture. Moreover, if any of these compositions had even a sub-linear cost randomized communication protocol, it would demonstrate that randomized parity decision tree complexity does not lift to randomized communication complexity in general (with the XOR gadget).</summary>
    <updated>2020-09-07T15:45:54Z</updated>
    <published>2020-09-07T15:45:54Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-08T00:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.02233</id>
    <link href="http://arxiv.org/abs/2009.02233" rel="alternate" type="text/html"/>
    <title>Access-Adaptive Priority Search Tree</title>
    <feedworld_mtime>1599436800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Massa:Haley.html">Haley Massa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uhlmann:Jeffrey.html">Jeffrey Uhlmann</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02233">PDF</a><br/><b>Abstract: </b>In this paper we show that the priority search tree of McCreight, which was
originally developed to satisfy a class of spatial search queries on
2-dimensional points, can be adapted to the problem of dynamically maintaining
a set of keys so that the query complexity adapts to the distribution of
queried keys. Presently, the best-known example of such a data structure is the
splay tree, which dynamically reconfigures itself during each query so that
frequently accessed keys move to the top of the tree and thus can be retrieved
with fewer queries than keys that are lower in the tree. However, while the
splay tree is conjectured to offer optimal adaptive amortized query complexity,
it may require O(n) for individual queries. We show that an access-adaptive
priority search tree (AAPST) can provide competitive adaptive query performance
while ensuring O(log n) worst-case query performance, thus potentially making
it more suitable for certain interactive (e.g.,online and real-time)
applications for which the response time must be bounded.
</p></div>
    </summary>
    <updated>2020-09-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.02207</id>
    <link href="http://arxiv.org/abs/2009.02207" rel="alternate" type="text/html"/>
    <title>Fair and Useful Cohort Selection</title>
    <feedworld_mtime>1599436800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Niklas Smedemark-Margulies, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Langton:Paul.html">Paul Langton</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Huy_L=.html">Huy L. Nguyen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02207">PDF</a><br/><b>Abstract: </b>As important decisions about the distribution of society's resources become
increasingly automated, it is essential to consider the measurement and
enforcement of fairness in these decisions. In this work we build on the
results of Dwork and Ilvento ITCS'19, which laid the foundations for the study
of fair algorithms under composition. In particular, we study the cohort
selection problem, where we wish to use a fair classifier to select $k$
candidates from an arbitrarily ordered set of size $n&gt;k$, while preserving
individual fairness and maximizing utility. We define a linear utility function
to measure performance relative to the behavior of the original classifier. We
develop a fair, utility-optimal $O(n)$-time cohort selection algorithm for the
offline setting, and our primary result, a solution to the problem in the
streaming setting that keeps no more than $O(k)$ pending candidates at all
time.
</p></div>
    </summary>
    <updated>2020-09-07T23:21:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.01986</id>
    <link href="http://arxiv.org/abs/2009.01986" rel="alternate" type="text/html"/>
    <title>Smoothed analysis of the condition number under low-rank perturbations</title>
    <feedworld_mtime>1599436800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shah:Rikhav.html">Rikhav Shah</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silwal:Sandeep.html">Sandeep Silwal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01986">PDF</a><br/><b>Abstract: </b>Let $M$ be an arbitrary $n$ by $n$ matrix of rank $n-k$. We study the
condition number of $M$ plus a \emph{low rank} perturbation $UV^T$ where $U, V$
are $n$ by $k$ random Gaussian matrices. Under some necessary assumptions, it
is shown that $M+UV^T$ is unlikely to have a large condition number. The main
advantages of this kind of perturbation over the well-studied dense Gaussian
perturbation where every entry is independently perturbed is the $O(nk)$ cost
to store $U,V$ and the $O(nk)$ increase in time complexity for performing the
matrix-vector multiplication $(M+UV^T)x$. This improves the $\Omega(n^2)$ space
and time complexity increase required by a dense perturbation, which is
especially burdensome if $M$ is originally sparse. We experimentally validate
our approach and consider generalizations to symmetric and complex settings.
Lastly, we show barriers in applying our low rank model to other problems
studied in the smoothed analysis framework.
</p></div>
    </summary>
    <updated>2020-09-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.01947</id>
    <link href="http://arxiv.org/abs/2009.01947" rel="alternate" type="text/html"/>
    <title>Nearly Linear-Time, Parallelizable Algorithms for Non-Monotone Submodular Maximization</title>
    <feedworld_mtime>1599436800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhnle:Alan.html">Alan Kuhnle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01947">PDF</a><br/><b>Abstract: </b>We study parallelizable algorithms for maximization of a submodular function,
not necessarily monotone, with respect to a cardinality constraint $k$. We
improve the best approximation factor achieved by an algorithm that has optimal
adaptivity and query complexity, up to logarithmic factors in the size $n$ of
the ground set, from $0.039 - \epsilon$ to $0.193 - \epsilon$. We provide two
algorithms; the first has approximation ratio $1/6 - \epsilon$, adaptivity $O(
\log n )$, and query complexity $O( n \log k )$, while the second has
approximation ratio $0.193 - \epsilon$, adaptivity $O( \log^2 n )$, and query
complexity $O(n \log k)$. Heuristic versions of our algorithms are empirically
validated to use a low number of adaptive rounds and total queries while
obtaining solutions with high objective value in comparison with highly
adaptive approximation algorithms.
</p></div>
    </summary>
    <updated>2020-09-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.01928</id>
    <link href="http://arxiv.org/abs/2009.01928" rel="alternate" type="text/html"/>
    <title>Efficient Algorithms to Mine Maximal Span-Trusses From Temporal Graphs</title>
    <feedworld_mtime>1599436800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Quintino Francesco Lotito, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Montresor:Alberto.html">Alberto Montresor</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01928">PDF</a><br/><b>Abstract: </b>Over the last decade, there has been an increasing interest in temporal
graphs, pushed by a growing availability of temporally-annotated network data
coming from social, biological and financial networks. Despite the importance
of analyzing complex temporal networks, there is a huge gap between the set of
definitions, algorithms and tools available to study large static graphs and
the ones available for temporal graphs. An important task in temporal graph
analysis is mining dense structures, i.e., identifying high-density subgraphs
together with the span in which this high density is observed. In this paper,
we introduce the concept of $(k, \Delta)$-truss (span-truss) in temporal
graphs, a temporal generalization of the $k$-truss, in which $k$ captures the
information about the density and $\Delta$ captures the time span in which this
density holds. We then propose novel and efficient algorithms to identify
maximal span-trusses, namely the ones not dominated by any other span-truss
neither in the order $k$ nor in the interval $\Delta$, and evaluate them on a
number of public available datasets.
</p></div>
    </summary>
    <updated>2020-09-07T23:21:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.01874</id>
    <link href="http://arxiv.org/abs/2009.01874" rel="alternate" type="text/html"/>
    <title>Sum-of-Squares Lower Bounds for Sherrington-Kirkpatrick via Planted Affine Planes</title>
    <feedworld_mtime>1599436800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Mrinalkanti.html">Mrinalkanti Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jeronimo:Fernando_Granha.html">Fernando Granha Jeronimo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Chris.html">Chris Jones</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potechin:Aaron.html">Aaron Potechin</a>, Goutham Rajendran <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01874">PDF</a><br/><b>Abstract: </b>The Sum-of-Squares (SoS) hierarchy is a semi-definite programming
meta-algorithm that captures state-of-the-art polynomial time guarantees for
many optimization problems such as Max-$k$-CSPs and Tensor PCA. On the flip
side, a SoS lower bound provides evidence of hardness, which is particularly
relevant to average-case problems for which NP-hardness may not be available.
</p>
<p>In this paper, we consider the following average case problem, which we call
the \emph{Planted Affine Planes} (PAP) problem: Given $m$ random vectors
$d_1,\ldots,d_m$ in $\mathbb{R}^n$, can we prove that there is no vector $v \in
\mathbb{R}^n$ such that for all $u \in [m]$, $\langle v, d_u\rangle^2 = 1$? In
other words, can we prove that $m$ random vectors are not all contained in two
parallel hyperplanes at equal distance from the origin? We prove that for $m
\leq n^{3/2-\epsilon}$, with high probability, degree-$n^{\Omega(\epsilon)}$
SoS fails to refute the existence of such a vector $v$.
</p>
<p>When the vectors $d_1,\ldots,d_m$ are chosen from the multivariate normal
distribution, the PAP problem is equivalent to the problem of proving that a
random $n$-dimensional subspace of $\mathbb{R}^m$ does not contain a boolean
vector. As shown by Mohanty--Raghavendra--Xu [STOC 2020], a lower bound for
this problem implies a lower bound for the problem of certifying energy upper
bounds on the Sherrington-Kirkpatrick Hamiltonian, and so our lower bound
implies a degree-$n^{\Omega(\epsilon)}$ SoS lower bound for the certification
version of the Sherrington-Kirkpatrick problem.
</p></div>
    </summary>
    <updated>2020-09-07T23:20:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-83517349672236531</id>
    <link href="https://blog.computationalcomplexity.org/feeds/83517349672236531/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/83517349672236531" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/83517349672236531" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html" rel="alternate" type="text/html"/>
    <title>Two Math Problems of interest (at least to me)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I will give two math problems that are of interest to me.</p><p>These are not new problems, however you will have more fun if you work on them yourself and leave comments on what you find. So if you want to work on it without hints, don't read the comments.</p><p><br/></p><p>I will post about the answers (not sure I will post THE answers) on Thursday.</p><p><br/></p><p>1) Let x(1)&gt;0. Let x(n+1) = (  1 + (1/x(n))  )^n. </p><p><br/></p><p>For how many values of x(1) does this sequence go to infinity?</p><p><br/></p><p>2) Find all (x,y) \in N \times N such that x^2+3y and y^2+3x are both squares. </p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2020-09-06T21:05:00Z</updated>
    <published>2020-09-06T21:05:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-07T22:17:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/131</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/131" rel="alternate" type="text/html"/>
    <title>TR20-131 |  A Direct Product Theorem for One-Way Quantum Communication | 

	Srijita Kundu, 

	Rahul  Jain</title>
    <summary>We prove a direct product theorem for the one-way entanglement-assisted quantum communication complexity of a general relation $f\subseteq\mathcal{X}\times\mathcal{Y}\times\mathcal{Z}$. For any $\varepsilon, \zeta &gt; 0$ and any $k\geq1$, we show that
\[ \mathrm{Q}^1_{1-(1-\varepsilon)^{\Omega(\zeta^6k/\log|\mathcal{Z}|)}}(f^k) = \Omega\left(k\left(\zeta^5\cdot\mathrm{Q}^1_{\varepsilon + 12\zeta}(f) - \log\log(1/\zeta)\right)\right),\]
where $\mathrm{Q}^1_{\varepsilon}(f)$ represents the one-way entanglement-assisted quantum communication complexity of $f$ with worst-case error $\varepsilon$ and $f^k$ denotes $k$ parallel instances of $f$.

As far as we are aware, this is the first direct product theorem for quantum communication -- direct sum theorems were previously known for one-way quantum protocols. Our techniques are inspired by the parallel repetition theorems for the entangled value of two-player non-local games, under product distributions due to Jain, Pereszl\'{e}nyi and Yao, and under anchored distributions due to Bavarian, Vidick and Yuen, as well as message-compression for quantum protocols due to Jain, Radhakrishnan and Sen. In particular, we show that a direct product theorem holds for the distributional one-way quantum communication complexity of $f$ under any distribution $q$ on $\mathcal{X}\times\mathcal{Y}$ that is anchored on one side, i.e., there exists a $y^*$ such that $q(y^*)$ is constant and $q(x|y^*) = q(x)$ for all $x$. This allows us to show a direct product theorem for general distributions, since for any relation $f$ and any distribution $p$ on its inputs, we can define a modified relation $\tilde{f}$ which has an anchored distribution $q$ close to $p$, such that a protocol that fails with probability at most $\varepsilon$ for $\tilde{f}$ under $q$ can be used to give  a protocol that fails with probability at most $\varepsilon + \zeta$ for $f$ under $p$.

Our techniques also work for entangled non-local games which have input distributions anchored on any one side, i.e., either there exists a $y^*$ as previously specified, or there exists an $x^*$ such that $q(x^*)$ is constant and $q(y|x^*) = q(y)$ for all $y$. In particular, we show that for any game $G = (q, \mathcal{X}\times\mathcal{Y}, \mathcal{A}\times\mathcal{B}, V)$ where $q$ is a distribution on $\mathcal{X}\times\mathcal{Y}$ anchored on any one side with anchoring probability $\zeta$, then
\[ \omega^*(G^k) = \left(1 - (1-\omega^*(G))^5\right)^{\Omega\left(\frac{\zeta^2 k}{\log(|\mathcal{A}|\cdot|\mathcal{B}|)}\right)}\]
where $\omega^*(G)$ represents the entangled value of the game $G$. This is a generalization of the result of Bavarian, Vidick and Yuen, who proved a parallel repetition theorem for games anchored on both sides, i.e., where both a special $x^*$ and a special $y^*$ exist, and potentially a simplification of their proof.</summary>
    <updated>2020-09-06T05:33:09Z</updated>
    <published>2020-09-06T05:33:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-08T00:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/130</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/130" rel="alternate" type="text/html"/>
    <title>TR20-130 |  Optimal Inapproximability of Satisfiable k-LIN over Non-Abelian Groups | 

	Amey Bhangale, 

	Subhash Khot</title>
    <summary>A seminal result of H\r{a}stad [J. ACM, 48(4):798–859, 2001]  shows that it is NP-hard to find an assignment that satisfies $\frac{1}{|G|}+\varepsilon$ fraction of the constraints of a given $k$-LIN instance over an abelian group, even if there is an assignment that satisfies $(1-\varepsilon)$ fraction of the constraints, for any constant $\varepsilon&gt;0$.  Engebretsen et al. [Theoretical Computer Science, 312(1):17–45, 2004] later showed that the same hardness result holds for $k$-LIN instances over any finite non-abelian group.

Unlike the abelian case, where we can efficiently find a solution if the instance is satisfiable, in the non-abelian case, it is NP-complete to decide if a given system of linear equations is satisfiable or not, as shown by Russell and Goldmann [Information and Computation, 178(1):253–262, 2002].  

Surprisingly, for certain non-abelian groups $G$, given a satisfiable $k$-LIN instance over $G$, one can in fact do better than just outputting a random assignment using a simple but clever algorithm. The approximation factor achieved by this algorithm varies with the underlying group. In this paper, we show that this algorithm is {\em optimal} by proving a  tight hardness of approximation of satisfiable $k$-LIN instance over {\em any} non-abelian $G$, assuming $P \neq NP$.

As a corollary, we also get $3$-query probabilistically checkable proofs with perfect completeness over large alphabets with improved soundness.</summary>
    <updated>2020-09-06T04:42:45Z</updated>
    <published>2020-09-06T04:42:45Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-08T00:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17507</id>
    <link href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/" rel="alternate" type="text/html"/>
    <title>Closing An Open Problem</title>
    <summary>Crawl, then walk, then run. Bogdan Grechuk is a lecturer in the math department at the University of Leicester. His office is in the Michael Atiyah Building. Pretty cool. He works in risk analysis, but is more broadly interested in math of all kinds. See his wonderful book Theorems of the 21st Century. Or go […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Crawl, then walk, then run.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/bg/" rel="attachment wp-att-17512"><img alt="" class="alignright  wp-image-17512" src="https://rjlipton.files.wordpress.com/2020/09/bg.png?w=150" width="150"/></a></p>
<p>Bogdan Grechuk is a lecturer in the math department at the University of Leicester. His office is in the Michael Atiyah Building. Pretty cool. He works in risk analysis, but is more broadly interested in math of all kinds. See his wonderful book <a href="https://link.springer.com/book/10.1007%2F978-3-030-19096-5">Theorems of the 21st Century</a>. Or go to his web <a href="https://theorems.home.blog/theorems-list/">site</a>.</p>
<p>
Today Ken and I want to talk about solving open problems.</p>
<p>
Grechuk’s site got us thinking about results that solve open problems. Most of us like to think our research solves an open problem. Personally I can say that I have tried to solve problems for the first time, but did not always succeed. </p>
<p>
Open problems usually mean something stronger. To be an open problem, a problem must be known to some community for some time. Advances in math and in complexity theory roughly fall into two categories: </p>
<ol>
<li>
Results that prove or disprove something that was explicitly stated before. The more who knew the problem the better. The longer the problem was known the better, too. <p/>
</li><li>
Results that prove something that is new. Something that no one had explicitly asked before.
</li></ol>
<p>Both type of results are important. The latter kind may ultimately be more important. They raise new questions, often contain new methods, and move the field ahead in a new direction. See our<br/>
<a href="https://rjlipton.wordpress.com/2011/02/01/godel&#x2019;s-lost-letter-is-two-years-old/">discussion</a> of Freeman Dyson and frogs and birds.</p>
<p>
Think of how Kurt Gödel’s incompleteness theorem was unsuspected, or how Alan Turing’s proof of undecidability of the halting problem came in tandem with settling the criterion for computability, or years latter the definition of public-key crypto-systems. But we will focus on open problems in the sense (1). </p>
<p/><h2> Claiming A Solution </h2><p/>
<p/><p>
Ken and I are amazed that when an open problem is claimed, especially for P versus NP, the claim swallows it whole. That is the claim is that the full problem is solved. We do not recall once when the claim was: </p>
<ul>
<li>
We are able to prove that SAT requires quadratic time, or <p/>
</li><li>
We can show that SAT is in co-NP.
</li></ul>
<p>Either of these would be a “stop the press” result. </p>
<p>
For a more concrete example, suppose you claim to have a polynomial-time algorithm for finding a maximum clique in an undirected graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. Of course this implies P<img alt="{=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{=}"/>NP. Your algorithm may require a chain of difficult lemmas that obscure its workings. Can you perhaps analyze its effectiveness more easily on <em>random</em> graphs? Here are two relevant facts:</p>
<ul>
<li>
In 1976, David Matula <a href="https://s2.smu.edu/~matula/Tech-Report76.pdf">proved</a> that with high probability for a random <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-vertex graph of edge probability <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/>, the size of the maximum clique is one of the two integers flanking <img alt="{2\log_{1/p}(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\log_{1/p}(n)}"/>. <p/>
</li><li>
As observed in 1976 by Dick Karp in his <a href="https://www.semanticscholar.org/paper/The-probabilistic-analysis-of-some-combinatorial-Karp/9a9558d79b93fd884354f1ae27463be2836d2ec0">paper</a>, “The Probabilistic Analysis of Some Combinatorial Search Algorithms,” no polynomial time algorithm is known to achieve size <img alt="{(1+\epsilon)\log_{1/p}(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2B%5Cepsilon%29%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1+\epsilon)\log_{1/p}(n)}"/>, for any <img alt="{\epsilon &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt; 0}"/> and sufficiently large <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>.
</li></ul>
<p>
You only need to close a gap of a factor of <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>, not to hit the maximum value exactly, and you do not need to succeed for all graphs. The behavior of random graphs should help your analysis. A more-recent mention of Karp’s open problem is in these 2005 <a href="https://www.math.cmu.edu/~af1p/MAA2005/L7.pdf">slides</a>.</p>
<p>
</p><p/><h2> Some Examples </h2><p/>
<p/><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b> Collatz Conjecture </b>: Terence Tao made important <a href="https://terrytao.wordpress.com/2019/09/10/almost-all-collatz-orbits-attain-almost-bounded-values/">progress</a> on this notorious <a href="https://terrytao.files.wordpress.com/2020/02/collatz.pdf">problem</a>. He said: </p>
<blockquote><p><b> </b> <em> In mathematics, when we cannot solve a problem completely, we look for partial results. Even if they do not lead to a complete solution, they often reveal insights about the problem. </em>
</p></blockquote>
<p>Recall this is also called the <img alt="{3n+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3n%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3n+1}"/> problem. It asks for the long-term behavior of the function: <img alt="f(n) " class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="f(n) "/> which is equal to <img alt="n/2 " class="latex" src="https://s0.wp.com/latex.php?latex=n%2F2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n/2 "/> for <img alt="n " class="latex" src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n "/> even and <img alt="3n+1 " class="latex" src="https://s0.wp.com/latex.php?latex=3n%2B1+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="3n+1 "/> for <img alt="n " class="latex" src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n "/> odd. </p>
<p>The conjecture is that for every <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, iterating the function eventually hits <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, i.e., <img alt="{f^i(n) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f^i(n) = 1}"/> for some <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>.</p>
<p>
There are two ways the conjecture can fail:</p>
<ul>
<li>
There is a finite cycle besides the trivial cycle <img alt="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Crightarrow+4+%5Crightarrow+2+%5Crightarrow+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}"/>. <p/>
</li><li>
For some <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, the sequence <img alt="{f^i(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f^i(n)}"/> goes off to infinity.
</li></ul>
<p>
What Tao proved is that “many” values <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> achieve: <img alt="f^i(n) &lt; \log^* n " class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ei%28n%29+%3C+%5Clog%5E%2A+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="f^i(n) &lt; \log^* n "/> for some <img alt="i " class="latex" src="https://s0.wp.com/latex.php?latex=i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="i "/>.</p>
<p>Grechuk's <a href="https://theorems.home.blog/2020/04/29/almost-all-orbits-of-the-collatz-map-attain-almost-bounded-value/">page</a> includes the definition of “many,” which turns out to be <em>weaker</em> than saying a <img alt="{(1-\epsilon)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281-%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1-\epsilon)}"/> proportion of values <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> “swing low” in this sense. Moreover, Tao proved this for any unbounded function <img alt="{g(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(n)}"/> in place of the iterated logarithm, such as the inverse Ackermann function. Note this is a case where randomized analysis worked—in the hands of a master.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b> Twin Prime Conjecture </b>: Yitang Zhang made tremendous progress on this long standing <a href="https://en.wikipedia.org/wiki/Twin_prime">conjecture</a>. He proved that infinitely often is a prime <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> so there is another prime <img alt="{q&gt;p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%3Ep%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q&gt;p}"/> bounded above by <img alt="{p +C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%2BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p +C}"/>. His <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> was <img alt="{70}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{70}"/> million, but this was still a breakthrough. Previously no bounded <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> was known. We discussed this before <a href="https://rjlipton.wordpress.com/2013/05/21/twin-primes-are-useful/">here</a>. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <b> Sensitivity Conjecture </b>: Hao Huang is given as a <a href="http://mentalfloss.com/article/52698/how-does-exception-prove-rule">counterexample</a> to partial progress—it is the exception that proves the rule. He solved the full <a href="https://arxiv.org/pdf/1907.00847.pdf">conjecture</a>. He proved that every <img alt="{2^{n}-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%7D-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{n}-1}"/> vertex induced subgraph of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-dimensional cube graph has maximum degree at least <img alt="{\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{n}}"/>. The previous best was only order logarithm in <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. But there does remain some slack: He proved a fourth-power bound, but can it be closed to cubic or even quadratic? We <a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/">discussed</a> this last year.</p>
<p>
By the <a href="https://www.mentalfloss.com/article/52698/how-does-exception-prove-rule">way</a>: </p>
<blockquote><p><b> </b> <em> The expression comes from the Latin legal principle exceptio probat regulam (the exception proves the rule), also rendered as exceptio firmat regulam (the exception establishes the rule) and exceptio confirmat regulam (the exception confirms the rule). The principle provides legal cover for inferences such as the following: if I see a sign reading “no swimming allowed after 10 pm,” I can assume swimming is allowed before that time. </em>
</p></blockquote>
<p>
</p><p/><h2> Advice To Claimers </h2><p/>
<p/><p>
Our general advice to claimers: </p>
<blockquote><p><b> </b> <em> <i>Okay you are sure you have solved the big problem. Write up the weakest new result that you can.</i> </em>
</p></blockquote>
<p/><p>
Use your methods, your insights, to minimize the work needed for someone to be 99.99% convinced that you have proved something <em>new</em>, rather than a lower confidence of your having proved something <em>huge</em>. For P<img alt="{=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{=}"/>NP show that you have a exponential algorithm that is better than known. Or for P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>NP give a non-linear lower bound. </p>
<p>
The rationale is: You are more likely to get someone to read your paper if you make a weaker claim. A paper titled: <i>A New SAT Algorithm that Runs in Sub-exponential Time</i> is more likely to get readers than a paper tiled <i>P=NP</i>. This shows that readership is non-monotone. </p>
<p>
This is consequence of two phenomena: One is believability. The weaker paper is more likely to be correct. One is human. The stronger paper, if correct, may not be easy to improve. A weaker paper could have results that the reader could improve and write a follow-on paper: </p>
<blockquote><p><b> </b> <em> In Carol Fletcher’s recent paper an <img alt="{2^{n^{1/3}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%5E%7B1%2F3%7D%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{2^{n^{1/3}}}"/> algorithm is found for SAT. She required the full Riemann Hypothesis. We remove that requirement and <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\dots}"/> </em>
</p></blockquote>
<p/><h2> Open Problems </h2><p/>
<p/><p>
What about our advice: what would you do if you solved a major open problem? Note that the examples we highlighted all have slack for improvement short of the optimum statements.</p></font></font></div>
    </content>
    <updated>2020-09-05T17:25:13Z</updated>
    <published>2020-09-05T17:25:13Z</published>
    <category term="News"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Advice"/>
    <category term="open problems"/>
    <category term="proof checking"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-08T00:21:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/129</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/129" rel="alternate" type="text/html"/>
    <title>TR20-129 |  A Lower Bound on Determinantal Complexity | 

	Mrinal Kumar, 

	Ben Lee Volk</title>
    <summary>The determinantal complexity of a polynomial $P \in \mathbb{F}[x_1,  \ldots, x_n]$ over a field $\mathbb{F}$ is the dimension of the smallest matrix $M$ whose entries are affine functions in $\mathbb{F}[x_1,  \ldots, x_n]$ such that $P = Det(M)$. We prove that the determinantal complexity of the polynomial $\sum_{i = 1}^n x_i^n$ is at least $1.5n - 3$. 

For every $n$-variate polynomial of degree $d$, the determinantal complexity is trivially at least $d$, and it is a long standing open problem to prove a lower bound which is super linear in $\max\{n,d\}$. Our result is the first lower bound for any explicit polynomial which is bigger by a constant factor than $\max\{n,d\}$, and improves upon the prior best bound of $n + 1$, proved by Alper, Bogart and Velasco [ABV17] for the same polynomial.</summary>
    <updated>2020-09-05T03:25:44Z</updated>
    <published>2020-09-05T03:25:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-08T00:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5519</id>
    <link href="https://adamsheffer.wordpress.com/2020/09/04/mathematics-for-human-flourishing/" rel="alternate" type="text/html"/>
    <title>Mathematics for Human Flourishing</title>
    <summary>I read a lot of “popular math” books. I also wrote about some in past posts. But I’ve never read a book similar to Mathematics for Human Flourishing by Francis Su. I already knew the math presented in this book and almost all other topics covered. I even knew some of Su’s personal stories from […]</summary>
    <updated>2020-09-04T21:21:41Z</updated>
    <published>2020-09-04T21:21:41Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2020-09-08T00:21:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20205</id>
    <link href="https://gilkalai.wordpress.com/2020/09/04/alef-corner-math-collaboration/" rel="alternate" type="text/html"/>
    <title>Alef Corner: Math Collaboration</title>
    <summary>Another artistic view by Alef on mathematical collaboration.   Other Alef’s corner posts</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Another artistic view by Alef on mathematical collaboration.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/09/mathcoll.jpg"><img alt="" class="alignnone size-full wp-image-20207" height="640" src="https://gilkalai.files.wordpress.com/2020/09/mathcoll.jpg?w=640&amp;h=640" width="640"/></a></p>
<p> </p>
<p>Other <a href="https://gilkalai.wordpress.com/tag/alefs-corner/">Alef’s corner</a> posts</p></div>
    </content>
    <updated>2020-09-04T07:17:13Z</updated>
    <published>2020-09-04T07:17:13Z</published>
    <category term="Art"/>
    <category term="Combinatorics"/>
    <category term="What is Mathematics"/>
    <category term="Alef's corner"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-09-08T00:20:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=52</id>
    <link href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/" rel="alternate" type="text/html"/>
    <title>Friday, Sept 11 — Bin Yu from UC Berkeley</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Friday, Sept 11th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  Bin Yu from UC Berkeley will speak about “Veridical Data Science”. Abstract: Building and expanding on principles of statistics, machine learning, and the sciences, we propose<a class="more-link" href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/">Continue reading <span class="screen-reader-text">"Friday, Sept 11 — Bin Yu from UC Berkeley"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next Foundations of Data Science virtual talk will take place on Friday, Sept 11th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Bin Yu </strong>from UC Berkeley will speak about “<em>Veridical Data Science</em>”.</p>



<p><strong>Abstract</strong>: Building and expanding on principles of statistics, machine learning, and the sciences, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework is comprised of both a workflow and documentation and aims to provide responsible, reliable, reproducible, and transparent results across the entire data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle for the data science life cycle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. We develop inference procedures that build on PCS, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations.</p>



<p>Moreover, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis.</p>



<p>The PCS framework will be illustrated through our DeepTune approach to model and characterize neurons in the difficult visual cortex area V4.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-09-04T00:47:54Z</updated>
    <published>2020-09-04T00:47:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-09-08T00:22:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsmath.wordpress.com/?p=2298</id>
    <link href="https://tcsmath.wordpress.com/2020/09/03/itcs-2021-call-for-papers-deadline-extension/" rel="alternate" type="text/html"/>
    <title>ITCS 2021 Call For Papers (deadline extension)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">NOTE: The deadline has been extended to Tuesday, September 8th to account for the Labor Day Holiday in the US. The 12th Innovations in Theoretical Computer Science (ITCS) conference will be held online from January 6-8, 2021. The submission deadline is now September 8, 2020. The program committee encourages you to send your papers our way! See the call for papers for information about submitting to the … <a class="more-link" href="https://tcsmath.wordpress.com/2020/09/03/itcs-2021-call-for-papers-deadline-extension/">Continue reading <span class="screen-reader-text">ITCS 2021 Call For Papers (deadline extension)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>NOTE</strong>:  The deadline has been extended to Tuesday, September 8th to account for the Labor Day Holiday in the US.</p>



<p>The <strong>12th Innovations in Theoretical Computer Science (ITCS)</strong> conference will be held <strong>online</strong> from <strong>January 6-8, 2021</strong>. The <strong>submission deadline</strong> is now <strong>September 8, 2020</strong>.</p>



<p>The <a href="http://itcs-conf.org/">program committee</a> encourages you to send your papers our way! See the <a href="http://itcs-conf.org/">call for papers</a> for information about submitting to the conference.</p>



<p>ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the greater theory community.</p>



<h3>Important dates</h3>



<ul><li><strong>Submission deadline: </strong>September 8, 2020 (05:59PM PDT)</li><li><strong>Notification to authors:</strong> November 1, 2020</li><li><strong>Conference dates: </strong>January 6-8, 2021</li></ul></div>
    </content>
    <updated>2020-09-03T21:53:56Z</updated>
    <published>2020-09-03T21:53:56Z</published>
    <category term="Math"/>
    <author>
      <name>James</name>
    </author>
    <source>
      <id>https://tcsmath.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsmath.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsmath.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsmath.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsmath.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>some mathematics &amp; computation</subtitle>
      <title>tcs math</title>
      <updated>2020-09-08T00:20:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4949</id>
    <link href="https://www.scottaaronson.com/blog/?p=4949" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4949#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4949" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">My Utility+ podcast with Matthew Putman</title>
    <summary xml:lang="en-US">Update (Sep. 5): Here’s another quantum computing podcast I did, “Dunc Tank” with Duncan Gammie. Enjoy! Thanks so much to Shtetl-Optimized readers, so far we’ve raised $1,371 for the Biden-Harris campaign and $225 for the Lincoln Project, which I intend to match for $3,192 total. If you’d like to donate by tonight (Thursday night), there’s […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Update (Sep. 5):</span></strong> <a href="https://dunctank.podbean.com/e/scott-aaronson-infinite-universes/">Here’s another quantum computing podcast I did</a>, “Dunc Tank” with Duncan Gammie.  Enjoy!</p>



<hr/>
<p><br/>Thanks so much to <em>Shtetl-Optimized</em> readers, so far we’ve raised $1,371 for the <a href="https://secure.actblue.com/donate/duforjoe">Biden-Harris campaign</a> and $225 for the <a href="https://lincolnproject.us/donate/">Lincoln Project</a>, which I intend to match for $3,192 total.  If you’d like to donate by tonight (Thursday night), there’s still $404 to go!</p>



<p>Meanwhile, a mere three days after declaring my <a href="https://www.scottaaronson.com/blog/?p=4942">“new motto,”</a> I’ve come up with a <em>new</em> new motto for this blog, hopefully a more cheerful one:</p>



<blockquote class="wp-block-quote"><p>When civilization seems on the brink of collapse, sometimes there’s nothing left to talk about but maximal separations between randomized and quantum query complexity.</p></blockquote>



<p>On that note, please enjoy my new <a href="https://nanotronics.co/thinkspace/24-scott-aaronson-before-and-after-the-machine/">one-hour podcast on Spotify</a> (if that link doesn’t work, <a href="https://overcast.fm/+UZFvS_CT8">try this one</a>) with <a href="https://en.wikipedia.org/wiki/Matthew_Putman_(scientist)">Matthew Putman</a> of Utility+.  Alas, my umming and ahhing were more frequent than I now aim for, but that’s partly compensated for by Matthew’s excellent decision to speed up the audio.  This was an unusually wide-ranging interview, covering everything from SlateStarCodex to quantum gravity to interdisciplinary conferences to the challenges of teaching quantum computing to 7-year-olds.  I hope you like it!</p></div>
    </content>
    <updated>2020-09-03T16:43:36Z</updated>
    <published>2020-09-03T16:43:36Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-09-05T22:49:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/128</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/128" rel="alternate" type="text/html"/>
    <title>TR20-128 |  An Optimal Separation of Randomized and Quantum Query Complexity | 

	Alexander A. Sherstov, 

	Andrey Storozhenko, 

	Pei Wu</title>
    <summary>We prove that for every decision tree, the absolute values of the Fourier coefficients of given order $\ell\geq1$ sum to at most $c^{\ell}\sqrt{{d\choose\ell}(1+\log n)^{\ell-1}},$ where $n$ is the number of variables, $d$ is the tree depth, and $c&gt;0$ is an absolute constant. This bound is essentially tight and settles a conjecture due to Tal (arxiv 2019; FOCS 2020). The bounds prior to our work degraded rapidly with $\ell,$ becoming trivial already at $\ell=\sqrt{d}.$

As an application, we obtain, for any positive integer $k,$ a partial Boolean function on $n$ bits that has bounded-error quantum query complexity at most $\lceil k/2\rceil$ and randomized query complexity $\tilde{\Omega}(n^{1-1/k}).$ This separation of bounded-error quantum versus randomized query complexity is best possible, by the results of Aaronson and Ambainis (STOC 2015). Prior to our work, the best known separation was polynomially weaker: $O(1)$ versus $n^{2/3-\epsilon}$ for any $\epsilon&gt;0$ (Tal, FOCS 2020).</summary>
    <updated>2020-09-03T13:24:26Z</updated>
    <published>2020-09-03T13:24:26Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-08T00:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/03/post-doc-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-october-26-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/03/post-doc-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-october-26-2020/" rel="alternate" type="text/html"/>
    <title>Post-doc at Basic Algorithms Research Copenhagen at IT University of Copenhagen (apply by October 26, 2020)</title>
    <summary>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a postdoc employed at IT University of Copenhagen (ITU). The ideal candidate will have proven their research ability through an outstanding PhD thesis, and by publishing in leading international venues for algorithms theory. Website: https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181207&amp;DepartmentId=3439&amp;MediaId=5 Email: pagh@itu.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a postdoc employed at IT University of Copenhagen (ITU).</p>
<p>The ideal candidate will have proven their research ability through an outstanding PhD thesis, and by publishing in leading international venues for algorithms theory.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181207&amp;DepartmentId=3439&amp;MediaId=5">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181207&amp;DepartmentId=3439&amp;MediaId=5</a><br/>
Email: pagh@itu.dk</p></div>
    </content>
    <updated>2020-09-03T08:36:42Z</updated>
    <published>2020-09-03T08:36:42Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-08T00:21:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/03/phd-student-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-september-28-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/03/phd-student-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-september-28-2020/" rel="alternate" type="text/html"/>
    <title>PhD student at Basic Algorithms Research Copenhagen at IT University of Copenhagen (apply by September 28, 2020)</title>
    <summary>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a PhD student, to be employed at IT University of Copenhagen. The ideal candidate will have proven potential through e.g.: 1) an outstanding MSc thesis, 2) publishing in leading international venues for algorithms theory, or 3) successful participation in international competitions in informatics or […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a PhD student, to be employed at IT University of Copenhagen.</p>
<p>The ideal candidate will have proven potential through e.g.: 1) an outstanding MSc thesis, 2) publishing in leading international venues for algorithms theory, or 3) successful participation in international competitions in informatics or mathematics.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181206&amp;DepartmentId=3439&amp;MediaId=1282">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181206&amp;DepartmentId=3439&amp;MediaId=1282</a><br/>
Email: pagh@itu.dk</p></div>
    </content>
    <updated>2020-09-03T08:35:16Z</updated>
    <published>2020-09-03T08:35:16Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-08T00:21:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-4174113397967272312</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/4174113397967272312/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=4174113397967272312" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/4174113397967272312" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/4174113397967272312" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2020/09/broad-testing-thank-you.html" rel="alternate" type="text/html"/>
    <title>Broad Testing Thank You</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I have a loose association with the Broad Institute, an institute created so that "complementary expertise of the genomic scientists and the chemical biologists across MIT and Harvard be brought together in one place to drive the transformation of medicine with molecular knowledge."  (See <a href="https://www.broadinstitute.org/history">https://www.broadinstitute.org/history</a> )</p><p>They recently passed an amazing milestone, having performed over 1 million Covid tests.  They weren't set up to be a Covid testing lab, but the converted their institute space to respond to the Covid crisis.  (See <a href="https://covid19-testing.broadinstitute.org/">https://covid19-testing.broadinstitute.org/</a> )  </p><p>In short, they stepped up.  They certainly didn't have to, but they did.  Maybe it will help them do good science, now and in the future.  But my understanding is that they saw a clear societal need (lots of <a href="https://www.amazon.com/gp/product/B07DFZ12KV/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=michaelmitzen-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=B07DFZ12KV&amp;linkId=5a5c250c33b2ba71012ff577c69e1a77">outbreak specialists there</a> ) and they realized they had the expertise and equipment to do good that went beyond science.  I just wanted to give a shout out to the Broad for their good works, scientific and societal.  </p></div>
    </content>
    <updated>2020-09-03T00:32:00Z</updated>
    <published>2020-09-03T00:32:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2020-09-03T00:32:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20194</id>
    <link href="https://gilkalai.wordpress.com/2020/09/02/alef-corner-math-collaboration-2/" rel="alternate" type="text/html"/>
    <title>Alef’s Corner: Math Collaboration 2</title>
    <summary>Other Alef’s corner posts</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2020/09/alef.jpg"><img alt="alef-math-collaboration" class="alignnone size-full wp-image-20202" src="https://gilkalai.files.wordpress.com/2020/09/alef-math-collaboration.jpg?w=640"/></a></p>
<p>Other <a href="https://gilkalai.wordpress.com/tag/alefs-corner/">Alef’s corner</a> posts</p></div>
    </content>
    <updated>2020-09-02T13:32:45Z</updated>
    <published>2020-09-02T13:32:45Z</published>
    <category term="Art"/>
    <category term="Combinatorics"/>
    <category term="Geometry"/>
    <category term="What is Mathematics"/>
    <category term="Alef's corner"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-09-08T00:20:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra</id>
    <link href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html" rel="alternate" type="text/html"/>
    <title>Isosceles polyhedra</title>
    <summary>My latest arXiv preprint is “On polyhedral realization with isosceles triangles”, arXiv:2009.00116. As the title suggests, it studies polyhedra whose faces are all isosceles triangles. Despite several new results in it, there’s a lot I still don’t know. The paper finds a sort-of-new1 infinite family of polyhedra with congruent isosceles faces, shown below, but I don’t know if there are any more such families. The family of polyhedra from the first image is only “sort-of-new” because the same combinatorial structure was previously described as a triangulation of the sphere by congruent spherical isosceles triangles: Dawson, Robert J. MacG. (2005), “Some new tilings of the sphere with congruent triangles”, Renaissance Banff. In exchange for re-purposing Dawson’s triangulation, my paper describes another infinite family of spherical triangulations by congruent spherical isosceles triangles, not given by Dawson, based on applying a similar \(2\pi/3\) twist to an infinite family of non-convex bipyramids with congruent isosceles faces like the one below. Again, I don’t know whether there are other such families of spherical triangulations. ↩</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest arXiv preprint is “On polyhedral realization with isosceles triangles”, <a href="http://arxiv.org/abs/2009.00116">arXiv:2009.00116</a>. As the title suggests, it studies polyhedra whose faces are all isosceles triangles. Despite several new results in it, there’s a lot I still don’t know. The paper finds a sort-of-new<sup id="fnref:1"><a class="footnote" href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fn:1">1</a></sup> infinite family of polyhedra with congruent isosceles faces, shown below, but I don’t know if there are any more such families.</p>

<p style="text-align: center;"><img alt="Twisted augmented bipyramid with isosceles-triangle faces" src="https://11011110.github.io/blog/assets/2020/twisted.svg"/></p>

<p>One of the other previously known families, shown below, has two integer parameters (the numbers of sides on the two half-bipyramids it combines), but I don’t know whether the same double parameterization is possible for the new family.</p>

<p style="text-align: center;"><img alt="Combination of two half-bipyramids with isosceles-triangle faces" src="https://11011110.github.io/blog/assets/2020/biarc.svg" width="80%"/></p>

<p>In 2001, Branko Grünbaum published an example of a polyhedron that could not be realized with congruent faces, even non-convexly,<sup id="fnref:2"><a class="footnote" href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fn:2">2</a></sup> but it can be realized as a convex polyhedron with all faces isosceles (or equilateral), as shown below. I don’t know whether there are polyhedra that cannot be realized with all faces isosceles, if one allows the realization to be non-convex (but non-self-crossing and combinatorially equivalent to a convex polyhedron) and the faces to be non-congruent.</p>

<p style="text-align: center;"><img alt="Gr&#xFC;nbaum's example of a polyhedron that cannot be realized with congruent faces, realized convexly with isosceles and equilateral triangle faces" src="https://11011110.github.io/blog/assets/2020/grunbaum.svg"/></p>

<p>My new preprint proves that there exist polyhedra (iterated Kleetopes) that cannot be realized as convex polyhedra with isosceles faces. But the construction is a little non-explicit and I don’t know how complicated these polyhedra need to be. For instance, I don’t know whether there is a convex isosceles-face realization of the double Kleetope of the octahedron, shown below.</p>

<p style="text-align: center;"><img alt="Double Kleetope of an octahedron" src="https://11011110.github.io/blog/assets/2020/kkoct.svg"/></p>

<p>Grünbaum’s example can be realized convexly with only two edge lengths, and my non-isosceles-faced polyhedra require at least three edge lengths in any convex realization. I don’t know whether the number of required edge lengths can be unbounded, or whether non-convex realizations ever require three lengths (although certain stacked polyhedra require at least two).</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The family of polyhedra from the first image is only “sort-of-new” because the same combinatorial structure was previously described as a triangulation of the sphere by congruent spherical isosceles triangles: Dawson, Robert J. MacG. (2005), “<a href="https://archive.bridgesmathart.org/2005/bridges2005-489.html">Some new tilings of the sphere with congruent triangles</a>”, Renaissance Banff. In exchange for re-purposing Dawson’s triangulation, my paper describes another infinite family of spherical triangulations by congruent spherical isosceles triangles, not given by Dawson, based on applying a similar \(2\pi/3\) twist to an infinite family of non-convex bipyramids with congruent isosceles faces like the one below. Again, I don’t know whether there are other such families of spherical triangulations.</p>

      <p style="text-align: center;"><img alt="Non-convex polyhedron with congruent isosceles-triangle faces" src="https://11011110.github.io/blog/assets/2020/nwb.svg"/> <a class="reversefootnote" href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>Grünbaum, Branko (2001), “<a href="https://sites.math.washington.edu/~grunbaum/Nonequifacettablesphere.pdf">A convex polyhedron which is not equifacettable</a>”, <em>Geombinatorics</em> 10: 165–171. I don’t know how to access old papers on this journal in general, but fortunately Grünbaum made his one available on his web site. <a class="reversefootnote" href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fnref:2">↩</a></p>
    </li>
  </ol>
</div></div>
    </content>
    <updated>2020-09-01T23:18:00Z</updated>
    <published>2020-09-01T23:18:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-09-02T06:49:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4407</id>
    <link href="https://lucatrevisan.wordpress.com/2020/09/01/time-flies-when-the-world-is-falling-apart/" rel="alternate" type="text/html"/>
    <title>Time Flies When the World is Falling Apart</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">It has been exactly one year since I moved to Italy. Coming here a year ago, I expected that I would face some challenges and that there would be major life changes. Obviously, I did not know the half of … <a href="https://lucatrevisan.wordpress.com/2020/09/01/time-flies-when-the-world-is-falling-apart/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>It has been exactly one year since I moved to Italy.</p>
<p>Coming here a year ago, I expected that I would face some challenges and that there would be major life changes. Obviously, I did not know the half of it.</p>
<p><span id="more-4407"/></p>
<p>In March and April, being in the hottest hot spot of a global pandemic was not ideal. Something rather unexpected, however, has happened. Italy is not known for very effective governance and Italians are not known as a rule-following people. Yet the government implemented a strict lockdown, and held it long enough that the “reopening” took place with a safely low circulation of the virus. People have been mostly following the rules after the reopening, and for most of the summer Italy has been doing quite well. (There has been a recent surge of new cases  in the last few weeks, so it may be too soon to declare victory, but the numbers of intensive care hospitalizations and of deaths are still low).</p>
<p>Meanwhile, there have been “second waves” in several other European countries and the situation in the United States, as the President memorably said, is what it is.</p>
<p>This picture by Noah Berger for AP has been circulating as the perfect description of what is going on in Northern California this summer</p>
<p><img alt="IMG_0077" class="alignnone size-full wp-image-4409" src="https://lucatrevisan.files.wordpress.com/2020/09/img_0077.jpeg?w=584"/></p>
<p>This New York Times headline also does a good job of packing at least five things that are wrong in California right now (the spread of covid-19, the overcrowding of prisons because of excessive sentencing, the budget cuts affecting firefighting efforts, the exploitation of prisoners for underpaid work, and the spread of wildfires) in a single sentence:</p>
<p><img alt="california-inmates" class="alignnone size-full wp-image-4410" src="https://lucatrevisan.files.wordpress.com/2020/09/california-inmates.png?w=584"/></p>
<p>At the cost of resorting to banalities and cliches, Italians often do badly when confronted with good opportunities and lucky breaks, that are often wasted, but we do unexpectedly well in times of crisis. This can even be seen in the national football team, that is known for epic come-from-behind victories and for missing penalty kicks. The American spirit is to be ambitious and optimistic, and to pursue high-risk high-reward opportunities when they present themselves. The flip side is that adversity is often met in America with either despair or anger, typically counterproductively. All things considered, I am happy that I am getting to spend 2020 in Italy. Earlier this summer, the president and the rector of Bocconi announced the new long-term strategic plan for the university, which involves creating a new computer science department. If/when such plans come to fruition, there will be several faculty positions in computer science, at internationally competitive salaries, with advantageous taxation for people moving to Italy from other countries, and with English as the language of instructions, so maybe you too will consider such a move.</p>
<p>Bocconi is preparing to restart in-person teaching in a couple of weeks. The plan is to record and post lectures, to allow students to attend lectures remotely if they prefer, and to use classrooms at most at half capacity. If a class has to be scheduled in a classroom whose capacity is less than twice the class enrollment, students will be split in two groups. Each group will (be allowed to) physically attend in alternating weeks, and will (be required to) attend online at other times.</p>
<p>The space between campus buildings has been marked with well-spaced walking paths, and there are even pedestrian roundabouts.</p>
<p><img alt="IMG_0122" class="alignnone size-full wp-image-4412" src="https://lucatrevisan.files.wordpress.com/2020/09/img_0122.jpeg?w=584"/></p>
<p>Are we heading for a second-wave disaster? Will students really follow the rules? On Sunday we were in Rome and we took a bus to the city center. Every other seat in the bus was marked with a “do not sit” sign to create distancing between sitting people. All marked seats were empty even as the bus was filling up and several people were standing. Then a group of German tourists got in, and they sat in all the forbidden seats. This is the upside-down world of 2020, all bets are off.</p></div>
    </content>
    <updated>2020-09-01T17:36:01Z</updated>
    <published>2020-09-01T17:36:01Z</published>
    <category term="Bocconi"/>
    <category term="Italy"/>
    <category term="philosophy"/>
    <category term="politics"/>
    <category term="things that are terrible"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-09-08T00:20:16Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4647242799583898034</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4647242799583898034/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/a-well-known-theorem-that-has-not-been.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4647242799583898034" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4647242799583898034" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/a-well-known-theorem-that-has-not-been.html" rel="alternate" type="text/html"/>
    <title>A well known theorem that has not been written down- so I wrote it down- CLIQ is #P-complete</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>(The two proofs that CLIQ is #P-complete that I discuss in this blog are written up by Lance and myself and are <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/sharpclique.pdf">here</a>. I think both are well known but I have not been able to find a writeup,so Lance and I did one.)</p><p><br/></p><p> I have been looking at #P (see my last blog on it it, <a href="https://blog.computationalcomplexity.org/2020/08/sharp-p-and-issue-of-natural-problems.html">here</a>, for a refresher on this topic) since I will be teaching this topic in Spring 2021.  Recall</p><p>#SAT(\phi) = the number of satisfying assignments for phi</p><p>#CLIQ((G,k))= the number of cliques of size \ge k of G</p><p>#SAT is #P-complete by a cursory look at the proof of the Cook-Levin theorem.</p><p>A function is #P-complete if everything else in #P is Turing-Poly red to it. To show for some set A </p><p>in NP, #A is #P-complete one usually uses pars reductions. </p><p>I wanted a proof that #CLIQ is #P-complete, so I wanted a parsimonious reduction from SAT to CLIQ (Thats a reduction f: SAT--&gt; CLIQ such that the number of satisfying assignments of phi equals the number of cliques of size \ge k of G.)</p><p>I was sure there is such a reduction and that it was well known and would be on the web someplace. So I tried to find it.</p><p>ONE:</p><p>I tracked some references to a paper by Janos Simon (<a href="https://link.springer.com/content/pdf/10.1007%2F3-540-08342-1_37.pdf">see here</a>)  where he claims that the reduction of SAT to CLIQ  by Karp <a href="https://blog.computationalcomplexity.org/feeds/posts/{http://cgi.di.uoa.gr/~sgk/teaching/grad/handouts/karp.pdf">(see here)</a> was pars.  I had already considered that and decided that Karps reduction was NOT pars.  I looked at both Simon's paper and Karp's paper to make sure I wasn't missing something (e.g., I misunderstood what Simon Says or what Karp ... darn, I can't think of anything as good as `Simon Says'). It seemed to me that Simon was incorrect. If I am wrong let me know.</p><p>TWO</p><p>Someone told me that it was in Valiant's  paper (see <a href="https://epubs.siam.org/doi/10.1137/0208032">here</a>). Nope. Valiant's paper shows that counting the number of maximal cliques is #P-complete. Same Deal Here- if I am wrong let me know. One can modify Valiant's argument to get #CLIQ #P-complete, and I do so in the write up. The proof is a string of reductions and starts with PERM is #P-complete. This does prove that# CLIQ is  #P-complete, but is rather complicated. Also, the reduction is not pars.</p><p>THREE <br/>Lance told me an easy pars reduction that is similar to Karp's non-pars reduction, but it really is NOT Karp's reduction. Its in my write up. I think it is well known since I recall hearing it about 10 years ago. From Lance. Hmmm, maybe its just well known to Lance.</p><p><br/></p><p>But here is my question: I am surprised I didn't find it on the web. If someone can point to a place on the web where it is, please let me know. In any case, its on the web NOW (my write up) so hopefully in the future someone else looking for it can find it.</p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2020-09-01T15:29:00Z</updated>
    <published>2020-09-01T15:29:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-07T22:17:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/01/quics-hartree-postdoctoral-fellowships-at-joint-center-for-quantum-information-and-computer-science-apply-by-december-1-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/01/quics-hartree-postdoctoral-fellowships-at-joint-center-for-quantum-information-and-computer-science-apply-by-december-1-2020/" rel="alternate" type="text/html"/>
    <title>QuICS Hartree Postdoctoral Fellowships at Joint Center for Quantum Information and Computer Science (apply by December 1, 2020)</title>
    <summary>The Joint Center for Quantum Information and Computer Science (QuICS, http://quics.umd.edu) is seeking exceptional candidates for the QuICS Hartree Postdoctoral Fellowships in Quantum Information and Computer Science. Applications should be submitted through AcademicJobsOnline at https://academicjobsonline.org/ajo/jobs/16778. Website: https://academicjobsonline.org/ajo/jobs/16778 Email: quics-coordinator@umiacs.umd.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Joint Center for Quantum Information and Computer Science (QuICS, <a href="http://quics.umd.edu">http://quics.umd.edu</a>) is seeking exceptional candidates for the QuICS Hartree Postdoctoral Fellowships in Quantum Information and Computer Science.<br/>
Applications should be submitted through AcademicJobsOnline at <a href="https://academicjobsonline.org/ajo/jobs/16778.">https://academicjobsonline.org/ajo/jobs/16778.</a></p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/16778">https://academicjobsonline.org/ajo/jobs/16778</a><br/>
Email: quics-coordinator@umiacs.umd.edu</p></div>
    </content>
    <updated>2020-09-01T14:39:18Z</updated>
    <published>2020-09-01T14:39:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-08T00:21:09Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-25562705.post-7958692547156288391</id>
    <link href="http://aaronsadventures.blogspot.com/feeds/7958692547156288391/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=7958692547156288391" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/7958692547156288391" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/7958692547156288391" rel="self" type="application/atom+xml"/>
    <link href="http://aaronsadventures.blogspot.com/2020/07/the-existence-of-no-regret-learning.html" rel="alternate" type="text/html"/>
    <title>No Regret Algorithms from the Min Max Theorem</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The existence of no-regret learning algorithms can be used to prove <a href="https://en.wikipedia.org/wiki/Minimax_theorem">Von-Neumann's min-max theorem</a>. This argument is originally due to <a href="https://www.cs.princeton.edu/~schapire/papers/FreundScYY.pdf">Freund and Schapire</a>, and I <a href="https://www.cis.upenn.edu/~aaroth/courses/slides/agt20/lect08.pdf">teach it to my undergraduates </a>in my algorithmic game theory class. The min-max theorem also can be used to prove the existence of no-regret learning algorithms. Here is a constructive version of the argument (Constructive in that in the resulting algorithm, you only need to solve polynomially sized zero-sum games, so you can do it via linear programming).<div><br/></div><div>Recall the setting. Play proceeds in rounds $t \in \{1,\ldots,T\}$. At each day $t$, the learner chooses one of $k$ actions $i_t \in \{1,\ldots,k\}$, and the adversary chooses a loss vector $\ell^t \in [0,1]^k$. The learner incurs loss $\ell^t_{i_t}$, corresponding to the action he chose. At the end of the interaction, the regret of the learner is defined to be the difference between the cumulative loss he incurred and the cumulative loss of the best fixed action (played consistently)  in hindsight:</div><div>$$\textrm{Regret}_T = \max_j \left(\sum_{t=1}^T \ell^t_{i_t} - \ell^t_j\right)$$ </div><div>A classical and remarkable result is that there exist algorithms that can guarantee that regret grows only sublinearly with time: $\textrm{Regret}_T = O(\sqrt{T})$. Lets prove this. </div><div><br/></div><div><br/></div><div>Define the non-negative portion of our cumulative regret with respect to action $j$ up until day $d$ as:</div><div>$$V_d^j = \left(\sum_{t=1}^d\left(\ell^t_{i_t} - \ell^t_j\right)\right)^+$$</div><div>and our additional regret at day $d+1$ with respect to action $j$ as:</div><div>$$r_{j}^{d+1} = \ell_{i_{d+1}}^{d+1} - \ell^{d+1}_j$$</div><div>Observe that if $V_{d}^j \geq 1$  then $V_{d+1}^j = V_d^j + r_j^{d+1}$. </div><div><br/></div><div><br/></div><div>Define a surrogate loss function as our squared cumulative regrets, summed over all actions: </div><div>$$L_d = \sum_{j=1}^k (V_d^j)^2$$</div><div>Observe that we can write the expected gain in our loss on day $d+1$, conditioned on the history thus far:</div><div>$$\mathbb{E}[L_{d+1} - L_d] \leq \sum_{j : V_d^j \geq 1} \mathbb{E}[(V_d^j+r_j^{d+1})^2 - (V_d^j)^2) ] + 3k$$</div><div>$$= \sum_{j : V_d^j \geq 1} \left(2V_d^j \mathbb{E}[r_{j}^{d+1}] + \mathbb{E}[(r_{j}^{d+1})^2]\right) + 3k $$</div><div>$$\leq \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right) + 4k$$</div><div>where the expectations are taken over the randomness of both the learner and the adversary in round $d+1$. </div><div><br/></div><div>Now consider a zero-sum game played between the learner and the adversary in which the learner is the minimization player, the adversary is the maximization player, and the utility function is $$u(i_{d+1}, \ell^{d+1}) = \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right)$$ The min-max theorem says that the learner can guarantee the same payoff for herself in the following two scenarios:</div><div><br/></div><div><ol style="text-align: left;"><li>The learner first has to commit to playing a distribution $p_{d+1}$ over actions $i$, and then the adversary gets to best respond by picking the worst possible loss vectors, or</li><li>The adversary has to first commit to a distribution over loss vectors $\ell$ and then the learner gets the benefit of picking the best action $i_{d+1}$ to respond with. </li></ol>Scenario 1) is the scenario our learner finds herself in, when playing against an adaptive adversary. But 2) is much easier to analyze. If the adversary first commits to a distribution over loss vectors $\ell^{d+1}$, the learner can always choose action $i_{d+1} = \arg\min_j \mathbb{E}[\ell^{d+1}_j]$, which guarantees that $\mathbb{E}[r_{j}^{d+1}] \leq 0$, which in turn guarantees that the value of the game $ \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right) \leq 0$.  Hence, the min-max theorem tells us that the learner always has a distribution over actions $p_{d+1}$ that guarantees that $\mathbb{E}[L_{d+1} - L_d] \leq 4k$, <i>even in the worst case over loss functions</i>. If the learner always plays according to this distribution, then by a telescoping sum, we have that:</div><div>$$\mathbb{E}[L_T] \leq 4kT$$.</div><div>We therefore have by Jensen's inequality that:</div><div>$$\mathbb{E}[\max_j (V^j_T)] \leq \sqrt{\mathbb{E}[\max_j (V^j_T)^2]}\leq \sqrt{\mathbb{E}[\sum_{j=1}^k (V_j^T)^2]} \leq 2\sqrt{kT}$$.</div></div>
    </content>
    <updated>2020-09-01T14:16:00Z</updated>
    <published>2020-09-01T14:16:00Z</published>
    <author>
      <name>Aaron</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/09952936358739421126</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-25562705</id>
      <category term="game theory"/>
      <category term="news"/>
      <author>
        <name>Aaron</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/09952936358739421126</uri>
      </author>
      <link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <title>Adventures in Computation</title>
      <updated>2020-09-03T08:11:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/01/postdoc-at-tu-hamburg-apply-by-september-30-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/01/postdoc-at-tu-hamburg-apply-by-september-30-2020/" rel="alternate" type="text/html"/>
    <title>postdoc at TU Hamburg (apply by September 30, 2020)</title>
    <summary>The Institute for Algorithms and Complexity at TUHH: Hamburg University of Technology invites applications for a 5-year postdoc position (attached to DASHH: Data Science in Hamburg). The focus is on algorithms, applied to combinatorial optimization, operations research, and discrete mathematics in the natural sciences, particularly on big data obtained at Germany’s largest particle accelerator. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Institute for Algorithms and Complexity at TUHH: Hamburg University of Technology invites applications for a 5-year postdoc position (attached to DASHH: Data Science in Hamburg). The focus is on algorithms, applied to combinatorial optimization, operations research, and discrete mathematics in the natural sciences, particularly on big data obtained at Germany’s largest particle accelerator.</p>
<p>Website: <a href="https://www.tuhh.de/algo/jobs.html">https://www.tuhh.de/algo/jobs.html</a><br/>
Email: matthias.mnich@tuhh.de</p></div>
    </content>
    <updated>2020-09-01T08:46:00Z</updated>
    <published>2020-09-01T08:46:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-08T00:21:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/icml2020/</id>
    <link href="https://differentialprivacy.org/icml2020/" rel="alternate" type="text/html"/>
    <title>Conference Digest - ICML 2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://icml.cc/virtual/2020">ICML 2020</a> is one of the premiere venues in machine learning, and generally features a lot of great work in differentially private machine learning.
This year is no exception: the relevant papers are listed below to the best of our ability, including links to the full versions of papers, as well as the conference pages (which contain slides and 15 minute videos for each paper).
As always, please inform us if we overlooked any papers on differential privacy.</p>

<h2 id="papers">Papers</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1909.12732">Alleviating Privacy Attacks via Causal Learning</a> (<a href="https://icml.cc/virtual/2020/poster/6346">page</a>)<br/>
<a href="https://www.microsoft.com/en-us/research/people/shtople/">Shruti Tople</a>, <a href="http://www.amitsharma.in/">Amit Sharma</a>, <a href="https://www.microsoft.com/en-us/research/people/adityan/">Aditya Nori</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1805.10341">An end-to-end Differentially Private Latent Dirichlet Allocation Using a Spectral Algorithm</a> (<a href="https://icml.cc/virtual/2020/poster/6240">page</a>)<br/>
<a href="https://github.com/dpeng817">Chris DeCarolis</a>, <a href="https://twitter.com/exsidius">Mukul Ram</a>, <a href="https://www.cs.umd.edu/people/sesmaeil">Seyed Esmaeili</a>, <a href="https://sites.cs.ucsb.edu/~yuxiangw/">Yu-Xiang Wang</a>, <a href="http://furong-huang.com/">Furong Huang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1901.09697">Bayesian Differential Privacy for Machine Learning</a> (<a href="https://icml.cc/virtual/2020/poster/6547">page</a>)<br/>
<a href="https://scholar.google.com/citations?user=BCWx7iQAAAAJ">Aleksei Triastcyn</a>, <a href="https://people.epfl.ch/boi.faltings">Boi Faltings</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.03030">Certified Data Removal from Machine Learning Models</a> (<a href="https://icml.cc/virtual/2020/poster/5895">page</a>)<br/>
<a href="https://sites.google.com/view/chuanguo">Chuan Guo</a>, <a href="https://www.cs.umd.edu/~tomg/">Tom Goldstein</a>, <a href="https://awnihannun.com/">Awni Hannun</a>, <a href="https://lvdmaaten.github.io/">Laurens van der Maaten</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.00038">Context Aware Local Differential Privacy</a> (<a href="https://icml.cc/virtual/2020/poster/5775">page</a>)<br/>
<a href="https://people.ece.cornell.edu/acharya/">Jayadev Acharya</a>, <a href="https://research.google/people/105175/">Kallista Bonawitz</a>, <a href="https://kairouzp.github.io/">Peter Kairouz</a>, <a href="https://research.google/people/106777/">Daniel Ramage</a>, <a href="http://www.zitengsun.com/">Ziteng Sun</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.12813">Data-Dependent Differentially Private Parameter Learning for Directed Graphical Models</a> (<a href="https://icml.cc/virtual/2020/poster/6262">page</a>)<br/>
<a href="https://scholar.google.com/citations?user=lWWAZ4YAAAAJ">Amrita Roy Chowdhury</a>, <a href="http://pages.cs.wisc.edu/~thodrek/">Theodoros Rekatsinas</a>, <a href="http://pages.cs.wisc.edu/~jha/">Somesh Jha</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.09745">Differentially Private Set Union</a> (<a href="https://icml.cc/virtual/2020/poster/6541">page</a>)<br/>
<a href="https://www.microsoft.com/en-us/research/people/sigopi/">Sivakanth Gopi</a>, <a href="https://www.linkedin.com/in/pankajgulhane/">Pankaj Gulhane</a>, <a href="https://www.microsoft.com/en-us/research/people/jakul/">Janardhan Kulkarni</a>, <a href="https://heyyjudes.github.io/">Judy Hanwen Shen</a>, <a href="https://www.microsoft.com/en-us/research/people/milads/">Milad Shokouhi</a>, <a href="http://www.yekhanin.org/">Sergey Yekhanin</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.11651">Fair Learning with Private Demographic Data</a> (<a href="https://icml.cc/virtual/2020/poster/6499">page</a>)<br/>
<a href="https://husseinmozannar.github.io/">Hussein Mozannar</a>, <a href="https://sites.google.com/site/mesrob/home/">Mesrob Ohannessian</a>, <a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.15744">Fast and Private Submodular and $k$-Submodular Functions Maximization with Matroid Constraint</a> (<a href="https://icml.cc/virtual/2020/poster/6365">page</a>)<br/>
<a href="https://dblp.org/pid/166/1694.html">Akbar Rafiey</a>, <a href="http://research.nii.ac.jp/~yyoshida/">Yuichi Yoshida</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.00706">(Locally) Differentially Private Combinatorial Semi-Bandits</a> (<a href="https://icml.cc/virtual/2020/poster/6315">page</a>)<br/>
<a href="https://scholar.google.com/citations?user=sioumZAAAAAJ">Xiaoyu Chen</a>, <a href="https://scholar.google.com/citations?user=Bw-WdyUAAAAJ">Kai Zheng</a>, <a href="https://twitter.com/zixinjackzhou">Zixin Zhou</a>, <a href="https://scholar.google.com/citations?user=m8m9nD0AAAAJ">Yunchang Yang</a>, <a href="https://www.microsoft.com/en-us/research/people/weic/">Wei Chan</a>, <a href="http://www.liweiwang-pku.com/">Liwei Wang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.05453">New Oracle-Efficient Algorithms for Private Synthetic Data Release</a> (<a href="https://icml.cc/virtual/2020/poster/5814">page</a>)<br/>
<a href="https://sites.google.com/umn.edu/giuseppe-vietri/home">Giuseppe Vietri</a>, <a href="https://scholar.google.com/citations?user=dDVIyEQAAAAJ">Grace Tian</a>, <a href="https://cs-people.bu.edu/mbun/">Mark Bun</a>, <a href="http://www.thomas-steinke.net/">Thomas Steinke</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/1190-Paper.pdf">On Differentially Private Stochastic Convex Optimization with Heavy-tailed Data</a> (<a href="https://icml.cc/virtual/2020/poster/5948">page</a>)<br/>
<a href="http://www.acsu.buffalo.edu/~dwang45/">Di Wang</a>, <a href="https://scholar.google.com/citations?user=e3ZhEDEAAAAJ">Hanshen Xiao</a>, <a href="https://people.csail.mit.edu/devadas/">Srinivas Devadas</a>, <a href="https://cse.buffalo.edu/~jinhui/">Jinhui Xu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1909.13830">Optimal Differential Privacy Composition for Exponential Mechanisms</a> (<a href="https://icml.cc/virtual/2020/poster/6687">page</a>)<br/>
<a href="https://www.math.upenn.edu/~jinshuo/">Jinshuo Dong</a>, <a href="https://dblp.org/pid/155/9794.html">David Durfee</a>, <a href="https://scholar.google.com/citations?user=jr7gGB4AAAAJ">Ryan Rogers</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1909.01783">Oracle Efficient Private Non-Convex Optimization</a> (<a href="https://icml.cc/virtual/2020/poster/5815">page</a>)<br/>
<a href="https://sethneel.com/">Seth Neel</a>, <a href="https://www.cis.upenn.edu/~aaroth/">Aaron Roth</a>, <a href="https://sites.google.com/umn.edu/giuseppe-vietri/home">Giuseppe Vietri</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2341-Paper.pdf">Private Counting from Anonymous Messages: Near-Optimal Accuracy with Vanishing Communication Overhead</a> (<a href="https://icml.cc/virtual/2020/poster/6134">page</a>)<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, <a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>, <a href="https://pasin30055.github.io/">Pasin Manurangsi</a>, <a href="https://www.itu.dk/people/pagh/">Rasmus Pagh</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/6298-Paper.pdf">Private Outsourced Bayesian Optimization</a> (<a href="https://icml.cc/virtual/2020/poster/6783">page</a>)<br/>
<a href="https://scholar.google.com/citations?user=7_2XTQ8AAAAJ">Dmitrii Kharkovskii</a>, <a href="https://daizhongxiang.github.io/">Zhongxiang Dai</a>, <a href="https://www.comp.nus.edu.sg/~lowkh/research.html">Bryan Kian Hsiang Low</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.10941">Private Query Release Assisted by Public Data</a> (<a href="https://icml.cc/virtual/2020/poster/6329">page</a>)<br/>
<a href="https://sites.google.com/view/rbassily">Raef Bassily</a>, <a href="https://www.ccs.neu.edu/home/albertcheu/">Albert Cheu</a>, <a href="http://www.cs.technion.ac.il/~shaymrn/">Shay Moran</a>, <a href="http://www.cs.toronto.edu/~anikolov/">Aleksandar Nikolov</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2453-Paper.pdf">Private Reinforcement Learning with PAC and Regret Guarantees</a> (<a href="https://icml.cc/virtual/2020/poster/6152">page</a>)<br/>
<a href="https://sites.google.com/umn.edu/giuseppe-vietri/home">Giuseppe Vietri</a>, <a href="https://borjaballe.github.io/">Borja Balle</a>, <a href="https://people.cs.umass.edu/~akshay/">Akshay Krishnamurthy</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1910.01327">Privately Detecting Changes in Unknown Distributions</a> (<a href="https://icml.cc/virtual/2020/poster/5854">page</a>)<br/>
<a href="https://sites.gatech.edu/rachel-cummings/">Rachel Cummings</a>, <a href="https://sites.google.com/view/skrehbiel/home">Sara Krehbiel</a>, <a href="https://scholar.google.com/citations?user=ayasb_wAAAAJ">Yuliia Lut</a>, <a href="https://wanrongz.github.io/">Wanrong Zhang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.09463">Privately Learning Markov Random Fields</a> (<a href="https://icml.cc/virtual/2020/poster/5776">page</a>)<br/>
<a href="https://huanyuzhang.github.io/">Huanyu Zhang</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://www.microsoft.com/en-us/research/people/jakul/">Janardhan Kulkarni</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1903.09822">Scalable Differential Privacy with Certified Robustness in Adversarial Learning</a> (<a href="https://icml.cc/virtual/2020/poster/6401">page</a>)<br/>
<a href="https://sites.google.com/site/ihaiphan/">NhatHai Phan</a>, <a href="https://www.cise.ufl.edu/~mythai/">My T. Thai</a>, <a href="https://scholar.google.com/citations?user=OgXtPDIAAAAJ">Han Hu</a>, <a href="http://www.cs.kent.edu/~jin/">Ruoming Jin</a>, <a href="https://research.adobe.com/person/tong-sun/">Tong Sun</a>, <a href="https://ix.cs.uoregon.edu/~dou/">Dejing Dou</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.04493">Sharp Composition Bounds for Gaussian Differential Privacy via Edgeworth Expansion</a> (<a href="https://icml.cc/virtual/2020/poster/6734">page</a>)<br/>
<a href="https://enosair.github.io/">Qinqing Zheng</a>, <a href="https://www.math.upenn.edu/~jinshuo/">Jinshuo Dong</a>, <a href="https://www.med.upenn.edu/apps/faculty/index.php/g275/p8939931">Qi Long</a>, <a href="http://www-stat.wharton.upenn.edu/~suw/">Weijie J. Su</a></p>
  </li>
</ul></div>
    </summary>
    <updated>2020-08-31T18:00:00Z</updated>
    <published>2020-08-31T18:00:00Z</published>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-09-07T23:24:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17496</id>
    <link href="https://rjlipton.wordpress.com/2020/08/31/mea-culpa/" rel="alternate" type="text/html"/>
    <title>Mea Culpa</title>
    <summary>Plus more on the separating word problem Mea Culpa is not someone we introduced on the blog before. She does not come from the same world as Lofa Polir or Neil L. As in the French movie poster at right, the meaning is “my fault.” Today we apologize for some oversights and hasty omissions regarding […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Plus more on the separating word problem</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2020/08/31/mea-culpa/mea/" rel="attachment wp-att-17499"><img alt="" class="alignright wp-image-17499" height="147" src="https://rjlipton.files.wordpress.com/2020/08/mea.jpg?w=110&amp;h=147" width="110"/></a></p>
<p>
Mea Culpa is not someone we introduced on the blog before. She does not come from the same world as Lofa Polir or Neil L. As in the French <a href="https://en.wikipedia.org/wiki/Mea_Culpa_(film)">movie</a> poster at right, the meaning is “my fault.”</p>
<p/><p>
Today we apologize for some oversights and hasty omissions regarding the last <a href="https://rjlipton.wordpress.com/2020/08/29/20000-comments-and-more/">post</a>. Then we will explain Dick’s “laws with errors”.<br/>
<span id="more-17496"/></p>
<p>
Let’s be clear. The paper we discussed in the last post claiming an order <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log n}"/> upper bound is wrong. The best upper bound is still due to Zachary Chase and is order <img alt="{n^{1/3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B1%2F3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^{1/3}}"/>, with some log’s. </p>
<p>
At GLL we try to be fair and upbeat, especially with proof attempts. We know how hard it is to put yourself out there when you claim a new result. Whether for a famous open problem or not, it is always difficult. Mistakes are possible, miss understandings are possible. </p>
<p>
But perhaps this time we have gone too far, and for that we apologize. We just love the SWP, and want to reiterate the idea the post was supposed to be (only) about, an approach to SWP. </p>
<p>
There is also Nostra Culpa—“our fault,” Ken says too.  Not only is there a short <a href="https://www.imdb.com/title/tt8513100/">film</a> with that title, there is a 2013 one-singer <a href="https://www.youtube.com/watch?v=t2paj1mrCQI">opera</a> of that title with libretto drawn from a 2012 Twitter <a href="https://news.err.ee/106193/nostra-culpa-an-interview-with-the-writers-of-a-financial-opera">feud</a> between the economist Paul Krugman and the president of Estonia. At least our issue did not break out on Twitter. We were instead contacted privately by some friends who knew more about the erroneous paper highlighted in our post. We could have contacted them first—that was our omission.</p>
<p>
Again sorry for any confusion we created. Let’s turn to the approach we want to share about SWP.</p>
<p>
</p><p/><h2> Laws with Errors </h2><p/>
<p/><p>
I think there is hope that positive laws must be large, especially if we extend what it mean by a law. Consider a positive law 	</p>
<p align="center"><img alt="\displaystyle  U = V, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++U+%3D+V%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  U = V, "/></p>
<p>where <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> and <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/> are words over the letters <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>. Recall we can tell if <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> and <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/> are different provided there is a finite group <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> so that for some <img alt="{A,B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B}"/> in <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> the value of <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> and <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/> are different. This can be computed by a FSA with at most order <img alt="{|G|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CG%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|G|}"/> states. So the smaller <img alt="{|G|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CG%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|G|}"/> is the better. </p>
<p>
The trouble is it does not seem to the case that strong enough lower bounds are known for positive laws. This suggest that we make it harder for a law to work. </p>
<p>
The idea is to use a FST, that is a finite state transducer. This is a finite state device that reads a symbol, updates its state, and outputs one or more symbols. If <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is the FST and <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> is a string, let <img alt="{T(U)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%28U%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T(U)}"/> be the result of applying <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>. For example, suppose that <img alt="{ABBBAB}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BABBBAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ABBBAB}"/> is an input <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/>. Let the transducer add a <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> after each <img alt="{AB}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{AB}"/>. Thus <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> maps to: 	</p>
<p align="center"><img alt="\displaystyle  ABCBBABC. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++ABCBBABC.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  ABCBBABC. "/></p>
<p>	 Then the following is true: </p>
<blockquote><p><b>Lemma 1</b> <em> Let <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/> be group and let <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{T}"/> be a transducer with <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S}"/> states. Then we can separate <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{U}"/> from <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{V}"/> by a FSA with order <img alt="{|G| \cdot S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CG%7C+%5Ccdot+S%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{|G| \cdot S}"/> states provided 	</em></p><em>
<p align="center"><img alt="\displaystyle  T(U) = T(V) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++T%28U%29+%3D+T%28V%29+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  T(U) = T(V) "/></p>
</em><p><em>is not a law in <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/>. </em>
</p></blockquote>
<p/><p>
<em>Proof:</em>  The key is to simulate <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> as we read the input, and at the same time update the group element. This can be done by a FSA with the product <img alt="{|G|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CG%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|G|}"/> and <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> states. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
The hope is that having a law that is invariant for all FST, even only those with few states, is difficult. </p>
<blockquote><p><b> </b> <em> <b>A conjecture</b>: Positive laws that can handle all FST transducers with <img alt="{n^{\epsilon}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Cepsilon%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n^{\epsilon}}"/> states must be large. </em>
</p></blockquote>
<p>That is are large enough to advance our best bounds on SWP. What do you think?</p>
<p/></font></font></div>
    </content>
    <updated>2020-08-31T13:41:04Z</updated>
    <published>2020-08-31T13:41:04Z</published>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="claims"/>
    <category term="mistake"/>
    <category term="open problem"/>
    <category term="separate words"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-08T00:21:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://nisheethvishnoi.wordpress.com/?p=99</id>
    <link href="https://nisheethvishnoi.wordpress.com/2020/08/31/algorithms-for-convex-optimization-book/" rel="alternate" type="text/html"/>
    <title>Algorithms for Convex Optimization Book</title>
    <summary>I am excited to announce that a pre-publication draft of my book Algorithms for Convex Optimization (to be published by Cambridge University Press) is now available for download here: Algorithms for Convex Optimization Book The goal of this book is to enable a reader to gain an in-depth understanding of algorithms for convex optimization. The emphasis is […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I am excited to announce that a pre-publication draft of my book <strong>Algorithms for Convex Optimization</strong> (to be published by Cambridge University Press) is now available for download here:</p>
<p><a href="https://convex-optimization.github.io/">Algorithms for Convex Optimization Book</a></p>
<p>The goal of this book is to enable a reader to gain an in-depth understanding of algorithms for convex optimization. The emphasis is to derive key algorithms for convex optimization from first principles and to establish precise running time bounds in terms of the input length.</p>
<p>The intended audience includes advanced undergraduate students, graduate students and researches from theoretical computer science, discrete optimization, and machine learning.</p>
<div class="page" title="Page 8">
<div class="layoutArea">
<div class="column">
<p>The book has four parts:</p>
</div>
</div>
</div>
<ol>
<li class="p1">Chapters 3,4, and 5: Introduction to convexity, models of computation and notions of efficiency in convex optimization, Lagrangian duality, Legendre-Fenchel duality, and KKT conditions.</li>
<li class="p1">Chapters 6,7, and 8: First-order methods such as gradient descent, mirror descent and the multiplicative weights update method, and accelerated gradient descent.</li>
<li class="p1">Chapters 9,10, and 11: Newton’s method, path-following interior point methods for linear programming, and self-concordant barrier functions.</li>
<li class="p1">Chapter 11 and 12: Cutting plane methods such as the ellipsoid method for linear and general convex programs.</li>
</ol>
<p class="p1">Chapter 1 summarizes the book via a brief history of the interplay between continuous and discrete optimization: how the search for fast algorithms for discrete problems is leading to improvements in algorithms for convex optimization.</p>
<p class="p1">Many chapters contain applications ranging from finding maximum flows, minimum cuts, and perfect matchings in graphs, to linear optimization over 0-1-polytopes, to submodular function minimization, to computing maximum entropy distributions over combinatorial polytopes.</p>
<p class="p1">The book is self-contained and starts with a review of calculus, linear algebra, geometry, dynamical systems, and graph theory in Chapter 2. Exercises posed in this book not only play an important role in checking one’s understanding, but sometimes important methods and concepts are introduced and developed entirely through them. Examples include the Frank-Wolfe method, coordinate descent, stochastic gradient descent, online convex optimization, the min-max theorem for zero-sum games, the Winnow algorithm for classification, the conjugate gradient method, primal-dual interior point method, and matrix scaling.</p>
<p>Feedback, corrections, and comments are welcome.</p></div>
    </content>
    <updated>2020-08-31T13:30:40Z</updated>
    <published>2020-08-31T13:30:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>nisheethvishnoi</name>
    </author>
    <source>
      <id>https://nisheethvishnoi.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://nisheethvishnoi.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://nisheethvishnoi.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Algorithms, Nature, and Society</title>
      <updated>2020-09-08T00:22:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/08/31/postdoc-at-epfl-apply-by-september-13-2020/</id>
    <link href="https://cstheory-jobs.org/2020/08/31/postdoc-at-epfl-apply-by-september-13-2020/" rel="alternate" type="text/html"/>
    <title>postdoc at EPFL (apply by September 13, 2020)</title>
    <summary>Applications are solicited for postdoctoral positions in all aspects of Theoretical Computer Science at EPFL. Each position will be for a period of up to two years and comes with a competitive salary and generous travel support. Website: https://theory.epfl.ch/postdoc.html Email: tcs-postdoc@groupes.epfl.ch</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are solicited for postdoctoral positions in all aspects of Theoretical Computer Science at EPFL. Each position will be for a period of up to two years and comes with a competitive salary and generous travel support.</p>
<p>Website: <a href="https://theory.epfl.ch/postdoc.html">https://theory.epfl.ch/postdoc.html</a><br/>
Email: tcs-postdoc@groupes.epfl.ch</p></div>
    </content>
    <updated>2020-08-31T08:29:08Z</updated>
    <published>2020-08-31T08:29:08Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-08T00:21:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4942</id>
    <link href="https://www.scottaaronson.com/blog/?p=4942" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4942#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4942" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">My new motto</title>
    <summary xml:lang="en-US">Update (Sep 1): Thanks for the comments, everyone! As you can see, I further revised this blog’s header based on the feedback and on further reflection. The Right could only kill me and everyone I know.The Left is scarier; it could convince me that it was my fault! (In case you missed it on the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Update (Sep 1):</span></strong> Thanks for the comments, everyone!  As you can see, I further revised this blog’s header based on the feedback and on further reflection.<br/><br/><strong>The Right could only kill me and everyone I know.<br/>The Left is scarier; it could convince me that it was my fault</strong>!</p>



<p>(In case you missed it on the blog’s revised header, right below “Quantum computers aren’t just nondeterministic Turing machines” and “Hold the November US election by mail.”  I added an exclamation point at the end to suggest a <em>slightly</em> comic delivery.)</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> A friend expressed concern that, because my new motto appears to “blame both sides,” it might generate confusion about my sympathies or what I want to happen in November.  So to eliminate all ambiguity: I hereby announce that I will match all reader donations made in the next 72 hours to either the <a href="https://secure.actblue.com/donate/duforjoe">Biden-Harris campaign</a> or the <a href="https://lincolnproject.us/donate/">Lincoln Project</a>, up to a limit of $2,000.  Honor system; just tell me in the comments what you donated.</p></div>
    </content>
    <updated>2020-08-30T23:21:02Z</updated>
    <published>2020-08-30T23:21:02Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Embarrassing Myself"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Obviously I'm Not Defending Aaronson"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Self-Referential"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-09-05T22:49:53Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/08/30/linkage</id>
    <link href="https://11011110.github.io/blog/2020/08/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>The evolution of mathematical word processing (\(\mathbb{M}\)). “Developments in computing over the last 30 years have not done as much as one might have thought to make writing mathematics easier.”</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://sinews.siam.org/Details-Page/the-evolution-of-mathematical-word-processing">The evolution of mathematical word processing</a> (<a href="https://mathstodon.xyz/@11011110/104701998974894412">\(\mathbb{M}\)</a>). “Developments in computing over the last 30 years have not done as much as one might have thought to make writing mathematics easier.”</p>
  </li>
  <li>
    <p><a href="https://prideout.net/blog/svg_wireframes/">3D Wireframes in SVG, via Python</a> (<a href="https://mathstodon.xyz/@11011110/104707509485635530">\(\mathbb{M}\)</a>, <a href="https://github.com/prideout/svg3d">code repository</a>). Once you generate the model, it does the rest. Its depth-ordering heuristics aren’t perfect (and will fail when the depth order is cyclic), but generally work pretty well. For example, here’s <a href="https://en.wikipedia.org/wiki/Jessen%27s_icosahedron">Jessen’s icosahedron</a> in a simple partially-transparent style without fancy shading (<a href="https://11011110.github.io/blog/assets/2020/jessen.py">Python code</a>). To get it to look right I had to edit the resulting svg and manually reorder the faces.</p>

    <p style="text-align: center;"><img alt="Jessen's icosahedron" src="https://11011110.github.io/blog/assets/2020/jessen.svg"/></p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2020/08/17/ip-grab-youngstown-state">Intellectual property grab at Youngstown State</a> (<a href="https://mathstodon.xyz/@11011110/104714502717608284">\(\mathbb{M}\)</a>). In negotiations with the faculty union, the university wants to replace the tradition of faculty holding copyright to research articles, textbooks, syllabi and lectures, etc by instead taking ownership of “all nonpatentable faculty work” as work-for-hire. They claim that they aren’t changing their policies and are merely doing this “to be consistent with the law” but this comes across as unlikely and disingenuous.</p>
  </li>
  <li>
    <p><a href="https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/15700/Lost%20Mathematics.pdf?fterence=1">Lectures on lost mathematics</a> (<a href="https://mathstodon.xyz/@11011110/104718951109084030">\(\mathbb{M}\)</a>). In this 79-page pdf from 1975, Branko Grünbaum discusses mathematical questions studied by non-mathematicians but snubbed by pure mathematicians, including which polygons tile, the girth of infinitely-repeating cubic spatial graphs, the classification of vertex-transitive polyhedral manifolds in space, generalization of Kempe universality to surfaces, flexible polyhedra, and tensegrity.</p>
  </li>
  <li>
    <p><a href="https://xkcd.com/2348/">A recent xkcd on river crossing puzzles</a> particularly amused me (<a href="https://mathstodon.xyz/@11011110/104726172352360084">\(\mathbb{M}\)</a>).</p>

    <p style="text-align: center;"><img alt="xkcd comic &quot;Boat Puzzle&quot;, https://xkcd.com/2348/" src="https://11011110.github.io/blog/assets/2020/xkcd-2348.png" width="80%"/></p>

    <p>It’s too bad xkcd uses an -NC clause in its CC license; if it didn’t, we could use it or its first frame to replace <a href="https://en.wikipedia.org/wiki/River_crossing_puzzle">the Wikipedia article’s illustration</a>, which is undergoing a long slow <a href="https://commons.wikimedia.org/wiki/Commons:Deletion_requests/File:Vovk_koza_kapusta.png">deletion discussion because copied from a 1954 Soviet book</a>. If you know something useful about the copyright status of 1954 Soviet books, please add your knowledge to that discussion.</p>
  </li>
  <li>
    <p><a href="https://www.flyingcoloursmaths.co.uk/eye-to-eye/">Eye to eye</a> (<a href="https://mathstodon.xyz/@11011110/104731872882549290">\(\mathbb{M}\)</a>). Let \(C\) and \(C'\) be circles with centers outside the other circle, and draw tangent rays from each center to the other circle. These rays cut their circles in chords of equal length. But I wonder: when only one circle has its center outside the other, its chord is still well defined, but the same length can’t be a chord of the other circle because it exceeds the diameter. But it should still have a geometric meaning with respect to the other circle: what is it?</p>
  </li>
  <li>
    <p><a href="https://petapixel.com/2020/08/20/lightroom-app-update-wipes-users-photos-and-presets-adobe-says-they-are-not-recoverable/">Lightroom App update irrecoverably loses users’ photos</a> (<a href="https://mathstodon.xyz/@11011110/104742916488291004">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24229864">via</a>). My choice to continue using an oldish powerbook with dubiously-reliable keyboard, to avoid giving up my paid-for non-subscription Adobe apps which won’t run on newer Macs, has an unexpected benefit: my files haven’t been auto-deleted. Also, let this be a reminder to do your backups, and make sure that they include a local non-cloud backup of your files.</p>
  </li>
  <li>
    <p><a href="https://github.andrewt.net/mines/">Minesweeper where forced guesses are always safe, but unforced guesses always explode</a> (<a href="https://mastodon.technology/@andrewt/104701318997810776">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://www.reddit.com/r/Scotland/comments/ig9jia/ive_discovered_that_almost_every_single_article/">“The Scots language version of Wikipedia is legendarily bad” — turns out because it was mostly written by an American teenager</a> (<a href="https://mathstodon.xyz/@11011110/104751072546716749">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/188374/The-problem-is-that-this-person-cannot-speak-Scots">via</a>). See <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2020-08-30/News_and_notes">the <em>Signpost</em> article</a> for an update.</p>
  </li>
  <li>
    <p>Today’s mini-episode of “not the Reuleaux triangle” (<a href="https://mathstodon.xyz/@11011110/104757581337870450">\(\mathbb{M}\)</a>): the logo of Polish football club Ruch Chorzów, which a supporter tried to add to the Wikipedia article. Unlike many non-Reuleaux round triangles, it appears to use circular arcs, but not centered at the corners and with non-equilateral corners. The arc across the bottom is longer than the two sides, and it is wider than it is tall. Image from <a href="http://kubamalicki.com/portfolio_page/ruch-chorzow-100-years-anniversary/">an article on a recent redesign of the official logo</a>.</p>

    <p style="text-align: center;"><img alt="Geometric analysis of the Ruch Chorz&#xF3;w logo showing that it is not a Reuleaux triangle" src="https://11011110.github.io/blog/assets/2020/ruch-logo.png" width="80%"/></p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@jsiehler/104763100825420197">J. Siehler asks for unsolved problems in mathematics whose statements are understandable by elementary school students, other than in number theory </a>. Examples given so far include <a href="https://en.wikipedia.org/wiki/Inscribed_square_problem">square pegs</a>, <a href="https://en.wikipedia.org/wiki/Bellman%27s_lost_in_a_forest_problem">lost in a forest</a>, <a href="https://en.wikipedia.org/wiki/Moving_sofa_problem">sofa-moving</a>, <a href="https://en.wikipedia.org/wiki/Moser%27s_worm_problem">Moser’s worm</a>, <a href="https://en.wikipedia.org/wiki/Net_(polyhedron)#Existence_and_uniqueness">Dürer’s nets</a>, the <a href="https://en.wikipedia.org/wiki/Tur%C3%A1n%27s_brick_factory_problem">brick factory</a>, and <a href="https://en.wikipedia.org/wiki/Lonely_runner_conjecture">lonely runners</a>.</p>
  </li>
  <li>
    <p><a href="https://www.rayawolfsun.com/2015/02/06/the-romance-of-al-asturlabiya/">Concerning “Mariam” Al-Asturlabiya</a> (<a href="https://mathstodon.xyz/@11011110/104771298687510642">\(\mathbb{M}\)</a>). A warning about how romanticizing past figures (in this case <a href="https://en.wikipedia.org/wiki/Mariam_al-Asturlabi">the only woman astrolabist known from the medieval Islamic world</a>) can result in creating biographical details for them out of thin air.</p>
  </li>
  <li>
    <p><a href="https://prideout.net/knotgl/">Interactive 3d knot table</a> (<a href="https://mathstodon.xyz/@11011110/104779316951663485">\(\mathbb{M}\)</a>). One of many interesting visualizations on “<a href="https://prideout.net/">the little grasshopper</a>”, by Philip Rideout (author of the svg3d Python library linked above).</p>
  </li>
  <li>
    <p><a href="https://gd2020.cs.ubc.ca/program-no-links/">The Graph Drawing 2020 program is online</a> (<a href="https://mathstodon.xyz/@11011110/104781215682863021">\(\mathbb{M}\)</a>). It is September 16-18, from 8AM to noon Pacific daylight time (the time in Vancouver, where the conference was originally to be held). The format has talk videos available pre-conference, with sessions consisting of 1-minute reminders of each talk and 5 minutes of live questions per talk. Jeff Erickson and Sheelagh Carpendale will give live invited talks. It’s free but requires registration, with deadline September 10.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-08-30T17:47:00Z</updated>
    <published>2020-08-30T17:47:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-09-02T06:49:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17466</id>
    <link href="https://rjlipton.wordpress.com/2020/08/29/20000-comments-and-more/" rel="alternate" type="text/html"/>
    <title>20,000 Comments and More</title>
    <summary>With more about the Separating Words Problem I.I.T. Madras page Anoop S K M is a PhD student in the theory group of I.I.T. Madras in Chennai, India. His comment two weeks ago Monday was the 20,000th reader comment (including trackbacks) on this blog. Today we thank Anoop and all our readers and say more […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>With more about the Separating Words Problem</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/08/anoopskm-1.jpg"><img alt="" class="alignright size-thumbnail wp-image-17474" height="150" src="https://rjlipton.files.wordpress.com/2020/08/anoopskm-1.jpg?w=128&amp;h=150" width="128"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">I.I.T. Madras <a href="http://www.cse.iitm.ac.in/~theory/memberdetails.php?memberid=512">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Anoop S K M is a PhD student in the theory group of I.I.T. Madras in Chennai, India. His <a href="https://rjlipton.wordpress.com/2020/08/03/cleverer-automata-exist/#comment-111947">comment</a> two weeks ago Monday was the <font color="green"><b>20,000</b></font>th reader comment (including trackbacks) on this blog.</p>
<p>
Today we thank Anoop and all our readers and say more about the subject of his comment.<br/>
<span id="more-17466"/></p>
<p>
Anoop’s advisor is Jayalal Sarma, who has a further connection to me (Ken). Sarma has <a href="https://arxiv.org/abs/1002.1496">co-authored</a> <a href="https://link.springer.com/article/10.1007/s00224-013-9519-3">three</a> <a href="https://arxiv.org/abs/0912.2565">papers</a> with my PhD graduate Maurice Jansen, whose work we have <a href="https://rjlipton.wordpress.com/2011/04/20/could-euler-have-solved-this/">also</a> featured <a href="https://rjlipton.wordpress.com/2011/04/28/succinct-constant-depth-arithmetic-circuits-are-weak/">here</a>. </p>
<p>
The comment was Anoop’s first and thus far only one on the blog. In passing we note that after one’s first comment is moderated through, subsequent ones appear freely and immediately. Thus one can say that Anoop had only a 0.005% chance of hitting the milestone. </p>
<p>
Dick and I would also like to give a warm shout out to someone else who in another sense had almost a 56% chance of hitting it: Jon Awbrey, who writes the blog <a href="https://inquiryintoinquiry.com/">Inquiry Into Inquiry</a> and also <a href="https://ncatlab.org/nlab/show/Jon+Awbrey">contributes</a> to the <a href="https://ncatlab.org/nlab/show/About">nLab</a> Project.</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/08/jon_awbrey.jpg"><img alt="" class="alignright size-thumbnail wp-image-17470" src="https://rjlipton.files.wordpress.com/2020/08/jon_awbrey.jpg?w=128&amp;h=150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2"><a href="https://www.researchgate.net/profile/Jon_Awbrey">src</a><br/>
</font>
</td>
</tr>
</tbody></table>
<p>As we were twenty-seven comments into our third myriad at the time of drafting this post, we note that most of his 15-of-27 comments are trackback citations of our previous <a href="https://rjlipton.wordpress.com/2020/08/19/logical-complexity-of-proofs/">post</a> on proof complexity, but he wrote <a href="https://rjlipton.wordpress.com/2020/08/19/logical-complexity-of-proofs/#comment-112175">two</a> long <a href="https://rjlipton.wordpress.com/2020/08/19/logical-complexity-of-proofs/#comment-112189">comments</a> as well. We have not had time to act on them; for my part, chess cheating has found ways to mutate into new kinds of cases even as I see hope for stemming the main online outbreak of it. But comments can also be resources for others, especially students who may be emboldened to take a swing at progress from where we’ve left off.</p>
<p>
</p><p/><h2> Anoop’s Comment and SWP Ideas </h2><p/>
<p/><p>
Anoop’s comment was in our recent <a href="https://rjlipton.wordpress.com/2020/08/03/cleverer-automata-exist/">post</a> on the Separating Word Problem (SWP). He asked about a technical <a href="http://invenio.nusl.cz/record/251413/files/content.csg.pdf">report</a> by Jiří Wiedermann claiming a clean best-possible <img alt="{O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\log n)}"/> bound on the SWP over alphabet <img alt="{\{0,1\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{0,1\}}"/>. </p>
<p>
Wiedermann’s idea is simple to state—maybe too simple: Consider finite state automata (FSAs) <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> whose <img alt="{2m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2m}"/> states form two <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/>-cycle “tracks”: </p>
<p align="center"><img alt="\displaystyle  q_0 \rightarrow q_1 \rightarrow \cdots \rightarrow q_{m-1} \rightarrow q_0; \qquad q'_0 \rightarrow q'_1 \rightarrow \cdots \rightarrow q'_{m-1} \rightarrow q'_0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++q_0+%5Crightarrow+q_1+%5Crightarrow+%5Ccdots+%5Crightarrow+q_%7Bm-1%7D+%5Crightarrow+q_0%3B+%5Cqquad+q%27_0+%5Crightarrow+q%27_1+%5Crightarrow+%5Ccdots+%5Crightarrow+q%27_%7Bm-1%7D+%5Crightarrow+q%27_0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  q_0 \rightarrow q_1 \rightarrow \cdots \rightarrow q_{m-1} \rightarrow q_0; \qquad q'_0 \rightarrow q'_1 \rightarrow \cdots \rightarrow q'_{m-1} \rightarrow q'_0 "/></p>
<p>Each edge goes to the next state on the same track on both <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> and <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> <em>unless</em> we place a “crossover” so that the edges switch tracks on <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. The primed track has accepting states. It does not matter whether the start state is <img alt="{q_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_0}"/> or <img alt="{q'_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%27_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q'_0}"/>, but it is the same for any pair of strings <img alt="{u,v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%2Cv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u,v}"/> of length <img alt="{n \gg m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgg+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \gg m}"/> that we wish to distinguish.</p>
<p>
Let <img alt="{S \subseteq [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Csubseteq+%5Bn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S \subseteq [n]}"/> be the set of places where <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> differ. Now suppose we have <img alt="{b \in [m]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb+%5Cin+%5Bm%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b \in [m]}"/> such that the intersection of <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> with the residue class <img alt="{R_{m,b} = \{am + b : a \in \mathbb{N}\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_%7Bm%2Cb%7D+%3D+%5C%7Bam+%2B+b+%3A+a+%5Cin+%5Cmathbb%7BN%7D%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R_{m,b} = \{am + b : a \in \mathbb{N}\}}"/> is odd. Make <img alt="{A = A_{m,b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%3D+A_%7Bm%2Cb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A = A_{m,b}}"/> by placing one “crossover” at point <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> on the tracks; that is, make the transitions for <img alt="{q_b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_b}"/> and <img alt="{q'_b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%27_b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q'_b}"/> be </p>
<p align="center"><img alt="\displaystyle  (q_b,0,q_{b+1}),~(q_b,1,q'_{b+1}),~(q'_b,0,q'_{b+1}),~(q'_b,1,q_{b+1}), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28q_b%2C0%2Cq_%7Bb%2B1%7D%29%2C%7E%28q_b%2C1%2Cq%27_%7Bb%2B1%7D%29%2C%7E%28q%27_b%2C0%2Cq%27_%7Bb%2B1%7D%29%2C%7E%28q%27_b%2C1%2Cq_%7Bb%2B1%7D%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (q_b,0,q_{b+1}),~(q_b,1,q'_{b+1}),~(q'_b,0,q'_{b+1}),~(q'_b,1,q_{b+1}), "/></p>
<p>where the addition is modulo <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/>. Then whenever a place where <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> differ enters the crossover, the strings flip their status of being on the same track or different tracks. Since this happens an odd number of times, <img alt="{A_{m,b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bm%2Cb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{m,b}}"/> accepts <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> and rejects <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> or vice-versa.</p>
<p>
If for every <img alt="{S \subseteq [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Csubseteq+%5Bn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S \subseteq [n]}"/> we could find such <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> with <img alt="{m = O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+O%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m = O(\log n)}"/> then SWP would fall with the best possible logarithmic size, nullifying anything like Zachary Chase’s improvement from <img alt="{m = \tilde{O}(n^{2/5})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+%5Ctilde%7BO%7D%28n%5E%7B2%2F5%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m = \tilde{O}(n^{2/5})}"/> to <img alt="{m = \tilde{O}(n^{1/3})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+%5Ctilde%7BO%7D%28n%5E%7B1%2F3%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m = \tilde{O}(n^{1/3})}"/>. The “<img alt="{m = O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+O%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m = O(\log n)}"/>” is not hiding any <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log n}"/> factors with a tilde—it is just a bare <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log n}"/> factor. The report claims this complete resolution, but it makes and then unmakes a point in a way we don’t follow. The point is that the idea of focusing on subsets <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> is not only independent of particular strings <img alt="{u,v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%2Cv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u,v}"/> but also independent of the lengths <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> of the string, except insofar as the maximum index in <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> constrains the smallest <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> that works. But then the report states a theorem that invests <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> itself with significance that contradicts said point, so it just appears wrong.</p>
<p>
The report anyway does not appear either on Wiedermann’s publications page or his DBLP page. Its conclusion references a 2015 <a href="https://rjlipton.wordpress.com/2015/09/03/open-problems-that-might-be-easy/">post</a> on this blog in which SWP was included, but we had not heard of this until Anoop’s comment. So it goes in our mistake file. We have been grappling all week with the subject of claims and mistakes on a much larger scale with complicated papers and techniques. In the old days, ideas and errors would be threshed out in-person at conferences and workshops and only the finished product would be visible to those outside the loop. Now we not only have Internet connectivity but also the pandemic has curtailed the “in-person” aspect. Thus we are not averse to promoting the threshing and wonder how best to extract the useful novelty from new attempts that likely have errors.</p>
<p>
The <img alt="{A_{m,b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bm%2Cb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{m,b}}"/> construction is neat but has a weakness that can be framed as motivation for Dick’s idea in the rest of this post.  Adding more crossovers for a fixed <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> does not help separate any more strings: If <img alt="{S \cap R_{m,b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Ccap+R_%7Bm%2Cb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S \cap R_{m,b}}"/> and <img alt="{S \cap R_{m,c}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Ccap+R_%7Bm%2Cc%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S \cap R_{m,c}}"/> both have even cardinality then adding <em>two</em> crossovers at <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> and <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> has no effect.  Intuitively speaking, all crossovers behave merely as the same non-unit element of the two-element group.  In particular, they commute with each other.  The idea moving forward is, can we get more interesting behavior from <em>non</em>-commutative groups that still yield small automata <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> separating given pairs of strings? </p>
<p>
</p><p/><h2> Laws on Finite Groups </h2><p/>
<p/><p>
Let’s start to explain a connection of SWP with finite groups. A <b>law</b> for a group <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is 	</p>
<p align="center"><img alt="\displaystyle  U = V, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++U+%3D+V%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  U = V, "/></p>
<p>where <img alt="{U,V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%2CV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U,V}"/> are words over <img alt="{\{ A,B \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+A%2CB+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ A,B \}}"/>. We will think of <img alt="{U,V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%2CV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U,V}"/> as embeddings within the group of the binary strings <img alt="{u,v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%2Cv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u,v}"/> we want to separate. The law says that <img alt="{U=V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%3DV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U=V}"/> for all substitutions of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> as elements in <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. Thus 	</p>
<p align="center"><img alt="\displaystyle  AB = BA " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++AB+%3D+BA+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  AB = BA "/></p>
<p>is a law that holds in abelian groups. Every finite group <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> satisfies the law 	</p>
<p align="center"><img alt="\displaystyle  A^{n} = 1, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A%5E%7Bn%7D+%3D+1%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A^{n} = 1, "/></p>
<p>where <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is the size of the group. </p>
<p>
There has been quite a bit of research on laws for groups. </p>
<ul>
<li>
Some are on the complexity of checking if they hold. It is a famous result that with randomness one can check to see if <img alt="{AB=BA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAB%3DBA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{AB=BA}"/> with few tests. <p/>
</li><li>
Some are on the structure of laws. <p/>
</li><li>
Most relevant to SWP is the interest in the smallest size law that holds for all groups of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> or less elements. The size of a law is the maximum of the length of the words <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> and <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/>.
</li></ul>
<p>
</p><p/><h2> Laws and SWP </h2><p/>
<p/><p>
Suppose that we wish to construct a FSA that can tell <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> from <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/>. We assume that <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> and <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> are distinct <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> long strings over <img alt="{A,B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B}"/>. Pick a finite group <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. The key is two observations: </p>
<ol>
<li>
If <img alt="{\alpha = \beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha+%3D+%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha = \beta}"/> is <b>not</b> a law over <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>, then we can set <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> to elements in <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> so that 	<p/>
<p align="center"><img alt="\displaystyle  \alpha \neq \beta. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Calpha+%5Cneq+%5Cbeta.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \alpha \neq \beta. "/></p>
</li><li>
There is a FSA that can compute the values of <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> and <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> as elements in <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> with order <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> states.
</li></ol>
<p>This then proves that we can separate the words <img alt="{A,B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B}"/> with order the size of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> states. </p>
<p>
<i>Warning</i> It is important that the FSA operates on <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> and <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> independently. Moreover, the FSA must run the same computation on these strings—the output must be different. </p>
<p>
I liked this approach since I hoped there would be useful results on laws in groups. There is quite a bit known. It would have been neat if there was a paper that proved: No law of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> holds in all finite groups of size at most <img alt="{n^{1/4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B1%2F4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^{1/4}}"/>. Alas this is not true. Well not exactly. Our laws are of the form 	</p>
<p align="center"><img alt="\displaystyle  U = V. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++U+%3D+V.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  U = V. "/></p>
<p>Since there are no inverses allowed in <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> and <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/> these are called <i>positive laws</i>. If we drop that restriction, then short laws do exist. There are laws of size <img alt="{O(n^{2/3})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B2%2F3%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^{2/3})}"/> that hold for all groups of order <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. But I believe it is still open whether positive laws can be small.</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2020/08/f2_cayley_graph.png"><img alt="" class="aligncenter size-full wp-image-17454" src="https://rjlipton.files.wordpress.com/2020/08/f2_cayley_graph.png?w=600"/></a></p>
<p/><h2> Positive Laws </h2><p/>
<p/><p>
I (Dick) started to search the web about laws in groups. I quickly found a <a href="https://arxiv.org/pdf/1609.03199.pdf">paper</a> by Andrei Bulatov, Olga Karpova, Arseny Shur, Konstantin Startsev on exactly this approach, titled “Lower Bounds on Words Separation: Are There Short Identities in Transformation Semigroups?” They study the problem, prove some results, and do some computer searches. They say: </p>
<blockquote><p><b> </b> <em> It is known that the shortest positive identity in the symmetric group on <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{3}"/> objects <img alt="{S_{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_%7B3%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S_{3}}"/> is <img alt="{x^{2}y^{2} = y^{2}x^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B2%7Dy%5E%7B2%7D+%3D+y%5E%7B2%7Dx%5E%7B2%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x^{2}y^{2} = y^{2}x^{2}}"/>. The shortest such identity in <img alt="{S_{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_%7B4%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S_{4}}"/> has length <img alt="{11}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B11%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{11}"/>: 	</em></p><em>
<p align="center"><img alt="\displaystyle  x^{6}y^{2}xy^{2} = y^{2}xy^{2}x^{6}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7B6%7Dy%5E%7B2%7Dxy%5E%7B2%7D+%3D+y%5E%7B2%7Dxy%5E%7B2%7Dx%5E%7B6%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  x^{6}y^{2}xy^{2} = y^{2}xy^{2}x^{6}. "/></p>
</em><p><em/>
</p></blockquote>
<p>They also note that:</p>
<blockquote><p><b> </b> <em> The problem is to find upper bounds on the function <img alt="{Sep}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BSep%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{Sep}"/>. This problem is inverse to finding the asymptotics of the length of the shortest identity in full transformation semigroups <img alt="{T_{k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_%7Bk%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{T_{k}}"/>. </em>
</p></blockquote>
<p/><p>
I believe the last part of their statement is true, but perhaps misleading. I would argue that laws are related to SWP with a twist. The hope is that this twist is enough to make progress. Let me explain. </p>
<p>
</p><p/><h2> Laws with Errors </h2><p/>
<p/><p>
I think there is hope that positive laws must be large, if we extend what it mean by a law. Suppose that we have a law 	</p>
<p align="center"><img alt="\displaystyle  U_{1} \cdots U_{n} = V_{1} \cdots V_{n}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++U_%7B1%7D+%5Ccdots+U_%7Bn%7D+%3D+V_%7B1%7D+%5Ccdots+V_%7Bn%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  U_{1} \cdots U_{n} = V_{1} \cdots V_{n}. "/></p>
<p>Where <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> and <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/> are as before words over <img alt="{\{A,B\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7BA%2CB%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{A,B\}}"/>. Assume that for all small groups this is always true when we set <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> to elements. </p>
<p>
We note however we can have the FSA do more than just substitutions. We could have it modify the <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> and <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/>. We can have the FSA make some changes to <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> and <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/>: Of course the FSA must do the same to <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> and <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/>, and it must keep the number of states down. </p>
<p>
This means that the law <img alt="{U=V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%3DV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U=V}"/> must be robust to modifications that can be done with few states. As examples, the FSA could: </p>
<ol>
<li>
The FSA could map <img alt="{A^{2} \rightarrow A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%7B2%7D+%5Crightarrow+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^{2} \rightarrow A}"/> and leave all others the same. <p/>
</li><li>
The FSA could map <img alt="{ABBBA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BABBBA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ABBBA}"/> to <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> when ever it sees this. <p/>
</li><li>
The FSA could delete the even letters. Thus <p/>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}   A^{6}B^{2}AB^{2} &amp;=&amp; B^{2}AB^{2}A^{6} \\  AAAAAA BB A BB &amp;=&amp; BB A BB AAAAAA \\  A~ A~ A~ B~ A ~ B &amp;=&amp; B~ A~ B ~ A~ A~ A \\  AAABA &amp;=&amp; BABAAA. \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%09A%5E%7B6%7DB%5E%7B2%7DAB%5E%7B2%7D+%26%3D%26+B%5E%7B2%7DAB%5E%7B2%7DA%5E%7B6%7D+%5C%5C+%09AAAAAA+BB+A+BB+%26%3D%26+BB+A+BB+AAAAAA+%5C%5C+%09A%7E+A%7E+A%7E+B%7E+A+%7E+B+%26%3D%26+B%7E+A%7E+B+%7E+A%7E+A%7E+A+%5C%5C+%09AAABA+%26%3D%26+BABAAA.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}   A^{6}B^{2}AB^{2} &amp;=&amp; B^{2}AB^{2}A^{6} \\  AAAAAA BB A BB &amp;=&amp; BB A BB AAAAAA \\  A~ A~ A~ B~ A ~ B &amp;=&amp; B~ A~ B ~ A~ A~ A \\  AAABA &amp;=&amp; BABAAA. \end{array} "/></p>
</li></ol>
<p>Note the FSA could even add new letters. Thus it could 	</p>
<p align="center"><img alt="\displaystyle  AB \rightarrow ACB, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++AB+%5Crightarrow+ACB%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  AB \rightarrow ACB, "/></p>
<p>where <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> is a new letter. Thus 	</p>
<p align="center"><img alt="\displaystyle  AAAAAA BB A BB = BB A BB AAAAAA " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++AAAAAA+BB+A+BB+%3D+BB+A+BB+AAAAAA+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  AAAAAA BB A BB = BB A BB AAAAAA "/></p>
<p>becomes 	</p>
<p align="center"><img alt="\displaystyle  AAAAAA CBB A CBB = BB A C BB AAAAAA. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++AAAAAA+CBB+A+CBB+%3D+BB+A+C+BB+AAAAAA.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  AAAAAA CBB A CBB = BB A C BB AAAAAA. "/></p>
<p>And so on. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Can we get better lower bounds on positive laws if we require them to be resilient to such modifications? Ones that can be done by a small state FSA. What do you think?</p>
<p>
Again we thank Anoop and all our readers for stimulating comments.</p>
<p/><p><br/>
[changed wording in section 2 and improved transition to the rest]</p></font></font></div>
    </content>
    <updated>2020-08-29T21:54:23Z</updated>
    <published>2020-08-29T21:54:23Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Anoop SKM"/>
    <category term="blog milestone"/>
    <category term="comments"/>
    <category term="finite automata"/>
    <category term="groups"/>
    <category term="readers"/>
    <category term="separating words problem"/>
    <category term="SWP"/>
    <category term="thanks"/>
    <category term="upper bounds"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-08T00:21:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/08/28/full-time-position-at-universidad-catolica-de-chile-apply-by-october-7-2020/</id>
    <link href="https://cstheory-jobs.org/2020/08/28/full-time-position-at-universidad-catolica-de-chile-apply-by-october-7-2020/" rel="alternate" type="text/html"/>
    <title>Full-time Position at Universidad Católica de Chile (apply by October 7, 2020)</title>
    <summary>Universidad Católica de Chile (UC) is offering one or more full-time positions at the assistant (tenure-track) or associate level. We invite applications from highly qualified candidates in Data Science, Machine Learning, Optimization, Statistics and Stochastics, although other areas from Computational Science and Engineering, Optimization and Applied Mathematics will also be considered. Website: http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc Email: vacancysearch.imc@uc.cl</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Universidad Católica de Chile (UC) is offering one or more full-time positions at the assistant (tenure-track) or associate level. We invite applications from highly qualified candidates in Data Science, Machine Learning, Optimization, Statistics and Stochastics, although other areas from Computational Science and Engineering, Optimization and Applied Mathematics will also be considered.</p>
<p>Website: <a href="http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc">http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc</a><br/>
Email: vacancysearch.imc@uc.cl</p></div>
    </content>
    <updated>2020-08-28T18:02:24Z</updated>
    <published>2020-08-28T18:02:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-08T00:21:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-08-28-what-is-a-cryptographic-hash-function/</id>
    <link href="https://decentralizedthoughts.github.io/2020-08-28-what-is-a-cryptographic-hash-function/" rel="alternate" type="text/html"/>
    <title>What is a Cryptographic Hash Function?</title>
    <summary>If you ever tried to understand Bitcoin, you’ve probably banged your head against the wall trying to understand what is a cryptographic hash function? The goal of this post is to: Give you a very simple mental model for how hash functions work, called the random oracle model Give you...</summary>
    <updated>2020-08-28T17:05:00Z</updated>
    <published>2020-08-28T17:05:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-09-07T23:24:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blog.simons.berkeley.edu/?p=276</id>
    <link href="https://blog.simons.berkeley.edu/2020/08/lattice-blog-reduction-part-iii-self-dual-bkz/" rel="alternate" type="text/html"/>
    <title>Lattice Blog Reduction – Part III: Self-Dual BKZ</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is the third and last entry in a series of posts about lattice block reduction. See here and here for the first and second part, resp. In this post I will assume you have read the other parts. In … <a href="https://blog.simons.berkeley.edu/2020/08/lattice-blog-reduction-part-iii-self-dual-bkz/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is the third and last entry in a series of posts about lattice block reduction. See <a class="uri" href="https://blog.simons.berkeley.edu/2020/04/lattice-blog-reduction-part-i-bkz/">here</a> and <a class="uri" href="https://blog.simons.berkeley.edu/2020/05/lattice-blog-reduction-part-ii-slide-reduction/">here</a> for the first and second part, resp. In this post I will assume you have read the other parts.</p>
<p>In the first two parts we looked at BKZ and Slide reduction, the former being the oldest and most useful in practice, while the latter achieves the best provable bounds and has the cleaner analysis. While BKZ is a natural generalization of LLL, we have seen that the analysis of LLL does not generalize well to BKZ. One can view Slide reduction as a different generalization of LLL with the goal of also naturally generalizing its analysis. As we mentioned in the first part, there is another analysis technique based on dynamical systems, introduced in [HPS11]. Unfortunately, as applied to BKZ, there are some cumbersome technicalities and the resulting bounds on the output quality are not as tight as we would like them to be (i.e. as for Slide reduction). One can view the algorithm we are considering today – SDBKZ [MW16] – as a generalization of LLL that lends itself much easier to this dynamical systems analysis: it is simpler, cleaner and yields better results. Since part of the goal of today’s post is to demonstrate this very useful analysis technique, SDBKZ is a natural candidate.</p>
<h1 id="sec:sdbkz">SDBKZ</h1>
<p>Recall the two tools we’ve been relying on in the first two algorithms, SVP and DSVP reduction of projected subblocks:</p>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-image"><figure class="alignleft size-large"><img alt="" class="wp-image-156" src="https://blog.simons.berkeley.edu/wp-content/uploads/2020/04/svp.png"/>Effect of a call to the DSVP oracle. GSO log norms of the input in black, of the output in blue. Note that the sum of the GSO log norms is a constant, so increasing the length of the last vector, decreases the (average of the) remaining vectors.</figure></div>
</div></div>



<div class="wp-block-image"><figure class="alignleft size-large"><img alt="" class="wp-image-262" src="https://blog.simons.berkeley.edu/wp-content/uploads/2020/05/dsvp.png"/>Effect of a call to the DSVP oracle. GSO log norms  of the input in black, of the output in blue. Note that the sum of the GSO log norms is a constant, so increasing the length of the last vector, decreases the (average of the) remaining vectors.</figure></div>



<p>We will use both of them again today. Like BKZ, a tour of SDBKZ starts by calling the SVP oracle on successive blocks of our basis. However, when we reach the end of the basis, we will not decrease the size of the window, since this is actually quite inconvenient for the analysis. Instead, we will keep the size of the window constant but switch to DSVP reduction, i.e. at the end of the BKZ tour we DSVP reduce the last block. This will locally maximize the last GSO vector in the basis, just as the first SVP call locally minimized the first vector of the basis. Then we will move the window successively backwards, mirroring a BKZ tour, but using DSVP reduction, until we reach the beginning of the basis again. At this point, we switch back to SVP reduction and move the window forward, etc. So SDBKZ runs in forward and backward tours.</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-283" src="https://blog.simons.berkeley.edu/wp-content/uploads/2020/08/sdbkz-1024x825.png"/>SDBKZ in one picture: apply the SVP oracle to the projected blocks from start to finish and when you reach the end, apply the DSVP oracle to from finish to start. Repeat.</figure>



<p>A nice observation here is that the backward tour can be viewed equivalently as: 1) compute the reversed dual basis (i.e. the dual basis with reversed columns), 2) run a forward tour, 3) compute the primal basis again. The first of these two steps is self-inverse: computing the reversed dual basis of the reversed dual basis yields the original primal basis. This means step 3) is actually the same as step 1). So in effect, one can view SDBKZ as simply repeating the following two steps: 1) run a forward tour, 2) compute the reversed dual basis. So it doesn’t matter if we use the primal or the dual basis as input, the operations of the algorithm are the same. This is why it is called <em>Self-Dual</em> BKZ.</p>
<p>There is one caveat with this algorithm: it is not clear, when one should terminate. In BKZ and Slide reduction one can formulate clear criteria, when the algorithm makes no more progress anymore. In SDBKZ this is not the case, but the analysis will show that we can bound the number of required tours ahead of time.</p>
<h4 id="the-analysis">The Analysis</h4>
<p>We will start by analyzing the effect of a forward tour. Let <span class="math inline">\({\mathbf{B}}\)</span> be our input basis. The first call to the SVP oracle in a forward tour replaces <span class="math inline">\({\mathbf{b}}_1\)</span> with the shortest vector in <span class="math inline">\({\mathbf{B}}_{[1,k]}\)</span>. This means that the new basis <span class="math inline">\({\mathbf{B}}’\)</span> satifies <span class="math inline">\(\| {\mathbf{b}}_1′ \| \leq \sqrt{\gamma_k} (\prod_{i=1}^k \|{\mathbf{b}}_i^* \|)^{1/k}\)</span> by Minkowski’s bound. Equivalently, this can be written as <span class="math display">\[\log \| {\mathbf{b}}_1′ \|
  \leq \log \sqrt{\gamma_k} + \frac1k (\sum_{i=1}^k \log \|{\mathbf{b}}_i^* \|).\]</span> So if we consider the <span class="math inline">\(\log \|{\mathbf{b}}_i^*\|\)</span> as variables, it seems like linear algebra could be useful here. So far, so good. The second step is more tricky though. We know that the next basis <span class="math inline">\({\mathbf{B}}”\)</span>, i.e. after the call to the SVP oracle on <span class="math inline">\({\mathbf{B}}’_{[2,k+1]}\)</span>, satisfies <span class="math inline">\({\mathbf{b}}_1” = {\mathbf{b}}_1’\)</span> and <span class="math inline">\(\| ({\mathbf{b}}_2”)^* \| \leq \sqrt{\gamma_k} (\prod_{i=2}^{k+1} \|({\mathbf{b}}’_i)^* \|)^{1/k}\)</span>. Unfortunately, we have no control over <span class="math inline">\(\|({\mathbf{b}}’_i)^* \|\)</span> for <span class="math inline">\(i \in {2,\dots,k} \)</span>, since we do not know how the SVP oracle in the first call changed these vector. However, we do know that the lattice <span class="math inline">\({\mathbf{B}}_{[1,k+1]}\)</span> did not change in that call. So we can write <span class="math display">\[\prod_{i=2}^{k+1} \|({\mathbf{b}}’_i)^* \| = \frac{\prod_{i=1}^{k+1} \|{\mathbf{b}}_i^* \|}{\| {\mathbf{b}}’_1 \|}\]</span> and thus we obtain <span class="math display">\[\log \| ({\mathbf{b}}_2′)^* \|
  \leq \log \sqrt{\gamma_k} + \frac1k (\sum_{i=1}^{k+1} \log \|{\mathbf{b}}_i^* \| – \log \|{\mathbf{b}}’_1 \|).\]</span> Again, this looks fairly “linear algebraicy”, so it could be useful. But there is another issue now: in order to get an inequality purely in the input basis <span class="math inline">\({\mathbf{B}}\)</span>, we would like to use our inequality for <span class="math inline">\(\log \|{\mathbf{b}}_1′ \|\)</span> in the one for <span class="math inline">\(\log \| ({\mathbf{b}}_2′)^* \|\)</span>. But the coefficient of <span class="math inline">\(\log \|{\mathbf{b}}_1′ \|\)</span> is negative, so we would need a lower bound for <span class="math inline">\(\log \|{\mathbf{b}}_1′ \|\)</span>. Furthermore, we would like to use upper bounds for our variables later, since the analysis of a tour will result in upper bounds and we would like to apply it iteratively. For this, negative coefficients are a problem. So, we need one more modification: we will use a change of variable to fix this. Instead of considering the variables <span class="math inline">\(\log \| {\mathbf{b}}_i^* \|\)</span>, we let the input variables to our forward tour be <span class="math inline">\(x_i = \sum_{j &lt; k+i} \log \|{\mathbf{b}}^*_i \|\)</span> and the output variables <span class="math inline">\(y_i = \sum_{j \leq i} \log \|({\mathbf{b}}’_i)^* \|\)</span> for <span class="math inline">\(i \in [1,\dots,n-k]\)</span>. Clearly, we can now write our upper bound on <span class="math inline">\(\log \|({\mathbf{b}}’_1)^*\|\)</span> as <span class="math display">\[y_1 \leq \log \sqrt{\gamma_k} + \frac{x_1}{k}.\]</span> More generally, we have <span class="math display">\[\|({\mathbf{b}}’_i)^* \| \leq \sqrt{\gamma_k} \left(\frac{\prod_{j=1}^{i+k-1} \|{\mathbf{b}}_j^* \|}{\prod_{j=1}^{i-1} \|({\mathbf{b}}’_j)^* \|} \right)^{\frac1k}\]</span> which means for our variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> that <span class="math display">\[y_i = y_{i-1} + \log \| ({\mathbf{b}}’_i)^* \|
  \leq y_{i-1} + \log \sqrt{\gamma_k} + \frac{x_i – y_{i-1}}{k}
  = (1-\frac1k) y_{i-1} + \frac1k x_i + \log \sqrt{\gamma_k}.\]</span></p>
<p>Note that we can write each <span class="math inline">\(y_i\)</span> in terms of <span class="math inline">\(x_i\)</span> and the previous <span class="math inline">\(y_i\)</span> with only positive coefficients. So now we can apply induction to write each <span class="math inline">\(y_i\)</span> only in terms of the <span class="math inline">\(x_i\)</span>’s, which shows that <span class="math display">\[y_i = \frac1k \sum_{j=1}^i \omega^{i-j} x_j + (1-\omega)^i k \alpha\]</span> where we simplified notation a little by defining <span class="math inline">\(\alpha = \log \sqrt{\gamma_k}\)</span> and <span class="math inline">\(\omega = 1-\frac1k\)</span>. By collecting the <span class="math inline">\(x_i\)</span>’s and <span class="math inline">\(y_i\)</span>’s in a vector each, we have the vectorial inequality <span class="math display">\[{\mathbf{y}} \leq {\mathbf{A}} {\mathbf{x}} + {\mathbf{b}}\]</span> where <span class="math display">\[{\mathbf{b}} = \alpha k \left[
\begin{array}{c}
1 – \omega \\ 
\vdots \\
1 – \omega^{n-k} 
\end{array}\right]
\qquad\qquad
{\mathbf{A}} = \frac1k 
\left[
\begin{array}{cccc}
  1 &amp; &amp; &amp; \\
  \omega &amp; 1 &amp; &amp; \\
   \vdots &amp; \ddots &amp; \ddots &amp; \\
   \omega^{n-k-1} &amp; \cdots &amp; \omega &amp; 1
\end{array}
\right].\]</span></p>
<p>Now recall that after a forward tour, SDBKZ computes the reversed dual basis. Given the close relationship between the primal and the dual basis and their GSO, one can show that simply reversing the vector <span class="math inline">\({\mathbf{y}}\)</span> will yield the right variables <span class="math inline">\({\mathbf{x}}’_i\)</span> to start the next “forward tour” (which is actually a backward tour, but on the dual). I.e. after reversing <span class="math inline">\({\mathbf{y}}\)</span>, the variables represent the logarithm of the corresponding subdeterminants of the dual basis. (For this we assume for convenience and w.l.o.g. that the lattice has determinant 1; otherwise, there would be a scaling factor involved in this transformation.)</p>
<p>In summary, the effect on the vector <span class="math inline">\({\mathbf{x}}\)</span> of executing once the two steps, 1) forward tour and 2) computing the reversed dual basis, can be described as <span class="math display">\[{\mathbf{x}}’ \leq {\mathbf{R}} {\mathbf{A}} {\mathbf{x}} + {\mathbf{R}} {\mathbf{b}}\]</span> where <span class="math inline">\({\mathbf{R}}\)</span> is the reversed identity matrix (i.e. the identity matrix with reversed columns). Iterating the two steps simply means we will be iterating the vectorial inequality above. So analyzing the affine dynamical system <span class="math display">\[{\mathbf{x}} \mapsto {\mathbf{R}} {\mathbf{A}} {\mathbf{x}} + {\mathbf{R}} {\mathbf{b}}\]</span> will allow us to deduce information about the basis after a certain number of iterations.</p>
<h4 id="small-digression-affine-dynamical-systems">Small Digression: Affine Dynamical Systems</h4>
<p>Consider some dynamical system <span class="math inline">\({\mathbf{x}} \mapsto {\mathbf{A}} {\mathbf{x}} + {\mathbf{b}} \)</span> and assume it has exactly one fixed point, i.e. <span class="math inline">\({\mathbf{x}}^*\)</span> such that <span class="math inline">\({\mathbf{A}} {\mathbf{x}}^* + {\mathbf{b}} = {\mathbf{x}}^* \)</span>. We can write any input <span class="math inline">\({\mathbf{x}}’\)</span> as <span class="math inline">\({\mathbf{x}}’ = {\mathbf{x}}^* + {\mathbf{e}}\)</span> for some “error vector” <span class="math inline">\({\mathbf{e}}\)</span>. When applying the system to it, we get <span class="math inline">\({\mathbf{x}}’ \mapsto {\mathbf{A}} {\mathbf{x}}’ + {\mathbf{b}} = {\mathbf{x}}^* + {\mathbf{A}} {\mathbf{e}}\)</span>. So the error vector <span class="math inline">\({\mathbf{e}}\)</span> is mapped to <span class="math inline">\({\mathbf{A}} {\mathbf{e}}\)</span>. Applying this <span class="math inline">\(t\)</span> times maps <span class="math inline">\({\mathbf{e}}\)</span> to <span class="math inline">\({\mathbf{A}}^t {\mathbf{e}}\)</span>, which means after <span class="math inline">\(t\)</span> iterations the error vector has norm <span class="math inline">\(\|{\mathbf{A}}^t {\mathbf{e}} \|_{p} \leq \|{\mathbf{A}}^t \|_{p} \| {\mathbf{e}} \|_{p} \)</span> (where <span class="math inline">\(\| \cdot \|_{p}\)</span> is the matrix norm induced by the vector <span class="math inline">\(p\)</span>-norm). If we can show that <span class="math inline">\(\|{\mathbf{A}} \|_p \leq 1 – \epsilon\)</span>, then <span class="math inline">\(\|{\mathbf{A}}^t \|_p \leq \|A \|^t \leq (1-\epsilon)^t \leq e^{-\epsilon t}\)</span>, so the error vector will decay exponentially in <span class="math inline">\(t\)</span> with base <span class="math inline">\(e^{-\epsilon}\)</span> and the algorithm converges to the fixed point <span class="math inline">\({\mathbf{x}}^*\)</span>.</p>
<p>Back to our concrete system above. As we just saw, we can analyze its output quality by computing its fixed point and its running time by computing <span class="math inline">\(\|{\mathbf{R}} {\mathbf{A}} \|_p\)</span> for some induced matrix <span class="math inline">\(p\)</span>-norm. Since this has been a lenghty post already, I hope you’ll trust me that our system above has a fixed point <span class="math inline">\({\mathbf{x}}^*\)</span>, which can be written out explicitely in closed form. As a teaser, its first coordinate is <span class="math display">\[x^*_1 = \frac{(n-k)k}{k-1} \alpha.\]</span> This means that if the algorithm converges, it will converge to a basis such that <span class="math inline">\(\sum_{j \leq k}\log \| {\mathbf{b}}_j^*\| \leq \frac{(n-k)k}{k-1} \log \sqrt{\gamma_k}\)</span>. Applying Minkowski’s Theorem to the first block <span class="math inline">\({\mathbf{B}}_{[1,k]}\)</span> now shows that the shortest vector in this block satisfies <span class="math inline">\(\lambda_1({\mathbf{B}}_{[1,k]}) \leq \sqrt{\gamma_k}^{\frac{n-1}{k-1}}\)</span>. Note that the next forward tour will find a vector of such length. Recall that we assumed that our lattice has determinant 1, so this is exactly the Hermite factor achieved by Slide reduction, but for arbitrary block size (we do not need to assume that <span class="math inline">\(k\)</span> divides <span class="math inline">\(n\)</span>) and better than what we can achieve for BKZ (even using the same technique). Moreover, the fixed point actually gives us more information: the other coordinates (that I have ommited here) allow us control over all but <span class="math inline">\(k\)</span> GSO vectors and by terminating the algorithm at different positions, it allows us to choose which vectors we want control over.</p>
<p>It remains to show that the algorithm actually converges and figure out how fast. It is fairly straight-forward to show that <span class="math display">\[\|{\mathbf{R}} {\mathbf{A}}\|_{\infty} = \|{\mathbf{A}}\|_{\infty} = 1 – \omega^{n-k} \approx e^{-\frac{n-k}{k}}.\]</span> (Consider the last row of <span class="math inline">\({\mathbf{A}}\)</span>.) This is always smaller than 1, so the algorithm does indeed converge. For <span class="math inline">\(k = \Omega(n)\)</span> this is bounded far enough from 1 such that the system will converge to the fixed point up to an arbitrary constant in a number of SVP calls that is polynomial in <span class="math inline">\(n\)</span>. Using another change of variable [N16] or considering the relative error instead of the absolute error [MW15], one can show that this also holds for smaller <span class="math inline">\(k\)</span>.</p>
<p>As mentioned before, this type of analysis was introduced in [HPS11] and has inspired new ideas even in the heuristic analysis of BKZ. In particular, one can predict the behavior of BKZ by simply running such a dynamical system on typical inputs (and making some heuristic assumptions). This idea has been and is being used extensively in cryptanalysis and in optimizing parameters of state-of-the-art algorithms.</p>
<p>Finally, a few last words on SDBKZ: we have seen that it achieves a good Hermite factor, but what can we say about the approximation factor? I actually do not know if the algorithm achieves a good approximation factor and also do not see a good way to analyze it. However, there is a reduction [L86] from achieving approximation factor <span class="math inline">\(\alpha\)</span> to achieving Hermite factor <span class="math inline">\(\sqrt{\alpha}\)</span>. So SDBKZ can be used to achieve approximation factor <span class="math inline">\(\gamma_k^{\frac{n-1}{k-1}}\)</span>. This is a little unsatisfactory in two ways: 1) the reduction results in a different algorithm, and 2) the bound is a little worse than the factor achieved by slide reduction, which is <span class="math inline">\(\gamma_k^{\frac{n-k}{k-1}}\)</span>. On a positive note, a recent work [ALNS20] has shown that, due to the strong bound on the Hermite factor, SDBKZ can be used to generalize Slide reduction to arbitrary block size <span class="math inline">\(k\)</span> in a way to achieve the approximation factor <span class="math inline">\(\gamma_k^{\frac{n-k}{k-1}}\)</span>. Another recent work [ABFKSW20] exploited the fact that SDBKZ allows to heuristically predict large parts of the basis to achieve better bounds on the running time of the SVP oracle.</p>
<ul>
<li><p>Lov<span>á</span>sz. An Algorithmic Theory of Numbers, Graphs and Convexity. 1986</p></li>
<li><p>Hanrot, Pujol, Stehlé. Analyzing blockwise lattice algorithms using dynamical systems. CRYPTO 2011</p></li>
<li><p>Micciancio, Walter. Practical, predictable lattice basis reduction – Full Version. <a class="uri" href="http://eprint.iacr.org/2015/1123">http://eprint.iacr.org/2015/1123</a></p></li>
<li><p>Micciancio, Walter. Practical, predictable lattice basis reduction. EUROCRYPT 2016</p></li>
<li><p>Neumaier. Bounding basis reduction properties. Designs, Codes and Cryptography 2016</p></li>
<li><p>Aggarwal, Li, Nguyen, Stephens-Davidowitz. Slide Reduction, Revisited—Filling the Gaps in SVP Approximation. CRYPTO 2020</p></li>
<li><p>Albrecht, Bai, Fouque, Kirchner, Stehlé, Wen. Faster Enumeration-based Lattice Reduction: Root Hermite Factor <span class="math inline">\(k^{(1/(2k))}\)</span> in Time <span class="math inline">\(k^{(k/8 + o(k))}\)</span>. CRYPTO 2020</p></li>
</ul></div>
    </content>
    <updated>2020-08-28T11:56:55Z</updated>
    <published>2020-08-28T11:56:55Z</published>
    <category term="General"/>
    <author>
      <name>Michael Walter</name>
    </author>
    <source>
      <id>https://blog.simons.berkeley.edu</id>
      <link href="https://blog.simons.berkeley.edu/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blog.simons.berkeley.edu" rel="alternate" type="text/html"/>
      <subtitle>What's New at the Simons Institute for the Theory of Computing.</subtitle>
      <title>Calvin Café: The Simons Institute Blog</title>
      <updated>2020-09-07T23:23:42Z</updated>
    </source>
  </entry>
</feed>
