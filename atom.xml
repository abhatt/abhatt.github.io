<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-08-27T19:21:48Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-08-28-what-is-a-cryptographic-hash-function/</id>
    <link href="https://decentralizedthoughts.github.io/2020-08-28-what-is-a-cryptographic-hash-function/" rel="alternate" type="text/html"/>
    <title>What is a Cryptographic Hash Function?</title>
    <summary>If you ever tried to understand Bitcoin, you’ve probably banged your head against the wall trying to understand what is a cryptographic hash function? The goal of this post is to: Give you a very simple mental model for how hash functions work, called the random oracle model Give you...</summary>
    <updated>2020-08-28T17:05:00Z</updated>
    <published>2020-08-28T17:05:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-08-27T17:21:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.11454</id>
    <link href="http://arxiv.org/abs/2008.11454" rel="alternate" type="text/html"/>
    <title>Vertex Ordering Algorithms for Graph Coloring Problem</title>
    <feedworld_mtime>1598486400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Arda Asik, Ibrahim Bugra Demir, Berker Demirel, Baris Batuhan Topal, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaya:Kamer.html">Kamer Kaya</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.11454">PDF</a><br/><b>Abstract: </b>Graph coloring is a fundamental problem in combinatorics with many
applications in practice. In this problem, the vertices in a given graph must
be colored by using the least number of colors in such a way that a vertex has
a different color than its neighbors. The problem, as well as its different
variants, has been proven to be NP-Hard. Therefore, there are greedy algorithms
in the literature aiming to use a small number of colors. These algorithms
traverse the vertices and color them one by one. The vertex visit order has a
significant impact on the number of colors used. In this work, we investigated
if social network analytics metrics can be used to find this order. Our
experiments showed that when closeness centrality is used to find vertex visit
order, a smaller number of colors is used by the greedy algorithms.
</p></div>
    </summary>
    <updated>2020-08-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.11388</id>
    <link href="http://arxiv.org/abs/2008.11388" rel="alternate" type="text/html"/>
    <title>A cost-scaling algorithm for computing the degree of determinants</title>
    <feedworld_mtime>1598486400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hirai:Hiroshi.html">Hiroshi Hirai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ikeda:Motoki.html">Motoki Ikeda</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.11388">PDF</a><br/><b>Abstract: </b>In this paper, we address computation of the degree $\mathop{\rm deg Det} A$
of Dieudonn\'e determinant $\mathop{\rm Det} A$ of \[ A = \sum_{k=1}^m A_k x_k
t^{c_k}, \] where $A_k$ are $n \times n$ matrices over a field $\mathbb{K}$,
$x_k$ are noncommutative variables, $t$ is a variable commuting $x_k$, $c_k$
are integers, and the degree is considered for $t$. This problem generalizes
noncommutative Edmonds' problem and fundamental combinatorial optimization
problems including the weighted linear matroid intersection problem. It was
shown that $\mathop{\rm deg Det} A$ is obtained by a discrete convex
optimization on a Euclidean building. We extend this framework by incorporating
a cost scaling technique, and show that $\mathop{\rm deg Det} A$ can be
computed in time polynomial of $n,m,\log_2 C$, where $C:= \max_k |c_k|$. We
apply this result to an algebraic combinatorial optimization problem arising
from a symbolic matrix having $2 \times 2$-submatrix structure.
</p></div>
    </summary>
    <updated>2020-08-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.11321</id>
    <link href="http://arxiv.org/abs/2008.11321" rel="alternate" type="text/html"/>
    <title>High-Performance Parallel Graph Coloring with Strong Guarantees on Work, Depth, and Quality</title>
    <feedworld_mtime>1598486400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Besta:Maciej.html">Maciej Besta</a>, Armon Carigiet, Zur Vonarburg-Shmaria, Kacper Janda, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gianinazzi:Lukas.html">Lukas Gianinazzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoefler:Torsten.html">Torsten Hoefler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.11321">PDF</a><br/><b>Abstract: </b>We develop the first parallel graph coloring heuristics with strong
theoretical guarantees on work and depth and coloring quality. The key idea is
to design a relaxation of the vertex degeneracy order, a well-known graph
theory concept, and to color vertices in the order dictated by this relaxation.
This introduces a tunable amount of parallelism into the degeneracy ordering
that is otherwise hard to parallelize. This simple idea enables significant
benefits in several key aspects of graph coloring. For example, one of our
algorithms ensures polylogarithmic depth and a bound on the number of used
colors that is superior to all other parallelizable schemes, while maintaining
work-efficiency. In addition to provable guarantees, the developed algorithms
have competitive run-times for several real-world graphs, while almost always
providing superior coloring quality. Our degeneracy ordering relaxation is of
separate interest for algorithms outside the context of coloring.
</p></div>
    </summary>
    <updated>2020-08-27T01:20:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.11315</id>
    <link href="http://arxiv.org/abs/2008.11315" rel="alternate" type="text/html"/>
    <title>Inapproximability of Diameter in super-linear time: Beyond the 5/3 ratio</title>
    <feedworld_mtime>1598486400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonnet:=Eacute=douard.html">Édouard Bonnet</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.11315">PDF</a><br/><b>Abstract: </b>We show, assuming the Strong Exponential Time Hypothesis, that for every
$\varepsilon &gt; 0$, approximating directed Diameter on $m$-arc graphs within
ratio $7/4 - \varepsilon$ requires $m^{4/3 - o(1)}$ time. Our construction uses
nonnegative edge weights but even holds for sparse digraphs, i.e., for which
the number of vertices $n$ and the number of arcs $m$ satisfy $m = n
\log^{O(1)} n$. This is the first result that conditionally rules out a
near-linear time $5/3$-approximation for Diameter.
</p></div>
    </summary>
    <updated>2020-08-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.11292</id>
    <link href="http://arxiv.org/abs/2008.11292" rel="alternate" type="text/html"/>
    <title>Flip Paths Between Lattice Triangulations</title>
    <feedworld_mtime>1598486400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>William Sims, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sitharam:Meera.html">Meera Sitharam</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.11292">PDF</a><br/><b>Abstract: </b>The problem of finding a diagonal flip path between two triangulations has
been studied for nearly a century in the combinatorial (topological) setting
and for decades in the geometric setting. In the geometric setting, finding a
diagonal flip path between two triangulations that minimizes the number of
diagonal flips over all such paths is NP-complete. However, when restricted to
lattice triangulations - i.e. triangulations of a finite subset of the integer
lattice, or an affine transformation of this lattice, bounded by a simple,
closed polygon with lattice points as vertices - the problem has a polynomial
time algorithm. Lattice triangulations have been studied for their uses in
discriminant theory, Hilbert's 16th problem, toric varieties, quantum spin
systems, and material science. Our first main result shows that there is a
polynomial-time computable, unique partially-ordered set of diagonal flips such
that there is a bijection between valid linear-orderings of this set and
minimum diagonal flip paths between two lattice triangulations. This provides
an alternative proof of the previously known result, as well as new structural
insights into these diagonal flip paths. Our second main result characterizes
pairs of triangulations, containing sets of edges $E$ and $E'$ respectively,
such that the minimum diagonal flip path between them contains the minimum
number of diagonal flips over all minimum diagonal flip paths between pairs of
triangulations, containing sets of edges $E$ and $E'$ respectively. Remarkably,
all of our results are derived from a simple relationship between edges in
triangulations and Farey sequences.
</p></div>
    </summary>
    <updated>2020-08-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-08-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.11235</id>
    <link href="http://arxiv.org/abs/2008.11235" rel="alternate" type="text/html"/>
    <title>Accelerating Force-Directed Graph Drawing with RT Cores</title>
    <feedworld_mtime>1598486400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zellmann:Stefan.html">Stefan Zellmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weier:Martin.html">Martin Weier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wald:Ingo.html">Ingo Wald</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.11235">PDF</a><br/><b>Abstract: </b>Graph drawing with spring embedders employs a V x V computation phase over
the graph's vertex set to compute repulsive forces. Here, the efficacy of
forces diminishes with distance: a vertex can effectively only influence other
vertices in a certain radius around its position. Therefore, the algorithm
lends itself to an implementation using search data structures to reduce the
runtime complexity. NVIDIA RT cores implement hierarchical tree traversal in
hardware. We show how to map the problem of finding graph layouts with
force-directed methods to a ray tracing problem that can subsequently be
implemented with dedicated ray tracing hardware. With that, we observe speedups
of 4x to 13x over a CUDA software implementation.
</p></div>
    </summary>
    <updated>2020-08-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/08/26/postdoc-at-uc-berkeley-apply-by-september-10-2020/</id>
    <link href="https://cstheory-jobs.org/2020/08/26/postdoc-at-uc-berkeley-apply-by-september-10-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc at UC Berkeley (apply by September 10, 2020)</title>
    <summary>Please send a cover letter, CV, and research statement to the email below. In CV please list at least 3 references. In cover letter please identify faculty of interest. Also, have references submit letters to the e-mail below, with your name in the subject line. Multiple opportunities available; earliest deadline requires nomination by September 10th. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Please send a cover letter, CV, and research statement to the email below. In CV please list at least 3 references. In cover letter please identify faculty of interest. Also, have references submit letters to the e-mail below, with your name in the subject line. Multiple opportunities available; earliest deadline requires nomination by<br/>
September 10th.</p>
<p>Website: <a href="http://theory.cs.berkeley.edu/postdoc.html">http://theory.cs.berkeley.edu/postdoc.html</a><br/>
Email: tcs-postdoc-inquiries@lists.eecs.berkeley.edu</p></div>
    </content>
    <updated>2020-08-26T04:54:53Z</updated>
    <published>2020-08-26T04:54:53Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-08-27T19:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.11058</id>
    <link href="http://arxiv.org/abs/2008.11058" rel="alternate" type="text/html"/>
    <title>On the Maximum Number of Crossings in Star-Simple Drawings of $K_n$ with No Empty Lens</title>
    <feedworld_mtime>1598400000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Felsner:Stefan.html">Stefan Felsner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoffmann:Michael.html">Michael Hoffmann</a>, Kristin Knorr, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parada:Irene.html">Irene Parada</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.11058">PDF</a><br/><b>Abstract: </b>A star-simple drawing of a graph is a drawing in which adjacent edges do not
cross. In contrast, there is no restriction on the number of crossings between
two independent edges. When allowing empty lenses (a face in the arrangement
induced by two edges that is bounded by a 2-cycle), two independent edges may
cross arbitrarily many times in a star-simple drawing. We consider star-simple
drawings of $K_n$ with no empty lens. In this setting we prove an upper bound
of $3((n-4)!)$ on the maximum number of crossings between any pair of edges. It
follows that the total number of crossings is finite and upper bounded by $n!$.
</p></div>
    </summary>
    <updated>2020-08-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-08-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.10932</id>
    <link href="http://arxiv.org/abs/2008.10932" rel="alternate" type="text/html"/>
    <title>Faster Reachability in Static Graphs</title>
    <feedworld_mtime>1598400000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanauer:Kathrin.html">Kathrin Hanauer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schulz:Christian.html">Christian Schulz</a>, Jonathan Trummer <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.10932">PDF</a><br/><b>Abstract: </b>One of the most fundamental problems in computer science is the reachability
problem: Given a directed graph and two vertices s and t, can s reach t via a
path? We revisit existing techniques and combine them with new approaches to
support a large portion of reachability queries in constant time using a
linear-sized reachability index. In an experimental study, we compare a variety
of algorithms with respect to their index-building and query times as well as
their memory footprint. All of them yield a time/space trade-off for queries.
Surprisingly, due to cache effects, a higher investment in space doesn't
necessarily pay off: Reachability queries can often be answered even
significantly faster than single memory accesses in a precomputed full
reachability matrix.
</p></div>
    </summary>
    <updated>2020-08-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.10898</id>
    <link href="http://arxiv.org/abs/2008.10898" rel="alternate" type="text/html"/>
    <title>PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization</title>
    <feedworld_mtime>1598400000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Zhize.html">Zhize Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bao:Hongyan.html">Hongyan Bao</a>, Xiangliang Zhang, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Richt=aacute=rik:Peter.html">Peter Richtárik</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.10898">PDF</a><br/><b>Abstract: </b>In this paper, we propose a novel stochastic gradient
estimator---ProbAbilistic Gradient Estimator (PAGE)---for nonconvex
optimization. PAGE is easy to implement as it is designed via a small
adjustment to vanilla SGD: in each iteration, PAGE uses the vanilla minibatch
SGD update with probability $p$ and reuses the previous gradient with a small
adjustment, at a much lower computational cost, with probability $1-p$. We give
a simple formula for the optimal choice of $p$. We prove tight lower bounds for
nonconvex problems, which are of independent interest. Moreover, we prove
matching upper bounds both in the finite-sum and online regimes, which
establish that Page is an optimal method. Besides, we show that for nonconvex
functions satisfying the Polyak-\L{}ojasiewicz (PL) condition, PAGE can
automatically switch to a faster linear convergence rate. Finally, we conduct
several deep learning experiments (e.g., LeNet, VGG, ResNet) on real datasets
in PyTorch, and the results demonstrate that PAGE converges much faster than
SGD in training and also achieves the higher test accuracy, validating our
theoretical results and confirming the practical superiority of PAGE.
</p></div>
    </summary>
    <updated>2020-08-26T23:21:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.10895</id>
    <link href="http://arxiv.org/abs/2008.10895" rel="alternate" type="text/html"/>
    <title>Decentralized Custody Scheme with Game-Theoretic Security</title>
    <feedworld_mtime>1598400000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Zhaohua.html">Zhaohua Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Guang.html">Guang Yang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.10895">PDF</a><br/><b>Abstract: </b>Custodian is a core financial service in which the custodian holds in
safekeeping assets on behalf of the client. Although traditional custody
service is typically endorsed by centralized authorities, decentralized custody
scheme has become technically feasible since the emergence of digital assets,
and furthermore it is badly needed by new applications such as blockchain and
DeFi (Decentralized Finance). In this work, we propose a framework of
decentralized asset custody scheme that is able to support a large number of
custodians and safely hold customer assets of multiple times value of the total
security deposit. The proposed custody scheme distributes custodians and assets
into many custodian groups via combinatorial designs and random sampling, where
each group fully controls the assigned assets. Since every custodian group is
small, the overhead cost is significantly reduced. The liveness is also
improved because even a single alive group would be able to process
transactions. The security of this custody scheme is guaranteed in the
game-theoretic sense, such that any adversary corrupting a bounded fraction of
custodians cannot move assets more than his own security deposit. We further
analyze the security and performance of our constructions, and give explicit
examples with concrete numbers and figures for a better understanding of our
results.
</p></div>
    </summary>
    <updated>2020-08-26T23:22:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.10828</id>
    <link href="http://arxiv.org/abs/2008.10828" rel="alternate" type="text/html"/>
    <title>Efficient Hierarchical Clustering for Classification and Anomaly Detection</title>
    <feedworld_mtime>1598400000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ishita Doshi, Sreekalyan Sajjalla, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choudhari:Jayesh.html">Jayesh Choudhari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhatt:Rushi.html">Rushi Bhatt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dasgupta:Anirban.html">Anirban Dasgupta</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.10828">PDF</a><br/><b>Abstract: </b>We address the problem of large scale real-time classification of content
posted on social networks, along with the need to rapidly identify novel spam
types. Obtaining manual labels for user-generated content using editorial
labeling and taxonomy development lags compared to the rate at which new
content type needs to be classified. We propose a class of hierarchical
clustering algorithms that can be used both for efficient and scalable
real-time multiclass classification as well as in detecting new anomalies in
user-generated content. Our methods have low query time, linear space usage,
and come with theoretical guarantees with respect to a specific hierarchical
clustering cost function (Dasgupta, 2016). We compare our solutions against a
range of classification techniques and demonstrate excellent empirical
performance.
</p></div>
    </summary>
    <updated>2020-08-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.10794</id>
    <link href="http://arxiv.org/abs/2008.10794" rel="alternate" type="text/html"/>
    <title>Simple Topological Drawings of $k$-Planar Graphs</title>
    <feedworld_mtime>1598400000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoffmann:Michael.html">Michael Hoffmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Chih=Hung.html">Chih-Hung Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reddy:Meghana_M=.html">Meghana M. Reddy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/T=oacute=th:Csaba_D=.html">Csaba D. Tóth</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.10794">PDF</a><br/><b>Abstract: </b>Every finite graph admits a \emph{simple (topological) drawing}, that is, a
drawing where every pair of edges intersects in at most one point. However, in
combination with other restrictions simple drawings do not universally exist.
For instance, \emph{$k$-planar graphs} are those graphs that can be drawn so
that every edge has at most $k$ crossings (i.e., they admit a \emph{$k$-plane
drawing}). It is known that for $k\le 3$, every $k$-planar graph admits a
$k$-plane simple drawing. But for $k\ge 4$, there exist $k$-planar graphs that
do not admit a $k$-plane simple drawing. Answering a question by Schaefer, we
show that there exists a function $f : \mathbb{N}\rightarrow\mathbb{N}$ such
that every $k$-planar graph admits an $f(k)$-plane simple drawing, for all
$k\in\mathbb{N}$. Note that the function $f$ depends on $k$ only and is
independent of the size of the graph. Furthermore, we develop an algorithm to
show that every $4$-planar graph admits an $8$-plane simple drawing.
</p></div>
    </summary>
    <updated>2020-08-26T23:25:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-08-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.10709</id>
    <link href="http://arxiv.org/abs/2008.10709" rel="alternate" type="text/html"/>
    <title>Algorithms and Lower Bounds for the Worker-Task Assignment Problem</title>
    <feedworld_mtime>1598400000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berger:Aaron.html">Aaron Berger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuszmaul:William.html">William Kuszmaul</a>, Adam Polak, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tidor:Jonathan.html">Jonathan Tidor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wein:Nicole.html">Nicole Wein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.10709">PDF</a><br/><b>Abstract: </b>We study the problem of assigning workers to tasks where each task has demand
for a particular number of workers, and the demands are dynamically changing
over time. Specifically, a worker-task assignment function $\phi$ takes a
multiset of $w$ tasks $T \subseteq [t]$ and produces an assignment $\phi(T)$
from the workers $1, 2, \ldots, w$ to the tasks $T$. The assignment function
$\phi$ is said to have switching cost at most $k$ if, for all task multisets
$T$, changing the contents of $T$ by one task changes $\phi(T)$ by at most $k$
worker assignments. The goal of the worker-task assignment problem is to
produce an assignment function $\phi$ with the minimum possible switching cost.
</p>
<p>Prior work on this problem (SSS'17, ICALP'20) observed a simple assignment
function $\phi$ with switching cost $\min(w, t - 1)$, but there has been no
success in constructing $\phi$ with sublinear switching cost. We construct the
first assignment function $\phi$ with sublinear, and in fact polylogarithmic,
switching cost. We give a probabilistic construction for $\phi$ that achieves
switching cost $O(\log w \log (wt))$ and an explicit construction that achieves
switching cost $\operatorname{polylog} (wt)$.
</p>
<p>From the lower bounds side, prior work has used involved arguments to prove
constant lower bounds on switching cost, but no super-constant lower bounds are
known. We prove the first super-constant lower bound on switching cost. In
particular, we show that for any value of $w$ there exists a value of $t$ for
which the optimal switching cost is $w$. That is, when $w \ll t$, the trivial
bound on switching cost is optimal.
</p>
<p>We also consider an application of the worker-task assignment problem to a
metric embeddings problem. In particular, we use our results to give the first
low-distortion embedding from sparse binary vectors into low-dimensional
Hamming space.
</p></div>
    </summary>
    <updated>2020-08-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=790</id>
    <link href="https://emanueleviola.wordpress.com/2020/08/25/my-monitor-setup/" rel="alternate" type="text/html"/>
    <title>My monitor setup</title>
    <summary>In the not-distant future there will be a single monitor that gets you the best of both worlds. For the contemporaneous, I maintain that the above is the best monitor setup available to us in 2020. I use the tiny E-ink monitor as much as possible, including now, for my blitz matches on chess.com, and […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><figure class="wp-block-image size-large"><img alt="" class="wp-image-792" src="https://emanueleviola.files.wordpress.com/2020/08/webcam-toy-photo7.jpg?w=800"/></figure>

<p>In the not-distant future there will be a single monitor that gets you the best of both worlds. For the contemporaneous, I maintain that the above is the best monitor setup available to us in 2020. I use the tiny E-ink monitor as much as possible, including now, for my blitz matches on chess.com, and of course for writing and sometimes reviewing papers. But as I mentioned earlier unfortunately for certain bureaucratic tasks that not all of us can skip altogether you just need a bigger monitor with color. So I push a button on the hdmi switch, and the image blasts open on the 30-inch screen, <em>m’illumino d’immenso</em>, and suddenly the mouse feels like the interface from <em>Minority Reports</em>.</p></div>
    </content>
    <updated>2020-08-25T19:35:48Z</updated>
    <published>2020-08-25T19:35:48Z</published>
    <category term="Uncategorized"/>
    <category term="health"/>
    <category term="tech"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2020-08-27T19:21:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/08/25/assistant-professor-tenure-track-and-professor-positions-in-computer-science-at-institute-of-science-and-technology-austria-apply-by-october-30-2020/</id>
    <link href="https://cstheory-jobs.org/2020/08/25/assistant-professor-tenure-track-and-professor-positions-in-computer-science-at-institute-of-science-and-technology-austria-apply-by-october-30-2020/" rel="alternate" type="text/html"/>
    <title>Assistant Professor (tenure-track) and Professor positions in Computer Science at Institute of Science and Technology Austria (apply by October 30, 2020)</title>
    <summary>The Institute of Science and Technology Austria (IST Austria) invites applications for several open positions in all areas of computer science. IST Austria values diversity and is committed to equal opportunity. We strive for increasing the number of women, particularly in fields where they are underrepresented, and therefore we strongly encourage female researchers to apply. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Institute of Science and Technology Austria (IST Austria) invites applications for several open positions in all areas of computer science.</p>
<p>IST Austria values diversity and is committed to equal opportunity. We strive for increasing the number of women, particularly in fields where they are underrepresented, and therefore we strongly encourage female researchers to apply.</p>
<p>Website: <a href="https://ist.ac.at/en/jobs/faculty/">https://ist.ac.at/en/jobs/faculty/</a><br/>
Email: faculty.recruiting@ist.ac.at</p></div>
    </content>
    <updated>2020-08-25T13:23:45Z</updated>
    <published>2020-08-25T13:23:45Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-08-27T19:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7790</id>
    <link href="https://windowsontheory.org/2020/08/24/highlights-of-algorithms-halg-free-aug-31-sep-2/" rel="alternate" type="text/html"/>
    <title>Highlights of Algorithms (HALG) -free – Aug 31- Sep 2</title>
    <summary>[Guest post by Yossi Azar] The 5th Highlights of Algorithms conference (HALG 2020) will take place Aug 31- Sep 2, 2020. http://highlightsofalgorithms.org/ The Highlights of Algorithms conference is a forum for presenting the highlights of recent developments in algorithms and for discussing potential further advances in this area. The conference will provide a broad picture […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Yossi Azar]</em></p>



<p>The 5th <strong>Highlights of Algorithms conference (HALG 2020)</strong> will take place Aug 31- Sep 2, 2020. <a href="http://highlightsofalgorithms.org/" rel="noreferrer noopener" target="_blank">http://highlightsofalgorithms.org/</a></p>



<p>The Highlights of Algorithms conference is a forum for presenting the highlights of recent developments in algorithms and for discussing potential further advances in this area. The conference will provide a broad picture of the latest research in algorithms through a series of invited talks, as well as short talks. <br/>Invited talks includes top algorithmic surveys and papers from FOCS/STOC/SODA/COLT/PODC/SPAA/ITCS/PODS (2019-20)</p>



<p><br/>PROGRAM<br/>The conference will take place online on Mon, Aug 31-Wed, Sep 2 from 12:00 noon until 19:30 CEST (Europe time) <a href="http://highlightsofalgorithms.org/programme" rel="noreferrer noopener" target="_blank">http://highlightsofalgorithms.org/programme</a></p>



<p>The short contributed talk are on Sep 1-2, 9:45-11:30 CEST <a href="http://highlightsofalgorithms.org/shorttalks" rel="noreferrer noopener" target="_blank">http://highlightsofalgorithms.org/shorttalks</a></p>



<p>REGISTRATION<br/>Registration is free but mandatory<br/>Please register on our webpage: <a href="https://highlightsofalgorithms2020.ethz.ch/registration" rel="noreferrer noopener" target="_blank">https://highlightsofalgorithms2020.ethz.ch/registration</a></p></div>
    </content>
    <updated>2020-08-24T19:17:23Z</updated>
    <published>2020-08-24T19:17:23Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-08-27T19:20:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/08/24/ideal-special-quarter-theory-of-deep-learning/</id>
    <link href="https://cstheory-events.org/2020/08/24/ideal-special-quarter-theory-of-deep-learning/" rel="alternate" type="text/html"/>
    <title>IDEAL Special Quarter (Theory of Deep Learning)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">September 21 – December 12, 2020 Online (https://www.ideal.northwestern.edu/special-quarters/fall-2020/) https://www.ideal.northwestern.edu/special-quarters/fall-2020/registration There will be a Special Quarter on Theory of Deep Learning this Fall as a part of IDEAL – The Institute for Data, Econometrics, Algorithms, and Learning, runs jointly with TTIC and the University of Chicago. The Special Quarter will be entirely online, and take place … <a class="more-link" href="https://cstheory-events.org/2020/08/24/ideal-special-quarter-theory-of-deep-learning/">Continue reading <span class="screen-reader-text">IDEAL Special Quarter (Theory of Deep Learning)</span></a></div>
    </summary>
    <updated>2020-08-24T16:02:27Z</updated>
    <published>2020-08-24T16:02:27Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-08-27T19:21:20Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1663724076500583508</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1663724076500583508/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/08/sharp-p-and-issue-of-natural-problems.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1663724076500583508" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1663724076500583508" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/08/sharp-p-and-issue-of-natural-problems.html" rel="alternate" type="text/html"/>
    <title>Sharp P and the issue of `natural problems'</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> #P was defined by Valiant as a way to pin down that the PERMANENT of a matrix is hard to compute.</p><p>The definition I give is equivalent to the one Valiant gave.</p><p>g is in #P if there exists p a poly and B in P such that</p><p>g(x) = | { y : |y| = p(|x|) and (x,y) \in B } |</p><p>A function f is #P-complete if g is in #P and for all g in #P,  f is poly-Turing reducible to g.</p><p>#SAT is the function that, given a formula, returns the number of satisfying assignments. It is #P-complete by looking at the proof the Cook-Levin Theorem. The reduction of f to #SAT only makes one query to #SAT. A common way to show that #A is #P-complete is to show that SAT \le A with a reduction that preserves the number of solutions. </p><p>Valiant proved that PERM was #P-complete (his reduction only used 1 call to PERM).</p><p>There are problems in P whose #-version is #-P complete: Matching and DNF-SAT are two of them.</p><p>Notice that I defined #SAT directly, not in terms of a poly p and a set B as above. Here is why: if you use poly p and set B one can do obnoxious things like: </p><p>SAT = { phi : exists yz 2n-bits long such that phi(y)=T and z is prime }</p><p>The # version of this definition is not really what I want (though I am sure its #P-complete).</p><p>Valiant (see <a href="https://www.math.cmu.edu/~af1p/Teaching/MCC17/Papers/enumerate.pdf">here</a> and <a href="http://www.math.cmu.edu/~af1p/Teaching/MCC17/Papers/permanent.pdf">here</a>) and Simon (see <a href="https://link.springer.com/content/pdf/10.1007%2F3-540-08342-1_37.pdf">here</a>) showed that  for many known NPC-problems A, #A is #P-complete. They meant NATURAL problems. Is it true for all natural NP-complete problems?</p><p>Unfortunately the statement `All NATURAL NPC problems give rise to #P-complete functions' is hard (impossible?) to state rigorously and hence hard (impossible?) to prove. </p><p>1) Is there a natural A in NP such that #A is NOT #P-complete (under assumptions)?</p><p>2) Are there any theorems that show a large set of NPC problems have #P counterparts? Or are we doomed to, when we want to show some #A is #P-complete, come up with a new proof?</p><p>3) Can one PROVE there are NPC problems A such that #A is NOT #P-complete? (under assumptions).</p></div>
    </content>
    <updated>2020-08-24T01:40:00Z</updated>
    <published>2020-08-24T01:40:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-08-27T12:43:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/08/22/bricards-jumping-octahedron</id>
    <link href="https://11011110.github.io/blog/2020/08/22/bricards-jumping-octahedron.html" rel="alternate" type="text/html"/>
    <title>Bricard’s jumping octahedron</title>
    <summary>The Schönhardt polyhedron is a non-convex octahedron that can be formed from a convex regular octahedron by twisting two opposite faces, stretching and deforming the other faces as you twist. It’s well known for not having any interior diagonals, and for being impossible to subdivide into tetrahedra without introducing new vertices. But long before Erich Schönhardt described it in 1928 in connection with these properties, Raoul Bricard was investigating flexible octahedra, in connection with Cauchy’s theorem on the rigidity of polyhedra. The Schönhardt polyhedron forms an interesting example of flexibility, as I learned from a 1975 collection of lecture notes by Branko Grünbaum on “Lost Mathematics”). I’m not entirely sure that it was known to Bricard (it’s not clear from Bricard’s paper and Grünbaum doesn’t really say so) but it wouldn’t surprise me if it was.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="https://en.wikipedia.org/wiki/Sch%C3%B6nhardt_polyhedron">Schönhardt polyhedron</a> is a non-convex octahedron that can be formed from a convex regular octahedron by twisting two opposite faces, stretching and deforming the other faces as you twist. It’s well known for not having any interior diagonals, and for being impossible to subdivide into tetrahedra without introducing new vertices. But long before Erich Schönhardt described it in 1928 in connection with these properties, Raoul Bricard was investigating <a href="https://en.wikipedia.org/wiki/Bricard_octahedron">flexible octahedra</a>, in connection with <a href="https://en.wikipedia.org/wiki/Cauchy%27s_theorem_(geometry)">Cauchy’s theorem on the rigidity of polyhedra</a>. The Schönhardt polyhedron forms an interesting example of flexibility, as I learned from a 1975 collection of lecture notes by Branko Grünbaum on “<a href="https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/15700/Lost%20Mathematics.pdf?fterence=1">Lost Mathematics</a>”). I’m not entirely sure that it was known to Bricard (it’s not clear from Bricard’s paper and Grünbaum doesn’t really say so) but it wouldn’t surprise me if it was.</p>

<p>Cauchy’s theorem states that the shape of every convex polyhedron is uniquely determined by the shapes and connectivity of its faces. There can be no other convex polyhedron that has faces of the same shape, connected in the same way. But in some cases (like the regular icosahedron) you can dent some of the faces in to make a different, non-convex polyhedron with the same face shapes and connectivity. So Cauchy’s theorem doesn’t immediately extend to non-convex polyhedra. In fact, certain non-convex “<a href="https://en.wikipedia.org/wiki/Flexible_polyhedron">flexible polyhedra</a>” can deform continuously into an infinite range of shapes, without changing the shape or connectivity of their faces. Bricard’s octahedra are self-crossing examples and later investigators found examples without self-crossings.</p>

<p>But Grünbaum describes a different, non-self-crossing non-convex octahedron, the “jumping octahedron”. Rather than having a continuous range of rigid shapes, it has exactly two shapes, both of which have faces of the same shapes and connectivity. Unlike the example of the regular icosahedron and dented icosahedron, the two shapes have dihedral angles that are convex and concave in the same places. If you make this polyhedron out of perfectly rigid faces, with hinged connections at their edges, it could only be in one or the other of its two shapes: you wouldn’t be able to get it to the other shape without taking it apart and rebuilding it. But if you make it out of a material that’s stiff enough to hold its shape but flexible enough to deform a little, you can make a model that jumps or snaps from one shape to the other when you twist it. If you deform it a little out of shape, it will snap back to the nearest of its two valid shapes. Here’s one I made very roughly from some light cardstock and transparent tape, in its two shapes, one with only slightly-concave long diagonals down its sides and the other much more twisted and folded up:</p>

<div><table style="margin-left: auto; margin-right: auto;">
<tbody><tr style="text-align: center; vertical-align: middle;">
<td style="padding: 10px;"><img alt="Jumping octahedron" src="http://www.ics.uci.edu/~eppstein/pix/jumping-octahedron/1-m.jpg" style="border-style: solid; border-color: black;" width="315"/></td>
<td style="padding: 10px;"><img alt="Jumping octahedron" src="http://www.ics.uci.edu/~eppstein/pix/jumping-octahedron/2-m.jpg" style="border-style: solid; border-color: black;" width="315"/></td>
</tr></tbody></table></div>

<p>My model makes an interesting squelchy sound when I twist it from the more upright shape to the more twisted one, because these two shapes have different volumes and the air has to get out through the cracks between the faces. If I had perfectly sealed all these cracks, the pressure change would prevent it from changing shape. This change in volume is a big contrast from the Bricard octahedra and other continuously-flexible polyhedra, which must maintain constant volume as they flex.</p>

<p>The net I folded it from looks like this:</p>

<p style="text-align: center;"><img alt="Net for jumping octahedron" src="https://11011110.github.io/blog/assets/2020/jumping-octahedron-net.svg"/></p>

<p>It’s in two parts in order to make the seams of my model be symmetric, but also that way it fits better onto a single sheet of paper or card. It consists of two equilateral-triangle faces (the faces at the top and bottom of the model shown above) and six identical obtuse triangles on the sides. In my net and model, these triangles are isosceles, but that’s not important. The important part is that, if one of these triangles is projected onto the line between it and the equilateral triangle, its projected length is slightly more than the length of the connecting edge (because it’s an obtuse triangle) but not too long: longer by a factor strictly between one and</p>

\[\frac{1}{2}+\frac{1}{\sqrt{3}}\approx 1.07735.\]

<p>If you make the projection too short (with a right or acute side triangle shape) then it will not have two different shapes that maintain the same faces and convex-concave relation at each dihedral. If you make the projection too long, then you won’t be able to put it together at all while keeping all the faces flat. An explanation for some of this behavior can be seen from the diagram below, which shows the bottom equilateral triangle and one of the obtuse side triangles of the polyhedron, flattened out into a single plane, from a top view.</p>

<p style="text-align: center;"><img alt="Overhead view of two faces of the jumping octahedron" src="https://11011110.github.io/blog/assets/2020/jumping-octahedron-overhead.svg"/></p>

<p>If you fold these two triangles on their connecting edge, keeping the bottom equilateral triangle fixed but lifting the obtuse triangle into space, then the outer vertex of the obtuse triangle will rotate through a semicircle, but the plane of this semicircle is perpendicular to the plane of view of the diagram, so in top view it just looks like a line, the red line in the diagram. The edge along which the two triangles are attached is the axis of rotation, so it’s perpendicular to the semicircle of rotation and to the projected red line. If you fold all three edges of the bottom equilateral triangle at equal angles, then by symmetry the tips of the three folded obtuse triangles will form another equilateral triangle, and the size of this equilateral triangle will depend on the fold angle. If the vertices of this second equilateral triangle project to points on the yellow circle (the circumcircle of the bottom equilateral triangle) then this triangle will have exactly the correct size to attach the top face. This is only possible when the vertex of the obtuse triangle in the drawing is folded to a point that projects to of the two crossings of the red line and the yellow circle. The two fold angles for which this happens give the two shapes of the jumping octahedron.</p>

<p>The constraint that the side triangles be obtuse is what is needed to make the two crossing points of the red line and the circle be in the same arc of the circle relative to the vertices of the bottom equilateral triangle. The constraint that their projected length should be only a little bit longer than the side length of the equilateral triangle is what is needed to make the red line cross the circle at all. So these two constraints are necessary to make the jumping octahedron work. There’s one more necessary constraint: the height of the obtuse triangle above the edge connecting it to the equilateral triangle has to be large enough to reach both of the crossing points of the red line. When I started to make the model I was worried about a different geometric constraint: maybe the twisted state of the model is so twisted that its inner folded parts cross each other near the center of the model? But that can’t happen. If it did happen, the projected view of the model would have the side triangles folded into a position where they cover the center of the yellow circle. But that would mean that the top triangle’s vertices are too far around the yellow circle, past the point where the farthest point of the red line can cross.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104737012685827990">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-08-22T17:46:00Z</updated>
    <published>2020-08-22T17:46:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-08-23T07:53:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20013</id>
    <link href="https://gilkalai.wordpress.com/2020/08/22/quantum-matters/" rel="alternate" type="text/html"/>
    <title>Quantum Matters</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A comparison between the Google estimator U for the fidelity and two improved estimators that we studied  MLE (maximum likelihood estimator) and V (a variant of U). (More figures at the end of the post.) Here are some links on … <a href="https://gilkalai.wordpress.com/2020/08/22/quantum-matters/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2020/08/fig_2.jpeg"><img alt="" class="alignnone size-full wp-image-20101" height="472" src="https://gilkalai.files.wordpress.com/2020/08/fig_2.jpeg?w=640&amp;h=472" width="640"/></a></p>
<p><span style="color: #ff0000;">A comparison between the Google estimator U for the fidelity and two improved estimators that we studied  MLE (maximum likelihood estimator) and V (a variant of U). (More figures at the end of the post.)</span></p>
<p>Here are some links on quantum matters. I hope to return to them in more detail in some future posts.</p>
<h2>1. A <a href="https://gilkalai.files.wordpress.com/2019/11/stat-quantum2.pdf">paper</a> with Yosi Rinott and Tomer Shoham on the Statistics of Google’s experiment</h2>
<p>Yosef Rinott, Tomer Shoham and Gil Kalai:  <a href="https://gilkalai.files.wordpress.com/2019/11/stat-quantum2.pdf">Statistical aspects of the quantum supremacy demonstration</a>, (<a href="https://arxiv.org/abs/2008.05177">arXive</a>)</p>
<p><strong>Abstract:</strong></p>
<blockquote><p><span style="color: #0000ff;"><em>The notable claim of quantum supremacy presented by Google’s team in 2019 consists of demonstrating the ability of a quantum circuit to generate, albeit with considerable noise, bitstrings from a distribution that is considered hard to simulate on classical computers. Verifying that the generated data is indeed from the claimed distribution and assessing the circuit’s noise level and its fidelity is a purely statistical undertaking.</em></span></p>
<p><span style="color: #0000ff;"><em>The objective of this paper is to explain the relations between quantum computing and some of the statistical aspects involved in demonstrating quantum supremacy in terms that are accessible to statisticians, computer scientists, and mathematicians.</em></span></p>
<p><span style="color: #0000ff;"><em>Starting with the statistical analysis in Google’s demonstration, which we explain, we study various estimators of the fidelity, and different approaches to testing the distributions generated by the quantum computer. We propose different noise models, and discuss their implications. A preliminary study of the Google data, focusing mostly on circuits of 12 and 14 qubits is discussed throughout the paper.</em></span></p></blockquote>
<p>I am greatly enjoying working with Yosi and Tomer, and I hope to devote a special post to the very interesting statistics of the Google supremacy experiment.</p>
<h2/>
<h2>2. My paper <a href="https://gilkalai.files.wordpress.com/2020/08/laws-blog2.pdf">The Argument against Quantum Computers, the Quantum Laws of Nature, and Google’s Supremacy Claims</a></h2>
<p>Here is how the paper concludes</p>
<blockquote><p><span style="color: #0000ff;"><em>Over the past four decades, the very idea of quantum computation has led to many advances in several areas of physics, engineering, computer science, and mathematics. I expect that the most important application will eventually be the understanding of the impossibility of quantum error-correction and quantum computation. Overall, the debate over quantum computing is a fascinating one, and I can see a clear silver lining: major advances in human ability to simulate quantum physics and quantum chemistry are expected to emerge if quantum computational supremacy can be demonstrated and quantum computers can be built, but also if quantum computational supremacy cannot be demonstrated and quantum computers cannot be built.</em></span></p>
<p><span style="color: #0000ff;"><em>Some of the insights and methods characteristic of the area of quantum computation might be useful for classical computation of realistic quantum systems – which is, apparently, what nature does.</em></span></p></blockquote>
<p> </p>
<p>The link above is the most recent version that will be updated; Here is the <a href="https://arxiv.org/abs/2008.05188">arXive version</a>. A discussion on <a href="https://news.ycombinator.com/item?id=23291071">Hacker News</a>.</p>
<h2/>
<h2>3. My Dec 2019 surprise lecture and the panel discussion</h2>
<p>My Dec  19 2019 (B.C.) surprise lecture at the mathematics of quantum computing school and the afternoon panel on the same day. It turned out that the lecture was videotaped. The slides can be seen in <a href="https://gilkalai.wordpress.com/2019/12/27/the-google-quantum-supremacy-demo/">this post</a>. Remarkably, social distancing was pioneered by the session chair toward the end of the lecture (while not justified in that case).</p>
<p/>
<p>Here once again again is <a href="https://youtu.be/_Yb7uIGBynU">the link for the panel discussion on quantum supremacy</a> of the same day (<a href="https://gilkalai.wordpress.com/2019/12/27/the-google-quantum-supremacy-demo/">reviewed here</a>) . Here is a quote of mine from the panel.</p>
<blockquote><p><em><span style="color: #0000ff;">Of course, it is important to think what are the implications of quantum supremacy, is it useful? what does it say on the extended Church-Turing thesis? on prospects for quantum error-correction and universal quantum computers? etc. but I think that in the next few years one thing that we need to also concentrate on is the following question: Is the Google experiment correct? Is this a correct scientific verification of quantum supremacy?</span></em></p></blockquote>
<h2/>
<h2>4. My July 15 USTLC lecture</h2>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2020/08/4slides.png"><img alt="" class="alignnone size-full wp-image-20091" height="364" src="https://gilkalai.files.wordpress.com/2020/08/4slides.png?w=640&amp;h=364" width="640"/></a></p>
<p><span style="color: #ff0000;">Four slides from my USTLC zoom lecture</span><span style="color: #ff0000;">. (Click to enlarge.)<br/>
</span></p>
<p>Here is the <a href="https://idc-il.zoom.us/rec/share/4v58MpbZ-CBJG7OO1E3SYYN-P426aaa82ndLr_cMyEgOuIwdOStnnIw18Xsg0dUr?fbclid=IwAR31ShatGHJ1bWpVCdBPUoWhG3VjqmhfD1kQBFVLGzmvtNlWNMW-T58_0dw">videotaped Zoom presentation</a> and <a href="https://gilkalai.files.wordpress.com/2019/11/july1.pptx">here are the slides</a>.</p>
<h2/>
<h2>5. A small taste of quantum poetry for the skeptics. (A full post is coming.)</h2>
<p><a href="https://gilkalai.files.wordpress.com/2019/12/dslide16.png"><img alt="" class="alignnone size-large wp-image-19013" height="362" src="https://gilkalai.files.wordpress.com/2019/12/dslide16.png?w=640&amp;h=362" width="640"/></a></p>
<p><span style="color: #ff0000;">Poems by Peter Shor and Renan Gross (click to enlarge)</span></p>
<p>Peter Shor <a href="https://twitter.com/PeterShor1/status/1199299777743204354">pioneered</a> quantum poetry for the skeptics over Twitter. There were many very nice contributions all over social media by <a href="https://gilkalai.files.wordpress.com/2019/12/dslide16.png">Renan Gross</a>, John Dowling, Nidit Nanda, ⟨dl|yonge|mallo⟩, Alfred Marcel Bruckstein, Kenneth Regan, and others. <strong>Keep the quantum poems coming!</strong> Of course, the poems should be taken with humor. Here is a small taste.</p>
<h3><span style="color: #993366;">My short response to Peter’s poem</span></h3>
<blockquote>
<h3><span style="color: #0000ff;"><em>Understanding nature and ourselves is a worthy dream</em></span><br/>
<span style="color: #0000ff;"><em>Requiring no interaction with the supreme</em></span></h3>
</blockquote>
<h3><span style="color: #993366;">Limericks</span></h3>
<h3>Jon Dowling</h3>
<p><span id="more-20013"/></p>
<blockquote><p><em>A quantum computer from Google,</em><br/>
<em>Turned the Church-Turing thesis to strudel.</em><br/>
<em>And yet there remain,</em><br/>
<em>Many doubting this claim,</em><br/>
<em>And we lash all of them with wet noodles.</em></p></blockquote>
<h3>Avi Wigderson</h3>
<blockquote><p><em>“There once was a quantum computer</em><br/>
<em>Whose pet was a five-legged hamster …”</em><br/>
<em>So Peter and Gil</em><br/>
<em>Their grandchildren will</em><br/>
<em>Tell “…happy they lived ever after”</em></p></blockquote>
<p>Avi suggests a kids’ chorus saying “yeah, right” or “sure, sure”, after each line</p>
<h3><span style="color: #993366;">Six-word stories</span></h3>
<h3>Ehud Friedgut</h3>
<blockquote>
<h3><span style="color: #0000ff;"><em>For sale: <span class="il">quantum</span> computer. Never used.</em></span></h3>
</blockquote>
<h3>Another 6-word story (mine) with a rhyme (of some sort)</h3>
<blockquote>
<h3><span style="color: #0000ff;"><em>Michelson and Morley weren’t </em></span><em><strong><span style="color: #0000ff;">G</span><span style="color: #ff0000;">o</span><span style="color: #ffcc00;">o</span><span style="color: #0000ff;">g</span><span style="color: #339966;">l</span><span style="color: #ff0000;">e</span></strong></em><span style="color: #0000ff;"><em> employees.</em></span></h3>
</blockquote>
<h2/>
<h2>6. More figures from my paper with Yosi and Tomer.</h2>
<p><a href="https://gilkalai.files.wordpress.com/2020/08/plot_3.jpeg"><img alt="" class="alignnone size-full wp-image-20102" height="350" src="https://gilkalai.files.wordpress.com/2020/08/plot_3.jpeg?w=640&amp;h=350" width="640"/></a></p>
<p><span style="color: #ff0000;">Various estimators for the fidelity for the Google data compared to simulations. </span></p>
<p><a href="https://gilkalai.files.wordpress.com/2020/08/fig_8.jpeg"><img alt="" class="alignnone size-full wp-image-20103" height="519" src="https://gilkalai.files.wordpress.com/2020/08/fig_8.jpeg?w=640&amp;h=519" width="640"/></a></p>
<p><span style="color: #ff0000;">The model expected values compared to the empirical distribution. On the left for the real data and on the right for simulated data.  As it turned out the Google noise model does not fit for the sample data. (Our readout model provides only a small improvement.)  </span></p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2020/08/hists.jpeg"><img alt="" class="alignnone size-full wp-image-20104" height="249" src="https://gilkalai.files.wordpress.com/2020/08/hists.jpeg?w=640&amp;h=249" width="640"/></a></p>
<p><span style="color: #ff0000;">The size biased distribution fits the model very well.</span></p></div>
    </content>
    <updated>2020-08-22T17:02:49Z</updated>
    <published>2020-08-22T17:02:49Z</published>
    <category term="Quantum"/>
    <category term="Statistics"/>
    <category term="Tomer Shoham"/>
    <category term="Yosi Rinott"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-08-27T19:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/127</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/127" rel="alternate" type="text/html"/>
    <title>TR20-127 |  $k$-Forrelation Optimally Separates Quantum and Classical Query Complexity | 

	Nikhil Bansal, 

	Makrand Sinha</title>
    <summary>Aaronson and Ambainis (SICOMP '18) showed that any partial function on $N$ bits that can be computed with an advantage $\delta$ over a random guess by making $q$ quantum queries, can also be computed classically with an advantage $\delta/2$ by a randomized decision tree making ${O}_q(N^{1-\frac{1}{2q}}\delta^{-2})$ queries. Moreover, they conjectured the $k$-Forrelation problem --- a partial function that can be computed with $q = \lceil k/2 \rceil$ quantum queries --- to be a suitable candidate for exhibiting such an extremal separation. 
    
     We prove their conjecture by showing a tight lower bound of $\widetilde{\Omega}_k(N^{1-1/k})$ for the randomized query complexity of $k$-Forrelation, where the advantage $\delta = 1/\mathrm{polylog}^k(N)$ and $\widetilde{\Omega}_k$ hides $\mathrm{polylog}^k(N)$ factors. Our proof relies on classical Gaussian tools, in particular, Gaussian interpolation and Gaussian integration by parts, and in fact, shows a more general statement, that to prove lower bounds for $k$-Forrelation against a family of functions, it suffices to bound the $\ell_1$-weight of the Fourier coefficients at levels $k, 2k, 3k, \ldots, (k-1)k$ for functions in the family.</summary>
    <updated>2020-08-21T19:02:19Z</updated>
    <published>2020-08-21T19:02:19Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-27T19:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/08/21/phd-or-postdoc-at-goethe-university-frankfurt-apply-by-august-31-2020/</id>
    <link href="https://cstheory-jobs.org/2020/08/21/phd-or-postdoc-at-goethe-university-frankfurt-apply-by-august-31-2020/" rel="alternate" type="text/html"/>
    <title>PhD or Postdoc at Goethe University Frankfurt (apply by August 31, 2020)</title>
    <summary>The research group by Holger Dell at Goethe University Frankfurt is inviting applications for a three-year PhD or Postdoc position, starting at the earliest possible date. Potential topics include the algorithmic theory of network science, algebraic graph algorithms, fine-grained and parameterized complexity, “classical” complexity theory, as well as adjacent areas. Website: https://www.t.cs.uni-frankfurt.de/positions/ Email: recruiting2020@holgerdell.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The research group by Holger Dell at Goethe University Frankfurt is inviting applications for a three-year PhD or Postdoc position, starting at the earliest possible date. Potential topics include the algorithmic theory of network science, algebraic graph algorithms, fine-grained and parameterized complexity, “classical” complexity theory, as well as adjacent areas.</p>
<p>Website: <a href="https://www.t.cs.uni-frankfurt.de/positions/">https://www.t.cs.uni-frankfurt.de/positions/</a><br/>
Email: recruiting2020@holgerdell.com</p></div>
    </content>
    <updated>2020-08-21T14:41:55Z</updated>
    <published>2020-08-21T14:41:55Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-08-27T19:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17421</id>
    <link href="https://rjlipton.wordpress.com/2020/08/19/logical-complexity-of-proofs/" rel="alternate" type="text/html"/>
    <title>Logical Complexity of Proofs</title>
    <summary>If you cannot find proofs, talk about them. Robert Reckhow with his advsior Stephen Cook famously started the formal study of the complexity of proofs with their 1979 paper. They were interested in the length of the shortest proofs of propositional statements. Georg Kreisel and others may have looked at proof length earlier, but one […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>If you cannot find proofs, talk about them.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/08/19/logical-complexity-of-proofs/rr/" rel="attachment wp-att-17427"><img alt="" class="alignright size-medium wp-image-17427" height="119" src="https://rjlipton.files.wordpress.com/2020/08/rr.png?w=300&amp;h=119" width="300"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Robert Reckhow with his advsior Stephen Cook famously started the formal study of the complexity of proofs with their 1979 <a href="https://www.cs.toronto.edu/~sacook/homepage/cook_reckhow.pdf">paper</a>. They were interested in the length of the shortest <a href="https://en.wikipedia.org/wiki/Proof_complexity">proofs</a> of propositional statements. Georg Kreisel and others may have looked at proof length earlier, but one of the key insights of Reckhow and Cook is that low level propositional logic is important.</p>
<p>
Today I thought we might look at the complexity of proofs.</p>
<p>
Cook and Reckhow were motivated by issues like: How hard is it to prove that a graph has no clique of a certain size? Or how hard to prove that some program halts on all inputs of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>? All of these questions ask about the length of proofs in a precise sense. Proofs have been around forever, back to Euclid at least, but Cook and Reckhow were the first to formally study the lengths of proofs. </p>
<p>
They were not directly interested in actual proofs. The kind you can find in the <a href="https://arxiv.org/archive/math">arXiv</a> or in a math journal, or at a conference—online or not. The kind that are in their paper.<br/>
<a href="https://rjlipton.wordpress.com/2020/08/19/logical-complexity-of-proofs/paper-5/" rel="attachment wp-att-17432"><img alt="" class="aligncenter size-medium wp-image-17432" height="60" src="https://rjlipton.files.wordpress.com/2020/08/paper.png?w=300&amp;h=60" width="300"/></a></p>
<p>We are talking today about these types of proofs. Not proofs that graphs have cliques. But proofs that a no planar graph can have a <img alt="{5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{5}"/> clique. </p>
<p><a href="https://rjlipton.wordpress.com/2020/08/19/logical-complexity-of-proofs/unknown-144/" rel="attachment wp-att-17430"><img alt="" class="aligncenter size-full wp-image-17430" src="https://rjlipton.files.wordpress.com/2020/08/unknown.png?w=600"/></a></p>
<p>
</p><p/><h2> Proofs </h2><p/>
<p/><p>
Proofs are what we strive to find ever day. They the coin that measures progress in a mathematical field like complexity theory. We do sometimes work out examples, sometimes do computations to confirm conjectures on small examples, sometimes consider analogies to other proofs. But mostly we want to understand proofs. We want to create new ones and understand others proofs. </p>
<p>
Years ago when studying the graph isomorphism problem, I did some extensive computations for the random case. That is for the case of isomorphism for a random dense graphs against a worst case other graph. The computations helped me improve my result. It did not yield a proof, of course, but helped me realize that a certain lemma could be improved from a bound <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log n}"/> to <img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1)}"/>. My results were dominated by <a href="https://www.researchgate.net/profile/Stanley_Selkow/publication/220618511_Random_Graph_Isomorphism/links/00463537d337e6a35d000000/Random-Graph-Isomorphism.pdf">paper</a> of Laszlo Babai, Paul Erdös, and Stanley Selkow. Oh well. </p>
<p>
</p><p/><h2> Proofs Complexity </h2><p/>
<p/><p>
There are several measures of complexity for proofs. One is the length. Long proofs are difficult to find, difficult to write up, difficult to read, and difficult to check. Another less obvious measure is the logical structure of a proof. What does this mean?</p>
<p>
Our idea is that a proof can be modeled by a formula from propositional logic. The <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> is what we are trying to prove and the letters <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and so on are for statements we already know.  </p>
<li>
<img alt="{(A \rightarrow P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28A+%5Crightarrow+P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(A \rightarrow P)}"/>  This is a direct proof. <p/>
</li><li>
<img alt="{(\neg P \rightarrow \neg A)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cneg+P+%5Crightarrow+%5Cneg+A%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\neg P \rightarrow \neg A)}"/>  This is a proof by contradiction. <p/>
</li><li>
<img alt="{( A \vee \neg A \rightarrow P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28+A+%5Cvee+%5Cneg+A+%5Crightarrow+P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{( A \vee \neg A \rightarrow P)}"/>  This is proof that uses a statement <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> that may be true or false. <p/>
<p>
The last is a slight cheat, we use <img alt="{A \vee \neg A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Cvee+%5Cneg+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \vee \neg A}"/> to stand for a kind of axiom. A perfect example is from number theory. Let <img alt="{\pi(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pi(X)}"/> be the number of primes less than <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and the function <img alt="{li(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bli%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{li(x)}"/> the <a href="https://en.wikipedia.org/w/index.php?title=Logarithmic_integral_function&amp;action=edit&amp;section=1">logarithmic function</a>. 	</p>
<p align="center"><img alt="\displaystyle  li(x) = \int_0^x \frac{dt}{\ln t}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++li%28x%29+%3D+%5Cint_0%5Ex+%5Cfrac%7Bdt%7D%7B%5Cln+t%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  li(x) = \int_0^x \frac{dt}{\ln t}. "/></p>
<p>The prime number <a href="https://en.wikipedia.org/wiki/Prime_number_theorem">theorem</a> says that 	</p>
<p align="center"><img alt="\displaystyle  \pi(x) = li(x) + E(x), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpi%28x%29+%3D+li%28x%29+%2B+E%28x%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \pi(x) = li(x) + E(x), "/></p>
<p>an error term. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/08/19/logical-complexity-of-proofs/graph/" rel="attachment wp-att-17425"><img alt="" class="aligncenter size-medium wp-image-17425" height="171" src="https://rjlipton.files.wordpress.com/2020/08/graph.png?w=300&amp;h=171" width="300"/></a>
</td>
</tr>
<tr>
</tr>
</tbody></table>
<p>
It was noted that <img alt="{li(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bli%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{li(x)}"/> is larger than <img alt="{\pi(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pi(x)}"/> for known values. The obvious question was that could 	</p>
<p align="center"><img alt="\displaystyle  li(x) \ge \pi(x), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++li%28x%29+%5Cge+%5Cpi%28x%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  li(x) \ge \pi(x), "/></p>
<p>be always true? If so this would be an interesting inequality. In 1914 John Littlewood famously <a href="https://www.google.com/books/edition/_/2SUrpE8NK6sC?hl=en&amp;gbpv=1&amp;pg=PA33&amp;dq=John+Littlewood+pi+nd+li">proved</a> that this was not true: </p>
<blockquote><p><b>Theorem 1</b> <em> If the Riemann Hypothesis is true: 	</em></p><em>
<p align="center"><img alt="\displaystyle  \pi(x) - li(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpi%28x%29+-+li%28x%29+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \pi(x) - li(x) "/></p>
<p>is infinitely often positive and negative. If the Riemann Hypothesis is false: 	</p>
<p align="center"><img alt="\displaystyle  \pi(x) - li(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpi%28x%29+-+li%28x%29+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \pi(x) - li(x) "/></p>
</em><p><em>is infinitely often positive and negative. </em>
</p></blockquote>
<p/><p>
Thus he proved that 	</p>
<p align="center"><img alt="\displaystyle  \pi(x) - li(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpi%28x%29+-+li%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \pi(x) - li(x) "/></p>
<p>is infinitely often positive and negative whether the the Riemann is true or not. </p>
<p>
</p><p/><h2> Proofs in Trouble </h2><p/>
<p/><p>
A sign of a proof in danger is, in my opinion, is not just the length. A better measure I think is the logical flow of proof. I know of no actual proof that uses this structure: 	</p>
<p align="center"><img alt="\displaystyle  (A \rightarrow B) \rightarrow ((A \vee C) \rightarrow (B \vee C)) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28A+%5Crightarrow+B%29+%5Crightarrow+%28%28A+%5Cvee+C%29+%5Crightarrow+%28B+%5Cvee+C%29%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (A \rightarrow B) \rightarrow ((A \vee C) \rightarrow (B \vee C)) "/></p>
<p>Do you? Even if your proof is only a few lines or even pages, if the high level flow was the above tautology I would be worried. </p>
<p>
Another example is <img alt="{P \rightarrow P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP+%5Crightarrow+P%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P \rightarrow P}"/>. This of course is a circular proof. It seems hard to believe we would actually do this, but it has happen. The key is that no one says: I will assume the theorem to prove it. The flaw is disguised better than that.</p>
<p>
I cannot formally define this measure. Perhaps it is known, but I do think that it would be an additional measure. For actual proofs, ones we use every day, perhaps it would be valuable. I know I have looked at an attempted proof of X and noticed the logical flow in this sense was too complex. So complex that it was wrong. The author of the potential proof was me. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Is this measure, the logical flow of a proof, of any interest? </p>
<p/></li></font></font></div>
    </content>
    <updated>2020-08-19T13:00:52Z</updated>
    <published>2020-08-19T13:00:52Z</published>
    <category term="Oldies"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="complexity proof"/>
    <category term="Logic"/>
    <category term="logical flow"/>
    <category term="proof length"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-08-27T19:20:36Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-25562705.post-41257542542066199</id>
    <link href="http://aaronsadventures.blogspot.com/feeds/41257542542066199/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=41257542542066199" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/41257542542066199" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/41257542542066199" rel="self" type="application/atom+xml"/>
    <link href="http://aaronsadventures.blogspot.com/2020/08/moment-multicalibration-for-uncertainty.html" rel="alternate" type="text/html"/>
    <title>Moment Multicalibration for Uncertainty Estimation</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This blog post is about<a href="https://arxiv.org/abs/2008.08037" target="_blank"> a new paper</a> that I'm excited about, which is joint work with <a href="https://www.cis.upenn.edu/~chrjung/" target="_blank">Chris Jung</a>,<a href="https://economics.sas.upenn.edu/people/changhwa-lee" target="_blank"> Changhwa Lee</a>, <a href="https://sites.google.com/view/malleshpai/">Mallesh Pai</a>, and <a href="https://sites.google.com/site/quaerereverum9/">Ricky Vohra</a>. <div><br/></div><div>Suppose you are diagnosed with hypertension, and your doctor recommends that you take a certain drug to lower your blood pressure. The latest research, she tells you, finds that the drug lowers diastolic blood pressure by an average of 10 mm Hg. You remember your statistics class from college, and so you ask about confidence intervals. She looks up the paper, and tells you that it reports a 95% confidence interval of [5, 15]. How should you interpret this? </div><div><br/></div><div>What you might naively hope is that [5, 15] represents a <i>conditional prediction interval</i>. If you have some set of observable features $x$, and a label $y$ (in this case corresponding to your decrease in diastolic blood pressure after taking the drug), a 95% conditional prediction interval would promise that:</div><div>$$\Pr_y [y \in [5, 15] | x] \geq 0.95$$</div><div><br/></div><div>In other words, a conditional prediction interval would promise that given all of your observed features, <i>over the unrealized/unmeasured randomness of the world</i>, there is a 95% chance that your diastolic blood pressure will decrease by between 5 and 15 points. </div><div><br/></div><div>But if you think about it, coming up with a conditional prediction interval is essentially impossible in a rich feature space. If $x$ contains lots of information about you, then probably there was nobody in the original study population that exactly matched your set of features $x$, and so we have no information at all about the conditional distribution on $y$ given $x$ --- i.e. no samples at all from the distribution over which our coverage probability supposedly holds! So how can you expect any sort of promise at all? There are two typical ways around this difficulty. </div><div><br/></div><div>The first is to make heroic assumptions about the data generation process. For example, if we assume that the world looks like an ordinary least squares model, and that there is a linear relationship between $y$ and $x$, then we can form a confidence region around the parameters of the model, and from that derive prediction intervals. But these prediction intervals are not valid if the model fails to hold, which it inevitably will. </div><div><br/></div><div>The second is to give up on conditional prediction intervals, and instead give <i>marginal prediction intervals</i>. This is what the <a href="https://arxiv.org/abs/0706.3188" target="_blank">conformal prediction</a> literature aims to do. A marginal prediction interval looks quite similar to a conditional prediction interval (at least syntactically), and promises:</div><div>$$\Pr_{(x,y)} [y \in [5, 15] ] \geq 0.95$$</div><div><br/></div><div>Rather than conditioning on your features $x$, a marginal prediction interval averages over all people, and promises that 95% of people who take the drug have their diastolic blood pressure lowered by between 5 and 15 points. But the semantics of this promise are quite different than that of a conditional prediction interval. Because the average is now taken over a large, heterogeneous population, very little is promised to <i>you</i>. For example, it might be that for patients in your demographic group (e.g. middle aged women with Sephardic Jewish ancestry and a family history of diabetes) that the drug is actually expected to raise blood pressure rather than lower it. Because this subgroup represents less than 5% of the population, it is entirely consistent with the marginal prediction interval being correct. Of course, if you are lucky, then perhaps someone has conducted a study of people from this demographic group and has computed marginal prediction intervals over it! But what if there are multiple different groups that you are a member of, over which the results seem to conflict? For example, you might also have a low BMI value and have unusually good cholesterol readings --- features of a group for which the drug works unusually well. Which uncertainty estimate should you trust, if you are a member of both groups? </div><div><br/></div><div>These concerns actually arise already when we think about the semantics of mean estimations ("the expected drop in blood pressure amongst patients who take this drug is 10 mm Hg"). Ideally, if you were a patient with features $x$, then 10 would be an estimate of $\mathbb{E}[y | x]$. But just as with uncertainty estimation, in a large feature space, we typically have no information about the distribution on $y$ conditional on $x$ (because we have never met anyone exactly like <i>you</i> before), and so instead what we have is just an estimate of $\mathbb{E}[y]$ --- i.e. averaging over people. If you have a method of making predictions $f(x)$ as a function of features $x$, then a standard performance metric is <i>calibration</i> --- which informally asks that for every prediction $p$, amongst all people for whom we predicted $f(x) = p$, the average of the realized labels $y$ should be $p$. Again, estimates of this form promise little to individuals, because they are averages over a large and heterogeneous population.   </div><div><br/></div><div>Several years ago, <a href="https://arxiv.org/abs/1711.08513" target="_blank">Hebert-Johnson et al.</a> proposed a nice way to interpolate between the (impossible) ideal of offering conditional mean predictions  $f(x) = \mathbb{E}[y | x]$, and the weak guarantee of merely offering calibrated predictions $f$. Roughly speaking, they proposed to specify a very large collection of potentially intersecting groups $G$ (representing e.g. demographic groups like Sephardic Jewish women with a family history of diabetes, and hypertensive patients with low cholesterol and BMI values, etc) and to ask that a trained predictor be <i>simultaniously</i> calibrated on each sufficiently large group in $G$. They showed how to accomplish this using a polynomially sized sample from the underlying distribution, with polynomial running time overhead, on top of the cost of solving learning problems over $G$. </div><div><br/></div><div>In our paper, we --- roughly speaking --- show how to accomplish the same thing, but for variances and other higher moments, in addition to just means. And our "multicalibrated moment estimates" can be used to construct prediction intervals in exactly the same way that real moments of the conditional label distribution could be used. If you used the real (unknown) label distribution moments, you would have gotten conditional prediction intervals. If you use our multi-calibrated moments, you get marginal prediction intervals that are simultaneously valid as averaged over each of the groups in $G$. So, for example, our hypertensive patient above could interpret her prediction interval --- if it was constructed from multicalibrated moment estimates computed from her features --- as an average over each of the demographic groups that she is a member of (so long as they are contained within $G$), and all of those interpretations would be simultaneously valid. </div><div><br/></div><div>I'll leave the details to the paper --- including what exactly we mean by "moment multicalibration". I'll just note that a major difficulty is that variances and higher moments --- unlike expectations --- do not combine linearly, so it is no longer sensible to ask that "amongst all people for whom we predicted variance v, the true variance should be v" --- because even the true conditional label variances do not satisfy this property. But it <i>is </i>sensible to ask that a pair of mean and moment predictions be calibrated in this way: "amongst all people for whom we predicted mean $\mu$ and variance v, the true mean should be $\mu$ and the true variance should be $v$." This is what we call "mean-conditioned moment calibration", and it is satisfied by the true distributional moments. </div><div><br/></div><div>The paper is here: <a href="https://arxiv.org/abs/2008.08037">Moment Multicalibration for Uncertainty Estimation</a>.</div><div><br/></div></div>
    </content>
    <updated>2020-08-19T11:47:00Z</updated>
    <published>2020-08-19T11:47:00Z</published>
    <author>
      <name>Aaron</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/09952936358739421126</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-25562705</id>
      <category term="game theory"/>
      <category term="news"/>
      <author>
        <name>Aaron</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/09952936358739421126</uri>
      </author>
      <link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <title>Adventures in Computation</title>
      <updated>2020-08-19T11:47:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/126</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/126" rel="alternate" type="text/html"/>
    <title>TR20-126 |  Indistinguishability Obfuscation from Well-Founded Assumptions | 

	Aayush  Jain, 

	Huijia Lin, 

	Amit Sahai</title>
    <summary>In this work, we show how to construct indistinguishability obfuscation from subexponential hardness of four well-founded assumptions. We prove:

Let $\tau \in (0,\infty), \delta \in (0,1), \epsilon \in (0,1)$ be arbitrary constants. Assume sub-exponential security of the following assumptions, where $\lambda$ is a security parameter, and the parameters $\ell,k,n$ below are large enough polynomials in $\lambda$:

- The SXDH assumption on asymmetric bilinear groups of a prime order $p = O(2^\lambda)$,

- The LWE assumption over $\mathbb{Z}_{p}$ with subexponential modulus-to-noise ratio $2^{k^\epsilon}$, where $k$ is the dimension of the LWE secret,

- The LPN assumption over $\mathbb{Z}_p$ with polynomially many LPN samples and error rate $1/\ell^\delta$, where $\ell$ is the dimension of the LPN secret,

- The existence of a Boolean PRG in $\mathsf{NC}^0$ with stretch $n^{1+\tau}$,
 
Then, (subexponentially secure) indistinguishability obfuscation for all polynomial-size circuits exists.</summary>
    <updated>2020-08-19T11:37:19Z</updated>
    <published>2020-08-19T11:37:19Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-27T19:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/125</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/125" rel="alternate" type="text/html"/>
    <title>TR20-125 |  Efficient reconstruction of depth three circuits with top fan-in two | 

	Gaurav  Sinha</title>
    <summary>In this paper we develop efficient randomized algorithms to solve the black-box reconstruction problem for polynomials(over finite fields) computable by depth three arithmetic circuits with alternating addition/multiplication gates, such that top(output) gate is an addition gate with in-degree $2$. Such circuits naturally compute polynomials of the form $G\times(T_1 + T_2)$, where $G,T_1,T_2$ are product of affine forms computed at the first(addition) layer in the circuit, and polynomials $T_1,T_2$ have no common factors. Rank of such a circuit is defined to be the dimension of vector space spanned by all affine factors of $T_1$ and $T_2$. For any polynomial $f$ computable by such a circuit, $rank(f)$ is defined to be the minimum rank of any such circuit computing it. Our work develops randomized algorithms, which take as input a black-box computing polynomial $f$, with coefficients in a finite field $\mathbb{F}$, exhibiting such a circuit. Here are the results. 

$[$Low rank$]:$ When $5\leq r = rank(f) = O(\log^3 d)$, it runs in time $(nd^{\log^3d}\log |\mathbb{F}|)^{O(1)}$ and outputs a depth three circuit computing $f$ (with high probability), with top addition gate having in-degree $\leq d^{rank(f)}$.

$[$High rank$]:$ When $rank(f) = \Omega(\log^3 d)$, it runs in time $(nd\log |\mathbb{F}|)^{O(1)}$, and with high probability outputs a depth three circuit computing $f$, with top addition gate having in-degree $2$.

Prior to our work, black-box reconstruction for this circuit class was addressed in [Shp07, KS09, Sin16b]. Reconstruction algorithm in [Shp07] runs in time quasi-polynomial in $n,d,|\mathbb{F}|$ and that in [KS09] is quasi-polynomial in $d,|\mathbb{F}|$. Algorithm in [Sin16b] works only for polynomials over characteristic zero fields. Thus ours is the first blackbox reconstruction algorithm for this class of circuits that runs in time polynomial in $\log |\mathbb{F}|$. This problem has been mentioned as an open problem in [GKL12] (STOC 2012). In the high rank case, our algorithm runs in $(nd\log|\mathbb{F}|)^{O(1)}$ time, thereby significantly improving the existing algorithms in [Shp07, KS09].</summary>
    <updated>2020-08-17T19:11:47Z</updated>
    <published>2020-08-17T19:11:47Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-27T19:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/124</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/124" rel="alternate" type="text/html"/>
    <title>TR20-124 |  A Strong XOR Lemma for Randomized Query Complexity | 

	Joshua Brody, 

	JaeTak Kim, 

	Peem Lerdputtipongporn, 

	Hariharan Srinivasulu</title>
    <summary>We give a strong direct sum theorem for computing $XOR \circ g$.  Specifically, we show that the randomized query complexity of computing the XOR of $k$ instances of $g$ satisfies $\bar{R}_\varepsilon(XOR \circ g)=\Theta(\bar{R}_{\varepsilon/k}(g))$.  This matches the naive success amplification bound and answers a question of Blais and Brody.

As a consequence of our strong direct sum theorem, we give a total function $g$ for which $R(XOR \circ g) = \Theta(k\log(k)R(g))$, answering an open question from Ben-David et al.</summary>
    <updated>2020-08-17T13:19:41Z</updated>
    <published>2020-08-17T13:19:41Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-27T19:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/123</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/123" rel="alternate" type="text/html"/>
    <title>TR20-123 |  An Optimal Tester for k-Linear | 

	Nader Bshouty</title>
    <summary>A Boolean function $f:\{0,1\}^n\to \{0,1\}$ is $k$-linear if it returns the sum (over the binary field $F_2$) of $k$ coordinates of the input. In this paper, we study property testing of the classes $k$-Linear, the class of all $k$-linear functions, and $k$-Linear$^*$, the class $\cup_{j=0}^kj$-Linear.
We give a non-adaptive distribution-free two-sided $\epsilon$-tester for $k$-Linear that makes
$$O\left(k\log k+\frac{1}{\epsilon}\right)$$ queries.
This matches the lower bound known from the literature.

We then give a non-adaptive distribution-free one-sided $\epsilon$-tester for $k$-Linear$^*$ that makes the same number of queries and show that any non-adaptive uniform-distribution one-sided $\epsilon$-tester for $k$-Linear must make at least $ \tilde\Omega(k)\log n+\Omega(1/\epsilon)$ queries. The latter bound, almost matches the upper bound $O(k\log n+1/\epsilon)$ known from the literature. We then show that any adaptive uniform-distribution one-sided $\epsilon$-tester for $k$-Linear must make at least $\tilde\Omega(\sqrt{k})\log n+\Omega(1/\epsilon)$ queries.</summary>
    <updated>2020-08-17T13:17:37Z</updated>
    <published>2020-08-17T13:17:37Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-27T19:20:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6748297921609096715</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6748297921609096715/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/what-if-history-of-science-factoring.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6748297921609096715" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6748297921609096715" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/what-if-history-of-science-factoring.html" rel="alternate" type="text/html"/>
    <title>Mathematics is not commutative</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In my poll of P vs NP and other issues, one of the questions was<br/>
<br/>
<br/>
                             <i>Is factoring in P?</i><br/>
<i><br/></i>
One of the most interesting answers was<br/>
<br/>
<br/>
                            <i> I don't really see why it shouldn't be. - Peter Shor</i><br/>
<i><br/></i>
Recall that Peter Shor proved Factoring is in Quantum-P which lead to intense interest in Quantum Computing.<br/>
<br/>
1) What if factoring was in P and this was shown before Shor's algorithm? Would Shor or someone else have ever proven factoring in quantum P? Would there be as much intense interest in quantum computing as there is now? Perhaps by physicists more than CS people?<br/>
<br/>
2) What if factoring was in P and this was shown before RSA? Where would crypto be now? Zip drives with a googleplex random (or nearly random) bits and more 1-time pads? More lattice based crypto? Or RSA but with larger numbers? This may depend on how good the factoring algorithm is.<br/>
<br/>
3) More generally, how much does the order of events matter for science?<br/>
<br/>
a) If the Banach-Tarski paradox was discovered early on, would we have just tossed out the Axiom of Choice before so much more was build on it? Darling thinks we should toss out AC NOW because of Banach-Tarski.<br/>
<br/>
b) In the model of set theory L you can do ALL of math except some parts of set theory and maybe a few other things (note quite: Harvey Friedman has found some combinatorial statements that need large cardinals to prove). Had L been discovered earlier then could we all now be working in L (except a few people who look at other models, but they are not in the mainstream)? We might know more about L and less about forcing. We would KNOW that AC and CH are true. Or we would think we know.<br/>
<br/>
c) If  Engineers were the first ones to look at SAT and reductions, might they have been content to know that  if SAT \le A then A is probably hard? No need for the Cook-Levin Theorem! And then when someone proved Cook-Levin would the Engineers not really cares since they already knew SAT was hard?<br/>
<br/>d) I can imagine Ramsey's Theorem being discovered much later for some application, or perhaps never being discovered at all.<div><br/></div><div>e) VDW's theorem has so few application, I can imagine it never being discovered. </div><div><br/></div><div>4) There are cases where if A was discovered before B then B has an easy proof, whereas if B was discovered before A, then B has a hard proof. I'll give one example:</div><div><br/></div><div>Given HALT is undecidable, Godel's theorem is easy.</div><div><br/></div><div>Assume HALT is undecidable. </div><div><br/></div><div>Let STAT(e) be the statement M_e(0) does not  halt.</div><div><br/></div><div>There is some e such that M_e(0) does not halt  but ZFC cannot prove this.</div><div><br/></div><div>PROOF: Assume, By Way of Contradiction that for all e such that M_e(0) does not halt,</div><div>ZFC could prove this. Then HALT is DECIDABLE:</div><div><br/>Given e, run M_e(0) and at the same time enumerate all proofs in ZFC. It is guaranteed that</div><div>you will either find M_e(0) halts or a proof that M_e(0) does not halt. Hence you will,</div><div>in finite time, know if M_e(0) halts OR NOT.</div><div><br/></div><div>END OF PROOF</div><div><br/></div><div>Is the sequence of events where HALT is proven undecidable  before Godel's theorem plausible.</div><div>I  think so</div><div><br/></div><div>I INVITE my readers to give there own examples of when Math is not commutative- meaning that</div><div>the order of events matters.</div><div>
<br/></div></div>
    </content>
    <updated>2020-08-17T12:35:00Z</updated>
    <published>2020-08-17T12:35:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-08-27T12:43:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/122</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/122" rel="alternate" type="text/html"/>
    <title>TR20-122 |  Size Bounds on Low Depth Circuits for Promise Majority | 

	Joshua Cook</title>
    <summary>We give two results on the size of AC0 circuits computing promise majority. $\epsilon$-promise majority is majority promised that either at most an $\epsilon$ fraction of the input bits are 1, or at most $\epsilon$ are 0.

First, we show super quadratic lower bounds on both monotone and general depth 3 circuits for promise majority.

For any $\epsilon \in (0, 1/2)$, monotone depth 3 AC0 circuits for $\epsilon$-promise majority have size 
$\tilde{\Omega}\left(\epsilon^3 n^{2 + \frac{\ln(1 - \epsilon)}{\ln(\epsilon)}}\right)$
         
For any $\epsilon \in (0, 1/2)$, general depth 3 AC0 circuits for $\epsilon$-promise majority have size
$\tilde{\Omega}\left(\epsilon^3 n^{2 + \frac{\ln(1 - \epsilon^2)}{2\ln(\epsilon)}}\right)$

These are the first nontrivial size lower bounds on depth 3 promise majority circuits for $\epsilon &lt; 0.45$.
        
Second, we give both uniform and non-uniform sub-quadratic size constant depth circuits for promise majority.

For integer $k \geq 1$, constant $\epsilon \in (0, 1/2)$, there exists monotone non uniform AC0 circuits of depth $2 + 2 \cdot k$ computing $\epsilon$-promise majority with size
$\tilde{O}\left(n^{\frac{1}{1 - 2^{-k}}}\right)$

For integer $k \geq 1$, constant $\epsilon \in (0, 1/2)$, there exists monotone uniform AC0 circuit of depth $2 + 2 \cdot k$ computing $\epsilon$-promise majority with size
$n^{\frac{1}{1 - \left(\frac{2}{3}\right)^k} + o(1)}$

These circuits are based on incremental improvements to existing depth 3 circuits for promise majority given by Ajtai and Viola combined with a divide and conquer strategy.</summary>
    <updated>2020-08-16T14:37:01Z</updated>
    <published>2020-08-16T14:37:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-27T19:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/121</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/121" rel="alternate" type="text/html"/>
    <title>TR20-121 |  Fractional Pseudorandom Generators from the $k$th Fourier Level | 

	Eshan Chattopadhyay, 

	Jason Gaitonde, 

	Abhishek Shetty</title>
    <summary>In recent work by Chattopadhyay et al.[CHHL19,CHLT19], the authors exhibit a simple and flexible construction of pseudorandom generators for classes of Boolean functions that satisfy $L_1$ Fourier bounds. [CHHL19] show that if a class satisfies such tail bounds at all levels, this implies a PRG whose seed length depends on the quality of these bounds through their innovative random walk framework that composes together fractional PRGs that polarize quickly to the Boolean hypercube. On the other hand, [CHLT19] show that, by derandomizing the analysis of [RT19], just level-two Fourier bounds suffice to construct a pseudorandom generator using their framework; as this is a much weaker assumption on the class, [CHLT19] naturally obtain exponentially worse dependence on the error in the seed length compared to [CHHL19]. Moreover, this derandomization relies on simulating nearly independent Gaussians for the fractional pseudorandom generator, which necessitates the  polynomial dependence on $1/\epsilon$ in each fractional step.
    
    In this work, we attempt to bridge the gap between these two results. Namely, we partially answer an open question by [CHLT19] that nearly interpolates between them. In particular, we show that if one has bounds up to the level-$k$ $L_1$ Fourier mass of a closely related class of functions, where $k&gt;2$, one can obtain improved seed length, the degree to which is determined by how high $k$ can be taken. Our analysis shows that for error $\epsilon=1/\text{poly}(n)$, one needs control at just level $O(\log n)$ to recover the seed length of [CHHL19], without assumptions on the entire tail. We avoid this by providing a simple, alternate analysis of their fractional PRG that instead relies on Taylor's theorem and $p$-biased Fourier analysis to avoid assumptions on the weights of the higher-order terms. This further allows us to show that this framework can handle the class of low-degree polynomials over $\mathbb{F}_2$, with slightly worse dependence than the current state-of-the-art, which was not previously known. We hope that this alternate analysis will be fruitful in improving the understanding of this new and powerful framework.</summary>
    <updated>2020-08-16T14:35:43Z</updated>
    <published>2020-08-16T14:35:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-27T19:20:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/08/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/08/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Two sites on toroidal polyhedra: Bonnie Stewarts Hohlkörper and Alex Doskey’s virtual reality models of Stewart’s polyhedra (\(\mathbb{M}\)). Found while researching a new WP article on Stewart’s book Adventures Among the Toroids. The first link is in German but readable through Google translate and has lots of pretty pictures. The second needs VR software to be usable.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p>Two sites on toroidal polyhedra: <a href="https://www.spektrum.de/alias/raeumliche-geometrie/bonnie-stewarts-hohlkoerper/681891">Bonnie Stewarts Hohlkörper</a> and <a href="http://polyhedra.doskey.com/Stewart00.html">Alex Doskey’s virtual reality models of Stewart’s polyhedra</a> (<a href="https://mathstodon.xyz/@11011110/104618649607830730">\(\mathbb{M}\)</a>). Found while researching a new WP article on Stewart’s book <em><a href="https://en.wikipedia.org/wiki/Adventures_Among_the_Toroids">Adventures Among the Toroids</a></em>. The first link is in German but readable through Google translate and has lots of pretty pictures. The second needs VR software to be usable.</p>
  </li>
  <li>
    <p><a href="https://www.robertdickau.com/mapfolding.html">The map folding problem, illustrated by Robert Dickau</a> (<a href="https://mathstodon.xyz/@11011110/104630030819531499">\(\mathbb{M}\)</a>). See <a href="https://www.robertdickau.com/default.html#math">Dickau’s home page</a> for many more mathematical illustrations, mostly of combinatorial enumeration problems and fractals.</p>
  </li>
  <li>
    <p>For some reason I wanted the name of a surface of revolution of a circular arc less than \(\pi\) around its chord (<a href="https://mathstodon.xyz/@11011110/104635297508909080">\(\mathbb{M}\)</a>). <a href="https://en.wikipedia.org/wiki/Lemon_(geometry)">Wikipedia said “lemon”</a> but sourced to MathWorld so I thought maybe MathWorld had made it up. Not so. Better sources say the same. And the surface for the complementary arc is an “apple”. It looks like a North American football but <a href="http://modellsammlung.uni-goettingen.de/index.php?lang=en&amp;r=5&amp;sr=17&amp;m=182">a “football” is a different surface of revolution, of constant positive Gaussian curvature</a>.</p>
  </li>
  <li>
    <p><a href="https://link.springer.com/journal/454/64/2">Special issue of <em>Discrete &amp; Computational Geometry</em> in memory of Branko Grünbaum</a> (<a href="https://mathstodon.xyz/@11011110/104643955543481823">\(\mathbb{M}\)</a>). I think many of the research papers in it are interesting but I want to draw particular attention to <a href="https://link.springer.com/article/10.1007/s00454-020-00214-y">the preface by Gil Kalai, Bojan Mohar, and Isabella Novik</a>, which provides a nice brief survey both of Grünbaum’s many contributions to discrete geometry and of the lines of active research they have led to.</p>
  </li>
  <li>
    <p><a href="http://gallery.bridgesmathart.org/exhibitions/2020-Bridges-Conference">2020 Bridges Conference Mathematical Art Gallery</a> (<a href="https://mathstodon.xyz/@11011110/104646771923669610">\(\mathbb{M}\)</a>). Many are great but a couple of my favorites are <a href="http://gallery.bridgesmathart.org/exhibitions/2020-bridges-conference/conan-chadbourne">Conan Chadbourne’s grid partition enumeration</a> and <a href="http://gallery.bridgesmathart.org/exhibitions/2020-bridges-conference/mdlevin_publicmsncom">Martin Levin’s ten-tetrahedron tensegrity</a>. I didn’t participate but apparently the Bridges conference itself was held virtually a few days ago; see <a href="https://2020.bridgesmathart.org/">the conference site</a> for more including papers and videos.</p>
  </li>
  <li>
    <p><a href="https://felixboiii.github.io/paper-plotter/">Paper plotter</a> (<a href="https://mathstodon.xyz/@11011110/104655233453519187">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24091297">via</a>): tool to make 3d paper cut-and-assemble models of the graphs of bivariate functions.</p>
  </li>
  <li>
    <p>Kowhaiwhai (<a href="https://mathstodon.xyz/@11011110/104663925881930344">\(\mathbb{M}\)</a>).  are repeating decorative patterns used in New Zealand on Maori buildings. <a href="https://natlib.govt.nz/photos?text=kowhaiwhai&amp;commit=Search">The National Library of NZ has a number of good examples</a>, including the <a href="https://natlib.govt.nz/records/23146518">sketches of patterns by Tamati Ngakoho (top) and of a traditional Arawa pattern (bottom)</a> shown below. There’s also <a href="http://www.maori.org.nz/whakairo/default.php?pid=sp55&amp;parent=52">a brief guide to their interpretation online</a>. I can’t find much analysis of their structure, though, beyond pointing to frieze groups for their symmetries. The part that interests me more is their fractal-like swooping structure, reminiscent of (and in some cases directly modeled on) fern fronds.</p>
  </li>
</ul>

<p style="text-align: center;"><img alt="Godber, Albert Percy, 1875-1949. Godber, Albert Percy, 1876-1949. Drawings of Maori rafter patterns or kowhaiwhai. 16. 22W. MA22; 17. 21W. MA21; and, 18. 25W. MA25. Puhoro. [1939-1947]. Ref: E-302-q-1-016/018. Alexander Turnbull Library, Wellington, New Zealand. From https://natlib.govt.nz/records/23146518" src="https://11011110.github.io/blog/assets/2020/Kowhaiwhai.jpg"/></p>

<ul>
  <li>
    <p><a href="https://www.wired.com/story/why-wikipedia-decided-to-stop-calling-fox-a-reliable-source/">Why Wikipedia decided to stop calling Fox a reliable source</a> (<a href="https://mathstodon.xyz/@11011110/104666160845673755">\(\mathbb{M}\)</a>). Note however that Fox has not actually been deemed unreliable, in general. <a href="https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources/Noticeboard/Archive_303#RfC:_Fox_News">The discussion had a no-consensus close</a>.</p>
  </li>
  <li>
    <p><a href="https://sinews.siam.org/Details-Page/untangling-random-polygons-and-other-things">Untangling random polygons</a> (<a href="https://mathstodon.xyz/@11011110/104677553383067578">\(\mathbb{M}\)</a>): repeatedly rescaling midpoint polygons always leads to an ellipse.</p>
  </li>
  <li>
    <p><a href="https://www.atlasobscura.com/articles/kek-lapis-sarawak">The mesmerizing geometry of Malaysia’s most complex cakes:
Bold colors and designs set kek lapis Sarawak apart</a> (<a href="https://mathstodon.xyz/@11011110/104680812259935603">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24116775">via</a>). As seen on The Great British Bake Off. These cakes have many parallel layers in bright colors, cut and rearranged to form complex designs. Mostly they involve 45 and 90-degree angles but at least one of the examples uses hexagonal symmetry instead.</p>
  </li>
  <li>
    <p>My Google Scholar profile has mildly broken down (<a href="https://mathstodon.xyz/@11011110/104683176033821672">\(\mathbb{M}\)</a>). When I go there, it offers me two new profiles to link as my coauthors: Man-Kwun Chiu and Matí Korman. They are indeed coauthors, from my new CCCG papers. But when I click to accept them as listed coauthors, it tells me I have too many coauthors, refuses to add them, and returns to offering me new profiles to link. I can see no way out of this other than to not accept my coauthors, which would be wrong. Google, fix this limitation!</p>
  </li>
  <li>
    <p>A use for old CDs: <a href="https://momath.org/home/math-monday-those-circles-are-great/">cut them up and glue the pieces together to make visualizations of great circle arrangements on the sphere</a> (<a href="https://mathstodon.xyz/@11011110/104690896919723051">\(\mathbb{M}\)</a>). The mathematical question posed by this is: for which numbers of great circles is it possible to make an arrangement in which all the arcs between pairs of neighbors have equal lengths?</p>
  </li>
  <li>
    <p><a href="https://thonyc.wordpress.com/">The Renaissance Mathematicus</a> (<a href="https://mathstodon.xyz/@pkra/104694183591138626">\(\mathbb{M}\)</a>), an interesting blogger on the history of science. See also the <a href="https://thonyc.wordpress.com/2020/08/15/keep-the-renaissance-mathematicus-online/">crowdfunding drive to replace their old creaky iMac</a>, from which I found this.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-08-15T17:02:00Z</updated>
    <published>2020-08-15T17:02:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-08-23T07:53:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20069</id>
    <link href="https://gilkalai.wordpress.com/2020/08/14/to-cheer-you-up-in-difficult-times-9-alexey-pokrovskiy-proved-that-rotas-basis-conjecture-holds-asymptotically/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 9: Alexey Pokrovskiy proved that Rota’s Basis Conjecture holds asymptotically</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Pokrovskiy’s startling morning  rainbow Rota’s Basis Conjecture holds asymptotically, by Alexey Pokrovskiy Abstract: Rota’s Basis Conjecture is a well known problem from matroid theory, that states that for any collection of n bases in a rank n matroid, it is … <a href="https://gilkalai.wordpress.com/2020/08/14/to-cheer-you-up-in-difficult-times-9-alexey-pokrovskiy-proved-that-rotas-basis-conjecture-holds-asymptotically/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h2><a href="https://gilkalai.files.wordpress.com/2020/08/alexeypokrovskiy.jpg"><img alt="" class="alignnone size-medium wp-image-20075" height="300" src="https://gilkalai.files.wordpress.com/2020/08/alexeypokrovskiy.jpg?w=209&amp;h=300" width="209"/></a></h2>
<h2>Pokrovskiy’s startling morning  <strong><span style="color: #ff0000;">r</span><span style="color: #0000ff;">ai</span><span style="color: #ff6600;">n</span><span style="color: #ff9900;">b</span><span style="color: #ff00ff;">o</span><span style="color: #800080;">w</span></strong></h2>
<p><a href="https://arxiv.org/abs/2008.06045">Rota’s Basis Conjecture holds asymptotically</a>, by Alexey <span style="color: #000000;">Pokrovskiy</span></p>
<p><strong>Abstract:</strong> Rota’s Basis Conjecture is a well known problem from matroid theory, that states that for any collection of n bases in a rank n matroid, it is possible to decompose all the elements into n disjoint rainbow bases. Here an asymptotic version of this is proved. We show that it is possible to find <em>n − o(n)</em> disjoint rainbow independent sets of size <em>n − o(n)</em>.</p>
<p>A <strong><span style="color: #ff0000;">r</span><span style="color: #0000ff;">ai</span><span style="color: #ff6600;">n</span><span style="color: #ff9900;">b</span><span style="color: #ff00ff;">o</span><span style="color: #800080;">w </span></strong><span style="color: #800080;"><span style="color: #000000;">basis is a basis with one element from each collection.</span></span></p>
<p>(I thank Nati Linial for telling me about it.)</p>
<p>Another way to formulate Rota’s basis conjecture (for representable matroids) is that if <em>B</em><sub>1</sub>, <em>B</em><sub>2</sub>, …, <em>B<sub>n</sub></em> are <em>n</em> bases of an <em>n</em>-dimensional vector space <em>V</em> (not necessarily distinct or disjoint), then there exists an <em>n</em> × <em>n</em> grid of vectors (<em>v<sub>ij</sub></em>) such that</p>
<p>1. the <em>n</em> vectors in row <em>i</em> are the members of the <em>i</em>th basis <em>B<sub>i</sub></em> (in some order), and</p>
<p>2. in each column of the matrix, the <em>n</em> vectors in that column form a basis of <em>V</em>.</p>
<p>If all the bases are the standard basis then this reduces to the existence of <a href="https://en.wikipedia.org/wiki/Latin_square">Latin squares</a>.</p>
<p><strong>Unrelated trivia question:</strong>  AGC-GTC-TGC-GTC-TGC-GAC-GATC-? what comes next in the sequence?</p>
<p>We mentioned Rota’s basis conjecture in various earlier posts.  A classic paper on the subject is the <a href="https://gilkalai.files.wordpress.com/2017/02/huang-rota.pdf">1989 paper by Rosa Huang and Gian Carlo-Rota</a>. Three and a half years ago Timothy Chow lunched a polymath project (Polymath 12) to solve it. (Here is my<a href="https://gilkalai.wordpress.com/2017/02/26/timothy-chow-launched-polymath12-on-rota-basis-conjecture-and-other-news/"> post on the project with various variants of the conjecture</a>, the <a href="https://polymathprojects.org/2017/02/23/rotas-basis-conjecture-polymath-12/">first post on the polymath blog</a>, and the <a href="https://asone.ai/polymath/index.php?title=Rota%27s_conjecture">wiki</a>). See <a href="https://gilkalai.wordpress.com/2014/08/08/jim-geelen-bert-gerards-and-geo%ef%ac%80-whittle-solved-rotas-conjecture-on-matroids/">this post</a> for several famous conjectures by Rota, and this post about the related <a href="https://gilkalai.wordpress.com/2017/03/15/test-your-intuition-about-the-alon-tarsi-conjecture/">Alon-Tarsi conjecture</a>.</p></div>
    </content>
    <updated>2020-08-14T10:09:36Z</updated>
    <published>2020-08-14T10:09:36Z</published>
    <category term="Combinatorics"/>
    <category term="Updates"/>
    <category term="Alexey Pokrovskiy"/>
    <category term="Gian Carlo Rota"/>
    <category term="Rosa Huang"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-08-27T19:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17416</id>
    <link href="https://rjlipton.wordpress.com/2020/08/13/thanks-to-an-explainer/" rel="alternate" type="text/html"/>
    <title>Thanks to An Explainer</title>
    <summary>Conrad explains all Keith Conrad is a professor in the mathematics department at UCONN—the University of Connecticut. My dear wife Kathryn Farley and I are about to move to join him—not as faculty but as another resident of the “Constitution State.” Today we thank him for his work on explaining mathematics. Conrad is a prolific […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Conrad explains all</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/thanks-to-an-explainer/keithconrad/" rel="attachment wp-att-17410"><img alt="" class="aligncenter size-full wp-image-17410" src="https://rjlipton.files.wordpress.com/2020/08/keithconrad.jpg?w=600"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Keith Conrad is a professor in the mathematics department at UCONN—the University of Connecticut. My dear wife Kathryn Farley and I are about to move to join him—not as faculty but as another resident of the “Constitution State.”</p>
<p>
Today we thank him for his work on explaining mathematics.</p>
<p>
Conrad is a prolific writer of articles on mathematics. He makes hard concepts clear, he makes easy concepts interesting. He has a sense of humor; his <a href="https://kconrad.math.uconn.edu">website</a> is filled with fun of all kinds.</p>
<p>
He has interesting license plates, photos of streets that bear his first name, a Russian <a href="https://kconrad.math.uconn.edu/">update</a> to Tom Lehrer’s “Elements” song, and more links to others’ items. </p>
<p>
If you’d like a video example of fun see <a href="http://i.stack.imgur.com/d2OKd.gif">this</a> for a short video illusion. Too bad it is an illusion and it does not work in reality. As a chocolate lover I wish it worked—free chocolate forever. A similar <a href="https://en.wikipedia.org/wiki/Missing_square_puzzle">illusion</a> with a triangle was once featured by Martin Gardner, leading Ken as a teenager to make a cardboard cutout version overlaid on a map of the Bermuda Triangle as an “explanation” of disappearances there.</p>
<p>
</p><p/><h2> Articles </h2><p/>
<p/><p>
I have not yet met Conrad in person, but have enjoyed reading his articles on math of all kinds. He has a giant <img alt="{63 \times 4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B63+%5Ctimes+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{63 \times 4}"/> <a href="https://kconrad.math.uconn.edu/blurbs/">grid</a> of clickable titles, grouped by subject area. </p>
<p>
For an example, he has a <a href="https://kconrad.math.uconn.edu/blurbs/galoistheory/numbersoncircle.pdf">title</a> “Roots on a Circle.” The file name says “numbers on a circle” and the essay begins disarmingly enough with a picture of the 7th roots of unity. The next page shows a simple polynomial where most but not all roots lie on the circle:</p>
<p><a href="https://rjlipton.wordpress.com/thanks-to-an-explainer/lehmerspolynomialroots/" rel="attachment wp-att-17412"><img alt="" class="aligncenter size-medium wp-image-17412" height="161" src="https://rjlipton.files.wordpress.com/2020/08/lehmerspolynomialroots.jpg?w=300&amp;h=161" width="300"/></a></p>
<p>
This is enough to draw you in and stay attached as things become more complicated beginning on page 3. It helps that Conrad does not stint on algebraic details. This essay supplements a cautionary tale in mathematics: a <a href="https://mathoverflow.net/questions/15444/examples-of-eventual-counterexamples">link</a> to a MathOverflow list whose top item is that the factors of <img alt="{x^n - 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5En+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^n - 1}"/> over <img alt="{\mathbb{Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BQ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Q}}"/> have no coefficient of absolute value greater than <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> for <img alt="{n = 1,\dots,104}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+1%2C%5Cdots%2C104%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 1,\dots,104}"/>. Before you try to prove this by induction check out <img alt="{n = 105}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+105%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 105}"/>.</p>
<p>
</p><p/><h2> Finite Groups </h2><p/>
<p/><p>
One of my favorite articles is <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/order.pdf">titled</a>, “Orders Of Elements In A Group.” Conrad starts with the humble concept of the order of an element in a group. Then he builds up a theory that explains various properties of order. </p>
<p>
I especially like that he supplies examples to help you with your intuition. For me, and Ken, finite groups are just counter-intuitive. Groups have magical properties but my naive conjectures about them usually fail. Conrad’s article ends with a discussion of primality testing which is dear to us in complexity theory.</p>
<p>
</p><p/><h2> Finite Groups that Encode Information </h2><p/>
<p/><p>
I recently needed a finite group with a certain structure. In complexity theory we sometimes use matrices to encode information in a way that makes an algorithm more efficient. Algorithms like matrices since they can be stored and multiplied efficiently. As an example, suupose that we have two matrices <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  AB = -BA. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++AB+%3D+-BA.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  AB = -BA. "/></p>
<p>That is the matrices anti-commute. Then we can use such matrices to encode information about a string <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> of <img alt="{A,B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B}"/>‘s. Every string <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> can be written as: 	</p>
<p align="center"><img alt="\displaystyle  \pm A^{k}B^{l}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpm+A%5E%7Bk%7DB%5E%7Bl%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \pm A^{k}B^{l}. "/></p>
<p>The <img alt="{\pm}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pm}"/> encodes the number of inversions in the string <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>. That is the number of times <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is followed by an <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>: 	</p>
<p align="center"><img alt="\displaystyle  \dots B \dots A \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cdots+B+%5Cdots+A+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \dots B \dots A \dots "/></p>
<p>So <img alt="{ABBA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BABBA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ABBA}"/> has <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> inversions, and <img alt="{AAABA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAAABA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{AAABA}"/> has <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. Here are matrices that <a href="https://en.wikipedia.org/wiki/Generalized_Clifford_algebra">anti-commute</a>. </p>
<p/><p align="center"><img alt="\displaystyle  A = \begin{pmatrix} 0&amp;1\\ 1&amp;0 \end{pmatrix}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A+%3D+%5Cbegin%7Bpmatrix%7D+0%261%5C%5C+1%260+%5Cend%7Bpmatrix%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A = \begin{pmatrix} 0&amp;1\\ 1&amp;0 \end{pmatrix}"/></p>
<p/><p align="center"><img alt="\displaystyle  B = \begin{pmatrix} 1&amp;0\\ 0&amp;-1 \end{pmatrix}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++B+%3D+%5Cbegin%7Bpmatrix%7D+1%260%5C%5C+0%26-1+%5Cend%7Bpmatrix%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  B = \begin{pmatrix} 1&amp;0\\ 0&amp;-1 \end{pmatrix}"/></p>
<p>
We can do even better. There are matrices so that 	</p>
<p align="center"><img alt="\displaystyle  AB = \lambda BA, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++AB+%3D+%5Clambda+BA%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  AB = \lambda BA, "/></p>
<p>where <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> is a root of unity. They exist as the example below shows for fourth roots of unity. But finding such matrices was curiously hard, at least for me. Lots of web searching.</p>
<p/><p align="center"><img alt="\displaystyle  A = \begin{pmatrix} 0&amp;1&amp;0&amp;0\\ 0&amp;0&amp;1&amp;0\\ 0&amp;0&amp;0&amp;1\\ 1&amp;0&amp;0&amp;0 \end{pmatrix} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A+%3D+%5Cbegin%7Bpmatrix%7D+0%261%260%260%5C%5C+0%260%261%260%5C%5C+0%260%260%261%5C%5C+1%260%260%260+%5Cend%7Bpmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A = \begin{pmatrix} 0&amp;1&amp;0&amp;0\\ 0&amp;0&amp;1&amp;0\\ 0&amp;0&amp;0&amp;1\\ 1&amp;0&amp;0&amp;0 \end{pmatrix} "/></p>
<p/><p align="center"><img alt="\displaystyle  B = \begin{pmatrix} 1&amp;0&amp;0&amp;0\\ 0&amp;i&amp;0&amp;0\\ 0&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;0&amp;-i \end{pmatrix} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++B+%3D+%5Cbegin%7Bpmatrix%7D+1%260%260%260%5C%5C+0%26i%260%260%5C%5C+0%260%26-1%260%5C%5C+0%260%260%26-i+%5Cend%7Bpmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  B = \begin{pmatrix} 1&amp;0&amp;0&amp;0\\ 0&amp;i&amp;0&amp;0\\ 0&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;0&amp;-i \end{pmatrix} "/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Conrad’s articles are helpful. In a wide variety of topics he presents both theorems and history of math concepts. What I find most attractive is the examples and additional comments that pepper his writing. </p>
<p>
Check him out.</p></font></font></div>
    </content>
    <updated>2020-08-13T18:40:44Z</updated>
    <published>2020-08-13T18:40:44Z</published>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="articles"/>
    <category term="explain"/>
    <category term="finite groups"/>
    <category term="surprise"/>
    <category term="writing"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-08-27T19:20:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-5601291488610843158</id>
    <link href="http://processalgebra.blogspot.com/feeds/5601291488610843158/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=5601291488610843158" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/5601291488610843158" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/5601291488610843158" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/08/an-interview-with-jos-baeten-outgoing.html" rel="alternate" type="text/html"/>
    <title>An interview with Jos Baeten, outgoing director of the CWI</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>After nine years, <a href="https://www.cwi.nl/people/jos-baeten" target="_blank">Jos Baeten</a> will step down as general director of <a href="https://www.cwi.nl/" target="_blank">CWI</a> on 30 September 2020 and there will be a <a href="https://www.cwi.nl/events/2020/farewell-symposium-jos-baeten/symposium-retirement-jos-baeten" target="_blank">retirement symposium</a> in his honour on 1 October 2020. (Jos Baeten's successor will be <a href="https://www.tue.nl/en/research/researchers/ton-de-kok/" target="_blank">Tom de Kok</a>. You can read the CWI news item related to Tom de Kok's appointment <a href="https://www.cwi.nl/news/2020/ton-de-kok-appointed-new-director-of-cwi" target="_blank">here</a>. Tom de Kok, just like Jos Baeten before him, joins CWI from Eindhoven University of Technology.) </p><p>Jos Baeten has been one of the prime movers in the development of process algebra since 1987, and has been the driving force behind the CONCUR conference series and the CONCUR Basic Research Actions in the late 1980s and the early 1990s. Apart from being general director of CWI, he has served the TCS community in a variety of roles and organisations, and has helped to connect the work of the concurrency-theory community with that done in control and mechanical engineering. He has also supervised more than 30 PhD theses. </p><p>I asked Jos a few questions via email and I am happy to share his answers with the readers of this blog. Thanks to Jos for all the contributions he has given to the research community throughout his career, and for sharing his opinions and reminiscences with us! </p><p><b>The interview</b> </p><p><b>Luca:</b> Let's start from the beginning. If I remember correctly, your background was in model theory. Could you tell us what prompted you to move to doing research in computer science? </p><p><b>Jos:</b> I have a Master in logic and foundations of mathematics (with a minor in philosophy) from Utrecht University, with a Master’s thesis on lambda calculus, and a PhD in logic and foundations of mathematics from the University of Minnesota, with a PhD thesis on definability theory (a cross between recursion theory and set theory) entitled "<a href="https://link.springer.com/chapter/10.1007/BFb0099378" target="_blank">Filters and ultrafilters over definable subsets of admissible ordinals</a>". Returning from the US, I got a job teaching undergraduate maths at Delft University. But I wanted getting into research, and meeting <a href="https://staff.fnwi.uva.nl/j.a.bergstra/" target="_blank">Jan Bergstra</a> he said there were lots of opportunities in computer science research, not in maths. Writing a paper with him and <a href="https://www.cs.vu.nl/~jwk/" target="_blank">Jan Willem Klop</a> on term rewriting systems would qualify me, he said. So it happened, and I got a postdoc at CWI in the framework of the ESPRIT I project. I kept on doing maths, but it was called computer science. </p><p><b>Luca:</b> I have heard that you were recruited by Jan Bergstra and Jan Willem Klop to work within their group at the CWI, and that <a href="http://theory.stanford.edu/~rvg/" target="_blank">Rob van Glabbeek</a> joined the team soon after you. Could you tell us about the environment at the CWI at that time? It must have been a very exciting place to be with all the activity related to work on <a href="https://en.wikipedia.org/wiki/Algebra_of_communicating_processes#:~:text=The%20algebra%20of%20communicating%20processes,process%20algebras%20or%20process%20calculi." target="_blank">ACP</a> and related topics. </p><p><b>Jos:</b> It was a heady period. The group of Jan and Jan Willem was expanding rapidly on EU money, with me, Rob van Glabbeek and <a href="http://www.cs.ru.nl/~fvaan/" target="_blank">Frits Vaandrager.</a> I really liked ACP and doing universal algebra. </p><p><b>Luca:</b> What was your first paper in CS about? Which paper from your initial period at the CWI are you most proud of? </p><p><b>Jos:</b> My first paper was "<a href="https://www.researchgate.net/publication/225810594_Term_rewriting_systems_with_priorities" target="_blank">On term rewriting, Term rewriting systems with priorities</a>". I am most proud of my first ACP paper, "<a href="https://pure.tue.nl/ws/files/2137499/335911.pdf" target="_blank">Syntax and defining equations for an interrupt mechanism in process algebra</a>". This is about adding the priority operator. For axiomatisation, it needed an auxiliary operator that later turned out to be almost optimal. I came up with an axiomatisation, and Jan Bergstra forced me to prove it correct by doing all the critical pairs. I found a small error in pair 101 (of the 102 pairs), corrected it and proved the result correct. This all in a couple of weeks. </p><p><b>Luca:</b> Let's move on to the CONCUR project: How did it come about and what was its legacy? I recall that you came to visit <a href="https://www.scss.tcd.ie/Matthew.Hennessy/" target="_blank">Matthew Hennessy</a> at the University of Sussex when I was a PhD student there to discuss the CONCUR proposal. How did you become involved in leading such a large and high profile group of researchers so early in your career at the CWI? What was it like to lead the CONCUR project?  <br/></p><p><b>Jos:</b> This is again due to Jan Bergstra: he believes in delegating responsibilities quickly. When the new instrument of Basic Research Action came up in the EU, he wanted a BRA with <a href="https://en.wikipedia.org/wiki/Tony_Hoare" target="_blank">Hoare</a>, <a href="https://en.wikipedia.org/wiki/Robin_Milner" target="_blank">Milner </a>and Hennessy. I got the assignment and made a trip to the UK visiting all three, and got all of them on board. The idea of the project was unification, but that did not come about, everyone kept doing their own thing. We did, however, produce excellent papers, including the curious </p><p>J.C.M. Baeten, J.A. Bergstra, C.A.R. Hoare, R. Milner, J. Parrow &amp; R. de Simone, The variety of process algebra. Deliverable ESPRIT Basic Research Action 3006, CONCUR, University of Edinburgh 1991 </p><p><b>Luca:</b> How would you summarize the history of the conference CONCUR? Can it be split into different periods, and if so, what characterizes them? </p><p><b>Jos:</b> CONCUR 90, which was the first CONCUR conference, was organised by BRA CONCUR, but with speakers from all 5 BRA’s in the area of concurrency; in 1991 Milner did not want such a conference to occur in Edinburgh, so Jan Bergstra and I did it again in Amsterdam. Then, we involved <a href="https://www.cs.stonybrook.edu/people/faculty/ScottSmolka" target="_blank">Scott Smolka</a> for 1992 and after that, it was established, with a steering committee comprised of representatives from the 5 BRAs and Scott. As of 1993, it was firmly established, we found a niche and time period (end of August, beginning of September). All branches of concurrency were involved from the start, and over the years, it shows fashion clearly, for instance in certain years there was a lot of <a href="https://en.wikipedia.org/wiki/%CE%A0-calculus" target="_blank">pi-calculus</a>. </p><p><b>Luca:</b> In the 80s and the 90s there were three "schools" in process algebra,     the ACP, CCS, and CSP schools. With the benefit of hindsight, which were heir main contributions? To which extent have they converged? Is this classification still important for current research? </p><p><b>Jos:</b> I refer to my paper  <br/></p><p>J.C.M. Baeten, <a href="https://pure.tue.nl/ws/files/2154050/200402.pdf" target="_blank">A brief history of process algebra</a>. Theoretical Computer Science 335 (2/3), 2005, pp. 131-146.  <br/></p><p>I do a comparison in there. In my opinion, the book  <br/></p><p>J.C.M. Baeten, T. Basten and M.A. Reniers, <a href="https://www.cambridge.org/core/books/process-algebra-equational-theories-of-communicating-processes/0A091BDAA17DA4D3D30FCD6F52E0E6B8" target="_blank">Process Algebra: Equational Theories for Communicating Processes</a>. Cambridge Tract in Theoretical Computer Science 50, Cambridge University Press 2010  <br/></p><p>has the unification, presenting all three in the same framework. However, process algebra is out of fashion, so who cares? </p><p><b>Luca:</b>  When did you become interested in supervisory control? What motivated you to apply techniques from process algebra in that research field? Looking at the outcome of that work, both in theory and in practice, that was a very good move! Could you tell us about the position you had in Mechanical Engineering? How did it come about and what was it like to work in that department? </p><p><b>Jos:</b> More than 20 years in the department of computer science, having been dean twice, I got to know the university very well indeed. <a href="https://research.tue.nl/en/persons/je-koos-rooda" target="_blank">Koos Rooda</a>, professor of systems engineering at Mechanical Engineering, requested my assistance to get more software engineering into systems engineering, to replace his cooperation with Martin Rem, who became rector. In particular, he was interested in the combination of discrete event reasoning (i.e., process algebra) with the continuous mathematics of systems and control. Having done process algebra with timing and probabilities, I was willing to take up this challenge. Upon his retirement, with the vigorous backing of the dean of Mechanical Engineering at the time, I took over his position. At mid-life, I was ready for a change. I liked the hands-on attitude of mechanical engineering students, who are no good at theory or proofs, but are very good in using mathematics and trying out software tools. But then, I was asked to apply to the vacancy of director of CWI. This was the job I had always wanted. I had enjoyed being dean and doing research management, and CWI is a very prestigious institute with (at that time) very independent management. </p><p> </p><p><b>Luca:</b> Back at CWI: What skills do you think one needs to be the director of such a high-profile institute? What was your vision for the CWI and how much of it did you manage to achieve? What role do you think the CWI can/should have in the coming decade and beyond? </p><p><b>Jos:</b> The most important skill is hiring those early career scientists that grow out to become top scientists. Internally, you have to manage your key personnel, operating by consensus and compromise. Externally, you have to find your way in national science politics and always justify your existence. The growing tension between Dutch universities and Dutch extra-university research institutes has almost defeated me, but I managed to complete my term until retirement, and leave CWI in good shape, to my opinion. I managed to keep the excellent reputation of the institute, and also managed to bring in dynamics, rejuvenation and new subjects. What I did not achieve, is that CWI again becomes a publisher, and publishes diamond journals. </p><p><b>Luca:</b> You have played a role in organizations such as the EATCS, ERCIM and IFIP. Do you think that such societies still play a role today? How do think they should evolve to fulfill their mission? </p><p><b>Jos:</b> I do, but they have to keep evolving if they want to stay relevant. It is a pity that the EATCS could not take a role in the open access movement (you certainly made a very good effort!). I think ERCIM is doing ok, but not great. IFIP, I think, has become irrelevant. </p><p><b>Luca:</b> You have been a strong supporter of open access. What is your opinion of the state of play in open access and what do you think we could do to move forward? </p><p><b>Jos:</b> Progress is slow but exists. I am a strong advocate of Plan S, that has really set some things in motion. A key point is that it looks like we can achieve immediate green, and still allow all researchers to submit articles wherever they want. I do not like the way it is going with transformative agreements, you'd think that by now, publishers would be putting all their efforts to become sellers of data analytics services.</p><p><b>Acknowledgements:</b> I thank <a href="https://www7.in.tum.de/~esparza/" target="_blank">Javier Esparza</a>, the chair of the CONCUR Steering Committee, for contributing some the questions to Jos.  <br/></p></div>
    </content>
    <updated>2020-08-13T17:40:00Z</updated>
    <published>2020-08-13T17:40:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-08-20T14:04:45Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6597301440817356598</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6597301440817356598/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/08/simons-institute-gets-another-decade.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6597301440817356598" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6597301440817356598" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/08/simons-institute-gets-another-decade.html" rel="alternate" type="text/html"/>
    <title>Simons Institute Gets Another Decade</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> <a href="https://simons.berkeley.edu/news/simons-foundation-announces-new-355-million-grant-simons-institute-theory-computing-press">Great news</a> out of the Simons Institute.</p><blockquote><p>The Simons Foundation has ensured a second decade of research and innovation for the Simons Institute for the Theory of Computing, based at UC Berkeley, through a $35.5 million grant. The grant, which will begin in 2022, after the conclusion of the Simons Institute's first 10 years, will support the Simons Institute's mission and activities through June 2032.</p></blockquote><p>Congrats to Shafi Goldwasser and her team and of course a special thanks to Jim Simons and his foundation for their support for the theory community. </p><p>Time flies. I remember when I was on Team Chicago in the final rounds back in 2011. We lost but the theory community won as Berkeley did the institute well with amazing collaborative spaces, thanks mainly I hear from Alistair Sinclair, and the strong programs and workshops organized by the many volunteers across the theory community. </p><p>The institute started right when I started as a department chair so I never had the opportunity for the true Simons experience, joining for a semester-long program. When I did sneak away for a week at Simons I purposely avoided the workshops for Simons is at its best when strong researchers, connected by one of the programs, just talk, work and socialize together. I joined an amazing collection of complexity theorists to form a rather mediocre pub trivia team.</p><p>Even if you never make it there, the institute has a great <a href="https://www.youtube.com/user/SimonsInstitute">collection of videos</a> of its workshops, talks and celebrations. COVID-19 has driven Simons on-line but that just opens up their <a href="https://simons.berkeley.edu/workshops">workshops</a> and other events to a wider audience.</p><p>Congrats again to the Simons Institute for what it's given to the theory community and to its next dozen years with hopefully many more to follow!</p></div>
    </content>
    <updated>2020-08-13T14:10:00Z</updated>
    <published>2020-08-13T14:10:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-08-27T12:43:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/breeds/</id>
    <link href="https://gradientscience.org/breeds/" rel="alternate" type="text/html"/>
    <title>Benchmarks for Subpopulation Shift</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/2008.04859">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/BREEDS-Benchmarks">
<i class="fab fa-github"/>
   Code
</a>
<a class="bbutton" href="https://gradientscience.org/breeds_class_hierarchy">
<i class="fa fa-tree"/>
   Hierarchies
</a>
<br/></p>

<p><i>In our <a href="https://arxiv.org/abs/2008.04859">new paper</a>, we develop a
framework for simulating realistic subpopulation shifts between training
and deployment conditions for machine learning models. Evaluating
standard models on the resulting benchmarks reveals that these models
are highly sensitive to such shifts. Moreover, training models to be
invariant to existing families of synthetic data perturbations falls
short of remedying this issue.   </i></p>

<p>Consider what is probably the most prototypical classification task:
distinguishing between pictures of dogs and cats. By now, we have a fairly
established approach to tackling this task. It involves sourcing a set of
(labeled) cat and dog pictures and then training our model on that set.
Intuitively, we understand that for this approach to succeed our training set
has to be sufficiently large and diverse, capturing a variety of real-world
conditions. In particular, one might want to make sure that it contains
representatives of many of the possible breeds of dogs and cats. Is it critical
though that <i>all</i> breeds are represented? That is, if our classifier truly
learns how to distinguish dogs from cats, wouldn’t we expect it to generalize to
unseen breeds as well?</p>

<p>This is just an illustration of a broader challenge: building ML models that are
robust to <i>subpopulation shift</i>, i.e., they are able to generalize to data
subpopulations that were not encountered during training. This notion of
robustness is essential for models to perform reliably in the real world. After
all, we cannot expect our training set to capture all possible subpopulation
variations it can encounter during deployment. (Think of, e.g., differences in
weather/road conditions in the context of self-driving cars, or variability in
diagnostic equipment and patients’s exact characteristics in the
context of medical applications.)</p>

<h2 id="tackling-subpopulation-shifts">Tackling subpopulation shifts</h2>

<p>To make progress on this challenge, we need to first develop a principled way to
measure such robustness (or lack thereof). So, how can we measure the extent to
which our models are robust to such train-test subpopulation shifts?</p>

<p>A natural approach would be to first partition the subpopulations present in data for each class (e.g., dog or cat breeds) into two
disjoint sets: one set that is used for training the model, and one set that is held-out for
evaluation. Then, one can just examine the performance of the model on the held-out data.</p>

<p>This setup ensures that the actual task remains unchanged between
training and testing—i.e., classifying inputs into the original
classes (“dog” vs. “cat”). Therefore, the observed performance can give
us a sense how well that model will perform when exposed to novel data
subpopulations encountered during deployment.</p>

<p>The chief difficulty in implementing this strategy, however, is that
standard datasets do not contain annotations that are fine-grained
enough to identify the subpopulations of interest. Also, rectifying this
issue by collecting additional human annotations would be complicated
(not every class lends itself to identifying a natural and explicit
subpopulation structure) and costly (due to the scale of
state-of-the-art classification datasets).</p>

<p>In this post, we will outline an approach that circumvents some of the
aforementioned issues, allowing us to automatically construct a suite of
subpopulation shift benchmarks of varying difficulty, with minimal effort.</p>

<h2 id="the-breeds-methodology">The <span class="sc">Breeds</span> methodology</h2>

<p>Our approach is to <i>simulate</i> class subpopulations by grouping semantically
similar classes into <i>super</i>classes (e.g., aggregate the 100+ dog breed classes
in ImageNet into a “dog superclass”). Then, considering the classification task
over such superclasses enables us to repurpose the original dataset annotations
as explicit indicators of the subpopulation structure. Note that this new task
is entirely based on <em>existing</em> datasets, so we do not need to deal with,
gathering new data points or annotations, hence avoiding the additional biases
these might introduce (see our previous posts on
<a href="https://gradientscience.org/data_rep_bias">dataset replication</a> and
<a href="https://gradientscience.org/benchmarks">annotation pipelines</a>).</p>

<p>After this task restructuring, we can train our model on the resulting
superclass classification task while having full control over  which
subpopulations are included in training and which subpopulations are encountered
during testing. For example, if our original dataset contains classes: Labrador,
Husky, Tabby cat, and Persian cat, which we group into “dog” and “cat”
superclasses as [Labrador, Husky] and [Tabby cat, Persian cat], we can train a
model on the Labrador vs. Tabby cat task and test how well it can distinguish
Huskies from Persian cats. (Note that this framework can also naturally
incorporate milder subpopulation shifts where test subpopulations are
underrepresented, but not entirely missing, in the training set—see
<a href="https://arxiv.org/abs/1909.02060">here</a>.) Pictorially:</p>

<p><img class="bigimg" id="pipeline" src="https://gradientscience.org/assets/breeds/figures/pipeline.png"/></p>
<div class="footnote"> <strong>Overview of the <span class="sc">Breeds</span>
methodology.</strong> We group classes into superclasses and derive a
classification task over superclasses. Then we randomly assign each of the
original classes to be used for training (source) or evaluation (target) of the
model, thus inducing a subpopulation shift between training and testing
conditions. By controlling the granularity of the superclass grouping we can
construct shifts of varying severity.</div>

<p>The dog vs. cat breeds example served as the motivation for our
framework (as well as its name), but the underlying principle is quite
general. In fact, we can (and do) apply this methodology to group
together a wide variety of classes that share similar characteristics.</p>

<h3 id="obtaining-meaningful-superclasses">Obtaining meaningful superclasses</h3>

<p>In order for such a “breeds” benchmark be tractable from an image classification
viewpoint, we need to ensure that the classes we group together into
superclasses actually share visual characteristics. (After all, we cannot expect
a model to generalize across an arbitrary train-test partition of classes).
Ideally, we would group dataset classes based on a hierarchy that captures their
<i>visual</i> similarity—thus, allowing us to construct superclasses of varying
granularity (which would lead to benchmarks of varying difficulty).</p>

<div class="footnote">     
  <strong>Note:</strong> Having access to a class
hierarchy is a relatively mild requirement. Most of the popular vision datasets
are already equipped with such a hierarchy (e.g., CIFAR-100, ImageNet, PASCAL-
VOC and OpenImages). Even in cases where an explicit class hierarchy is missing,
it is more likely than not that the dataset classes obey some natural groupings
which could be recovered either manually, or with the assistance of proxies such
as word embeddings. 
</div>

<p>From this point of view, the ImageNet dataset is a natural candidate for
building our benchmarks—aside from its breadth and scale, it also comes
equipped with a class hierarchy, i.e., the <a href="https://wordnet.princeton.edu/">WordNet hierarchy</a>. However,
taking a closer look at that hierarchy reveals a few important shortcomings: (a)
classes are grouped together based on abstract (as opposed to visual)
characteristics (e.g., “umbrella” and “roof” are both “coverings); (b) nodes at
the same level of the hierarchy can have vastly different granularity (e.g.,
“street sign” and “living thing”), not necessarily mirroring the specificity of
the class itself; and (c) the hierarchy is not a tree (e.g., “bread” is in both
“starches” and “baked goods”). Grouping ImageNet classes based on this hierarchy
would thus result in poorly calibrated tasks.</p>

<p>All these shortcomings stem from the fact that WordNet is a <em>semantic</em> rather
than <em>visual</em> hierarchy—words are grouped together based on their meaning
rather than the visual appearance of the objects they correspond to. To remedy
this, we performed extensive edits to the existing ImageNet class hierarchy to
better capture visual object similarity. Specifically, we removed all abstract
nodes (e.g., “covering”) and calibrated the depth to ensure that nodes at the
same level have comparable granularity (by adding/removing redundant
nodes)—the modified hierarchy can be explored 
<a href="https://gradientscience.org/breeds_class_hierarchy">here</a> or through the
notebook we provide in the <a href="https://github.com/MadryLab/BREEDS-Benchmarks">code repo</a>.</p>

<h3 id="constructing-breeds-benchmarks">Constructing <span class="sc">Breeds</span> benchmarks</h3>

<p>Equipped with such a modified class hierarchy, we can construct subpopulation
shift benchmarks of varying difficulty in an essentially automated manner: we
simply choose a level of the hierarchy and treat each node at that level as a
superclass. The original dataset classes that are descendants of this node then
become the subclasses or “breeds”. These subclasses are then split
<em>randomly</em> into two groups—one of these is used to sample
training data points (<em>source domain</em>), and the other to sample test data points
(<em>target domain</em>).</p>

<div class="footnote">     
  <strong>Note:</strong> Using our methodology, one can also precisely
  control the extent to which the source and target domain subpopulations 
  differ—i.e., instead of random splits one 
  could make the splits more/less 
  adversarial)—see our paper for details.
</div>
<p>By varying the level in the hierarchy of the superclasses we
select, we can create an entire suite of benchmarks. For our analysis below, we
will use the following benchmarks.
(You can find more details in <a href="https://arxiv.org/abs/2008.04859">our paper</a> and
interactively browse the hierarchy
<a href="https://gradientscience.org/breeds_class_hierarchy">here</a>).</p>

<div>
  <div class="stages block">
      <div class="stage rbutton block clicked sc" id="entity13">Entity-13</div>
      <div class="stage rbutton block sc" id="entity30">Entity-30</div>
      <div class="stage rbutton block sc" id="living17">Living-17</div>
      <div class="stage rbutton block sc" id="nonliving26">NonLiving-26</div>
  </div>
  <div class="row" id="all_classes">
  </div>
  <div class="row">
    <div class="custom_column">
        <img class="bigimg" id="source_img" src="https://gradientscience.org/feed.xml"/>
    </div>
    <div class="custom_column">
        <img class="bigimg" id="target_img" src="https://gradientscience.org/feed.xml"/>
    </div>
  </div>
</div>

<div class="footnote"> <strong>Explore the <span class="sc">Breeds</span>
benchmarks.</strong> Select a dataset to see the set of superclasses it
contains, i.e., the classes with respect to which the classification task is
defined. Then, select any of these classes to see the subpopulations assigned
to the source and target domain, along with random images from each domain (you
need to scroll to the right to see all the classes).
</div>

<h3 id="validating-the-benchmarks-using-human-annotation">Validating the benchmarks using human annotation</h3>

<p>As already discussed, to ensure that the benchmarks we create are actually
meaningful, we need to validate that the superclasses therein contain visually
coherent subpopulations, so that good cross-subpopulation generalization is
possible. To this end, we leverage human annotators to measure the robustness of
humans to the subpopulation shifts encapsulated by (simplified versions of)
<span class="sc">Breeds</span> tasks. Specifically, we randomly pick pairs of superclasses and then show
annotators two groups of images corresponding to samples from the source domain
of the respective superclasses. Then, we present them with random images from
the target domain of <em>both</em> superclasses (mixed together) and ask them to assign
these images to one of the two groups (superclasses). Crucially, we do <em>not</em>
reveal to the annotators the name of either of the superclasses. Here is what
the task looks like:</p>

<div>
  <img class="bigimg" id="pipeline" src="https://gradientscience.org/assets/breeds/figures/task.png" width="80%"/>
</div>
<div class="footnote"> <strong>Illustration of subpopulation shift task presented 
to annotators.</strong> Annotators are shown random images from the source
domains of two random superclasses. Then, they are presented with images from
the target domains of these superclasses and asked to assign them into the
correct superclass.</div>

<p>If our superclasses are indeed well-calibrated for the task, annotators should
be able to associate new images with the correct group. That is, images from the
target domain of a superclass should be more similar to the source domain of the
<em>same</em> superclass, rather than the other one. Also, to establish a baseline, we
repeat the same experiment while asking the annotators to classify unseen
samples from the source domain (i.e., when there is no subpopulation shift).</p>

<p>So how well do our annotators perform?</p>

<div id="anno"> 
<canvas height="35%" id="bar" width="100%"/>
</div>

<div id="anno_model"> 
<canvas height="35%" id="bar_models" width="100%"/>
</div>

<div class="footnote"> <strong>Annotator performance on <span class="sc">Breeds</span> tasks.</strong> Annotators are quite robust to subpopulation shift.
Their ability to identify the correct superclass does not change significantly
between the source and the target domain, especially for the
<span class="sc">Living-17</span> and <span class="sc">Entity-30</span>
benchmarks. </div>

<p>As we can see, the difference in average annotator accuracy (in terms of
classifying unseen inputs into the correct group/superclass) between the source
and target domain is quite small. This suggests that the superclasses do indeed
correspond to visually meaningful object groupings, and, moreover, that humans
are quite robust to the distribution shifts captured within the <span class="sc">Breeds</span> tasks.
Let us now compare this drop to the one that our models incur.</p>

<h3 id="subpopulation-robustness-of-standard-models">Subpopulation robustness of standard models</h3>

<p>To assess whether standard models are sensitive to subpopulation shifts, we
will, for each <span class="sc">Breeds</span> task, train a number of standard architectures to
distinguish between the corresponding superclasses, using data from the source
domain and then measure model performance on data from the target domain.
Specifically, we will plot the <em>target accuracy</em> of each model as a function of
its <em>source accuracy</em>.</p>

<div id="std_full">
    <div class="rates block">
      <div class="std_stage rbutton block clicked sc" id="e13">Entity-13</div>
      <div class="std_stage rbutton block sc" id="e30">Entity-30</div>
      <div class="std_stage rbutton block sc" id="e17">Living-17</div>
      <div class="std_stage rbutton block sc" id="e26">NonLiving-26</div>
    </div>
  <div id="std_e13"> 
    <canvas height="40%" id="std_acc_e13" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">Entity-13</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    </div>
  </div>
  <div id="std_e30"> 
    <canvas height="40%" id="std_acc_e30" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">Entity-30</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    </div>
  </div>
  <div id="std_e17"> 
    <canvas height="40%" id="std_acc_l17" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">LIVING-17</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    </div>
  </div>
  <div id="std_e26"> 
  <canvas height="40%" id="std_acc_nl26" width="90%"/>
  <div class="footnote">
  <strong>Performance of standard models on the <span class="sc">NONLIVING-26</span> benchmark.
  </strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
  </div>
</div>
</div>

<div id="std_full_ft">
    <div class="rates_ft block">
      <div class="std_stage rbutton block clicked sc" id="e13f">Entity-13</div>
      <div class="std_stage rbutton block sc" id="e30f">Entity-30</div>
      <div class="std_stage rbutton block sc" id="e17f">Living-17</div>
      <div class="std_stage rbutton block sc" id="e26f">NonLiving-26</div>
    </div>
  <div id="ft_e13f"> 
    <canvas height="40%" id="ft_acc_e13" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">Entity-13</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    Re-training the last layer of the model on the target distribution
    significantly improves performance but does not recover the original model
    accuracy.
    </div>
  </div>
  <div id="ft_e30f"> 
    <canvas height="40%" id="ft_acc_e30" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">Entity-30</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    Re-training the last layer of the model on the target distribution
    significantly improves performance but does not recover the original model
    accuracy.
    </div>
  </div>
  <div id="ft_e17f"> 
    <canvas height="40%" id="ft_acc_l17" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">Living-17</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    Re-training the last layer of the model on the target distribution
    significantly improves performance but does not recover the original model
    accuracy.
    </div>
  </div>
  <div id="ft_e26f"> 
    <canvas height="40%" id="ft_acc_nl26" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">NonLiving-26</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    Re-training the last layer of the model on the target distribution
    significantly improves performance but does not recover the original model
    accuracy.
    </div>
  </div>
</div>

<p>Across the board, all models suffer a clear drop in performance under
subpopulation shifts—-accuracy drops by more than 30 percentage points between
the source and target domains. This indicates that the features models rely on
to perform well are somewhat specific to the subpopulations they encounter
during training, and thus tend to lose their predictive power even under
seemingly mild shifts in test-time subpopulations. 
<a href="https://gradientscience.org/feed.xml#anno_model" id="reveal_model">[CLICK THIS to compare model
performance to that of human annotators.]</a></p>

<p>A natural question to ask then is: can these models be adapted to the target
domain by simply re-training their last layer with data from this domain? 
<a href="https://gradientscience.org/feed.xml#std_ft" id="reveal_retrain">[CLICK THIS to add the re-training 
line.]</a> The answer seems to be nuanced. On one hand,
re-training significantly increases model performance, indicating that the
representations learned in the earlier layers of these (source-domain trained)
models are useful also for the subpopulations in the target domain. On the other
hand, these representations still fall short of what one would hope for—the
target accuracy post retraining remains much lower than that of a model trained
directly on the target domain.</p>

<h3 id="robustness-interventions">Robustness interventions</h3>

<p>Given that standard models turned out to be very sensitive to subpopulation
shifts, one might hope that applying a number of existing robustness
interventions—designed to increase model robustness to specific synthetic
perturbations—might change this state of affairs. To assess this, we
evaluate the subpopulation shift robustness of classifiers that have been
trained: (a) via <a href="https://gradientscience.org/robust_opt_pt1">robust optimization</a> against L2 adversaries of
different epsilon; (b) on a <a href="https://arxiv.org/abs/1811.12231">stylized version</a> of ImageNet (relying less on
texture and thus more on shape); and (c) with random noise (Gaussian or
<a href="https://arxiv.org/abs/1708.04896">Erase</a>).</p>

<p>To control for the significant impact on the standard (source) accuracy that
these methods can have, we focus on  the <em>relative target accuracy</em> of the
resulting models—that is the fraction of accuracy that is preserved in the
target domain (target accuracy over source accuracy).</p>

<div>
  <img class="bigimg" id="intervene" src="https://gradientscience.org/assets/breeds/figures/intervene.png"/>
</div>
<div class="footnote">
<strong>Effect of train-time interventions on subpopulation shift robustness.</strong>
We plot the ratio of the target accuracy to the source accuracy as a function of
the source accuracy. Train-time interventions have small, yet non-trivial,
impact on model robustness.
</div>

<p>We observe that these robustness interventions do have an impact on the target
accuracy of the model. Moreover, even when the resulting models have comparable
source accuracies, their target accuracies can be quite different. For instance,
erase noise improves robustness without particularly impacting source accuracy
(relative to standard models), while adversarially trained models improve
robustness at the expense of source accuracy. Overall, these results indicate
that while existing interventions make models slightly less sensitive to
subpopulation shifts, there is still significant room for improvement.</p>

<p>To obtain a more complete picture, we also measure the performance of the above
models after retraining the final layer with data from the target domain. Again,
our goal is to understand whether the employed training methods lead to
representations that are more general, and can be directly repurposed for the
target domain.</p>

<div>
  <img class="bigimg" id="intervene_ft" src="https://gradientscience.org/assets/breeds/figures/intervene_ft.png"/>
</div>
<div class="footnote">
<strong>Adapting model trained with robustness interventions for subpopulation shift.</strong> We plot model accuracy on the target domain after re-training the last layer on that domain. We can see that adversarially-trained model have significantly better accuracy, in certain cases almost recovering the original source accuracy.
</div>

<p>Interestingly, we see that models trained with erase noise are not better than
standard models after we allow for such fine-tuning. This indicates that the
gain in robustness it provides may not stem from learning better
representations. In contrast, adversarially trained models are significantly
better than other models (including standard ones) after fine-tuning. This is in
line with recent results showing that adversarially trained models are more
suited for transfer learning (<a href="https://arxiv.org/abs/2007.05869">here</a> and
<a href="https://arxiv.org/abs/2007.08489">here</a>). At
the same time, models trained on the stylized version of ImageNet are
consistently worse than other models, even relative to models with similar
source accuracy. This could indicate that texture is a particularly useful
feature for this classification task, especially in the presence of
subpopulation shift.</p>

<h3 id="conclusions">Conclusions</h3>
<p>In this post, we demonstrate how one can utilize a class
hierarchy to simulate a range of subpopulation shifts within existing
classification datasets. These shifts turn out to pose a challenge for standard
models (seemingly more so than for humans) and this challenge cannot really be
overcome using existing robustness interventions or re-training the last
layer. Thus, there is a need for designing new methods to address this core
robustness requirement. Overall, we believe that subpopulation shifts are an
important piece of the robustness puzzle and we hope that the benchmarks we
develop will serve as a guide for future progress on this front.</p></div>
    </summary>
    <updated>2020-08-12T00:00:00Z</updated>
    <published>2020-08-12T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2020-08-26T23:28:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/120</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/120" rel="alternate" type="text/html"/>
    <title>TR20-120 |  A Parallel Repetition Theorem for the GHZ Game | 

	Justin Holmgren, 

	Ran Raz</title>
    <summary>We prove that parallel repetition of the (3-player) GHZ game reduces the value of the game polynomially fast to 0. That is, the value of the GHZ game repeated in parallel $t$ times is at most $t^{-\Omega(1)}$. Previously, only a bound of $\approx \frac{1}{\alpha(t)}$, where $\alpha$ is the inverse Ackermann function, was known.

The GHZ game was recently identified by Dinur, Harsha, Venkat and Yuen as a multi-player game where all existing techniques for proving strong bounds on the value of the parallel repetition of the game fail. Indeed, to prove our result we use a completely new proof technique. Dinur, Harsha, Venkat and Yuen speculated that progress on bounding the value of the parallel repetition of the GHZ game may lead to further progress on the general question of parallel repetition of multi-player games. They suggested that the strong correlations present in the GHZ question distribution represent the ``hardest instance'' of the multi-player parallel repetition problem.

Another motivation for studying the parallel repetition of the GHZ game comes from the field of quantum information. The GHZ game, first introduced by Greenberger, Horne and Zeilinger, is a central game in the study of quantum entanglement and has been studied in numerous works. For example, it is used for testing quantum entanglement and for device-independent quantum cryptography. In such applications a game is typically repeated to reduce the probability of error, and hence bounds on the value of the parallel repetition of the game may be useful.</summary>
    <updated>2020-08-11T23:02:11Z</updated>
    <published>2020-08-11T23:02:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-27T19:20:26Z</updated>
    </source>
  </entry>
</feed>
