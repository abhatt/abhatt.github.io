<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-08-16T16:21:58Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/122</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/122" rel="alternate" type="text/html"/>
    <title>TR20-122 |  Size Bounds on Low Depth Circuits for Promise Majority | 

	Joshua Cook</title>
    <summary>We give two results on the size of AC0 circuits computing promise majority. $\epsilon$-promise majority is majority promised that either at most an $\epsilon$ fraction of the input bits are 1, or at most $\epsilon$ are 0.

First, we show super quadratic lower bounds on both monotone and general depth 3 circuits for promise majority.

For any $\epsilon \in (0, 1/2)$, monotone depth 3 AC0 circuits for $\epsilon$-promise majority have size 
$\tilde{\Omega}\left(\epsilon^3 n^{2 + \frac{\ln(1 - \epsilon)}{\ln(\epsilon)}}\right)$
         
For any $\epsilon \in (0, 1/2)$, general depth 3 AC0 circuits for $\epsilon$-promise majority have size
$\tilde{\Omega}\left(\epsilon^3 n^{2 + \frac{\ln(1 - \epsilon^2)}{2\ln(\epsilon)}}\right)$

These are the first nontrivial size lower bounds on depth 3 promise majority circuits for $\epsilon &lt; 0.45$.
        
Second, we give both uniform and non-uniform sub-quadratic size constant depth circuits for promise majority.

For integer $k \geq 1$, constant $\epsilon \in (0, 1/2)$, there exists monotone non uniform AC0 circuits of depth $2 + 2 \cdot k$ computing $\epsilon$-promise majority with size
$\tilde{O}\left(n^{\frac{1}{1 - 2^{-k}}}\right)$

For integer $k \geq 1$, constant $\epsilon \in (0, 1/2)$, there exists monotone uniform AC0 circuit of depth $2 + 2 \cdot k$ computing $\epsilon$-promise majority with size
$n^{\frac{1}{1 - \left(\frac{2}{3}\right)^k} + o(1)}$

These circuits are based on incremental improvements to existing depth 3 circuits for promise majority given by Ajtai and Viola combined with a divide and conquer strategy.</summary>
    <updated>2020-08-16T14:37:01Z</updated>
    <published>2020-08-16T14:37:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-16T16:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/121</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/121" rel="alternate" type="text/html"/>
    <title>TR20-121 |  Fractional Pseudorandom Generators from the $k$th Fourier Level | 

	Eshan Chattopadhyay, 

	Jason Gaitonde, 

	Abhishek Shetty</title>
    <summary>In recent work by Chattopadhyay et al.[CHHL19,CHLT19], the authors exhibit a simple and flexible construction of pseudorandom generators for classes of Boolean functions that satisfy $L_1$ Fourier bounds. [CHHL19] show that if a class satisfies such tail bounds at all levels, this implies a PRG whose seed length depends on the quality of these bounds through their innovative random walk framework that composes together fractional PRGs that polarize quickly to the Boolean hypercube. On the other hand, [CHLT19] show that, by derandomizing the analysis of [RT19], just level-two Fourier bounds suffice to construct a pseudorandom generator using their framework; as this is a much weaker assumption on the class, [CHLT19] naturally obtain exponentially worse dependence on the error in the seed length compared to [CHHL19]. Moreover, this derandomization relies on simulating nearly independent Gaussians for the fractional pseudorandom generator, which necessitates the  polynomial dependence on $1/\epsilon$ in each fractional step.
    
    In this work, we attempt to bridge the gap between these two results. Namely, we partially answer an open question by [CHLT19] that nearly interpolates between them. In particular, we show that if one has bounds up to the level-$k$ $L_1$ Fourier mass of a closely related class of functions, where $k&gt;2$, one can obtain improved seed length, the degree to which is determined by how high $k$ can be taken. Our analysis shows that for error $\epsilon=1/\text{poly}(n)$, one needs control at just level $O(\log n)$ to recover the seed length of [CHHL19], without assumptions on the entire tail. We avoid this by providing a simple, alternate analysis of their fractional PRG that instead relies on Taylor's theorem and $p$-biased Fourier analysis to avoid assumptions on the weights of the higher-order terms. This further allows us to show that this framework can handle the class of low-degree polynomials over $\mathbb{F}_2$, with slightly worse dependence than the current state-of-the-art, which was not previously known. We hope that this alternate analysis will be fruitful in improving the understanding of this new and powerful framework.</summary>
    <updated>2020-08-16T14:35:43Z</updated>
    <published>2020-08-16T14:35:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-16T16:20:29Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/privacy-composition/</id>
    <link href="https://differentialprivacy.org/privacy-composition/" rel="alternate" type="text/html"/>
    <title>Why Privacy Needs Composition</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We’re back!  In our last <a href="https://differentialprivacy.org/\average-case-dp">post</a> we discussed some of the subtle pitfalls of formulating the assumptions underlying average-case relaxations of differential privacy.  This time we’re going to look at the composition property of differential privacy—that is, the fact that running two independent differentially private algorithms on your data and combining their outputs is still differentially private. This is a key property of differential privacy and is actually closely related to the worst-case nature of differential privacy.</p>

<p>Composition is really the crucial property that has made differential privacy successful. Data analysis doesn’t happen in a vacuum, and the greatest threat to privacy comes from combining multiple pieces of information. These pieces of information can come from a single source that releases detailed statistics, or they could come from separate sources. So it’s critical to understand how the composition of multiple pieces of information can affect privacy.</p>

<p>In this post we’ll give some examples to illustrate why we need composition, and why composition is challenging for average-case relaxations of differential privacy.  Composition is what allows you to design sophisticated differentially private algorithms out of simple building blocks, and it’s what allows one organization to release differentially private statistics without having to understand the entire ecosystem of related information that has been or will be released.  As we’ll see, the challenges of composing average-case privacy guarantees are also very closely related to the subtleties that arise in thinking about the adversary’s beliefs.</p>

<h3 id="differencing-attacks">Differencing Attacks</h3>

<p>Let’s start with a simple example of composition that was alluded to in our last post.</p>

<p>You’ve just started a new job and signed up for the health insurance provided by your employer. Thus, your employer is able to obtain aggregated data from the insurance provider. In particular, your employer can ask “How many of our employees have submitted claims for condition X?”  However, your employer should not be able to find out whether or not <em>you</em> have condition X. 
For concreteness, condition X could be a mental health condition, drug addiction, being pregnant, terminal cancer, or an expensive chronic illness. Each of these could result in some kind of employment discrimination.</p>

<p>The employer may find out that 417 employees have condition X.  That’s OK; on its own, this number reveals very little about whether or not <em>you</em> have condition X, as long as your employer is uncertain about how many employees <em>other than you</em> have condition X.  We can formalize this as some kind of average-case or Bayesian privacy guarantee. Thus the health-insurance company is comfortable releasing this number exactly.  But, yesterday, before you started your job, it also seemed reasonable to allow your employer to ask the exact same question, and yesterday the answer was 416. Thus your employer concludes that you have condition X.</p>

<p>In this example, we see how two pieces of information—the count before you started and the count after you started—each of which seems innocuous on its own can be combined to reveal private information. This is a simple example of a <em>differencing attack</em> and composition is important in part because it prevents these attacks.</p>

<p>This example involves only two pieces of information. However, an attack could combine many pieces of information. For example, the counts could be broken down by sex, race/ethnicity, age, location, and tobacco use.<sup id="fnref:1"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:1">1</a></sup> Additional data may also be obtained from other sources, such as public records, social media, voluntary disclosures, healthcare providers, financial records, employment records, or even illicit sources. The possibilities for attacks grow rapidly as more information is made available. And an employer is only one example of a potential privacy adversary.</p>

<p>The point of this example is that it’s easy to argue that one piece of information is harmless to privacy by making plausible-looking assumptions about the adversary. But this intuition rapidly breaks down once you consider the bigger picture where there are many pieces of information that can complete the puzzle. That’s why we need rigorous methods for understanding privacy and its composition.</p>

<h3 id="quantifying-composition">Quantifying Composition</h3>

<p>How does differential privacy prevent a differencing attack like the one we just discussed? The simplest way is to add a little bit of random noise to each answer. On the first day, instead of releasing the exact count 416, we could release a noisy count, say, 420. Then on the second day, instead of releasing the true count 417, we release another noisy count, say, 415. More precisely, it is common to add noise to counts drawn from a Laplace or Gaussian distribution.  These figures are still close enough to the true values to be useful, but the difference of 1 is now obscured by the noise, so your privacy is protected.</p>

<p>Since the noise is unknown to <em>any</em> potential adversary, it introduces uncertainty that protects the contribution that an individual makes to the count. Taking the difference of two independent noisy counts results in something that is still noisy. However, we must be careful to quantify this privacy guarantee, particularly when it comes to composition.</p>

<p>So, how much noise do we need to add? Let’s go back to the example and suppose the insurance company provides noisy answers where the noise has mean zero and some fixed variance. Your employer could simply ask the same question again and again and each time receive a different noisy answer. Averaging these noisy answers will effectively reduce the variance of the added noise and allow the true answer to be discerned. That leaves us back where we started.</p>

<p>The moral of this revised example is that the scale of the noise must increase if we allow more access to the data, so more questions means more noise in each answer.<sup id="fnref:2"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:2">2</a></sup> 
Asking the same question again and again may seem silly. There are easy ways to defend against this and some similar attacks. (E.g., by returning the same answer each time instead of generating fresh noise.) But, unfortunately, the underlying phenomenon cannot be circumvented. One of the seminal works that led to differential privacy <a href="https://dl.acm.org/doi/10.1145/773153.773173" title="Irit Dinur, Kobbi Nissim. Revealing Information While Preserving Privacy. PODS 2003"><strong>[DN03]</strong></a> showed that there is an inherent tradeoff between the number of questions to be answered and the amount of noise that needs to be added to protect privacy. The general attack is simple: Instead of asking the same query again and again, the attacker asks “random” queries.<sup id="fnref:3"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:3">3</a></sup> This attack only requires basic linear algebra and, importantly, has been demonstrated on real systems <a href="https://arxiv.org/abs/1810.05692" title="Aloni Cohen, Kobbi Nissim. Linear Program Reconstruction in Practice. 2018."><strong>[CN18]</strong></a>.</p>

<h3 id="adaptive-composition">Adaptive Composition</h3>

<p>There are actually two kinds of composition to consider. There is <strong>non-adaptive composition</strong>, where the questions to be asked are pre-specified and thus independent of the data, and there is <strong>adaptive composition</strong>, where the questions may themselves depend on the results of prior access to the data. Adaptive composition arises in an interactive system where queries are submitted one-by-one and each answer is returned before the next query is submitted. So far, we have really only considered non-adaptive composition.</p>

<p>Any interactive system must take adaptive composition into account.  A natural algorithm which asks adaptive questions is gradient descent for minimizing a function that is determined by private data (e.g., for logistic regression on medical records). At each step, the algorithm asks for a gradient of the function, which depends on the private data, at the current point. Then the point is updated according to the reported gradient and the process repeats. Since the updated point depends on the previous answer, the next gradient computation is adaptive.</p>

<p>The good news is that differential privacy can handle adaptive composition just fine.  However, to handle adaptive composition, it’s really important that you have a worst-case privacy definition like differential privacy. As we will see below, average-case variants of differential privacy cannot handle adaptive composition. Intuitively, the problem is that whatever distributional assumption you might make about the data or query a priori is unlikely to hold when you condition on past interactions with the same data or related data.</p>

<p>Here’s a technical example that shows the difficulty of adaptive composition. Our data \(x \in \{-1,+1\}^n\) is a vector of \(n\) bits, one bit per person.<sup id="fnref:4"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:4">4</a></sup>  Because we’re considering average-case differential privacy, we’ll model this vector as uniformly random.  Consider the following slightly odd algorithm \(M_2(x,v)\)—it takes a vector \(v \in \{-1,+1\}^n\) from the user, and, if the correlation \(\langle x, v \rangle / n\) between \(v\) and \(x\) is smaller than \(\varepsilon/2\), the query returns \(\emptyset\), but, if the correlation between \(v\) and \(x\) is larger than \(\varepsilon/2\), the query returns the dataset \(x\).  In isolation this algorithm satisfies an average-case version of differential privacy, because if \(n\) is large enough and \(x\) is uniformly random, then it’s very unlikely that the user can guess a vector \(v\) that causes this algorithm to output anything other than \(\emptyset\).  This algorithm may seem contrived; it is a simple stand-in for any algorithm that behaves very well most of the time, but fails completely on some rare inputs.</p>

<p>Now, consider another, more familiar differentially private algorithm called randomized response <a href="https://www.jstor.org/stable/2283137?seq=1" title="Stanley Warner. Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias. Journal of the American Statistical Association 1965."><strong>[W65]</strong></a>.  For those not familiar, this algorithm \(M_1(x)\) outputs a vector \(y \in \{-1,+1\}^n\), where \(y_i\) is slightly more likely to be \(x_i\) than \(-x_i\).  Specifically, we set \(y_i = x_i\) with probability \((1+\varepsilon)/2\) and \(y_i = - x_i\) otherwise. This satisfies \(\log(\frac{1+\varepsilon}{1-\varepsilon})\)-differential privacy or, roughly, \(2\varepsilon\)-differential privacy. The upshot is that we obtain a vector \(y\) where the correlation between \(x\) and \(y\) is about \(\varepsilon\), i.e. \(\langle x , y \rangle / n \approx \varepsilon\).</p>

<p>OK, so \(M_1\) and \(M_2\) both satisfy strong average-case versions of differential privacy when the data is uniform, but what about their composition?  Well, the bad news is that running \(y = M_1(x)\) followed by \(M_2(x,y)\) is going to return the dataset \(x\) with probability approaching 100%!  That’s because \(y\) was designed precisely to be a vector with correlation about \(\varepsilon\) with \(x\), and this is exactly the key that gets \(M_2\) to unlock the dataset.</p>

<p>What went wrong here is that, even if \(x\) really is uniformly random, it’s very far from it when conditioned on the output \(y=M_1(x)\). To analyze \(M_2(x,y)\) we must look at the distribution of \(x\) conditioned on \(y\). This distribution is going to be messy and may as well be a worst-case distribution, which means we must leave the realm of average-case privacy.</p>

<h3 id="conclusion">Conclusion</h3>
<p>Composition guarantees that, as long as each part of your system is differentially private, then the overall system is too. It would be difficult to build sophisticated systems without this property. And it’s what allows one organization to release differentially private statistics without having to worry about what other information might be out there. In short, composition is what allows differential privacy to deal with the complexities of the real world.</p>

<p>It is unlikely that differential privacy would have taken off as a field of research without this composition property. Any proposal for an alternative approach to privacy-preserving data analysis should first be evaluated in terms of how it handles composition.</p>

<p>This post only scratches the surface. In particular, we haven’t talked about the quantitative aspects of composition; that’s where the fun really begins. We will leave you with some pointers to further reading on the topic:</p>

<ul>
  <li><a href="https://privacytools.seas.harvard.edu/publications/exposed-survey-attacks-private-data" title="Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman. Exposed! A Survey of Attacks on Private Data. Annual Review of Statistics and its Applications 2017."><strong>[DSSU17]</strong></a> This is a survey of attacks which explains quantitatively the relationship between noise, number of questions, and privacy risks.</li>
  <li><a href="https://arxiv.org/abs/1311.0776" title="Peter Kairouz, Sewoong Oh, Pramod Viswanath. The Composition Theorem for Differential Privacy. ICML 2015."><strong>[KOV15]</strong></a> <a href="https://arxiv.org/abs/1507.03113" title="Jack Murtagh, Salil Vadhan. The Complexity of Computing the Optimal Composition of Differential Privacy. TCC 2016"><strong>[MV15]</strong></a> <a href="https://arxiv.org/abs/1603.01887" title="Cynthia Dwork, Guy Rothbum. Concentrated Differential Privacy. 2016."><strong>[DR16]</strong></a> <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a> <a href="https://arxiv.org/abs/1702.07476" title="Ilya Mironov. Renyi Differential Privacy. CSF 2017."><strong>[M17]</strong></a> <a href="https://arxiv.org/abs/1905.02383" title="Jinshuo Dong, Aaron Roth, Weijie Su. Gaussian Differential Privacy. Journal of the Royal Statistical Society: Series B. 2020"><strong>[DRS19]</strong></a> On the positive side, these papers analyze how differential privacy composes, yielding sharp quantitative bounds.</li>
</ul>

<hr/>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>A good rule of thumb is that, if the number of released values is much larger than the number of people, then a privacy attack is probably possible. This is analogous to the rule from algebra that, if the number of constraints (released values) is greater than the number of unknown variables (people’s data), then the unknowns can be worked out. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>Exactly quantifying how much noise is needed as the number of questions grows leads to the concept of a “privacy budget.” That is, we must precisely quantify how differential privacy degrades under composition. This is a very deep topic and is something we hope to discuss in future posts. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:2">↩</a></p>
    </li>
    <li id="fn:3">
      <p>The queries do not need to be random <strong><a href="https://iacr.org/archive/crypto2008/51570469/51570469.pdf" title="Cynthia Dwork, Sergey Yekhanin. New Efficient Attacks on Statistical Disclosure Control Mechanisms. CRYPTO 2008">[DY08]</a></strong>. The queries simply need to be “sufficiently distinct”, which can be formulated precisely as being nearly orthogonal vectors. Random, or even pseudorandom queries (e.g., hash functions), will almost certainly satisfy this property. In general, it is fairly likely that a set of queries will have this property and allow a reconstruction attack; that is, it is hard to <em>avoid</em> this phenomenon. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:3">↩</a></p>
    </li>
    <li id="fn:4">
      <p>This representation of the dataset as a vector of bits \(x \in \{-1,+1\}^n \) is an abstraction. The entries in the dataset would actually be something like a set of pairs \( ( u_i, x_i ) \) for \(i = 1, \cdots, n \), where \(u_i\) is various information that identifies the individual concerned (name, address, race, date of birth, etc.). <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:4">↩</a></p>
    </li>
  </ol>
</div></div>
    </summary>
    <updated>2020-08-16T14:00:00Z</updated>
    <published>2020-08-16T14:00:00Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-08-16T16:21:57Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/08/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/08/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Two sites on toroidal polyhedra: Bonnie Stewarts Hohlkörper and Alex Doskey’s virtual reality models of Stewart’s polyhedra (\(\mathbb{M}\)). Found while researching a new WP article on Stewart’s book Adventures Among the Toroids. The first link is in German but readable through Google translate and has lots of pretty pictures. The second needs VR software to be usable.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p>Two sites on toroidal polyhedra: <a href="https://www.spektrum.de/alias/raeumliche-geometrie/bonnie-stewarts-hohlkoerper/681891">Bonnie Stewarts Hohlkörper</a> and <a href="http://polyhedra.doskey.com/Stewart00.html">Alex Doskey’s virtual reality models of Stewart’s polyhedra</a> (<a href="https://mathstodon.xyz/@11011110/104618649607830730">\(\mathbb{M}\)</a>). Found while researching a new WP article on Stewart’s book <em><a href="https://en.wikipedia.org/wiki/Adventures_Among_the_Toroids">Adventures Among the Toroids</a></em>. The first link is in German but readable through Google translate and has lots of pretty pictures. The second needs VR software to be usable.</p>
  </li>
  <li>
    <p><a href="https://www.robertdickau.com/mapfolding.html">The map folding problem, illustrated by Robert Dickau</a> (<a href="https://mathstodon.xyz/@11011110/104630030819531499">\(\mathbb{M}\)</a>). See <a href="https://www.robertdickau.com/default.html#math">Dickau’s home page</a> for many more mathematical illustrations, mostly of combinatorial enumeration problems and fractals.</p>
  </li>
  <li>
    <p>For some reason I wanted the name of a surface of revolution of a circular arc less than \(\pi\) around its chord (<a href="https://mathstodon.xyz/@11011110/104635297508909080">\(\mathbb{M}\)</a>). <a href="https://en.wikipedia.org/wiki/Lemon_(geometry)">Wikipedia said “lemon”</a> but sourced to MathWorld so I thought maybe MathWorld had made it up. Not so. Better sources say the same. And the surface for the complementary arc is an “apple”. It looks like a North American football but <a href="http://modellsammlung.uni-goettingen.de/index.php?lang=en&amp;r=5&amp;sr=17&amp;m=182">a “football” is a different surface of revolution, of constant positive Gaussian curvature</a>.</p>
  </li>
  <li>
    <p><a href="https://link.springer.com/journal/454/64/2">Special issue of <em>Discrete &amp; Computational Geometry</em> in memory of Branko Grünbaum</a> (<a href="https://mathstodon.xyz/@11011110/104643955543481823">\(\mathbb{M}\)</a>). I think many of the research papers in it are interesting but I want to draw particular attention to <a href="https://link.springer.com/article/10.1007/s00454-020-00214-y">the preface by Gil Kalai, Bojan Mohar, and Isabella Novik</a>, which provides a nice brief survey both of Grünbaum’s many contributions to discrete geometry and of the lines of active research they have led to.</p>
  </li>
  <li>
    <p><a href="http://gallery.bridgesmathart.org/exhibitions/2020-Bridges-Conference">2020 Bridges Conference Mathematical Art Gallery</a> (<a href="https://mathstodon.xyz/@11011110/104646771923669610">\(\mathbb{M}\)</a>). Many are great but a couple of my favorites are <a href="http://gallery.bridgesmathart.org/exhibitions/2020-bridges-conference/conan-chadbourne">Conan Chadbourne’s grid partition enumeration</a> and <a href="http://gallery.bridgesmathart.org/exhibitions/2020-bridges-conference/mdlevin_publicmsncom">Martin Levin’s ten-tetrahedron tensegrity</a>. I didn’t participate but apparently the Bridges conference itself was held virtually a few days ago; see <a href="https://2020.bridgesmathart.org/">the conference site</a> for more including papers and videos.</p>
  </li>
  <li>
    <p><a href="https://felixboiii.github.io/paper-plotter/">Paper plotter</a> (<a href="https://mathstodon.xyz/@11011110/104655233453519187">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24091297">via</a>): tool to make 3d paper cut-and-assemble models of the graphs of bivariate functions.</p>
  </li>
  <li>
    <p>Kowhaiwhai (<a href="https://mathstodon.xyz/@11011110/104663925881930344">\(\mathbb{M}\)</a>).  are repeating decorative patterns used in New Zealand on Maori buildings. <a href="https://natlib.govt.nz/photos?text=kowhaiwhai&amp;commit=Search">The National Library of NZ has a number of good examples</a>, including the <a href="https://natlib.govt.nz/records/23146518">sketches of patterns by Tamati Ngakoho (top) and of a traditional Arawa pattern (bottom)</a> shown below. There’s also <a href="http://www.maori.org.nz/whakairo/default.php?pid=sp55&amp;parent=52">a brief guide to their interpretation online</a>. I can’t find much analysis of their structure, though, beyond pointing to frieze groups for their symmetries. The part that interests me more is their fractal-like swooping structure, reminiscent of (and in some cases directly modeled on) fern fronds.</p>
  </li>
</ul>

<p style="text-align: center;"><img alt="Godber, Albert Percy, 1875-1949. Godber, Albert Percy, 1876-1949. Drawings of Maori rafter patterns or kowhaiwhai. 16. 22W. MA22; 17. 21W. MA21; and, 18. 25W. MA25. Puhoro. [1939-1947]. Ref: E-302-q-1-016/018. Alexander Turnbull Library, Wellington, New Zealand. From https://natlib.govt.nz/records/23146518" src="https://11011110.github.io/blog/assets/2020/Kowhaiwhai.jpg"/></p>

<ul>
  <li>
    <p><a href="https://www.wired.com/story/why-wikipedia-decided-to-stop-calling-fox-a-reliable-source/">Why Wikipedia decided to stop calling Fox a reliable source</a> (<a href="https://mathstodon.xyz/@11011110/104666160845673755">\(\mathbb{M}\)</a>). Note however that Fox has not actually been deemed unreliable, in general. <a href="https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources/Noticeboard/Archive_303#RfC:_Fox_News">The discussion had a no-consensus close</a>.</p>
  </li>
  <li>
    <p><a href="https://sinews.siam.org/Details-Page/untangling-random-polygons-and-other-things">Untangling random polygons</a> (<a href="https://mathstodon.xyz/@11011110/104677553383067578">\(\mathbb{M}\)</a>): repeatedly rescaling midpoint polygons always leads to an ellipse.</p>
  </li>
  <li>
    <p><a href="https://www.atlasobscura.com/articles/kek-lapis-sarawak">The mesmerizing geometry of Malaysia’s most complex cakes:
Bold colors and designs set kek lapis Sarawak apart</a> (<a href="https://mathstodon.xyz/@11011110/104680812259935603">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24116775">via</a>). As seen on The Great British Bake Off. These cakes have many parallel layers in bright colors, cut and rearranged to form complex designs. Mostly they involve 45 and 90-degree angles but at least one of the examples uses hexagonal symmetry instead.</p>
  </li>
  <li>
    <p>My Google Scholar profile has mildly broken down (<a href="https://mathstodon.xyz/@11011110/104683176033821672">\(\mathbb{M}\)</a>). When I go there, it offers me two new profiles to link as my coauthors: Man-Kwun Chiu and Matí Korman. They are indeed coauthors, from my new CCCG papers. But when I click to accept them as listed coauthors, it tells me I have too many coauthors, refuses to add them, and returns to offering me new profiles to link. I can see no way out of this other than to not accept my coauthors, which would be wrong. Google, fix this limitation!</p>
  </li>
  <li>
    <p>A use for old CDs: <a href="https://momath.org/home/math-monday-those-circles-are-great/">cut them up and glue the pieces together to make visualizations of great circle arrangements on the sphere</a> (<a href="https://mathstodon.xyz/@11011110/104690896919723051">\(\mathbb{M}\)</a>). The mathematical question posed by this is: for which numbers of great circles is it possible to make an arrangement in which all the arcs between pairs of neighbors have equal lengths?</p>
  </li>
  <li>
    <p><a href="https://thonyc.wordpress.com/">The Renaissance Mathematicus</a> (<a href="https://mathstodon.xyz/@pkra/104694183591138626">\(\mathbb{M}\)</a>), an interesting blogger on the history of science. See also the <a href="https://thonyc.wordpress.com/2020/08/15/keep-the-renaissance-mathematicus-online/">crowdfunding drive to replace their old creaky iMac</a>, from which I found this.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-08-15T17:02:00Z</updated>
    <published>2020-08-15T17:02:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-08-16T00:02:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05844</id>
    <link href="http://arxiv.org/abs/2008.05844" rel="alternate" type="text/html"/>
    <title>On seat allocation problem with multiple merit lists</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Rahul_Kumar.html">Rahul Kumar Singh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saxena:Sanjeev.html">Sanjeev Saxena</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05844">PDF</a><br/><b>Abstract: </b>In this note, we present a simpler algorithm for joint seat allocation
problem in case there are two or more merit lists. In case of two lists (the
current situation for Engineering seats in India), the running time of the
algorithm is proportional to sum of running time for two separate (delinked)
allocations. The algorithm is straight forward and natural and is not (at least
directly) based on deferred acceptance algorithm of Gale and Shapley. Each
person can only move higher in his or her preference list. Thus, all steps of
the algorithm can be made public. This will improve transparency and trust in
the system.
</p></div>
    </summary>
    <updated>2020-08-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05801</id>
    <link href="http://arxiv.org/abs/2008.05801" rel="alternate" type="text/html"/>
    <title>On graphs of bounded degree that are far from being Hamiltonian</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adler:Isolde.html">Isolde Adler</a>, Noleen Köhler University of Leeds) <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05801">PDF</a><br/><b>Abstract: </b>Hamiltonian cycles in graphs were first studied in the 1850s. Since then, an
impressive amount of research has been dedicated to identifying classes of
graphs that allow Hamiltonian cycles, and to related questions. The
corresponding decision problem, that asks whether a given graph is Hamiltonian
(i. e. admits a Hamiltonian cycle), is one of Karp's famous NP-complete
problems. It remains NP-complete on planar cubic graphs. In this paper we study
graphs of bounded degree that are far from being Hamiltonian, where a graph G
on n vertices is far from being Hamiltonian, if modifying a constant fraction
of n edges is necessary to make G Hamiltonian. We exhibit classes of graphs of
bounded degree that are locally Hamiltonian, i.e. every subgraph induced by the
neighbourhood of a small vertex set appears in some Hamiltonian graph, but that
are far from being Hamiltonian. We then use these classes to obtain a lower
bound in property testing. We show that in the bounded-degree graph model,
Hamiltonicity is not testable with one-sided error probability and query
complexity o(n). This contrasts the known fact that on planar (or minor-free)
graph classes, Hamiltonicity is testable with constant query complexity in the
bounded-degree graph model with two-sided error. Our proof is an intricate
construction that shows how to turn a d-regular graph into a graph that is far
from being Hamiltonian, and we use d-regular expander graphs to maintain local
Hamiltonicity.
</p></div>
    </summary>
    <updated>2020-08-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05800</id>
    <link href="http://arxiv.org/abs/2008.05800" rel="alternate" type="text/html"/>
    <title>On Testability of First-Order Properties in Bounded-Degree Graphs</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adler:Isolde.html">Isolde Adler</a>, Noleen Köhler, Pan Peng University of Leeds, University of Sheffield) <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05800">PDF</a><br/><b>Abstract: </b>We study property testing of properties that are definable in first-order
logic (FO) in the bounded-degree graph and relational structure models. We show
that any FO property that is defined by a formula with quantifier prefix
$\exists^*\forall^*$ is testable (i.e., testable with constant query
complexity), while there exists an FO property that is expressible by a formula
with quantifier prefix $\forall^*\exists^*$ that is not testable. In the dense
graph model, a similar picture is long known (Alon, Fisher, Krivelevich,
Szegedy, Combinatoria 2000), despite the very different nature of the two
models. In particular, we obtain our lower bound by a first-order formula that
defines a class of bounded-degree expanders, based on zig-zag products of
graphs. We expect this to be of independent interest. We then prove testability
of some first-order properties that speak about isomorphism types of
neighbourhoods, including testability of $1$-neighbourhood-freeness, and
$r$-neighbourhood-freeness under a mild assumption on the degrees.
</p></div>
    </summary>
    <updated>2020-08-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05728</id>
    <link href="http://arxiv.org/abs/2008.05728" rel="alternate" type="text/html"/>
    <title>Dynamic Complexity of Expansion</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Datta:Samir.html">Samir Datta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tawari:Anuj.html">Anuj Tawari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vasudev:Yadu.html">Yadu Vasudev</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05728">PDF</a><br/><b>Abstract: </b>Dynamic Complexity was introduced by Immerman and Patnaik
\cite{PatnaikImmerman97} (see also \cite{DongST95}). It has seen a resurgence
of interest in the recent past, see
\cite{DattaHK14,ZeumeS15,MunozVZ16,BouyerJ17,Zeume17,DKMSZ18,DMVZ18,BarceloRZ18,DMSVZ19,SchmidtSVZK20,DKMTVZ20}
for some representative examples. Use of linear algebra has been a notable
feature of some of these papers. We extend this theme to show that the gap
version of spectral expansion in bounded degree graphs can be maintained in the
class $\DynACz$ (also known as $\dynfo$, for domain independent queries) under
batch changes (insertions and deletions) of $O(\frac{\log{n}}{\log{\log{n}}})$
many edges.
</p>
<p>The spectral graph theoretic material of this work is based on the paper by
Kale-Seshadri \cite{KaleS11}. Our primary technical contribution is to maintain
up to logarithmic powers of the transition matrix of a bounded degree
undirected graph in $\DynACz$.
</p></div>
    </summary>
    <updated>2020-08-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05677</id>
    <link href="http://arxiv.org/abs/2008.05677" rel="alternate" type="text/html"/>
    <title>Some Preliminary Result About the Inset Edge and Average Distance of Trees</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khalifeh:M=_H=.html">M. H. Khalifeh</a>, A.-H. Esfahanian <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05677">PDF</a><br/><b>Abstract: </b>An added edge to a graph is called an inset edge. Predicting k inset edges
which minimize the average distance of a graph is known to be NP-Hard. However,
when k = 1 the complexity of the problem is polynomial. In this paper, some
tools for a precise analysis of the problem for the trees are established.
Using the tools, we can avoid using the distance matrix. This leads to more
efficient algorithms and a better analysis of the problem. Several applications
of the tools as well as a tight bound for the change of average distance when
an inset edge is added to a tree are presented.
</p></div>
    </summary>
    <updated>2020-08-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05674</id>
    <link href="http://arxiv.org/abs/2008.05674" rel="alternate" type="text/html"/>
    <title>Inset Edges Effect and Average Distance of Trees</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khalifeh:M=_H=.html">M. H. Khalifeh</a>, A.-H. Esfahanian <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05674">PDF</a><br/><b>Abstract: </b>An added edge to a graph is called an inset edge. Predicting k inset edges
which minimize the average distance of a graph is known to be NP-Hard. When k =
1 the complexity of the problem is polynomial. In this paper, we further find
the single inset edge(s) of a tree with the closest change on the average
distance to a given input. To do that we may require the effect of each inset
edge for the set of inset edges. For this, we propose an algorithm with the
time complexity between O(m) and O(m/m) and an average of less than O(
m.log(m)), where m stands for the number of possible inset edges. Then it takes
up to O(log(m)) to find the target inset edges for a custom change on the
average distance. Using theoretical tools, the algorithm strictly avoids
recalculating the distances with no changes, after adding a new edge to a tree.
Then reduces the time complexity of calculating remaining distances using some
matrix tools which first introduced in [8] with one additional technique. This
gives us a dynamic time complexity and absolutely depends on the input tree
which is proportion to the Wiener index of the input tree.
</p></div>
    </summary>
    <updated>2020-08-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05648</id>
    <link href="http://arxiv.org/abs/2008.05648" rel="alternate" type="text/html"/>
    <title>Cut Sparsification of the Clique Beyond the Ramanujan Bound</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Antares.html">Antares Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shi:Jonathan.html">Jonathan Shi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trevisan:Luca.html">Luca Trevisan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05648">PDF</a><br/><b>Abstract: </b>We prove that a random $d$-regular graph, with high probability, is a cut
sparsifier of the clique with approximation error at most $\left(2\sqrt{\frac 2
\pi} + o_{n,d}(1)\right)/\sqrt d$, where $2\sqrt{\frac 2 \pi} = 1.595\ldots$
and $o_{n,d}(1)$ denotes an error term that depends on $n$ and $d$ and goes to
zero if we first take the limit $n\rightarrow \infty$ and then the limit $d
\rightarrow \infty$.
</p>
<p>This is established by analyzing linear-size cuts using techniques of
Jagannath and Sen \cite{jagannath2017unbalanced} derived from ideas from
statistical physics and analyzing small cuts via martingale inequalities.
</p>
<p>We also prove that every spectral sparsifier of the clique having average
degree $d$ and a certain high "pseudo-girth" property has an approximation
error that is at least the "Ramanujan bound" $(2-o_{n,d}(1))/\sqrt d$, which is
met by $d$-regular Ramanujan graphs, generalizing a lower bound of Srivastava
and Trevisan \cite{ST18}.
</p>
<p>Together, these results imply a separation between spectral sparsification
and cut sparsification. If $G$ is a random $\log n$-regular graph on $n$
vertices, we show that, with high probability, $G$ admits a (weighted subgraph)
cut sparsifier of average degree $d$ and approximation error at most
$(1.595\ldots + o_{n,d}(1))/\sqrt d$, while every (weighted subgraph) spectral
sparsifier of $G$ having average degree $d$ has approximation error at least
$(2-o_{n,d}(1))/\sqrt d$.
</p></div>
    </summary>
    <updated>2020-08-15T23:23:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05594</id>
    <link href="http://arxiv.org/abs/2008.05594" rel="alternate" type="text/html"/>
    <title>Cadences in Grammar-Compressed Strings</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pape=Lange:Julian.html">Julian Pape-Lange</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05594">PDF</a><br/><b>Abstract: </b>Cadences are structurally maximal arithmetic progressions of indices
corresponding to equal characters in an underlying string.
</p>
<p>This paper provides a polynomial time detection algorithm for 3-cadences in
grammar-compressed binary strings. This algorithm also translates to a linear
time detection algorithm for 3-cadences in uncompressed binary strings.
</p>
<p>Furthermore, this paper proves that several variants of the cadence detection
problem are NP-complete for grammar-compressed strings. As a consequence, the
equidistant subsequence matching problem with patterns of length three is
NP-complete for grammar-compressed ternary strings.
</p></div>
    </summary>
    <updated>2020-08-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05584</id>
    <link href="http://arxiv.org/abs/2008.05584" rel="alternate" type="text/html"/>
    <title>Graph Drawing via Gradient Descent, $(GD)^2$</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Reyan Ahmed, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Luca:Felice_De.html">Felice De Luca</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Devkota:Sabin.html">Sabin Devkota</a>, Stephen Kobourov, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Mingwei.html">Mingwei Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05584">PDF</a><br/><b>Abstract: </b>Readability criteria, such as distance or neighborhood preservation, are
often used to optimize node-link representations of graphs to enable the
comprehension of the underlying data. With few exceptions, graph drawing
algorithms typically optimize one such criterion, usually at the expense of
others. We propose a layout approach, Graph Drawing via Gradient Descent,
$(GD)^2$, that can handle multiple readability criteria. $(GD)^2$ can optimize
any criterion that can be described by a smooth function. If the criterion
cannot be captured by a smooth function, a non-smooth function for the
criterion is combined with another smooth function, or auto-differentiation
tools are used for the optimization. Our approach is flexible and can be used
to optimize several criteria that have already been considered earlier (e.g.,
obtaining ideal edge lengths, stress, neighborhood preservation) as well as
other criteria which have not yet been explicitly optimized in such fashion
(e.g., vertex resolution, angular resolution, aspect ratio). We provide
quantitative and qualitative evidence of the effectiveness of $(GD)^2$ with
experimental data and a functional prototype:
\url{<a href="http://hdc.cs.arizona.edu/~mwli/graph-drawing/">this http URL</a>}.
</p></div>
    </summary>
    <updated>2020-08-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05569</id>
    <link href="http://arxiv.org/abs/2008.05569" rel="alternate" type="text/html"/>
    <title>A new notion of commutativity for the algorithmic Lov\'{a}sz Local Lemma</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Harris:David_G=.html">David G. Harris</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iliopoulos:Fotis.html">Fotis Iliopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolmogorov:Vladimir.html">Vladimir Kolmogorov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05569">PDF</a><br/><b>Abstract: </b>The Lov\'{a}sz Local Lemma (LLL) is a powerful tool in probabilistic
combinatorics which can be used to establish the existence of objects that
satisfy certain properties. The breakthrough paper of Moser and Tardos and
follow-up works revealed that the LLL has intimate connections with a class of
stochastic local search algorithms for finding such desirable objects. In
particular, it can be seen as a sufficient condition for this type of
algorithms to converge fast.
</p>
<p>Besides conditions for existence of and fast convergence to desirable
objects, one may naturally ask further questions regarding properties of these
algorithms. For instance, "are they parallelizable?", "how many solutions can
they output?", "what is the expected "weight" of a solution?", etc. These
questions and more have been answered for a class of LLL-inspired algorithms
called commutative. In this paper we introduce a new, very natural and more
general notion of commutativity (essentially matrix commutativity) which allows
us to show a number of new refined properties of LLL-inspired local search
algorithms with significantly simpler proofs.
</p></div>
    </summary>
    <updated>2020-08-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05558</id>
    <link href="http://arxiv.org/abs/2008.05558" rel="alternate" type="text/html"/>
    <title>On the complexity of finding a local minimizer of a quadratic function over a polytope</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahmadi:Amir_Ali.html">Amir Ali Ahmadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Jeffrey.html">Jeffrey Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05558">PDF</a><br/><b>Abstract: </b>We show that unless P=NP, there cannot be a polynomial-time algorithm that
finds a point within Euclidean distance $c^n$ (for any constant $c \ge 0$) of a
local minimizer of an $n$-variate quadratic function over a polytope. This
result (even with $c=0$) answers a question of Pardalos and Vavasis that
appeared in 1992 on a list of seven open problems in complexity theory for
numerical optimization. Our proof technique also implies that the problem of
deciding whether a quadratic function has a local minimizer over an (unbounded)
polyhedron, and that of deciding if a quartic polynomial has a local minimizer
are NP-hard.
</p></div>
    </summary>
    <updated>2020-08-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.05504</id>
    <link href="http://arxiv.org/abs/2008.05504" rel="alternate" type="text/html"/>
    <title>On the tree-width of even-hole-free graphs</title>
    <feedworld_mtime>1597449600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aboulker:Pierre.html">Pierre Aboulker</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adler:Isolde.html">Isolde Adler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Eun_Jung.html">Eun Jung Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sintiari:Ni_Luh_Dewi.html">Ni Luh Dewi Sintiari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trotignon:Nicolas.html">Nicolas Trotignon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.05504">PDF</a><br/><b>Abstract: </b>The class of all even-hole-free graphs has unbounded tree-width, as it
contains all complete graphs. Recently, a class of (even-hole, $K_4$)-free
graphs was constructed, that still has unbounded tree-width [Sintiari and
Trotignon, 2019]. The class has unbounded degree and contains arbitrarily large
clique-minors. We ask whether this is necessary.
</p>
<p>We prove that for every graph $G$, if $G$ excludes a fixed graph $H$ as a
minor, then $G$ either has small tree-width, or $G$ contains a large wall or
the line graph of a large wall as induced subgraph. This can be seen as a
strengthening of Robertson and Seymour's excluded grid theorem for the case of
minor-free graphs. Our theorem implies that every class of even-hole-free
graphs excluding a fixed graph as a minor has bounded tree-width. In fact, our
theorem applies to a more general class: (theta, prism)-free graphs. This
implies the known result that planar even hole-free graph have bounded
tree-width [da Silva and Linhares Sales, Discrete Applied Mathematics 2010].
</p>
<p>We conjecture that even-hole-free graphs of bounded degree have bounded
tree-width. If true, this would mean that even-hole-freeness is testable in the
bounded-degree graph model of property testing. We prove the conjecture for
subcubic graphs and we give a bound on the tree-width of the class of (even
hole, pyramid)-free graphs of degree at most 4.
</p></div>
    </summary>
    <updated>2020-08-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20069</id>
    <link href="https://gilkalai.wordpress.com/2020/08/14/to-cheer-you-up-in-difficult-times-9-alexey-pokrovskiy-proved-that-rotas-basis-conjecture-holds-asymptotically/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 9: Alexey Pokrovskiy proved that Rota’s Basis Conjecture holds asymptotically</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Pokrovskiy’s startling morning  rainbow Rota’s Basis Conjecture holds asymptotically, by Alexey Pokrovskiy Abstract: Rota’s Basis Conjecture is a well known problem from matroid theory, that states that for any collection of n bases in a rank n matroid, it is … <a href="https://gilkalai.wordpress.com/2020/08/14/to-cheer-you-up-in-difficult-times-9-alexey-pokrovskiy-proved-that-rotas-basis-conjecture-holds-asymptotically/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h2><a href="https://gilkalai.files.wordpress.com/2020/08/alexeypokrovskiy.jpg"><img alt="" class="alignnone size-medium wp-image-20075" height="300" src="https://gilkalai.files.wordpress.com/2020/08/alexeypokrovskiy.jpg?w=209&amp;h=300" width="209"/></a></h2>
<h2>Pokrovskiy’s startling morning  <strong><span style="color: #ff0000;">r</span><span style="color: #0000ff;">ai</span><span style="color: #ff6600;">n</span><span style="color: #ff9900;">b</span><span style="color: #ff00ff;">o</span><span style="color: #800080;">w</span></strong></h2>
<p><a href="https://arxiv.org/abs/2008.06045">Rota’s Basis Conjecture holds asymptotically</a>, by Alexey <span style="color: #000000;">Pokrovskiy</span></p>
<p><strong>Abstract:</strong> Rota’s Basis Conjecture is a well known problem from matroid theory, that states that for any collection of n bases in a rank n matroid, it is possible to decompose all the elements into n disjoint rainbow bases. Here an asymptotic version of this is proved. We show that it is possible to find <em>n − o(n)</em> disjoint rainbow independent sets of size <em>n − o(n)</em>.</p>
<p>A <strong><span style="color: #ff0000;">r</span><span style="color: #0000ff;">ai</span><span style="color: #ff6600;">n</span><span style="color: #ff9900;">b</span><span style="color: #ff00ff;">o</span><span style="color: #800080;">w </span></strong><span style="color: #800080;"><span style="color: #000000;">basis is a basis with one element from each collection.</span></span></p>
<p>(I thank Nati Linial for telling me about it.)</p>
<p>Another way to formulate Rota’s basis conjecture (for representable matroids) is that if <em>B</em><sub>1</sub>, <em>B</em><sub>2</sub>, …, <em>B<sub>n</sub></em> are <em>n</em> bases of an <em>n</em>-dimensional vector space <em>V</em> (not necessarily distinct or disjoint), then there exists an <em>n</em> × <em>n</em> grid of vectors (<em>v<sub>ij</sub></em>) such that</p>
<p>1. the <em>n</em> vectors in row <em>i</em> are the members of the <em>i</em>th basis <em>B<sub>i</sub></em> (in some order), and</p>
<p>2. in each column of the matrix, the <em>n</em> vectors in that column form a basis of <em>V</em>.</p>
<p>If all the bases are the standard basis then this reduces to the existence of <a href="https://en.wikipedia.org/wiki/Latin_square">Latin squares</a>.</p>
<p><strong>Unrelated trivia question:</strong>  AGC-GTC-TGC-GTC-TGC-GAC-GATC-? what comes next in the sequence?</p>
<p>We mentioned Rota’s basis conjecture in various earlier posts.  A classic paper on the subject is the <a href="https://gilkalai.files.wordpress.com/2017/02/huang-rota.pdf">1989 paper by Rosa Huang and Gian Carlo-Rota</a>. Three and a half years ago Timothy Chow lunched a polymath project (Polymath 12) to solve it. (Here is my<a href="https://gilkalai.wordpress.com/2017/02/26/timothy-chow-launched-polymath12-on-rota-basis-conjecture-and-other-news/"> post on the project with various variants of the conjecture</a>, the <a href="https://polymathprojects.org/2017/02/23/rotas-basis-conjecture-polymath-12/">first post on the polymath blog</a>, and the <a href="https://asone.ai/polymath/index.php?title=Rota%27s_conjecture">wiki</a>). See <a href="https://gilkalai.wordpress.com/2014/08/08/jim-geelen-bert-gerards-and-geo%ef%ac%80-whittle-solved-rotas-conjecture-on-matroids/">this post</a> for several famous conjectures by Rota, and this post about the related <a href="https://gilkalai.wordpress.com/2017/03/15/test-your-intuition-about-the-alon-tarsi-conjecture/">Alon-Tarsi conjecture</a>.</p></div>
    </content>
    <updated>2020-08-14T10:09:36Z</updated>
    <published>2020-08-14T10:09:36Z</published>
    <category term="Combinatorics"/>
    <category term="Updates"/>
    <category term="Alexey Pokrovskiy"/>
    <category term="Gian Carlo Rota"/>
    <category term="Rosa Huang"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-08-16T16:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17416</id>
    <link href="https://rjlipton.wordpress.com/2020/08/13/thanks-to-an-explainer/" rel="alternate" type="text/html"/>
    <title>Thanks to An Explainer</title>
    <summary>Conrad explains all Keith Conrad is a professor in the mathematics department at UCONN—the University of Connecticut. My dear wife Kathryn Farley and I are about to move to join him—not as faculty but as another resident of the “Constitution State.” Today we thank him for his work on explaining mathematics. Conrad is a prolific […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Conrad explains all</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/thanks-to-an-explainer/keithconrad/" rel="attachment wp-att-17410"><img alt="" class="aligncenter size-full wp-image-17410" src="https://rjlipton.files.wordpress.com/2020/08/keithconrad.jpg?w=600"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Keith Conrad is a professor in the mathematics department at UCONN—the University of Connecticut. My dear wife Kathryn Farley and I are about to move to join him—not as faculty but as another resident of the “Constitution State.”</p>
<p>
Today we thank him for his work on explaining mathematics.</p>
<p>
Conrad is a prolific writer of articles on mathematics. He makes hard concepts clear, he makes easy concepts interesting. He has a sense of humor; his <a href="https://kconrad.math.uconn.edu">website</a> is filled with fun of all kinds.</p>
<p>
He has interesting license plates, photos of streets that bear his first name, a Russian <a href="https://kconrad.math.uconn.edu/">update</a> to Tom Lehrer’s “Elements” song, and more links to others’ items. </p>
<p>
If you’d like a video example of fun see <a href="http://i.stack.imgur.com/d2OKd.gif">this</a> for a short video illusion. Too bad it is an illusion and it does not work in reality. As a chocolate lover I wish it worked—free chocolate forever. A similar <a href="https://en.wikipedia.org/wiki/Missing_square_puzzle">illusion</a> with a triangle was once featured by Martin Gardner, leading Ken as a teenager to make a cardboard cutout version overlaid on a map of the Bermuda Triangle as an “explanation” of disappearances there.</p>
<p>
</p><p/><h2> Articles </h2><p/>
<p/><p>
I have not yet met Conrad in person, but have enjoyed reading his articles on math of all kinds. He has a giant <img alt="{63 \times 4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B63+%5Ctimes+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{63 \times 4}"/> <a href="https://kconrad.math.uconn.edu/blurbs/">grid</a> of clickable titles, grouped by subject area. </p>
<p>
For an example, he has a <a href="https://kconrad.math.uconn.edu/blurbs/galoistheory/numbersoncircle.pdf">title</a> “Roots on a Circle.” The file name says “numbers on a circle” and the essay begins disarmingly enough with a picture of the 7th roots of unity. The next page shows a simple polynomial where most but not all roots lie on the circle:</p>
<p><a href="https://rjlipton.wordpress.com/thanks-to-an-explainer/lehmerspolynomialroots/" rel="attachment wp-att-17412"><img alt="" class="aligncenter size-medium wp-image-17412" height="161" src="https://rjlipton.files.wordpress.com/2020/08/lehmerspolynomialroots.jpg?w=300&amp;h=161" width="300"/></a></p>
<p>
This is enough to draw you in and stay attached as things become more complicated beginning on page 3. It helps that Conrad does not stint on algebraic details. This essay supplements a cautionary tale in mathematics: a <a href="https://mathoverflow.net/questions/15444/examples-of-eventual-counterexamples">link</a> to a MathOverflow list whose top item is that the factors of <img alt="{x^n - 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5En+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^n - 1}"/> over <img alt="{\mathbb{Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BQ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Q}}"/> have no coefficient of absolute value greater than <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> for <img alt="{n = 1,\dots,104}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+1%2C%5Cdots%2C104%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 1,\dots,104}"/>. Before you try to prove this by induction check out <img alt="{n = 105}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+105%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 105}"/>.</p>
<p>
</p><p/><h2> Finite Groups </h2><p/>
<p/><p>
One of my favorite articles is <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/order.pdf">titled</a>, “Orders Of Elements In A Group.” Conrad starts with the humble concept of the order of an element in a group. Then he builds up a theory that explains various properties of order. </p>
<p>
I especially like that he supplies examples to help you with your intuition. For me, and Ken, finite groups are just counter-intuitive. Groups have magical properties but my naive conjectures about them usually fail. Conrad’s article ends with a discussion of primality testing which is dear to us in complexity theory.</p>
<p>
</p><p/><h2> Finite Groups that Encode Information </h2><p/>
<p/><p>
I recently needed a finite group with a certain structure. In complexity theory we sometimes use matrices to encode information in a way that makes an algorithm more efficient. Algorithms like matrices since they can be stored and multiplied efficiently. As an example, suupose that we have two matrices <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  AB = -BA. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++AB+%3D+-BA.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  AB = -BA. "/></p>
<p>That is the matrices anti-commute. Then we can use such matrices to encode information about a string <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> of <img alt="{A,B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B}"/>‘s. Every string <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> can be written as: 	</p>
<p align="center"><img alt="\displaystyle  \pm A^{k}B^{l}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpm+A%5E%7Bk%7DB%5E%7Bl%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \pm A^{k}B^{l}. "/></p>
<p>The <img alt="{\pm}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pm}"/> encodes the number of inversions in the string <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>. That is the number of times <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is followed by an <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>: 	</p>
<p align="center"><img alt="\displaystyle  \dots B \dots A \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cdots+B+%5Cdots+A+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \dots B \dots A \dots "/></p>
<p>So <img alt="{ABBA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BABBA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ABBA}"/> has <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> inversions, and <img alt="{AAABA}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAAABA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{AAABA}"/> has <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. Here are matrices that <a href="https://en.wikipedia.org/wiki/Generalized_Clifford_algebra">anti-commute</a>. </p>
<p/><p align="center"><img alt="\displaystyle  A = \begin{pmatrix} 0&amp;1\\ 1&amp;0 \end{pmatrix}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A+%3D+%5Cbegin%7Bpmatrix%7D+0%261%5C%5C+1%260+%5Cend%7Bpmatrix%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A = \begin{pmatrix} 0&amp;1\\ 1&amp;0 \end{pmatrix}"/></p>
<p/><p align="center"><img alt="\displaystyle  B = \begin{pmatrix} 1&amp;0\\ 0&amp;-1 \end{pmatrix}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++B+%3D+%5Cbegin%7Bpmatrix%7D+1%260%5C%5C+0%26-1+%5Cend%7Bpmatrix%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  B = \begin{pmatrix} 1&amp;0\\ 0&amp;-1 \end{pmatrix}"/></p>
<p>
We can do even better. There are matrices so that 	</p>
<p align="center"><img alt="\displaystyle  AB = \lambda BA, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++AB+%3D+%5Clambda+BA%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  AB = \lambda BA, "/></p>
<p>where <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> is a root of unity. They exist as the example below shows for fourth roots of unity. But finding such matrices was curiously hard, at least for me. Lots of web searching.</p>
<p/><p align="center"><img alt="\displaystyle  A = \begin{pmatrix} 0&amp;1&amp;0&amp;0\\ 0&amp;0&amp;1&amp;0\\ 0&amp;0&amp;0&amp;1\\ 1&amp;0&amp;0&amp;0 \end{pmatrix} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A+%3D+%5Cbegin%7Bpmatrix%7D+0%261%260%260%5C%5C+0%260%261%260%5C%5C+0%260%260%261%5C%5C+1%260%260%260+%5Cend%7Bpmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A = \begin{pmatrix} 0&amp;1&amp;0&amp;0\\ 0&amp;0&amp;1&amp;0\\ 0&amp;0&amp;0&amp;1\\ 1&amp;0&amp;0&amp;0 \end{pmatrix} "/></p>
<p/><p align="center"><img alt="\displaystyle  B = \begin{pmatrix} 1&amp;0&amp;0&amp;0\\ 0&amp;i&amp;0&amp;0\\ 0&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;0&amp;-i \end{pmatrix} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++B+%3D+%5Cbegin%7Bpmatrix%7D+1%260%260%260%5C%5C+0%26i%260%260%5C%5C+0%260%26-1%260%5C%5C+0%260%260%26-i+%5Cend%7Bpmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  B = \begin{pmatrix} 1&amp;0&amp;0&amp;0\\ 0&amp;i&amp;0&amp;0\\ 0&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;0&amp;-i \end{pmatrix} "/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Conrad’s articles are helpful. In a wide variety of topics he presents both theorems and history of math concepts. What I find most attractive is the examples and additional comments that pepper his writing. </p>
<p>
Check him out.</p></font></font></div>
    </content>
    <updated>2020-08-13T18:40:44Z</updated>
    <published>2020-08-13T18:40:44Z</published>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="articles"/>
    <category term="explain"/>
    <category term="finite groups"/>
    <category term="surprise"/>
    <category term="writing"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-08-16T16:20:41Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-5601291488610843158</id>
    <link href="http://processalgebra.blogspot.com/feeds/5601291488610843158/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=5601291488610843158" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/5601291488610843158" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/5601291488610843158" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/08/an-interview-with-jos-baeten-outgoing.html" rel="alternate" type="text/html"/>
    <title>An interview with Jos Baeten, outgoing director of the CWI</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>After nine years, <a href="https://www.cwi.nl/people/jos-baeten" target="_blank">Jos Baeten</a> will step down as general director of <a href="https://www.cwi.nl/" target="_blank">CWI</a> on 30 September 2020 and there will be a <a href="https://www.cwi.nl/events/2020/farewell-symposium-jos-baeten/symposium-retirement-jos-baeten" target="_blank">retirement symposium</a> in his honour on 1 October 2020. (Jos Baeten's successor will be <a href="https://www.tue.nl/en/research/researchers/ton-de-kok/" target="_blank">Tom de Kok</a>. You can read the CWI news item related to Tom de Kok's appointment <a href="https://www.cwi.nl/news/2020/ton-de-kok-appointed-new-director-of-cwi" target="_blank">here</a>. Tom de Kok, just like Jos Baeten before him, joins CWI from Eindhoven University of Technology.) </p><p>Jos Baeten has been one of the prime movers in the development of process algebra since 1987, and has been the driving force behind the CONCUR conference series and the CONCUR Basic Research Actions in the late 1980s and the early 1990s. Apart from being general director of CWI, he has served the TCS community in a variety of roles and organisations, and has helped to connect the work of the concurrency-theory community with that done in control and mechanical engineering. He has also supervised more than 30 PhD theses. </p><p>I asked Jos a few questions via email and I am happy to share his answers with the readers of this blog. Thanks to Jos for all the contributions he has given to the research community throughout his career, and for sharing his opinions and reminiscences with us! </p><p><b>The interview</b> </p><p><b>Luca:</b> Let's start from the beginning. If I remember correctly, your background was in model theory. Could you tell us what prompted you to move to doing research in computer science? </p><p><b>Jos:</b> I have a Master in logic and foundations of mathematics (with a minor in philosophy) from Utrecht University, with a Master’s thesis on lambda calculus, and a PhD in logic and foundations of mathematics from the University of Minnesota, with a PhD thesis on definability theory (a cross between recursion theory and set theory) entitled "<a href="https://link.springer.com/chapter/10.1007/BFb0099378" target="_blank">Filters and ultrafilters over definable subsets of admissible ordinals</a>". Returning from the US, I got a job teaching undergraduate maths at Delft University. But I wanted getting into research, and meeting <a href="https://staff.fnwi.uva.nl/j.a.bergstra/" target="_blank">Jan Bergstra</a> he said there were lots of opportunities in computer science research, not in maths. Writing a paper with him and <a href="https://www.cs.vu.nl/~jwk/" target="_blank">Jan Willem Klop</a> on term rewriting systems would qualify me, he said. So it happened, and I got a postdoc at CWI in the framework of the ESPRIT I project. I kept on doing maths, but it was called computer science. </p><p><b>Luca:</b> I have heard that you were recruited by Jan Bergstra and Jan Willem Klop to work within their group at the CWI, and that <a href="http://theory.stanford.edu/~rvg/" target="_blank">Rob van Glabbeek</a> joined the team soon after you. Could you tell us about the environment at the CWI at that time? It must have been a very exciting place to be with all the activity related to work on <a href="https://en.wikipedia.org/wiki/Algebra_of_communicating_processes#:~:text=The%20algebra%20of%20communicating%20processes,process%20algebras%20or%20process%20calculi." target="_blank">ACP</a> and related topics. </p><p><b>Jos:</b> It was a heady period. The group of Jan and Jan Willem was expanding rapidly on EU money, with me, Rob van Glabbeek and <a href="http://www.cs.ru.nl/~fvaan/" target="_blank">Frits Vaandrager.</a> I really liked ACP and doing universal algebra. </p><p><b>Luca:</b> What was your first paper in CS about? Which paper from your initial period at the CWI are you most proud of? </p><p><b>Jos:</b> My first paper was "<a href="https://www.researchgate.net/publication/225810594_Term_rewriting_systems_with_priorities" target="_blank">On term rewriting, Term rewriting systems with priorities</a>". I am most proud of my first ACP paper, "<a href="https://pure.tue.nl/ws/files/2137499/335911.pdf" target="_blank">Syntax and defining equations for an interrupt mechanism in process algebra</a>". This is about adding the priority operator. For axiomatisation, it needed an auxiliary operator that later turned out to be almost optimal. I came up with an axiomatisation, and Jan Bergstra forced me to prove it correct by doing all the critical pairs. I found a small error in pair 101 (of the 102 pairs), corrected it and proved the result correct. This all in a couple of weeks. </p><p><b>Luca:</b> Let's move on to the CONCUR project: How did it come about and what was its legacy? I recall that you came to visit <a href="https://www.scss.tcd.ie/Matthew.Hennessy/" target="_blank">Matthew Hennessy</a> at the University of Sussex when I was a PhD student there to discuss the CONCUR proposal. How did you become involved in leading such a large and high profile group of researchers so early in your career at the CWI? What was it like to lead the CONCUR project?  <br/></p><p><b>Jos:</b> This is again due to Jan Bergstra: he believes in delegating responsibilities quickly. When the new instrument of Basic Research Action came up in the EU, he wanted a BRA with <a href="https://en.wikipedia.org/wiki/Tony_Hoare" target="_blank">Hoare</a>, <a href="https://en.wikipedia.org/wiki/Robin_Milner" target="_blank">Milner </a>and Hennessy. I got the assignment and made a trip to the UK visiting all three, and got all of them on board. The idea of the project was unification, but that did not come about, everyone kept doing their own thing. We did, however, produce excellent papers, including the curious </p><p>J.C.M. Baeten, J.A. Bergstra, C.A.R. Hoare, R. Milner, J. Parrow &amp; R. de Simone, The variety of process algebra. Deliverable ESPRIT Basic Research Action 3006, CONCUR, University of Edinburgh 1991 </p><p><b>Luca:</b> How would you summarize the history of the conference CONCUR? Can it be split into different periods, and if so, what characterizes them? </p><p><b>Jos:</b> CONCUR 90, which was the first CONCUR conference, was organised by BRA CONCUR, but with speakers from all 5 BRA’s in the area of concurrency; in 1991 Milner did not want such a conference to occur in Edinburgh, so Jan Bergstra and I did it again in Amsterdam. Then, we involved <a href="https://www.cs.stonybrook.edu/people/faculty/ScottSmolka" target="_blank">Scott Smolka</a> for 1992 and after that, it was established, with a steering committee comprised of representatives from the 5 BRAs and Scott. As of 1993, it was firmly established, we found a niche and time period (end of August, beginning of September). All branches of concurrency were involved from the start, and over the years, it shows fashion clearly, for instance in certain years there was a lot of <a href="https://en.wikipedia.org/wiki/%CE%A0-calculus" target="_blank">pi-calculus</a>. </p><p><b>Luca:</b> In the 80s and the 90s there were three "schools" in process algebra,     the ACP, CCS, and CSP schools. With the benefit of hindsight, which were heir main contributions? To which extent have they converged? Is this classification still important for current research? </p><p><b>Jos:</b> I refer to my paper  <br/></p><p>J.C.M. Baeten, <a href="https://pure.tue.nl/ws/files/2154050/200402.pdf" target="_blank">A brief history of process algebra</a>. Theoretical Computer Science 335 (2/3), 2005, pp. 131-146.  <br/></p><p>I do a comparison in there. In my opinion, the book  <br/></p><p>J.C.M. Baeten, T. Basten and M.A. Reniers, <a href="https://www.cambridge.org/core/books/process-algebra-equational-theories-of-communicating-processes/0A091BDAA17DA4D3D30FCD6F52E0E6B8" target="_blank">Process Algebra: Equational Theories for Communicating Processes</a>. Cambridge Tract in Theoretical Computer Science 50, Cambridge University Press 2010  <br/></p><p>has the unification, presenting all three in the same framework. However, process algebra is out of fashion, so who cares? </p><p><b>Luca:</b>  When did you become interested in supervisory control? What motivated you to apply techniques from process algebra in that research field? Looking at the outcome of that work, both in theory and in practice, that was a very good move! Could you tell us about the position you had in Mechanical Engineering? How did it come about and what was it like to work in that department? </p><p><b>Jos:</b> More than 20 years in the department of computer science, having been dean twice, I got to know the university very well indeed. <a href="https://research.tue.nl/en/persons/je-koos-rooda" target="_blank">Koos Rooda</a>, professor of systems engineering at Mechanical Engineering, requested my assistance to get more software engineering into systems engineering, to replace his cooperation with Martin Rem, who became rector. In particular, he was interested in the combination of discrete event reasoning (i.e., process algebra) with the continuous mathematics of systems and control. Having done process algebra with timing and probabilities, I was willing to take up this challenge. Upon his retirement, with the vigorous backing of the dean of Mechanical Engineering at the time, I took over his position. At mid-life, I was ready for a change. I liked the hands-on attitude of mechanical engineering students, who are no good at theory or proofs, but are very good in using mathematics and trying out software tools. But then, I was asked to apply to the vacancy of director of CWI. This was the job I had always wanted. I had enjoyed being dean and doing research management, and CWI is a very prestigious institute with (at that time) very independent management. </p><p> </p><p><b>Luca:</b> Back at CWI: What skills do you think one needs to be the director of such a high-profile institute? What was your vision for the CWI and how much of it did you manage to achieve? What role do you think the CWI can/should have in the coming decade and beyond? </p><p><b>Jos:</b> The most important skill is hiring those early career scientists that grow out to become top scientists. Internally, you have to manage your key personnel, operating by consensus and compromise. Externally, you have to find your way in national science politics and always justify your existence. The growing tension between Dutch universities and Dutch extra-university research institutes has almost defeated me, but I managed to complete my term until retirement, and leave CWI in good shape, to my opinion. I managed to keep the excellent reputation of the institute, and also managed to bring in dynamics, rejuvenation and new subjects. What I did not achieve, is that CWI again becomes a publisher, and publishes diamond journals. </p><p><b>Luca:</b> You have played a role in organizations such as the EATCS, ERCIM and IFIP. Do you think that such societies still play a role today? How do think they should evolve to fulfill their mission? </p><p><b>Jos:</b> I do, but they have to keep evolving if they want to stay relevant. It is a pity that the EATCS could not take a role in the open access movement (you certainly made a very good effort!). I think ERCIM is doing ok, but not great. IFIP, I think, has become irrelevant. </p><p><b>Luca:</b> You have been a strong supporter of open access. What is your opinion of the state of play in open access and what do you think we could do to move forward? </p><p><b>Jos:</b> Progress is slow but exists. I am a strong advocate of Plan S, that has really set some things in motion. A key point is that it looks like we can achieve immediate green, and still allow all researchers to submit articles wherever they want. I do not like the way it is going with transformative agreements, you'd think that by now, publishers would be putting all their efforts to become sellers of data analytics services.</p><p><b>Acknowledgements:</b> I thank <a href="https://www7.in.tum.de/~esparza/" target="_blank">Javier Esparza</a>, the chair of the CONCUR Steering Committee, for contributing some the questions to Jos.  <br/></p></div>
    </content>
    <updated>2020-08-13T17:40:00Z</updated>
    <published>2020-08-13T17:40:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-08-13T17:40:13Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6597301440817356598</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6597301440817356598/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/08/simons-institute-gets-another-decade.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6597301440817356598" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6597301440817356598" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/08/simons-institute-gets-another-decade.html" rel="alternate" type="text/html"/>
    <title>Simons Institute Gets Another Decade</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> <a href="https://simons.berkeley.edu/news/simons-foundation-announces-new-355-million-grant-simons-institute-theory-computing-press">Great news</a> out of the Simons Institute.</p><blockquote><p>The Simons Foundation has ensured a second decade of research and innovation for the Simons Institute for the Theory of Computing, based at UC Berkeley, through a $35.5 million grant. The grant, which will begin in 2022, after the conclusion of the Simons Institute's first 10 years, will support the Simons Institute's mission and activities through June 2032.</p></blockquote><p>Congrats to Shafi Goldwasser and her team and of course a special thanks to Jim Simons and his foundation for their support for the theory community. </p><p>Time flies. I remember when I was on Team Chicago in the final rounds back in 2011. We lost but the theory community won as Berkeley did the institute well with amazing collaborative spaces, thanks mainly I hear from Alistair Sinclair, and the strong programs and workshops organized by the many volunteers across the theory community. </p><p>The institute started right when I started as a department chair so I never had the opportunity for the true Simons experience, joining for a semester-long program. When I did sneak away for a week at Simons I purposely avoided the workshops for Simons is at its best when strong researchers, connected by one of the programs, just talk, work and socialize together. I joined an amazing collection of complexity theorists to form a rather mediocre pub trivia team.</p><p>Even if you never make it there, the institute has a great <a href="https://www.youtube.com/user/SimonsInstitute">collection of videos</a> of its workshops, talks and celebrations. COVID-19 has driven Simons on-line but that just opens up their <a href="https://simons.berkeley.edu/workshops">workshops</a> and other events to a wider audience.</p><p>Congrats again to the Simons Institute for what it's given to the theory community and to its next dozen years with hopefully many more to follow!</p></div>
    </content>
    <updated>2020-08-13T14:10:00Z</updated>
    <published>2020-08-13T14:10:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-08-16T08:52:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/breeds/</id>
    <link href="https://gradientscience.org/breeds/" rel="alternate" type="text/html"/>
    <title>Benchmarks for Subpopulation Shift</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/2008.04859">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/BREEDS-Benchmarks">
<i class="fab fa-github"/>
   Code
</a>
<a class="bbutton" href="https://gradientscience.org/breeds_class_hierarchy">
<i class="fa fa-tree"/>
   Hierarchies
</a>
<br/></p>

<p><i>In our <a href="https://arxiv.org/abs/2008.04859">new paper</a>, we develop a
framework for simulating realistic subpopulation shifts between training
and deployment conditions for machine learning models. Evaluating
standard models on the resulting benchmarks reveals that these models
are highly sensitive to such shifts. Moreover, training models to be
invariant to existing families of synthetic data perturbations falls
short of remedying this issue.   </i></p>

<p>Consider what is probably the most prototypical classification task:
distinguishing between pictures of dogs and cats. By now, we have a fairly
established approach to tackling this task. It involves sourcing a set of
(labeled) cat and dog pictures and then training our model on that set.
Intuitively, we understand that for this approach to succeed our training set
has to be sufficiently large and diverse, capturing a variety of real-world
conditions. In particular, one might want to make sure that it contains
representatives of many of the possible breeds of dogs and cats. Is it critical
though that <i>all</i> breeds are represented? That is, if our classifier truly
learns how to distinguish dogs from cats, wouldn’t we expect it to generalize to
unseen breeds as well?</p>

<p>This is just an illustration of a broader challenge: building ML models that are
robust to <i>subpopulation shift</i>, i.e., they are able to generalize to data
subpopulations that were not encountered during training. This notion of
robustness is essential for models to perform reliably in the real world. After
all, we cannot expect our training set to capture all possible subpopulation
variations it can encounter during deployment. (Think of, e.g., differences in
weather/road conditions in the context of self-driving cars, or variability in
diagnostic equipment and patients’s exact characteristics in the
context of medical applications.)</p>

<h2 id="tackling-subpopulation-shifts">Tackling subpopulation shifts</h2>

<p>To make progress on this challenge, we need to first develop a principled way to
measure such robustness (or lack thereof). So, how can we measure the extent to
which our models are robust to such train-test subpopulation shifts?</p>

<p>A natural approach would be to first partition the subpopulations present in data for each class (e.g., dog or cat breeds) into two
disjoint sets: one set that is used for training the model, and one set that is held-out for
evaluation. Then, one can just examine the performance of the model on the held-out data.</p>

<p>This setup ensures that the actual task remains unchanged between
training and testing—i.e., classifying inputs into the original
classes (“dog” vs. “cat”). Therefore, the observed performance can give
us a sense how well that model will perform when exposed to novel data
subpopulations encountered during deployment.</p>

<p>The chief difficulty in implementing this strategy, however, is that
standard datasets do not contain annotations that are fine-grained
enough to identify the subpopulations of interest. Also, rectifying this
issue by collecting additional human annotations would be complicated
(not every class lends itself to identifying a natural and explicit
subpopulation structure) and costly (due to the scale of
state-of-the-art classification datasets).</p>

<p>In this post, we will outline an approach that circumvents some of the
aforementioned issues, allowing us to automatically construct a suite of
subpopulation shift benchmarks of varying difficulty, with minimal effort.</p>

<h2 id="the-breeds-methodology">The <span class="sc">Breeds</span> methodology</h2>

<p>Our approach is to <i>simulate</i> class subpopulations by grouping semantically
similar classes into <i>super</i>classes (e.g., aggregate the 100+ dog breed classes
in ImageNet into a “dog superclass”). Then, considering the classification task
over such superclasses enables us to repurpose the original dataset annotations
as explicit indicators of the subpopulation structure. Note that this new task
is entirely based on <em>existing</em> datasets, so we do not need to deal with,
gathering new data points or annotations, hence avoiding the additional biases
these might introduce (see our previous posts on
<a href="https://gradientscience.org/data_rep_bias">dataset replication</a> and
<a href="https://gradientscience.org/benchmarks">annotation pipelines</a>).</p>

<p>After this task restructuring, we can train our model on the resulting
superclass classification task while having full control over  which
subpopulations are included in training and which subpopulations are encountered
during testing. For example, if our original dataset contains classes: Labrador,
Husky, Tabby cat, and Persian cat, which we group into “dog” and “cat”
superclasses as [Labrador, Husky] and [Tabby cat, Persian cat], we can train a
model on the Labrador vs. Tabby cat task and test how well it can distinguish
Huskies from Persian cats. (Note that this framework can also naturally
incorporate milder subpopulation shifts where test subpopulations are
underrepresented, but not entirely missing, in the training set—see
<a href="https://arxiv.org/abs/1909.02060">here</a>.) Pictorially:</p>

<p><img class="bigimg" id="pipeline" src="https://gradientscience.org/assets/breeds/figures/pipeline.png"/></p>
<div class="footnote"> <strong>Overview of the <span class="sc">Breeds</span>
methodology.</strong> We group classes into superclasses and derive a
classification task over superclasses. Then we randomly assign each of the
original classes to be used for training (source) or evaluation (target) of the
model, thus inducing a subpopulation shift between training and testing
conditions. By controlling the granularity of the superclass grouping we can
construct shifts of varying severity.</div>

<p>The dog vs. cat breeds example served as the motivation for our
framework (as well as its name), but the underlying principle is quite
general. In fact, we can (and do) apply this methodology to group
together a wide variety of classes that share similar characteristics.</p>

<h3 id="obtaining-meaningful-superclasses">Obtaining meaningful superclasses</h3>

<p>In order for such a “breeds” benchmark be tractable from an image classification
viewpoint, we need to ensure that the classes we group together into
superclasses actually share visual characteristics. (After all, we cannot expect
a model to generalize across an arbitrary train-test partition of classes).
Ideally, we would group dataset classes based on a hierarchy that captures their
<i>visual</i> similarity—thus, allowing us to construct superclasses of varying
granularity (which would lead to benchmarks of varying difficulty).</p>

<div class="footnote">     
  <strong>Note:</strong> Having access to a class
hierarchy is a relatively mild requirement. Most of the popular vision datasets
are already equipped with such a hierarchy (e.g., CIFAR-100, ImageNet, PASCAL-
VOC and OpenImages). Even in cases where an explicit class hierarchy is missing,
it is more likely than not that the dataset classes obey some natural groupings
which could be recovered either manually, or with the assistance of proxies such
as word embeddings. 
</div>

<p>From this point of view, the ImageNet dataset is a natural candidate for
building our benchmarks—aside from its breadth and scale, it also comes
equipped with a class hierarchy, i.e., the <a href="https://wordnet.princeton.edu/">WordNet hierarchy</a>. However,
taking a closer look at that hierarchy reveals a few important shortcomings: (a)
classes are grouped together based on abstract (as opposed to visual)
characteristics (e.g., “umbrella” and “roof” are both “coverings); (b) nodes at
the same level of the hierarchy can have vastly different granularity (e.g.,
“street sign” and “living thing”), not necessarily mirroring the specificity of
the class itself; and (c) the hierarchy is not a tree (e.g., “bread” is in both
“starches” and “baked goods”). Grouping ImageNet classes based on this hierarchy
would thus result in poorly calibrated tasks.</p>

<p>All these shortcomings stem from the fact that WordNet is a <em>semantic</em> rather
than <em>visual</em> hierarchy—words are grouped together based on their meaning
rather than the visual appearance of the objects they correspond to. To remedy
this, we performed extensive edits to the existing ImageNet class hierarchy to
better capture visual object similarity. Specifically, we removed all abstract
nodes (e.g., “covering”) and calibrated the depth to ensure that nodes at the
same level have comparable granularity (by adding/removing redundant
nodes)—the modified hierarchy can be explored 
<a href="https://gradientscience.org/breeds_class_hierarchy">here</a> or through the
notebook we provide in the <a href="https://github.com/MadryLab/BREEDS-Benchmarks">code repo</a>.</p>

<h3 id="constructing-breeds-benchmarks">Constructing <span class="sc">Breeds</span> benchmarks</h3>

<p>Equipped with such a modified class hierarchy, we can construct subpopulation
shift benchmarks of varying difficulty in an essentially automated manner: we
simply choose a level of the hierarchy and treat each node at that level as a
superclass. The original dataset classes that are descendants of this node then
become the subclasses or “breeds”. These subclasses are then split
<em>randomly</em> into two groups—one of these is used to sample
training data points (<em>source domain</em>), and the other to sample test data points
(<em>target domain</em>).</p>

<div class="footnote">     
  <strong>Note:</strong> Using our methodology, one can also precisely
  control the extent to which the source and target domain subpopulations 
  differ—i.e., instead of random splits one 
  could make the splits more/less 
  adversarial)—see our paper for details.
</div>
<p>By varying the level in the hierarchy of the superclasses we
select, we can create an entire suite of benchmarks. For our analysis below, we
will use the following benchmarks.
(You can find more details in <a href="https://arxiv.org/abs/2008.04859">our paper</a> and
interactively browse the hierarchy
<a href="https://gradientscience.org/breeds_class_hierarchy">here</a>).</p>

<div>
  <div class="stages block">
      <div class="stage rbutton block clicked sc" id="entity13">Entity-13</div>
      <div class="stage rbutton block sc" id="entity30">Entity-30</div>
      <div class="stage rbutton block sc" id="living17">Living-17</div>
      <div class="stage rbutton block sc" id="nonliving26">NonLiving-26</div>
  </div>
  <div class="row" id="all_classes">
  </div>
  <div class="row">
    <div class="custom_column">
        <img class="bigimg" id="source_img" src="https://gradientscience.org/feed.xml"/>
    </div>
    <div class="custom_column">
        <img class="bigimg" id="target_img" src="https://gradientscience.org/feed.xml"/>
    </div>
  </div>
</div>

<div class="footnote"> <strong>Explore the <span class="sc">Breeds</span>
benchmarks.</strong> Select a dataset to see the set of superclasses it
contains, i.e., the classes with respect to which the classification task is
defined. Then, select any of these classes to see the subpopulations assigned
to the source and target domain, along with random images from each domain (you
need to scroll to the right to see all the classes).
</div>

<h3 id="validating-the-benchmarks-using-human-annotation">Validating the benchmarks using human annotation</h3>

<p>As already discussed, to ensure that the benchmarks we create are actually
meaningful, we need to validate that the superclasses therein contain visually
coherent subpopulations, so that good cross-subpopulation generalization is
possible. To this end, we leverage human annotators to measure the robustness of
humans to the subpopulation shifts encapsulated by (simplified versions of)
<span class="sc">Breeds</span> tasks. Specifically, we randomly pick pairs of superclasses and then show
annotators two groups of images corresponding to samples from the source domain
of the respective superclasses. Then, we present them with random images from
the target domain of <em>both</em> superclasses (mixed together) and ask them to assign
these images to one of the two groups (superclasses). Crucially, we do <em>not</em>
reveal to the annotators the name of either of the superclasses. Here is what
the task looks like:</p>

<div>
  <img class="bigimg" id="pipeline" src="https://gradientscience.org/assets/breeds/figures/task.png" width="80%"/>
</div>
<div class="footnote"> <strong>Illustration of subpopulation shift task presented 
to annotators.</strong> Annotators are shown random images from the source
domains of two random superclasses. Then, they are presented with images from
the target domains of these superclasses and asked to assign them into the
correct superclass.</div>

<p>If our superclasses are indeed well-calibrated for the task, annotators should
be able to associate new images with the correct group. That is, images from the
target domain of a superclass should be more similar to the source domain of the
<em>same</em> superclass, rather than the other one. Also, to establish a baseline, we
repeat the same experiment while asking the annotators to classify unseen
samples from the source domain (i.e., when there is no subpopulation shift).</p>

<p>So how well do our annotators perform?</p>

<div id="anno"> 
<canvas height="35%" id="bar" width="100%"/>
</div>

<div id="anno_model"> 
<canvas height="35%" id="bar_models" width="100%"/>
</div>

<div class="footnote"> <strong>Annotator performance on <span class="sc">Breeds</span> tasks.</strong> Annotators are quite robust to subpopulation shift.
Their ability to identify the correct superclass does not change significantly
between the source and the target domain, especially for the
<span class="sc">Living-17</span> and <span class="sc">Entity-30</span>
benchmarks. </div>

<p>As we can see, the difference in average annotator accuracy (in terms of
classifying unseen inputs into the correct group/superclass) between the source
and target domain is quite small. This suggests that the superclasses do indeed
correspond to visually meaningful object groupings, and, moreover, that humans
are quite robust to the distribution shifts captured within the <span class="sc">Breeds</span> tasks.
Let us now compare this drop to the one that our models incur.</p>

<h3 id="subpopulation-robustness-of-standard-models">Subpopulation robustness of standard models</h3>

<p>To assess whether standard models are sensitive to subpopulation shifts, we
will, for each <span class="sc">Breeds</span> task, train a number of standard architectures to
distinguish between the corresponding superclasses, using data from the source
domain and then measure model performance on data from the target domain.
Specifically, we will plot the <em>target accuracy</em> of each model as a function of
its <em>source accuracy</em>.</p>

<div id="std_full">
    <div class="rates block">
      <div class="std_stage rbutton block clicked sc" id="e13">Entity-13</div>
      <div class="std_stage rbutton block sc" id="e30">Entity-30</div>
      <div class="std_stage rbutton block sc" id="e17">Living-17</div>
      <div class="std_stage rbutton block sc" id="e26">NonLiving-26</div>
    </div>
  <div id="std_e13"> 
    <canvas height="40%" id="std_acc_e13" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">Entity-13</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    </div>
  </div>
  <div id="std_e30"> 
    <canvas height="40%" id="std_acc_e30" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">Entity-30</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    </div>
  </div>
  <div id="std_e17"> 
    <canvas height="40%" id="std_acc_l17" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">LIVING-17</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    </div>
  </div>
  <div id="std_e26"> 
  <canvas height="40%" id="std_acc_nl26" width="90%"/>
  <div class="footnote">
  <strong>Performance of standard models on the <span class="sc">NONLIVING-26</span> benchmark.
  </strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
  </div>
</div>
</div>

<div id="std_full_ft">
    <div class="rates_ft block">
      <div class="std_stage rbutton block clicked sc" id="e13f">Entity-13</div>
      <div class="std_stage rbutton block sc" id="e30f">Entity-30</div>
      <div class="std_stage rbutton block sc" id="e17f">Living-17</div>
      <div class="std_stage rbutton block sc" id="e26f">NonLiving-26</div>
    </div>
  <div id="ft_e13f"> 
    <canvas height="40%" id="ft_acc_e13" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">Entity-13</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    Re-training the last layer of the model on the target distribution
    significantly improves performance but does not recover the original model
    accuracy.
    </div>
  </div>
  <div id="ft_e30f"> 
    <canvas height="40%" id="ft_acc_e30" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">Entity-30</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    Re-training the last layer of the model on the target distribution
    significantly improves performance but does not recover the original model
    accuracy.
    </div>
  </div>
  <div id="ft_e17f"> 
    <canvas height="40%" id="ft_acc_l17" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">Living-17</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    Re-training the last layer of the model on the target distribution
    significantly improves performance but does not recover the original model
    accuracy.
    </div>
  </div>
  <div id="ft_e26f"> 
    <canvas height="40%" id="ft_acc_nl26" width="90%"/>
    <div class="footnote">
    <strong>Performance of standard models on the <span class="sc">NonLiving-26</span> benchmark.</strong> Models are quite brittle to subpopulation shifts, their accuracy drops by more than 25 percentage points across architectures.
    Re-training the last layer of the model on the target distribution
    significantly improves performance but does not recover the original model
    accuracy.
    </div>
  </div>
</div>

<p>Across the board, all models suffer a clear drop in performance under
subpopulation shifts—-accuracy drops by more than 30 percentage points between
the source and target domains. This indicates that the features models rely on
to perform well are somewhat specific to the subpopulations they encounter
during training, and thus tend to lose their predictive power even under
seemingly mild shifts in test-time subpopulations. 
<a href="https://gradientscience.org/feed.xml#anno_model" id="reveal_model">[CLICK THIS to compare model
performance to that of human annotators.]</a></p>

<p>A natural question to ask then is: can these models be adapted to the target
domain by simply re-training their last layer with data from this domain? 
<a href="https://gradientscience.org/feed.xml#std_ft" id="reveal_retrain">[CLICK THIS to add the re-training 
line.]</a> The answer seems to be nuanced. On one hand,
re-training significantly increases model performance, indicating that the
representations learned in the earlier layers of these (source-domain trained)
models are useful also for the subpopulations in the target domain. On the other
hand, these representations still fall short of what one would hope for—the
target accuracy post retraining remains much lower than that of a model trained
directly on the target domain.</p>

<h3 id="robustness-interventions">Robustness interventions</h3>

<p>Given that standard models turned out to be very sensitive to subpopulation
shifts, one might hope that applying a number of existing robustness
interventions—designed to increase model robustness to specific synthetic
perturbations—might change this state of affairs. To assess this, we
evaluate the subpopulation shift robustness of classifiers that have been
trained: (a) via <a href="https://gradientscience.org/robust_opt_pt1">robust optimization</a> against L2 adversaries of
different epsilon; (b) on a <a href="https://arxiv.org/abs/1811.12231">stylized version</a> of ImageNet (relying less on
texture and thus more on shape); and (c) with random noise (Gaussian or
<a href="https://arxiv.org/abs/1708.04896">Erase</a>).</p>

<p>To control for the significant impact on the standard (source) accuracy that
these methods can have, we focus on  the <em>relative target accuracy</em> of the
resulting models—that is the fraction of accuracy that is preserved in the
target domain (target accuracy over source accuracy).</p>

<div>
  <img class="bigimg" id="intervene" src="https://gradientscience.org/assets/breeds/figures/intervene.png"/>
</div>
<div class="footnote">
<strong>Effect of train-time interventions on subpopulation shift robustness.</strong>
We plot the ratio of the target accuracy to the source accuracy as a function of
the source accuracy. Train-time interventions have small, yet non-trivial,
impact on model robustness.
</div>

<p>We observe that these robustness interventions do have an impact on the target
accuracy of the model. Moreover, even when the resulting models have comparable
source accuracies, their target accuracies can be quite different. For instance,
erase noise improves robustness without particularly impacting source accuracy
(relative to standard models), while adversarially trained models improve
robustness at the expense of source accuracy. Overall, these results indicate
that while existing interventions make models slightly less sensitive to
subpopulation shifts, there is still significant room for improvement.</p>

<p>To obtain a more complete picture, we also measure the performance of the above
models after retraining the final layer with data from the target domain. Again,
our goal is to understand whether the employed training methods lead to
representations that are more general, and can be directly repurposed for the
target domain.</p>

<div>
  <img class="bigimg" id="intervene_ft" src="https://gradientscience.org/assets/breeds/figures/intervene_ft.png"/>
</div>
<div class="footnote">
<strong>Adapting model trained with robustness interventions for subpopulation shift.</strong> We plot model accuracy on the target domain after re-training the last layer on that domain. We can see that adversarially-trained model have significantly better accuracy, in certain cases almost recovering the original source accuracy.
</div>

<p>Interestingly, we see that models trained with erase noise are not better than
standard models after we allow for such fine-tuning. This indicates that the
gain in robustness it provides may not stem from learning better
representations. In contrast, adversarially trained models are significantly
better than other models (including standard ones) after fine-tuning. This is in
line with recent results showing that adversarially trained models are more
suited for transfer learning (<a href="https://arxiv.org/abs/2007.05869">here</a> and
<a href="https://arxiv.org/abs/2007.08489">here</a>). At
the same time, models trained on the stylized version of ImageNet are
consistently worse than other models, even relative to models with similar
source accuracy. This could indicate that texture is a particularly useful
feature for this classification task, especially in the presence of
subpopulation shift.</p>

<h3 id="conclusions">Conclusions</h3>
<p>In this post, we demonstrate how one can utilize a class
hierarchy to simulate a range of subpopulation shifts within existing
classification datasets. These shifts turn out to pose a challenge for standard
models (seemingly more so than for humans) and this challenge cannot really be
overcome using existing robustness interventions or re-training the last
layer. Thus, there is a need for designing new methods to address this core
robustness requirement. Overall, we believe that subpopulation shifts are an
important piece of the robustness puzzle and we hope that the benchmarks we
develop will serve as a guide for future progress on this front.</p></div>
    </summary>
    <updated>2020-08-12T00:00:00Z</updated>
    <published>2020-08-12T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2020-08-15T23:27:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/120</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/120" rel="alternate" type="text/html"/>
    <title>TR20-120 |  A Parallel Repetition Theorem for the GHZ Game | 

	Justin Holmgren, 

	Ran Raz</title>
    <summary>We prove that parallel repetition of the (3-player) GHZ game reduces the value of the game polynomially fast to 0. That is, the value of the GHZ game repeated in parallel $t$ times is at most $t^{-\Omega(1)}$. Previously, only a bound of $\approx \frac{1}{\alpha(t)}$, where $\alpha$ is the inverse Ackermann function, was known.

The GHZ game was recently identified by Dinur, Harsha, Venkat and Yuen as a multi-player game where all existing techniques for proving strong bounds on the value of the parallel repetition of the game fail. Indeed, to prove our result we use a completely new proof technique. Dinur, Harsha, Venkat and Yuen speculated that progress on bounding the value of the parallel repetition of the GHZ game may lead to further progress on the general question of parallel repetition of multi-player games. They suggested that the strong correlations present in the GHZ question distribution represent the ``hardest instance'' of the multi-player parallel repetition problem.

Another motivation for studying the parallel repetition of the GHZ game comes from the field of quantum information. The GHZ game, first introduced by Greenberger, Horne and Zeilinger, is a central game in the study of quantum entanglement and has been studied in numerous works. For example, it is used for testing quantum entanglement and for device-independent quantum cryptography. In such applications a game is typically repeated to reduce the probability of error, and hence bounds on the value of the parallel repetition of the game may be useful.</summary>
    <updated>2020-08-11T23:02:11Z</updated>
    <published>2020-08-11T23:02:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-16T16:20:29Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1361848584634042636</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1361848584634042636/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/08/random-thoughts-on-pandemic.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1361848584634042636" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1361848584634042636" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/08/random-thoughts-on-pandemic.html" rel="alternate" type="text/html"/>
    <title>Random Thoughts on the Pandemic</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> </p><p>1) Contrast the following two points and, if you have an intelligent way to fill-in-the-blank for the second one, please comment.</p><p>a) When Trump says `open the schools or I will cut of funding' I disagree, or at least he should talk more about how to do it carefully BUT there is an underlying important and serious issue: how to balance health and education. Similar for when he talks about opening up business's - how to balance heath and the economy.</p><p>b) When Trump says `hydroxychloroquine is a potential cure for covid' I (and most of the medical community) disagree, BUT FILL IN THE BLANK. Is there SOME serious issue involved here that I am just missing?</p><p>2) I have seen several approaches to large gathering, say Churchs, beach parties, motorcycle meeting etc. </p><p>a) DO IT ONLINE.  UMCP classes will be on-line in the fall. My church has been online since mid March and seems like it will do the same in the fall. (Note- Choir is much worse than normal talking for spreading it, so Choir might not resume for a much longer time than Church).</p><p>b) HAVE THEM but SOCIAL DISTANCE and MASKS and other precautions. And it works. I hope it works.</p><p>c) HAVE THEM, realize that there IS an issue, TRY to do some of the precautions, but fail. This may be what has happened at some high schools, see <a href="https://www.thedailybeast.com/9-people-test-positive-at-georgia-school-with-photo-of-crowded-hallway?ref=home">Here</a>.</p><p>d) HAVE THEM and ignore ALL precautions either because </p><p>i) God will protect you</p><p>ii) The Government is not my boss. Such people are usually pro-business so they should NOT mind if a business on its own requires masks and SHOULD mind if its the government. I have not seen them making that distinction. Such people are usually against Federalism and for localism. So they should be upset when a Prez or a Gov overides a Mayor's Mask Mandate. I have not seen this.</p><p>iii) The whole think is a Hoax. How can people still believe this?</p><p>These three may overlap.</p><p>3) The Sturgis Motorcycle Rally in South Dakota seems to believe ii and iii. Can someone give any rational reason why they want to have this rally and why its allowed to happen. Given how many people come (usually 250,000- less now?) from all across the country, this could make thinks far far worse. But see next point.</p><p>4) Libertarians think that there is too much Government in our lives. This is a fair point of view but it needs to be tested on a case by case basis. There are some things that REQUIRE a more national response. When this happens Libertarians have two choices:</p><p>a) Apply their thinking to figure out how Gov can help with the least damage. For example, Carbon tax for global warning. In the current pandemic they might have LOCAL govs pass mandatory mask laws.. Or they would at least set a good example by wearing masks themselves. Perhaps relax regulations on medical stuff so that companies can work together without worrying about anti-trust, and perhaps look more carefully at regulations that get in the way (might be a good idea anyway).</p><p>b) Deny its a problem. Man made Global Warming is a myth. Covid is a hoax. Hence its okay to have the Motorcycle Rally. But the problem is, this doesn't just harm the people showing up, it harms others as well.</p><p>5) Early on I was puzzled that Trump didn't care more since older people (his base!) are at more risk. Did he think it would only affect democrats (early on NY was hit badly). Why didn't his advisors tell him that it would kill his own base?I ASK NON-RHETORICALLY  Or did they and he didn't list. My point is, for purely selfish reasons Trump should have taken more action earlier, so I am surprised he did not. See next point.</p><p>6) Trump could have even taken action in a Trumpian way:</p><p>The Chinese and the Democrats are waging a bio-war on real Americans (white rural older) !!! We will STOP them in their tracks and show them they can't mess with us!!  I will LOCKDOWN some cities AND provide online stuff to schools, business's (my cronies)  and churchs (SCREW that stupid sep of church and state- that was only one passage in a letter Thomas Jefferson wrote).  </p><p>He might have even got Democrats to complain that it will choke the economy and hurt the poor, and that giving stuff to churchs is against the constitution (it prob is). </p><p>7) If a vax for covid is found, will Trump urge his base (some of whom are anti-vax) to take it? Having said that, if it comes out in October it may be untested so I doubt I will take it. I'll wait until Dr Fauci says its okay to take it. (See <a href="https://www.youtube.com/watch?v=lUiDLcp_hIw">here</a> for a nice song about Dr. Fauci.)</p><p>8) Some people who thought it was a Hoax and then got a bad case of it are saying loudly `I WAS WRONG! Its Real!'  I wonder if Herman Cain realized it was real before he died of it? I wish he had said `I WAS WRONG! ITS REAL! Wear a MASK' (The Whitehouse denies that he got Covid from the Tulsa Rally, here is headline only since something is behind paywalls: <a href="https://tulsaworld.com/news/local/white-house-denies-herman-cain-contracted-covid-19-at-tulsa-trump-rally-prior-to-his/article_b0c83d3c-c3de-547e-9a5f-434b6a82ac61.html">here</a>)</p><p>9) At Tulsa the presidents people took DOWN signs about safe distancing. So again, why does the Prez want to kill his own base? I ask non rhetorically. I am serious- I want someone to defend his actions intelligently both this one and the hydro-whatever in the first point.</p><p>10) Authoritarian governments often claim that they bring order and can act fast to get things done. So they SHOULD have an advantage during the pandemic since they can more easily ORDER a lockdown. Yet they don't seem to be doing better.  </p><p>Russia: <a href="https://www.worldometers.info/coronavirus/country/russia/">See Here</a>, number of cases going up fast.</p><p>Iran: <a href="https://www.worldometers.info/coronavirus/country/iran/">See here</a> and <a href="https://www.bbc.com/news/world-middle-east-53598965">Here</a>, number of cases going up fast.</p><p>These countries and others, and America, had their leaders LIE about the extend of the virus early on. What is the upside of doing this?  I ask non-rhetorically. </p><p><br/></p></div>
    </content>
    <updated>2020-08-10T01:38:00Z</updated>
    <published>2020-08-10T01:38:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-08-16T08:52:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/119</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/119" rel="alternate" type="text/html"/>
    <title>TR20-119 |  On parity decision trees for Fourier-sparse Boolean functions | 

	Nikhil Mande, 

	Swagato Sanyal</title>
    <summary>We study parity decision trees for Boolean functions. The motivation of our study is the log-rank conjecture for XOR functions and its connection to Fourier analysis and parity decision tree complexity. Our contributions are as follows. Let $f : \mathbb{F}_2^n \to \{-1, 1\}$ be a Boolean function with Fourier support $\mathcal{S}$ and Fourier sparsity $k$.

1) We prove via the probabilistic method that there exists a parity decision tree of depth $O(\sqrt{k})$ that computes $f$. This matches the best known upper bound on the parity decision tree complexity of Boolean functions (Tsang, Wong, Xie, and Zhang, FOCS 2013). Moreover, while previous constructions (Tsang et al., FOCS 2013, Shpilka, Tal, and Volk, Comput. Complex. 2017) build the trees by carefully choosing the parities to be queried in each step, our proof shows that a naive sampling of the parities suffices.

2) We generalize the above result by showing that if the Fourier spectra of Boolean functions satisfy a natural "folding property", then the above proof can be adapted to establish existence of a tree of complexity polynomially smaller than $O(\sqrt k)$. More concretely, the folding property we consider is that for most distinct $\gamma, \delta$ in $\mathcal{S}$, there are at least a polynomial (in $k$) number of pairs $(\alpha, \beta)$ of parities in $\mathcal{S}$ such that $\alpha+\beta=\gamma+\delta$. We make a conjecture in this regard which, if true, implies that the communication complexity of an XOR function is bounded above by the fourth root of the rank of its communication matrix, improving upon the previously known upper bound of square root of rank (Tsang et al., FOCS 2013, Lovett, J. ACM. 2016).

3) Motivated by the above, we present some structural results about the Fourier spectra of Boolean functions. It can be shown by elementary techniques that for any Boolean function $f$ and all $(\alpha, \beta)$ in ${\mathcal{S} \choose 2}$, there exists another pair $(\gamma, \delta)$ in ${\mathcal{S} \choose 2}$ such that $\alpha + \beta = \gamma + \delta$. One can view this as a "trivial" folding property that all Boolean functions satisfy. Prior to our work, it was conceivable that for all $(\alpha, \beta) \in {\mathcal{S} \choose 2}$, there exists exactly one other pair $(\gamma, \delta) \in {\mathcal{S} \choose 2}$ with $\alpha + \beta = \gamma + \delta$. We show, among other results, that there must exist several $\gamma \in \mathbb{F}_2^n$ such that there are at least three pairs of parities $(\alpha_1, \alpha_2) \in {\mathcal{S} \choose 2}$ with $\alpha_1+\alpha_2=\gamma$. This, in particular, rules out the possibility stated earlier.</summary>
    <updated>2020-08-09T16:45:08Z</updated>
    <published>2020-08-09T16:45:08Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-16T16:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4933</id>
    <link href="https://www.scottaaronson.com/blog/?p=4933" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4933#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4933" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Seven announcements</title>
    <summary xml:lang="en-US">Good news, everyone! Following years of requests, this blog finally supports rich HTML and basic TeX in comments. Also, the German spam that used to plague the blog (when JavaScript was disabled) is gone. For all this, I owe deep gratitude to a reader and volunteer named Filip Dimitrovski. Filip refused to accept any payment […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><ol><li>Good news, everyone!  Following years of requests, this blog finally supports rich HTML and basic TeX in comments.  Also, the German spam that used to plague the blog (when JavaScript was disabled) is gone.  For all this, I owe deep gratitude to a reader and volunteer named <a href="https://filiparena.github.io/">Filip Dimitrovski</a>.<br/></li><li>Filip refused to accept any payment for fixing this blog.  Instead, he asked only one favor: namely, that I use my platform to raise public awareness about the plight of the MAOI antidepressant <a href="https://en.wikipedia.org/wiki/Phenelzine">Nardil</a>.  Filip tells me that, while tens of thousands of people desperately need Nardil—no other antidepressant ever worked for them—it’s become increasingly unavailable because the pharma companies can no longer make money on it.  He points me to a <a href="https://slatestarcodex.com/2015/04/30/prescriptions-paradoxes-and-perversities/">SlateStarCodex post from 2015</a> that explains the problem in more detail (anyone else miss SlateStarCodex?).  More recent links about the worsening crisis <a href="https://www.sps.nhs.uk/articles/shortage-of-phenelzine-15mg-tablets-nardil/">here</a>, <a href="https://healthycanadians.gc.ca/recall-alert-rappel-avis/hc-sc/2020/73411a-eng.php">here</a>, and <a href="https://www.tga.gov.au/sites/default/files/phenelzine-nardil-discontinuation.pdf">here</a>.<br/></li><li>Here’s a <a href="https://www.wired.com/story/bill-gates-on-covid-most-us-tests-are-completely-garbage/">fantastic interview of Bill Gates by Steven Levy</a>, about the coronavirus debacle in the US.  Gates, who’s always been notoriously and strategically nonpartisan, is more explicit than I’ve ever seen him before in explaining how the Trump administration’s world-historic incompetence led to where we are.<br/></li><li>Speaking of which, here’s <a href="https://www.the-american-interest.com/2020/08/06/getting-from-november-to-january/">another excellent article</a>, this one in <em>The American Interest</em>, about the results of “wargames” trying to simulate what happens in the extremely likely event that Trump contests a loss of the November election.  Notably, the article sets out six steps that could be taken over the next few months to decrease the chance of a crisis next to which <em>all the previous crises of 2020 will pale</em>.<br/></li><li>A reader asked me to share a link to an <a href="https://algorithmcompetition.github.io/doc/contest/overview.html">algorithm competition</a>, related to cryptographic “proofs of time,” that ends on August 31.  Apparently, my having shared a link to a predecessor of this competition—at the request of friend-of-the-blog Bram Cohen—played a big role in attracting good entries.<br/></li><li>Huge congratulations to my former PhD student Shalev Ben-David, as well as Eric Blais, for <a href="https://focs2020.cs.duke.edu/index.php/awards/">co-winning the FOCS’2020 Best Paper Award</a>—along with two other papers—for <a href="https://arxiv.org/abs/2002.10802">highly unconventional work</a> about a new minimax theorem for randomized algorithms.  (Ben-David and Blais also have a <a href="https://arxiv.org/abs/2002.10809">second FOCS paper</a>, which applies their award paper to get the first tight composition theorem for randomized query complexity.  Here’s the <a href="https://focs2020.cs.duke.edu/index.php/accepted-papers/">full list</a> of FOCS papers—lots of great stuff, for a conference that of course won’t physically convene!)  Anyway, a central idea in Ben-David and Blais’s new work is to use <a href="https://en.wikipedia.org/wiki/Scoring_rule">proper scoring rules</a> to measure the performance of randomized algorithms—algorithms that now make statements like “I’m 90% sure that this is a yes-input,” rather than just outputting a 1-bit guess.  Notably, Shalev tells me that he learned about proper scoring rules by <em>reading rationalist blogs</em>.  So next time you lament your untold hours sacrificed to that pastime, remind yourself of where it once led!<br/></li><li>What have I been up to lately?  Besides Busy Beaver, hanging out with my kids, and just trying to survive?  Mostly giving a lot of Zoom lectures!  For those interested, <a href="https://m.youtube.com/watch?v=n2nnY_xLCN4">here’s a Q&amp;A that I recently did</a> on the past and present of quantum computing, hosted by Andris Ambainis in Latvia.  It did feel a bit surreal when my “interviewer” asked me to explain how I got into quantum computing research, and my answer was basically: “well, <em>as you know</em>, Andris, a lot of it started when I got hold of <a href="https://arxiv.org/abs/quant-ph/0002066">your seminal paper</a> back in 1999…”</li></ol>



<p/></div>
    </content>
    <updated>2020-08-09T08:36:47Z</updated>
    <published>2020-08-09T08:36:47Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Procrastination"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-08-13T17:04:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17394</id>
    <link href="https://rjlipton.wordpress.com/2020/08/08/fran-allen-1932-2020/" rel="alternate" type="text/html"/>
    <title>Fran Allen: 1932-2020</title>
    <summary>We lost a great computer scientist. Frances Allen was one of the leaders who helped create the field of compilers research. Fran was an elite researcher at IBM, and won a Turing Award for this pioneering work. Allen also collected other awards. Perhaps the coolest award is one that Fran could never win: The Frances […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>We lost a great computer scientist.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p>
Frances Allen was one of the leaders who helped create the field of <a href="https://en.wikipedia.org/wiki/Compiler">compilers</a> research. Fran was an elite researcher at IBM, and won a Turing Award for this pioneering work. Allen also collected other awards. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/08/08/fran-allen-1932-2020/fran/" rel="attachment wp-att-17396"><img alt="" class="alignright size-medium wp-image-17396" height="180" src="https://rjlipton.files.wordpress.com/2020/08/fran.png?w=300&amp;h=180" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>Perhaps the coolest award is one that Fran could never win: The <a href="https://www.ieee.org/about/awards/medals/frances-e-allen-medal.html">Frances E. Allen</a> award created this year by the IEEE: </p>
<blockquote><p><b> </b> <em> <i>For innovative work in computing leading to lasting impact on other fields of engineering, technology, or science.</i> </em>
</p></blockquote>
<p/><p>
Today we will talk about Fran, who sadly just passed away.</p>
<p>
I consider Fran a friend, although we never worked together—our areas of interest were different. One fond memory of mine is being on panel a while ago with Fran. What a delightful presence.</p>
<p>
Fran always seemed to be smiling, always with that smile. The following images come in large part from interviews and lectures and award events-the fact that it is so easy to find them is a testament to her public engagement as well as scientific contributions.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/08/08/fran-allen-1932-2020/collage/" rel="attachment wp-att-17398"><img alt="" class="aligncenter size-medium wp-image-17398" height="290" src="https://rjlipton.files.wordpress.com/2020/08/collage.png?w=300&amp;h=290" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p/><h2> Compliers were Key </h2><p/>
<p/><p>
There was a time when compliers were the most important program available on any new computer. Perhaps on any computer. Here is proof: </p>
<ol>
<li>
Computers are there to solve problems. <p/>
</li><li>
We must write programs to solve problems. <p/>
</li><li>
Details of the computer and instructions, are complicated, which make writing programs hard. <p/>
</li><li>
Thus, automating the writing of programs is important. <p/>
</li><li>
QED: we must have compliers.
</li></ol>
<p>
Okay this is not really a “proof”, but there is some truth to the argument. Fran was at IBM and worked on some of the early compliers, including FORTRAN and related languages. IBM wanted to sell computers, well actually in the early days rent them. One potential roadblock, IBM realized, was that new computers could be hard to program. Thus to ensure that companies rented new machines as fast as IBM could manufacture them was important. This created the need for compliers and even more for <i>optimizing compilers</i>.</p>
<p>
In order to ship more machines, the code that a complier created had to be efficient. Hence, a stress on Allen was to figure out how compliers could generate high quality code. This led Fran and others like John Cocke to discover many complier techniques that are still used today. A short list of the ideas is: </p>
<ul>
<li>
Branch prediction <p/>
</li><li>
Register allocation <p/>
</li><li>
Control flow graphs <p/>
</li><li>
Program dependence graphs <p/>
</li><li>
And many more.
</li></ul>
<p>
What is so important is that Allen’s work was not just applicable to this machine or that language. Rather the work was able to be used for almost any machine and for almost any language. This universal nature of the work on compliers reminds me of what we try to do in theory. Allen’s research was so important because it could be used for future hardware as well as future languages. </p>
<p>
</p><p/><h2> Register Allocation </h2><p/>
<p/><p>
Guy Steele interviewed Allen for the ACM <a href="https://cacm.acm.org/magazines/2011/1/103191-an-interview-with-frances-e-allen/fulltext">here</a>. During the interview Fran talked about register allocation:</p>
<blockquote><p><b> </b> <em> I have a story about register allocation. FORTRAN back in the 1950’s had the beginnings of a theory of register allocation, even though there were only three registers on the target machine. Quite a bit later, John Backus became interested in applying graph coloring to allocating registers; he worked for about 10 years on that problem and just couldn’t solve it. I considered it the biggest outstanding problem in optimizing compilers for a long time. Optimizing transformations would produce code with symbolic registers; the issue was then to map symbolic registers to real machine registers, of which there was a limited set. For high-performance computing, register allocation often conflicts with instruction scheduling. There wasn’t a good algorithm until the Chaitin algorithm. Chaitin was working on the PL compiler for the 801 system. Ashok Chandra, another student of Knuth’s, joined the department and told about how he had worked on the graph coloring problem, which Knuth had given out in class, and had solved it not by solving the coloring problem directly, but in terms of what is the minimal number of colors needed to color the graph. Greg immediately recognized that he could apply this solution to the register allocator issue. It was a wonderful kind of serendipity. </em>
</p></blockquote>
<p>
</p><p/><h2> Compliers Create Theory </h2><p/>
<p/><p>
The early goal of creating compliers lead directly to some wonderful theory problems. One whole area that dominated early theory research was language theory. In particular understanding questions that arise in defining programming languages. Syntax came first—later semantics was formalized.</p>
<p>
Noam Chomsky created context-free grammars to help understand natural languages in the 1950s. His ideas were used by John Backus, also a Turing award winner from IBM, to describe the then new programming language IAL. This is known today as ALGOL 58, which became ALGOL 60. Peter Naur on the ALGOL 60 committee called Backus’s notation for ALGOL’s syntax: Backus normal form, but is now called BNF-Backus-Naur form. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/08/08/fran-allen-1932-2020/bnf-2/" rel="attachment wp-att-17399"><img alt="" class="aligncenter size-medium wp-image-17399" height="39" src="https://rjlipton.files.wordpress.com/2020/08/bnf.png?w=300&amp;h=39" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
Theorists worked on, I confess I did, many questions about such languages. Existence problems, decidability problems, efficient algorithms, and closure properties were just some of the examples. Not clear how much of this theory effected compiler design, but I would like to think that some was useful. Theorist should thank the complier researchers. I do. </p>
<p>
For instance the 1970 STOC <a href="https://dblp.org/db/conf/stoc/stoc70">program</a> had many papers on language related topics—here are some:</p>
<ul>
<li>
A Result on the Relationship between Simple Precedence Languages and Reducing Transition Languages. 		Gary Lindstrom <p/>
</li><li>
The Design of Parsers for Incremental Language Processors: 		Ronald Book, Sheila Greibach, Ben Wegbreit <p/>
</li><li>
Tape- and Time-Bounded Turing Acceptors and AFLs. 		David Lewis <p/>
</li><li>
Closure of Families of Languages under Substitution Operators. 		William Rounds <p/>
</li><li>
Tree-Oriented Proofs of Some Theorems on Context-Free and Indexed Languages. 		Barry Rosen <p/>
</li><li>
On Syntax-Directed Transduction and Tree Transducers. 		Alfred Aho, Jeffrey Ullman <p/>
</li><li>
The Analysis of Two-Dimensional Patterns using Picture Processing Grammars. 		Jean-Francois Perrot <p/>
</li><li>
On Some Families of Languages Related to the Dyck Language. 		Joseph Ullian <p/>
</li><li>
Three Theorems on Abstract Families of Languages. 	Joseph Ullian
</li></ul>
<p>
By the way Abstract Families of Languages or <a href="https://en.wikipedia.org/wiki/Abstract_family_of_languages">AFL</a>s was created by Seymour Ginsburg and Sheila Greibach in 1967 as a way to generalize context free languages. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Fran was asked by Steele in that <a href="https://cacm.acm.org/magazines/2011/1/103191-an-interview-with-frances-e-allen/fulltext">interview</a>: <i>Any advice for the future?</i></p>
<blockquote><p><b> </b> <em> Yes, I do have one thing. Students aren’t joining our field, computer science, and I don’t know why. It’s just such an amazing field, and it’s changed the world, and we’re just at the beginning of the change. We have to find a way to get our excitement out to be more publicly visible. It is exciting, in the 50 years that I’ve been involved, the change has been astounding. </em>
</p></blockquote>
<p/><p>
Thanks Fran. Much of that change is due to pioneers like you. Thanks for everything.</p>
<p/></font></font></div>
    </content>
    <updated>2020-08-08T14:40:43Z</updated>
    <published>2020-08-08T14:40:43Z</published>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="code"/>
    <category term="complier"/>
    <category term="optimization"/>
    <category term="register"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-08-16T16:20:43Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/08/07/report-from-cccg</id>
    <link href="https://11011110.github.io/blog/2020/08/07/report-from-cccg.html" rel="alternate" type="text/html"/>
    <title>Report from CCCG</title>
    <summary>I spent the last few days participating in the Canadian Conference in Computational Geometry, originally planned for Saskatoon but organized virtually instead.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I spent the last few days participating in the <a href="http://vga.usask.ca/cccg2020/">Canadian Conference in Computational Geometry</a>, originally planned for Saskatoon but organized virtually instead.</p>

<p>The way the conference was organized was that (after the usual submission reviewing process) the accepted authors provided both a proceedings paper and a 10-15 minute talk video to the conference organizers. Participants were required to register, but with no registration fee, and were provided with links to the papers and talks (which are all still live on the conference program). Then, during the conference itself, live online Zoom sessions ran for only 2-3 hours daily, scheduled for 10AM-1PM Saskatoon time: very convenient for anywhere in North America or Europe, not so much for the participants in Iran, India, China, Japan, and Korea (all of which did have participants). The sessions included a daily 1-hour live invited talk, and question-and-answer sessions for the contributed works, in which we were shown a one-minute teaser for each video and then invited to ask questions of authors, at least one of whom was required to be present.</p>

<p>I think it all worked very well; so well, in fact, that during the business meeting there were calls for having at least some of the content similarly online so that people could participate remotely again. The ability to ask and answer questions either by live video on Zoom or through Zoom chat was useful, and used. Attendance was far above previous levels: 162 registrants, and over 120 unique participants at the most heavily attended of the two daily parallel sessions, compared to roughly 60 attendees each at the last two physical conferences. Despite the free registration, the conference organization was not without cost: they spent roughly $1500 (Canadian) in video production costs and video conferencing fees, but this was more than made up for by institutional support for the conference, so they ended up running a surplus which may (if it can be kept) end up providing some float for future conference organizers.</p>

<p>There are two changes I would suggest for future events of this type:</p>

<ul>
  <li>
    <p>The contributed sessions were for a short enough time that holding them in parallel seemed unnecessary, and made it impossible to participate in all discussions. So this format may work better with less parallelism.</p>
  </li>
  <li>
    <p>The one-minute teaser videos were cut together by the conference organizers from the longer videos provided by the authors, but in some cases the pacing of the longer videos from which they were cut meant that these teaser videos could not clearly state the results of their papers. I think it would have been better to ask authors to provide these alongside the longer talk videos.</p>
  </li>
</ul>

<p>Some of the highlights of the event:</p>

<ul>
  <li>
    <p>Wednesday’s invited talk by Erik Demaine was a moving tribute to Godfried Toussaint and a survey of some of both Toussaint’s research, and Erik’s research on problems started by Toussaint, including <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Nagy_theorem">convexifying polygons by flips</a>, sona curves (<a href="https://arxiv.org/abs/2007.15784">the subject of one of my own contributions</a>), the <a href="https://en.wikipedia.org/wiki/The_Geometry_of_Musical_Rhythm">geometry of musical rhythms</a>, and perhaps most importantly “supercollaboration”, the model of shared research and shared authorship developed by Toussaint at the Barbados research workshops and also used by Erik within many of his MIT classes. <a href="https://www.youtube.com/watch?v=exzxGODi2YU">Erik’s talk was recorded and is now on YouTube</a>; I hope the same will be true of the other two invited talks.</p>
  </li>
  <li>
    <p>In Wednesday’s contributed session on unfolding (in which I had two papers) I particularly liked Satyan Devadoss’s talk, “Nets of higher-dimensional cubes”. The main result is that if you unfold a hypercube, then no path of facets of the unfolded shape can contain a u-turn: if the path takes a step in one any coordinate direction, it cannot step in the opposite direction. This implies that all dual spanning trees unfold flat without self-intersection. The same property was known for all the Platonic solids, and Satyan can prove it for regular simplices in any dimension, but that still leaves several regular polytopes for which it is still open whether all unfoldings work: the cross-polytopes in all dimensions, and the three exceptional regular polytopes in four dimensions.</p>
  </li>
  <li>
    <p>Thursday’s invited talk was by Jeff Erickson, “Chasing puppies”. It was an entertaining presentation of an elegant topological proof of the following result: if you and a puppy can both move around a simple closed curve in the plane, with the puppy always moving along the curve to a local minimum of distance to you, then you can always find a path to follow that will bring you and the puppy to the same point.</p>
  </li>
  <li>
    <p>There wasn’t an official prize for best contributed presentation, but in the data structures session on Thursday, several comments nominated <a href="https://medium.com/photos-we-love/the-trinity-of-incongruity-or-why-i-still-love-this-tuxedo-sewing-machine-ups-truck-photograph-39712dd32fcb">Don Sheehy</a> as the unofficial winner, for a video artfully mixing live action with computer animations. His paper, “One-hop greedy permutations” concerned heuristics for improving the <a href="https://en.wikipedia.org/wiki/Farthest-first_traversal">farthest-first traversal</a> of a set of points by looking near each point as the sequence is constructed for a better point to use instead.</p>
  </li>
  <li>
    <p>There was an official best student paper award, and it went to <a href="https://www.cs.umd.edu/people/afloresv">Alejandro Flores Velazco</a> for his paper “Social distancing is good for points too!” It concerns the problem of reducing the size of a data set while preserving the quality of nearest-neighbor classification using the reduced set. It proves that FCNN, which it calls the most popular heuristic for this problem, can produce significantly less-reduced outputs than some other proven heuristics, and shows how to modify FCNN to get a heuristic with guaranteed output quality.</p>
  </li>
  <li>
    <p>The third of the invited sessions was by Yusu Wang, newly moved from Ohio State to UC San Diego. She gave a nice introduction to combinatorial methods for reconstructing road networks (or other networks embedded into higher-dimensional geometry) from noisy samples of points on the network by combining discrete Morse theory to find the ridge lines of sample density with persistent homology to clean some of the noise from the data.</p>
  </li>
  <li>
    <p>The final technical component of the conference was an open problem session, also recorded and presumably to be uploaded at some point. Satyan posed his question on regular polytope unfolding there. Mike Paterson asked whether one can construct “Plato’s torus”, an embedded torus with six equilateral-triangle faces meeting at each vertex; <a href="https://11011110.github.io/blog/2009/02/03/flat-equilateral-tori.html">in a blog post I made on this problem in 2009 I traced its history to Nick Halloway in 1997</a> but Mike says he discussed it already with Christopher Zeeman in the 1970s. Another problem that caught my attention asked for an algorithmic version of the polyhedral <a href="https://en.wikipedia.org/wiki/Theorem_of_the_three_geodesics">theorem of the three geodesics</a>, the existence of a path across the surface of a convex polyhedron that stays straight across each edge or face of the polyhedron, and has at most \(\pi\) surface angle on each side of it when it passes through a vertex. Again there’s some history here: Joe O’Rourke says he once mentioned the problem to Gromov, who said it was easy but unfortunately didn’t elaborate.</p>
  </li>
</ul>

<p>CCCG 2021 is planned for Halifax, colocated with WADS. One somewhat controversial issue is that the current plan is to have both conferences overlap for two days, with one overlap-free day for each conference at each end of the overlap period. But if both conferences are double-session, this means that participants can only choose one of four overlapping talks. At this point everyone is still hoping that events allow for a physical conference by then but that remains to be seen.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104651090140382005">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-08-07T17:46:00Z</updated>
    <published>2020-08-07T17:46:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-08-16T00:02:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4290135672537490640</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4290135672537490640/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/08/do-senators-have-advantage-for-being.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4290135672537490640" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4290135672537490640" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/08/do-senators-have-advantage-for-being.html" rel="alternate" type="text/html"/>
    <title>Do Senators have an Advantage for being Dem VP Nominee?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is a non-partisan post. Even so:  I plan to vote for Joe Biden.<div><br/></div><div>(Lance says that whenever I write `this is a non-partisan post'  its partisan anyway.)</div><div><br/></div><div>I've had posts on predicting VPs before:</div><div><br/></div><div>In <a href="https://blog.computationalcomplexity.org/2019/02/using-who-will-be-dem-vp-choice-article.html">this post</a> I looked at a Nate Silver column a few months ago where they were trying to predict the democratic VP nomination <i>before the Prez nominee was know</i>n. Some of what they said seems relevant.</div><div><br/></div><div>In <a href="https://blog.computationalcomplexity.org/2008/09/i-would-be-on-intrade-that-intrade-will.html">this post</a> I PREDICTED that bets on INTRADE would not do well on picking VPs since VP picks are hard to predict and often are not from anyone's short list. I DID NOT PREDICT that INTRADE would go out of business.</div><div><br/></div><div>I came across a statistic recently that seems relevant and inspired this post. Actually this post asks IS this statistic relevant?</div><div><br/></div><div>From Prez candidate  Harry Truman to Prez candidate Hillary Clinton there have been 15 Dem VP nominees (not  counting incumbents) of which 13 have been Senators:</div><div><br/></div><div>Harry Truman(Prez)-Alben Barkely. Sen from Kentucky</div><div>Adlai Stevenson(Gov-Illinois) -John Sparkman. Sen from Alabama</div><div>Adlai Stevenson(Gov-Illinois)-Estes Kefauver, Sen from Tennessee </div><div>John F Kennedy(Sen-Mass)-Lyndon B Johnson, Sen from Texas</div><div>Lyndon B Johnson(Prez)-Hubert Humphrey,Sen from Minnesoda</div><div>Hubert Humphrey(VP)-Ed Muskie, Sen from Maine</div><div>George McGovern Sen-South Dakota)-Sgt Shriver Amb to France, Office of Econ Activity</div><div>Jimmy Carter(Gov Georgia)-Walter Mondale- Senator from Minnesota</div><div>Walter Mondale(Senator Minnesota)-Geraldine Ferraro Congressperson- NY</div><div>Mike Dukakis (Gov Mass), Lloyd Benson-Sen Texas</div><div>Bill Clinton(Gov Arkansas), Al Gore Sen from Tennesee</div><div>Al Gore (VP), Joe Lieberman Senator from Conn</div><div>John Kerry (Sen-Mass), John Edwards Sen from NC</div><div>Barack Obama (Sen-Illinois), Joe Biden Sen from Del</div><div>Hillary Clinton (Sen-NY), Tim Kaine- Sen Virginia</div><div><br/></div><div>Of the 15 Dem VP nominees, 13 are Senators</div><div><br/></div><div>Of the 13 Rep VP nominees, 4 are Senators  <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/rebvp.txt">see here</a></div><div><br/></div><div>My Speculations: </div><div><br/></div><div>1)  When a gov runs he wants someone with fed experience. That explains 5 of the cases above.</div><div><br/></div><div>2) Senators tend to be better known than other politicians, so perhaps being well known is what you need. Note on the Republican Side Paul Ryan was a House member but was well known.</div><div><br/></div><div>3) Why do the Reps and Dems differ on this? Is the sample size too small to make this question interesting? </div><div><br/></div><div>So here is the question: When trying to predict who the Dem VP will be (this year or any year) should being a Senator give someone a plus? A higher weight in a Neural Net? </div><div><br/></div><div>Kamala Harris is the favorite on the betting markets.  Is this because she is a Senator?</div><div><br/></div><div>She has a national profile and has already been vetted since she rana serious campaign  for Prez in the primaries. So I would say that THATS the reason (and other reasons), not that she is a Senator. But would she have been able to run a serious campaign  in the primaries if she wasn't already a Senator? I ask non rhetorically.</div><div><br/></div><div>ADDED LATER- one of the comments inspired me to also include who the Republicans picked for VP since Truman. Much more variety in the jobs they held prior. This is neither good or bad.</div><div><br/></div><div><br/></div><div><div><br/></div><div>Thomas Dewey (Gov-NY), Earl Warren (Gov-California)</div><div><br/></div><div>Dwight Eisenhower (General), Richard Nixon (Sen-California)</div><div><br/></div><div>Richard Nixon (VP), Henry Cabot Lodge (Sen-Mass, Amb-UN)</div><div><br/></div><div>Barry Goldwater (Sen-Arizona), William Miller (Representive-NY)</div><div><br/></div><div>Richard Nixon (VP), Spiro Agnew (Gov-MD)</div><div><br/></div><div>Gerald Ford (Prez, Senator-Michigan), Bob Dole (Senator-Kansas)</div><div><br/></div><div>Ronald Reagan (Gov California), George Bush (Dir of CIA)</div><div><br/></div><div>George Bush (VP), Dan Quayle (Sen-Indiana)</div><div><br/></div><div>Bob Dole (Sen-Kansas), Jack Kemp (Representiative-NY)</div><div><br/></div><div>George W Bush (Gov-Texas), Dick Cheney (Cabinet)</div><div><br/></div><div>John McCain (Sen-Arizona), Sarah Palin (Gov-Alaska)</div><div><br/></div><div>Mitt Romney (Gov-Mass), Paul Ryan (Represenative-NY)</div><div><br/></div><div>Donald Trump (Businessman-NY), Mike Pence (Gov-Indiana)</div></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div></div>
    </content>
    <updated>2020-08-06T18:59:00Z</updated>
    <published>2020-08-06T18:59:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-08-16T08:52:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1380</id>
    <link href="https://ptreview.sublinear.info/?p=1380" rel="alternate" type="text/html"/>
    <title>Videos from the WoLA 2020 workshop</title>
    <summary>The 4th Workshop on Local Algorithms (WoLA 2020) recently concluded: aimed at “fostering dialogue and cross-pollination of ideas between the various communities” related to local algorithms, broadly construed, it featured invited and contributed talks on a variety of topics, many (if not most) very relevant to the sublinear algorithms and property testing community. Because of […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The 4th <a href="https://www.mit.edu/~mahabadi/workshops/WOLA-2020.html">Workshop on Local Algorithms (WoLA 2020)</a> recently concluded: aimed at <em>“fostering dialogue and cross-pollination of ideas between the various communities</em>” related to local algorithms, broadly construed, it featured invited and contributed talks on a variety of topics, many (if not most) very relevant to the sublinear algorithms and property testing community.</p>



<p>Because of the online format of the workshop (imposed by the current circumstances), all talks were recorded and posted online. As such, all videos all available on the workshop’s <a href="https://www.mit.edu/~mahabadi/workshops/WOLA-2020.html">website</a> and <a href="https://www.youtube.com/channel/UCMBSZ3q2FKJntpcK72h9eeA/videos">YouTube channel</a>: a good list of resources to peruse!</p></div>
    </content>
    <updated>2020-08-06T00:33:58Z</updated>
    <published>2020-08-06T00:33:58Z</published>
    <category term="Conference reports"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-08-15T23:28:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5510</id>
    <link href="https://adamsheffer.wordpress.com/2020/08/05/mind-benders-for-the-quarantined/" rel="alternate" type="text/html"/>
    <title>Mind-Benders for the Quarantined</title>
    <summary>Peter Winkler is a world expert on mathematical puzzles (he is also an excellent researcher and this year’s resident mathematician of the MoMath). I just learned about two things that he is currently up to. Winkler is running Mind-Benders for the Quarantined. After signing up to this free service, you receive a mathematical puzzle every […]</summary>
    <updated>2020-08-05T22:51:31Z</updated>
    <published>2020-08-05T22:51:31Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2020-08-16T16:21:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1091</id>
    <link href="http://corner.mimuw.edu.pl/?p=1091" rel="alternate" type="text/html"/>
    <title>Faster PageRank on MPC</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Years ago when I learned about Google PageRank algorithm, my first reaction was this is not the way it should be done! There should be some proof. This probably just shows that my CS education was too theoretical ;). Years … <a href="http://corner.mimuw.edu.pl/?p=1091">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Years ago when I learned about Google PageRank algorithm, my first reaction was this is not the way it should be done! There should be some proof. This probably just shows that my CS education was too theoretical ;). Years later I have learned that indeed there are some nice tools to argue about the running time of PageRank algorithm. And very recently we were able to give some new parallel (in MPC model) algorithms for computing vanilla PageRank. We improved the number of rounds needed from O(log n) to O(log^2 log n) time. You can hear Solbodan talking out it here:  <a href="https://www.youtube.com/watch?v=xoodhmjJ9Xs">https://www.youtube.com/watch?v=xoodhmjJ9Xs</a> .<br/> </p></div>
    </content>
    <updated>2020-08-05T18:55:13Z</updated>
    <published>2020-08-05T18:55:13Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2020-08-15T23:27:52Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/open-problem-all-pairs/</id>
    <link href="https://differentialprivacy.org/open-problem-all-pairs/" rel="alternate" type="text/html"/>
    <title>Open Problem: Private All-Pairs Distances</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Background:</strong> Suppose we are interested in computing the distance between two vertices in a graph. Under edge or node differential privacy, this problem is not promising because the removal of a single edge can make distances change from 1 to \(n − 1\) or can even disconnect the graph. However, a different setting that makes sense to consider is that of a weighted graph \((G, w)\) whose topology \(G = (V, E)\) is publicly known but edge weight function \(w : E \to \mathbb{R}^+\) must be kept private. (For instance, consider transit times on a road network. The topology of the road network may be publicly available as a map, but the edge weights corresponding to transit times may be based on private GPS locations of individual cars.)</p>

<p>Suppose that two weight functions \(w\) and \(w’\) of the same graph \(G\) are considered to be neighbors if they differ by at most 1 in \(\ell^1\) norm. Then the length of any fixed path is sensitivity-one, so the distance between any pair of vertices is also sensitivity-one and can be released privately via the Laplace mechanism. But what if we want to release all \(\Theta(n^2)\) distances between all pairs of vertices in \(G\)? We can do this with accuracy roughly \(O(n \log n )/\varepsilon\) by adding noise to each edge, or roughly \(O(n \sqrt{\log(1/\delta)}/\varepsilon)\) using composition theorems. Both of these are roughly \(n/\varepsilon\). But is this linear dependence on \(n\) inherent, or is it possible to release all-pairs distances with error sublinear in \(n\)?</p>

<p>This setting and question were considered in [<a href="https://arxiv.org/abs/1511.04631">S16</a>].</p>

<p><strong>Problem 1:</strong> Let \(G\) be an arbitrary public graph, and \(w : E \to \mathbb{R}^+\) be an edge weight function. Can we release approximate all-pairs distances in \((G, w)\) with accuracy sublinear in \(n\) while preserving the privacy of the edge weight function, where two weight functions \(w, w’\) are neighbors if \(\|w − w’\|_1 \le 1\)? Or can we show that any private algorithm must have error \(\Omega(n)\)? A weaker (but nontrivial) lower bound would also be nice.</p>

<p><strong>Reward:</strong> A bar of chocolate.</p>

<p><strong>Other related work:</strong> [<a href="https://arxiv.org/abs/1511.04631">S16</a>] provided algorithms with better error for two special cases, trees and graphs of a priori bounded weight. For trees, it is possible to release all-pairs distances with error roughly \(O(\log^{1.5} n)/\varepsilon\), while for arbitrary graphs with edge weights restricted to the interval \([0,M]\), it is possible to release all-pairs distances with error roughly \(O( \sqrt{nM\varepsilon^{-1}\log(1/\delta)})\)</p>

<p><em>Submitted by <a href="http://www.mit.edu/~asealfon/">Adam Sealfon</a> on April 9, 2019.</em></p></div>
    </summary>
    <updated>2020-08-05T18:00:00Z</updated>
    <published>2020-08-05T18:00:00Z</published>
    <author>
      <name>Audra McMillan</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-08-16T16:21:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=755</id>
    <link href="https://emanueleviola.wordpress.com/2020/08/05/previous-vs-concurrent-independent-work/" rel="alternate" type="text/html"/>
    <title>Previous vs. concurrent/independent work</title>
    <summary>Should Paper X cite Paper Y as previous or concurrent/independent work? This is sometimes tricky: maybe Paper Y circulated privately before Paper X, maybe the authors of Paper X knew about Paper Y maybe not — nobody can know for sure. One can say that the authors of Paper Y should have posted Paper Y […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Should Paper X cite Paper Y as previous or concurrent/independent work?  This is sometimes tricky: maybe Paper Y circulated privately before Paper X, maybe the authors of Paper X knew about Paper Y maybe not — nobody can know for sure.  One can say that the authors of Paper Y should have posted Paper Y online earlier to prevent this issue, but <a href="https://emanueleviola.wordpress.com/2014/07/02/only-papers-on-the-arxiv-can-be-submitted-for-publication/">that is not standard practice </a>and might lead to other problems, <a href="https://emanueleviola.wordpress.com/2016/08/10/paper-x-on-the-arxiv-keeps-getting-rejected/">including Paper Y never getting published!</a></p>



<p>I propose the following guiding principle:</p>



<p>“If a different accept/reject outcome would have forced paper X to cite paper Y as previous work, then paper X should cite paper Y as previous work.”<br/></p>



<p>The reasons behind my principle seem to me especially valid in the fast-moving theoretical computer science community, where papers are typically sent to conferences and thus seen by the entire program committee plus around 3 external referees, who are typically experts — only to be rejected.  Moreover, the progress is extremely fast, with the next conference cycle making obsolete a number of papers in just the previous cycle.</p></div>
    </content>
    <updated>2020-08-05T14:50:33Z</updated>
    <published>2020-08-05T14:50:33Z</published>
    <category term="Uncategorized"/>
    <category term="utopia-tcs"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2020-08-16T16:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/118</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/118" rel="alternate" type="text/html"/>
    <title>TR20-118 |  On Testing Asymmetry in the Bounded Degree Graph Model | 

	Oded Goldreich</title>
    <summary>We consider the problem of testing asymmetry in the bounded-degree graph model, where a graph is called asymmetric if the identity permutation is its only automorphism. Seeking to determine the query complexity of this testing problem, we provide partial results. Considering the special case of $n$-vertex graphs with connected components of size at most $s(n)=\Omega(\log n)$, we show that the query complexity of $\epsilon$-testing asymmetry (in this case) is at most $O({\sqrt n}\cdot s(n)/\epsilon)$, whereas the query complexity of $o(1/s(n))$-testing asymmetry (in this case) is at least $\Omega({\sqrt{n/s(n)}})$.

In addition, we show that testing asymmetry in the dense graph model is almost trivial.</summary>
    <updated>2020-08-04T21:41:44Z</updated>
    <published>2020-08-04T21:41:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-16T16:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=2561</id>
    <link href="https://francisbach.com/integration-by-parts-abel-transformation/" rel="alternate" type="text/html"/>
    <title>The many faces of integration by parts – I : Abel transformation</title>
    <summary>Integration by parts is a highlight of any calculus class. It leads to multiple classical applications for integration of logarithms, exponentials, etc., and it is the source of an infinite number of exercises and applications to special functions. In this post, I will look at a classical discrete extension that is useful in machine learning...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Integration by parts is a highlight of any calculus class. It leads to multiple classical applications for integration of logarithms, exponentials, etc., and it is the source of an infinite number of exercises and applications to <a href="https://en.wikipedia.org/wiki/Special_functions">special functions</a>. In this post, I will look at a classical discrete extension that is useful in machine learning and optimization, namely <a href="https://en.wikipedia.org/wiki/Summation_by_parts">Abel transformation</a>, with applications to convergence proofs for the (stochastic) <a href="https://en.wikipedia.org/wiki/Subgradient_method">subgradient method</a>. Next month, extensions to higher dimensions will be considered, with applications to score functions [<a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">2</a>, <a href="https://www.jstor.org/stable/1914309">3</a>] and randomized smoothing [4, <a href="https://arxiv.org/pdf/2002.08676">5</a>].</p>



<h2>Abel transformation: from continuous to discrete</h2>



<p class="justify-text">The most classical version of integration by parts goes as follows. Given two continuously differentiable functions from \(\mathbb{R}\) to \(\mathbb{R}\), we have: $$ \int_a^b \!\!\!\!f(x)g'(x) dx = \Big[ f(x) g(x) \Big]_a^b \!-\! \int_a^b\!\!\! \!f'(x) g(x) dx =  f(b) g(b)\, – f(a)g(a)-\! \int_a^b\! \!\!\! f'(x) g(x) dx.$$ This is valid for less regular functions, but this is not the main concern here. The proof follows naturally from the derivative of a product, but there is a nice “proof without words” (see, e.g., [1, p. 42] or <a href="https://en.wikipedia.org/wiki/Integration_by_parts#Visualization">here</a>).</p>



<p class="justify-text">There is a discrete analogue referred to as <a href="https://en.wikipedia.org/wiki/Summation_by_parts">Abel transformation</a> or summation by parts, where derivatives are replaced by increments: given two real-valued sequences \((a_n)_{n \geq 0}\) and \((b_n)_{n \geq 0}\) (the second sequence could also be taken vector-valued), we can expand $$ \sum_{k=1}^n a_k ( b_k\, – b_{k-1}) =\sum_{k=1}^n a_k  b_k \ – \sum_{k=1}^n a_k  b_{k-1} = \sum_{k=1}^n a_k b_k \ – \sum_{k=0}^{n-1} a_{k+1} b_{k},$$ using a simple index increment in the second sum.  Rearranging terms, this leads to $$ \sum_{k=1}^n a_k ( b_k\, – b_{k-1}) = a_n b_n \ – a_0 b_0\  – \sum_{k=0}^{n-1} ( a_{k+1} – a_{k } ) b_k.$$ In other words, we can transfer the first-order difference from the sequence \((b_k)_{k \geq 0}\) to the sequence \((a_k)_{k \geq 0}\).  A few remarks:</p>



<ul class="justify-text"><li><strong>Warning</strong>! It is very easy/common to make mistakes with indices and signs.</li><li>I gave the direct proof but a proof through explicit integration by part is also possible, by introducing the piecewise-constant function \(f\) equal to \(a_k\) on \([k,k+1)\), and \(g\) continuous  piecewise affine equal to \(b_{k} + (t-k) ( b_{k+1}-b_{k})\) for \(t \in [k,k+1]\), and integrating between \(0+\) and \(n+\). </li></ul>



<p class="justify-text">There are classical applications for the convergence of series (see <a href="https://en.wikipedia.org/wiki/Summation_by_parts">here</a>), but in this post, I will show how it can lead to an elegant result for stochastic gradient descent for non-smooth functions and <em>decaying</em> step-sizes.</p>



<h2>Decaying step-sizes in stochastic gradient descent</h2>



<p class="justify-text">The Abel summation formula is quite useful when analyzing optimization algorithms, and we give a simple example below. We consider a sequence of random potentially <em>non-smooth</em> convex functions \((f_k)_{k \geq 0}\) which are independent and identically distributed functions from \(\mathbb{R}^d \) to \(\mathbb{R}\), with expectation \(F\). The goal is to find a minimizer \(x_\ast\) of \(F\) over a some convex bounded set \(\mathcal{C}\), only being given access to some stochastic gradients of \(f_k\) at well-chosen points. The most classical example is supervised machine learning, where \(f_k(\theta)\) is the loss of a random observation for the predictor parameterized by \(\theta\).  The difficulty here is the potential non-smoothness of the function \(f_k\) (e.g., for the <a href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a> and the <a href="https://en.wikipedia.org/wiki/Support_vector_machine">support vector machine</a>).</p>



<p class="justify-text">We consider the projected stochastic subgradient descent method. The deterministic version of this method dates back to Naum Shor [6] in 1962 (see nice history <a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/43_goffin-jean-louis.pdf">here</a>). The method goes as follows: starting from some \(\theta_0 \in \mathbb{R}^d\), we perform the iteration $$ \theta_{k} = \Pi_{ \mathcal{C} } \big( \theta_{k-1} – \gamma_k  \nabla f_k(\theta_{k-1}) \big),$$ where \(\Pi_{ \mathcal{C}}: \mathbb{R}^d \to \mathbb{R}^d\) is the orthogonal projection onto the set \(\mathcal{C}\), and \(\nabla f_k(\theta_{k-1})\) is any subgradient of \(f_k\) at \(\theta_{k-1}\). </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4324" height="211" src="https://francisbach.com/wp-content/uploads/2020/07/gradient_contours_projection-1024x410.png" width="528"/>One step of projected (sub)gradient descent: from a vector \(\theta\), we go down the direction of a negative subgradient \(\nabla f(\theta)\) of the function \(f\) (here typically a random function) and an orthogonal projection is performed to obtain the new vector \(\theta_+\).</figure></div>



<p class="justify-text">We make the following standard assumptions: (a) the set \(\mathcal{C}\) is convex and compact with diameter \(\Delta\) (with respect to the \(\ell_2\)-norm), (b) the functions \(f_k\) are almost surely convex and \(B\)-Lipschitz-continuous (or equivalently with gradients bounded in \(\ell_2\)-norm by \(B\)). We denote by \(\theta_\ast\) a minimizer of \(f\) on \(\mathcal{C}\) (there can be multiple ones). </p>



<p class="justify-text">For non-smooth problems, choosing a constant step-size does not lead to an algorithm converging to a global minimizer: decaying step-sizes are then needed.</p>



<h2>Convergence proof through Lyapunov functions</h2>



<p class="justify-text">Since the functions \(f_k\) are non-smooth, we cannot use Taylor expansions, and we rely on a now classical proof technique dating back from the 1960’s (see, e.g., a <a href="http://www.mathnet.ru/links/5d71a255cae8f1a313ac599b8f20a123/dan33049.pdf">paper</a> by Boris Polyak [7] in Russian), that has led to several extensions in particular for online learning [<a href="http://www.cs.cmu.edu/~maz/publications/techconvex.pdf">8</a>]. The proof relies on the concept of “<a href="https://en.wikipedia.org/wiki/Lyapunov_function">Lyapunov functions</a>“, often also referred to as “potential functions”. This is a non-negative function \(V(\theta_k)\) of the iterates \(\theta_k\), that is supposed to go down along iterations (at least in expectation). In optimization, standard Lyapunov functions are \(V(\theta)  = F(\theta)\, – F(\theta_\ast)\) or \(V(\theta) = \| \theta \ – \theta_\ast\|_2^2\). </p>



<p class="justify-text">For the subgradient method, we will not be able to show that the Lyapunov function is decreasing, but this will lead through a manipulation which is standard in linear dynamical system analysis to a convergence proof for the averaged iterate: that is, if \(V(\theta_k) \leqslant V(\theta_{k-1})\ – W(\theta_{k-1}) + \varepsilon_k\),  for a certain function \(W\) and extra positive terms \(\varepsilon_k\), then, using telescoping sums, $$ \frac{1}{n} \sum_{k=1}^n W(\theta_{k-1}) \leqslant \frac{1}{n} \big( V(\theta_0)\ – V(\theta_n) \big) + \frac{1}{n} \sum_{k=1}^n \varepsilon_k.$$ We can then either use <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s inequality</a> to get a bound on \(W \big( \frac{1}{n} \sum_{k=1}^n \theta_{k-1} \big)\), or directly get a bound on \(\min_{k \in \{1,\dots,n\}} W(\theta_{k-1})\). The first solution gives a performance guarantee for a well-defined iterate, while the second solution only shows that among the first \(n-1\) iterates, one of them has a performance guarantee; in the stochastic set-up where latex \(W\) is an expectation, it is not easily possible to know which one, so we will consider only averaging below.</p>



<p class="justify-text"><strong>Standard inequality. </strong>We have, by contractivity of orthogonal projections: $$ \|\theta_k \ – \theta_\ast\|_2^2 =  \big\|  \Pi_{ \mathcal{C} } \big( \theta_{k-1} – \gamma_k   \nabla f_k(\theta_{k-1}) \big) – \Pi_{ \mathcal{C} } (\theta_\ast)  \big\|_2^2 \leqslant  \big\|   \theta_{k-1} – \gamma_k  \nabla f_k(\theta_{k-1}) -\   \theta_\ast  \big\|_2^2.$$ We can then expand the squared Euclidean norm to get: $$ \|\theta_k – \theta_\ast\|_2^2 \leqslant  \|\theta_{k-1} – \theta_\ast\|_2^2 \ – 2\gamma_k (\theta_{k-1} – \theta_\ast)^\top \nabla f_k (\theta_{k-1}) + \gamma_k^2 \|  \nabla f_k(\theta_{k-1})\|_2^2.$$ The last term is upper-bounded by \(\gamma_k^2 B^2\) because of the regularity assumption on \(f_k\). For the middle term, we use the convexity of \(f_k\), that is,  the function \(f_k\) is greater than its tangent at \(\theta_{k-1}\). See figure below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4331" height="220" src="https://francisbach.com/wp-content/uploads/2020/07/tangent_convex-1-1024x440.png" width="513"/>Convex function above its tangent at \(\theta_{k-1}\), leading to the desired inequality.</figure></div>



<p class="justify-text">We then obtain $$ f_k(\theta_\ast) \geqslant f_k(\theta_{k-1}) + \nabla f_k(\theta_{k-1})^\top ( \theta_{\ast} – \theta_{k-1}).$$</p>



<p class="justify-text">Putting everything together, this leads to $$ \|\theta_k \ – \theta_\ast\|_2^2 \leqslant  \|\theta_{k-1}\  – \theta_\ast\|_2^2 \ – 2\gamma_k  \big[ f_k(\theta_{k-1}) \ – f_k(\theta_\ast) \big] + \gamma_k^2 B^2.$$ At this point, except the last term, all terms are random. We can now take expectations, with a particular focus on the term \(\mathbb{E} \big[ f_k(\theta_{k-1}) \big]\), for which we can use the fact that the random function \(f_k\) is independent from the past, so that $$ \mathbb{E} \big[ f_k(\theta_{k-1}) \big] =  \mathbb{E} \Big[  \mathbb{E} \big[ f_k(\theta_{k-1}) \big| f_{1},\dots,f_{k-1}  \big] \Big] =\mathbb{E} \big[   F(\theta_{k-1})   \big] . $$ We thus get $$ \mathbb{E} \big[ \|\theta_k – \theta_\ast\|_2^2\big] \leqslant  \mathbb{E} \big[ \|\theta_{k-1} – \theta_\ast\|_2^2\big]  – 2\gamma_k \big( \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast) \big) + \gamma_k^2 B^2.$$ As above, we can now isolate the excess in function values as: $$ \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast)  \leqslant \frac{1}{2 \gamma_k} \Big( \mathbb{E} \big[ \|\theta_{k-1} – \theta_\ast\|_2^2\big] – \mathbb{E} \big[ \|\theta_{k} – \theta_\ast\|_2^2\big] \Big) + \frac{\gamma_k}{2} B^2.$$ At this point, the “optimization part” of the proof is done. Only algebraic manipulations are needed to obtain a convergence rate. This is where Abel transformation will come in.</p>



<h2>From fixed horizon to anytime algorithms</h2>



<p class="justify-text"><strong>The lazy way.</strong> At this point, many authors (including me sometimes) will take a constant step-size \(\gamma_k = \gamma\) so as to obtain a telescopic sum, leading to $$ \frac{1}{n} \sum_{k=1}^n \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast) \leqslant \frac{1}{2n\gamma}     \Big( \mathbb{E} \big[ \|\theta_{0} \ – \theta_\ast\|_2^2\big] – \mathbb{E} \big[ \|\theta_{n}\  – \theta_\ast\|_2^2\big] \Big) + \frac{\gamma}{2} B^2,$$ which is less than \(\displaystyle \frac{\Delta^2}{2n \gamma} + \frac{\gamma}{2} B^2\), and minimized for \(\displaystyle \gamma = \frac{ \Delta}{B \sqrt{n}}\), leading to a convergence rate less than \(\displaystyle \frac{ B \Delta}{\sqrt{n}}\). Using Jensen’s inequality, we then get for \(\bar{\theta}_n = \frac{1}{n} \sum_{k=1}^n \theta_{k-1}\): $$\mathbb{E} \big[ F(\bar{\theta}_{n}) \big] – F(\theta_\ast) \leqslant \frac{ B \Delta}{\sqrt{n}} .$$ This result leads to the desired rate but can be improved in at least one way: the step-size currently has to depend on the “horizon” \(n\) (which has to be known in advance), and the algorithm is not “anytime”, which is not desirable in practice (where one often launches an algorithm and stops it when it the performance gains have plateaued or when the user gets bored waiting).</p>



<p class="justify-text"><strong>Non-uniform averaging.</strong> Another way [<a href="https://www2.isye.gatech.edu/~nemirovs/SIOPT_RSA_2009.pdf">9</a>] is to consider the non-uniform average $$ \eta_{k} =   \frac{\sum_{k=1}^n \gamma_{k} \theta_{k-1}}{\sum_{k=1}^n \gamma_{k}}, $$ for which telescoping sums apply as before, to get $$ \mathbb{E} \big[ F(\eta_k) \big] – F(\theta_\ast) \leqslant \frac{1}{2} \frac{\Delta^2 + B^2 \sum_{k=1}^n \gamma_k^2}{\sum_{k=1}^n \gamma_{k}}.$$  Then, by selecting a decaying step-size \(\displaystyle \gamma_k = \frac{ \Delta}{B \sqrt{k}}\), that depends on the iteration number, we get a rate proportional to \(\displaystyle \frac{ B \Delta}{\sqrt{n}} ( 1 + \log n)\). We now have an anytime algorithm, but we have lost a logarithmic term, which is not the end of the world, but still disappointing. In [<a href="https://www2.isye.gatech.edu/~nemirovs/SIOPT_RSA_2009.pdf">9</a>], “tail-averaging” (only averaging iterates between a constant times \(n\) and \(n\)) is proposed, that removes the logarithmic term but requires to store iterates (moreover, the non-uniform averaging puts too much weight on the first iterates, slowing down convergence).</p>



<p class="justify-text"><strong>Using Abel transformation.</strong> If we start to sum inequalities from \(k=1\) to \(k=n\), we get, with \(\delta_k = \mathbb{E} \big[ \|\theta_{k} – \theta_\ast\|_2^2\big]\) (which is always between \(0\) and \(\Delta^2\)): $$ \frac{1}{n} \sum_{k=1}^n \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast)  \leqslant  \frac{1}{n} \sum_{k=1}^n \bigg( \frac{1}{2 \gamma_k} \Big( \delta_{k-1} –  \delta_k \Big)\bigg) +  \frac{1}{n} \sum_{k=1}^n \frac{\gamma_k}{2} B^2,$$ which can be transformed through Abel transformation into $$ \frac{1}{n} \sum_{k=1}^n \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast) \leqslant \frac{1}{n} \sum_{k=1}^{n-1}  {\delta_k} \bigg(\frac{1}{ 2 \gamma_{k+1}}- \frac{1}{ 2 \gamma_{k}} \bigg) + \frac{\delta_0}{2 n \gamma_1}- \frac{\delta_t}{2 n \gamma_t}+ \frac{1}{n} \sum_{k=1}^n \frac{\gamma_k}{2} B^2.$$ For decreasing step-size sequences, this leads to $$ \frac{1}{n} \sum_{k=1}^n \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast) \leqslant \frac{1}{n} \sum_{k=1}^{n-1} {\Delta^2} \bigg(\frac{1}{ 2\gamma_{k+1}}- \frac{1}{ 2\gamma_{k}} \bigg) + \frac{\Delta^2}{2n \gamma_1}+ \frac{1}{n} \sum_{k=1}^n \frac{\gamma_k}{2} B^2,$$ and thus $$ \frac{1}{n} \sum_{k=1}^n \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast) \leqslant \frac{\Delta^2 }{2 n \gamma_n} + \frac{1}{n} \sum_{k=1}^n \frac{\gamma_k}{2} B^2.$$ For \(\gamma_k = \frac{  \Delta}{B \sqrt{k}}\), this leads to an upper bound $$\frac{\Delta B }{2 \sqrt{n}} \big( 1+ \frac{1}{\sqrt{n}} \sum_{k=1}^n \frac{1}{\sqrt{k}}\big) \leqslant \frac{3 \Delta B }{2 \sqrt{n}},$$ which is up to a factor \(\frac{3}{2}\) exactly the same bound as with a constant step-size, but now with an anytime algorithm.</p>



<h2>Experiments</h2>



<p class="justify-text">To illustrate the behaviors above, let’s consider minimizing \(\mathbb{E}_x \| x – \theta \|_1\), with respect to \(\theta\), with \(f_k(\theta) = \| x_k- \theta\|_1\), where \(x_k\) is sampled independently from a given distribution (here independent log-normal distributions for each coordinate). The global optimum \(\theta_\ast\) is the per-coordinate median of the distribution of \(x\)’s.</p>



<p class="justify-text">When applying SGD, the chosen subgradient of \(f_k\) has components in \(\{-1,1\}\). Hence in the plots below in two dimensions, the iterates are always on a grid. With a constant step-size: if the \(\gamma\) is too large (right), there are large oscillations, while if \(\gamma\) is too small (left), optimization is too slow. Note that while the SGD iterate with a constant step-size is always oscillating, the averaged iterate converges to some point (which is not the global optimum, and is typically at distance \(O(\gamma)\) away from it [<a href="https://arxiv.org/pdf/1707.06386">11</a>]).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4339" height="230" src="https://francisbach.com/wp-content/uploads/2020/07/sgd-1.gif" width="545"/>Stochastic gradient descent (averaged or not), with constant step-size. Left: small step-size. Right: large step-size (8 times larger).</figure></div>



<p class="justify-text">With a decaying step-size (figure below), the initial conditions are forgotten reasonably fast and the iterates converge to the global optimum (and of course, we get an anytime algorithm!).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4340" height="295" src="https://francisbach.com/wp-content/uploads/2020/07/sgd_decaying.gif" width="310"/>Stochastic gradient descent (averaged or not), with decreasing step-size.</figure></div>



<p>We can now compare in terms of function values, showing that a constant step-size only works well for a specific range of iteration numbers.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4335" height="276" src="https://francisbach.com/wp-content/uploads/2020/07/convergence_proofs.png" width="371"/>Comparison of expected performance for decaying and constant-step sizes. Several constant step-sizes are tested, with uniform spacings in log-scale (hence the the uniform spacings in performance for large \(n\)).</figure></div>



<h2>Conclusion</h2>



<p class="justify-text">Being able to deal with decaying step-sizes and anytime algorithms is arguably not a major improvement, but quite a satisfactory one, at least to me! Discrete integration by parts is the key enabler here.</p>



<p class="justify-text">There is another rewarding aspect which is totally unrelated to integration by parts: when applied to supervised machine learning, we just obtained from elementary principles (convexity) and few calculations a generalization bound <em>on unseen data</em>, which is as good as regular bounds from statistics [<a href="https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf">10</a>] that use much more complex tools such as <a href="https://en.wikipedia.org/wiki/Rademacher_complexity">Rademacher complexities</a> (but typically no convexity assumptions): here, statistics considered independently from optimization is not only slower (considering the empirical risk and minimizing it using the plain non-stochastic subgradient method would lead to an \(n\) times slower algorithm) but also more difficult to analyze! </p>



<h2>References</h2>



<p class="justify-text">[1] Roger B. Nelsen, <em>Proofs without Words: Exercises in Visual Thinking</em>, Mathematical Association of America, 1997.<br/>[2] Aapo Hyvärinen, <a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Estimation of non-normalized statistical models by score matching</a>. <em>Journal of Machine Learning Research</em>, <em>6</em>(Apr), 695-709, 2005.<br/>[3] Thomas M. Stoker, <a href="https://www.jstor.org/stable/1914309">Consistent estimation of scaled coefficients</a>.  <em>Econometrica: Journal of the Econometric Society</em>, 54(6):1461-1481, 1986.<br/>[4] Tamir Hazan, George Papandreou, and Daniel Tarlow. <a href="https://mitpress.mit.edu/books/perturbations-optimization-and-statistics">Perturbation, Optimization, and Statistics</a>. MIT Press, 2016.<br/>[5] Quentin Berthet, Matthieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach, <a href="https://arxiv.org/pdf/2002.08676">Learning with differentiable perturbed optimizers</a>. Technical report arXiv 2002.08676, 2020.<br/>[6] Naum Z. Shor. An application of the method of gradient descent to the solution of the network transportation problem. <em>Notes of Scientific Seminar on Theory and Applications of Cybernetics and Operations Research</em>, <em>Ukrainian Academy of Sciences</em>, Kiev, 9–17, 1962.<br/>[7] Boris T. Polyak, <a href="http://www.mathnet.ru/links/5d71a255cae8f1a313ac599b8f20a123/dan33049.pdf">A general method for solving extremal problems</a>. <em>Doklady Akademii Nauk SSSR</em>, 174(1):33–36, 1967.<br/>[8] Martin Zinkevich. <a href="http://www.cs.cmu.edu/~maz/publications/techconvex.pdf">Online convex programming and generalized infinitesimal gradient ascent</a>. <em>Proceedings of the international conference on machine learning )(ICML)</em>, 2003.<br/>[9] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, Alexander Shapiro<em>.</em> <a href="https://www2.isye.gatech.edu/~nemirovs/SIOPT_RSA_2009.pdf">Robust stochastic approximation approach to stochastic programming</a>. <em>SIAM Journal on optimization</em>, 19(4):1574-1609, 2009.<br/>[10] Stéphane Boucheron, Olivier Bousquet, Gabor Lugosi. <a href="https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf">Theory of classification: A survey of some recent advances</a>. <em>ESAIM: probability and statistics</em>, <em>9</em>, 323-375, 2005.<br/>[11] Aymeric Dieuleveut, Alain Durmus, and Francis Bach. <a href="https://arxiv.org/pdf/1707.06386">Bridging the gap between constant step size stochastic gradient descent and Markov chains</a>. Annals of Statistics, 48(3):1348-1382, 2020.</p></div>
    </content>
    <updated>2020-08-04T15:55:26Z</updated>
    <published>2020-08-04T15:55:26Z</published>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-08-16T16:21:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/117</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/117" rel="alternate" type="text/html"/>
    <title>TR20-117 |  New bounds on the half-duplex communication complexity | 

	Alexander Smal, 

	Yuriy Dementiev, 

	Artur Ignatiev, 

	Vyacheslav Sidelnik, 

	Mikhail Ushakov</title>
    <summary>In this work, we continue the research started in [HIMS18], where the authors suggested to study the half-duplex communication complexity. Unlike the classical model of communication complexity introduced by Yao, in the half-duplex model, Alice and Bob can speak or listen simultaneously, as if they were talking using a walkie-talkie. The motivation for such a communication model comes from the study of the KRW conjecture. Following the open questions formulated in [HIMS18], we prove lower bounds for the disjointness function in all variants of half-duplex models and an upper bound in the half-duplex model with zero, that separates disjointness from the inner product function in this setting. Next, we prove lower and upper bounds on the half-duplex complexity of the Karchmer-Wigderson games for the counting functions and for the recursive majority function, adapting the ideas used in the classical communication complexity. Finally, we define the non-deterministic half-duplex complexity and establish bounds connecting it with non-deterministic complexity in the classical model.</summary>
    <updated>2020-08-04T15:11:42Z</updated>
    <published>2020-08-04T15:11:42Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-16T16:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1371</id>
    <link href="https://ptreview.sublinear.info/?p=1371" rel="alternate" type="text/html"/>
    <title>News for July 2020</title>
    <summary>We hope you’re all staying safe and healthy! To bring you some news (and distraction?) during this… atypical summer,here are the recent papers on property testing and sublinear algorithms we saw appear this month. Graphs, probability distributions, functions… there is a something for everyone. On Testing Hamiltonicity in the Bounded Degree Graph Model, by Oded […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We hope you’re all staying safe and healthy! To bring you some news (and distraction?) during this… atypical summer,here are the recent papers on property testing and sublinear algorithms we saw appear this month. Graphs, probability distributions, functions… there is a something for everyone.</p>



<p><strong>On Testing Hamiltonicity in the Bounded Degree Graph Model,</strong> by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2020/109/">ECCC</a>). The title sort of gives it away: this relatively short paper shows that testing whether an unknown bounded-degree graph has a Hamiltonian path (or Hamiltonian cycle) in the bounded-degree model requires a number of queries linear in \(n\), the number of nodes. The results also hold for directed graphs (with respect to directed Hamiltonian path or cycle), and are shown via a local reduction to a promise problem of satisfiability of 3CNF formulae. Also included: a complete proof of the linear lower bound for another problem, Independent Set Size; and an open problem: <em>what is the query complexity of testing graph isomorphism in the bounded-degree model?</em></p>



<p><strong>Local Access to Sparse Connected Subgraphs Via Edge Sampling</strong>, by Rogers Epstein (<a href="https://arxiv.org/abs/2007.05523">arXiv</a>). Given access to a connected graph \(G=(V,E)\), can we efficiently provide access to some <em>sparse</em> connected subgraph \(G’=(V,E’)\subseteq G\) with \(|E’|\ll |E|\)? This question, well-studied in particular for the case where \(G\) had bounded degree and the goal is to achieve \(|E’|\leq (1-\varepsilon)|V|\), is the focus of this paper which provides a trade-off between the query complexity of the oracle and \(|E’|\). Specifically, for every parameter \(T\), one can give oracle access to \(G’\) with \(|E’|=O(|V|T)\), with a query complexity \(=\tilde{O}(|E|/T)\). </p>



<p>Switching gears, we move from graphs to probability distributions:</p>



<p><strong>Tolerant Distribution Testing in the Conditional Sampling Model</strong>, by Shyam Narayanan (<a href="https://arxiv.org/abs/2007.09895">arXiv</a>). In the conditional sampling model for distribution testing, which we have covered a few times on this blog, the algorithm at each step gets to specify a subset \(S\) of the domain, and observe a sample from the distribution <em>conditioned on \(S\).</em> As it turns out, this can speed things up a <strong>lot</strong>: as Canonne, Ron, and Servedio (2015) showed, even tolerant uniformity testing, which with i.i.d. samples requires a near-linear (in the domain size \(n\)) number of samples, can be done in a <em>constant</em> number of conditional queries. Well, sort of constant: no dependence on \(n\), but the dependence on the distance parameter \(\varepsilon\) was, in CRS15, quite bad: \(\tilde{O}(1/\varepsilon^{20})\).  This work gets rid of this badness, and shows the (nearly) optimal \(\tilde{O}(1/\varepsilon^{2})\) query complexity! Among other results, it also generalizes it to tolerant identity testing  (\(\tilde{O}(1/\varepsilon^{4})\)), for which previously no constant-query upper bound was known. Things have become <em>truly</em> sublinear.</p>



<p>I<strong>nteractive Inference under Information Constraints</strong>, by Jayadev Acharya, Clément Canonne, Yuhan Liu, Ziteng Sun, and Himanshu Tyagi (<a href="https://arxiv.org/abs/2007.10976">arXiv</a>). Say you want to do uniformity/identity testing (or learn, but let’s focus on testing) on a discrete distribution, but you can’t actually observe the i.i.d. samples: instead, you can only do some sort of limited, “local” measurement on each sample. How hard is the task, compared to what you’d do if you fully had the samples? This setting, which captures things like distributed testing with communication or local privacy constraints, erasure channels, etc., was well-understood from previous recent work in the <em>non-adaptive</em> setting. But what if the “measurements” could be made <em>adaptively</em>? This paper shows general lower bounds for identity testing and learning, as a function of the type of local measurement allowed: as a corollary, this gives tight bounds for communication constraints and local privacy, and shows the first separation between adaptive and non-adaptive uniformity testing, for a type of “leaky” membership query measurement.</p>



<p><strong>Efficient Parameter Estimation of</strong> <strong>Truncated Boolean Product Distributions</strong>, by Dimitris Fotakis, Alkis Kalavasis, and Christos Tzamos (<a href="https://arxiv.org/abs/2007.02392">arXiv</a>). Suppose there is a fixed and unknown subset \(S\) of the hypercube, a “truncation” set, which you can only accessible via membership query; and you receive i.i.d. samples from an unknown product distribution on the hypercube, <em>truncated</em> on that set \(S\) (for instance, because your polling strategy or experimental measurements have limitations). Can you still learn that distribution efficiently? Can you test it for various properties, as you typically really would like to? (or is it just me?) This paper identifies some natural sufficient condition on \(S\), which they call <em>fatness</em>, under which the answer is a resounding <em>yes</em>. Specifically, if \(S\) satisfies this condition, one can actually generate honest-to-goodness i.i.d. samples (non-truncated) from the true distribution, given truncated samples! </p>



<p>Leaving distribution testing, our last paper is on testing functions in the <em>distribution-free</em> model:</p>



<p><strong>Downsampling for Testing and Learning in Product Distributions,</strong> by Nathaniel Harms and Yuichi Yoshida (<a href="https://arxiv.org/abs/2007.07449">arXiv</a>). Suppose you want to test (or learn) a class of Boolean functions \(\mathcal{C}\) over some domain \(\Omega^n\), with respect to some (unknown) product distribution (i.e., in the distribution-free testing model, or PAC-learning model). This paper develops a general technique, downsampling, which allows one to reduce such distribution-free testing of \(\mathcal{C}\) under a product distribution to testing \(\mathcal{C}\) over \([r]^d\) under the <em>uniform</em> distribution, for a suitable parameter \(r=r(d,\varepsilon,\mathcal{C})\). This allows the authors, among many other things and learning results, to easily re-establish (and, in the second case, improve upon) recent results on testing of monotonicity over \([n]^d\) (uniform distribution) and over \(\mathbb{R}^d\) (distribution-free). </p></div>
    </content>
    <updated>2020-08-04T02:59:57Z</updated>
    <published>2020-08-04T02:59:57Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-08-15T23:28:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17379</id>
    <link href="https://rjlipton.wordpress.com/2020/08/03/cleverer-automata-exist/" rel="alternate" type="text/html"/>
    <title>Cleverer Automata Exist</title>
    <summary>A breakthrough on the separating words problem Zachary Chase is a graduate student of Ben Green at Oxford. Chase has already solved a number of interesting problems–check his site for more details. His advisor is famous for his brilliant work—especially in additive combinatorics. One example is his joint work with Terence Tao proving this amazing […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A breakthrough on the separating words problem</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/?attachment_id=17371" rel="attachment wp-att-17371"><img alt="" class="alignright wp-image-17371" src="https://rjlipton.files.wordpress.com/2020/08/chase.png?w=125" width="125"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p/><p>
Zachary Chase is a graduate student of Ben Green at Oxford. Chase has already solved a number of interesting problems–check his <a href="http://people.maths.ox.ac.uk/~chase/">site</a> for more details. His advisor is famous for his brilliant work—especially in additive combinatorics. One example is his joint work with Terence Tao <a href="https://en.wikipedia.org/wiki/Green-Tao_theorem">proving</a> this amazing statement:</p>
<blockquote><p><b>Theorem 1</b> <em> The prime numbers contain arbitrarily long arithmetic progressions. </em>
</p></blockquote>
<p>
</p><p>
Today we wish to report Chase’s new <a href="https://arxiv.org/pdf/2007.12097.pdf">paper</a> on a problem we have twice discussed before. </p>
<p>
But first Ken wants to say something about Oxford where he got his degree long before Green arrived. </p>
<p>
</p><p/><h2> Oxford Making Waves </h2><p/>
<p/><p>
Green moved to Oxford in 2013. He holds a professorship associated to Magdalen College. I (Ken) did not know him when I started at Oxford in 1981. It would have been hard, as Green was only 4 years old at the time. But I did know the preteen Ruth Lawrence when she started there and even once played a departmental croquet match including her in which Bryan Birch made some epic long shots. Lawrence had <a href="https://en.wikipedia.org/wiki/Ruth_Lawrence">joined</a> St. Hugh’s College in 1983 at the age of twelve.</p>
<p>
Oxford has been Dick’s and my mind more in the past six years than before. Both of us were guests of Joël Ouaknine in 2012–2015 when he was there. Oxford has developed a front-line group in quantum computation, which fits as David Deutsch’s role as an originator began from there—note my story in the middle of this recent <a href="https://rjlipton.wordpress.com/2020/01/15/halting-is-poly-time-quantum-provable/">post</a>.</p>
<p>
Recently Oxford has been in the <a href="https://www.statnews.com/2020/07/20/study-provides-first-glimpse-of-efficacy-of-oxford-astrazeneca-covid-19-vaccine/">news</a> for developing a promising Covid-19 vaccine. <a href="https://www.precisionvaccinations.com/vaccines/chadox1-mers-coronavirus-vaccine">ChAdOx1</a> heads Wikipedia’s <a href="https://en.wikipedia.org/wiki/COVID-19_vaccine#Vaccine_candidates">list</a> of candidate vaccines and has gone to final <a href="https://www.nationalgeographic.com/science/2020/07/oxford-vaccine-enters-final-phase-of-covid-19-trials-in-brazil-cvd/">trials</a>, though there is still a long evaluation process before approval for general use.</p>
<p>
Before that, a modeling <a href="https://nymag.com/intelligencer/2020/03/oxford-study-coronavirus-may-have-infected-half-of-u-k.html">study</a> from Oxford in March raised the question of whether many more people have had Covid-19 without symptoms or any knowledge. This kind of possibility has since been <a href="https://marginalrevolution.com/marginalrevolution/2020/06/karl-friston-on-immunological-dark-matter.html">likened</a> to a “dark matter” hypothesis, not just now regarding Covid-19 but a decade <a href="https://pubmed.ncbi.nlm.nih.gov/21839767/">ago</a> and before. </p>
<p>
A main <a href="https://theconversation.com/coronavirus-techniques-from-physics-promise-better-covid-19-models-can-they-deliver-139925">supporting</a> <a href="https://www.brunel.ac.uk/news-and-events/news/articles/Coronavirus-techniques-from-physics-promise-better-COVID-19-models-can-they-deliver">argument</a> is that a wide class of mathematical models can be fitted with higher relative likelihood if the hypothesis is true. I have wanted to take time to evaluate this argument amid the wider backdrop of <a href="https://rjlipton.wordpress.com/2018/05/19/lost-in-complexity/">controversy</a> over inference methods in physics, but online chess with unfortunately ramped-up frequency of cheating has filled up all disposable time and more.</p>
<p>
</p><p/><h2> The Problem </h2><p/>
<p/><p>
Back to Chase’s new results on the following problem: </p>
<blockquote><p><b> </b> <em> Given two distinct binary strings of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> there is always a finite state deterministic automaton (FSA) that accepts one and rejects the other. <i>How few states can such a machine have?</i> </em>
</p></blockquote>
<p/><p>
This is called the <em>separating words problem</em> (SWP). Here we consider it for binary strings only.</p>
<p>
John Robson proved <img alt="{O(n^{2/5})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B2%2F5%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^{2/5})}"/> states are enough—we suppress any log factors. Some like to write this as <img alt="{\tilde{O}(n^{2/5})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7BO%7D%28n%5E%7B2%2F5%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tilde{O}(n^{2/5})}"/>. Chase <a href="https://arxiv.org/pdf/2007.12097.pdf">improves</a> this to <img alt="{\tilde{O}(n^{1/3})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7BO%7D%28n%5E%7B1%2F3%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tilde{O}(n^{1/3})}"/>:</p>
<blockquote><p><b>Theorem 2</b> <em><a name="Chasethm"/> For any distinct <img alt="{x,y \in \{0,1\}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy+%5Cin+%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x,y \in \{0,1\}^{n}}"/>, there is a finite state deterministic automaton with <img alt="{O(n^{1/3} \log^{7} n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B1%2F3%7D+%5Clog%5E%7B7%7D+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^{1/3} \log^{7} n)}"/> states that accepts <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x}"/> but not <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{y}"/>. </em>
</p></blockquote>
<p/><p>
We previously discussed this twice at GLL. We discussed the background and early results <a href="https://rjlipton.wordpress.com/2019/09/08/separating-words-by-automata/">here</a>. The original problem is due to Pavel Goralcik and Vaclav Koubek. They proved an upper bound that was <img alt="{o(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bo%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{o(n)}"/>. Then we went over Robson’s bound <a href="https://rjlipton.wordpress.com/2019/09/16/separating-words-decoding-a-paper/">here</a>. The best upper bound was Robson’s result until Chase came along.</p>
<p>
</p><p/><h2> The Approach </h2><p/>
<p/><p>
All the approaches to SWP seem to have a common thread. They find some family of “hash” functions <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> so that:</p>
<ol>
<li>
Any <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> in <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> can be computed by a FSA with few states. <p/>
</li><li>
For any <img alt="{x \neq y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cneq+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x \neq y}"/> binary strings of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, there is an <img alt="{h \in H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh+%5Cin+H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h \in H}"/> so that <img alt="{h(x) \neq h(y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29+%5Cneq+h%28y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h(x) \neq h(y)}"/>.
</li></ol>
<p>
The challenge is to find clever families that can do do both. Be easy to compute and also be able to tell strings apart. Actually this is only a coarse outline—Chase’s situation is a bit more complicated. </p>
<p>
</p><p/><h2> The Proof </h2><p/>
<p/><p>
We have taken the statement of Theorem <a href="https://rjlipton.wordpress.com/feed/#Chasethm">2</a> verbatim from the paper. It has a common pecadillo of beginning a sentence for a specific <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> but writing <img alt="{O(\cdots n \cdots)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Ccdots+n+%5Ccdots%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\cdots n \cdots)}"/> later. However, this is how we think intuitively: in terms of how the pieces of the formula behave. Chase declares right away his intent to ignore the power of <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log n}"/>. How he gets the power <img alt="{1/3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/3}"/> of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is the real point. We can convey the intuition in brief.</p>
<p>
A length-<img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> binary string can be identified with its set <img alt="{A \subseteq [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Csubseteq+%5Bn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \subseteq [n]}"/> of positions where the string has a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. Chase begins by showing how a power of <img alt="{1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/2}"/> on <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is obtainable by considering sets of the form </p>
<p align="center"><img alt="\displaystyle  A_{i,p} = \{j : j \in A \wedge j \equiv i \pmod{p}\}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A_%7Bi%2Cp%7D+%3D+%5C%7Bj+%3A+j+%5Cin+A+%5Cwedge+j+%5Cequiv+i+%5Cpmod%7Bp%7D%5C%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A_{i,p} = \{j : j \in A \wedge j \equiv i \pmod{p}\}, "/></p>
<p>where <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> is prime and <img alt="{i &lt; p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%3C+p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i &lt; p}"/>. Suppose we know a bound <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> such that for all distinct <img alt="{A,B \subseteq n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB+%5Csubseteq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B \subseteq n}"/> (that is, all distinct binary strings of legnth <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>) there is a prime <img alt="{p &lt; k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%3C+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p &lt; k}"/> and <img alt="{i &lt; p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%3C+p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i &lt; p}"/> such that </p>
<p align="center"><img alt="\displaystyle  |A_{i,p}| \neq |B_{i,p}|. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA_%7Bi%2Cp%7D%7C+%5Cneq+%7CB_%7Bi%2Cp%7D%7C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |A_{i,p}| \neq |B_{i,p}|. "/></p>
<p>Then by the Chinese Remainder Theorem, there is a prime <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/> of magnitude about <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log n}"/> such that </p>
<p align="center"><img alt="\displaystyle  |A_{i,p}| \not\equiv |B_{i,p}| \pmod{q}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA_%7Bi%2Cp%7D%7C+%5Cnot%5Cequiv+%7CB_%7Bi%2Cp%7D%7C+%5Cpmod%7Bq%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |A_{i,p}| \not\equiv |B_{i,p}| \pmod{q}. "/></p>
<p>Now we can make a finite automaton <img alt="{M_{A,B}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM_%7BA%2CB%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M_{A,B}}"/> with states <img alt="{(j,\ell)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28j%2C%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(j,\ell)}"/> that always increments <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> modulo <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> and increments <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell}"/> modulo <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/> each time it reads a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> when <img alt="{j \equiv i \pmod{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj+%5Cequiv+i+%5Cpmod%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j \equiv i \pmod{p}}"/>. Then <img alt="{M_{A,B}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM_%7BA%2CB%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M_{A,B}}"/> has order-of <img alt="{pq \approx k\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bpq+%5Capprox+k%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{pq \approx k\log n}"/> states. The finisher is that <img alt="{k = \tilde{O}(n^{1/2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+%5Ctilde%7BO%7D%28n%5E%7B1%2F2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k = \tilde{O}(n^{1/2})}"/> suffices. Again we ignore the pecadillo but we add some redundant words to the statement in the paper between dashes:</p>
<blockquote><p><b>Lemma 3</b> <em> For any distinct <img alt="{A,B \subseteq [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB+%5Csubseteq+%5Bn%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A,B \subseteq [n]}"/>—of size at most <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/>—there is a prime <img alt="{p = \tilde{O}(n^{1/2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%3D+%5Ctilde%7BO%7D%28n%5E%7B1%2F2%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{p = \tilde{O}(n^{1/2})}"/> such that for some <img alt="{i \in [p]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%5Cin+%5Bp%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i \in [p]}"/>, <img alt="{|A_{i,p}| \neq |B_{i,p}|.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CA_%7Bi%2Cp%7D%7C+%5Cneq+%7CB_%7Bi%2Cp%7D%7C.%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{|A_{i,p}| \neq |B_{i,p}|.}"/> </em>
</p></blockquote>
<p/><p>
The power <img alt="{1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/2}"/> is of course weaker than Robson’s <img alt="{2/5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%2F5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2/5}"/>, but this statement conceals two “<a href="https://rjlipton.wordpress.com/2011/08/05/give-me-a-lever/">levers</a>” that enable leap-frogging <img alt="{2/5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%2F5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2/5}"/> to get <img alt="{1/3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/3}"/>. The first is that we don’t have to limit attention to sets <img alt="{A,B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B}"/> that come from places where the corresponding strings <img alt="{x,y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y}"/> have a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. Consider any string <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> and take <img alt="{A_w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_w%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_w}"/> to be the set of index positions <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> in which <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> has the substring <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> beginning at place <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/>. Define <img alt="{B_w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB_w%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B_w}"/> likewise for <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>. Then we can try to prove results of the following form given <img alt="{m &lt; n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3C+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m &lt; n}"/>:</p>
<blockquote><p><b>Proposition 4</b> <em> For all distinct <img alt="{x,y \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x,y \in \{0,1\}^n}"/> there is <img alt="{w \in \{0,1\}^m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw+%5Cin+%5C%7B0%2C1%5C%7D%5Em%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{w \in \{0,1\}^m}"/> such that <img alt="{A_w \neq B_w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_w+%5Cneq+B_w%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A_w \neq B_w}"/> and </em></p><em>
<p align="center"><img alt="\displaystyle  |A_w|,|B_w| = O(\frac{n}{m}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA_w%7C%2C%7CB_w%7C+%3D+O%28%5Cfrac%7Bn%7D%7Bm%7D%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  |A_w|,|B_w| = O(\frac{n}{m}). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
A finite automaton using this extension needs <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> states to store <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> in its finite control. The second lever is to try to prove results of this form, where now the words “of size at most <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/>” are not redundant:</p>
<blockquote><p><b>Lemma 5 (?)</b> <em><a name="conjlemma"/> For any distinct <img alt="{A,B \subseteq [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB+%5Csubseteq+%5Bn%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A,B \subseteq [n]}"/> of size at most <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{N}"/> there is a prime <img alt="{p = \tilde{O}(N^{1/2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%3D+%5Ctilde%7BO%7D%28N%5E%7B1%2F2%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{p = \tilde{O}(N^{1/2})}"/> such that for some <img alt="{i \in [p]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%5Cin+%5Bp%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i \in [p]}"/>, <img alt="{|A_{i,p}| \neq |B_{i,p}|.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CA_%7Bi%2Cp%7D%7C+%5Cneq+%7CB_%7Bi%2Cp%7D%7C.%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{|A_{i,p}| \neq |B_{i,p}|.}"/>  </em> [Update: see note below.]
</p></blockquote>
<p/><p>
Now we need to balance the levers using the proposition and the lemma together.  Since <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> will add order-<img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> states to the automaton, we balance it against <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> from the previous argument.  So take <img alt="{m = n^{1/3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+n%5E%7B1%2F3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m = n^{1/3}}"/>. Then <img alt="{N = \frac{n}{m} \approx n^{2/3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+%5Cfrac%7Bn%7D%7Bm%7D+%5Capprox+n%5E%7B2%2F3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N = \frac{n}{m} \approx n^{2/3}}"/>. Lemma <a href="https://rjlipton.wordpress.com/feed/#conjlemma">5</a> then gives the bound </p>
<p align="center"><img alt="\displaystyle  k = \tilde{O}(N^{1/2}) = \tilde{O}(n^{1/3}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++k+%3D+%5Ctilde%7BO%7D%28N%5E%7B1%2F2%7D%29+%3D+%5Ctilde%7BO%7D%28n%5E%7B1%2F3%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  k = \tilde{O}(N^{1/2}) = \tilde{O}(n^{1/3}) "/></p>
<p>on the magnitude of the needed primes <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/>. This yields the <img alt="{\tilde{O}(n^{1/3})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7BO%7D%28n%5E%7B1%2F3%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tilde{O}(n^{1/3})}"/> breakthrough on SWP.</p>
<p/><p>
Here a famous New Yorker <a href="https://www.allposters.com/-sp/Oh-if-only-it-were-so-simple-New-Yorker-Cartoon-Posters_i9168200_.htm?UPI=PGQEG50&amp;PODConfigID=8419447&amp;sOrigID=169338">cartoon</a> with the caption “If only it were so simple” comes to mind.  But there is a catch. Chase is not quite able to prove lemma <a href="https://rjlipton.wordpress.com/feed/#conjlemma">5</a>. However, the <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> lever comes with extra flexibility that enables finding <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> that make <img alt="{A_w \neq B_w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_w+%5Cneq+B_w%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_w \neq B_w}"/> and also give those sets an extra regularity property <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>. Using <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>, he is able to show the existence of good hash functions of a certain type. The modified lemma is enough to prove his new bound.  The proof still uses intricate analysis including integrals.</p>
<p>
This is classic high-power mathematics. When some idea is blocked, try to weaken the requirements. Sometimes it is possible to still proceed. It is a lesson that we sometimes forget, but a valuable one nevertheless.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We like the SWP and think Chase’s contribution is impressive. Note that it adds a third element <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> to <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> and <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/> in the automaton.  Can the argument be pushed further by finding more levers to add more elements?  Is Lemma 5 true as stated, and with what (other) tradeoffs of <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> and <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> between it and Proposition 4?  [<b>Update</b>: not for extreme tradeoffs—see this <a href="https://rjlipton.wordpress.com/2020/08/03/cleverer-automata-exist/#comment-111915">comment</a>—but plausibly for <img alt="{m,n,N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%2Cn%2CN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m,n,N}"/> polynomially related.]</p>
<p>
We feel there could also be interesting applications for his theorem as it stands. Is the ability to tell two strings apart with a simple device—a FSA with not many states—useful? Could it solve some open problem? It does seem like a basic insight, yet we have no candidate application. Perhaps you have an idea. </p>
<p/><p><br/>
[added Q on Lemma 5 to “Open Problems”, “lower” bound –&gt; “upper” bound in third section, update in Open Problems.]</p></font></font></div>
    </content>
    <updated>2020-08-03T14:50:54Z</updated>
    <published>2020-08-03T14:50:54Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="primes"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="trick"/>
    <category term="Ben Green"/>
    <category term="estimates"/>
    <category term="finite automata"/>
    <category term="hash functions"/>
    <category term="levers"/>
    <category term="log factors"/>
    <category term="Oxford"/>
    <category term="separating words problem"/>
    <category term="SWP"/>
    <category term="Zachary Chase"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-08-16T16:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsmath.wordpress.com/?p=2293</id>
    <link href="https://tcsmath.wordpress.com/2020/08/02/itcs-2021-call-for-papers/" rel="alternate" type="text/html"/>
    <title>ITCS 2021 Call for Papers</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The 12th Innovations in Theoretical Computer Science (ITCS) conference will be held online from January 6-8, 2021. The submission deadline is September 7, 2020. The program committee encourages you to send your papers our way! See the call for papers for information about submitting to the conference. ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing … <a class="more-link" href="https://tcsmath.wordpress.com/2020/08/02/itcs-2021-call-for-papers/">Continue reading <span class="screen-reader-text">ITCS 2021 Call for Papers</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <strong>12th Innovations in Theoretical Computer Science (ITCS)</strong> conference will be held <strong>online</strong> from <strong>January 6-8, 2021</strong>.   The <strong>submission deadline</strong> is <strong>September 7, 2020</strong>.</p>



<p>The <a href="http://itcs-conf.org/">program committee</a> encourages you to send your papers our way!  See the <a href="http://itcs-conf.org/">call for papers</a> for information about submitting to the conference.</p>



<p>ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the greater theory community.</p>



<p/>



<h3>Important dates</h3>



<ul><li><strong>Submission deadline: </strong> September 7, 2020 (05:59PM PDT) </li><li><strong>Notification to authors:</strong> November 1, 2020</li><li><strong>Conference dates: </strong>January 6-8, 2021</li></ul>



<p/>



<p/></div>
    </content>
    <updated>2020-08-03T02:06:40Z</updated>
    <published>2020-08-03T02:06:40Z</published>
    <category term="Announcement"/>
    <category term="ITCS"/>
    <category term="theory conference"/>
    <author>
      <name>James</name>
    </author>
    <source>
      <id>https://tcsmath.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsmath.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsmath.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsmath.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsmath.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>some mathematics &amp; computation</subtitle>
      <title>tcs math</title>
      <updated>2020-08-16T16:20:20Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1233384953933252805</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1233384953933252805/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/the-gauss-story-is-false-yet-we-still.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1233384953933252805" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1233384953933252805" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/the-gauss-story-is-false-yet-we-still.html" rel="alternate" type="text/html"/>
    <title>The Gauss story is false yet we still tell it. Should we?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
When teaching discrete math a while back I told the following story which some had already heard in High School: <br/>
<i><br/></i>
<i>When Gauss was in 1st grade the class was being bad. So the teacher made them sit down and add up the numbers from 1 to 100. Gauss did it in 2 minutes by noting that if S was the answer then </i><br/>
<i><br/></i>
<i>2S = (100+1) +(99+2) + ... + (1 + 100) = 100*101</i><br/>
<i><br/></i>
<i>So S = 50*101.  Then he went to Google and typed in 50*101 for the answer.</i><br/>
<i><br/></i>
The class laughed because of course the last part about Google was false. But I then told them that the entire story was false and showed them the following slides:  <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/gaussstory.pdf">here</a>  Take a look at them (there are only 4 of them) before reading on.<div><br/></div><div>(ADDED LATER: here is an article by Brian Hayes that documents the history of the story.</div><div><br/></div><div><a href="http://bit-player.org/wp-content/extras/bph-publications/AmSci-2006-05-Hayes-Gauss.pdf" target="_blank">http://bit-player.org/wp-content/extras/bph-publications/AmSci-2006-05-Hayes-Gauss.pdf</a></div><div><br/></div><div>)<br/>
<br/>
<br/>
So I told them the Gauss Story was false (I am right about this) and then told them a lie- that the story's progression over time was orderly. I then told them that that was false (hmmm- actually I might not of, oh well).<br/>
<br/>
One of my students emailed me this semester<br/>
<br/>
<i>Dr Gasarch- one of my Math professors is telling the Gauss story as if its true! You should make a public service announcement and tell people its false!</i><div><i><br/></i></div><div>I do not think this is needed. I also don't know how one goes about making a public service announcement  I also  suspect the teacher knew it was false but told it anyway.<div>
<br/>
OKAY- what do you do if you have a nice story that has some good MATH in it but  its not true?<br/>
<br/><b>Options:</b></div><div><br/></div><div>Tell it and let the students think its true.</div><div><br/></div><div>Tell it and debunk it.</div><div><br/></div><div>Tell it and debunk it and tell another myth</div><div><br/></div><div>Tell it and debunk it and tell another myth and then debunk that</div><div><br/></div><div>Ask your readers what they would do. Which I do now: What do you do? <br/><br/></div></div></div></div>
    </content>
    <updated>2020-08-03T02:02:00Z</updated>
    <published>2020-08-03T02:02:00Z</published>
    <author>
      <name>Unknown</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-08-16T08:52:25Z</updated>
    </source>
  </entry>
</feed>
