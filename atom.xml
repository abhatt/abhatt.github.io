<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-08-26T03:39:09Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-542741463481169013</id>
    <link href="http://blog.computationalcomplexity.org/feeds/542741463481169013/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/the-long-road.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/542741463481169013" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/542741463481169013" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/the-long-road.html" rel="alternate" type="text/html"/>
    <title>The Long Road</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><i>Guest blogger Varsha Dani tells us why it's never too late.</i></p><p>This week, I am starting as an Assistant Professor at RIT and I am super excited about it. What's the big deal, you are probably thinking. Don't lots of people get hired in tenure track positions every year? Sure. But the difference in my case is that I got my Ph.D. in 2008. </p><div>Why didn't I look for a position right away? There were a number of reasons. I was burned out. I was going to have a baby. The job market was not particularly good that year. I would have had a two-body problem. And most importantly, I thought it was just going to be a short break. I thought that there was no rush...</div><div><br/></div><div>What did I do in those intervening years? A lot of things. I spent a lot of time with my kids, including part home-schooling them for some time. I found some new interests, both in Math and CS and outside. I found new collaborators and did some research in new areas, just for fun. Sometimes I was funded for it through grants, but mostly I wasn't. I wrote a lot of Computer Science and some math materials for Brilliant.org. I organized math clubs at my kids' elementary and middle schools. I hiked. I wrote <a href="https://blog.computationalcomplexity.org/2020/06/on-chain-letters-and-pandemics.html">poetry</a>. I never intended nearly 13 years to go by, but somehow they did.  At some point I remembered that I had meant to take a short break, not to give up on being an academic altogether. But by then it seemed too late. At the beginning of my meant-to-be-short hiatus, I used to jokingly refer to myself as a "scholar at large" but by the time a decade had gone by I had started to feel extremely isolated and being a scholar for its own sake was not something to joke about anymore.  Each year I rolled the job-search dice, but with each passing year it seemed more and more futile to do so, and more and more of an imposition to ask people to write recommendations for me. And then, out of the blue, last year, I found a whole community of independent researchers who, like me, were pursuing their scholarly interests despite not being employed to do so, and who felt unapologetically unembarrassed, even proud of it.  And, even more out of the blue, this year I got an offer. And now, this Fall, I am actually doing it. I am actually an academic! To be honest, I feel more than a little trepidation about it, mixed in with the excitement. </div><div><br/></div><div>So why am I telling you this? Partly to celebrate. Partly to publicly thank my spouse, who has been extremely supportive of all my decisions (and all my actions that were born of indecision) over many years. Partly to give a shout-out to the wonderful folks at the <a href="http://ronininstitute.org/">Ronin Institute</a> who helped me remember that we do science (and computer science) because we love it, not because it pays the bills. Ironically, I believe I needed to be reminded of that before I could get a job! But mostly, I'm writing this to reach out to anyone out there who thinks that their decisions have led to a one-way street they no longer want to be on. It may be hard, and there are, of course, no guarantees, but you won't know whether you can turn around, unless you try. </div></div>
    </content>
    <updated>2021-08-25T20:07:00Z</updated>
    <published>2021-08-25T20:07:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-25T20:33:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19054</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/08/24/a-negative-comment-on-negations/" rel="alternate" type="text/html"/>
    <title>A Negative Comment on Negations</title>
    <summary>Always turn a negative situation into a positive situation—Michael Jordan (MJ) src Michael I. Jordan of the University of California, Berkeley, is a pioneer of AI that few outside of his field recognize. “He’s known as the Michael Jordan of machine learning,” quips Oren Etzioni, director of the Seattle-based Allen Institute. Today, Ken and I […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Always turn a negative situation into a positive situation—Michael Jordan (<a href="https://www.azquotes.com/quote/150625">MJ</a>)</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/08/24/a-negative-comment-on-negations/mij/" rel="attachment wp-att-19056"><img alt="" class="alignright wp-image-19056" height="135" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/08/mij.png?resize=135%2C135&amp;ssl=1" width="135"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://ds.cs.umass.edu/external-advisory-board">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Michael I. Jordan of the University of California, Berkeley, is a pioneer of AI that few outside of his field recognize. “He’s known as the Michael Jordan of machine learning,” quips Oren Etzioni, director of the Seattle-based Allen Institute.</p>
<p>
Today, Ken and I discuss some linkage between machine learning and computational complexity theory.</p>
<p>
The quip by Etzioni runs longer—see <a href="https://www.sciencemag.org/news/2016/04/who-s-michael-jordan-computer-science-new-tool-ranks-researchers-influence">this</a>. The Allen Institute’s <a href="https://www.semanticscholar.org/">Semantic Scholar</a> project aims to achieve content-based associative search for academic publications, so one can search for X that are close to a topic Y you have defined. They created an “influence graph” for millions of academic publications that goes well beyond citation counts. The graph’s most influential computer science paper was one by Jordan.</p>
<p>
Neither Jordan has ever worked in complexity theory. Somehow, I thought the above is relevant to complexity theory. It got me thinking about MIJ and his connection to machine learning (ML). This is connected to complexity theory again, and so <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> </p>
<p>
My thoughts converged on how the notion of <em>monotone</em> plays in both fields. Influence strikes us as naturally monotone—“negative influence” is still influence—and building the graph is a kind of positively weighted transitive closure. Many ML algorithms work by monotonically increasing an objective function, such as <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm">expectation-maximization</a> as expounded by MIJ. Non-monotone components such as negative (inhibiting) neural links and <a href="http://www.optimization-online.org/DB_FILE/2014/08/4488.pdf">line</a>–<a href="https://www.caam.rice.edu/~yzhang/caam565/L1_reg/BB-step.pdf">search</a> add-ons to gradient descent are employed, but when and to what extent are they critical? We think negation is critical in complexity theory, but maybe not so much. Here are my thoughts.</p>
<p>
</p><p/><h2> Upper Bounds and Lower Bounds </h2><p/>
<p/><p>
<a href="https://rjlipton.wpcomstaging.com/2021/08/24/a-negative-comment-on-negations/easy/" rel="attachment wp-att-19057"><img alt="" class="aligncenter wp-image-19057" height="252" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/08/easy.png?resize=338%2C252&amp;ssl=1" width="338"/></a></p>
<p>
Complexity theory is all about how hard is it to solve some problem. How hard is it to decide if a number is a prime? How hard is it to find a factor of a composite number? How hard is to color a graph with three colors? And so on.</p>
<p>
There are two issues: </p>
<p>
<b>Upper Bounds</b> Find an algorithm for the problem that is more efficient in some manner. Often in number of steps, but can be in other measures of complexity like memory used or amount of randomness and so on.</p>
<p>
<b>Lower Bounds</b> Show that no algorithm for the problem is more efficient in some manner. For example, show that no algorithm can solve the problem is fewer than a certain number of steps.</p>
<p>
The first is an <img alt="{\exists}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cexists%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> problem; the second is a <img alt="{\forall}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cforall%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. An upper bound is proving there exists an algorithm that is efficient enough. You job is to present an algorithm that solves the problem. A lower bound is proving all algorithms that solve the problem, must take more steps than allowed. </p>
<p>
There is something about <img alt="{\exists}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cexists%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that is fundamentally simpler than <img alt="{\forall}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cforall%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. An upper bound requires you to design an algorithm that works in some clever manner. The algorithm must exploit some trick about the problem that is powerful. For example to decide if a number is a prime, do not try all possible factors. Rather use a key consequence of a number being a prime: use the value of the number raised to a high power. </p>
<p>
The lower bound requires you to show that there is no algorithm that operates more efficiently that a certain bound. You must show that being too fast, for example, makes you wrong.</p>
<p>
</p><p/><h2> Negations </h2><p/>
<p/><p>
One approach to lower bounds is via Boolean complexity. The idea is that any computation can be done with <img alt="{AND}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAND%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{OR}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BOR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="{NOT}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BNOT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. But, if we leave out <img alt="{NOT}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BNOT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, some computations cannot be done. However: </p>
<p>
Suppose that you wish to show that something cannot be computed efficiently. An obvious first step is to explain what operations are needed to compute it at all. Once you fix on operations that are enough, you can then focus on the question of whether the given operations can <em>efficiently</em> compute it.</p>
<p>
Suppose you wish to show that it will take a long time to get from A to another location B. How can you prove this? You might measure how long the distance is between A and B. Then if you know that you must drive a car from A to B, a lower bound is simple: How fast can the car go at maximum and argue that the time is therefore at least </p>
<p align="center"><img alt="\displaystyle  \frac{\mathit{Distance}(A,B)}{M}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5Cmathit%7BDistance%7D%28A%2CB%29%7D%7BM%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Here M is the maximum speed the car goes.</p>
<p>
Suppose that Distance is <img alt="{100}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B100%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <i>miles</i> and the car can go at most <img alt="{40}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B40%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> miles per hour. Then a simple calculation is that it must take at least  	</p>
<p align="center"><img alt="\displaystyle  \frac{100}{40} = 2.5 \text{ hours.}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B100%7D%7B40%7D+%3D+2.5+%5Ctext%7B+hours.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Of course it could be higher if sometimes you go slower—perhaps you have to stop now and then for traffic lights. 	</p>
<p>
</p><p/><h2> Monotone Circuits </h2><p/>
<p/><p>
There are many theorems like <a href="https://www.cs.cornell.edu/~eva/Gap.Between.Monotone.NonMonotone.Circuit.Complexity.is.Exponential.pdf">this</a>: </p>
<blockquote><p><b>Theorem 1</b> <em> Any monotone circuit that computes <img alt="{F(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28X%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> correctly has size at least exponential in <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> . </em>
</p></blockquote>
<p/><p>
These theorems are often hard to prove and require clever methods, but there are now tons of them. The field of complexity continues to extend and refine these theorems.</p>
<p>
This feeds in to the approach of showing <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> via Boolean circuit complexity. That is, to show that for some particular <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-complete problem, any Boolean computation for it requires a super-polynomial number of gates. A big idea is to build off the monotone lower bound results by moderately adding negation. </p>
<p>
But there is a big issue: it takes only order <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> negations to make the results fail. We <a href="https://rjlipton.wpcomstaging.com/2017/08/17/on-the-edge-of-eclipses-and-pnp/">covered</a> some recent <a href="https://rjlipton.wpcomstaging.com/2020/06/16/pnp/">attempts</a> by Norbert Blum to make this kind of idea work, but they do not seem to have established a new <a href="https://rjlipton.wpcomstaging.com/2021/08/10/p-vs-np-proof-claims/">base camp</a> for this approach, beyond <a href="https://link.springer.com/article/10.1007/BF01978380">previously</a>–<a href="http://people.cs.uchicago.edu/~razborov/files/clique.pdf">known</a> <a href="https://www.sciencedirect.com/science/article/pii/0304397586901222">results</a> and <a href="file://C:/Users/KWRegan/Downloads/TR16-188.pdf">more</a>–<a href="https://theoryofcomputing.org/articles/v016a013/">recent</a> <a href="https://sol.sbc.org.br/index.php/ctd/article/view/15761">advances</a>.</p>
<p>
This failure led many to think that negations are somehow the “final frontier.” But we do not agree with this either. We feel that negations are not the critical issue.</p>
<p>
</p><p/><h2> The Slice Issue—Again </h2><p/>
<p/><p>
The key is the concept of <em>slice</em>. A slice of <img alt="{\{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a subset <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that has all the elements the same weight. That is, its member binary strings have the same number of <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>s.</p>
<p>
There are many theorems like <a href="https://epubs.siam.org/doi/abs/10.1137/0215037?journalCode=smjcat">this</a>:</p>
<blockquote><p><b>Theorem 2</b> <em> Any monotone circuit that computes <img alt="{F(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28X%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> correctly has size polynomial in <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Provided it only need work on some slice of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </em>
</p></blockquote>
<p/><p>
For a simple insight, suppose we want to say that a given bit <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of a string <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. If we are promised that <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> belongs to a slice with <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>s, then we can do this with a monotone function: one saying that bits <img alt="{1\dots i-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%5Cdots+i-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{i+1\dots n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%2B1%5Cdots+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> collectively have at least <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>s. </p>
<p>
For an example of how slices show that negations cannot be the issue, consider factoring of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> bit numbers. This requires perhaps super polynomial size circuits. But note that if we fix factoring to <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-bit numbers that have <img alt="{n/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>‘s, say, then it becomes a function on a slice. And it is easy to see that a computation without any negations is at most order <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> bigger. Thus negations cannot be the secret to factoring. This is the key insight.</p>
<p>
We still have not said anything new. This insight says that strong lower bounds on monotone complexity for slice functions will give lower bounds for general circuits. But we still do not know how to get such bounds. We have shifted around the side of the mountain, but the resulting “Slice Gap” seems to block ascent as before.</p>
<p>
</p><p/><h2> Climbing Mount ML </h2><p/>
<p/><p>
This is what has me thinking about whether machine learning can supply new insight. </p>
<p>
Here is a simple old story of what I mean. One of the limits on the simple neural model of <a href="https://en.wikipedia.org/wiki/Perceptron">perceptrons</a> that was shown a half-century ago by Marvin Minsky and Seymour Papert is that they cannot compute the binary XOR function. This <a href="https://en.wikipedia.org/wiki/Perceptrons_(book)#The_XOR_affair">led</a> people to expect that small assemblages of perceptrons would be similarly powerless. But it only takes three layers of perceptrons to compute XOR. The point is, a small control structure over monotone can replace the free use of negation.</p>
<p>
To go deeper—into deep learning for instance—we’ve peeked at the <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">book</a> <em>Understanding Machine Learning: From Theory to Algorithms</em> by Shai Shalev-Shwartz and Shai Ben-David. The latter we know for some <a href="https://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/1991/CS/CS0699.revised.pdf">work</a> on Boolean circuits and P=NP. The nub is more easily approached via a 2014 <a href="http://lear.inrialpes.fr/workshop/osl2015/slides/osl2015_shalev_shwartz.pdf">talk</a> by the former, titled “On the Computational Complexity of Deep Learning,” and his 2016 <a href="https://www.ima.umn.edu/2015-2016/SW5.16-19.16/24770">followup</a>. One point is that “theoretically weak” structures perform well when requirements of worst-case correctness and realizability are lifted—and the details make us wonder about possible parallels to the case of slices.  The point also prompts us to ask:</p>
<blockquote><p>
What control structures, or intermediate operations short of full negation, create practically powerful algorithms that are still theoretically nearly as weak as monotone?  Can researching them help us close the gap on lower bounds?
</p></blockquote>
<p>
To go wider, it seems that both we humans and ML algorithms perform well at many tasks using largely monotone components. Can they help give an account relevant to complexity of what power necessarily comes from non-monotone components? We note caveats expressed by MIJ in a much-noted 2014 <a href="https://spectrum.ieee.org/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts">interview</a> for <em>IEEE Spectrum</em> about generalizing from brain function to ML practice, but our intent is to look for particulars.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Can machine learning help us understand the power of monotone functions?  </p>
<p/></font></font></div>
    </content>
    <updated>2021-08-25T02:07:10Z</updated>
    <published>2021-08-25T02:07:10Z</published>
    <category term="algorithms"/>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="P=NP"/>
    <category term="circuits"/>
    <category term="complexity"/>
    <category term="deep learning"/>
    <category term="lower bounds"/>
    <category term="machine learning"/>
    <category term="Michael I. Jordan"/>
    <category term="monotone"/>
    <category term="slice"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-08-26T03:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.10865</id>
    <link href="http://arxiv.org/abs/2108.10865" rel="alternate" type="text/html"/>
    <title>On Specialization of a Program Model of Naive Pattern Matching in Strings (Extended Abstract)</title>
    <feedworld_mtime>1629849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nemytykh:Andrei_P=.html">Andrei P. Nemytykh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.10865">PDF</a><br/><b>Abstract: </b>We have proved that for any pattern p the tail recursive program model of
naive pattern matching may be automatically specialized w.r.t. the pattern p to
a specialized version of the so-called KMP-algorithm, using the Higman-Kruskal
relation that controls the unfolding/folding. Given an input string, the
corresponding residual program finds the first occurrence of p in the string in
linear time on the string length. The current state of the automated program
specialization art based on unfolding/folding is too weak in order to be able
to reproduce the proof, done by hands, of the uniform property above, while it
known before that program specialization is sometimes able to produce the
KMP-algorithm for a few concrete static patterns.
</p></div>
    </summary>
    <updated>2021-08-25T22:44:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.10808</id>
    <link href="http://arxiv.org/abs/2108.10808" rel="alternate" type="text/html"/>
    <title>Greenformers: Improving Computation and Memory Efficiency in Transformer Models via Low-Rank Approximation</title>
    <feedworld_mtime>1629849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cahyawijaya:Samuel.html">Samuel Cahyawijaya</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.10808">PDF</a><br/><b>Abstract: </b>In this thesis, we introduce Greenformers, a collection of model efficiency
methods to improve the model efficiency of the recently renowned transformer
models with a low-rank approximation approach. The development trend of deep
learning models tends to results in a more complex and larger model. Although
it leads to a better and more accurate prediction, the resulting model becomes
even more costly, as it requires weeks of training with a huge amount of GPU
resources. Particularly, the size and computational cost of transformer-based
models have increased tremendously since its first debut in 2017 from ~100
million parameters up to ~1.6 trillion parameters in early 2021. This
computationally hungry model also incurs a substantial cost to the environment
and even reaches an alarming level of carbon footprint. Some of these models
are so massive that it is even impossible to run the model without a GPU
cluster.
</p>
<p>Greenformers improve the model efficiency of transformer models by applying
low-rank approximation approaches. Specifically, we propose a low-rank
factorization approach to improve the efficiency of the transformer model
called Low-Rank Transformer. We further compare our model with an existing
low-rank factorization approach called Linformer. Based on our analysis, the
Low-Rank Transformer model is suitable for improving both the time and memory
efficiency in processing short-sequence (&lt;= 512) input data, while the
Linformer model is suitable for improving the efficiency in processing
long-sequence input data (&gt;= 512). We also show that Low-Rank Transformer is
more suitable for on-device deployment, as it significantly reduces the model
size. Additionally, we estimate that applying LRT to the existing BERT-base
model can significantly reduce the computational, economical, and environmental
costs for developing such models by more than 30% of its original costs.
</p></div>
    </summary>
    <updated>2021-08-25T22:39:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-08-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.10790</id>
    <link href="http://arxiv.org/abs/2108.10790" rel="alternate" type="text/html"/>
    <title>Consistent Simplification of Polyline Tree Bundles</title>
    <feedworld_mtime>1629849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Yannick Bosch, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sch=auml=fer:Peter.html">Peter Schäfer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Spoerhase:Joachim.html">Joachim Spoerhase</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Storandt:Sabine.html">Sabine Storandt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zink:Johannes.html">Johannes Zink</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.10790">PDF</a><br/><b>Abstract: </b>The Polyline Bundle Simplification (PBS) problem is a generalization of the
classical polyline simplification problem. Given a set of polylines, which may
share line segments and points, PBS asks for the smallest consistent
simplification of these polylines with respect to a given distance threshold.
Here, consistent means that each point is either kept in or discarded from all
polylines containing it. In previous work, it was proven that PBS is NP-hard to
approximate within a factor of $n^{\frac{1}{3}-\varepsilon}$ for any
$\varepsilon &gt; 0$ where $n$ denotes the number of points in the input. This
hardness result holds even for two polylines. In this paper we first study the
practically relevant setting of planar inputs. While for many combinatorial
optimization problems the restriction to planar settings makes the problem
substantially easier, we show that the inapproximability bound known for
general inputs continues to hold even for planar inputs. We proceed with the
interesting special case of PBS where the polylines form a rooted tree. Such
tree bundles naturally arise in the context of movement data visualization. We
prove that optimal simplifications of these tree bundles can be computed in
$O(n^3)$ for the Fr\'echet distance and in $O(n^2)$ for the Hausdorff distance
(which both match the computation time for single polylines). Furthermore, we
present a greedy heuristic that allows to decompose polyline bundles into tree
bundles in order to make our exact algorithm for trees useful on general
inputs. The applicability of our approaches is demonstrated in an experimental
evaluation on real-world data.
</p></div>
    </summary>
    <updated>2021-08-25T22:46:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-08-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.10784</id>
    <link href="http://arxiv.org/abs/2108.10784" rel="alternate" type="text/html"/>
    <title>Quasi-upward Planar Drawings with Minimum Curve Complexity</title>
    <feedworld_mtime>1629849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Binucci:Carla.html">Carla Binucci</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giacomo:Emilio_Di.html">Emilio Di Giacomo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liotta:Giuseppe.html">Giuseppe Liotta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tappini:Alessandra.html">Alessandra Tappini</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.10784">PDF</a><br/><b>Abstract: </b>This paper studies the problem of computing quasi-upward planar drawings of
bimodal plane digraphs with minimum curve complexity, i.e., drawings such that
the maximum number of bends per edge is minimized. We prove that every bimodal
plane digraph admits a quasi-upward planar drawing with curve complexity two,
which is worst-case optimal. We also show that the problem of minimizing the
curve complexity in a quasi-upward planar drawing can be modeled as a min-cost
flow problem on a unit-capacity planar flow network. This gives rise to an
$\tilde{O}(m^\frac{4}{3})$-time algorithm that computes a quasi-upward planar
drawing with minimum curve complexity; in addition, the drawing has the minimum
number of bends when no edge can be bent more than twice. For a contrast, we
show bimodal planar digraphs whose bend-minimum quasi-upward planar drawings
require linear curve complexity even in the variable embedding setting.
</p></div>
    </summary>
    <updated>2021-08-25T22:47:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-08-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.10779</id>
    <link href="http://arxiv.org/abs/2108.10779" rel="alternate" type="text/html"/>
    <title>Randomized C/C++ dynamic memory allocator</title>
    <feedworld_mtime>1629849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Irina Aleksandrovna Astrakhantseva, Roman Gennadevich Astrakhantsev, Arseny Viktorovich Mitin <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.10779">PDF</a><br/><b>Abstract: </b>Dynamic memory management requires special attention in programming. It
should be fast and secure at the same time. This paper proposes a new
randomized dynamic memory management algorithm designed to meet these
requirements. Randomization is a key feature intended to protect applications
from "use-after-free" or similar attacks. At the same time, the state in the
algorithm consists only of one pointer, so it does not consume extra memory for
itself. However, our algorithm is not a universal solution. It does not solve
the memory fragmentation problem and it needs further development and testing.
</p></div>
    </summary>
    <updated>2021-08-25T22:39:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.10776</id>
    <link href="http://arxiv.org/abs/2108.10776" rel="alternate" type="text/html"/>
    <title>Succinct Data Structures for Series-Parallel, Block-Cactus and 3-Leaf Power Graphs</title>
    <feedworld_mtime>1629849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakraborty:Sankardeep.html">Sankardeep Chakraborty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jo:Seungbum.html">Seungbum Jo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sadakane:Kunihiko.html">Kunihiko Sadakane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Satti:Srinivasa_Rao.html">Srinivasa Rao Satti</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.10776">PDF</a><br/><b>Abstract: </b>We design succinct encodings of {\it series-parallel, block-cactus} and {\it
3-leaf power} graphs while supporting the basic navigational queries such as
degree, adjacency and neighborhood {\it optimally} in the RAM model with
logarithmic word size. One salient feature of our representation is that it can
achieve optimal space even though the exact space lower bound for these graph
classes is not known. For these graph classes, we provide succinct data
structures with optimal query support for the first time in the literature. For
series-parallel multigraphs, our work also extends the works of Uno et al.
(Disc. Math. Alg. and Appl., 2013) and Blelloch and Farzan (CPM, 2010) to
produce optimal bounds.
</p></div>
    </summary>
    <updated>2021-08-25T22:39:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.10711</id>
    <link href="http://arxiv.org/abs/2108.10711" rel="alternate" type="text/html"/>
    <title>Layered Area-Proportional Rectangle Contact Representations</title>
    <feedworld_mtime>1629849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/N=ouml=llenburg:Martin.html">Martin Nöllenburg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Villedieu:Ana=iuml=s.html">Anaïs Villedieu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wulms:Jules.html">Jules Wulms</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.10711">PDF</a><br/><b>Abstract: </b>We investigate two optimization problems on area-proportional rectangle
contact representations for layered, embedded planar graphs. The vertices are
represented as interior-disjoint unit-height rectangles of prescribed widths,
grouped in one row per layer, and each edge is ideally realized as a rectangle
contact of positive length. Such rectangle contact representations find
applications in semantic word or tag cloud visualizations, where a collection
of words is displayed such that pairs of semantically related words are close
to each other. In this paper, we want to maximize the number of realized
rectangle contacts or minimize the overall area of the rectangle contact
representation, while avoiding any false adjacencies. We present a network flow
model for area minimization, a linear-time algorithm for contact maximization
of two-layer graphs, and an ILP model for maximizing contacts of $k$-layer
graphs.
</p></div>
    </summary>
    <updated>2021-08-25T22:44:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-08-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.10573</id>
    <link href="http://arxiv.org/abs/2108.10573" rel="alternate" type="text/html"/>
    <title>The staircase property: How hierarchical structure can guide deep learning</title>
    <feedworld_mtime>1629849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abbe:Emmanuel.html">Emmanuel Abbe</a>, Enric Boix-Adsera, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brennan:Matthew.html">Matthew Brennan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bresler:Guy.html">Guy Bresler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nagaraj:Dheeraj.html">Dheeraj Nagaraj</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.10573">PDF</a><br/><b>Abstract: </b>This paper identifies a structural property of data distributions that
enables deep neural networks to learn hierarchically. We define the "staircase"
property for functions over the Boolean hypercube, which posits that high-order
Fourier coefficients are reachable from lower-order Fourier coefficients along
increasing chains. We prove that functions satisfying this property can be
learned in polynomial time using layerwise stochastic coordinate descent on
regular neural networks -- a class of network architectures and initializations
that have homogeneity properties. Our analysis shows that for such staircase
functions and neural networks, the gradient-based algorithm learns high-level
features by greedily combining lower-level features along the depth of the
network. We further back our theoretical results with experiments showing that
staircase functions are also learnable by more standard ResNet architectures
with stochastic gradient descent. Both the theoretical and experimental results
support the fact that staircase properties have a role to play in understanding
the capabilities of gradient-based learning on regular networks, in contrast to
general polynomial-size networks that can emulate any SQ or PAC algorithms as
recently shown.
</p></div>
    </summary>
    <updated>2021-08-25T22:42:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.10547</id>
    <link href="http://arxiv.org/abs/2108.10547" rel="alternate" type="text/html"/>
    <title>The complexity of testing all properties of planar graphs, and the role of isomorphism</title>
    <feedworld_mtime>1629849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Basu:Sabyasachi.html">Sabyasachi Basu</a>, Akash Kumar, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seshadhri:C=.html">C. Seshadhri</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.10547">PDF</a><br/><b>Abstract: </b>Consider property testing on bounded degree graphs and let $\varepsilon&gt;0$
denote the proximity parameter. A remarkable theorem of Newman-Sohler (SICOMP
2013) asserts that all properties of planar graphs (more generally hyperfinite)
are testable with query complexity only depending on $\varepsilon$. Recent
advances in testing minor-freeness have proven that all additive and monotone
properties of planar graphs can be tested in $poly(\varepsilon^{-1})$ queries.
Some properties falling outside this class, such as Hamiltonicity, also have a
similar complexity for planar graphs. Motivated by these results, we ask: can
all properties of planar graphs can be tested in $poly(\varepsilon^{-1})$
queries? Is there a uniform query complexity upper bound for all planar
properties, and what is the "hardest" such property to test? We discover a
surprisingly clean and optimal answer. Any property of bounded degree planar
graphs can be tested in $\exp(O(\varepsilon^{-2}))$ queries. Moreover, there is
a matching lower bound, up to constant factors in the exponent. The natural
property of testing isomorphism to a fixed graph needs
$\exp(\Omega(\varepsilon^{-2}))$ queries, thereby showing that (up to
polynomial dependencies) isomorphism to an explicit fixed graph is the hardest
property of planar graphs. The upper bound is a straightforward adapation of
the Newman-Sohler analysis that tracks dependencies on $\varepsilon$ carefully.
The main technical contribution is the lower bound construction, which is
achieved by a special family of planar graphs that are all mutually far from
each other. We can also apply our techniques to get analogous results for
bounded treewidth graphs. We prove that all properties of bounded treewidth
graphs can be tested in $\exp(O(\varepsilon^{-1}\log \varepsilon^{-1}))$
queries. Moreover, testing isomorphism to a fixed forest requires
$\exp(\Omega(\varepsilon^{-1}))$ queries.
</p></div>
    </summary>
    <updated>2021-08-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.10461</id>
    <link href="http://arxiv.org/abs/2108.10461" rel="alternate" type="text/html"/>
    <title>Improving update times of dynamic matching algorithms from amortized to worst case</title>
    <feedworld_mtime>1629849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kiss:Peter.html">Peter Kiss</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.10461">PDF</a><br/><b>Abstract: </b>We present an algorithm for maintaining a $3/2 + \epsilon$-approximate
maximum matching on fully dynamic graphs in deterministic worst-case update
time $\hat{O}(\sqrt{n})$. In order to derive the result we introduce a
relaxation of the EDCS matching sparsifier which first appeared in
<a href="http://export.arxiv.org/abs/1506.07076">arXiv:1506.07076</a> called 'damaged EDCS'. We also present a deterministic near
linear time algorithm for constructing a damaged EDCS. We also explicitly
present a black-box framework for improving amortized rebuild based dynamic
algorithm's update time to worst-case which was first implicitly shown by
<a href="http://export.arxiv.org/abs/2004.08432">arXiv:2004.08432</a>. Further we show new deterministic and randomized (against
adaptive adversary) reductions from $\alpha$-approximate dynamic matching
algorithms to $(\alpha, \delta)$-approximate matching algorithms with
$\hat{O}(1)$ and $\tilde{O}(1)$ blowup in update time respectively.
</p></div>
    </summary>
    <updated>2021-08-25T22:41:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.10398</id>
    <link href="http://arxiv.org/abs/2108.10398" rel="alternate" type="text/html"/>
    <title>Approximation and parameterized algorithms to find balanced connected partitions of graphs</title>
    <feedworld_mtime>1629849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moura:Phablo_F=_S=.html">Phablo F. S. Moura</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ota:Matheus_J=.html">Matheus J. Ota</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wakabayashi:Yoshiko.html">Yoshiko Wakabayashi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.10398">PDF</a><br/><b>Abstract: </b>Partitioning a connected graph into $k$~vertex-disjoint connected subgraphs
of similar (or given) orders is a classical problem that has been intensively
investigated since late seventies. Given a connected graph $G=(V,E)$ and a
weight function $w : V \to \mathbb{Q}_\geq$, a connected $k$-partition of $G$
is a partition of $V$ such that each class induces a connected subgraph. The
balanced connected $k$-partition problem consists in finding a connected
$k$-partition in which every class has roughly the same weight. To model this
concept of balance, one may seek connected $k$-partitions that either maximize
the weight of a lightest class $(\text{max-min BCP}_k)$ or minimize the weight
of a heaviest class $(\text{min-max BCP}_k)$. Such problems are equivalent when
$k=2$, but they are different when $k\geq 3$. In this work, we propose a simple
pseudo-polynomial $\frac{k}{2}$-approximation algorithm for $\text{min-max
BCP}_k$ which runs in time $\mathcal{O}(W|V||E|)$, where $W = \sum_{v \in V}
w(v)$. Based on this algorithm and using a scaling technique, we design a
(polynomial) $(\frac{k}{2} +\varepsilon)$-approximation for the same problem
with running-time $\mathcal{O}(|V|^3|E|/\varepsilon)$, for any fixed
$\varepsilon&gt;0$. Additionally, we propose a fixed-parameter tractable algorithm
based on integer linear programming for the unweighted $\text{max-min BCP}_k$
parameterized by the size of a vertex cover.
</p></div>
    </summary>
    <updated>2021-08-25T22:41:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/123</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/123" rel="alternate" type="text/html"/>
    <title>TR21-123 |  On public-coin zero-error randomized communication complexity | 

	Hamed Hatami, 

	Ben Davis, 

	William Pires, 

	Ran Tao, 

	Hamza Usmani</title>
    <summary>We prove that for every Boolean function, the public-coin zero-error randomized communication complexity and the  deterministic communication complexity are polynomially equivalent.</summary>
    <updated>2021-08-24T22:39:39Z</updated>
    <published>2021-08-24T22:39:39Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-26T03:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/08/24/phd-postdoc-at-tu-munich-apply-by-september-30-2021/</id>
    <link href="https://cstheory-jobs.org/2021/08/24/phd-postdoc-at-tu-munich-apply-by-september-30-2021/" rel="alternate" type="text/html"/>
    <title>PhD / Postdoc at TU Munich (apply by September 30, 2021)</title>
    <summary>The group of Theoretical Foundations of Artificial Intelligence at Technical University of Munich invites applications for the position of a full-time research position (PhD or Postdoc) for conducting research in the computational/statistical learning theory, specifically generalisation and consistency of neural networks. Website: https://portal.mytum.de/jobs/wissenschaftler/NewsArticle_20210727_120151/index_html Email: ghoshdas@in.tum.de</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The group of Theoretical Foundations of Artificial Intelligence at Technical University of Munich invites applications for the position of a full-time research position (PhD or Postdoc) for conducting research in the computational/statistical learning theory, specifically generalisation and consistency of neural networks.</p>
<p>Website: <a href="https://portal.mytum.de/jobs/wissenschaftler/NewsArticle_20210727_120151/index_html">https://portal.mytum.de/jobs/wissenschaftler/NewsArticle_20210727_120151/index_html</a><br/>
Email: ghoshdas@in.tum.de</p></div>
    </content>
    <updated>2021-08-24T12:13:56Z</updated>
    <published>2021-08-24T12:13:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-08-26T03:37:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/122</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/122" rel="alternate" type="text/html"/>
    <title>TR21-122 |  The complexity of testing all properties of planar graphs, and the role of isomorphism | 

	Sabyasachi Basu, 

	Akash Kumar, 

	C. Seshadhri</title>
    <summary>Consider property testing on bounded degree graphs and let $\varepsilon &gt; 0$ denote the proximity parameter.  A remarkable theorem of Newman-Sohler (SICOMP 2013) asserts that all properties of planar graphs (more generally hyperfinite) are testable with query complexity only depending on $\varepsilon$. Recent advances in testing minor-freeness have proven that all additive and monotone properties of planar graphs can be tested in $\mathrm{poly}(\varepsilon^{-1})$ queries. Some properties falling outside this class, such as Hamiltonicity, also have a similar complexity for planar graphs.  Motivated by these results, we ask: can all properties of planar graphs can be tested in $\mathrm{poly}(\varepsilon^{-1})$ queries?  Is there a uniform query complexity upper bound for all planar properties, and what is the ``hardest" such property to test?

	We discover a surprisingly clean and optimal answer. Any property of bounded degree planar graphs can be tested in $\exp(O(\varepsilon^{-2}))$ queries.  Moreover, there is a matching lower bound, up to constant factors in the exponent.  The natural property of testing isomorphism to a fixed graph requires $\exp(\Omega(\varepsilon^{-2}))$ queries, thereby showing that (up to polynomial dependencies) isomorphism to an explicit fixed graph is the hardest property of planar graphs.  The upper bound is a straightforward adapation of the Newman-Sohler analysis that tracks dependencies on $\varepsilon$ more carefully.  The main technical contribution is the lower bound construction, which is achieved by a special family of planar graphs that are all mutually far from each other. 

    We can also apply our techniques to get analogous results for bounded treewidth graphs. We prove that all properties of bounded treewidth graphs can be tested in $\exp(O(\varepsilon^{-1}\log \varepsilon^{-1}))$ queries. Moreover, testing isomorphism to a fixed forest requires $\exp(\Omega(\varepsilon^{-1}))$ queries.</summary>
    <updated>2021-08-24T09:54:37Z</updated>
    <published>2021-08-24T09:54:37Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-26T03:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=1871</id>
    <link href="https://toc4fairness.org/1st-acm-conference-on-equity-and-access-in-algorithms-mechanisms-and-optimization-eaamo21/" rel="alternate" type="text/html"/>
    <title>1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO’21)</title>
    <summary>From Irene Yuan Lo: We are thrilled to announce that the registration for EAAMO ‘21 is now live! Please register for regular admission on Eventbrite by September 10, 2021. Conference registration is $20 ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>From <a href="https://sites.google.com/view/irene-lo/home?authuser=0">Irene Yuan Lo</a>:</p>



<p>We are thrilled to announce that the registration for EAAMO ‘21 is now live! Please register for regular admission on <a href="https://eaamo21.eventbrite.com/" rel="noreferrer noopener" target="_blank">Eventbrite</a> by September 10, 2021.</p>



<p><strong>Conference registration</strong> is $20 for ACM members, $15 for students, and $35 for non-ACM members. We also provide <strong>financial assistance and data grants</strong> in order to waive registration fees and provide data plans to facilitate virtual attendance. Please apply <a href="https://docs.google.com/forms/d/e/1FAIpQLSe-SHJToB7Sm59M9L0ZNaPcGbzoHK5vUrg3DsZC8pcgDOJziQ/viewform" rel="noreferrer noopener" target="_blank">here</a> before September 10, 2021.</p>



<p>A main goal of the conference is to bridge research and practice. Please <a href="https://forms.gle/8kcKvQExYB7rJQaM7" rel="noreferrer noopener" target="_blank">nominate practitioners</a> working with underserved and disadvantaged communities to join us at the conference (you can also nominate yourself if you are a practitioner). Invited practitioners will be included in facilitated discussions with researchers. </p>



<p>For more information, please see below or visit <a href="https://eaamo.org/#home" rel="noreferrer noopener" target="_blank">our website</a> and contact us at <a href="mailto:gc@eaamo.org" rel="noreferrer noopener" target="_blank">gc@eaamo.org</a> with any questions.</p>



<p class="has-text-align-center">***</p>



<p>The inaugural <strong>Conference on </strong><strong>Equity and Access in Algorithms, Mechanisms, and Optimization</strong> (EAAMO ‘21) will take place on October 5-9, 2021, virtually, on Zoom and Gather.town. EAAMO ‘21 will be sponsored by ACM <a href="https://sigai.acm.org/" rel="noreferrer noopener" target="_blank">SIGAI</a> and <a href="https://www.sigecom.org/" rel="noreferrer noopener" target="_blank">SIGecom</a>. </p>



<p>The goal of this event is to highlight work where techniques from algorithms, optimization, and mechanism design, along with insights from the social sciences and humanistic studies, can improve access to opportunity for historically underserved and disadvantaged communities. </p>



<p>The conference aims to foster a multi-disciplinary community, facilitating interactions between academia, industry, and the public and voluntary sectors. The program will feature keynote presentations from researchers and practitioners as well as contributed presentations in the research and policy &amp; practice tracks. </p>



<p>We are excited to host a series of <strong>keynote speakers</strong> from a variety of fields: <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-sassefa" rel="noreferrer noopener" target="_blank">Solomon Assefa</a> (IBM Research), <a href="https://campuspress.yale.edu/dirkbergemann/" rel="noreferrer noopener" target="_blank">Dirk Bergemann</a> (Yale University), <a href="https://www.elloraderenoncourt.com/" rel="noreferrer noopener" target="_blank">Ellora Derenoncourt</a> (University of California, Berkeley), <a href="https://web.stanford.edu/~ashishg/" rel="noreferrer noopener" target="_blank">Ashish Goel</a> (Stanford University), <a href="https://marylgray.org/" rel="noreferrer noopener" target="_blank">Mary Gray</a> (Microsoft Research), <a href="https://people.mpi-sws.org/~gummadi/" rel="noreferrer noopener" target="_blank">Krishna Gummadi</a> (Max Planck Institute for Software Systems), <a href="https://u.cs.biu.ac.il/~avinatan/" rel="noreferrer noopener" target="_blank">Avinatan Hassidim</a> (Bar Ilan University), <a href="https://www.some.ox.ac.uk/people/radhika-khosla/" rel="noreferrer noopener" target="_blank">Radhika Khosla</a> (University of Oxford), <a href="https://www.gob.mx/mejoredu/articulos/sylvia-ortega-salazar-fue-elegida-por-unanimidad-como-presidenta-del-consejo-ciudadano-de-mejoredu" rel="noreferrer noopener" target="_blank">Sylvia Ortega Salazar</a> (National College of Vocational and Professional Training), and <a href="https://bdtrust.org/trooper-sanders/" rel="noreferrer noopener" target="_blank">Trooper Sanders</a> (Benefits Data Trust).</p>



<p><em>ACM EAAMO is part of the </em><a href="http://md4sg.com/" rel="noreferrer noopener" target="_blank"><em>Mechanism Design for Social Good</em></a><em> (MD4SG) initiative,</em><em> and builds on the MD4SG technical </em><a href="http://md4sg.com/workshop/EC19/cfp.html" rel="noreferrer noopener" target="_blank"><em>workshop series</em></a><em> and tutorials at conferences including ACM EC, ACM COMPASS, ACM FAccT, and WINE.</em></p></div>
    </content>
    <updated>2021-08-24T03:02:19Z</updated>
    <published>2021-08-24T03:02:19Z</published>
    <category term="Blog"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i1.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2021-08-26T03:39:06Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-3831768219100304909</id>
    <link href="http://processalgebra.blogspot.com/feeds/3831768219100304909/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=3831768219100304909" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/3831768219100304909" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/3831768219100304909" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/08/interview-with-concur-2021-tot-award_23.html" rel="alternate" type="text/html"/>
    <title>Interview with CONCUR 2021 ToT Award Recipients: Ahmed Bouajjani and Javier Esparza</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><i>I am delighted to post <a href="http://people.rennes.inria.fr/Nathalie.Bertrand/" target="_blank">Nathalie Bertrand</a>'s splendid interview with <a href="https://qonfest2021.lacl.fr/test-of-time.php" target="_blank">CONCUR 2021 Test-of-Time Award</a> recipients <a href="https://www.irif.fr/~abou/" target="_blank">Ahmed Bouajjani</a> and <a href="https://www7.in.tum.de/~esparza/" target="_blank">Javier Esparza</a>. (Ahmed and Javier receive the award for a paper their co-authored with the late <a href="https://hsb2019.fit.vutbr.cz/oded_maler.php" target="_blank">Oded Maler</a>.)<br/><br/>Thanks to Ahmed and Javier for their answers and to Nathalie for conducting such an interesting interview, which is rich in information on the technical context for, and impact of, their work as well as on their research collaborations over the years. I now leave the floor to Nathalie and her interviewees. Enjoy! </i><br/></p><p>This post is devoted to the interview I was honored to conduct with Ahmed Bouajjani and Javier Esparza for <a href="https://doi.org/10.1007/3-540-63141-0_10" target="_blank">their paper</a> with Oded Maler on reachability in pushdown automata.<br/><br/><b>Nathalie: </b>You receive the CONCUR ToT Award 2021 for your paper with Oded Maler <a href="https://doi.org/10.1007/3-540-63141-0_10" target="_blank">Reachability Analysis of Pushdown Automata: Application to Model-Checking</a>, which appeared at CONCUR 1997. In that article, you develop symbolic techniques to represent and manipulate sets of configurations of pushdown automata, or even of the broader class of alternating pushdown systems. The data structure you define to represent potentially infinite sets of configurations is coined alternating multi-automata, and you provide algorithms to compute the<br/>set of predecessors (pre*) of a given set of configurations.  Could you briefly explain to our readers what alternating multi-automata are?<br/><br/><b>Ahmed:</b> The paper is based on two ideas. The first one is to use finite automata as a data structure to represent infinite sets of configurations of the pushdown automaton.  We called them <i>multi-automata</i> because they have multiple initial states, one per control state of the pushdown automaton, but there is nothing deep there.  The second idea is that this representation is closed under the operation of computing predecessors, immediate or not. So, given a multi-automaton representing a set of configurations, we can compute another multi-automaton representing all their predecessors. If you compute first the immediate predecessors, then their immediate predecessors, and so on, you don't converge, because your automata grow bigger and bigger. The surprising fact is that you can compute <i>all</i> predecessors in one go by just adding transitions to the original<br/>automaton, without adding any new states. This guarantees termination. Later we called this process ``saturation''.<br/><br/>Once you can compute predecessors, it is not too difficult to obtain a model-checking algorithm for LTL model checking. But for branching-time logics you must also be able to compute intersections of sets of configurations. That's where alternation kicks in, we use it to represent intersections without having to add new states.<br/><br/><b>Nathalie:</b> Could you also tell us how you came to study the question addressed in your award-winning article?  Which of the results in your paper did you find most surprising or challenging?<br/><br/><b>Javier:</b> In the late 80s and early 90s many people were working on symbolic model-checking, the idea of using data structures to compactly represent sets of configurations. BBDs for finite-state model-checking were a hot topic, and for quite a few years dominated CAV. BDDs can be seen as acyclic automata, and so it was natural to investigate general finite automata as data structure for infinite-state systems.  Pierre Wolper and his group also did very good work on that.<br/><br/>About your second question, when I joined the team Ahmed and Oded had already been working on the topic for a while, and they had already developed the saturation algorithm. When they showed it to me I was blown away, it was so beautiful. A big surprise.<br/><br/><b>Nathalie:</b> In contrast to most previous work, your approach applied to model checking of pushdown systems treats in a uniform way linear-time and branching time logics. Did you apply this objective in other contributions?<br/><br/><b>Javier:</b> I didn't. The reason is that I was interested in concurrency, and when you bring together concurrency and procedures even tiny fragments of branching-time logics become undecidable. So I kind of stuck to the linear-time case. Did you work on branching-time, Ahmed?<br/><br/><b>Ahmed:</b> Somehow yes (although it is not precisely about linear vs. branching time properties), in the context of Regular Model Checking, a uniform framework for symbolic analysis of infinite-state systems using automata-based data structures. There, we worked on two versions, one based on word automata for systems where configurations can be encoded as words or vectors of words, such as stacks, queues, etc., and another one based on tree automata for configurations of a larger class of systems like heap manipulating programs, parametrized systems with tree-like architectures, etc.  The techniques we developed for both cases are based on the same principles.<br/><br/><b>Nathalie:</b> As it is often the case, the paper leaves some open questions. For instance, I believe, the precise complexity of verification of pushdown systems against CTL specifications is PSPACE-hard and in EXPTIME. Did you or others close this gap since? Did your techniques help to establish the precise complexity?<br/><br/><b>Ahmed:</b> In our paper we showed that model checking the alternating modal<br/>mu-calculus is EXPTIME-hard. CTL is less expressive, and it  was the most popular logic in the verification community at the time, so it was natural to ask if it had lower complexity. <br/><br/><b>Javier:</b> Yes, as a first step in the paper we showed that a fragment of CTL, called EF, had PSPACE complexity. But I made a mistake in the proof, which was later found by Igor Walukiewicz. Igor cracked the problem in a paper at FSTTCS'00. It turns out that EF is indeed PSPACE-complete (so at least we got the result right!), and full CTL is EXPTIME-complete. I wish Igor had used our technique, but he<br/>didn't, he applied the ideas of his beautiful CAV'96 paper on parity pushdown games.<br/><br/><b>Nathalie:</b> It is often interesting to understand how research collaborations start as it can be inspiring to PhD students or colleagues. Could you tell us how you started your collaboration on the award-winning paper? Did you continue working together (on a similar topic or on something totally different) after 1997?<br/><br/><b>Ahmed:</b> Javier and I first met in Liege for CAV 95. French universities have this program that allows us to bring foreign colleagues to France for a month as invited professors, and I invited Javier to Grenoble in 96.<br/><br/><b>Javier:</b> It was great fun; Verimag was a fantastic place to do verification, we both liked cinema, Ahmed knew all restaurants, and the Alps were beautiful. Ahmed invited me again to Grenoble in 97. This time I came with my wife, and we again had a great time.<br/><br/>When I arrived in Grenoble in 96 Ahmed and Oded had already written most of the work that went into the paper. My contribution was not big, I only extended the result to the alternation-free mu-calculus, which was easy, and proved a matching lower bound. I think that my main contribution came <b>after</b> this paper. Ahmed and Oded were too modest, they thought the result was not so important, but I found it not only beautiful, I thought it'd be great implement the LTL part,<br/>and build a model checker for programs with procedures. We could do that thanks to Stefan Schwoon, who started his PhD in Munich around this time--he is now at Paris-Saclay---and was as good a theoretician as a tool builder. Around 2000 he implemented a symbolic version of the algorithms in MOPED, which was quite successful.<br/><br/><b>Ahmed:</b> In 99 I moved to LIAFA in Paris, and I remember your kids were born.<br/><br/><b>Javier:</b> Yes, you sent my wife beautiful flowers!<br/><br/><b>Ahmed:</b> But we kept in touch, and we wrote a paper together in POPL'03 with my PhD student Tayssir Touili, now professor in Paris. We extended the ideas of the CONCUR paper to programs with both procedures and concurrency. Other papers came, the last in 2008.<br/><br/><b>Javier:</b> And Ahmed is visiting Munich next year, pandemic permitting, so I hope there'll be more.<br/><br/><b>Nathalie:</b> How would you say this award-winning paper influenced your later work? Did any of your subsequent research build explicitly on it?<br/><br/><b>Ahmed:</b> This paper was the first of many I have co-authored on verification of infinite-state systems using automata.  All of them use various automata classes to represent sets of configurations, and compute reachable configurations by iterative application of automata operations. We call these procedures accelerations; instead of computing a fixed point of a function by repeated iteration, you "jump" to the fixed point after finitely many steps, or at least converge faster. Accelerations were implicitly present in the CONCUR'97 paper. They have been also used by many other authors, for example Bernard Boigelot and Pierre Wolper.<br/><br/>My first paper on accelerations was with Peter Habermehl, my PhD student at the time and now at IRIF.  We worked on the verification of systems communicating through queues, using finite automata with Presburger constraints as data structure.  Then came several works on communicating systems with my student Aurore Annichini and Parosh Abdulla and Bengt Jonsson from Uppsala. As a natural continuation, with the Uppsala group and my student Tayssir Touili we developed the framework of Regular Model Checking. And then, with Peter Habermehl, Tomas Vojnar and Adam Rogalewicz from TU Brno, we extended Regular Model Checking to Abstract Regular Model Checking, which proved<br/>suitable and quite effective for the analysis of heap manipulating programs.<br/><br/>We also applied the CONCUR'97 results to the analysis of concurrent programs. The first work was a POPL'03 paper with Javier, Tayssir, and me on an abstraction framework. Two years later, Shaz Qadeer and Jakob Rehof proposed bounded-context switch analysis for bug detection. That paper created a line of research, and we contributed to it in many ways, together with Shaz, Mohamed Faouzi Atig, who was my student then, and is now Professor at Uppsala, and others.<br/><br/><b>Javier:</b> The CONCUR'97 paper was very important for my career. As I said before, it directly led to MOPED, and later to jMOPED, a version of MOPED for Java programs developed by Stefan Schwoon and Dejvuth Suwimonteerabuth. Then, Tony Kucera, Richard Mayr, and I asked ourselves if it was possible to extend probabilistic verification to pushdown systems, and wrote some papers on the topic, the first in LICS'04. This was just the right moment, because at the same time Kousha Etessami and Mihalis Yannakakis started to write brilliant papers on recursive Markov chains, an equivalent model. The POPL'03 paper with Ahmed and Tayssir also came, and it triggered my work on Newtonian program analysis with two very talented PhD students, Stefan Kiefer, now in Oxford, and Michael Luttenberger, now my colleague at TUM.  So the CONCUR'97 paper was at the root of a large part of my work of the next 15 years.<br/><br/><b>Nathalie: </b>Is there any result obtained by other researchers that builds on your work and that you like in particular or found surprising?<br/><br/><b>Javier:</b> After implementing MOPED, Stefan worked with Tom Reps on an extension to weighted pushdown automata, the Weighted Pushdown Library. Tom and Somesh Jha also found beautiful applications to security. This was great work.  I was also very impressed by the work of Luke Ong and his student Matthew Hague. In 97 Ahmed and I tried to apply the saturation method to the full mu-calculus but failed, we thought it couldn't be done. But first Thierry Cachat gave a saturation algorithm for Büchi pushdown games, then Luke, Matthew cracked the mu-calculus problem, and then they even extended it to<br/>higher-order pushdown automata, together with Arnaud Carayol, Olivier Serre, and others.  That was really surprising.<br/><br/><b>Ahmed:</b> I agree. I'd also mention Qadeer and Rehof's TACAS'05 paper. They built on our results to prove that context-bounded analysis of concurrent programs is decidable. They initiated a whole line of research.<br/><br/><b>Nathalie:</b> What are the research topics that you find most interesting right now? Is there any specific problem in your current field of interest that you'd like to see solved?<br/><br/><b>Javier:</b> Ten years ago I've had said the complexity of the reachability problem for Petri nets and of solving parity games, but now the first one is solved and the second almost solved! Now I don't have a specific problem, but in the last years I've been working on parameterized systems with an arbitrary number of agents, and many aspects of the theory are still very unsatisfactory. Automatically proving a mutual exclusion algorithm correct for a few processes was already routine 20 years ago, but proving it for an arbitrary number is still very much an open problem.<br/><br/><b>Ahmed:</b> I think that invariant and procedure summary synthesis will remain hard and challenging problems that we need to investigate with new approaches and techniques. It is hard to discover the right level of abstraction at which the invariant must be expressed, which parts of the state are involved and how they are related. Of course the problem is unsolvable in general but finding good methodologies on how to tackle it depending on the class of programs is an important issue. I think that the recent emergence of data-driven approaches is<br/>promising. The problem is to develop well principled methods combining data-driven techniques and formal analysis that are efficient and that offer well understood guarantees.<br/><br/><b>Nathalie:</b> Would you have an anecdote or a tip from a well-established researcher to share to PhD students and young researchers?<br/><br/><b>Javier:</b> Getting this award reminded me of the conference dinner at CAV 12 in St. Petersburg. I ended up at a table with some young people I didn't know. The acoustics was pretty bad.  When the CAV Award was being announced, somebody at the table asked "What's going on?", and somebody else answered "Not much, some senior guys getting some award". Never take yourself very seriously ...<br/><br/><b>Nathalie:</b> Oded Maler passed away almost 3 years ago. Do you have any memory of him to share with our readers?<br/><br/><b>Ahmed:</b> Oded was very amused by the number of citations. He used to say "Look at all the damage we've done."<br/><br/><b>Javier:</b> Yes, Oded had a wonderful sense of humor, very dry and deadpan. When I arrived in Grenoble it took me a few days to learn how to handle it!  I miss it very much.<br/><br/></p></div>
    </content>
    <updated>2021-08-23T09:44:00Z</updated>
    <published>2021-08-23T09:44:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-08-24T20:52:02Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1522961788125667849</id>
    <link href="http://blog.computationalcomplexity.org/feeds/1522961788125667849/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/when-words-get-stretched-beyond-their.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/1522961788125667849" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/1522961788125667849" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/when-words-get-stretched-beyond-their.html" rel="alternate" type="text/html"/>
    <title>When Words Get Stretched Beyond Their Original meaning</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>STORY ONE:</p><p> On a Jeopardy rerun with Alex Trebek the question (actually the answer, given the shows format) was (I paraphrase)</p><p><br/>Who resigned his commision in the US Army Air Force in April 1941 after President Roosevelt publicly rebuked him for his views?</p><p>The answer (actually the question--Why does Jeopardy do this answer-question thing, drives me nuts!) was</p><p>Charles Lindbergh.</p><p>Alex Trebek then said  <i>Charles Lindberg's views on WW II were not politically correct.</i></p><p>This really struck me since <i>Politically correct </i>means, to quote Wikipedia:</p><p><i>a term used to describe language, policies, or measures that are intended to avoid offense of disadvantage to members of particular groups in society. </i></p><p>Wikipedia also adds that the term is generally used pejoratively with an implication that these policies are excessive or unwarranted. </p><p> But Alex Trebek is using the term to mean  <i>incorrect </i>or perhaps <i>incorrect given what we know now</i> or if you think history is written by the winners, then perhaps <i>incorrect since Germany lost the wa</i>r.  But my point is that I really don't think the term  <i>politically incorrect </i> makes sense here.</p><p>STORY TWO</p><p>More recently I heard an anti-masker say</p><p><i>We should not let some woke school board take the right to not wear a mask away from parents and children</i>.</p><p>Independent of if you are anti-mask-mandates or pro-mask-mandates, this seems like a strange use of the word  <i>woke </i> which means, to paraphrase Wikipedia:</p><p><i>Having an awareness of racial prejudice, gender prejudice, sexual orientation prejudice, and the past and current discrimination they have and do cause. </i></p><p>I've seen it both positively and negatively.</p><p>The anti-masker's using of the term seems odd in that mask wearing is not a woke issue. Perhaps he should have said </p><p><i>We should not let some Nazi school board take the right to not wear a mask away from parents and children.</i></p><p>The term  <i>Nazi </i>while not actually correct, conveys that the school board is authoritarian. However, he really could not use the term that since he was was a neo-Nazi and proud of it. That raises a question: what pejorative  term can a Neo-Nazi use when they want to say someone is  Authoritarian? I ask non-rhetoically. </p><p>But I am getting off topic here- my real point is that the word<i> woke </i>is being used to mean Authoritarian which is not even close to its original meaning. </p><p><br/></p><p>MY POINT</p><p>The above are examples of how a word in English may change its definition over time, which is not really news, but I found the examples interesting since I saw the origin of these words.</p><p>BILL, THIS IS A COMPLEXITY BLOG! SO TALK ABOUT COMPLEXITY. OR MATH!</p><p>In math do words change their meaning over time? Yes. Here are a few</p><p><i>Function</i>: at one time `function' implicitly means a function that occurs in nature. So only continous and perhaps diff functions qualified. </p><p><i>Sets</i>: probably similar.</p><p><i>Efficient</i>: At one time this was an informal notion (Joe Kruskal's paper on MST (see <a href="https://www.ams.org/journals/proc/1956-007-01/S0002-9939-1956-0078686-7/S0002-9939-1956-0078686-7.pdf">here</a>) is an example of that), then it seemed to be P or perhaps BPP. For some its linear or O(n log n) with a small constant. Rather than say the notion changed, its more like it was never that well defined in the first place, and still isn't. </p><p><i>Constructive: </i>The many diff definitions of this word could be a blog post of its own. In fact, I thought it was, but I could not find it. I did find lots of blog posts that use the word <i>constructive</i> in diff ways. </p><p><i>Elementary</i>: Also has many definitions, though they are closer together than for Constructive. This one I did do a post on <a href="https://blog.computationalcomplexity.org/2010/02/what-is-elementary-proof.html#comment-form">here</a></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-08-23T01:01:00Z</updated>
    <published>2021-08-23T01:01:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-25T20:33:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=2765</id>
    <link href="https://theorydish.blog/2021/08/22/itcs-2022-cfp-is-out/" rel="alternate" type="text/html"/>
    <title>ITCS 2022 – CFP is out</title>
    <summary>ITCS 2022 Call for Papers is out. Submission deadline is pretty soon: All papers should be pre-registered with title, authors, and abstract, by September 5, 7:59PM EDT (UTC−04:00).Final submissions are due by September 9, 7:59PM EDT (UTC−04:00).(Intended author notification: November 1.)</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>ITCS 2022 <a href="http://itcs-conf.org/itcs22/itcs22-cfp.html">Call for Papers</a> is out. Submission deadline is pretty soon:</p>



<p>All papers should be pre-registered with title, authors, and abstract, by <strong>September 5, 7:59PM EDT (UTC−04:00)</strong>.<br/>Final submissions are due by <strong>September 9, 7:59PM EDT (UTC−04:00)</strong>.<br/>(Intended author notification: <strong>November 1</strong>.)</p></div>
    </content>
    <updated>2021-08-23T00:00:47Z</updated>
    <published>2021-08-23T00:00:47Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-08-26T03:38:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/121</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/121" rel="alternate" type="text/html"/>
    <title>TR21-121 |  Matroid Intersection: A pseudo-deterministic parallel reduction from search to weighted-decision | 

	Sumanta Ghosh, 

	Rohit Gurjar</title>
    <summary>We study the matroid intersection problem from the parallel complexity perspective. Given
two matroids over the same ground set, the problem asks to decide whether they have a common base and its search version asks to find a common base, if one exists. Another widely studied variant is the weighted decision version where with the two matroids, we are given small weights on the ground set elements and a target weight W, and the question is to decide whether there is a common base of weight at least W. From the perspective of parallel complexity, the relation between the search and the decision versions is not well understood. We make a significant progress on this question by giving a pseudo-deterministic parallel (NC) algorithm for the search version that uses an oracle access to the weighted decision.

The notion of pseudo-deterministic NC was recently introduced by Goldwasser and Grossman [GG17], which is a relaxation of NC. A pseudo-deterministic NC algorithm for a search problem is a randomized NC algorithm that, for a given input, outputs a fixed solution with high probability. In case the given matroids are linearly representable, our result implies a pseudo-deterministic NC algorithm (without the weighted decision oracle). This resolves an open
question posed by Anari and Vazirani [AV20].</summary>
    <updated>2021-08-22T05:42:20Z</updated>
    <published>2021-08-22T05:42:20Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-26T03:37:33Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8678359761350967382</id>
    <link href="http://blog.computationalcomplexity.org/feeds/8678359761350967382/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/trusting-scientists.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/8678359761350967382" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/8678359761350967382" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/trusting-scientists.html" rel="alternate" type="text/html"/>
    <title>Trusting Scientists</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> A tweet that made me think.</p>
<blockquote class="twitter-tweet"><p dir="ltr" lang="en">If you think you don't trust scientists, you're mistaken. You trust scientists in a million different ways every time you step on a plane, or for that matter turn on your tap or open a can of beans. The fact that you're unaware of this doesn't mean it's not so.</p>— Paul Graham (@paulg) <a href="https://twitter.com/paulg/status/1419765657080578052?ref_src=twsrc%5Etfw">July 26, 2021</a></blockquote><p>The point here is subtle. We don't get on a plane because we "trust scientists", rather we do so because of the strong safety record of commercial aviation. I knew some physicists who won't get on a commuter plane because they worry about the science. Never stopped me.</p><p>It is science that we trust to tell us why planes fly, or the water is our tap is (mostly) safe and healthy. I'm not a big fan of beans but not because of the science. Of course I trust science that created the vaccines.</p><p>It's not just science, but solid engineering and lots and lots of testing. </p><p>Science isn't always right or consistent. When I was a kid not that long ago, we had nine planets in this solar system, dinosaurs were killed off by climate change and homosexuality was a mental illness. Science is fluid, updating as we learn with new data, models and experimentation. Science is at its best when it doesn't trust itself.</p><p>Sometimes people say trust in science to reinforce their beliefs. I've seen smart people say "Trust in the science" about whether vaccinated people should wear masks with completely different conclusions.</p><p>I'm a scientist, should you trust me? Let me quote another Paul G. </p><blockquote><p>“There’s a slightly humorous stereotype about computational complexity that says what we often end up doing is taking a problem that is solved a lot of the time in practice and proving that it’s actually very difficult,” said Goldberg.</p></blockquote><p>The quote comes from a recent <a href="https://www.quantamagazine.org/computer-scientists-discover-limits-of-major-research-algorithm-20210817/">Quanta Magazine article</a> about Paul's recent work with John Fearnley, Alexandros Hollender and Rahul Savani on the hardness of gradient descent. Even many NP-complete problems these days can often be solved in practice.</p><p>Let's end with the quote attributed to statistician George Box, "All models are wrong, but some are useful". Science gives us ways to understand the world and we need to both trust in the science but know the limitations of what it has to say.</p></div>
    </content>
    <updated>2021-08-20T14:51:00Z</updated>
    <published>2021-08-20T14:51:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-25T20:33:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/08/19/research-staff-member-at-ibm-research-apply-by-august-19-2021/</id>
    <link href="https://cstheory-jobs.org/2021/08/19/research-staff-member-at-ibm-research-apply-by-august-19-2021/" rel="alternate" type="text/html"/>
    <title>Research Staff Member at IBM Research (apply by August 19, 2021)</title>
    <summary>We are seeking to hire ASAP a Research Scientist for the Mathematics of AI research group, with a demonstrated publication record in related areas of mathematics and theoretical computer science. We are specifically interested in expertise in formal logic as well as learning theory, generalizability, information theory, graph algorithms, reinforcement learning, and optimization. Website: https://krb-sjobs.brassring.com/TGnewUI/Search/home/HomeWithPreLoad?PageType=JobDetails&amp;partnerid=26059&amp;siteid=5016&amp;Areq=435593BR#jobDetails=458585_5016 […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are seeking to hire ASAP a Research Scientist for the Mathematics of AI research group, with a demonstrated publication record in related areas of mathematics and theoretical computer science. We are specifically interested in expertise in formal logic as well as learning theory, generalizability, information theory, graph algorithms, reinforcement learning, and optimization.</p>
<p>Website: <a href="https://krb-sjobs.brassring.com/TGnewUI/Search/home/HomeWithPreLoad?PageType=JobDetails&amp;partnerid=26059&amp;siteid=5016&amp;Areq=435593BR#jobDetails=458585_5016">https://krb-sjobs.brassring.com/TGnewUI/Search/home/HomeWithPreLoad?PageType=JobDetails&amp;partnerid=26059&amp;siteid=5016&amp;Areq=435593BR#jobDetails=458585_5016</a><br/>
Email: lhoresh@us.ibm.com</p></div>
    </content>
    <updated>2021-08-19T17:19:34Z</updated>
    <published>2021-08-19T17:19:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-08-26T03:37:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://bit-player.org/?p=2359</id>
    <link href="http://bit-player.org/2021/riding-the-covid-coaster" rel="alternate" type="text/html"/>
    <link href="http://bit-player.org/2021/riding-the-covid-coaster#comments" rel="replies" type="text/html"/>
    <link href="http://bit-player.org/2021/riding-the-covid-coaster/feed/atom" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Riding the Covid coaster</title>
    <summary type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml">Figure 1 Peaks and troughs, lumps and slumps, wave after wave of surge and retreat: I have been following the ups and downs of this curve, day by day, for a year and a half. The graph records the number … <a href="http://bit-player.org/2021/riding-the-covid-coaster">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Figure 1<img alt="US daily Covid cases from New York Times data, showing raw counts and a seven-day rolling average" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/daily_case_count_graph.svg" width="660"/></p>
<p>Peaks and troughs, lumps and slumps, wave after wave of surge and retreat: I have been following the ups and downs of this curve, day by day, for a year and a half. The graph records the number of newly reported cases of Covid-19 in the United States for each day from 21 January 2020 through 20 July 2021. That’s 547 days, and also exactly 18 months. The faint, slender vertical bars in the background give the raw daily numbers; the bold blue line is a seven-day trailing average. (In other words, the case count for each day is averaged with the counts for the six preceding days.)</p>
<p>I struggle to understand the large-scale undulations of that graph. If you had asked me a few years ago what a major epidemic might look like, I would have mumbled something about exponential growth and decay, and I might have sketched a curve like this one:</p>
<p>Figure 2<img alt="Sketch of simple exponential growth and decay" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/sketch-of-exponential-growth-and-decay-crop.jpg" width="400"/></p>
<p class="undent">My imaginary epidemic is so much simpler than the real thing! The number of daily infections goes up, and then it comes down again. It doesn’t bounce around like a nervous stock market. It doesn’t have seasonal booms and busts. </p>
<p>The graph tracing the actual incidence of the disease makes at least a dozen reversals of direction, along with various small-scale twitches and glitches. The big mountain in the middle has foothills on both sides, as well as some high alpine valleys between craggy peaks. I’m puzzled by all this structural embellishment. Is it mere noise—a product of random fluctuations—or is there some driving mechanism we ought to know about, some switch or dial that’s turning the infection process on and off every few months?</p>
<p>I have a few ideas about possible explanations, but I’m not so keen on any of them that I would try to persuade you they’re correct. However, I <em>do</em> hope to persuade you there’s something here that needs explaining.</p>
<p>Before going further, I want to acknowledge my sources. The data files I’m working with are curated by <em>The New York Times</em>, based on information collected from state and local health departments. Compiling the data is a big job; the <em>Times</em> lists more than 150 workers on the project. They need to reconcile the differing and continually shifting policies of the reporting agencies, and then figure out what to do when the incoming numbers look fishy. (Back in June, Florida had a day with –40,000 new cases.) The entire data archive, now about 2.3 gigabytes, is freely available on <a href="https://github.com/nytimes/covid-19-data">GitHub</a>. Figure 1 in this article is modeled on a graph <a href="https://www.nytimes.com/interactive/2021/us/covid-cases.html">updated daily</a> in the <em>Times</em>.</p>
<p>I must also make a few disclaimers. In noodling around with this data set I am not trying to forecast the course of the epidemic, or even to retrocast it—to develop a model accurate enough to reproduce details of timing and magnitude observed over the past year and a half. I’m certainly not offering medical or public-health advice. I’m just a puzzled person looking for simple mechanisms that might explain the overall shape of the incidence curve, and in particular the roller coaster pattern of recurring hills and valleys.</p>
<hr/>
<p>So far, four main waves of infection have washed over the U.S., with a fifth wave now beginning to look like a tsunami. Although the waves differ greatly in height, they seem to be coming at us with some regularity. Eyeballing Figure 1, I get the impression that the period from peak to peak is pretty consistent, at roughly four months.</p>
<p>Periodic oscillations in epidemic diseases have been noticed many times before. The classical example is measles in Great Britain, for which there are weekly records going back to the early 18th century. In 1917 John Brownlee studied the measles data with a form of Fourier analysis called the periodogram. He found that the strongest peak in the frequency spectrum came at a period of 97 weeks, reasonably consistent with the widespread observation that the disease reappears every second year. But Brownlee’s periodograms bristle with many lesser peaks, indicating that the measles rhythm is not a simple, uniform drumbeat. Of particular note is the work of M. S. Bartlett in the 1950s, which includes an early instance of computer modeling in epidemiology, using the Manchester University Computer.Later work, using different methods, suggested that the dynamics of measles epidemics may actually be chaotic, with no long-term order.</p>
<p>The mechanism behind the oscillatory pattern in measles is easy to understand. The disease strikes children in the early school years, and the virus is so contagious that it can run through an entire community in a few weeks. Afterwards, another outbreak can’t take hold until a new cohort of children has reached the appropriate age. No such age dependence exists in Covid-19, and the much shorter period of recurrence suggests that some other mechanism must be driving the oscillations. Nevertheless, it seems worthwhile to try applying Fourier methods to the data in Figure 1.</p>
<p>The Fourier transform de­composes any curve representing a function of time into a sum of simple sine and cosine waves of various frequencies. In textbook examples, the algorithm works like magic. Take a wiggly curve like this one:</p>
<p>Figure 3<img alt="Graph of cos(x) + cos(3x) over range from 0 to 4?" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/cosx+cos3xcurve.svg" width="500"/></p>
<p class="undent">Feed it into the Fourier transformer, turn the crank, and out comes a graph that looks like this, showing the coefficients of various frequency components:</p>
<p>Figure 4<img alt="DCT coefficiencts of the compound cosine curve" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/DCT_coefficiencts_of_coscurve.svg" width=""/></p>
<p class="undent">Technical details: The classical Fourier transform yields complex coefficients, with real and imaginary parts. I am using a variant called the discrete cosine transform, which produces real coefficients. The input curve is generated by the function \(\cos(x) + \cos(3x)\) over the interval from \(0\) to \(4\pi\).Only two coefficients are substantially different from zero, corresponding to waves that make two or six full cycles over the span of the input curve. Those two coefficients capture all the information needed to reconstruct the input. If you draw the curves specified by the two coefficients, then add them up point by point, you get back a replica of the original.</p>
<p>It would be illuminating to have such a succinct encoding of the Covid curve—a couple of numbers that explain its comings and goings. Alas, that’s not so easy. When I poured the Covid data into the Fourier machine, this is what came out:</p>
<p>Figure 5<img alt="DCT coefficients for the Covid curve" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/dct_coef_graph_for_cases_7avg.svg" width=""/></p>
<p>More than a dozen coefficients have significant magnitude; some are positive and some are negative; no obvious pattern leaps off the page. This spectrum, like the simpler one in Figure 4, holds all the information needed to reconstruct its input. I confirmed that fact with a quick computational experiment. But looking at the jumble of coefficients doesn’t help me to understand the structure of the Covid curve. The Fourier-transformed version is even more baffling than the original.</p>
<p>One lesson to be drawn from this exercise is that the Fourier transform is indeed magic: If you want to make it work, you need to master the dark arts. I am no wizard in this department; as a matter of fact, most of my encounters with Fourier analysis have ended in tears and trauma. No doubt someone with higher skills could tease more insight from the numbers than I can. But I doubt that any degree of Fourier finesse will lead to some clear and concise description of the Covid curve. Even with \(200\) years of measles records, Brownlee wasn’t able to isolate a clear signal; with just a year and a half of Covid data, success is unlikely.</p>
<p>Yet my foray into the Fourier realm was not a complete waste of time. Applying the inverse Fourier transform to the first 13 coefficients (for wavenumbers 0 through 6) yields this set of curves:</p>
<p>Figure 6<img alt="Curves created by inverse Fourier transform from the first 13 coefficients" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/fourier_components_from_dct_cases_7avg.svg" width="400"/></p>
<p class="undent">It looks a mess, but the sum of these 13 sinusoidal waves yields quite a handsome, smoothed version of the Covid curve. In Figure 7 below, the pink area in the background shows the <em>Times</em> data, smoothed with the seven-day rolling average. The blue curve, much smoother still, is the waveform reconstructed from the 13 Fourier coefficients.</p>
<p>Figure 7<img alt="Reconstructed covid curve from 14 Fourier coefficients" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/reconstructed_covid_curve_from_14_coefs_helv.svg" width="640"/></p>
<p>The reconstruction traces the outlines of all the large-scale features of the Covid curve, with serious errors only at the end points (which are always problematic in Fourier analysis). The Fourier curve also fails to reproduce the spiky triple peak atop the big surge from last winter, but I’m not sure that’s a defect.</p>
<hr/>
<p>Let’s take a closer look at that triple peak. The graph below is an expanded view of the two-month interval from 20 November 2020 through 20 January 2021. The light-colored bars in the background are raw data on new cases for each day; the dark blue line is the seven-day rolling average computed by the <em>Times</em>. </p>
<p>Figure 8<img alt="Expanded view of the Covid graph showing Nov 20 to Jan 20, raw data and rolling average" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/nov_jan_details_graph.svg" width="640"/></p>
<p>The peaks and valleys in this view are just as high and low as those in Figure 1; they look less dramatic only because the horizontal axis has been stretched ninefold. My focus is not on the peaks but on the troughs between them. (After all, there wouldn’t be three peaks if there weren’t two troughs to separate them.) Three data points marked by pink bars have case counts far lower than the surrounding days. Note the dates of those events. November 26 was Thanksgiving Day in the U.S. in 2020; December 25 is Christmas Day, and January 1 is New Years Day. It looks like the virus went on holiday, but of course it was actually the medical workers and public health officials who took a day off, so that many cases did not get recorded on those days.</p>
<p>There may be more to this story. Although the holidays show up on the chart as low points in the progress of the epidemic, they were very likely occasions of higher-than-normal contagion, because of family gatherings, religious services, revelry, and so on. (I commented on the <a href="http://bit-player.org/2020/we-gather-together-2">Thanksgiving risk</a> last fall.) Those “extra” infections would not show up in the statistics until several days later, along with the cases that went undiagnosed or unreported on the holidays themselves. Thus each dip appears deeper because it is followed by a surge.</p>
<p>All in all, it seems likely that the troughs creating the triple peak are a reporting anomaly, rather than a reflection of genuine changes in the viral transmission rate. Thus a curve that smooths them away may give a better account of what’s really going on in the population.</p>
<hr/>
<p>There’s another transformation—quite different from Fourier analysis—that might tell us something about the data. The time derivative of the Covid curve gives the rate of change in the infection rate—positive when the epidemic is surging, negative when it’s retreating. Because we’re working with a series of discrete values, computing the derivative is trivially easy: It’s just the series of differences between successive values.</p>
<p>Figure 9<img alt="First derivatives of three versions of the Covid curve--the raw data, the curve smoothed with a seven-day rolling average, and the curve reconstructed from 13 Fourier coefficients." border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/first_derivatives.svg" width=""/></p>
<p>The derivative of the raw data <em>(blue)</em> looks like a seismograph recording from a jumpy day along the San Andreas. The three big holiday anomalies—where case counts change by \(100,000\) per day—produce dramatic excursions. The smaller jagged waves that extend over most of the \(18\)-month interval are probably connected with the seven-day cycle of data collection, which typically show case counts increasing through the work week and then falling off on the weekend.</p>
<p>The seven-day trailing average is designed to suppress that weekly cycle, and it also smooths over some larger fluctuations. The resulting curve <em>(red)</em> is not only less jittery but also has much lower amplitude. (I have expanded the vertical scale by a factor of two for clarity.)</p>
<p>Finally, the reconstituted curve built by summing \(13\) Fourier components yields a derivative curve <em>(green)</em> whose oscillations are mere ripples, even when stretched ver­tically by a factor of four.</p>
<p>The points where the derivative curves cross the zero line—going from positive to negative or vice versa—correspond to peaks or troughs in the underlying case-count curve. Each zero crossing marks a moment when the epidemic’s trend reversed direction, when a growing daily case load began to decline, or a falling one turned around and started gaining again. The blue raw-data curve has \(255\) zero crossings, and the red averaged curve has \(122\). Even the lesser figure implies that the infection trend is reversing course every four or five days, which is not plausible; most of those sign changes must result from noise in the data. </p>
<p>The silky smooth green curve has nine zero crossings, most of which seem to signal real changes in the course of the epidemic. I would like to understand what’s causing those events.</p>
<hr/>
<p>You catch a virus. (Sorry about that.) Some days later you infect a few others, who after a similar delay pass the gift on to still more people. This is the mechanism of exponential (or geometric) growth. With each link in the chain of transmission the number of new cases is multiplied by a factor of \(R\), which is the natural growth ratio of the epidemic—the average number of cases spawned by each infected individual. Starting with a single case at time \(t = 0\), the number of new infections at any later time \(t\) is \(R^t\). If \(R\) is greater than \(1\), even very slightly, the number of cases increases without limit; if \(R\) is less than \(1\), the epidemic fades away.</p>
<p>The average delay between when you become infected and when you infect others is known as the serial passage time, which I am going to abbreviate T<sub>SP</sub> and take as the basic unit for measuring the duration of events in the epidemic. For Covid-19, one T<sub>SP</sub> is probably about five days.</p>
<p>Exponential growth is famously unmanageable. If \(R = 2\), the case count doubles with every iteration: \(1, 2, 4, 8, 16, 32\dots\). It increases roughly a thousandfold after \(10\) T<sub>SP</sub>, and a millionfold after \(20\) T<sub>SP</sub>. The rate of increase becomes so steep that I can’t even graph it except on a logarithmic scale, where an exponential trajectory becomes a straight line.</p>
<p>Figure 10<img alt="Exponential growth for R = 0.8, 1.0, 1.25, 2.0, and 3.0, plotted on a log scale." border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/exp_log_plot.svg" width="500"/></p>
<p class="indent">What is the value of \(R\) for the SARS-CoV-2 virus? No one knows for sure. The number is difficult to measure, and it varies with time and place. Another number, \(R_0\), is often regarded as an intrinsic property of the virus itself, an indicator of how easily it passes from person to person. The Centers for Disease Control and Prevention (CDC) <a href="https://www.cdc.gov/coronavirus/2019-ncov/hcp/planning-scenarios.html">suggests</a> that \(R_0\) for SARS-CoV-2 probably lies between \(2.0\) and \(4.0\), with a best guess of \(2.5\). That would make it catchier than influenza but less so than measles. However, the CDC has also published <a href="https://wwwnc.cdc.gov/eid/article/25/1/17-1901_article">a report</a> arguing that \(R_0\) is “easily misrepresented, misinterpreted, and misapplied.” I’ve certainly been confused by much of what I’ve read on the subject.</p>
<p>Whatever numerical value we assign to \(R\), if it’s greater than \(1\), it cannot possibly describe the complete course of an epidemic. As \(t\) increases, \(R^t\) will grow at an accelerating pace, and before you know it the predicted number of cases will exceed the global human population. For \(R = 2\), this absurdity arrives after about \(33\) T<sub>SP</sub>, which is less than six months.</p>
<p>What we need is a mathematical model with a built-in limit to growth. As it happens, the best-known model in epidemiology features just such a mechanism. Introduced almost 100 years ago by W. O. Kermack and A. G. McKendrick of the Royal College of Physicians in Edinburgh, Recent descriptions of the SIR model usually say \((\mathcal{R})\) stands for <em>removed</em>, acknowledging that recovery is not the only way an infection can end. But I don’t want to be grim today. Also note that I’m using a calligraphic font for \(\mathcal{S},\mathcal{I}\), and \(\mathcal{R}\) to avoid confusion between the growth rate \(R\) and the recovered group \(\mathcal{R}\).it is now called the SIR model because it partitions the human population into three subsets called <em>susceptible</em> \((\mathcal{S})\), <em>infective</em> \((\mathcal{I})\), and <em>recovered</em> \((\mathcal{R})\). Initially (before a pathogen enters the population), everyone is of type \(\mathcal{S}\). Susceptibles who contract the virus become infectives—capable of transmitting the disease to other susceptibles. Then, after each infective’s illness has run its course, that person joins the recovered class. Having acquired immunity through infection, the recovereds will never be susceptible again.</p>
<p>A SIR epidemic can’t keep growing indefinitely for the same reason that a forest fire can’t keep burning after all the trees are reduced to ashes. At the beginning of an epidemic, when the entire population is susceptible, the case count can grow exponentially. But growth slows later, when each infective has a harder time finding susceptibles to infect. Kermack and McKendrick made the interesting discovery that the epidemic dies out before it has reached the entire population. That is, the last infective recovers before the last susceptible is infected, leaving a residual \(\mathcal{S}\) population that has never experienced the disease.</p>
<p>The SIR model itself has gone viral in the past few years. There are tutorials everywhere on the web, as well as scholarly articles and books. (I recommend <a href="https://www.cambridge.org/core/books/epidemic-modelling/6F7376322E00A98D6801B97D9429A0CF#"><em>Epidemic Modelling: An Introduction</em></a>, by Daryl J. Daley and Joseph Gani. Or try <a href="https://people.maths.ox.ac.uk/maini/PKM%20publications/384.pdf"><em>Mathematical Modelling of Zombies</em></a> if you’re feeling brave.) Most accounts of the SIR model, including the original by Kermack and McKendrick, are presented in terms of differential equations. I’m instead going to give a version with discrete time steps—\(\Delta t\) rather than \(dt\)—because I find it easier to explain and because it translates line for line into computer code. In the equations that follow, \(\mathcal{S}\), \(\mathcal{I}\), and \(\mathcal{R}\) are real numbers in the range \([0, 1]\), representing proportions of some fixed-size population.</p>
<p>\[\begin{align}<br/>
	\Delta\mathcal{I} &amp; = \beta \mathcal{I}\mathcal{S}\\[0.8ex]<br/>
	\Delta\mathcal{R} &amp; = \gamma \mathcal{I}\\[1.5ex]<br/>
	\mathcal{S}_{t+\Delta t} &amp; = \mathcal{S}_{t} - \Delta\mathcal{I}\\[0.8ex]<br/>
	\mathcal{I}_{t+\Delta t} &amp; = \mathcal{I}_{t} + \Delta\mathcal{I} - \Delta\mathcal{R}\\[0.8ex]<br/>
	\mathcal{R}_{t+\Delta t} &amp; = \mathcal{R}_{t} + \Delta\mathcal{R}\\[0.8ex]<br/>
\end{align}\]</p>
<p class="undent">The first equation, with \(\Delta\mathcal{I}\) on the left hand side, describes the actual contagion process—the recruitment of new infectives from the susceptible population. The number of new cases is proportional to the product of \(\mathcal{I}\) and \(\mathcal{S}\), since the only way to propagate the disease is to bring together someone who already has it with someone who can catch it. The constant of proportionality, \(\beta\), is a basic parameter of the model. It measures how often (per T<sub>SP</sub>) an infective person encounters others closely enough to communicate the virus.</p>
<p>The second equation, for \(\Delta\mathcal{R}\), similarly describes recovery. For epidemiological pur­poses, you don’t have to be feeling tiptop again to be done with the disease; recovery is defined as the moment when you are no long capable of infecting other people. The model takes a simple approach to this idea, withdrawing a fixed fraction of the infectives in every time step. The fraction is given by the parameter \(\gamma\).</p>
<p>After the first two equations calculate the number of people who are changing their status in a given time step, the last three equations update the population segments accordingly. The susceptibles lose \(\Delta\mathcal{I}\) members; the infectives gain \(\Delta\mathcal{I}\) and lose \(\Delta\mathcal{R}\); the recovereds gain \(\Delta\mathcal{R}\). The total popu­la­tion \(\mathcal{S} + \mathcal{I} + \mathcal{R}\) remains constant throughout.</p>
<p>InIn the recent literature, the ratio \(\beta / \gamma\) is commonly presented not just as analogous to \(R_0\) but as a definition of \(R_0\). I resist this practice because \(R_0\) has too many definitions already. In adopting the symbol \(\rho\) I am following the precedent of David G. Kendall in a 1956 paper. this version of the SIR model, the ratio \(\rho = \beta / \gamma\) determines a natural growth rate, closely analogous to \(R_0\). Higher \(\beta\) means faster recruitment of infectives; lower \(\gamma\) means they remain infective longer. Either of those adjustments increases the growth rate \(\rho\), although the rate also depends on \(\mathcal{S}\) and \(\mathcal{I}\). </p>
<p>Here’s what happens when you put the model in motion. For this run I set \(\beta = 0.6\) and  \(\gamma = 0.2\), which implies that \(\rho =  3.0\). Another number that needs to be specified is the initial proportion of infectives; I chose \(10^{-6}\), or in other words one in a million.  The model ran for \(100\) T<sub>SP</sub>, with a time step of \(\Delta t = 0.1\) T<sub>SP</sub>; thus there were \(1{,}000\) iterations overall.</p>
<p>Figure 11<img alt="SIR model: a graph of susceptible, infective, and recovered populations as a function of time" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/SIR_model.svg" width="640"/></p>
<p>Let me call your attention to a few features of this graph. At the outset, nothing seems to happen for weeks and weeks, and then all of a sudden a huge blue wave rises up out of the calm waters. Starting from one case in a population of a million, it takes \(18\) T<sub>SP</sub> to reach one case in a thousand, but just \(12\) more T<sub>SP</sub> to reach one in \(10\). </p>
<p>Note that the population of infectives reaches a peak near where the susceptible and recovered curves cross—that is, where \(\mathcal{S} = \mathcal{R}\). This relationship holds true over a wide range of parameter values. That’s not surprising, because the whole epidemic process acts as a mechanism for converting susceptibles into recovereds, via a brief transit through the infective stage. But, as Kermack and McKendrick predicted, the conversion doesn’t quite go to completion. At the end, about \(6\) percent of the population remains in the susceptible category, and there are no infectives left to convert them. This is the condition called herd immunity, where the population of susceptibles is so diluted that most infectives recover before they can find someone to infect. It’s the end of the epidemic, though it comes only after \(90+\) percent of the people have gotten sick. (That’s not what I would call a victory over the virus.)</p>
<p>The \(\mathcal{I}\) class in the SIR model can be taken as a proxy for the tally of new cases tracked in the <em>Times</em> data. The two variables are not quite the same—infectives remain in the class \(\mathcal{I}\) until they recover, whereas new cases are counted only on the day they are reported—but they are very similar and roughly proportional to one another. And that brings me to the main point I want to make about the SIR model: In Figure 11 the blue curve for infectives looks nothing like the corresponding new-case tally in Figure 1. In the SIR model, the number of infectives starts near zero, rises steeply to a peak, and thereafter tapers gradually back to zero, never to rise again. It’s a one-hump camel. The roller coaster Covid curve is utterly different. </p>
<p>The detailed geometry of the \(\mathcal{I}\) curve depends on the values assigned to the parameters \(\beta\) and \(\gamma\). Changing those variables can make the curve longer or shorter, taller or flatter. But no choice of parameters will give the curve multiple ups and downs. There are no oscillatory solutions to these equations.</p>
<p>The SIR model strikes me as so plausible that it—or some variant of it—really <em>must</em> be a correct description of the natural course of an epidemic. But that doesn’t mean it can explain what’s going on right now with Covid-19. A key element of the model is saturation: the spread of the disease stalls when there are too few susceptibles left to catch it. That can’t be what caused the steep downturn in Covid incidence that began in January of this year, or the earlier slumps that began in April and July of 2020. We were nowhere near saturation during any of those events, and we still aren’t now. (For the moment I’m ignoring the effects of vaccination. I’ll take up that subject below.)</p>
<p>In Figure 11 there comes a dramatic triple point where each of the three categories constitutes about a third of the total population. If we projected that situation onto the U.S., we would have (in very round numbers) \(100\) million active infections, another \(100\) million people who have recovered from an earlier bout with the virus, and a third \(100\) million who have so far escaped (but most of whom will catch it in the coming weeks). That’s orders of magnitude beyond anything seen so far. The cumulative case count, which combines the \(\mathcal{I}\) and \(\mathcal{R}\) categories, is approaching \(37\) million, or \(11\) percent of the U.S. population. Figure 12<img alt="Us cases avg full population" border="0" class="alignright" height="" src="http://bit-player.org/wp-content/uploads/2021/08/us_cases_avg_full_population.svg" width="330"/>Even if the true case count is double the official tally, we are still far short of the model’s crucial triple point. Judged from the per­spective of the SIR model, we are still in the early stages of the epidemic, where case counts are too low to see in the graph. (If you rescale Figure 1 so that the <em>y</em> axis spans the full U.S. population of 330 million, you get the flatline graph at right.)</p>
<hr/>
<p>If we are still on the early part of the curve, in the regime of rampant exponential growth, it’s easy to understand the surging accelerations we’ve seen in the worst moments of the epidemic. The hard part is explaining the repeated slowdowns in viral transmission that punctuate the Covid curve. In the SIR model, the turnaround comes when the virus begins to run out of victims, but that’s a one-time phenomenon, and we haven’t gotten there yet. What can account for the deep valleys in the <em>Times</em> Covid curve?</p>
<p>Among the ideas that immediately come to mind, one strong contender is feedback. We all have access to real-time information on the status of the epidemic. It comes from governmental agencies, from the news media, from idiots on Facebook, and from the personal communications of family, friends, and neighbors. Most of us, I think, respond appropriately to those messages, modulating our anti-contagion precautions according to the perceived severity of the threat. When it’s scary outside, we hunker down and mask up. When the risk abates, it’s party time again! I can easily imagine a scenario where such on-again, off-again measures would trigger oscillations in the incidence of the disease.</p>
<p>If this hypothesis turns out to be true, it is cause for both hope and frustration. Hope because the interludes of viral retreat suggest that our tools for fighting the epidemic must be reasonably effective. Frustration because the rebounds indicate we’re not deploying those tools as well as we could. Look again at the Covid curve of Figure 1, specifically at the steep downturn following the winter peak. In early February, the new case rate was dropping by \(30{,}000\) per week. Over the first three weeks of that month, the rate was cut in half. Whatever we were doing then, it was working brilliantly. If we had just continued on the same trajectory, the case count would have hit zero in early March. Instead, the downward slope flattened, and then turned upward again. </p>
<p>We had another chance in June. All through April and May, new cases had been falling steadily, from \(65{,}000\) to \(17{,}000\), a pace of about \(-800\) cases a day. If we’d been able to sustain that rate for just three more weeks, we’d have crossed the finish line in late June. But again the trend reversed course, and by now we’re back up well above \(100{,}000\) cases a day.</p>
<p>Are these pointless ups and downs truly caused by feedback effects? I don’t know. I am particularly unsure about the “if only” part of the story—the idea that if only we’d kept the clamps on for just a few more weeks, the virus would have been eradicated, or nearly so. But it’s an idea to keep in mind.</p>
<p>Perhaps we could learn more by creating a feedback loop in the SIR model, and looking for oscillatory dynamics. Negative feedback is anything that acts to slow the infection rate when that rate is high, and to boost it when it’s low. Such a contrarian mechanism could be added to the model in several ways. Perhaps the simplest is a lockdown threshold: Whenever the number of infectives rises above some fixed limit, everyone goes into isolation; when the \(\mathcal{I}\) level falls below the threshold again, all cautions and restrictions are lifted. It’s an all-or-nothing rule, which makes it simple to implement. We need a constant to represent the threshold level, and a new factor (which I am naming \(\varphi\), for fear) in the equation for \(\Delta \mathcal{I}\):</p>
<p>\[\Delta\mathcal{I} = \beta \varphi \mathcal{I} \mathcal{S}\]</p>
<p class="undent">The \(\varphi\) factor is \(1\) whenever \(\mathcal{I}\) is below the threshold, and \(0\) when it rises above. The effect is to shut down all new infections as soon as the threshold is reached, and start them up again when the rate falls.</p>
<p>Does this scheme produce oscillations in the \(\mathcal{I}\) curve? Strictly speaking, the answer is yes, but you’d never guess it by looking at the graph.</p>
<p>Figure 13<img alt="SIR model with feedback but no visible oscillations" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/SIR_model_with_feedback.svg" width=""/></p>
<p class="undent">The feedback loop serves as a control system, like a thermostat that switches the furnace off and on to maintain a set temperature. In this case, the feedback loop holds the infective population steady at the threshold level, which is set at \(0.05\). On close examination, it turns out that \(\mathcal{I}\) is oscillating around the threshold level, but with such a short period and tiny amplitude that the waves are invisible. The value bounces back and forth between \(0.049\) and \(0.051\).</p>
<p>To get macroscopic oscillations, we need more than feedback. The SIR output shown below comes from a model that combines feedback with a delay between measuring the state of the epidemic and acting on that information. Introducing such a delay is not the only way to make the model swing, but it’s certainly a plausible one. As a matter of fact, a model <em>without</em> any delay, in which a society responds instantly to every tiny twitch in the case count, seems wholly unrealistic.</p>
<p>Figure 14<img alt="SIR model with oscillations caused by feedback combined with a sensory delay" border="0" class="alignright" height="" src="http://bit-player.org/wp-content/uploads/2021/08/SIR_model_with_oscillations.svg" width="640"/></p>
<p>The model of Figure 14 adopts the same parameters, \(\beta = 0.6\) and \(\gamma = 0.2\), as the version of Figure 13, as well as the same lockdown threshold \((0.05)\). It differs only in the timing of events. If the infective count climbs above the threshold at time \(t\), control measures do not take effect until \(t + 3\); in the meantime, infections continue to spread through the population. The delay and overshoot on the way up are matched by a delay and undershoot at the other end of the cycle, when lockdown continues for three T<sub>SP</sub> after the threshold is crossed on the way down.</p>
<p>Given these specific parameters and settings, the model produces four cycles of diminishing amplitude and increasing wavelength. (No further cycles are possible because \(\mathcal{I}\) remains below the threshold.) Admittedly, those four spiky sawtooth peaks don’t look much like the humps in the Covid curve. If we’re going to seriously consider the feedback hypothesis, we’ll need stronger evidence than this. But the model is very crude; it could be refined and improved.</p>
<p>The fact is, I really want to believe that feedback could be a major component in the oscillatory dynamics of Covid-19. It would be comforting to know that our measures to combat the epidemic have had a powerful effect, and that we therefore have some degree of control over our fate. But I’m having a hard time keeping the faith. For one thing, I would note that our countermeasures have not always been on target. In the epidemic’s first wave, when the characteristics of the virus were largely unknown, the use of facemasks was discouraged (except by medical personnel), and there was a lot of emphasis on hand washing, gloves, and sanitizing surfaces. Not to mention drinking bleach. Those measures were probably not very effective in stopping the virus, but the wave receded anyway.</p>
<p>Another source of doubt is that wavelike fluctuations are not unique to Covid-19. Figure 15<img alt="1918 spanish flu waves  Wikipedia" border="0" class="alignright" height="" src="http://bit-player.org/wp-content/uploads/2021/08/1918_spanish_flu_waves-Wikipedia.gif" width="400"/>On the contrary, they seem to be a common characteristic of epi­demics across many times and places. The \(1918\textrm{–}1919\) influ­enza epidemic had at least three waves. Figure 15, which <a href="https://commons.wikimedia.org/wiki/File:1918_spanish_flu_waves.gif">Wikipedia attrib­utes</a> to the CDC, shows influ­enza deaths per \(1{,}000\) people in the United Kingdom. Those humps look awfully familiar. They are similar enough to the Covid waves that it seems natural to look for a common cause. But if both patterns are a product of feedback effects, we have to suppose that public health measures undertaken a century ago, in the middle of a world war, worked about as well as those today. (I’d like to think there’s been some progress.)</p>
<hr/>
<p>One detail of the SIR model troubles me. As formulated by Kermack and McKendrick, the model treats infection and recovery as symmetrical, mirror-image processes, both of them described by exponential functions. The exponential rule for infections makes biological sense. You can only get the virus via transmission from someone who already has it, so the number of new infections is proportional to the number of existing infections. But recovery is different; it’s not contagious. Although the duration of the illness may vary to some extent, there’s no reason to suppose it would depend on the number of other people who are sick at the same time.</p>
<p>In the model, a fixed fraction of the infectives, \(\gamma \mathcal{I}\), recover at every time step. Figure 16<img alt="Exponential decline bargraph" border="0" class="alignright" height="" src="http://bit-player.org/wp-content/uploads/2021/08/exponential_decline_bargraph.svg" width=""/>For \(\gamma = 0.2\), this rule generates the ex­ponential distribution seen in Figure 16. Imagine that some large group of people have all been infected at the same time, \(t = 0\). At \(t = 1\), a fifth of the infectives recover, leaving \(80\) per­cent of the cohort still infective. At \(t = 2\), a fifth of the remaining \(80\) per­cent are removed from the infective class, leaving \(64\) percent. And so it goes. Even after \(10\) T<sub>SP</sub>, more than \(10\) percent of the original group remain infectious.</p>
<p>The long tail of this distribution corresponds to illnesses that persist for many weeks. Such cases exist, but they are rare. <a href="https://www.cdc.gov/coronavirus/2019-ncov/hcp/duration-isolation.html">According to the CDC</a>, most Covid patients have no detectable “replication-competent virus” \(10\) days after the onset of symptoms. Even in the most severe cases, with immunocompromised patients, \(20\) days of infectivity seems to be the outer limit. I don’t know who was the first to notice that the exponential distribution of recovery times is too broad, but I know it wasn’t me. In 2001 Alun L. Lloyd wrote on this theme in the context of measles epidemics (<a href="https://pubmed.ncbi.nlm.nih.gov/11589638/"><em>Theoretical Population Biology</em> Vol. 60, No. 1, pp. 59–71</a>).These observations suggest a different strategy for modeling recovery. Rather than assuming that a fixed fraction of patients recover at every time step, we might get a better approximation to the truth by assuming that all patients recover (or at least become noninfective) after a fixed duration of illness.</p>
<p>Modifying the model for a fixed period of infectivity is not difficult. We can keep track of the infectives with a data structure called a queue. Each new batch of newly recruited infectives goes into the tail of the queue, then advances one place with each time step. After \(m\) steps (where \(m\) is the duration of the illness), the batch reaches the head of the queue and joins the company of the recovered. Here is what happens when \(m = 3\) T<sub>SP</sub>:</p>
<p>Figure 17<img alt="SIR model outcome when infection lasts 3 TSP." border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/sirq_0p000001_0p6_3p0_0p1_100.svg" width="640"/></p>
<p>I chose \(3\) T<sub>SP</sub> for this example because it is close to the median duration in the expon­ential distribution in Figure 11, and therefore ought to resemble the earlier result. And so it does, approximately. As in Figure 11, the peak in the infectives curve lies near where the susceptible and recovered curves cross. But the peak never grows quite as tall; and, for obvious reasons, it decays much faster. As a result, the epidemic ends with many more susceptibles untouched by the disease—more than 25 percent.</p>
<p>A disease duration of \(3\) T<sub>SP</sub>, or about \(15\) days, is still well over the CDC estimates of the typical length. Shortening the queue to \(2\) T<sub>SP</sub>, or about \(10\) days, transforms the outcome even more dramatically. Now the susceptible and recovered curves never cross, and almost \(70\) percent of the susceptible population remains uninfected when the epidemic peters out.</p>
<p>Figure 18<img alt="SIR model outcome when infection lasts 2 TSP" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/sirq_0p000001_0p6_2p0_0p1_100.svg" width="640"/></p>
<p>Figure 18 comes a little closer to describing the current Covid situation in the U.S. than the other models considered above. It’s not that the curves’ shape resembles that of the data, but the overall magnitude or intensity of the epidemic is closer to observed levels. Of the models presented so far, this is the first that reaches a natural limit without burning through most of the population. Maybe we’re on to something.</p>
<p>On the other hand, there are a couple of reasons for caution. First, with these parameters, the initial growth of the epidemic is extremely slow; it takes \(40\) or \(50\) T<sub>SP</sub> before infections have a noticeable effect on the population. That’s well over six months. Second, we’re still dealing with a one-hump camel. Even though most of the population is untouched, the epidemic has run its course, and there will not be a second wave. Something important is still missing.</p>
<p>Before leaving this topic behind, I want to point out that the finite time span of a viral infection gives us a special point of leverage for controlling the spread of the disease. The viruses that proliferate in your body must find a new host within a week or two, or else they face extinction. Therefore, if we could completely isolate every individual in the country for just two or three weeks, the epidemic would be over. Admittedly, putting each and every one of us into solitary confinement is not feasible (or morally acceptable), but we could strive to come as close as possible, strongly discouraging all forms of person-to-person contact. Testing, tracing, and quarantines would deal with straggler cases. My point is that a very strict but brief lockdown could be both more effective and less disruptive than a loose one that goes on for months. Where other strategies aim to flatten the curve, this one attempts to break the chain.</p>
<hr/>
<p>When Covid emerged late in 2019, it was soon labeled a <em>pandemic</em>, signifying that it’s bigger than a mere epidemic, that it’s everywhere. But it’s not everywhere at once. Flareups have skipped around from region to region and country to country. Perhaps we should view the pandemic not as a single global event but as an ensemble of more localized outbreaks.</p>
<p>Suppose small clusters of infections erupt at random times, then run their course and subside. By chance, several geographically isolated clusters might be active over the same range of dates and add up to a big bump in the national case tally. Random fluctuations could also produce interludes of widespread calm, which would cause a dip in the national curve.</p>
<p>We can test this notion with a simple computational experiment, modeling a popu­lation divided into \(N\) clusters or communities. For each cluster a SIR model generates a curve giving the proportion of infectives as a function of time. The initiation time for each of these mini-epidemics is chosen randomly and independently. Summing the \(N\) curves gives the total case count for the country as a whole, again as a function of time.</p>
<p>Before scrolling down to look at the graphs generated by this process, you might make a guess about how the experiment will turn out. In particular, how will the shape of the national curve change as the number of local clusters increases?</p>
<p>If there’s just one cluster, then the national curve is obviously identical to the trajectory of the disease in that one place. With two clusters, there’s a good chance they will not overlap much, and so the national curve will probably have two humps, with a deep valley between them. With \(N = 3\) or \(4\), overlap becomes more of an issue, but the sum curve still seems likely to have \(N\) humps, perhaps with shallower depressions separating them. Before I saw the results, I made the following guess about the behavior of the sum as \(N\) continues increasing: The sum curve will always have approximately \(N\) peaks, I thought, but the height difference between peaks and troughs should get steadily smaller. Thus at large \(N\) the sum curve would have many tiny ripples, small enough that the overall curve would appear to be one broad, flat-topped hummock.</p>
<p>So much for my intuition. Here are two examples of sum curves generated by clusters of \(N\) mini-epidemics, one curve for \(N = 6\) and one for \(N = 50\). The histories for individual clusters are traced by fine red lines; the sums are blue. All the curves have been scaled so that the highest peak of the sum curve touches \(1.0\).</p>
<p>Figure 19<img alt="MultiSVG curves 6 and 50 with 4 peaks" border="0" class="alignleft" height="" src="http://bit-player.org/wp-content/uploads/2021/08/multiSVG_curves_6_and_50_with_4_peaks.svg" width="900"/></p>
<p class="indent">My Technical details: Cluster initiation time is chosen uniformly at random between \(0\) and \(80\). \(\beta\) is a random normal variable with mean \(0.6\) and standard deviation \(0.1\), allowing some variation in the intensity and duration of the individual sub-epidemics. The initial infective level is \(0.001\).guess about the “broad, flat-topped hummock” with many shallow ripples was altogether wrong. The number of peaks does <em>not</em> increase in proportion to \(N\). As a matter of fact, both of the sum curves in Figure 19 have four distinct peaks (possibly five in the example at right), even though the number of component curves contributing to the sum is only six in one case and is \(50\) in the other.</p>
<p>I have to confess that the two examples in Figure 19 were not chosen at random. I picked them because they looked good, and because they illustrated a point I wanted to make—namely that the number of peaks in the sum curve remains nearly constant, regardless of the value of \(N\). Figure 20 assembles a more representative sample, selected without deliberate bias but again showing that the number of peaks is not sensitive to \(N\), although the valleys separating those peaks get shallower as \(N\) grows.</p>
<p>Figure 20<img alt="MultiSIR tableau A" border="0" class="alignleft" height="" src="http://bit-player.org/wp-content/uploads/2021/08/multiSIR_tableau_A.svg" width="900"/></p>
<p class="indent">The Figure 21<img alt="Peak numbers bar graph" border="0" class="marginalia alignleft" height="" src="http://bit-player.org/wp-content/uploads/2021/08/peak_numbers_bar_graph.svg" width=""/>takeaway message from these simulations seems to be that almost any collection of randomly timed mini-epidemics will combine to form a macro-epidemic with just a few waves. The number of peaks is not always four, but it’s seldom very far from that number. The bar graphs in Figure 21 offer some quantitative evidence on this point. They record the distribution of the number of peaks in the sum curve for values of \(N\) between \(4\) and \(100\). Each set of bars represents \(1{,}000\) repetitions of the process. In all cases the peak falls at \(N = 3, 4,\) or \(5\).</p>
<p>The question is: Why \(4 \pm 1\)? Why do we keep seeing those particular numbers? And if \(N\), the number of components being summed, has little influence on this property of the sum curve, then what <em>does</em> govern it? I puzzled over these questions for some time before a helpful analogy occurred to me. </p>
<p>Suppose you have a bunch of sine waves, all at the same frequency \(f\) but with randomly assigned phases; that is, the waves all have the same shape, but they are shifted left or right along the \(x\) axis by random amounts. What would the sum of those waves look like? The answer is: another sine wave of frequency \(f\). This is a little fact that’s been known for ages (at least since Euler) and is <a href="https://math.stackexchange.com/questions/535600/sum-of-sinusoids-with-same-frequency-sinusoid-proof/1239123">not hard to prove</a>, but it still comes as a bit of a shock every time I run into it. I believe the same kind of argument can explain the behavior of a sum of SIR curves, even though those curves are not sinusoidal. The component SIR curves have a period of \(20\) to \(30\) T<sub>SP</sub>. In a model run that spans \(100\) T<sub>SP</sub>, these curves can be considered to have a frequency of between three and five cycles per epidemic period. Their sum should be a wave with the same frequency—something like the Covid curve, with its four (or four and a half) prominent humps. In support of this thesis, when I let the model run to \(200\) T<sub>SP</sub>, I get a sum curve with seven or eight peaks.</p>
<p>I am intrigued by the idea that an epidemic might arrive in cyclic waves not because of anything special about viral or human behavior but because of a mathematical process akin to wave interference. It’s such a cute idea, dressing up an obscure bit of counterintuitive mathematics and bringing it to bear on a matter of great importance to all of us. And yet, alas, a closer look at the Covid data suggests that nature doesn’t share my fondness for summing waves with random phases.</p>
<p>Figure 22, again based on data extracted from the <em>Times</em> archive, plots \(49\) curves, representing the time course of case counts in the Lower \(48\) states and the District of Columbia. I have separated them by region, and in each group I’ve labeled the trace with the highest peak. We already know that these curves yield a sum with four tall peaks; that’s where this whole investigation began. But the \(49\) curves do not support the notion that those peaks might be produced by summing randomly timed mini-epidemics. The oscillations in the \(49\) curves are <em>not</em> randomly timed; there are strong correlations between them. And many of the curves have multiple humps, which isn’t possible if each mini-epidemic is supposed to act like a SIR model that runs to completion. </p>
<p>Figure 22<img alt="Covid curves for 48 U.S. states and the District of Columbia, in six regional groups." border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/region_curves.svg" width="640"/></p>
<p>Although these curves spoil a hypothesis I had found alluring, they also reveal some interesting facts about the Covid epidemic. I knew that the first wave was concentrated in New York City and surrounding areas, but I had not realized how much the second wave, in the summer of 2020, was confined to the country’s southern flank, from Florida all the way to California. The summer wave this year is also most intense in Florida and along the Gulf Coast. Coincidence? When I showed the graphs to a friend, she responded: “Air conditioning.”</p>
<hr/>
<p>Searching for the key to Covid, I’ve tried out three slightly whimsical notions: the possibility of a periodic signal, like the sunspot cycle, bringing us waves of infection on a regular schedule; feedback loops producing yo-yo dynamics in the case count; and randomly timed mini-epidemics that add up to a predictable, slow variation in the infection rate. In retrospect they still seem like ideas worth looking into, but none of them does a convincing job of explaining the data. </p>
<p>In my mind the big questions remain unanswered. In November of 2020 the daily tally of new Covid cases was above \(100{,}000\) and rising at a fearful rate. Three months later the infection rate was falling just as steeply. What changed between those dates? What action or circumstance or accident of fate blunted the momentum of the onrushing epidemic and forced it into retreat? And now, just a few months after the case count bottomed out, we are again above \(100{,}000\) cases per day and still climbing. What has changed again to bring the epidemic roaring back?</p>
<p>There are a couple of obvious answers to these questions. As a matter of fact, those answers are sitting in the back of the room, frantically waving their hands, begging me to call on them. First is the vaccination campaign, which has now reached half the U.S. population. The incredibly swift development, manufacture, and distribution of those vaccines is a wonder. In the coming months and years they are what will save us, if anything can. But it’s not so clear that vaccination is what stopped the big wave last winter. The sharp downturn in infection rates began in the first week of January, when vaccination was just getting under way in the U.S. On January 9 (the date when the decline began) only about \(2\) percent of the population had received even one dose. The vaccination effort reached a peak in April, when more than three million doses a day were being administered. By then, however, the dropoff in case numbers had stopped and reversed. If you want to argue that the vaccine ended the big winter surge, it’s hard to align causation with chronology.</p>
<p>On the other hand, the level of vaccination that has now been achieved should exert a powerful damping effect on any future waves. Removing half the people from the susceptible list may not be enough to reach herd immunity and eliminate the virus from the population, but it ought to be enough to turn a growing epidemic into a wilting one.</p>
<p>Figure 23<img alt="SIR model with vaccination of 50 percent of the population and R_0 left unchanged at 3.0." border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/sirv_graph_0p6_0p2_0p5_0.svg" width=""/></p>
<p>The SIR model of Figure 23 has the same parameters as the model of Figure 3 \((\beta = 0.6, \gamma = 0.2,\) implying \(\rho = 3.0)\), but \(50\) percent of the people are vaccinated at the start of the simulation. With this diluted population of susceptibles, essentially nothing happens for almost a year. The epidemic is dormant, if not quite defunct.</p>
<p>That’s the world we should be living in right now, according to the SIR model. Instead, today’s new case count is \(141{,}365\); almost \(81{,}556\) people are hospitalized with Covid infections; and 704 people have died. What gives? How can this be happening?</p>
<p>At this point I must acknowledge the other hand waving in the back of the room: the Delta variant, otherwise known as B.1.617.2. Half a dozen mutations in the viral spike protein, which binds to a human cell-surface receptor, have apparently made this new strain at least twice as contagious as the original one.</p>
<p>Figure 24<img alt="Sirv graph 0p6 0p1 0p5 0" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/sirv_graph_0p6_0p1_0p5_0.svg" width=""/></p>
<p class="undent">In Figure 24 contagiousness is doubled by increasing \(\rho\) from \(3.0\) to \(6.0\). That boost brings the epidemic back to life, although there is still quite a long delay before the virus becomes widespread in the unvaccinated half of the population.</p>
<p>The situation is may well be worse than the model suggests. All the models I have reported on here pretend that the human population is homogeneous, or thoroughly mixed. If an infected person is about to spread the virus, everyone in the country has the same probability of being the recipient. This assumption greatly simplifies the con­struction of the model, but of course it’s far from the truth. In daily life you most often cross paths with people like yourself—people from your own neighborhood, your own age group, your own workplace or school. Those frequent contacts are also people who share your vaccination status. If you are unvaccinated, you are not only more vulnerable to the virus but also more likely to meet people who carry it. This somewhat subtle birds-of-a-feather effect is what allows us to have “<a href="https://www.nytimes.com/2021/07/16/health/covid-delta-cdc-walensky.html">a pandemic of the unvaccinated</a>.”</p>
<p>Recent reports have brought still more unsettling and unwelcome news, with evidence that even fully vaccinated people may sometimes spread the virus. I’m waiting for confirmation of that before I panic. (But I’m waiting with my face mask on.)</p>
<p>Having demonstrated that I understand nothing about the history of the epidemic in the U.S.—why it went up and down and up and down and up and down and up and down—I can hardly expect to understand the present upward trend. About the future I have no clue at all. Will this new wave tower over all the previous ones, or is it Covid’s last gasp? I can believe anything.</p>
<p>But let us not despair. This is not the zombie apocalypse. The survival of humanity is not in question. It’s been a difficult ordeal for the past \(18\) months, and it’s not over yet, but we can get through this. Perhaps, at some point in the not-too-distant future, we’ll even understand what’s going on.</p>
<h3>Data and Source Code</h3>
<p><em>The New York Times</em> data archive for Covid-19 cases and deaths in the United States is available in <a href="https://github.com/nytimes/covid-19-data">this GitHub repository</a>. The version I used in preparing this article, cloned on 21 July 2021, is identified as “commit c3ab8c1beba1f4728d284c7b1e58d7074254aff8″. You should be able to access the identical set of files through <a href="https://github.com/nytimes/covid-19-data/tree/c3ab8c1beba1f4728d284c7b1e58d7074254aff8">this link</a>.</p>
<p>Source code for the SIR models and for generating the illustrations in this article is also <a href="https://github.com/bit-player/Covidcurve">available on GitHub</a>. The code is written in the <a href="https://julialang.org/">Julia programming language</a> and organized in <a href="https://github.com/fonsp/Pluto.jl">Pluto</a> notebooks.</p>
<h3>Further Reading</h3>
<p class="biblio">Bartlett, M. S. 1956. Deterministic and stochastic models for recurrent epidemics. In <em>Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 4: Contributions to Biology and Problems of Health</em>, pp. 81–109. Berkeley: University of California Press.</p>
<p class="biblio">Bartlett, M. S. 1957. Measles periodicity and community size. <em>Journal of the Royal Statistical Society, Series A (General)</em> 120(1):48–70.</p>
<p class="biblio">Brownlee, John. 1917. An investigation into the periodicity of measles epidemics in London from 1703 to the present day by the method 0f the periodogram. <em>Philosophical Transactions of the Royal Society  of London, Series B</em>, 208:225–250.</p>
<p class="biblio">Daley, D. J., and  J. Gani. 1999.  <em>Epidemic Modelling: An Introduction</em>.  Cam­bridge: Cambridge University Press.</p>
<p class="biblio">Kendall, David G. 1956. Deterministic and stochastic epidemics in closed populations. In <em>Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 4: Contributions to Biology and Problems of Health</em>, pp. 149–165. Berkeley: University of California Press.</p>
<p class="biblio">Kermack, W. O., and A. G. McKendrick. 1927. A contribution to the mathematical theory of epidemics. <em>Proceedings of the Royal Society of London, Series A</em> 115:700–721.</p>
<p class="biblio">Lloyd, Alun L. 2001. Realistic distributions of infectious periods in epidemic models: changing patterns of persistence and dynamics. <em>Theoretical Population Biology</em> 60:59–71.</p>
<p class="biblio">Smith?, Robert (editor). 2014. <em>Mathematical Modelling of Zombies</em>. Ottawa: University of Ottawa Press.</p></div>
    </content>
    <updated>2021-08-18T22:00:22Z</updated>
    <published>2021-08-18T22:00:22Z</published>
    <category scheme="http://bit-player.org" term="biology"/>
    <category scheme="http://bit-player.org" term="computing"/>
    <category scheme="http://bit-player.org" term="mathematics"/>
    <category scheme="http://bit-player.org" term="modern life"/>
    <category scheme="http://bit-player.org" term="statistics"/>
    <author>
      <name>Brian Hayes</name>
      <uri>http://bit-player.org</uri>
    </author>
    <source>
      <id>http://bit-player.org/feed/atom</id>
      <link href="http://bit-player.org" rel="alternate" type="text/html"/>
      <link href="http://bit-player.org/feed/atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">An amateur's outlook on computation and mathematics</subtitle>
      <title xml:lang="en-US">bit-player</title>
      <updated>2021-08-25T14:02:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/120</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/120" rel="alternate" type="text/html"/>
    <title>TR21-120 |  How to Find Water in the Ocean: A Survey on Quantified Derandomization | 

	Roei Tell</title>
    <summary>The focus of this survey is the question of *quantified derandomization*, which was introduced by Goldreich and Wigderson (2014): Does derandomization of probabilistic algorithms become easier if we only want to derandomize algorithms that err with extremely small probability? How small does this probability need to be in order for the problem's complexity to be affected?

This question opens the door to studying natural relaxed versions of the derandomization problem, and allows us to construct algorithms that are more efficient than in the general case as well as to make gradual progress towards solving the general case. In the survey I describe the body of knowledge accumulated since the question's introduction, focusing on the following directions and results:

1. *Hardness vs ``quantified'' randomness:* Assuming sufficiently strong circuit lower bounds, we can derandomize probabilistic algorithms that err extremely rarely while incurring essentially no time overhead.

2. For general probabilistic polynomial-time algorithms, *improving on the brute-force algorithm for quantified derandomization implies breakthrough circuit lower bounds*, and this statement holds for any given probability of error.

3. Unconditional *algorithms for quantified derandomization of low-depth circuits and formulas*, as well as *near-matching reductions* of the general derandomization problem to quantified derandomization for such models.

4. *Arithmetic quantified derandomization*, and in particular constructions of hitting-set generators for polynomials that vanish extremely rarely. 

5. *Limitations of certain black-box techniques* in quantified derandomization, as well as a tight connection between black-box quantified derandomization and the classic notion of *pseudoentropy*.

Most of the results in the survey are from known works, but several results are either new or are strengthenings of known results. The survey also offers a host of concrete challenges and open questions surrounding quantified derandomization.</summary>
    <updated>2021-08-18T20:15:46Z</updated>
    <published>2021-08-18T20:15:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-26T03:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21911</id>
    <link href="https://gilkalai.wordpress.com/2021/08/18/to-cheer-you-up-in-difficult-times-29-free-will-predictability-and-quantum-computers/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 29: Free will, predictability and quantum computers</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I wrote a paper, in Hebrew, entitled “Free will, predictability and quantum computers.” Click for the pdf file (Version of Aug. 24, 2021; orig. version). As you probably know, the free will problem is the apparent contradiction between the fact … <a href="https://gilkalai.wordpress.com/2021/08/18/to-cheer-you-up-in-difficult-times-29-free-will-predictability-and-quantum-computers/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I wrote a paper, in Hebrew, entitled “<a href="https://gilkalai.files.wordpress.com/2021/08/free-will-aug-24.pdf">Free will, predictability and quantum computers</a>.” Click for the pdf file (Version of Aug. 24, 2021; <a href="https://gilkalai.files.wordpress.com/2021/08/freewill.pdf">orig. version</a>). As you probably know, the free will problem is the apparent contradiction between the fact that the laws of nature are deterministic on the one hand, and the notion that people are making free choices that affect their future on the other hand. Here on my blog, I mentioned the free will problem twice: once, briefly, in connection with a <a href="https://gilkalai.wordpress.com/2008/09/03/the-prisonner-dilemma-sympathy-and-yaaris-challenge/">lecture by Menachem Yaari</a> (2008), and once in <a href="https://gilkalai.wordpress.com/2017/10/30/test-your-intuition-33-the-great-free-will-poll/">TYI 33</a> (2017) while conducting the “great free will poll” (see picture below for the outcomes). As for the paper, I hesitated whether to write it in Hebrew or in English; I finally chose Hebrew and I plan to prepare also an English version sometime in the fall.</p>
<p>Here is the abstract of my new paper.</p>
<blockquote>
<p style="text-align: right;"><span style="color: #0000ff;"><strong>תקציר:</strong> המאמר עוסק בקשר בין השאלות הנוגעות להיתכנות מחשבים קוונטיים, הפרדיקטביליות של מערכות קוונטיות מורכבות בטבע, והסתירה הקיימת לכאורה בין חוקי הטבע לבין רצון חופשי. נדון במקביל במחשב הקוונטי “סיקמור” בעל 12 יחידות חישוב (קיוביטים), ובאליס, שעל רצונה החופשי ננסה לתהות. התאוריה של המחבר העוסקת באי האפשרות של חישוב קוונטי, מצביעה באופן ישיר על אי האפשרות לנבא במדויק את מחשב הסיקמור, כמו גם את מוחה של אליס.  בניתוח מורכב יותר נראה, שאי האפשרות של חישוב קוונטי תומכת בגישה לפיה חוקי הטבע אינם שוללים בחירה חופשית. בבסיס הטיעון הזה עומדת עמימות בדרך שבה העתיד נקבע מהעבר ואשר איננה נעוצה באופי המתמטי של חוקי הפיסיקה (שהם לגמרי דטרמיניסטים), אלא בתיאור הפיסיקלי של העצמים שאנו דנים בהם. אנו דנים גם בהפרדה בין טענות לגבי העתיד שטמונות במארג הסיבתי הקיים בין העבר והעתיד, לבין טענות הנוגעות לעתיד ואשר אינן נמצאות במארג זה</span></p>
<p><em><span style="color: #0000ff;"><strong>Abstract:</strong> We study the connection between the possibility of quantum computers, the predictability of complex quantum systems in nature, and the free will problem. We consider in parallel two examples: the Sycamore quantum computer with 12 qubits (computing elements) and Alice, whose decisions and free will we try to question. The author’s theory that quantum computation is impossible (in principle) directly indicates that the future of both Alice’s brain and the Sycamore quantum computer cannot be predicted. A more involved analysis shows that failure of quantum computation supports the view that the laws of nature do not contradict free will. At the center of the argument is ambiguity in the way the future is determined by the past, not in terms of the mathematical laws of physics (which are fully deterministic) but in terms of the physical description of the objects we discuss. In addition, we discuss the separation between claims about the future that belong to the causal fabric of the past and the future, and claims that don’t belong to this fabric.</span></em></p>
</blockquote>
<p>Two examples that we consider in parallel throughout the paper are “Alice”, whose free will we try to explore, and Google’s “Sycamore” quantum computer with 12 qubits. So, even if Google’s Sycamore does not lead to “quantum supremacy” (and I suspect it does not!) it could still be used in the pursuit of human free will <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/> . Aram Harrow’s hypothetical quantum computer from our 2012 quantum debate (<a href="https://rjlipton.wpcomstaging.com/2012/03/05/the-quantum-super-pac/">post 4</a>) also plays a role in my paper.</p>
<p>My interest in the problem was provoked by Avishai Margalit in the mid 90s. Later I was engaged in a long email debate with <a href="https://gilkalai.wordpress.com/2012/11/25/happy-birthday-ron-aharoni/">Ron Aharoni</a> following his 2009 (Hebrew) book on philosophy about this problem and related philosophical questions.  Since then, I have been following with interest writings on the problem by Scott Aaronson, Sabine Hossenfelder, Tzahi Gilboa, and others, and had brief discussions about free will with Bernard Chazelle and a few other friends. Writing the paper provided me an<br/>immensely enjoyable opportunity to discuss the problem once again with<br/>philosophers, friends and colleagues.</p>
<p>One question that I initially discussed but later left out is the following:</p>
<blockquote>
<p><span style="color: #ff0000;"><em>From the point of view of people in the mid 20th century, did quantum mechanics offer an opportunity for understanding the apparent contradiction between free will and the deterministic laws of nature? (Schrödinger himself wrote a paper where he was skeptical about this.)</em></span></p>
</blockquote>
<p>My <em>a priori</em> intuition about the free will problem is in analogy with Zeno’s famous motions paradoxes. Zeno’s paradoxes offered an opportunity to re-examine the mathematics and physics of motion while not shaking the common sense understanding of motion. (In fact, the ancient Greeks knew enough about the mathematics and physics of motion to conclude that motion is possible and that Achilles will overtake the tortoise, and they could compute precisely when.)  Similarly, the question of free will is an opportunity to explore determinism and related issues, but probably not to challenge our basic understanding of human choice. </p>
<p>Here are a few relevant links. Papers by <a href="https://idp.nature.com/authorize/casa?redirect_uri=https://www.nature.com/articles/138013a0.pdf%3Forigin%3Dppub&amp;casa_token=QYfC6JDMEysAAAAA:3L1hIHNrjrkwAfM7R3gDlMOiNf6qt2G0gfeIbv-uEktIikDVOz-0m38p2QEyQHAEMw4hKN9rxXH_mS4F8w">Schrödinger</a> (1936), <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwifwZvCkbDyAhUK_rsIHZ-WCNIQFnoECAYQAQ&amp;url=http%3A%2F%2Ftotallyrandom.info%2Fwp-content%2Fuploads%2F2018%2F05%2FBohrcausandcomp.pdf&amp;usg=AOvVaw2GWtvQ0cMc-G8srN2dE57z">Bohr</a> (1937), and an essay by Einstein on free will (see picture below). <a href="https://www.jstor.org/stable/23349956?seq=1#metadata_info_tab_contents">Ron Aharoni’s paper (Hebrew)</a> on Newcomb’s paradox published in “Iyun” from 1984 (Ron kindly agreed to explain his view on the FW problem in some future post.); <a href="https://www.scottaaronson.com/blog/?p=1438">A post</a> by Scott Aaronson about <a href="http://arxiv.org/abs/1306.0159">his paper</a> on the matter; Posts by Sabine Hossenfelder (<a href="http://backreaction.blogspot.com/2016/01/free-will-is-dead-lets-bury-it.html">2016</a>), (<a href="http://backreaction.blogspot.com/2014/01/10-misconceptions-about-free-will.html">2014</a>), (<a href="http://backreaction.blogspot.com/2020/10/you-dont-have-free-will-but-dont-worry.html">2020</a>), (<a href="http://backreaction.blogspot.com/2013/07/you-probably-have-no-free-will-but-dont.html">2013</a>), (<a href="http://backreaction.blogspot.com/2019/05/how-to-live-without-free-will.html">2019</a>), (<a href="http://backreaction.blogspot.com/2011/09/predetermined-lunch-and-moral.html">2011</a>), (<a href="http://backreaction.blogspot.com/2012/02/free-will-function.html">2012</a>) (and <a href="https://arxiv.org/abs/1202.0720">her paper</a> on the matter); <a href="https://www.preposterousuniverse.com/blog/2011/07/13/free-will-is-as-real-as-baseball/">A post</a> by Sean Carrol (2011); <a href="https://www.tau.ac.il/~igilboa/pdf/Gilboa_Free_Will.pdf">Itzhak Gilboa’s take</a>; A <a href="https://arxiv.org/abs/2104.11591">paper</a> by Neven, Read and Rees with a proposed engineering of conscious quantum animat that possesses agency and feelings.</p>
<p>This post can be seen as my response to <a href="https://gilkalai.wordpress.com/2017/10/30/test-your-intuition-33-the-great-free-will-poll/">TTY33</a> for which <a href="https://gilkalai.wordpress.com/2017/10/30/test-your-intuition-33-the-great-free-will-poll/#comment-36833">Ori was eagerly waiting</a>. The paper could be seen as a FW booster for Hebrew-speaking audience, and, as I said, I hope to produce a similar FW booster for English-speaking audience in the upcoming fall.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/08/fw-pic.png"><img alt="FW-pic" class="alignnone size-full wp-image-21924" height="796" src="https://gilkalai.files.wordpress.com/2021/08/fw-pic.png" width="1144"/></a></p>
<p><span style="color: #ff0000;">(Clockwise) The cover of Jennan Ismael’s 2016 book, Alice, the Sycamore computer, and a cartoon about free will.</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2021/08/fw-pic5.png"><img alt="FW-pic5" class="alignnone size-full wp-image-21993" height="868" src="https://gilkalai.files.wordpress.com/2021/08/fw-pic5.png" width="1279"/></a></p>
<p><span style="color: #ff0000;">Three figures from the paper</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2021/08/fw-pic2.png"><img alt="FW-pic2" class="alignnone size-full wp-image-21934" height="1053" src="https://gilkalai.files.wordpress.com/2021/08/fw-pic2.png" width="1321"/></a></p>
<p><span style="color: #ff0000;">Einstein’s Essay on free will (left) and the outcomes of our 2017 great free will poll. (Einstein’s opening statement about the moon was inspired by Spinoza.)<br/></span></p>


<p/></div>
    </content>
    <updated>2021-08-18T19:07:55Z</updated>
    <published>2021-08-18T19:07:55Z</published>
    <category term="Philosophy"/>
    <category term="Quantum"/>
    <category term="Free will"/>
    <category term="predictability"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-08-26T03:37:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/08/18/doomed-apartments</id>
    <link href="https://11011110.github.io/blog/2021/08/18/doomed-apartments.html" rel="alternate" type="text/html"/>
    <title>Doomed apartments</title>
    <summary>The University of California, Irvine, maintains a sprawling faculty housing complex, University Hills, in which I live, in order to make homes affordable in what would otherwise be a very expensive part of the country. Most of it is single-family homes, owned by faculty on long-term land leases, but it also has several clusters of rental units. The oldest of these, and the nearest to my office, is unimaginatively named Las Lomas (“the hills” in Spanish, a frequent language for California place names).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The University of California, Irvine, maintains a sprawling faculty housing complex, <a href="https://en.wikipedia.org/wiki/University_Hills,_Irvine">University Hills</a>, in which I live, in order to make homes affordable in what would otherwise be a very expensive part of the country. Most of it is single-family homes, owned by faculty on long-term land leases, but it also has several clusters of rental units. The oldest of these, and the nearest to my office, is unimaginatively named Las Lomas (“the hills” in Spanish, a frequent language for California place names).</p>

<p style="text-align: center;"><img alt="Las Lomas Apartments, UC Irvine" src="https://www.ics.uci.edu/~eppstein/pix/laslomas/C-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p>I haven’t spent a lot of time within Las Lomas, but I walk past it every time I go to work, as it’s the closest part of University Hills to my office. It’s mainly been used for short-term visitors: when Martin Nöllenburg was a postdoc, he lived there, and when Vijay Vazirani joined our department he stayed there until his house with Milena Mihail (who moved here a little later) became ready.</p>

<p style="text-align: center;"><img alt="Las Lomas Apartments, UC Irvine" src="https://www.ics.uci.edu/~eppstein/pix/laslomas/G-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p>Already in December 2019, the university announced that it would demolish Las Lomas to make way for higher-density for-purchase faculty homes, even less imaginatively named “<a href="https://icha.uci.edu/area12/">Area 12</a>”. The last apartment residents were kicked out last Spring, and it’s been closed off to vehicles (but still open to pedestrians, with the grounds still maintained) since then. The most recent timeline I’ve seen has demolition scheduled to begin in November, with move-ins to the new homes ready in time for the 2023–2024 academic year. I’m not sure what it’s likely to end up looking like; the project site used to have two site plans (<a href="https://icha.uci.edu/wp-content/uploads/2020/04/ICHA-PA12-CONCEPTUAL-SITE-PLAN-EXHIBIT_2020-04-27-with-Ortho-Image-at-150dpi-1.pdf">1</a>, <a href="https://icha.uci.edu/wp-content/uploads/2020/04/PA12-illustrative-L1.1_Illustrative-Site-Plan-2.pdf">2</a>) and <a href="https://web.archive.org/web/20200922150048/https://icha.uci.edu/area12/">elevations</a> but they’re no longer linked and maybe no longer current.</p>

<p>Because of its imminent demise, I made a project this summer of visiting Las Lomas and taking some photos. There are no residents, and some of the amenities have been slowly falling apart for lack of maintenance, but it still has a quiet residential feeling.</p>

<p style="text-align: center;"><img alt="Las Lomas Apartments, UC Irvine" src="https://www.ics.uci.edu/~eppstein/pix/laslomas/K-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p><a href="https://www.ics.uci.edu/~eppstein/pix/laslomas/">The rest of the photo album</a>.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106779815577019752">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-08-18T16:46:00Z</updated>
    <published>2021-08-18T16:46:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-08-18T23:52:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://kamathematics.wordpress.com/?p=328</id>
    <link href="https://kamathematics.wordpress.com/2021/08/18/how-to-ask-for-a-letter-of-recommendation/" rel="alternate" type="text/html"/>
    <title>How to Ask for a Letter of Recommendation</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The end of the year is approaching at an alarming rate, which means that many students will require letters of recommendations for graduate school or jobs. These letters are written by more senior academics and researchers at the busiest time of the year, when they’re handling other responsibilities like research, teaching, grant writing, and reviewing, … <a class="more-link" href="https://kamathematics.wordpress.com/2021/08/18/how-to-ask-for-a-letter-of-recommendation/">Continue reading<span class="screen-reader-text"> "How to Ask for a Letter of Recommendation"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The end of the year is approaching at an alarming rate, which means that many students will require letters of recommendations for graduate school or jobs. These letters are written by more senior academics and researchers at the busiest time of the year, when they’re handling other responsibilities like research, teaching, grant writing, and reviewing, and may be writing a dozen letters on top of it all. The purpose of this post is to guide applicants on how to ask for a letter of recommendation, simultaneously strengthening the applicant’s package, and making the letter writing process as painless as possible for their writers. Feel free to share this with anyone applying for grad school or jobs. If someone is asking you for a letter, you could share this post with them, so they know what you need.</p>



<p>Note that this document is written by a computer scientist and intended for applications to computer science things, and may or may not carry over to other fields.</p>



<p><strong>Whom to ask</strong></p>



<p>Ask people who know you and your work well. Ideally people who you’ve worked on research with. Their words will carry the most influence for whatever position you’re applying to. On the other hand, if you don’t have a letter from someone you did a major research project with (especially when applying for grad school), this is a red flag. Aim to have at least one such letter. While grad school applications typically solicit three letters of recommendation, very few people have all focused on research (I personally had two, from co-advisors on one project).</p>



<p>Some have suggested having someone “at arm’s length” is beneficial for faculty applications (and anything that comes later, including awards and fellowships), in order to show that your work is known more broadly in the community. </p>



<p>Letters from people who did nothing more than teach you a course are typically valued much less than research-based letters. Nonetheless, most grad school applicants will have at least one or two such letters, since it is rare to have the opportunity to do research projects with three different advisors. If you must, try to request letters from people who you left an impression on, or otherwise had meaningful interactions with. Try to avoid letters from people who can do nothing more than repeat information on your CV/transcript: these letters are often very short and uninformative, and as with any such letter, are unlikely to help your case (and may even hurt it).</p>



<p><strong>When to ask</strong></p>



<p>Please ask <strong>well </strong>in advance. As a rough rule of thumb, by late October is reasonable for “standard” deadlines which are in December, which is common for CS grad school, postdoc, and faculty applications. You might have to ask earlier if there are earlier deadlines (which is the case for some postdoc applications), but give at least a 6 week lead time. Try to mention the earliest deadline in your email request.</p>



<p>Asking early also benefits the applicant. You should have a solid picture about these key details of your application well in advance.</p>



<p>I may still agree to write a letter for you even after the end of October, but chances decrease the longer you wait. On the other hand, I am more likely to agree to late requests if I know you very well or we have worked together very closely. However, I would still be annoyed, and you probably want to avoid annoying your allies in this process.</p>



<p><strong>What to provide</strong></p>



<p>You should give your letter writers as much information as possible. They might not use all of it, but it is good to have on hand when they are inevitably writing letters at 4 AM the day <s>before</s> after the deadline. I’m writing this as a catch-all list, so some points might not be relevant for the position you’re applying to. Say, when you’re applying for faculty positions, they don’t need to know your undergraduate grades. The goal is to give your writers the gist of who you’re trying to market yourself as, so they can support your story as best they can.</p>



<p>This doesn’t all need to be provided when you first ask, but please share it as early as possible. Make it as easy to find as you can. For example, a single email with everything as an attachment, and a clear title to the email such as “X’s application materials”. You could also share a link to a Dropbox or Google Drive folder with all the materials. This has the benefit that you can include preliminary versions of your materials, and update them as you refine them.</p>



<p>Mandatory:</p>



<ul><li>A shared spreadsheet with a list of all the places you’re applying to, the deadlines, any special instructions for letter submission, and a space to mark when the letter is submitted.</li><li>Your research statement/statement of purpose</li><li>Your CV</li></ul>



<p>Optional but recommended:</p>



<ul><li>Anything else that the application asks you for, e.g. your cover letter, teaching statement,  diversity statement, transcript.</li><li>Who else is writing letters for you, and what your relationship to them is.</li><li>A brief summary of the work you’ve done or the contributions you’ve made with the recommender, to refresh their memory.</li></ul>



<p>The last one is helpful since I like to know what “role” I’m playing. Am I your lead letter writer who people will look to first? Or am I the third letter, mostly confirming what other people already know? There may also be some unique perspective I can give, based on who your other reviewers are.</p>



<p><strong>Afterwards?</strong></p>



<p>To the best of my knowledge, it is not customary in Computer Science to send gifts to your letter writers. You should just thank them genuinely, and pass on the favour when you are in a similar position of power.</p>



<p><strong>What if…</strong></p>



<p>It has been brought to my attention that, unfortunately, many letter writers ask the applicant for either a draft or full letter, which they simply sign. I could write a whole post on this topic, but in short, I consider this unacceptable behaviour from the perspective of the letter writer, and I urge them to rethink this practice. </p>



<p>But what should the applicant do when put in this position? It helps if you can see what typical academic recommendation letters look like, since they have a certain tone, but access to such letters is generally out of the reach of most applicants. My recommendation is to be positive (now is not the time for modesty), but don’t lie (these things can follow you around for longer than you might think). Try to make the first paragraph a brief summary that conveys the overall sentiment of the letter. In subsequent paragraphs, you should describe the research you worked on with the letter writer (don’t assume the reader is highly familiar with the topic, but be succinct in terms of background), as well as your specific contributions to the project. Include anecdotes, if appropriate. This may be awkward, but you have been put into an awkward position.</p>



<p><strong>Acknowledgments</strong></p>



<p>Thanks to Anand Sarwate, Thomas Steinke, and Jon Ullman for helpful comments.</p></div>
    </content>
    <updated>2021-08-18T16:35:21Z</updated>
    <published>2021-08-18T16:35:21Z</published>
    <category term="Academic"/>
    <author>
      <name>Gautam</name>
    </author>
    <source>
      <id>https://kamathematics.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://kamathematics.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://kamathematics.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://kamathematics.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://kamathematics.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Kamathematics</title>
      <updated>2021-08-26T03:38:58Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/08/17/hyperbolic-geometry-squishes</id>
    <link href="https://11011110.github.io/blog/2021/08/17/hyperbolic-geometry-squishes.html" rel="alternate" type="text/html"/>
    <title>Hyperbolic geometry squishes graphs</title>
    <summary>An important theme in information visualization is focus+context: allowing viewers to look at part of a visualization in detail, while still keeping a big picture of the whole. One common method simulates a magnifying glass by having a small zoomed-in window that expands the point your mouse is pointing to. But if this window is placed over the mouse it can block parts of the visualization at an intermediate level of focus, near enough that they’re covered by the zoom window but not so near that they’re visible in it. If it’s off to the side, your eye bounces back and forth between the zoom and the big picture. Instead, another old method for focus+context shows everything in a single view by bringing in hyperbolic geometry. Draw the big visualization in the hyperbolic plane, and then view it using a Poincaré disk model, centered on your point of interest. This smoothly interpolates all scales of interest, from your focus to the whole visualization, into a single view, without any distortion of angles and with controlled distortion of other shape properties.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>An important theme in information visualization is focus+context: allowing viewers to look at part of a visualization in detail, while still keeping a big picture of the whole. One common method simulates a magnifying glass by having a small zoomed-in window that expands the point your mouse is pointing to. But if this window is placed over the mouse it can block parts of the visualization at an intermediate level of focus, near enough that they’re covered by the zoom window but not so near that they’re visible in it. If it’s off to the side, your eye bounces back and forth between the zoom and the big picture. Instead, another old method for focus+context shows everything in a single view by bringing in hyperbolic geometry. Draw the big visualization in the hyperbolic plane, and then view it using a <a href="https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model">Poincaré disk model</a>, centered on your point of interest. This smoothly interpolates all scales of interest, from your focus to the whole visualization, into a single view, without any distortion of angles and with controlled distortion of other shape properties.</p>

<p>Hyperbolic focus+context seems like it could be a good fit for graph drawing. And it can work pretty well for trees, which always have nice drawings in the hyperbolic plane. Here’s <a href="https://commons.wikimedia.org/wiki/File:Hyperbolic_tree_Space_in_general.jpg">an example</a> of what it ends up looking like, produced by information visualization specialist <a href="http://www.johnold.org/">L. John Old</a>:</p>

<p style="text-align: center;"><img alt="Hyperbolic tree visualization, by L. John Old, from https://commons.wikimedia.org/wiki/File:Hyperbolic_tree_Space_in_general.jpg" src="https://11011110.github.io/blog/assets/2021/old-hypertree.jpg"/></p>

<p>But my newest preprint, “Limitations on Realistic Hyperbolic Graph Drawing” (<a href="https://arxiv.org/abs/2108.07441">arXiv:2108.07441</a>) argues that for other kinds of graphs, this idea of drawing a graph hyperbolically and then viewing it through a Euclidean model of hyperbolic geometry has some significant limitations, in some ways more severe than the limitations imposed by making Euclidean drawings (and then using some other method for focus+context).</p>

<p>To understand why, it’s important to know that hyperbolic geometry lacks the scalability of Euclidean geometry. What I mean is that, in Euclidean geometry, you can expand any shape or drawing by a constant scale factor and get another drawing that looks the same. The shape of the plane does not depend on the units that you use to measure length: light-years, miles, millimeters, angstroms, etc., all just differ from each other by an arbitrary choice of scale factor. On the other hand, the hyperbolic plane has a single natural unit of length. At smaller scales, it looks very much like the Euclidean plane. At larger scales, the differences between hyperbolic and Euclidean geometry kick in. When we use the Poincaré disk model to view the hyperbolic plane, we see that unit distance. The Euclidean radius of the whole disk model is proportional to the Euclidean size that we would see when viewing an object of unit hyperbolic size at the center of our focus. We can’t zoom in the Poincaré model any deeper than that.</p>

<p>This means that, to use hyperbolic drawings that work with the Poincaré disk model in focus+context applications, we need graph drawings in the hyperbolic plane where the sizes of the parts of the graph we want to zoom in on are not much smaller than the unit of length. But this is just not possible, in many cases. The geometry of the hyperbolic plane forces the features of graphs drawn in it to be squeezed tightly together, much more tightly than unit distance. In particular:</p>

<ul>
  <li>
    <p>Maximal planar graphs can never be drawn planarly with straight edges and with their vertices farther than distance \(O(1/\sqrt{n})\) from the nearest edge, and for some graphs this goes down to \(O(1/n)\).</p>
  </li>
  <li>
    <p>When a maximal planar graph is drawn with straight edges its vertices all constant distance apart from each other (which is always possible), some of its angles will be exponentially sharp.</p>
  </li>
  <li>
    <p>Even very simple graphs like \(K_{1,1,n}\) (below) and the \(n\times n\) grid graphs have both kinds of bad behavior: vertices very close to edges, and (if vertices are well separated) very sharp angles.</p>

    <p style="text-align: center;"><img alt="The complete tripartite graph K_{1,1,9}" src="https://11011110.github.io/blog/assets/2021/k119.svg" width="80%"/></p>
  </li>
  <li>
    <p>Nonplanar drawings can be forced to have angles of \(O(1/n^2)\), compared to the Euclidean plane where less-sharp angles proportional to \(1/n\) are always possible.</p>
  </li>
  <li>
    <p>When drawing nonplanar graphs, with nonzero thickness for the edges and nonzero radius for the vertices, you may be forced to use very thin edges with thickness \(O(1/n)\). Otherwise some edges may be completely covered and invisible.</p>
  </li>
</ul>

<p>Of course, there still exist plenty of graphs other than the trees and the bad examples of my paper that have nice hyperbolic drawings. It might be interesting to try to characterize which ones they are and to find good drawings when they exist.</p>

<p style="text-align: center;"><img alt="Order-5 square tiling of the hyperbolic plane, viewed in the Poincar&#xE9; disk model" src="https://11011110.github.io/blog/assets/2021/45tess.svg" width="60%"/></p>

<p>I’ll be presenting this at <a href="https://algo.inf.uni-tuebingen.de/gd2021/">this year’s Graph Drawing symposium</a>, in mid-September, for which registration is currently open. They’re trying a hybrid conference with both limited in-person participation (in Tübingen, Germany) and online participation; I’ll be on the online side, but I’m curious to see how this format works.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106775723303189658">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-08-17T23:14:00Z</updated>
    <published>2021-08-17T23:14:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-08-18T23:52:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8195</id>
    <link href="https://windowsontheory.org/2021/08/16/random-approx-conference/" rel="alternate" type="text/html"/>
    <title>RANDOM/APPROX conference</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">[Guest post by Mary Wooters; the conference already started but there are still great activities tomorrow and Wednesday. On an unrelated note, please make sure to watch the new Theory Shorts episode by the Simons Institute of Computing on lower bounds in computational complexity, featuring Madhu Sudan, Paul Beame, Faith Ellen, Jelani Nelson, and Manuel … <a class="more-link" href="https://windowsontheory.org/2021/08/16/random-approx-conference/">Continue reading <span class="screen-reader-text">RANDOM/APPROX conference</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Mary Wooters;  the conference already started but there are still great activities tomorrow and Wednesday.  On an unrelated note, please make sure to watch the new <a href="https://www.youtube.com/watch?v=-DWmBhMgWrI">Theory Shorts episode </a>by the Simons Institute of Computing on lower bounds in computational complexity, featuring Madhu Sudan, Paul Beame, Faith Ellen, Jelani Nelson, and Manuel Sabin.–Boaz]</em></p>



<p><strong>Call for Participation: APPROX/RANDOM 2021</strong></p>



<p>The 25th International Workshop on Randomization and Computation (RANDOM 2021) and the 24th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX 2021) will be starting on Monday August 16!  The conferences will be held as parallel virtual conferences, August 16-18, 2021. RANDOM 2021 focuses on applications of randomness to computational and combinatorial problems while APPROX 2021 focuses on algorithmic and complexity theoretic issues relevant to the development of efficient approximate solutions to computationally difficult problems.</p>



<p>To learn more about the conferences and program, visit:</p>



<p>APPROX: <a href="https://approxconference.wordpress.com/approx-2021/" rel="noreferrer noopener" target="_blank">https://approxconference.wordpress.com/approx-2021/</a><br/>RANDOM: <a href="https://randomconference.com/random-2021-home/" rel="noreferrer noopener" target="_blank">https://randomconference.com/random-2021-home/</a></p>



<p>In addition to an exciting program of live talks and discussion (to complement pre-recorded talks), the conference will feature two invited talks, by Jelani Nelson (UC Berkeley) and Vera Traub (ETH Zurich), as well as a social event with trivia and a cartoon caption contest!</p>



<p>Registration is only $10 for general audience members.  You can register here: <a href="https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience" rel="noreferrer noopener" target="_blank">https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience</a></p>



<p/>



<p/></div>
    </content>
    <updated>2021-08-16T22:20:17Z</updated>
    <published>2021-08-16T22:20:17Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-08-26T03:38:08Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4373523315220884395</id>
    <link href="http://blog.computationalcomplexity.org/feeds/4373523315220884395/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/what-are-most-important-46-papers-in.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4373523315220884395" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4373523315220884395" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/what-are-most-important-46-papers-in.html" rel="alternate" type="text/html"/>
    <title>What are the most important 46 papers in Computer Science? Harry Lewis has a book about them!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> (Disclosure:  Harry Lewis was my PhD advisor. For a blog post on  disclosures and bias see my post on that topic <a href="https://blog.computationalcomplexity.org/2010/07/conflicts-of-interest.html">here</a>.)</p><p>Harry Lewis has a book out: <a href="https://www.amazon.com/Ideas-That-Created-Future-Computer/dp/0262045303/ref=sr_1_1?dchild=1&amp;qid=1626491251&amp;refinements=p_27%3AHarry+Lewis&amp;s=books&amp;sr=1-1">Ideas that Created the Future: Classic Papers in Computer Science</a></p><p>He picked out the 46 (why 46? Why not 46?) classic papers in computer science and, for each one, has a short article saying why its important, and then has the paper itself, though perhaps shortened (leave out the boring parts) or in some cases he has an excerpt of a book (e.g.,<i> The Mythical Man Month</i> which is why I blogged about that book recently <a href="https://blog.computationalcomplexity.org/search?q=brooks">here</a>).</p><p>Harry Lewis has blogged about his book <a href="http://harry-lewis.blogspot.com/2021/07/book-reviews-and-talk-i-gave-in-hong.html">here</a> where he points to my review which is in SIGACT News. </p><p>OR you an use my link to my review <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/greatideas.pdf">here</a>. </p><p>The list of 46 papers had some constraints, so if you wonder <i>why isn't X ther</i>e it might have hit one of those constraints.</p><p>1) No paper past 1980 (he had to stop somewhere).</p><p>2) He preferred short readable papers to long or unreadable ones (don't we all!). Before thinking `Gee why isn't paper X in the book' go read paper X. </p><p>3) Some papers cost to much to get permission to reprint. My review points to one such paper that I found 5 links to on the web. </p><p>4) We don't need X papers on topic Y.</p><p>Of more interest is some papers that you had not heard of but we can now see are important.</p><p>For more thought, read my review!</p><p>For even more information, buy the book!</p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-08-16T01:10:00Z</updated>
    <published>2021-08-16T01:10:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-25T20:33:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/08/15/linkage</id>
    <link href="https://11011110.github.io/blog/2021/08/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Trying to watch Olympics replays on Roku / NBC Sports is an exercise in frustration (\(\mathbb{M}\)):</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p>Trying to watch Olympics replays on Roku / NBC Sports is an exercise in frustration (<a href="https://mathstodon.xyz/@11011110/106684830455526311">\(\mathbb{M}\)</a>):</p>

    <ul>
      <li>Up to 14 unskippable ads in a row</li>
      <li>Trying to fast-foward over breaks in sports action gets into a broken mode showing the same ads over and over while the underlying fast-foward goes on with disabled controls</li>
      <li>Rewinding to content you missed while uncontrollably fast-fowarding gets back into ad overload mode.</li>
    </ul>

    <p>Who designed this unusable app and why do they think this will bring me back for more?</p>
  </li>
  <li>
    <p>Rankings in all-play-all competitions (like group play stages of many Olympic games) typically use total numbers of points for wins. It’s simple, so audiences understand it. But really you might want to do something more complicated, like finding a ranking having the minimum number of upsets.  The good news is that <a href="https://cse.buffalo.edu/faculty/atri/papers/algos/fas-soda.pdf">point score approximates the minimum-upset ranking</a> (<a href="https://mathstodon.xyz/@11011110/106691070346474540">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>Topology is witchcraft (<a href="https://lainchan.gay/objects/0f77d3e4-aa38-428e-aa4a-250574c66dda">\(\mathbb{M}\)</a>): link goes to animated gif of surprising disentanglements, made possible because topologically things were never tangled to begin with.</p>
  </li>
  <li>
    <p><a href="https://www.bloomberg.com/news/articles/2021-08-03/facebook-disables-accounts-tied-to-nyu-research-project">Facebook shuts down the personal accounts of university researchers studying how Facebook violates its users’ privacy in targeting political ads</a> (<a href="https://mathstodon.xyz/@11011110/106700052721454124">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=28066214">via</a>), citing as an excuse to do so…its agreements with the FTC over its violations of user privacy. Mozilla weighs in on <a href="https://blog.mozilla.org/en/mozilla/news/why-facebooks-claims-about-the-ad-observer-are-wrong/">why Facebook’s side of the story doesn’t hold water</a>.</p>
  </li>
  <li>
    <p><a href="https://muboard.net/">Muboard</a> (<a href="https://mathstodon.xyz/@11011110/106713423407783688">\(\mathbb{M}\)</a>, <a href="https://lobste.rs/s/mqfkap/mathematics_chalkboard_with_mathjax">via</a>). There are any number of demo sites where you can type LaTeX math and get mathjax to format it for you in your web browser window, but this one looks like a good choice for sharing your screen when conferencing, office hours, etc: markdown for the non-math formatting, set up to look like a blackboard, with no unnecessary distractions, free to copy or modify.</p>
  </li>
  <li>
    <p>The truncated octahedron can tile space, forming the <a href="https://en.wikipedia.org/wiki/Bitruncated_cubic_honeycomb">bitruncated cubic honeycomb</a> (<a href="https://mathstodon.xyz/@11011110/106717788120113051">\(\mathbb{M}\)</a>). One of my neighborhood playgrounds has a climbing structure in this pattern, so kids can learn some geometry as they play. It wasn’t easy to find views without distracting background houses, but here’s one straight up from below its center.</p>

    <p style="text-align: center;"><img alt="Bitruncated cubic honeycomb play structure" src="https://www.ics.uci.edu/~eppstein/pix/bchps/1-m.jpg" style="border-style: solid; border-color: black;" width="80%"/></p>

    <p>I used this set to test out Lightroom instead of Photoshop for processing photos, with settings on auto. <a href="https://www.ics.uci.edu/~eppstein/pix/bchps/">A few more shots</a>. See discussion for another tessellation-based play structure and its grown-up architectural relative, the <a href="https://en.wikipedia.org/wiki/Nakagin_Capsule_Tower">Nakagin Capsule Tower</a>.</p>
  </li>
  <li>
    <p><a href="https://www.freemangallerysantafe.com/art/guardian-by-susan-latham">Guardian</a>, by sculptor <a href="https://susanlatham.net/">Susan Latham</a> (<a href="https://mathstodon.xyz/@11011110/106724752509935406">\(\mathbb{M}\)</a>). Its strange crescent moon or fortune cookie shapes are what you get from two overlapping circles intersecting in a <a href="https://en.wikipedia.org/wiki/Vesica_piscis">vesica piscis</a>, by folding  the center arcs to make the two outer arcs meet. See also <a href="https://www.youtube.com/watch?v=H6UeMmWx6i0">a short animation of this folding process</a> and <a href="http://archive.bridgesmathart.org/2018/bridges2018-535.html">mathematical analysis by Klara Mundilova and Tony Wills</a>.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.06751">‘Tortured phrases’ give away fabricated research papers</a> (<a href="https://mathstodon.xyz/@11011110/106727203820504644">\(\mathbb{M}\)</a>, <a href="https://boingboing.net/2021/08/09/tortured-phrases-used-to-fool-plagiarism-detectors-now-infest-scientific-papers.html">via</a>, <a href="https://www.nature.com/articles/d41586-021-02134-0">via2</a>, <a href="https://retractionwatch.com/2021/07/12/elsevier-says-integrity-and-rigor-of-peer-review-for-400-papers-fell-beneath-the-high-standards-expected/">see also</a>). The gist of the story appears to be: paper mills are using automated synonym replacement to hide plagiarism, producing odd wording (“irregular timberland” for “random forest”, in the math/CS sense of forest), and bypassing normal editorial processes in hijacked established journals, particularly through topical special issues.</p>
  </li>
  <li>
    <p><a href="http://jdh.hamkins.org/the-lattice-of-sets-of-natural-numbers-is-rich/">Joel David Hamkins tries to visualize the power set of the naturals</a> (<a href="https://mathstodon.xyz/@11011110/106741533744552581">\(\mathbb{M}\)</a>), showing also that it contains all countable partial orders.</p>
  </li>
  <li>
    <p>Given recent news breathlessly hyping the “discovery” that the people of Sippar, in the First Babylonian Dynasty, knew about and used the Pythagorean theorem, based on the existence of rectangles in a land survey, I think maybe it’s relevant to point to Eleanor Robson’s review on the Pythagorean theorem in First Dynasty Sippar, citing work on the topic back to 1916. It’s “<a href="https://www.jstor.org/stable/1359891">Three Old Babylonian Methods for Dealing with Pythagorean Triangles</a>”, <em>J. Cuneiform Studies</em>, 1997 (<a href="https://mathstodon.xyz/@11011110/106747623214954539">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>The WADS (Algorithms and Data Structures Symposium) and CCCG (Canadian Conference on Computational Geometry) conferences were online this week (<a href="https://mathstodon.xyz/@11011110/106750493101878056">\(\mathbb{M}\)</a>). The talks were mostly live on zoom rather than prerecorded, and although recordings exist temporarily, they’re only available to participants. The proceedings are online and public, though. The <a href="https://doi.org/10.1007/978-3-030-83508-8">WADS proceedings</a> through Springer LNCS is paywalled but the <a href="https://projects.cs.dal.ca/cccg2021/wordpress/wp-content/uploads/2021/08/CCCG2021.pdf">CCCG proceedings</a> is free.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Steinitz%27s_theorem">Steinitz’s theorem</a> (<a href="https://mathstodon.xyz/@11011110/106755883632277176">\(\mathbb{M}\)</a>), showing that the graphs of convex polyhedra can be described in a purely combinatorial way using only planarity and connectivity, and (through its proofs) describing how to turn a graph into a polyhedron. Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p>Etienne Cliquet makes kinetic artworks by cutting and folding sheets of paper, floating them on water, and <a href="https://thekidshouldseethis.com/post/flottille-etienne-cliquet-video">filming them as the water causes them to unfold</a> (<a href="https://mathstodon.xyz/@11011110/106762367791482461">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/192329/Floating-Origami">via</a>).</p>
  </li>
</ul></div>
    </content>
    <updated>2021-08-15T17:39:00Z</updated>
    <published>2021-08-15T17:39:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-08-18T23:52:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=2762</id>
    <link href="https://theorydish.blog/2021/08/13/random-2021-starts-on-monday/" rel="alternate" type="text/html"/>
    <title>RANDOM 2021 Starts on Monday</title>
    <summary>Call for Participation: APPROX/RANDOM 2021 The 25th International Workshop on Randomization and Computation (RANDOM 2021) and the 24th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX 2021) will be starting on Monday August 16!  The conferences will be held as parallel virtual conferences, August 16-18, 2021. RANDOM 2021 focuses on applications of randomness to computational and combinatorial problems while APPROX 2021 focuses on algorithmic and complexity theoretic issues relevant to the development of efficient approximate solutions to computationally difficult problems. To learn more about the conferences and program, visit: APPROX: https://approxconference.wordpress.com/approx-2021/RANDOM: https://randomconference.com/random-2021-home/ In addition to an exciting program of live talks and discussion (to complement pre-recorded talks), the conference will feature two invited talks, by Jelani Nelson (UC Berkeley) and Vera Traub (ETH Zurich), as well as a social event with trivia and a cartoon caption contest! Registration is only $10 for general audience members.  You can register here: https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience Hope to see you at the conference!</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Call for Participation: APPROX/RANDOM 2021</p>



<p>The 25th International Workshop on Randomization and Computation (RANDOM 2021) and the 24th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX 2021) will be starting on Monday August 16!  The conferences will be held as parallel virtual conferences, August 16-18, 2021. RANDOM 2021 focuses on applications of randomness to computational and combinatorial problems while APPROX 2021 focuses on algorithmic and complexity theoretic issues relevant to the development of efficient approximate solutions to computationally difficult problems.</p>



<p>To learn more about the conferences and program, visit:</p>



<p>APPROX: <a href="https://approxconference.wordpress.com/approx-2021/" rel="noreferrer noopener" target="_blank">https://approxconference.wordpress.com/approx-2021/</a><br/>RANDOM: <a href="https://randomconference.com/random-2021-home/" rel="noreferrer noopener" target="_blank">https://randomconference.com/random-2021-home/</a></p>



<p>In addition to an exciting program of live talks and discussion (to complement pre-recorded talks), the conference will feature two invited talks, by Jelani Nelson (UC Berkeley) and Vera Traub (ETH Zurich), as well as a social event with trivia and a cartoon caption contest!</p>



<p>Registration is only $10 for general audience members.  You can register here: <a href="https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience" rel="noreferrer noopener" target="_blank">https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience</a></p>



<p>Hope to see you at the conference!</p></div>
    </content>
    <updated>2021-08-13T23:18:23Z</updated>
    <published>2021-08-13T23:18:23Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-08-26T03:38:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5730</id>
    <link href="https://www.scottaaronson.com/blog/?p=5730" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5730#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5730" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Stephen Wiesner (1942-2021)</title>
    <summary xml:lang="en-US">These have not been an auspicious few weeks for Jewish-American-born theoretical physicists named Steve who made epochal contributions to human knowledge in the late 1960s, and who I had the privilege to get to know a bit when they were old. This morning, my friend and colleague Or Sattath brought me the terrible news that […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p/>



<p/>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" src="https://www.scottaaronson.com/wiesner-sm.jpg"/>Photo credit: Lev Vaidman</figure></div>



<p>These have <a href="https://www.scottaaronson.com/blog/?p=5566">not</a> been an auspicious few weeks for Jewish-American-born theoretical physicists named Steve who made epochal contributions to human knowledge in the late 1960s, and who I had the privilege to get to know a bit when they were old.</p>



<p>This morning, my friend and colleague <a href="https://orsattath.wordpress.com/about/">Or Sattath</a> brought me the terrible news that <a href="https://en.wikipedia.org/wiki/Stephen_Wiesner">Stephen Wiesner</a> has passed away in Israel.  [Because people have asked: I’ve now also heard directly from Wiesner’s daughter Sarah.]</p>



<p>Decades ago, Wiesner left academia, embraced Orthodox Judaism, moved from the US to Israel, and took up work there as a construction laborer—believing (or so he told me) that manual labor was good for the soul.  In the late 1960s, however, Wiesner was still a graduate student in physics at Columbia University, when he wrote <a href="http://users.cms.caltech.edu/~vidick/teaching/120_qcrypto/wiesner.pdf">Conjugate Coding</a>: arguably the foundational document of the entire field of quantum information science.  <s>Famously, this paper was so far ahead of its time that it was rejected over and over from journals, taking nearly 15 years to get published.</s>  (Fascinatingly, Gilles Brassard tells me that this isn’t true: it was rejected <em>once</em>, from <em>IEEE Transactions on Information Theory</em>, and then Wiesner simply shelved it.)  When it finally appeared, in 1983, it was in <em><a href="https://dl.acm.org/newsletter/sigact">SIGACT News</a></em>—a venue that I know and love, where I’ve published too, but that’s more like the house newsletter for theoretical computer scientists than an academic journal.</p>



<p>But it didn’t matter.  By the early 1980s, Wiesner’s ideas had been successfully communicated to <a href="https://en.wikipedia.org/wiki/Charles_H._Bennett_(physicist)">Charlie Bennett</a> and <a href="https://en.wikipedia.org/wiki/Gilles_Brassard">Gilles Brassard</a>, who refashioned them into the first scheme for <a href="https://en.wikipedia.org/wiki/Quantum_key_distribution">quantum key distribution</a>—what we now call <a href="https://en.wikipedia.org/wiki/BB84">BB84</a>.  Even as Bennett and Brassard received scientific acclaim for the invention of quantum cryptography—including, a few years ago, the <a href="https://en.wikipedia.org/wiki/Wolf_Prize">Wolf Prize</a> (often considered second only to the Nobel Prize), at a ceremony in the Knesset in Jerusalem that I attended—the two B’s were always careful to acknowledge their massive intellectual debt to Steve Wiesner.</p>



<hr class="wp-block-separator"/>



<p>Let me explain what Wiesner does in the Conjugate Coding paper.  As far as I know, this is the first paper ever to propose that quantum information—what Wiesner called “polarized light” or “spin-1/2 particles” but we now simply call <a href="https://en.wikipedia.org/wiki/Qubit">qubits</a>—works differently than classical bits, in ways that could <em>actually be useful</em> for achieving cryptographic tasks that are impossible in a classical world.  What could enable these cryptographic applications, wrote Wiesner, is the fact that there’s no physical means for an attacker or eavesdropper to <em>copy</em> an unknown qubit, to produce a second qubit in the same quantum state.  This observation—now called the <a href="https://en.wikipedia.org/wiki/No-cloning_theorem">No-Cloning Theorem</a>—would only be named and published in 1982, but Wiesner treats it in his late-1960s manuscript as just obvious background.</p>



<p>Wiesner went further than these general ideas, though, to propose an explicit scheme for <a href="https://en.wikipedia.org/wiki/Quantum_money">quantum money</a> that would be physically impossible to counterfeit—a scheme that’s still of enormous interest half a century later (I teach it every year in my <a href="https://www.scottaaronson.com/qclec.pdf">undergraduate course</a>).  In what we now call the Wiesner money scheme, a central bank prints “quantum bills,” each of which contains a classical serial number as well as a long string of qubits.  Each qubit is prepared in one of four possible quantum states:</p>



<ul><li>|0⟩,</li><li>|1⟩,</li><li>|+⟩ = (|0⟩+|1⟩)/√2, or</li><li>|-⟩ = (|0⟩-|1⟩)/√2.</li></ul>



<p>The bank, in a central database, stores the serial number of every bill in circulation, as well as the preparation instructions for each of the bill’s qubits.  If you want to <em>verify</em> a bill as genuine—this, as Wiesner knew, is the big drawback—you have to bring it back to the bank.  The bank, using its secret knowledge of how each qubit was prepared, measures each qubit in the appropriate basis—the {<span style="font-size: revert; color: initial;">|0⟩,|1⟩</span>} basis for <span style="font-size: revert; color: initial;">|0⟩ or |1⟩</span> qubits, the {<span style="font-size: revert; color: initial;">|+⟩,|-⟩</span>} basis for <span style="font-size: revert; color: initial;">|+⟩ or |-⟩</span> qubits—and checks that it gets the expected outcomes.  If even one qubit yields the wrong outcome, the bill is rejected as counterfeit.</p>



<p>Now consider the situation of a counterfeiter, who holds a quantum bill but lacks access to the bank’s secret database.  When the counterfeiter tries to copy the bill, they won’t know the right basis in which to measure each qubit—and if they make the wrong choice, then it’s not only that they fail to make a copy; it’s that the measurement destroys even the <em>original</em> copy!  For example, measuring a <span style="font-size: revert; color: initial;">|+⟩ or |-⟩</span> qubit in the {<span style="font-size: revert; color: initial;">|0⟩,|1⟩</span>} basis will randomly collapse the qubit to either <span style="font-size: revert; color: initial;">|0⟩ or |1⟩</span>—so that, when the bank later measures the same qubit in the correct {<span style="font-size: revert; color: initial;">|+⟩,|-⟩</span>} basis, it will see the wrong outcome, and realize that the bill has been compromised, with 1/2 probability (with the probability increasing to nearly 1 as we repeat across hundreds or thousands of qubits).</p>



<p>Admittedly, the handwavy argument above, which Wiesner offered, is far from a security proof by cryptographers’ standards.  In 2011, I <a href="https://cstheory.stackexchange.com/questions/11363/rigorous-security-proof-for-wiesners-quantum-money">pointed that out on StackExchange</a>.  My post, I’m happy to say, spurred Molina, Vidick, and Watrous to write a <a href="https://arxiv.org/abs/1202.4010">beautiful 2012 paper</a>, where they rigorously proved for the first time that in Wiesner’s money scheme, no counterfeiter consistent with the laws of quantum mechanics can turn a single n-qubit bill into two bills that both pass the bank’s verification with success probability greater than (3/4)<sup>n</sup> (and this is tight).  But the intuition was already clear enough to Wiesner in the 1960s.</p>



<p>In 2003—when I was already a PhD student in quantum information, but incredibly, had never heard of Stephen Wiesner or his role in founding my field—I rediscovered the idea of quantum states |ψ⟩ that you could store, measure, and feed into a quantum computer, but that would be <em>usefully uncopyable</em>.  (My main interest was in whether you could create “unpiratable quantum software programs.”)  Only in 2006, at the University of Waterloo, did Michele Mosca and his students make the connection for me to quantum money, Stephen Wiesner, and his Conjugate Coding paper, which I then read with amazement—along with a <a href="https://static.aminer.org/pdf/PDF/000/120/546/quantum_cryptography_or_unforgeable_subway_tokens.pdf">comparably amazing followup work</a> by Bennett, Brassard, Breidbart, and Wiesner.</p>



<p>But it was clear that there was still a great deal to do.  Besides unpiratable software, Wiesner and his collaborators had lacked the tools in the early 1980s seriously to tackle the problem of secure quantum money that <em>anybody</em> could verify, not only the bank that had created the money.  I realized that, if such a thing was possible at all, then just like unpiratable software, it would require cryptographic hardness assumptions, a restriction to polynomial-time counterfeiters, and (hence) ideas from quantum computational complexity.  The No-Cloning Theorem couldn’t do the job on its own.</p>



<p>That realization led to my 2009 paper <a href="https://arxiv.org/abs/1110.5353">Quantum Copy-Protection and Quantum Money</a>, and from there, to the “modern renaissance” of Wiesner’s old idea of quantum money, with well over a hundred papers (e.g., <a href="https://arxiv.org/abs/1203.4740">my 2012 paper with Paul Christiano</a>, Farhi et al.’s <a href="https://arxiv.org/abs/0912.3823">quantum state restoration paper</a>, their <a href="https://arxiv.org/abs/1004.5127">quantum money from knots paper</a>, Mark Zhandry’s 2017 <a href="https://arxiv.org/abs/1711.02276">quantum lightning paper</a>, Dmitry Gavinsky’s <a href="https://arxiv.org/abs/1109.0372">improvement of Wiesner’s scheme</a> wherein the money is verified by classical communication with the bank, Broduch et al.’s <a href="https://arxiv.org/abs/1404.1507">adaptive attack</a> on Wiesner’s original scheme, my <a href="https://arxiv.org/abs/1711.01053">shadow tomography paper</a> proving the necessity for the bank to keep a giant database in information-theoretic quantum money schemes like Wiesner’s, Daniel Kane’s <a href="https://arxiv.org/abs/1809.05925">strange scheme based on modular forms</a>…).  The purpose of many of these papers was either to break the quantum money schemes proposed in previous papers, or to patch the schemes that were previously broken.</p>



<p>After all this back-and-forth, spanning more than a decade, I’d say that Wiesner’s old idea of quantum money is now in good enough theoretical shape that the main obstacle to its practical realization is merely the “engineering difficulty”—namely, how to get the qubits in a bill, sitting in your pocket or whatever, to maintain their quantum coherence for more than a few nanoseconds!  (Or possibly a few hours, if you’re willing to schlep a cryogenic freezer everywhere you go.)  It’s precisely because quantum key distribution doesn’t have this storage problem—because there the qubits are simply sent across a channel and then immediately measured on arrival—that QKD is actually practical today, although the market for it has proven to be extremely limited so far.</p>



<p>In the meantime, while the world waits for the quantum error-correction that could keep qubits alive indefinitely, there’s Bitcoin.  The latter perversely illustrates just how immense the demand for quantum money might someday be: the staggering lengths to which people will go, diverting the electricity to power whole nations into mining rigs, to get around our current inability to realize Wiesner’s elegant quantum-mechanical solution to the same problem.  When I first learned about Bitcoin, shortly after its invention, it was in the context of: “here’s something I’d better bring up in my lectures on quantum money, in order to explain how much better WiesnerCoin could eventually be, when it’s the year 2200 or whatever and we all have quantum computers wired up by a quantum Internet!”  It never occurred to me that I should forget about the year 2200, liquidate my life savings, and immediately buy up all the Bitcoin I could.  [Added: I’ve since learned that Wiesner’s daughter Sarah is a professional in the Bitcoin space.]</p>



<hr class="wp-block-separator"/>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" src="https://www.scottaaronson.com/wiesnaar-sm.jpg"/>Photo credit: Or Sattath</figure></div>



<p>In his decades as a construction laborer, Wiesner had (as far as I know) no Internet presence; many of my colleagues didn’t even realize he was still alive.  Even then, though, Wiesner never turned his back <em>so</em> far on his previous life, his academic life, that the quantum information faculty at Hebrew University in Jerusalem couldn’t entice him to participate in some seminars there.  Those seminars are where I had the privilege to meet and talk to him several times over the last decade.  He was thoughtful and kind, listening with interest as I told him how I and others were trying to take quantum money into the modern era by making it publicly verifiable.</p>



<p>I also vividly remember a conversation in 2013 where Steve shared his fears about the American physics establishment and military-industrial complex, and passionately urged me to</p>



<ol><li>quit academia and get a “real job,” and</li><li>flee the US immediately and move my family to Israel, because of a wave of fascism and antisemitism that was about to sweep the US, just like with Germany in the 1930s.</li></ol>



<p>I politely nodded along, pointing out that my Israeli wife and I had considered living in Israel but the job opportunities were better in US, silently wondering when Steve had gone <em>completely</em> off his rocker.  Today, Steve’s urgent warning about an impending fascist takeover of the US seems … uh, <em>slightly</em> less crazy than in 2013?  Maybe, just like with quantum money, Wiesner was simply too far ahead of his time to sound sane.</p>



<p>Wiesner also talked to me about his father, <a href="https://en.wikipedia.org/wiki/Jerome_Wiesner">Jerome Wiesner</a>, who was a legendary president of MIT—still spoken about in reverent tones when I taught there—as well as the chief science advisor to John F. Kennedy.  One of JFK’s most famous decisions was to override the elder Wiesner’s fervent opposition to sending humans to the moon (Wiesner thought it a waste of money, as robots could do the same science for vastly cheaper).</p>



<p>While I don’t know all the details (I hope someone someday researches it and writes a book), Steve Wiesner made it clear to me that he did not get along with his famous father <em>at all</em>—in fact they became estranged.  Steve told me that his embrace of Orthodox Judaism was, at least in part, a reaction against everything his father had stood for, including militant scientific atheism.  I suppose that in the 1960s, millions of young Americans defied their parents via sex, drugs, and acoustic guitar; only a tiny number did so by donning <a href="https://en.wikipedia.org/wiki/Tzitzit">tzitzit</a> and moving to Israel to pray and toil with their hands.  The two groups of rebels did, however, share a tendency to grow long beards.</p>



<p>Wiesner’s unique, remarkable, <em>uncloneable</em> life trajectory raises the question: who are the young Stephen Wiesners of our time?  Will we be faster to recognize their foresight than Wiesner’s contemporaries were to recognize his?</p>



<hr class="wp-block-separator"/>



<p>Feel free to share any other memories of Stephen Wiesner or his influence in the comments.</p>



<hr class="wp-block-separator"/>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Aug. 14):</span></strong> See also <a href="https://orsattath.wordpress.com/2021/08/14/stephen-wiesner/">Or Sattath’s memorial post</a>, which (among other things) points out something that my narrative missed: namely, besides quantum money, Wiesner <em>also</em> invented <a href="https://en.wikipedia.org/wiki/Superdense_coding">superdense coding</a> in 1970, although he and Bennett only published the idea 22 years later (!).</p>



<p>And I have more photos!  Here’s <a href="https://www.scottaaronson.com/wiesner2.jpg">Wiesner with an invention of his</a> and <a href="https://www.scottaaronson.com/wiesner3.jpg">another photo</a> (thanks to his daughter Sarah).  Here’s <a href="https://www.scottaaronson.com/wiesner1970.jpg">another photo from 1970</a> and <a href="https://www.scottaaronson.com/wiesnernotes1970.jpg">Charlie Bennett’s handwritten notes</a> (!) after first meeting Wiesner in 1970 (thanks to Charlie Bennett).</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Another Update:</span></strong> Stephen’s daughter Sarah gave me the following fascinating information to share.</p>



<blockquote class="wp-block-quote"><p>In the 70’s he lived in California where he worked in various Silicon Valley startups while also working weekends as part of a produce (fruits and vegetables) distribution co-op. During this time he became devoted to the ideas of solar energy, clean energy and space migration and exploration. He also became interested in Judaism. He truly wanted to help and make our world more peaceful and safe with his focus being on clean energy and branching out into space. He also believed that instead of fighting over the temple mount in Jerusalem, the Third Temple should be built in outer-space or in a structure above the original spot, an idea he tried to promote to prevent wars over land.</p></blockquote></div>
    </content>
    <updated>2021-08-13T20:54:29Z</updated>
    <published>2021-08-13T20:54:29Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-08-20T03:09:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/119</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/119" rel="alternate" type="text/html"/>
    <title>TR21-119 |  Visible Rank and Codes with Locality | 

	Omar Alrabiah, 

	Venkatesan Guruswami</title>
    <summary>We propose a framework to study the effect of local recovery requirements of codeword symbols on the dimension of linear codes, based on a combinatorial proxy that we call "visible rank." The locality constraints of a linear code are stipulated by a matrix $H$ of $\star$'s and $0$'s (which we call a "stencil"), whose rows correspond to the local parity checks (with the $\star$'s indicating the support of the check). The visible rank of $H$ is the largest $r$ for which there is a $r \times r$ submatrix in $H$ with a unique generalized diagonal of $\star$'s. The visible rank yields a field-independent combinatorial lower bound on the rank of $H$ and thus the co-dimension of the code. 

We point out connections of the visible rank to other notions in the literature such as unique restricted graph matchings, matroids, spanoids, and min-rank. In particular, we prove a rank-nullity type theorem relating visible rank to the rank of an associated construct called symmetric spanoid, which was introduced by Dvir, Gopi, Gu, and Wigderson [DGWW]. Using this connection and a construction of appropriate stencils, we answer a question posed in [DGGW] and demonstrate that symmetric spanoid rank cannot improve the currently best known $\widetilde{O}(n^{(q-2)/(q-1)})$ upper bound on the dimension of $q$-query locally correctable codes (LCCs) of length $n$. This also pins down the efficacy of visible rank as a proxy for the dimension of LCCs. 

We also study the $t$-Disjoint Repair Group Property ($t$-DRGP) of codes where each codeword symbol must belong to $t$ disjoint check equations. It is known that  linear codes with $2$-DRGP must have co-dimension $\Omega(\sqrt{n})$ (which is matched by a simple product code construction). We show that there are stencils corresponding to $2$-DRGP with visible rank as small as $O(\log n)$. However, we show the second tensor of any $2$-DRGP stencil has visible rank $\Omega(n)$, thus recovering the $\Omega(\sqrt{n})$ lower bound for $2$-DRGP. For $q$-LCC, however, the $k$'th tensor power for $k\le n^{o(1)}$ is unable to improve the $\widetilde{O}(n^{(q-2)/(q-1)})$ upper bound on the dimension of $q$-LCCs by a polynomial factor. Inspired by this and as a notion of intrinsic interest, we define the notion of visible capacity of a stencil as the limiting visible rank of high tensor powers, analogous to Shannon capacity, and pose the question whether there can be large gaps between visible capacity and algebraic rank.</summary>
    <updated>2021-08-13T14:04:34Z</updated>
    <published>2021-08-13T14:04:34Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-26T03:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/118</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/118" rel="alternate" type="text/html"/>
    <title>TR21-118 |  Efficient multivariate low-degree tests via interactive oracle proofs of proximity for polynomial codes | 

	Sarah Bordage, 

	Daniel Augot, 

	Jade Nardi</title>
    <summary>We consider the proximity testing problem for error-correcting codes which consist in evaluations of multivariate polynomials either of bounded individual degree or bounded total degree. Namely, given an
oracle function $f : L^m \rightarrow \mathbb F_q$, where $L\subset \mathbb F_q$, a verifier     distinguishes whether $f$ is the evaluation of a low-degree polynomial or is far (in relative Hamming distance) from being one, by making only a few queries to $f$.  This topic has been studied in the context of locally testable codes, interactive proofs, probabilistically checkable proofs, and interactive oracle proofs.
We present the first interactive oracle proofs of proximity (IOPP) for tensor products of Reed-Solomon codes (evaluation of polynomials with bounds on individual degrees) and for Reed-Muller codes (evaluation of polynomials with a bound on the total
degree).
        
Such low-degree polynomials play a central role in constructions of probabilistic proof systems and succinct non-interactive arguments of knowledge with zero-knowledge. For these applications, highly-efficient multivariate low-degree tests are
desired, but prior probabilistic proofs of proximity required super-linear proving time. In contrast, for multivariate codes of length $N$, our constructions admit a prover running in time linear in $N$ and a verifier which is logarithmic in $N$.
        
      For fixed constant  number of variables $m$, the efficiency parameters of our IOPPs for multivariate codes compare well, all things equal,  with those of the IOPP for Reed-Solomon codes of [Ben-Sasson et al., ICALP 2018] from which they are directly inspired.</summary>
    <updated>2021-08-13T13:52:54Z</updated>
    <published>2021-08-13T13:52:54Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-26T03:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/117</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/117" rel="alternate" type="text/html"/>
    <title>TR21-117 |  Simplicity Meets Near-Optimal Rate: Non-malleable Codes and Non-malleable Two-source Extractors via Rate Boosters | 

	Divesh Aggarwal, 

	Bhavana Kanukurthi, 

	SaiLakshmiBhavana Obbattu, 

	Maciej Obremski, 

	Sruthi Sekar</title>
    <summary>At ITCS 2010, Dziembowski, Pietrzak, and Wichs introduced Non-malleable Codes (NMCs). Non-malleability is one of the strongest and most challenging notions of security considered in cryptography and protects against tampering attacks. In the context of coding schemes, non-malleability requires that it be infeasible to tamper the codeword of a message into the codeword of a related message. A natural and well-studied model of tampering is the $2$-split-state model where the codeword consists of two independently tamperable states. As with standard error-correcting codes, it is of great importance to build codes with high rates. Cheraghchi and Guruswami (ITCS 2014) showed that one cannot obtain NMCs in the $2$-split state model with a rate better than $1/2$.  Since its inception, this area has witnessed huge strides leading to the construction of a constant-rate NMC in the $2$-split state model due to Aggarwal and Obremski (FOCS 2020). However, the rate of this construction -- roughly $1/1,500,000$ -- is nowhere close to the best achievable rate of $1/2$! In this work, we dramatically improve this status quo by building a rate booster that converts any augmented non-malleable code into an augmented non-malleable code with a rate of $1/3$. Using similar, but simpler techniques we also obtain rate boosters that convert any unbalanced (with sources of unequal length) non-malleable $2$-source extractor into an unbalanced non-malleable $2$-source extractor with rate $1/2$.
  
The beauty of our construction lies in its simplicity. In particular, if we apply our rate booster to the non-malleable code construction by Aggarwal, Dodis, and Lovett (STOC 2014), then all we need is one instance of the inner-product extractor, one instance of a seeded extractor, and an affine-evasive function for the construction to work.

Further, as an application of our $1/3$-rate augmented NMC (which we also prove to be leakage resilient), we give an extremely simple computational binding and statistical hiding non-malleable commitment scheme using only standard assumptions, and with a communication cost of $41$ times the length of the message to be committed, which is optimal up to a constant factor.</summary>
    <updated>2021-08-13T13:49:56Z</updated>
    <published>2021-08-13T13:49:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-26T03:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/08/13/postdoc-at-unsw-sydney-apply-by-september-13-2021/</id>
    <link href="https://cstheory-jobs.org/2021/08/13/postdoc-at-unsw-sydney-apply-by-september-13-2021/" rel="alternate" type="text/html"/>
    <title>postdoc at UNSW Sydney (apply by September 13, 2021)</title>
    <summary>A 2-year postdoc position is available at UNSW Sydney. The position is part of a new research project on “Improved algorithms via random sampling” (Serge Gaspers, Fedor Fomin, Daniel Lokshtanov) funded by the Australian Research Council. It revolves around the design and analysis of parameterized, moderately exponential, randomized, and approximation algorithms for graph problems. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A 2-year postdoc position is available at UNSW Sydney.</p>
<p>The position is part of a new research project on “Improved algorithms via random sampling” (Serge Gaspers, Fedor Fomin, Daniel Lokshtanov) funded by the Australian Research Council. It revolves around the design and analysis of parameterized, moderately exponential, randomized, and approximation algorithms for graph problems.</p>
<p>Website: <a href="https://www.cse.unsw.edu.au/~sergeg/contact.html#postdocs">https://www.cse.unsw.edu.au/~sergeg/contact.html#postdocs</a><br/>
Email: serge.gaspers@unsw.edu.au</p></div>
    </content>
    <updated>2021-08-13T05:13:27Z</updated>
    <published>2021-08-13T05:13:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-08-26T03:37:52Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2021/08/13/relative-risk/</id>
    <link href="http://benjamin-recht.github.io/2021/08/13/relative-risk/" rel="alternate" type="text/html"/>
    <title>Relative risk is more informative than effectiveness.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The past few weeks have sent a tremendous amount of disappointing news about the Delta variant and the possible waning effectiveness of vaccines. <a href="https://www.washingtonpost.com/health/2021/07/30/provincetown-covid-outbreak-vaccinated/">An outbreak among partygoers in Provincetown found a high number of infected individuals who had already been fully vaccinated.</a> <a href="https://nymag.com/intelligencer/2021/08/breakthrough-covid-19-cases-may-be-a-bigger-problem.html">News stories and anecdotes about “breakthrough infections” abound.</a> <a href="https://apnews.com/article/health-coronavirus-pandemic-79959d313428d98ab8aa905bbe287ba0">And the CDC has confusingly made an about face on its recommendations about face coverings for the vaccinated.</a> This has fed a growing anxiety that the vaccines have not delivered what was promised.</p>

<p>The Pfizer and Moderna vaccine, <a href="https://www.pfizer.com/news/press-release/press-release-detail/pfizer-and-biontech-conclude-phase-3-study-covid-19-vaccine">heralded as “95% effective”</a>, led many to hope that they could eradicate this coronavirus. However, and unfortunately, from the beginning, there has been a lack of clarity among not only the media but among health professionals themselves around what “effectiveness” means. Indeed, <a href="https://www.npr.org/sections/goatsandsoda/2021/08/11/1026190062/covid-delta-variant-transmission-cdc-chickenpox">misunderstandings of vaccine effectiveness persist in media reporting on the pandemic</a>.</p>

<p>Pfizer ran a randomized control trial where they took a diverse pool of sixty thousand people and randomly gave half of the people the vaccine and half a placebo injection with no effect at all. Effectiveness measures the percent reduction of infection in the vaccinated group when compared against the control group. A vaccine effectiveness of 95% means that Pfizer observed 95% fewer infections amongst the vaccinated than the unvaccinated.</p>

<p>An identical way of stating the result of the trial is that 20 times as many people who received the placebo developed symptomatic COVID-19 as those that received the vaccine. In the language of risk, we would say that the vaccinated group had 20 times less risk of symptomatic COVID-19 infection than the control group.</p>

<p>While “95% effective” is numerically the same as “20-fold risk reduction,” it leads to more confusion. It is far too common for “95% effective” to be incorrectly interpreted as a 5% chance of getting infected at all, which is not the case. If the message had been that vaccines reduce risk by a factor of 20, then we would be equipped to both better understand the power of these vaccines and better plan for moving into an open, but increasingly vaccinated world.</p>

<p>20-fold risk reduction allows one to consider their everyday experiences and make judgement calls. Hanging out with friends outside was already very low risk. If everyone is vaccinated, the risk after vaccination becomes effectively zero. On the other hand, going to a packed, sweaty dance club and sharing drinks with strangers was very high risk before vaccination, so vaccination makes it 20 times less risky but still risky.</p>

<p>The Delta variant itself is more infectious than previous strains and can partially evade immunity. This further reduces vaccine effectiveness. But, like with the clubbing example, behavioral changes also change effectiveness. The virus is not the same today as it was in the Fall of 2020, but we are also not the same people.</p>

<p>Clinical studies are done in the best of circumstances, and most medical professionals expect the effectiveness to decrease in the general population. Scientists do their best to ensure that the population of individuals in a study are representative of all of the people in the world. They perform complex matching and outreach to ensure characteristic diversity in the pool of subjects. But one of the hardest factors to control for is the psychological and behavioral changes in the broader population over time.</p>

<p>In particular, people are taking more risks now than they did in 2020, and the risk of infection has increased for all, vaccinated or not. Just imagine you get vaccinated and then increase your risk by a factor of four. For example, you stop staying at home on Zoom all day, and go back to the office, bars, and restaurants. You start meeting with all sorts of people you don’t know very well. Then your risk of infection is now a fifth, not a twentieth of what it was. And further imagine that a new variant comes along that is twice as infectious as the old one. Now your risk reduction is down to 2.5, which would amount to a total “effectiveness”—combining the vaccine, your behavior, and the variant—of only 60%. But 60% is still better than most seasonal flu shots.</p>

<p>Perhaps this is the hardest part about where we are in the pandemic. Everyone wants to return to normal and never think about coronaviruses again. But a <a href="https://www.newyorker.com/science/annals-of-medicine/coexisting-with-the-coronavirus">preponderance of evidence indicates that SARS-CoV-2 will become endemic and will circulate like other common viruses</a>. It is likely that all of us will become immune to COVID-19 in one way or another, though, because of the vaccines, the disease will cause much less death and suffering than it did before. But even with such dramatic reductions in mortality, any COVID-19-associated deaths in the United States feel like too many for a society that has been terrorized and torn apart by the pandemic.</p>

<p>And given that children do not yet have a vaccine available, parents worry that their children remain at risk. But we accept much more deadly risks in our lives. It is uncomfortable to accept that a 3 year old has a similar risk of severe COVID-19 as a vaccinated 40 year old. Doing a cost benefit analysis about your child is emotionally impossible. I understand how any risk, no matter how small, can feel intolerable. Unfortunately, that same 3 year old is <a href="https://www.nytimes.com/2021/04/22/opinion/covid-vaccine-kids.html">more vulnerable to death or serious injury by driving, swimming, or even eating</a>. The risks entailed with COVID-19 do not seem to be much different than those of just growing up. Since we want nothing more than for our kids to be safe, we delude ourselves by never conceptualizing the risks of their ordinary activities. We live our lives as though these risks are zero. Though it seems impossible to imagine this now, in time, we’ll accept the remaining risk from COVID-19 as well.</p>

<p><em>Many thanks to Sarah Dean, Jordan Ellenberg, Eric Jonas, Lauren Kroiz, Deb Raji, Lawrence Recht, Chris Re, and Isaac Sparks for reading drafts of this post and offering insightful comments and suggestions.</em></p></div>
    </summary>
    <updated>2021-08-13T00:00:00Z</updated>
    <published>2021-08-13T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2021-08-25T22:54:08Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4087249606769943482</id>
    <link href="http://blog.computationalcomplexity.org/feeds/4087249606769943482/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/recognizing-faces.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4087249606769943482" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4087249606769943482" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/recognizing-faces.html" rel="alternate" type="text/html"/>
    <title>Recognizing Faces</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I sometimes have trouble recognizing faces, matching faces to people I've interacted with in the past. It's not a disease like <a href="https://en.wikipedia.org/wiki/Prosopagnosia">prosopagnosia</a>, I can certainly tell the difference between faces and have no trouble with people I work with directly. But if I haven't seen someone in a while, I may not recognize them or confuse them for someone else. It's especially bad out of context, say running into a professor in my campus on the streets of Frankfurt. It's gotten worse with age but I've had challenges my whole life.</p><p>I have my coping mechanisms. I start a conversation to get enough clues to figure out who I'm talking to. I'll google an image before I'm supposed to meet someone I haven't seen in a while. Sometimes I'll just say "Remind me how to pronounce your name again". Sometimes I'll just say something embarrassing thinking the person I'm talking to is someone else.</p><p>Name tags are useful, if it isn't obvious you are looking at them. Zoom has been great--everyone's name is just there. I worry that 18 months of zoom meetings means I've lost much of my coping ability, much the way I can no longer navigate by maps the way I used to.</p><p>We have technological solutions but mostly unable to make use of them. Through the magic of machine learning, computers have gotten extremely good at recognizing faces. Nevertheless Google Googles actively prevented their one killer app, telling you who you were looking at, for privacy reasons. Perhaps they could limit it to people in your contacts with pictures you uploaded. It would only recognize people you already know.</p><p>I know I'm not alone, and I'm writing this post so others won't feel alone. And next time you see me and I look confused, remind me of your name.</p></div>
    </content>
    <updated>2021-08-12T14:16:00Z</updated>
    <published>2021-08-12T14:16:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-25T20:33:35Z</updated>
    </source>
  </entry>
</feed>
