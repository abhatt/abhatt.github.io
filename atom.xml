<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2022-03-16T05:39:00Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>https://11011110.github.io/blog/2022/03/15/linkage</id>
    <link href="https://11011110.github.io/blog/2022/03/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Here’s a silly but probably new proof that the harmonic series diverges (\(\mathbb{M}\)). The expected number of comparisons used by randomized quicksort on an input of size \(n\) is at most \(2nH_n\), where \(H_n\) is the \(n\)th partial sum of the harmonic series (see Cormen et al, Introduction to Algorithms, Chapter 7). However, every comparison sorting algorithm requires at least \(\log_2n!=n\log_2n-O(n)\) comparisons, by the standard decision tree argument (Cormen et al, Section 8.1). Therefore, \(H_n=\Omega(\log n)\).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p>Here’s a silly but probably new proof that the harmonic series diverges <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107883892455172986">\(\mathbb{M}\)</a>).</span> The expected number of comparisons used by randomized quicksort on an input of size \(n\) is at most <span style="white-space: nowrap;">\(2nH_n\),</span> where \(H_n\) is the <span style="white-space: nowrap;">\(n\)th</span> partial sum of the harmonic series (see Cormen et al, <em>Introduction to Algorithms</em>, Chapter 7). However, every comparison sorting algorithm requires at least \(\log_2n!=n\log_2n-O(n)\) comparisons, by the standard decision tree argument (Cormen et al, Section 8.1). Therefore, <span style="white-space: nowrap;">\(H_n=\Omega(\log n)\).</span></p>
  </li>
  <li>
    <p>To be fair, the lecture hall I teach in this term doesn’t look quite so much like a prison if you enter by the main door at the top of the hall instead of the back door by the stage <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107890151210168412">\(\mathbb{M}\)</a>).</span></p>

    <p style="text-align: center;"><img alt="Humanities Lecture Hall, UC Irvine" src="https://www.ics.uci.edu/~eppstein/pix/hlh/hlh-m.jpg" style="border-style: solid; border-color: black;"/></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2203.00671">Maximum flow and minimum-cost flow in almost-linear time</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@FreddyR/107890263250219998">\(\mathbb{M}\)</a>).</span> New arXiv preprint by Li Chen, Rasmus Kyng, Yang P. Liu, Richard Peng, Maximilian Probst Gutenberg, Sushant Sachdeva. It assumes integer capacities but that’s enough to get near-linear bipartite maximum matching, itself a breakthrough.</p>
  </li>
  <li>
    <p>In early March, UC Berkeley was <a href="https://www.latimes.com/california/story/2022-03-04/how-much-will-uc-berkeley-have-to-cut-admissions-after-supreme-court-loss-what-we-know">ordered to drastically cut enrollment under California’s strict environmental impact review laws</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107899108816738487">\(\mathbb{M}\)</a>).</span> In practice these laws  are often used as a pretext for lawsuits to shake down or stop public developments for reasons unrelated to environmental impact (this is a heavily built-up area already; the impact is that it would have more students living in it and the people suing wanted to shake down UC for non-university-related low-income housing expansion). By mid-March, the state legislature had passed <a href="https://www.latimes.com/california/story/2022-03-14/california-legislature-passes-bill-berkeley-enrollment">emergency legislation to temporarily sidestep the issue</a>.</p>
  </li>
  <li>
    <p>Another batch of Wikipedia Good Articles <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107905159574583760">\(\mathbb{M}\)</a>):</span></p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Fibonacci_nim">Fibonacci nim</a>: subtraction game with a Fibonacci number based strategy.</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Kepler_triangle">Kepler triangle</a>: not the shape of the great pyramid of Giza, but one of its other properties inspired me to make the illustration below.</p>

        <p style="text-align: center;"><img alt="Isosceles triangle is formed from two Kepler triangles, reflected across their short sides, and its inscribed circle, having the maximum radius possible among all isosceles triangles with the same side length" src="https://11011110.github.io/blog/assets/2022/kepler.svg"/></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Component_(graph_theory)">Connected components of undirected graphs</a>: saving this batch from complete frivolity.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>New book in discrete geometry <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107914084986329325">\(\mathbb{M}\)</a>):</span> <em>Polynomial Methods and Incidence Geometry</em>, Adam Sheffer, Cambridge University Press. See <a href="https://adamsheffer.wordpress.com/2022/03/03/new-book-polynomial-methods-and-incidence-theory/">Adam’s announcement</a> and <a href="http://faculty.baruch.cuny.edu/ASheffer/000book.pdf">an early draft with missing chapters</a>.</p>
  </li>
  <li>
    <p>When I’ve been thinking recently about who I might know who is Ukrainian or Ukrainian-American, the first to mind was Andrea Danyluk, with whom I went to grad school. We lost touch later, but she had a long distinguished career at Williams College. <a href="https://president.williams.edu/in-memoriam/the-passing-of-andrea-danyluk/">Sadly, she died a few days ago</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107916240235667375">\(\mathbb{M}\)</a>).</span> The Computing Research Association chose her as the <a href="https://cra.org/about/awards/a-nico-habermann-award/#2022">2022 winner of their A. Nico Habermann Award for increasing diversity in computing research</a>, shortly before her death.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@jsiehler/107927992555262577">The SET card game is not accessible to the color-impaired</a>, its manufacturer shows no interest in fixing it or providing accessible alternatives, and is actively blocking any attempts by others to do the same. Sadly, this makes it unusable as a classroom activity.</p>
  </li>
  <li>
    <p><a href="https://www.asbmb.org/asbmb-today/careers/030822/what-s-with-wikipedia-and-women">What’s with Wikipedia and women?
Things are changing, little by little, at the open-source encyclopedia</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107933443665051198">\(\mathbb{M}\)</a>).</span> New article from the American Society for Biochemistry and Molecular Biology mentions in passing my efforts creating articles on women in STEM and patrolling deletion discussions on them.</p>
  </li>
  <li>
    <p><a href="https://sinews.siam.org/Details-Page/in-pursuit-of-perfect-pinnacles">In pursuit of perfect pinnacles</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107945184119710916">\(\mathbb{M}\)</a>).</span> Why do spiky shapes form in nature, for instance in limestone and ice? Leif Ristroph, Jinzi Mac Huang, and Michael Shelley survey recent research in this <em>SIAM News</em> column.</p>
  </li>
  <li>
    <p>Another Wikipedia Good Article, on an important rather than recreational topic: <a href="https://en.wikipedia.org/wiki/Harmonic_series_(mathematics)">harmonic series</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107950806929679318">\(\mathbb{M}\)</a>)</span>  on the divergent series</p>

\[\sum_{n=1}^\infty\frac{1}{n} = 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5} + \cdots.\]

    <p>While cleaning it up I learned that the term “harmonic number” and notation for its partial sums comes from Knuth, and also that the “crossing the desert” puzzle, one of the standard examples for harmonic series, dates to long before the harmonic series itself.</p>
  </li>
  <li>
    <p><a href="https://blog.plover.com/math/se/notation.html">Bad but interesting mathematical notation</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@mjd/107927925348652783">\(\mathbb{M}\)</a>).</span> Minimal subsystems of arithmetic aside, Mark-Jason Dominus wrestles with the problem of finding an intuitive visual representation for expressions that combine a single associative operation with two mutually inverse unary operations.</p>
  </li>
  <li>
    <p>Wikimedia foundation VP Maggie Dennis warns Wikipedia editors writing about the Russian invasion of Ukraine that <a href="https://lists.wikimedia.org/hyperkitty/list/wikimedia-l@lists.wikimedia.org/message/KIMZHJMWMKXFRCMWIE5WL3YIJNFMSNVH/">they are likely to be doxxed</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107956409088287040">\(\mathbb{M}\)</a>,</span> <a href="https://en.wikipedia.org/wiki/Wikipedia_talk:Did_you_know">via</a>), especially when their “activities are seen as opposing the Russian narrative of the war”. One assumes by the Russians, although she does not say that explicitly.</p>
  </li>
  <li>
    <p><a href="http://reconf.wikidot.com/">A wiki on combinatorial reconfiguration problems</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107963773811594015">\(\mathbb{M}\)</a>).</span> The main content at this point appears to be <a href="http://reconf.wikidot.com/papers/">their extensive bibliography of papers on the topic</a>, available both on-wiki and at a linked overleaf site. I can’t tell whether the wiki or overleaf version of the .bib file is supposed to be primary, though.</p>
  </li>
</ul></div>
    </content>
    <updated>2022-03-15T22:26:00Z</updated>
    <published>2022-03-15T22:26:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-03-16T05:27:41Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/dp-fine-tuning/</id>
    <link href="https://differentialprivacy.org/dp-fine-tuning/" rel="alternate" type="text/html"/>
    <title>Differentially private deep learning can be effective with self-supervised models</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Differential Privacy (DP) is a formal definition of privacy which guarantees that the outcome of a statistical procedure does not vary much regardless of whether an individual input is included or removed from the training dataset. 
This guarantee is desirable when we are tasked to train machine learning models on private datasets that should not memorize individual inputs. 
Past works have shown that differentially private models can be resilient to strong membership inference [<a href="https://proceedings.mlr.press/v37/kairouz15.html">1</a>, <a href="https://ieeexplore.ieee.org/abstract/document/9519424">34</a>, <a href="https://proceedings.neurips.cc/paper/2020/hash/fc4ddc15f9f4b4b06ef7844d6bb53abf-Abstract.html">35</a>] and data reconstruction attacks [<a href="https://www.usenix.org/conference/usenixsecurity19/presentation/carlini">2</a>, <a href="https://arxiv.org/abs/2201.12383">3</a>] when the privacy parameter is set to be sufficiently small. 
See a <a href="https://differentialprivacy.org/how-to-deploy-ml-with-dp/">prior post</a> for more background on differentially private machine learning.</p>

<p>Yet, in practice, most attempts at training differentially private deep learning models on moderately-sized datasets have resulted in large performance drops compared to when training without privacy-protection baked in. 
These performance drops are oftentimes large enough to discourage the adoption of differential privacy protection into machine learning pipelines altogether.</p>

<p>To provide a reference of the potential performance hit, the authors of [<a href="https://arxiv.org/abs/2102.12677">5</a>] trained a ResNet-20 from scratch on CIFAR-10 with a privacy budget of \(\epsilon=8\) that has test accuracy barely over 62% (see their Table 1). 
Contrast this with the 8.75% error rate (91.25% accuracy) reported for training the same architecture without enforcing differential privacy [<a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">6</a>]. 
While some works report private learning results better than the above, absent additional data, pre-training, or external knowledge, most improvements have been incremental, and the test accuracy for CIFAR-10 models trained under modest privacy leakage (\(\epsilon=3\)) has roughly settled to ~70% in the literature [<a href="https://arxiv.org/abs/2011.11660">4</a>].</p>

<p>One reason behind the performance drop lies in sample efficiency — differentially private learning generally requires much more data than non-private learning to reach an acceptable level of performance. 
This also means that learning the high-level features (e.g., syntactic structure in text, edge detectors for images) necessary to perform specific tasks with private data can be much more sample-costly.</p>

<p>This blog post surveys results that leverage public self-supervised pre-training to obtain high-performing models through differentially private fine-tuning.
The pre-train-fine-tune paradigm is straightforward to execute and results in high-performing models under modest privacy budgets for many standard computer vision and natural language processing tasks. 
Moreover, existing results have shown that private fine-tuning consistently benefits from improvements in public pre-training.</p>

<h2 id="self-supervised-pre-training">Self-Supervised Pre-Training</h2>

<p>Self-supervised learning is a paradigm which leverages unlabeled data to learn representations that can be useful for a range of downstream tasks.
Since self-supervised learning doesn’t target specific tasks itself, 
the (pre-)training procedure doesn’t require labeled data — in many cases, mildly curated unlabeled data is sufficient for self-supervised pre-training to produce models for subsequent fine-tuning. 
So far, there have been two broadly successful instantiations of this learning paradigm in computer vision [<a href="http://proceedings.mlr.press/v119/chen20j.html">9</a>] and natural language processing [<a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">7</a>, <a href="https://arxiv.org/abs/1810.04805">8</a>]. 
We recap the two approaches below.<sup id="fnref:1"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:1" rel="footnote">1</a></sup></p>

<p><strong>Contrastive pre-training for vision:</strong> 
One class of self-supervised methods in computer vision (SimCLR, [<a href="http://proceedings.mlr.press/v119/chen20j.html">9</a>]) performs pre-training through contrastive learning. 
Algorithms of this type produce embeddings for images with the goal of creating different embeddings for semantically different images and similar embeddings for similar ones. 
Concretely, the algorithm used in SimCLR forces models to produce similar embeddings for an image and its augmented siblings (e.g., image rotated by some degrees), 
and different embeddings for separate images (and their augmentations). 
The SimCLR framework with large scale models and compute led to state-of-the-art (non-private) ImageNet fine-tuning results at the time of its writing.</p>

<p><strong>Masked language modeling and autoregressive language modeling for text:</strong> 
Masked Language Modeling (MLM) and Auto-regressive Language Modeling (ALM) are two self-supervised pre-training approaches. 
While the former asks models to predict deliberately masked out tokens from a piece of text, the latter asks models to simply predict the next token in a sequence. 
With large amounts of unlabeled text data, large and expressive Transformer models [<a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">24</a>], and lots of compute, both approaches produce powerful models that are good starting points for downstream fine-tuning. 
For instance, Bidirectional Encoder Representations from Transformers (BERT, [<a href="https://arxiv.org/abs/1810.04805">8</a>]), produced state-of-the-art (non-private) results (at the time) for a large collection of language understanding tasks when fine-tuned on each.</p>

<h2 id="fine-tuning-self-supervised-models-with-dp-optimization">Fine-Tuning Self-Supervised Models With DP-Optimization</h2>
<p>Self-supervised pre-training is appealing in the context of differentially private machine learning. 
This is because (i) the mildly curated data needed for pre-training can usually be obtained cheaply from the public domain, and (ii) pre-trained models may contain useful domain knowledge that can reduce the sample complexity of subsequent private learning. 
A paradigm for private learning that leverages self-supervised pre-training could follow two steps:</p>

<ul>
  <li>collect cheap and public (unlabeled) data from the task domain (e.g., vision, language, etc.) to pre-train a model with self-supervised learning, and</li>
  <li>collect moderate amounts of task-specific private (labeled) data and fine-tune the pre-trained model under differential privacy to perform the task.<sup id="fnref:2"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:2" rel="footnote">2</a></sup></li>
</ul>

<p>To date, some of the best differentially private deep learning results in the literature have resulted from instantiating this paradigm [<a href="https://arxiv.org/abs/2011.11660">4</a>, <a href="https://arxiv.org/abs/2110.05679">11</a>, <a href="https://arxiv.org/abs/2110.06500">12</a>].
Below, we review works which capitalize on self-supervised pre-training by differentially privately fine-tuning pre-trained models with an iterative gradient method like DP-SGD [<a href="https://dl.acm.org/doi/abs/10.1145/2976749.2978318">19</a>, <a href="https://ieeexplore.ieee.org/abstract/document/6736861">20</a>].<sup id="fnref:3"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:3" rel="footnote">3</a></sup>
<img alt="" src="https://differentialprivacy.org/images/fine-tuning-paradigm.png"/></p>

<p><strong>Private fine-tuning with SimCLR features:</strong> 
The authors of [<a href="https://arxiv.org/abs/2011.11660">4</a>] fine-tuned a linear model on top of the embedding vectors produced by SimCLRv2 from the CIFAR-10 dataset. Under a privacy budget of \(\epsilon=2\), 
these models reached an average test accuracy of 92.7%. This number can be further improved to ~94% with the use of larger and wider pre-trained models in the SimCLRv2 family.<sup id="fnref:4"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:4" rel="footnote">4</a></sup> 
These test accuracies are very close to some standard non-private results attained by an off-the-shelf ResNet architecture [<a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">6</a>].</p>

<p><strong>Privately fine-tuning BERT variants and GPT-2:</strong> 
The authors of [<a href="https://arxiv.org/abs/2110.05679">11</a>, <a href="https://arxiv.org/abs/2110.06500">12</a>, <a href="http://proceedings.mlr.press/v139/yu21f.html">16</a>] showed that with appropriate hyper-parameters, fine-tuning BERT variants and GPT-2 with DP-optimization results in high-performing private models for text classification and language generation — even on datasets of modest sizes and under modest privacy budgets. 
Notably, some of these models attain a task performance close to non-private models from previous years in the literature. 
These results also exceed many non-private learning results from the pre-BERT and pre-GPT years.<sup id="fnref:5"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:5" rel="footnote">5</a></sup></p>

<p>More interestingly, the authors showed that the larger (and thus better) the pre-trained model, the better the private fine-tuning performance gets. 
This empirical observation in private fine-tuning of large Transformers is qualitatively different from what’s implied by the usual minimax optimal rates derived for vanilla private learning with convex loss functions under approximate differential privacy [<a href="https://ieeexplore.ieee.org/abstract/document/6979031">14</a>, <a href="https://proceedings.neurips.cc/paper/2019/hash/3bd8fdb090f1f5eb66a00c84dbc5ad51-Abstract.html">15</a>]. 
This discrepancy between experimental results for training large models and the theory for learning with convex losses suggests there is more to be understood.<sup id="fnref:6"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:6" rel="footnote">6</a></sup></p>

<p>Overall, for both vision and language tasks, private learning performance has consistently improved with the improvement in the quality of pre-training, 
where the latter is measured by the non-private fine-tuning performance.<sup id="fnref:7"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:7" rel="footnote">7</a></sup></p>

<p>
  <img src="https://differentialprivacy.org/../images/figure1_classification.png" width="48%"/>
  <img src="https://differentialprivacy.org/../images/figure1_generation.png" width="48%"/> 
  Figure 1: Privately fine-tuning better (and larger) pre-trained models lead to consistently improving performance for text classification and language generation. 
Left: text classification on MNLI [<a href="https://arxiv.org/abs/1704.05426">25</a>]. Right: language generation on E2E [<a href="https://arxiv.org/abs/1706.09254">26</a>].
</p>

<h2 id="conclusion-and-outlook">Conclusion and Outlook</h2>

<p>We surveyed recent works in the literature that obtained highly performant private machine learning models leveraging self-supervised pre-training. 
Common to these results is the trend that the performance of private learning consistently improved with the quality of public pre-training. 
We therefore anticipate that the general paradigm may be useful in additional settings (e.g., federated learning) and tasks (e.g., private synthetic image generation), and lead to better private learning results.</p>

<p>We have thus far assumed that the data for public pre-training can be cheaply obtained.
This, however, does not imply that determining whether a particular source of data is appropriate for public pre-training is an easy problem.
Using publicly available data is not necessarily risk-free in terms of privacy.
For instance, the authors of [<a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting">33</a>] were able to extract personally identifiable information from a GPT-2 model pre-trained on data scraped from the public internet.</p>

<p>Self-supervised pre-training has led to progress in private deep learning, but leveraging pre-trained models alone will not address several fundamental challenges to differentially private learning.
First and foremost, the datasets of machine learning tasks may be sampled from long-tailed distributions [<a href="https://proceedings.neurips.cc/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html">21</a>]. 
When privately trained on such datasets, a machine learning model may fail to acquire the learning signal necessary to perform accurate predictions for examples on the tail [<a href="https://dl.acm.org/doi/abs/10.1145/3442188.3445934">28</a>] or from underrepresented (sub)populations [<a href="https://proceedings.neurips.cc/paper/2019/hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html">29</a>]. 
Second, many machine learning problems are in a domain where public data (even unlabeled data) may be sparse, e.g., medical imaging. 
Developing refined versions of the pre-train-fine-tune approach for problems from these domains is an interesting avenue for future work.</p>

<p>Lastly, differential privacy as one specific definition of privacy may not capture all that’s desired for privacy in reality. 
For instance, while differentially private algorithms naturally give machine unlearning guarantees [<a href="https://ieeexplore.ieee.org/abstract/document/9519428">30</a>, <a href="https://ieeexplore.ieee.org/abstract/document/7163042">32</a>], tailored unlearning algorithms tend to have higher capacities of unlearning [<a href="https://proceedings.neurips.cc/paper/2021/hash/9627c45df543c816a3ddf2d8ea686a99-Abstract.html">31</a>].
In addition, what constitutes a record in the differential privacy framework can oftentimes be unclear. 
Inappropriately defined example boundaries can create correlated records which cause differential privacy guarantees to degrade [<a href="https://arxiv.org/abs/1603.01508">22</a>].
Moreover, differential privacy guarantees won’t directly prevent the inference of private data outside the original context [<a href="https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/washlr79&amp;section=16">23</a>]. 
These are fundamental limitations of differential privacy which improvements to differentially private learning won’t touch on.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>The authors thank Nicolas Papernot and Gautam Kamath for detailed feedback and edit suggestions.</p>

<hr/>

<h2 id="references">References</h2>
<p>[1] Rahman MA, Rahman T, Laganière R, Mohammed N, Wang Y. Membership Inference Attack against Differentially Private Deep Learning Model. Trans. Data Priv.. 2018 Apr 1;11(1):61-79.</p>

<p>[2] Carlini N, Liu C, Erlingsson Ú, Kos J, Song D. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX Security Symposium (USENIX Security 19) 2019 (pp. 267-284).</p>

<p>[3] Guo C, Karrer B, Chaudhuri K, van der Maaten L. Bounding Training Data Reconstruction in Private (Deep) Learning. arXiv preprint arXiv:2201.12383. 2022 Jan 28.</p>

<p>[4] Tramer F, Boneh D. Differentially private learning needs better features (or much more data). arXiv preprint arXiv:2011.11660. 2020 Nov 23.</p>

<p>[5] Yu D, Zhang H, Chen W, Liu TY. Do not let privacy overbill utility: Gradient embedding perturbation for private learning. arXiv preprint arXiv:2102.12677. 2021 Feb 25.</p>

<p>[6] He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).</p>

<p>[7] Radford A, Narasimhan K, Salimans T, Sutskever I. Improving language understanding by generative pre-training.</p>

<p>[8] Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. 2018 Oct 11.</p>

<p>[9] Chen T, Kornblith S, Norouzi M, Hinton G. A simple framework for contrastive learning of visual representations. InInternational conference on machine learning 2020 Nov 21 (pp. 1597-1607). PMLR.</p>

<p>[10] Li XL, Liang P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190. 2021 Jan 1.</p>

<p>[11] Li X, Tramer F, Liang P, Hashimoto T. Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679. 2021 Oct 12.</p>

<p>[12] Yu D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J, Lee YT, Manoel A, Wutschitz L, Yekhanin S. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500. 2021 Oct 13.</p>

<p>[13] Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. 2019 Jul 26.</p>

<p>[14] Bassily R, Smith A, Thakurta A. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th Annual Symposium on Foundations of Computer Science 2014 Oct 18 (pp. 464-473). IEEE.</p>

<p>[15] Bassily R, Feldman V, Talwar K, Guha Thakurta A. Private stochastic convex optimization with optimal rates. Advances in Neural Information Processing Systems. 2019;32.</p>

<p>[16] Yu D, Zhang H, Chen W, Yin J, Liu TY. Large scale private learning via low-rank reparametrization. InInternational Conference on Machine Learning 2021 Jul 1 (pp. 12208-12218). PMLR.</p>

<p>[17] Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are unsupervised multitask learners. OpenAI blog. 2019 Feb 24;1(8):9.</p>

<p>[18] Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, Bernstein MS, Bohg J, Bosselut A, Brunskill E, Brynjolfsson E, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. 2021 Aug 16.</p>

<p>[19] Abadi M, Chu A, Goodfellow I, McMahan HB, Mironov I, Talwar K, Zhang L. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security 2016 Oct 24 (pp. 308-318).</p>

<p>[20] Song S, Chaudhuri K, Sarwate AD. Stochastic gradient descent with differentially private updates. In2013 IEEE Global Conference on Signal and Information Processing 2013 Dec 3 (pp. 245-248). IEEE.</p>

<p>[21] Feldman V, Zhang C. What neural networks memorize and why: Discovering the long tail via influence estimation. Advances in Neural Information Processing Systems. 2020;33:2881-91.</p>

<p>[22] Ghosh A, Kleinberg R. Inferential privacy guarantees for differentially private mechanisms. arXiv preprint arXiv:1603.01508. 2016 Mar 4.</p>

<p>[23] Nissenbaum H. Privacy as contextual integrity. Wash. L. Rev.. 2004;79:119.</p>

<p>[24] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. Advances in neural information processing systems. 2017;30.</p>

<p>[25] Williams A, Nangia N, Bowman SR. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426. 2017 Apr 18.</p>

<p>[26] Novikova J, Dušek O, Rieser V. The E2E dataset: New challenges for end-to-end generation. arXiv preprint arXiv:1706.09254. 2017 Jun 28.</p>

<p>[27] Papernot N, Chien S, Song S, Thakurta A, Erlingsson U. Making the shoe fit: Architectures, initializations, and tuning for learning with privacy.</p>

<p>[28] Suriyakumar VM, Papernot N, Goldenberg A, Ghassemi M. Chasing your long tails: Differentially private prediction in health care settings. InProceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency 2021 Mar 3 (pp. 723-734).</p>

<p>[29] Bagdasaryan E, Poursaeed O, Shmatikov V. Differential privacy has disparate impact on model accuracy. Advances in Neural Information Processing Systems. 2019;32.</p>

<p>[30] Bourtoule L, Chandrasekaran V, Choquette-Choo CA, Jia H, Travers A, Zhang B, Lie D, Papernot N. Machine unlearning. In2021 IEEE Symposium on Security and Privacy (SP) 2021 May 24 (pp. 141-159). IEEE.</p>

<p>[31] Sekhari A, Acharya J, Kamath G, Suresh AT. Remember what you want to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems. 2021 Dec 6;34.</p>

<p>[32] Cao Y, Yang J. Towards making systems forget with machine unlearning. In2015 IEEE Symposium on Security and Privacy 2015 May 17 (pp. 463-480). IEEE.</p>

<p>[33] Carlini N, Tramer F, Wallace E, Jagielski M, Herbert-Voss A, Lee K, Roberts A, Brown T, Song D, Erlingsson U, Oprea A. Extracting training data from large language models. In30th USENIX Security Symposium (USENIX Security 21) 2021 (pp. 2633-2650).</p>

<p>[34] Nasr M, Songi S, Thakurta A, Papemoti N, Carlin N. Adversary instantiation: Lower bounds for differentially private machine learning. In2021 IEEE Symposium on Security and Privacy (SP) 2021 May 24 (pp. 866-882). IEEE.</p>

<p>[35] Jagielski M, Ullman J, Oprea A. Auditing differentially private machine learning: How private is private sgd?. Advances in Neural Information Processing Systems. 2020;33:22205-16.</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Authors of [<a href="https://arxiv.org/abs/2108.07258">18</a>] framed these self-supervised models which are trained on broad data at scale that are adaptable to a wide range of downstream tasks as “foundation models.” <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>The idea of privately fine-tuning a publicly pre-trained model certainly isn’t new. One of the first differentially private deep learning papers [<a href="https://arxiv.org/abs/1607.00133">19</a>] considered an experiment which fine-tuned convolutional nets on CIFAR-10 which were pre-trained on CIFAR-100. Results on privately fine-tuning <em>self-supervised</em> models are, on the other hand, more recent. Covering these results is our main focus here. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:2">↩</a></p>
    </li>
    <li id="fn:3">
      <p>Blue and pink sphere avatars taken from [<a href="https://arxiv.org/abs/2108.07258">18</a>]. Credit to <a href="https://cs.stanford.edu/~dorarad/">Drew A. Hudson</a> for making these. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:3">↩</a></p>
    </li>
    <li id="fn:4">
      <p>Unpublished result. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:4">↩</a></p>
    </li>
    <li id="fn:5">
      <p>Hyper-parameters that work well for non-private learning typically aren’t those that work best for differentially private learning [<a href="https://openreview.net/pdf?id=rJg851rYwH">27</a>]. It’s crucial to use a large batch size, a small clipping norm, an appropriate learning rate, and a reasonably large number of training epochs to obtain the mentioned private learning results [<a href="https://arxiv.org/abs/2110.05679">11</a>]. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:5">↩</a></p>
    </li>
    <li id="fn:6">
      <p>In practice, past works have presented mixed results on whether larger models would yield better performance. While some showed that using more filters in a convolutional network can degrade the performance of private learning after some threshold [<a href="https://openreview.net/pdf?id=rJg851rYwH">27</a>], others showed that a larger model can outperform a smaller model from a different model family [<a href="https://arxiv.org/abs/2011.11660">4</a>]. Note these results are conditioned on their particular hyperparameter choices. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:6">↩</a></p>
    </li>
    <li id="fn:7">
      <p>Since the pre-training data for large language models are oftentimes collected through large scale web scraping (e.g., WebText), a common concern is that some training and test instances for downstream tasks may already appear in the pre-training data. Self-supervised pre-training therefore can give models an opportunity to “see” this data even before they are privately fine-tuned. Authors of [<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">17</a>] confirmed that there is a 1-6% overlap between the test set of many natural language processing tasks and the pre-training data they collected (WebText); these common tasks, however, don’t include those studied by authors of [<a href="https://arxiv.org/abs/2110.05679">11</a>]. The numbers suggest a possibility that existing private fine-tuning results in the literature could be slightly inflated compared to when the pre-training data didn’t contain any instance for any downstream task for which evaluation was performed. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:7">↩</a></p>
    </li>
  </ol>
</div></div>
    </summary>
    <updated>2022-03-15T19:00:00Z</updated>
    <published>2022-03-15T19:00:00Z</published>
    <author>
      <name>Tatsunori Hashimoto</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2022-03-15T22:46:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3121871012533153752</id>
    <link href="http://blog.computationalcomplexity.org/feeds/3121871012533153752/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/problem-x-wont-be-solved-in-my-lifetime.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/3121871012533153752" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/3121871012533153752" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/problem-x-wont-be-solved-in-my-lifetime.html" rel="alternate" type="text/html"/>
    <title>Problem X won't be solved in MY lifetime- but what about...</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>1) In 1989 on the episde The Royale of Star Trek: The Next Generation (which takes place in the far future)  Captain Picard is working on Fermat's last theorem which he says quite explicitly is still open.</p><p>When I saw the episode I asked Larry Washington, a Number Theorist at Univ of MD, when he thought FLT would be solved. He said</p><p>                                      <i>It will be solved within the next 10 years.</i></p><p>And indeed- Wiles solved it in 1993-sort of. There was a flaw in the proof which he fixed in 1994 with the help of his former student Richard Taylor. Wiles published the correction to the flaw in 1995, so we will date it as having been solved in 1995. Larry Washington was correct.  And in an episode of Star Trek: Deep Space Nine in 1995 (episode name:Facets) Dax says that a previous host, Tobin Dax, had done the most creative work on FLT since Wiles. Maybe Tobin wrote this limerick:</p><p>A challenge for many long ages</p><p>Had baffled the savants and sages</p><p>Yet at last came the light</p><p>Seems that Fermat was right</p><p>To the margin add 200 pages.</p><p><br/></p><p>I asked Larry W when he thought Riemann would be solved. He said  </p><p>                   <i> In your lifetime but not in mine.</i></p><p>He is about 10 years older than I am and I think we are both in good health. This seems like a rather precise prediction so I am skeptical. But he did get FLT right...</p><p>2) In class I sometimes say things like </p><p><i>I do not think Quantum Computers will factor faster than classical in my lifetime. </i></p><p><i>I do not think P vs NP will be solved in my lifetime.</i></p><p><i>I can imagine P=BPP will be proven in my lifetime. (I said that 10 years ago. I am less imaginative now.) </i></p><p><i>I hope the muffin problem is solved in my lifetime (it was, see <a href="https://arxiv.org/abs/1907.08726">here</a>).</i></p><p>I didn't quite think about the difference in my age and the students until recently when I was working with Ilya Hajiaghayi (Mohammd H's 9 year old son) on cryptography and he said </p><p><i>In your recorded lecture you said you don't think quantum computers will be a threat to cryptography  in your lifetime. What about in my lifetime?</i></p><p>Indeed- his lifetime and mine are rather far apart. </p><p>I am reminded that one of the answers to my P vs NP poll made the point that while we have some sense of what will happen in the next 10 years, maybe even 20, math and life can change so much that conjectures beyond that are guesswork. Any  prediction for x years from now you should have confidence &lt; 1/ln(x) of it being true.</p><p><i><br/></i></p><p><i><br/></i></p><p><i><br/></i></p><p><br/></p></div>
    </content>
    <updated>2022-03-15T14:30:00Z</updated>
    <published>2022-03-15T14:30:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-03-15T21:37:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2022/03/15/external-validity/</id>
    <link href="http://benjamin-recht.github.io/2022/03/15/external-validity/" rel="alternate" type="text/html"/>
    <title>Machine Learning has a validity problem.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>One of the central tenets of machine learning warns the more times you run experiments with the same test set, the more you overfit to that test set. This conventional wisdom is mostly wrong and prevents machine learning from reconciling its inductive nihilism with the rest of the empirical sciences.</p>

<p>Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar led an passionate quest to test the overfitting hypothesis, devoting countless hours to reproducing machine learning benchmarks. In particular, they painstakingly recreated a test set of the famous <a href="https://www.image-net.org/">ImageNet benchmark</a>, which itself is responsible for bringing about the latest AI feeding frenzy. Out of the many surprises in my research career, what <a href="https://arxiv.org/abs/1902.10811">they found surprised me the most.</a></p>

<p class="center"><img alt="The scatterplot of nightmares" src="http://www.argmin.net/assets/RSS_Scatter.png" width="90%"/></p>

<p>In this graph, the x-axis is the accuracy on the original ImageNet benchmark, which has been used millions of times by individual researchers at Google alone. On the y-axis is the accuracy evaluated on “ImageNet v2” set, which was made by closely trying to replicate the data creation method for the benchmark. Each blue dot represents a single machine learning model trained on the original ImageNet data. The red line is a linear fit to these models, and the dashed line is what we would see if the accuracy was the same on both test sets. What do we see? The models which perform the best on the original test set perform the best on the new test set. That is, there is no evidence of overfitting.</p>

<p>What is clear, however, is a noticeable drop in performance on the new test set. Despite their best efforts in reproducing the ImageNet protocol, there is evidence of a <em>distribution shift</em>. Distribution shift is a far reaching term describing whenever the data on which a machine learning algorithm is deployed is different from the data on which it is trained. The Mechanical Turk workers who labeled the images were different from those originally employed. The API used for the labeling was slightly different. The selection mechanism to aggregate differences in opinions between labelers is slightly different. The small differences add up to around a 10% drop in accuracy, equivalent to five years of progress on the benchmark.</p>

<p>Folks in my research group have reproduced this phenomenon several times. In <a href="https://papers.nips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning">Kaggle competitions</a>, where the held out set and validation set were <em>identically</em> distributed, we saw no overfitting <em>and</em> no distribution shift. We found sensitivity to distribution shifts in CIFAR10, in <a href="https://arxiv.org/abs/1906.02168">video</a>, and in <a href="https://arxiv.org/abs/2004.14444">question answering</a> benchmarks. And Chhavi Yadav and Leon Bottou showed that we have not yet overfit to the <a href="https://arxiv.org/abs/1905.10498">venerable MNIST data set</a>, but distribution shift remains a challenge.</p>

<p>The marked sensitivity to distribution shift is a huge issue. If small ambiguities in reproductions lead to large losses in predictive performance, what happens when we take ML systems designed on static benchmarks and deploy them in important applications? A decade of AI fever has delivered piles of evidence that distribution shift is machine learning’s achilles heel. Algorithms run inside the big tech companies need to be constantly retrained with their huge computing resources. <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002683">Data-driven algorithms for radiology often fail if one changes the X-ray machine</a>. <a href="https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2781307">AI algorithms for sepsis fail if you change hospitals</a>. And self-driving car systems are readily confused in new environments (No citation needed. Keep your Tesla away from me.).</p>

<p>The only way forward is for machine learning to engage more broadly with other scientists who have been tackling similar issues for centuries. My first proposal is simple: let’s change our terminology to align with the rest of the sciences. The study of distribution shift in machine learning has always been insular and, while machine learning is particularly sensitive, all empirical science must deal with the jump from experiment to reality.</p>

<p>With this in mind, <a href="https://twitter.com/rajiinio">Deb Raji</a> and I have been digging through the scientific literature for a while now hoping to find some answers. In most other parts of science, “robustness to distribution shift” is called external validity. External validity quantifies how well a finding generalizes beyond a specific experiment. For example, a significant result on a particular cohort may not generalize to a broader population.</p>

<p>Predictive algorithms and experimental science both rely on repeatability. “The sun has always risen in the east.” “The apple always falls straight to the ground.” We expect that given the same contexts, the natural world more or less repeats itself. There is unfortunately a big leap from the sun rising in the morning, to an experimental finding in machine learning or biomedicine being reproducible. Why?</p>

<p>The experimental contexts under which predictions and inferences are designed are often far too narrow. The results of a study performed on young male college students in Maine may not help us understand properties of a retirement community in Arizona. These populations are different! However, it may give us insights into other cohorts of male college students: a study at Bates may generalize to Colby or Bowdoin.</p>

<p>Contexts can change in a myriad of ways. Some examples include the following:</p>

<ol>
  <li>The context can just be too narrow in the experiment. Do studies on adults generalize to children? Do studies on medications with only men generalize to women?</li>
  <li>The measured quantity may itself change. It is often easier to measure, detect, and control for exogenous disturbances in a lab setting than in the real world.</li>
  <li>Populations can change over time. For example, medical recommendations from the 1980s may no longer apply to the current population. Recent developments have led to <a href="https://www.npr.org/2021/10/13/1045746669/task-force-says-most-people-should-not-take-daily-aspirin-to-prevent-a-heart-att">not recommending aspirin to prevent heart attacks</a>.  Machine Learners like to call this <em>covariate shift</em>.</li>
  <li>Even more nefariously, the population can change in response to the intervention. A classic example of this is <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart’s Law</a> which states “Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.”</li>
</ol>

<p>How can we grapple with these external validity challenges? Verifying external validity is daunting and the set of potential solutions remains quite limited. As I mentioned, Deb and I have been chatting about this for a year now, and we’ve now dragged the rest of the group into our investigations. So I’m going to share the blog with Deb for a few posts now, and we’ll both expand on what we’ve been reading and thinking about. In the next few posts, we’ll explore some of the intricacies of when external validity can fail and will also try to spell out some of the research directions that might help bridge the gaps between experiments and reality.</p></div>
    </summary>
    <updated>2022-03-15T00:00:00Z</updated>
    <published>2022-03-15T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2022-03-15T22:45:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2203.06766</id>
    <link href="http://arxiv.org/abs/2203.06766" rel="alternate" type="text/html"/>
    <title>On the $d$-Claw Vertex Deletion Problem</title>
    <feedworld_mtime>1647302400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hsieh:Sun=Yuan.html">Sun-Yuan Hsieh</a>, Hoang-Oanh Le, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Le:Van_Bang.html">Van Bang Le</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Sheng=Lung.html">Sheng-Lung Peng</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2203.06766">PDF</a><br/><b>Abstract: </b>Let $d$-claw (or $d$-star) stand for $K_{1,d}$, the complete bipartite graph
with 1 and $d\ge 1$ vertices on each part. The $d$-claw vertex deletion
problem, $d$-CLAW-VD, asks for a given graph $G$ and an integer $k$ if one can
delete at most $k$ vertices from $G$ such that the resulting graph has no
$d$-claw as an induced subgraph. Thus, 1-CLAW-VD and 2-CLAW-VD are just the
famous VERTEX COVER problem and the CLUSTER VERTEX DELETION problem,
respectively. In this paper, we strengthen a hardness result in [M. Yannakakis,
Node-Deletion Problems on Bipartite Graphs, SIAM J. Comput. (1981)], by showing
that CLUSTER VERTEX DELETION remains NP-complete when restricted to bipartite
graphs of maximum degree 3. Moreover, for every $d\ge 3$, we show that
$d$-CLAW-VD is NP-complete even when restricted to bipartite graphs of maximum
degree $d$. These hardness results are optimal with respect to degree
constraint. By extending the hardness result in [F. Bonomo-Braberman et al.,
Linear-Time Algorithms for Eliminating Claws in Graphs, COCOON 2020], we show
that, for every $d\ge 3$, $d$-CLAW-VD is NP-complete even when restricted to
split graphs without $(d+1)$-claws, and split graphs of diameter 2. On the
positive side, we prove that $d$-CLAW-VD is polynomially solvable on what we
call $d$-block graphs, a class properly contains all block graphs. This result
extends the polynomial-time algorithm in [Y. Cao et al., Vertex deletion
problems on chordal graphs, Theor. Comput. Sci. (2018)] for 2-CLAW-VD on block
graphs to $d$-CLAW-VD for all $d\ge 2$ and improves the polynomial-time
algorithm proposed by F. Bonomo-Brabeman et al. for (unweighted) 3-CLAW-VD on
block graphs to 3-block graphs.
</p></div>
    </summary>
    <updated>2022-03-15T22:40:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-03-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2203.06718</id>
    <link href="http://arxiv.org/abs/2203.06718" rel="alternate" type="text/html"/>
    <title>Local Hadwiger's Conjecture</title>
    <feedworld_mtime>1647302400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moore:Benjamin.html">Benjamin Moore</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Postle:Luke.html">Luke Postle</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Turner:Lise.html">Lise Turner</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2203.06718">PDF</a><br/><b>Abstract: </b>We propose local versions of Hadwiger's Conjecture, where only balls of
radius $\Omega(\log(v(G)))$ around each vertex are required to be
$K_{t}$-minor-free. We ask: if a graph is locally-$K_{t}$-minor-free, is it
$t$-colourable? We show that the answer is yes when $t \leq 5$, even in the
stronger setting of list-colouring, and we complement this result with a
$O(\log v(G))$-round distributed colouring algorithm in the LOCAL model.
Further, we show that for large enough values of $t$, we can list-colour
locally-$K_{t}$-minor-free graphs with $13 \cdot \max\left\{h(t),\left\lceil
\frac{31}{2}(t-1) \right\rceil \right\}$ colours, where $h(t)$ is any value
such that all $K_{t}$-minor-free graphs are $h(t)$-list-colourable. We again
complement this with a $O(\log v(G))$-round distributed algorithm.
</p></div>
    </summary>
    <updated>2022-03-15T22:42:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-03-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2203.06660</id>
    <link href="http://arxiv.org/abs/2203.06660" rel="alternate" type="text/html"/>
    <title>Maximally Satisfying Lower Quotas in the Hospitals/Residents Problem with Ties and Incomplete Lists</title>
    <feedworld_mtime>1647302400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makino:Kazuhisa.html">Kazuhisa Makino</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miyazaki:Shuichi.html">Shuichi Miyazaki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yokoi:Yu.html">Yu Yokoi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2203.06660">PDF</a><br/><b>Abstract: </b>To mitigate imbalance in the numbers of assignees in the Hospitals/Residents
problem, Goko et al. [Goko et al., Maximally Satisfying Lower Quotas in the
Hospitals/Residents Problem with Ties, Proc. STACS 2022, pp. 31:1--31:20.]
studied the Hospitals/Residents problem with lower quotas whose goal is to find
a stable matching that satisfies lower quotas as much as possible. In their
paper, preference lists are assumed to be complete. In this paper, we study
more general model where preference lists may be incomplete. For four natural
scenarios, we obtained maximum gaps of the best and worst solutions,
approximability results, and inapproximability results.
</p></div>
    </summary>
    <updated>2022-03-15T22:38:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-03-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2203.06659</id>
    <link href="http://arxiv.org/abs/2203.06659" rel="alternate" type="text/html"/>
    <title>Short Topological Decompositions of Non-Orientable Surfaces</title>
    <feedworld_mtime>1647302400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Niloufar Fuladi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hubard:Alfredo.html">Alfredo Hubard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mesmay:Arnaud_de.html">Arnaud de Mesmay</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2203.06659">PDF</a><br/><b>Abstract: </b>In this article, we investigate short topological decompositions of
non-orientable surfaces and provide algorithms to compute them. Our main result
is a polynomial-time algorithm that for any graph embedded in a non-orientable
surface computes a canonical non-orientable system of loops so that any loop
from the canonical system intersects any edge of the graph in at most 30
points. The existence of such short canonical systems of loops was well known
in the orientable case and an open problem in the non-orientable case. Our
proof techniques combine recent work of Schaefer-\v{S}tefankovi\v{c} with ideas
coming from computational biology, specifically from the signed reversal
distance algorithm of Hannenhalli-Pevzner. The existence of short canonical
non-orientable systems of loops confirms a special case of a conjecture of
Negami on the joint crossing number of two embeddable graphs. We also provide a
correction for an argument of Negami bounding the joint crossing number of two
non-orientable graph embeddings.
</p></div>
    </summary>
    <updated>2022-03-15T22:43:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-03-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2203.06633</id>
    <link href="http://arxiv.org/abs/2203.06633" rel="alternate" type="text/html"/>
    <title>A square root velocity framework for curves of bounded variation</title>
    <feedworld_mtime>1647302400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grasmair:Markus.html">Markus Grasmair</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2203.06633">PDF</a><br/><b>Abstract: </b>The square root velocity transform is a powerful tool for the efficient
computation of distances between curves. Also, after factoring out
reparametrisations, it defines a distance between shapes that only depends on
their intrinsic geometry but not the concrete parametrisation. Though
originally formulated for smooth curves, the square root velocity transform and
the resulting shape distance have been thoroughly analysed for the setting of
absolutely continuous curves using a relaxed notion of reparametrisations. In
this paper, we will generalise the square root velocity distance even further
to a class of discontinuous curves. We will provide an explicit formula for the
natural extension of this distance to curves of bounded variation and analyse
the resulting quotient distance on the space of unparametrised curves. In
particular, we will discuss the existence of optimal reparametrisations for
which the minimal distance on the quotient space is realised.
</p></div>
    </summary>
    <updated>2022-03-15T22:44:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-03-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2203.06630</id>
    <link href="http://arxiv.org/abs/2203.06630" rel="alternate" type="text/html"/>
    <title>Maximum cut on interval graphs of interval count two is NP-complete</title>
    <feedworld_mtime>1647302400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Barsukov:Alexey.html">Alexey Barsukov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bose:Kaustav.html">Kaustav Bose</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roy:Bodhayan.html">Bodhayan Roy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2203.06630">PDF</a><br/><b>Abstract: </b>We show that the Max-Cut problem is NP-complete on interval graphs of
interval count two.
</p></div>
    </summary>
    <updated>2022-03-15T22:37:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-03-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2203.06597</id>
    <link href="http://arxiv.org/abs/2203.06597" rel="alternate" type="text/html"/>
    <title>Distributed Subgraph Finding: Progress and Challenges</title>
    <feedworld_mtime>1647302400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Censor=Hillel:Keren.html">Keren Censor-Hillel</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2203.06597">PDF</a><br/><b>Abstract: </b>This is a survey of the exciting recent progress made in understanding the
complexity of distributed subgraph finding problems. It overviews the results
and techniques for assorted variants of subgraph finding problems in various
models of distributed computing, and states intriguing open questions. This
version contains some updates over the ICALP 2021 version, and I will try to
keep updating it as additional progress is made.
</p></div>
    </summary>
    <updated>2022-03-15T22:43:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-03-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19736</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/" rel="alternate" type="text/html"/>
    <title>Are These the Last Digits of Pi?</title>
    <summary>Ghoulish reflections on whether mathematics is emergent Composite of src1, src2 Thomas Keller and Heiko Rölke led a team at the University of Applied Sciences in Graubünden, Switzerland, that set a new record for the computation of last August. They computed 62.8 trillion digits of . The last ten digits they obtained are 7817924264. Today, […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Ghoulish reflections on whether mathematics is emergent</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/kellerrolkepi/" rel="attachment wp-att-19738"><img alt="" class="alignright wp-image-19738" height="95" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/KellerRolkePi.png?resize=222%2C95&amp;ssl=1" width="222"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://www.fhgr.ch/en/specialist-areas/applied-future-technologies/davis-centre/pi-challenge/">src1</a>, <a href="https://www.welt.de/wissenschaft/plus233700070/Zahl-Pi-Forscher-haben-62-8-Billionen-Stellen-ermittelt.html">src2</a></font></td>
</tr>
</tbody>
</table>
<p>
Thomas Keller and Heiko Rölke led a team at the University of Applied Sciences in Graubünden, Switzerland, that set a new record for the computation of <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> last August. They computed 62.8 trillion digits of <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The last ten digits they obtained are 7817924264.</p>
<p>
Today, we wish people Happy Pi Day amid wishes for happier days overall.</p>
<p>
Pi Day needs the day to be written American-style as 3/14/22. In international style it would be 31/4/22, but April does not have 31 days. This year involves the numerator of the simplest serviceable approximation to pi: </p>
<p align="center"><img alt="\displaystyle  \pi \approx \frac{22}{7} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpi+%5Capprox+%5Cfrac%7B22%7D%7B7%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Because <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is irrational, any finite fraction or number of digits is an approximation. Because <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is computable—indeed highly efficiently computable in <a href="https://rjlipton.wpcomstaging.com/2009/03/15/cooks-class-contains-pi/">senses</a> we have <a href="https://rjlipton.wpcomstaging.com/2010/07/14/making-an-algorithm-an-algorithm-bbp/">covered</a>—we can adduce that the code for doing so represents <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> exactly. Furthermore, the symbol <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> lends itself to many other calculations that yield exact finite results. </p>
<p>
The digits, however, have their own mystique. We still do not know whether they are <a href="http://pi314.at/math/normal.html">normal</a> in any base, let alone base ten. Speaking as mathematical Platonists, we regard the infinite sequence as a completed, unchanging entity—one for which assertions like “it is normal” have currently-definite truth values. </p>
<p>
This is what events of the past few weeks have prompted Dick and me to question. If our world presently stops existing, 7817924264 will be the last digits of <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that we know.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/pimug/" rel="attachment wp-att-19739"><img alt="" class="alignright wp-image-19739" height="168" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/PiMug.jpg?resize=150%2C168&amp;ssl=1" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Etsy Math Mug <a href="https://www.etsy.com/listing/497368583/math-mug-my-pin-is-the-last-4-digits-of">source</a></font>
</td>
</tr>
</tbody></table>
<p/><h2> Not Just WW III </h2><p/>
<p/><p>
I already have a story of impermanent truth rooted in Russia. On my phone I have a free app for the <a href="https://chessok.com/?page_id=27966">Lomonosov Tablebases</a>, which give the perfect result of all chess positions with 7 or fewer pieces. Those tables took years to compile and exist only as 100+ terabytes on a machine at the Lomonosov Moscow State University computer center. </p>
<p>
Sometime over the new year, I noticed that the server stopped working. It was <a href="http://talkchess.com/forum3/viewtopic.php?f=2&amp;t=74046">reportedly</a> the victim of a ransomware attack. Is it meaningful to say that the tables currently exist? Mathematically, yes, and physically likely also yes—assuming the bits were merely blocked and not altered on the storage plates. </p>
<p>
Happily, there is a second 7-piece table called <a href="https://lichess.org/blog/W3WeMyQAACQAdfAL/7-piece-syzygy-tablebases-are-complete">Syzygy</a>, which uses only about 18 terabytes and exists in multiple dowloaded copies. However, this leads us to another question about correctness. The Syzygy computation reproduced some key extremal features of the Lomonosov compilation, such as the position with the longest number of moves needed to win. Not all have been verifiable, because Syzygy counts the time needed to make concrete progress in the form of a capture or pawn advance rather than the time to give checkmate. What I don’t know—and maybe now won’t know—is:</p>
<blockquote><p><b> </b> <em> Has Syzygy been used to verify every win/draw/loss (WDL) verdict computed by Lomonosov, and vice-versa? </em>
</p></blockquote>
<p/><p>
Doing so would require cross-checking many trillions of chess positions. Of course, we should expect that the algorithms used to produce these tables have been verified as completely correct. </p>
<p>
That is worth saying again: The chess algorithms have been verified in themselves. This is not a case of a shock I had before Christmas, when a module in my own chess software threw an error for the first time since I wrote it in 2014, having worked perfectly on over 100 million moves in several million games. The function in question records not only the exact source and destination squares of a move but also the minimum information required by short-form notation systems to disambiguate it from other pieces that could move to the square. There was one game where the players horsed around until one side had 5 queens that could all move to the same square, and that was 1 more than my scheme had presumed possible. My code is thus not-quite correct.</p>
<p>
But even analytical correctness fails in cases of hardware error. A cosmic ray temporarily <a href="https://www.independent.co.uk/news/science/subatomic-particles-cosmic-rays-computers-change-elections-planes-autopilot-a7584616.html">changed</a> the outcome of an election in Belgium. Fortunately, the systems have cross-checks to catch these events. When the subject is <em>mathematical truths</em>, however, how are we checking? On what basis can we be satisfied by such checks?</p>
<p>
</p><p/><h2> Emergence </h2><p/>
<p/><p>
Despite our Platonist convictions, as practitioners of mathematics we experience its truths as <a href="https://en.wikipedia.org/wiki/Emergence">emergent</a>. By “emergent” we mean more than saying theorems are unknown until the point in time where they are clearly proved. <em>Pace</em> our <a href="https://rjlipton.wpcomstaging.com/2019/04/21/pnp-proofs/">claimers</a>, P versus NP has not been proved either way, and we live in a world where even those who strongly believe <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> will aver it is unknown. We <a href="https://rjlipton.wpcomstaging.com/2021/12/31/make-a-trillion-dollars/">covered</a> this recently.</p>
<p>
Our sense of <em>emergent</em> meets at least the “weak” criterion enunciated <a href="http://consc.net/papers/emergence.pdf">here</a> by David Chalmers, a sense of unexpectedness <a href="https://rjlipton.wpcomstaging.com/2015/10/29/guessing-conjectures/">that</a> we <a href="https://rjlipton.wpcomstaging.com/2011/04/13/even-great-mathematicians-guess-wrong/">have</a> often <a href="https://rjlipton.wpcomstaging.com/2010/06/19/guessing-the-truth/">discussed</a> going <a href="https://rjlipton.wpcomstaging.com/2009/09/27/surprises-in-mathematics-and-theory/">back</a> to the <a href="https://rjlipton.wpcomstaging.com/2009/02/19/we-all-guessed-wrong/">beginning</a> of the blog. </p>
<p>
Chalmers’s strong sense, when applied to mathematics, leads into independence results of the kind effected by Kurt Gödel, this blog’s eponym. We realize that none of our 1,000+ posts has yet attempted a deep appraisal of what these independence results <em>mean</em>. We will not do so now because we are questioning something more basic: Discussion of Gödelian independence is with regard to a truth value that is presumed to exist. What if it doesn’t exist?</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/provemewrong/" rel="attachment wp-att-19741"><img alt="" class="aligncenter wp-image-19741" height="218" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/ProveMeWrong.jpg?resize=277%2C218&amp;ssl=1" width="277"/></a></p>
<p>
We are not just catching the tension of Platonism with <a href="https://en.wikipedia.org/wiki/Logical_positivism">logical positivism</a> and related paths to asking, what is science? We are asking whether reality aligns with the position that we already feel is best for <em>practice</em>. </p>
<p>
“Emergent Mathematics” is a teaching movement that, in the words of one prominent <a href="https://www.jstor.org/stable/23434871">paper</a>, gets away from mathematics courses that “are focused on completed results that often hide the messiness and complication that led to their production.” In trying to explain much data showing that the most experienced mathematicians are not the most accomplished teachers, the paper’s two authors seem to identify the former with the position of putting emphasis on completed results. They seek the best attitude for childhood <em>learning</em> of mathematics, and believe it to be orthogonal to that of presenting finished mathematics.  But going another 90 degrees around the dial, perhaps their compass needle’s other end points to the best philosophical position for <em>creating</em> mathematics.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I think we would all still agree that the next trillion digits of <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> currently exist. The tougher question is whether it is scientifically meaningful to postulate knowledge of them, without knowing them. We may get an opinion on that from a regular friend late Wednesday into Thursday.</p></font></font></div>
    </content>
    <updated>2022-03-14T20:34:36Z</updated>
    <published>2022-03-14T20:34:36Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="P=NP"/>
    <category term="Proofs"/>
    <category term="emergence"/>
    <category term="Heiko R&#xF6;lke"/>
    <category term="Pi"/>
    <category term="Pi Day"/>
    <category term="program correctness"/>
    <category term="records"/>
    <category term="Thomas Keller"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-03-16T05:37:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://www.solipsistslog.com/?p=551</id>
    <link href="http://www.solipsistslog.com/a-mysterious-constant-called-pi-arising-from-the-gaussian-integral-with-a-minor-application-to-circles/" rel="alternate" type="text/html"/>
    <title>A mysterious constant called pi, arising from the Gaussian integral (with a minor application to circles)</title>
    <summary>Hi, nerd blog! (This is a post that I wrote a long time ago and then never published. I figured it would be nice to publish it on March 14th.) Today, we’re interested in the Gaussian integral     for . This integral of course has lots of very serious practical applications, as it arises […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Hi, nerd blog! (This is a post that I wrote a long time ago and then never published. I figured it would be nice to publish it on March 14th.)</p>



<p>Today, we’re interested in the Gaussian integral </p><p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[f(C) := \int_{-\infty}^\infty e^{-Cx^2} {\rm d} x\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-096cdf537e620c218a57694e9aca2bf4_l3.png" title="Rendered by QuickLaTeX.com" width="170"/></p> for <img alt="C &gt; 0" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-87ddd944257c5bf57009a80226e5c414_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="47"/>. This integral of course has lots of very serious practical applications, as it arises naturally in the study of the Gaussian/normal distribution. But, more importantly, it’s a lot of fun to play with and is simply beautiful. We’ll see a bit about what it makes it so pretty below. We start by simply trying to figure out the value of this thing, which isn’t super easy.<p/>



<p>By a change of variables, we immediately see that <img alt="f(C)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-051ceeda9119a5fc9be66053f9dd40a3_l3.png" title="Rendered by QuickLaTeX.com" width="37"/> is proportional to <img alt="1/\sqrt{C}" class="ql-img-inline-formula quicklatex-auto-format" height="21" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-94fd71739495e33d6c86fe0b70cdbcf1_l3.png" title="Rendered by QuickLaTeX.com" width="46"/>. But, what is the constant of proportionality? It’s actually nicer to ask a slightly different question: what is the unique value of <img alt="C" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f34f74d98915e33f37a086f8cbfb996a_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="14"/> such that <img alt="f(C) = 1" class="ql-img-inline-formula quicklatex-auto-format" height="18" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a4141e7514a6f611842effecf51c1199_l3.png" title="Rendered by QuickLaTeX.com" width="70"/>. A quick numerical computation shows that <img alt="f(3.14159) \approx 1" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-24b068172655b4e41a85afb20c734882_l3.png" title="Rendered by QuickLaTeX.com" width="114"/>. E.g., here’s some Mathematica code to find this value:<br/><img alt="" class="wp-image-552" height="110" src="http://www.solipsistslog.com/wp-content/uploads/2021/03/Screen-Shot-2021-03-27-at-3.25.48-PM.png" style="width: 400px;" width="1066"/>.</p>



<p>This constant <img alt="C \approx 3.14159" class="ql-img-inline-formula quicklatex-auto-format" height="13" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-3351e9436eef51bd249c328e212df088_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="97"/> is so important for this blog post that it is worth giving it a name. So, I looked through the Greek alphabet for a nice letter that doesn’t get used much and chose the obscure lowercase letter <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>—spelled <em>pi</em> in English, and pronounced like “pie”. In other words, by definition <img alt="f(\pi) = 1" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-64cecd17d70571e9f6e5d0e701cda533_l3.png" title="Rendered by QuickLaTeX.com" width="67"/>. (If this implicit definition bothers you, we can equivalently just define <img alt="\pi := f(1)^{2}" class="ql-img-inline-formula quicklatex-auto-format" height="20" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-fd24a2e8eab1e07787b7429db8502015_l3.png" title="Rendered by QuickLaTeX.com" width="80"/>. But, I find the implicit definition to be more elegant.)</p>



<p>So, we have this brand new mysterious constant <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>. What should we do with it? It is of course natural to try to find different expressions for it (though our integral expression can already be used to compute it to quite high precision). A first idea is to apply the change of variables <img alt="u = \pi x^2" class="ql-img-inline-formula quicklatex-auto-format" height="15" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-fcb6cf4e246632c794949355411b046b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="62"/> to obtain </p><p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[1 = 2\int_{0}^{\infty} e^{-\pi x^2}{\rm d} x = \pi^{-1/2} \int_0^{\infty} e^{-u}/u^{1/2} {\rm d} u\; .\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f10d9cf7ba99c79b8b6410cfbffbcb0d_l3.png" title="Rendered by QuickLaTeX.com" width="343"/></p> So, <p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\pi =\Big( \int_0^\infty e^{-u}/u^{1/2} {\rm d} u\Big)^2\; ,\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a70346615d75d7bd8aad3e871d66f99c_l3.png" title="Rendered by QuickLaTeX.com" width="199"/></p> which you might recognize as the square of the <a href="https://en.wikipedia.org/wiki/Gamma_function" rel="noreferrer noopener" target="_blank">Gamma function</a> evaluated at <img alt="1/2" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-872abde626b8cdf8e983a3345ee98925_l3.png" title="Rendered by QuickLaTeX.com" width="25"/>, i.e., <img alt="\pi = \Gamma(1/2)^2" class="ql-img-inline-formula quicklatex-auto-format" height="20" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f89e7a589afd5b47baf442a3c38013ee_l3.png" title="Rendered by QuickLaTeX.com" width="93"/>. (Recalling that <img alt="\Gamma(n) = (n-1)!" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-59278ef14f8421f91a9c38456784aa70_l3.png" title="Rendered by QuickLaTeX.com" width="118"/> for integer <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>, one might interpret this as saying that <img alt="\sqrt{\pi}" class="ql-img-inline-formula quicklatex-auto-format" height="18" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a0df3b7b3c88b80bd1b77220c28fb5ec_l3.png" title="Rendered by QuickLaTeX.com" width="25"/> is “the factorial of <img alt="-1/2" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b5205e45813b524be8d323d4b2d0fade_l3.png" title="Rendered by QuickLaTeX.com" width="39"/>.”) <p/>



<p>This mysterious identity will play a key role later. We could, of course, find other identities involving this new constant <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>. But, I thought instead I’d jump ahead to a rather obscure fact about the relationship between <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> and a circle.</p>



<h2>Our constant’s relationship with circles</h2>



<p>In my opinion, the Gaussian distribution is far more interesting in dimensions larger than one. In particular, consider the distribution on <img alt="\mathbb{R}^n" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f9868b4451c5811a288f7fdd10be5558_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="21"/> given by the probability density function </p><p class="ql-center-displayed-equation" style="line-height: 24px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\Pr[\mathbf{x}] = e^{-\pi \|\mathbf{x}\|^2}\; .\]" class="ql-img-displayed-equation quicklatex-auto-format" height="24" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-cbeee2e0bc6e4a6f5288818cb4aa2a7a_l3.png" title="Rendered by QuickLaTeX.com" width="129"/></p> Notice that <p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\int_{\mathbb{R}^n}e^{-\pi \|\mathbf{x}\|^2} {\rm d} \mathbf{x} = \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} e^{-\pi x_1^2 -\cdots - \pi x_n^2} {\rm d}x_1 \ldots {\rm d} x_n = 1\; ,\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8c88f8ef8350b5bf5e3924147dbed5de_l3.png" title="Rendered by QuickLaTeX.com" width="457"/></p> so that this is in fact a distribution. <p/>



<p>In fact, up to scaling, this distribution is the <em>unique</em> continuous radial product distribution—i.e., the unique distribution such that <img alt="\Pr[\mathbf{x}]" class="ql-img-inline-formula quicklatex-auto-format" height="18" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-17a91f072153c1ba54d02a280dccc979_l3.png" title="Rendered by QuickLaTeX.com" width="38"/> can be written both as a function only of the norm of <img alt="\mathbf{x}" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-bcda923e732ff6e429d93d0fa7ea8a47_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>, <img alt="\Pr[\mathbf{x}] = f^*(\|\mathbf{x}\|)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-2c1674e9adcb304d86d51e6e3e89bd93_l3.png" title="Rendered by QuickLaTeX.com" width="124"/> for some continuous function <img alt="f^*" class="ql-img-inline-formula quicklatex-auto-format" height="16" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-d1bc0c1e9f63254ef68d09d8ea3e316d_l3.png" title="Rendered by QuickLaTeX.com" width="17"/>, <em>and</em> as a product of functions of its coordinates, <img alt="\Pr[\mathbf{x}] = f_1(x_1)\cdots f_n(x_n)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-724a32e8712eb42c68f40510a4461d33_l3.png" title="Rendered by QuickLaTeX.com" width="188"/>. This makes the Gaussian a uniquely powerful tool for reducing complicated multi-dimensional problems to one-dimensional problems. </p>



<p>For example, suppose that for some strange reason we wish to know the circumference of a circle with radius one. (If we were less civilized mathematicians, we might instead set the diameter to be equal to <img alt="1" class="ql-img-inline-formula quicklatex-auto-format" height="13" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-4868771cbc422b5818f85500909ce433_l3.png" title="Rendered by QuickLaTeX.com" width="7"/>, so that the radius would be <img alt="1/2" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-872abde626b8cdf8e983a3345ee98925_l3.png" title="Rendered by QuickLaTeX.com" width="25"/>.) We can try to write this as some kind of path integral or something—and suffer quite a bit in the process—or we can use the following beautiful trick. We can write<br/></p><p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[1 = \int_{\mathbb{R}^2} e^{-\pi \|x\|^2} {\rm d} x = \int_0^\infty e^{-\pi r^2} \sigma_r {\rm d} r= \sigma_1 \int_0^\infty e^{-\pi r^2} r {\rm d} r\;,\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-7d71e3575dad757294955cdf449254fa_l3.png" title="Rendered by QuickLaTeX.com" width="430"/></p><br/>where <img alt="\sigma_r" class="ql-img-inline-formula quicklatex-auto-format" height="11" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-192e8dc0de46182a65dec93ebf8b5081_l3.png" title="Rendered by QuickLaTeX.com" width="16"/> is the circumference of a circle of with radius <img alt="r" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-c409433a9e2dfcdb83360a974d243f18_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="8"/>. (The <em>only</em> facts that we have used here are our definition of <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> together with the fact that <img alt="\sigma_r = r \sigma_1" class="ql-img-inline-formula quicklatex-auto-format" height="11" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-c8ca93d60deca29a9a89487ef2eddf50_l3.png" title="Rendered by QuickLaTeX.com" width="66"/>.) Fortunately, the last integral is easy to compute as <br/><p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\int_0^\infty e^{-\pi r^2} r {\rm d} r = \frac{1}{2\pi} \cdot \int_0^\infty e^{-u} {\rm d} u = \frac{1}{2\pi} \;.\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f7b0b0433407f63b0c9f6682c331ab63_l3.png" title="Rendered by QuickLaTeX.com" width="303"/></p> Rearranging, we see that <img alt="\sigma_1 = 2\pi" class="ql-img-inline-formula quicklatex-auto-format" height="15" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-bceb0469014bc513c98176ca8e9baf1d_l3.png" title="Rendered by QuickLaTeX.com" width="61"/>!<p/>



<p>So, surprisingly, our mysterious constant <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> is actually intimately related with the circumference of a circle. (If we were less civilized mathematicians, we might even have simply defined <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> to be the circumference of a circle with radius <img alt="1/2" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-872abde626b8cdf8e983a3345ee98925_l3.png" title="Rendered by QuickLaTeX.com" width="25"/>.)</p>



<h2>What’s so special about two dimensions? Surface area of n-spheres.</h2>



<p>But, why stop in dimension <img alt="2" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-e584dd0bab4e6c8efc164939c28db757_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="8"/>? This same <em>one neat trick</em> is just as useful in higher dimensions. E.g., what is the surface area <img alt="\sigma_1^{(n-1)}" class="ql-img-inline-formula quicklatex-auto-format" height="24" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-e14b74f2c92c5d34a31da4576bfaecc9_l3.png" title="Rendered by QuickLaTeX.com" width="46"/> of a unit sphere in <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> dimensions? (Conventionally, we write the <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>-sphere as <img alt="S^{n-1}" class="ql-img-inline-formula quicklatex-auto-format" height="15" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8897f8b728befe1f7421e387094e6fd4_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="37"/> because it as an <img alt="(n-1)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8c10e83257454bc711811cc71f964a7e_l3.png" title="Rendered by QuickLaTeX.com" width="53"/>-dimensional object that happens to be embedded in <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>-dimensional space. This is why I write <img alt="\sigma^{(n-1)}_1" class="ql-img-inline-formula quicklatex-auto-format" height="24" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-1529a8cd27bcb6dcd64708282b0f0f0f_l3.png" title="Rendered by QuickLaTeX.com" width="46"/> for its surface area.) Well, we have </p><p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[1 = \int_{-\mathbb{R}^n} e^{-\pi \|x\|^2} {\rm d} x = \int_0^\infty e^{-\pi r^2} \sigma^{(n-1)}_r {\rm d} r= \sigma^{(n-1)}_1 \int_0^\infty e^{-\pi r^2} r^{n-1} {\rm d} r\; .\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f260e2a2d6410051fb74cc87df3c7792_l3.png" title="Rendered by QuickLaTeX.com" width="531"/></p> (Again, the only property that I have used here is that <img alt="\sigma_r^{(n-1)} = r^{n-1} \sigma_1^{(n-1)}" class="ql-img-inline-formula quicklatex-auto-format" height="24" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b61ebafad59fad72671514841700ebab_l3.png" title="Rendered by QuickLaTeX.com" width="153"/>.) This integral is a bit less pretty, but using the same approach, we see that  <p class="ql-center-displayed-equation" style="line-height: 42px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\int_0^\infty e^{-\pi r^2} r^{n-1} {\rm d} r = \frac{1}{2\pi^{n/2}} \cdot \int_0^{\infty} e^{-u} u^{n/2-1} {\rm d} u = \frac{\Gamma(n/2)}{2\pi^{n/2}}\; ,\]" class="ql-img-displayed-equation quicklatex-auto-format" height="42" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-65c828363c086f1b820b730be79a4c39_l3.png" title="Rendered by QuickLaTeX.com" width="437"/></p> where the last step is literally just plugging in the definition of the Gamma function. Rearranging, we see that the surface area of the unit sphere in <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> dimensions is exactly <img alt="\frac{2\pi^{n/2}}{\Gamma(n/2)}" class="ql-img-inline-formula quicklatex-auto-format" height="30" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-50d143fa8519caff967068ce2d9de43f_l3.png" title="Rendered by QuickLaTeX.com" width="42"/>.<p/>



<p>If the Gamma function intimidates you, that’s fine. (It certainly intimidates me.) We can go a bit further by remembering that for integers <img alt="m" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-6b41df788161942c6f98604d37de8098_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="15"/>, <img alt="\Gamma(m) = (m-1)!" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-d226e3a5659b079c31cbf51beb7d6f69_l3.png" title="Rendered by QuickLaTeX.com" width="128"/>, while <img alt="\Gamma(m+1/2) = \sqrt{\pi}(2m)!/(4^m m!)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-5d2b39f9e1062dcc5fa00ec58d82eb91_l3.png" title="Rendered by QuickLaTeX.com" width="246"/>. (Both of these identities follow from the relation <img alt="\Gamma(x) = (x-1) \Gamma(x-1)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-fb2599738079fdbd25e5b92aa6f75617_l3.png" title="Rendered by QuickLaTeX.com" width="178"/>, which follows from integration by parts, together with the values <img alt="\Gamma(1) = 1" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-53c41acb4ea9695fbe3fbed4d494e73a_l3.png" title="Rendered by QuickLaTeX.com" width="65"/> and <img alt="\Gamma(1/2) = \sqrt{\pi}" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-119c76244632be47b3cddb44bcce032b_l3.png" title="Rendered by QuickLaTeX.com" width="101"/>.) </p>



<p>Then, we see that the surface area of the unit sphere in <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> dimensions is </p><p class="ql-center-displayed-equation" style="line-height: 55px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\sigma_1^{(n-1)} = \begin{cases}\pi^{n/2} \cdot \frac{2}{(n/2-1)!} &amp;n\text{ even}\\\pi^{(n-1)/2} \cdot \frac{2^n \cdot ((n-1)/2)!}{(n-1)!} &amp;n\text{ odd}.\end{cases}\]" class="ql-img-displayed-equation quicklatex-auto-format" height="55" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-7093395acfb9701717087f97c5b73145_l3.png" title="Rendered by QuickLaTeX.com" width="318"/></p> In particular, from this formula, we immediately see the perhaps surprising fact that the surface area of the sphere in <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> dimensions rapidly approaches <img alt="0" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a5e437be25f29374d30f66cd46adf81c_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="9"/> as <img alt="n \to \infty" class="ql-img-inline-formula quicklatex-auto-format" height="10" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-032ad3d6327a419ab33b269b23c31b7c_l3.png" title="Rendered by QuickLaTeX.com" width="55"/>. (I.e., “<img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>-dimensional unit spheres are tiny.”) We also see the rather strange fact that <img alt="\sigma_1^{(n-1)}" class="ql-img-inline-formula quicklatex-auto-format" height="24" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-e14b74f2c92c5d34a31da4576bfaecc9_l3.png" title="Rendered by QuickLaTeX.com" width="46"/> is a rational multiple of <img alt="\pi^{\lfloor n/2 \rfloor}" class="ql-img-inline-formula quicklatex-auto-format" height="17" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-362edd7540e9e43db2e65a0ddc45e684_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="43"/>.<p/>



<p>We can also plug in low values of <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> to see what we get. E.g., I have heard that some people are interested in the case when <img alt="n = 2" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a47a3377f14fefc160d10b089a4aab45_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="42"/> and <img alt="n = 3" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8eda9fc1983d40517cca42fa671a0f51_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="43"/>. Plugging in, one sees that the circumference of a circle with radius one is <img alt="2\pi" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-5bfa2124624f767670227d1aeab8d85c_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="20"/> (which, ok, we already saw before), and that the surface area of a sphere with radius one is <img alt="4\pi" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a64f86508ea52835b7fd42736282275d_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="20"/>. But, we can easily go farther: the surface area in four dimensions is <img alt="2\pi^2" class="ql-img-inline-formula quicklatex-auto-format" height="15" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-7249ec3f5fb52ad8207e0f9d873c7a4f_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="27"/>, and in five dimensions, it is <img alt="8\pi^{2}/3" class="ql-img-inline-formula quicklatex-auto-format" height="20" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-5b263bf1209b153f5824b853138f0510_l3.png" title="Rendered by QuickLaTeX.com" width="45"/>.</p>



<p>And, we can of course find the volume of the unit <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>-ball by computing a simple integral </p><p class="ql-center-displayed-equation" style="line-height: 44px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[V^{(n)} = \int_0^{1}\sigma_r^{(n)} {\rm} d r = \sigma_1^{(n)} \cdot \int_0^{1} r^{n-1} {\rm d} r = \sigma_1^{(n)}/n \; .\]" class="ql-img-displayed-equation quicklatex-auto-format" height="44" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-08b8932dcb424209e24a111106e869b2_l3.png" title="Rendered by QuickLaTeX.com" width="366"/></p><p/>



<p>In short, I think this mysterious constant <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> is rather nice. Perhaps it will find other applications.</p></div>
    </content>
    <updated>2022-03-14T14:34:24Z</updated>
    <published>2022-03-14T14:34:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Noah Stephens-Davidowitz</name>
    </author>
    <source>
      <id>http://www.solipsistslog.com</id>
      <link href="http://www.solipsistslog.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="http://www.solipsistslog.com" rel="alternate" type="text/html"/>
      <subtitle>Life inside the confused and curious mind of a nerd.</subtitle>
      <title>Solipsist's Log</title>
      <updated>2022-03-15T22:46:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/039</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/039" rel="alternate" type="text/html"/>
    <title>TR22-039 |  Parallel Repetition For All 3-Player Games Over Binary Alphabet | 

	Kunal Mittal, 

	Uma Girish, 

	Justin Holmgren, 

	Wei Zhan, 

	Ran Raz</title>
    <summary>We prove that for every 3-player (3-prover) game, with binary questions and answers and value less than $1$, the value of the $n$-fold parallel repetition of the game decays polynomially fast to $0$. That is, for every such game, there exists a constant $c&gt;0$, such that the value of the $n$-fold parallel repetition of the game is at most $n^{-c}$.

Along the way to proving this theorem, we prove two additional parallel repetition theorems for multiplayer (multiprover) games, that may be of independent interest:

$\textbf{Playerwise Connected Games (with any number of players and any Alphabet size):}$ We identify a large class of multiplayer games and prove that for every game with value less than $1$ in that class, the value of the $n$-fold parallel repetition of the game decays polynomially fast to $0$.

More precisely, our result applies for $\textit{playerwise connected games}$, with any number of players and any alphabet size:
For each player $i$, we define the graph $G_i$, whose vertices are the possible questions for that player and two questions $x,x'$ are connected by an edge if there exists a vector $y$ of questions for all other players, such that both $(x,y)$ and $(x',y)$ are asked by the referee with non-zero probability. We say that the game is $\textit{playerwise connected}$ if for every $i$, the graph $G_i$ is connected.

Our class of playerwise connected games is strictly larger than the class of connected games that was defined in [DHVY17] and for which exponentially fast decay bounds are known [DHVY17]. For playerwise connected games that are not connected, only inverse Ackermann decay bounds were previously known [Ver96].

$\textbf{Exponential Bounds for the Anti-Correlation Game:}$ In the 3-player $\textit{anti-correlation game}$, two out of three players are given $1$ as input, and the remaining player is given $0$. The two players who were given $1$ must produce different outputs in $\{0,1\}$. We prove that the value of the $n$-fold parallel repetition of that game decays exponentially fast to $0$. That is, there exists a constant $c&gt;0$, such that the value of the $n$-fold parallel repetition of the game is at most $2^{-cn}$. Only inverse Ackermann decay bounds were previously known [Ver96].

The 3-player anti-correlation game was studied and motivated in several previous works. In particular, Holmgren and Yang gave it as an example for a 3-player game whose non-signaling value (is smaller than $1$ and yet) does not decrease at all under parallel repetition [HY19].</summary>
    <updated>2022-03-14T05:14:56Z</updated>
    <published>2022-03-14T05:14:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-03-16T05:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/038</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/038" rel="alternate" type="text/html"/>
    <title>TR22-038 |  Lower bounds for Polynomial Calculus with extension variables over finite fields | 

	Sasank Mouli, 

	Russell Impagliazzo, 

	Toniann Pitassi</title>
    <summary>For every prime p &gt; 0, every n &gt; 0 and ? = O(logn), we show the existence
of an unsatisfiable system of polynomial equations over O(n log n) variables of degree O(log n) such that any Polynomial Calculus refutation over F_p with M extension variables, each depending on at most ? original variables requires size exp(?????(n2/(?^2*2^?*(M + ????nlog(n))))) .</summary>
    <updated>2022-03-13T20:03:44Z</updated>
    <published>2022-03-13T20:03:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-03-16T05:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/13/ukraine-student-senior-research-fellows-at-tel-aviv-university-apply-by-june-1-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/13/ukraine-student-senior-research-fellows-at-tel-aviv-university-apply-by-june-1-2022/" rel="alternate" type="text/html"/>
    <title>Ukraine student / senior research fellows at Tel Aviv University (apply by June 1, 2022)</title>
    <summary>Tel Aviv University offers emergency scholarship for research students from Ukraine (see attached). Further, I (Gil Cohen) have immediately available senior / students research fellows in theoretical computer science, coding theory, spectral graph theory, and adjacent mathematical branches. Website: https://c1f423b8-ee8e-41b1-a3a7-2cfc865115ec.filesusr.com/ugd/d112fa_4de343bf5b3a410eae40b3853dcef087.pdf Email: coheng@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Tel Aviv University offers emergency scholarship for research students from Ukraine (see attached). Further, I (Gil Cohen) have immediately available senior / students research fellows in theoretical computer science, coding theory, spectral graph theory, and adjacent mathematical branches.</p>
<p>Website: <a href="https://c1f423b8-ee8e-41b1-a3a7-2cfc865115ec.filesusr.com/ugd/d112fa_4de343bf5b3a410eae40b3853dcef087.pdf">https://c1f423b8-ee8e-41b1-a3a7-2cfc865115ec.filesusr.com/ugd/d112fa_4de343bf5b3a410eae40b3853dcef087.pdf</a><br/>
Email: coheng@gmail.com</p></div>
    </content>
    <updated>2022-03-13T11:10:32Z</updated>
    <published>2022-03-13T11:10:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-03-16T05:37:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-527025314741039880</id>
    <link href="http://processalgebra.blogspot.com/feeds/527025314741039880/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=527025314741039880" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/527025314741039880" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/527025314741039880" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2022/03/focs-2021-test-of-time-award-winners.html" rel="alternate" type="text/html"/>
    <title>FOCS 2021 Test-of-Time Award winners (and one deserving paper that missed out)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As members of the TCS community will most likely know, FOCS established<a href="https://focs2021.cs.colorado.edu/test-of-time-awards/" target="_blank"> Test-of-Time Awards from its 2021 edition</a> to celebrate contributions published at that conference 30, 20 and 10 years before. The first list of selected winners is, as one might have expected, stellar: <br/></p><ul style="text-align: left;"><li>Uriel Feige, Shafi Goldwasser, László Lovász, Shmuel Safra, Mario Szegedy:<br/>Approximating Clique is Almost NP-Complete.<br/>FOCS 1991</li><li>David Zuckerman:<br/>Simulating BPP Using a General Weak Random Source.<br/>FOCS 1991</li><li>Serge A. Plotkin, David B. Shmoys, Éva Tardos:<br/>Fast Approximation Algorithms for Fractional Packing and Covering Problems.<br/>FOCS 1991</li><li>Ran Canetti:<br/>Universally Composable Security: A New Paradigm for Cryptographic Protocols.<br/>FOCS 2001</li><li>Boaz Barak:<br/>How to Go Beyond the Black-Box Simulation Barrier.<br/>FOCS 2001</li><li>Amit Chakrabarti, Yaoyun Shi, Anthony Wirth, Andrew Chi-Chih Yao:<br/>Informational Complexity and the Direct Sum Problem for Simultaneous Message Complexity.<br/>FOCS 2001</li><li>Zvika Brakerski, Vinod Vaikuntanathan:<br/>Efficient Fully Homomorphic Encryption from (Standard) LWE.<br/>FOCS 2011</li></ul><p>FWIW, I offer my belated congratulations to all the award recipients, whose work has had, and continues to have, a profound influence on the "Volume A" TCS community. </p><p>Apart from celebrating their achievement, the purpose of this post is to highlight a paper from FOCS 1991 that missed out on the Test-of-Time Award, but that, IMHO, would have fully deserved it. </p><p>I am fully aware that the number of deserving papers/scientists is typically larger, if not much larger, than the number of available awards. Awards are a scarce resource! My goal with this post is simply to remind our community (and especially its younger members) of a seminal contribution that they might want to read or re-read. <br/></p><p>The paper in question is "<a href="https://www.cs.cornell.edu/courses/cs6860/2019sp/Handouts/EmersonJutla91.pdf" target="_blank">Tree automata, mu-calculus and determinacy</a>" by <a href="https://www.cs.utexas.edu/~emerson/" target="_blank">Allen Emerson</a> and <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-csjutla" target="_blank">Charanjit S. Jutla</a>, which appeared at FOCS 1991. (Emerson shared the 2007 A.M. Turing Award for the invention of model checking and Jutla went on to doing path-breaking work in cryptography.) That paper is absolutely fundamental for the mu-calculus, but also for automata theory, and verification in general. It introduced many ideas and results that became the basis for extensive research. </p><p>As a first contribution, the article introduced <a href="https://en.wikipedia.org/wiki/Parity_game" target="_blank">parity games</a> and proved their fundamental properties. The parity condition was a missing link in automata theory on infinite objects. It made the whole theory much simpler than that proposed in earlier work.  Technically, the parity condition is both universal and positional. Universal means that tree automata with parity conditions are as expressive as those with Rabin or Muller conditions. Positional means that in the acceptance game if a player has a winning strategy then she has one depending only on the current position and not on the history of the play so far. This is a huge technical advance for all automata-theoretic constructions and for the analysis of  infinite-duration games. It allows one, for instance,  to avoid the complicated arguments of Gurevich and Harlington in <a href="https://doi.org/10.1145/800070.802177" target="_blank">their seminal STOC 1982 article</a>, which were already a huge simplification of <a href="https://www.ams.org/journals/tran/1969-141-00/S0002-9947-1969-0246760-1/S0002-9947-1969-0246760-1.pdf" target="_blank">Rabin's original argument from 1969</a> proving the decidability of the <a href="https://en.wikipedia.org/wiki/Monadic_second-order_logic#Decidability_and_complexity_of_satisfiability" target="_blank">monadic second-order theory of the infinite binary tree</a> and much more. In passing, let me remark that Rabin has gone on record saying that "I consider this to be the most difficult research I have ever done." See <a href="https://cacm.acm.org/magazines/2010/2/69370-an-interview-with-michael-rabin/fulltext" target="_blank">this interview</a> in CACM. </p><p>The second main contribution of that paper is the discovery of the relation between parity games and the mu-calculus. The authors show how a mu-calculus model-checking problem can be reduced to solving a parity game, and conversely, how the set of winning positions in a parity game can be described by a mu-calculus formula. This result is the birth of the "model-checking via games" approach. It also shows that establishing a winner in parity games is contained both in NP and in co-NP. As a corollary, the model-checking problem is as complex as solving games. It is still not known if the problem is in PTIME. A <a href="https://doi.org/10.1145/3055399.3055409" target="_blank">recent advance from STOC'17</a> gives a quasi-polynomial-time algorithm. (See <a href="https://blog.computationalcomplexity.org/2017/03/parity-games-in-quasipolynomial-time.html" target="_blank">this blog post</a> for a discussion of that result, which received the STOC 2017 best paper award and was immediately followed up by a flurry of related papers.) </p><p>Finally, the paper also shows how to prove Rabin's complementation lemma, which is the most difficult step in his celebrated aforementioned decidability result, with the help of parity conditions. The proof is radically simpler than previous approaches. The paper puts this contribution most prominently, but actually the conceptual and technical contributions presented later in the paper turned out to be most important for the community. </p><p>Overall, the above-mentioned paper by Emerson and Jutla is a truly seminal contribution that has stood the test of time, has sown the seeds for much research  over the last thirty years (as partly witnessed by the over 1,130 citations it has received so far) and is still stimulating advances at the cutting edge of theoretical computer science that bridge the Volume A-Volume B divide. </p><p>I encourage everyone to read it!</p><p><b>Acknowledgement:</b> I have learnt much of the content of this post from Igor Walukiewicz. The responsibility for any infelicity is mine alone. <br/></p></div>
    </content>
    <updated>2022-03-12T12:24:00Z</updated>
    <published>2022-03-12T12:24:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2022-03-12T20:51:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1362</id>
    <link href="https://thmatters.wordpress.com/2022/03/11/call-for-nominations-knuth-prize/" rel="alternate" type="text/html"/>
    <title>Call for nominations: Knuth Prize</title>
    <summary>Deadline: March 31, 2022. The Donald E. Knuth Prize for outstanding contributions to the foundations of computer science is awarded for major research accomplishments and contributions to the foundations of computer science over an extended period of time. The Prize is awarded annually by the ACMSpecial Interest Group on Algorithms and Computation Theory (SIGACT) and the IEEETechnical Committee […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Deadline: March 31, 2022.</strong></p>



<p>The Donald E. Knuth Prize for outstanding contributions to the foundations of computer science is awarded for major research accomplishments and contributions to the foundations of computer science over an extended period of time. The Prize is awarded annually by the <a href="https://urldefense.com/v3/__https://acm.org/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-Ykc0qrYCK$" rel="noreferrer noopener" target="_blank">ACM</a><a href="https://urldefense.com/v3/__https://www.sigact.org/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-YkRfc620V$" rel="noreferrer noopener" target="_blank">Special Interest Group on Algorithms and Computation Theory</a> (SIGACT) and the <a href="https://urldefense.com/v3/__https://www.ieee.org/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-YkSouJjK-$" rel="noreferrer noopener" target="_blank">IEEE</a><a href="https://urldefense.com/v3/__https://tc.computer.org/tcmf/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-YkcMUrupP$" rel="noreferrer noopener" target="_blank">Technical Committee on the Mathematical Foundations of Computing</a> (TCMF).</p>



<p><strong>Nomination Procedure.</strong> Anyone in the Theoretical Computer Science community may nominate a candidate. To do so, please send nominations to <strong><a rel="noreferrer noopener" target="_blank">knuth.prize.2022@gmail.com</a></strong> by <strong>March 31, 2022</strong>. The nomination should state the nominee’s name, summarize their contributions in one or two pages, provide a CV for the nominee or a pointer to the nominee’s web page, and give telephone and email contact information for the nominator. Any supporting letters from other members of the community (up to a limit of 5) should be included in the package that the nominator submits. Supporting letters should contain substantial information not in the nomination. Others may endorse the nomination simply by adding their names to the nomination letter. If you have nominated a candidate in past years, you can re-nominate the candidate by sending a message to that effect to the above email address. (You may revise the nominating materials if you so desire.)</p>



<p><strong>Criteria for Selection.</strong> The winner is selected by a Prize Committee consisting of six people appointed by the SIGACT and TCMF Chairs, see below for the composition of the committee.</p>



<p>Previous nominations made or updated in the last 5 years will be considered. Older nominations must be updated for consideration. Note that the Knuth Prize is awarded to a single individual each year. Nominations of groups of researchers will not be considered.</p>



<p>In selecting the Knuth Prize winner, the Committee pays particular attention to a <em>sustained record</em> of high-impact, seminal contributions to the foundations of computer science. The selection may also be based partly on educational accomplishments and contributions such as fundamental textbooks and high-quality students. The award is not given for service to the theoretical computer science community, but service may be included in the citation for a winner if appropriate.</p>



<p>The 2022 prize committee consists of Harold Gabow (U. Colorado), Monika Henzinger (U. Vienna), Kurt Mehlhorn (Max Planck Institute), Dana Randall (Chair, Georgia Tech), Madhu Sudan (Harvard U.), and Andy Yao (Tsinghua U.).</p></div>
    </content>
    <updated>2022-03-11T20:49:39Z</updated>
    <published>2022-03-11T20:49:39Z</published>
    <category term="awards"/>
    <category term="Deadlines"/>
    <category term="TCS Community"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2022-03-16T05:37:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1357</id>
    <link href="https://thmatters.wordpress.com/2022/03/11/call-for-nominations-godel-prize/" rel="alternate" type="text/html"/>
    <title>Call for nominations: Godel Prize</title>
    <summary>Deadline for nominations extended to March 31st 2022. https://www.sigact.org/prizes/g%C3%B6del.html The Gödel Prize for outstanding papers in the area of theoretical computer science is sponsored jointly by the European Association for Theoretical Computer Science (EATCS) and the Special Interest Group on Algorithms and Computation Theory of the Association for Computing Machinery (ACM SIGACT). This award is presented annually, with the presentation taking place alternately […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Deadline for nominations extended to March 31st 2022.</strong></p>



<p><a href="https://urldefense.com/v3/__https://www.sigact.org/prizes/g**Adel.html__;w7Y!!IBzWLUs!EguPGLYWUarkIsJJaLrhDzTZKGjc97LX_FB94NzWMobXbrXlbTKzzky4VPAE-mxj$" rel="noreferrer noopener" target="_blank">https://www.sigact.org/prizes/g%C3%B6del.html</a></p>



<p>The Gödel Prize for outstanding papers in the area of theoretical computer science is sponsored jointly by the European Association for Theoretical Computer Science (EATCS) and the Special Interest Group on Algorithms and Computation Theory of the Association for Computing Machinery (ACM SIGACT). This award is presented annually, with the presentation taking place alternately at the International Colloquium on Automata, Languages, and Programming (ICALP) and the ACM Symposium on Theory of Computing (STOC). The thirtieth Gödel Prize will be awarded at the forty-ninth International Colloquium on Automata, Languages and Programming (ICALP), which will be hybrid, happening both physically and virtually. The physical meeting will take place in Paris, France, July 4–8 2022.</p>



<p>The Prize is named in honor of Kurt Gödel in recognition of his major contributions to mathematical logic and of his interest, discovered in a letter he wrote to John von Neumann shortly before von Neumann’s death, in what has become the famous “P versus NP” question. The Prize includes an award of USD 5,000.</p>



<p><strong>Award Committee</strong></p>



<p>The 2022 Award Committee consists of Samson Abramsky (Chair, University College London), Nikhil Bansal (University of Michigan), Irit Dinur (Weizmann Institute), Anca Muscholl (University of Bordeaux), Ronitt Rubinfeld (Massachusetts Institute of Technology), and David Zuckerman (University of Texas at Austin).</p>



<p><strong>Eligibility</strong></p>



<p>The 2022 Prize rules are given below and they supersede any different interpretation of the generic rule to be found on websites of both SIGACT and EATCS. Any research paper or series of papers by a single author or by a team of authors is deemed eligible if:</p>



<p>• The main results were not published (in either preliminary or final form) in a journal or conference proceedings before January 1, 2009.</p>



<p>• The paper was published in a recognized refereed journal no later than December 31, 2021.<br/>The research work nominated for the award should be in the area of theoretical computer science. Nominations are encouraged from the broadest spectrum of the theoretical computer science community so as to ensure that potential award winning papers are not overlooked. The Award Committee shall have the ultimate authority to decide whether a particular paper is eligible for the Prize.</p>



<p><strong>Nominations</strong></p>



<p>Nominations for the award should be submitted by email to the Award Committee Chair: <a rel="noreferrer noopener" target="_blank">s.abramsky@ucl.ac.uk</a>. Please make sure that the Subject line of all nominations and related messages begin with “Goedel Prize 2022”. To be considered, nominations for the 2022 Prize must be received by March 31, 2022.</p>



<p>A nomination package should include:</p>



<p>• A printable copy (or copies) of the journal paper(s) being nominated, together with a complete citation (or citations) thereof.</p>



<p>• A statement of the date(s) and venue(s) of the first conference or workshop publication(s) of the nominated work(s) or a statement that no such publication has occurred.</p>



<p>• A brief summary of the technical content of the paper(s) and a brief explanation of its significance.</p>



<p>• A support letter or letters signed by at least two members of the scientific community.<br/>Additional support letters may also be received and are generally useful. The nominated paper(s) may be in any language. However, if a nominated publication is not in English, the nomination package must include an extended summary written in English.</p>



<p>Those intending to submit a nomination should contact the Award Committee Chair by email well in advance. The Chair will answer questions about eligibility, encourage coordination among different nominators for the same paper(s), and also accept informal proposals of potential nominees or tentative offers to prepare formal nominations. The committee maintains a database of past nominations for eligible papers, but fresh nominations for the same papers (especially if they highlight new evidence of impact) are always welcome.</p></div>
    </content>
    <updated>2022-03-11T20:47:31Z</updated>
    <published>2022-03-11T20:47:31Z</published>
    <category term="awards"/>
    <category term="Deadlines"/>
    <category term="TCS Community"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2022-03-16T05:37:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/11/postdoc-at-utrecht-university-apply-by-april-4-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/11/postdoc-at-utrecht-university-apply-by-april-4-2022/" rel="alternate" type="text/html"/>
    <title>Postdoc at Utrecht University (apply by April 4, 2022)</title>
    <summary>The ERC starting grant project “Finding Cracks in the Wall of NP-completeness” of PI Jesper Nederlof aims to improve classical algorithms for NP-hard problems. For example: Can the Bellman-Held-Karp dynamic programming algorithm from the 1960’s that solves TSP with n cities in 2^n time be improved to 1.9999^n time? Join the project as a postdoc […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The ERC starting grant project “Finding Cracks in the Wall of NP-completeness” of PI Jesper Nederlof aims to improve classical algorithms for NP-hard problems. For example: Can the Bellman-Held-Karp dynamic programming algorithm from the 1960’s that solves TSP with n cities in 2^n time be improved to 1.9999^n time? Join the project as a postdoc now!</p>
<p>Website: <a href="https://www.uu.nl/en/organisation/working-at-utrecht-university/jobs/postdoc-position-in-algorithmic-theory-10-fte">https://www.uu.nl/en/organisation/working-at-utrecht-university/jobs/postdoc-position-in-algorithmic-theory-10-fte</a><br/>
Email: j.nederlof@uu.nl</p></div>
    </content>
    <updated>2022-03-11T19:49:33Z</updated>
    <published>2022-03-11T19:49:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-03-16T05:37:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1634</id>
    <link href="https://ptreview.sublinear.info/2022/03/news-for-february-2022/" rel="alternate" type="text/html"/>
    <title>News for February 2022</title>
    <summary>This month has seen a flurry of activity in sublinear algorithms and a diverse collection of papers have come up, with topics ranging from differentially private sublinear algorithms to local testers for multiplicity codes. Apologies to the readers for the delay in putting this post together! Almost-Optimal Sublinear-Time Edit Distance in the Low Distance Regime […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This month has seen a flurry of activity in sublinear algorithms and a diverse collection of papers have come up, with topics ranging from differentially private sublinear algorithms to local testers for multiplicity codes. Apologies to the readers for the delay in putting this post together!</p>



<p><strong>Almost-Optimal Sublinear-Time Edit Distance in the Low Distance Regime</strong> by Karl Bringmann, Alejandro Cassis, Nick Fischer and Vasileios Nakos (<a href="https://arxiv.org/abs/2202.08066">arXiv</a>)</p>



<p>This paper considers the problem of gap edit distance, i.e., of determining if the edit distance between two strings \(x\) and \(y\) is at most \(k\) or at least \(K\). Their main result is an algorithm that runs in time \(O(n/k + \text{poly}(k))\) and solves the problem for \(K =  k \cdot 2^{\tilde{O}(\sqrt{\log k})}\). The paper improves upon earlier results of Goldenberg, Krauthgamer and Saha (2019) and Kociumaka and Saha (2020) who solved the problem for \(K = k^2\) with the same asymptotic guarantee on the query complexity. </p>



<p>One of the interesting takeaways from the paper is that the complexity of solving the gap Hamming distance and gap edit distance are similar in the low distance regime. For both, the complexity of solving the \((k, k^{1+o(1)})\)-gap problem is \(n/k^{1 \pm o(1)}\). This needs to be contrasted with the fact that solving \((k, \Omega(n))\)-gap edit distance requires \(\Omega(\sqrt{k})\) queries as shown by Batu, Ergun, Kilian, Magen and Raskhodnikova (2003), whereas \((k, \Omega(n))\)-gap Hamming distance can be solved in \(O(1)\) time. </p>



<p>These results are incomparable to those obtained by Goldenberg, Kociumaka, Krauthgamer and Saha (which was discussed in our November 2021 post), where they give a nonadaptive algorithm with complexity \(O(n/k^{1.5})\) for \((k, k^2)\)-gap edit distance problem. The algorithm in the present paper is adaptive and works faster for smaller values of \(k\).</p>



<p/>



<p><strong>Privately Estimating Graph Parameters in Sublinear time</strong> by Jeremiah Blocki, Elena Grigorescu, Tamalika Mukherjee (<a href="https://arxiv.org/abs/2202.05776">arXiv</a>)</p>



<p>Differentially private approximation algorithms for optimization problems on graphs is a well-studied topic. This paper opens up an exciting research direction by initiating a systematic study of the design of differentially private sublinear-time algorithms. The setting is that graphs are viewed as databases and two graphs are neighboring if they differ in an edge (or a node). An algorithm \(A\) is \(\epsilon\)-differentially private if for every pair of edge-neighboring(or node-neighboring) graphs \(G, G’\) and for every subset \(S\) of outputs, \(\Pr[A(G) \in S] \leq \exp(\epsilon) \cdot \Pr[A(G’) \in S]\).</p>



<p>The paper presents \(\epsilon\)-differentially private sublinear-time algorithms for well-studied problems such as estimating the average degree, the size of a min vertex cover and the size of a maximum matching. These algorithms access the input graphs via <em>neighbor queries</em> and <em>degree queries</em>. </p>



<p>In addition to providing a strong privacy guarantee, their algorithms nearly match the approximation and complexity guarantees of their non-differentially private counterparts. The main idea seems to be the formalization of a sensitivity notion, which they refer to as Global Coupled Sensitivity, and bounding it for the known sublinear-time algorithms for the aforementioned problems. Finally, they add Laplace noise calibrated with this sensitivity value to the output of the algorithms to make them differentially private.  </p>



<p/>



<p><strong>Testability and Local Certification of Monotone Properties in Minor-closed Classes</strong> by Louis Esperet And Sergey Norin (<a href="https://arxiv.org/abs/2202.00543">arXiv</a>)</p>



<p>One of the major interests in graph property testing is to characterize which properties are testable, i.e, can be \(\epsilon\)-tested with query complexity that depends only on the parameter \(\epsilon\). The question of testability is well-understood in the dense graph model as well as the bounded degree model. This paper concerns itself with testability questions in the general model or the sparse model of graph property testing, where graphs are represented as adjacency lists with no bound on the maximum degree. </p>



<p>The authors prove that every monotone property of<strong> </strong>minor-closed graph classes is testable with one-sided error, where a property is monotone if it is closed under taking subgraphs and a graph class is minor-closed if it is closed under taking minors. A crucial fact to be noted here is that a tester is allowed to make only uniformly random nerighbor queries. </p>



<p>This result is a significant generalization of a 2019 result by Czumaj and Sohler, who proved that for every finite set of graphs \(\mathcal{H}\), every \(\mathcal{H}\)-free property of minor-closed graph classes is testable with one-sided error, where a graph satisfies \(\mathcal{H}\)-freeness if none of its subgraphs belong to \(\mathcal{H}\). </p>



<p>They show an interesting consequence of their results to designing a short local certification scheme for monotone properties of minor-closed graph classes. Roughly speaking, they show the existence of a prover-verifier system for the aforementioned testing problem where proofs of length \(O(\log n)\) are assigned to each vertex and that verifier needs to observe only the proofs assigned to a vertex and its neighbors.<br/></p>



<p><strong>The plane test is a local tester for Multiplicity Codes</strong> by Dan Karliner, Roie Salama, and Amnon Ta-Shma (<a href="https://eccc.weizmann.ac.il/report/2022/028/">ECCC</a>)</p>



<p>Multiplicity codes are a generalization of Reed-Muller codes and was first studied by Guruswami and Wang (2013) and Kopparty, Saraf and Yekhanin (2014). The messages here are polynomials of degree \(d\) over \(m\) variables and the codeword corresponding to a polynomial \(p\) is the evaluation of \(p\) and of all of its directional derivatives of order upto \(s\) over all the points in \(\mathbb{F}_q^m\), where \(q\) is a prime power. </p>



<p>Even though multiplicity codes are known to be locally decodable, it was open whether they are locally testable. Local testers for Reed Muller codes work by restricting the evaluations to a uniformly random line in \(\mathbb{F}_q^m\) and checking whether it corresponds to the evaluations of a degree \(d\) univariate polynomial. The authors first show that such a tester does not work for the case of multiplicity codes when \(d\) is large. They then show that a <em>plane test</em> is a good local tester for multiplicity codes even for larger values of \(d\). Specifically, a plane test checks whether the restriction of a given word, which is purportedly the evaluation of a polynomial of degree \(d\) and of its derivatives, to a uniformly random plane in \(\mathbb{F}_q^m\) is a bivariate multiplicity code of degree \(d\). </p>



<p/>



<p>We conclude the post with a short note from Nader Bshouty and Oded Goldreich on a fundamental characterization result in property testing. </p>



<p><strong>On properties that are non-trivial to test</strong> by Nader H. Bshouty and Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2022/013/">ECCC</a>)</p>



<p>A property on binary strings is nontrivial if for infinitely many \(n\), the property contains at least one string of length \(n\) and at most \(2^{n – \Omega(n)}\) strings of length \(n\). The note shows that every nontrivial property requires \(\Omega(1/\epsilon)\) queries to \(\epsilon\)-test. </p></div>
    </content>
    <updated>2022-03-11T04:09:38Z</updated>
    <published>2022-03-11T04:09:38Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Nithin Varma</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2022-03-15T22:46:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/10/postdoc-and-phd-student-in-graph-algorithms-at-tel-aviv-university-apply-by-april-15-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/10/postdoc-and-phd-student-in-graph-algorithms-at-tel-aviv-university-apply-by-april-15-2022/" rel="alternate" type="text/html"/>
    <title>Postdoc and PhD student in graph algorithms at Tel Aviv University (apply by April 15, 2022)</title>
    <summary>Applications are invited for postdoc and PhD positions at the group of Shay Solomon, Tel Aviv University, funded by an ERC starting grant. The selected candidates will confront fundamental problems in graph algorithms. Applications will be accepted until the positions are filled (flexible start date). To apply, send a CV and research statement, and arrange […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are invited for postdoc and PhD positions at the group of Shay Solomon, Tel Aviv University, funded by an ERC starting grant.<br/>
The selected candidates will confront fundamental problems in graph algorithms. Applications will be accepted until the positions are filled (flexible start date).<br/>
To apply, send a CV and research statement, and arrange for three letters of recommendation.</p>
<p>Website: <a href="https://sites.google.com/site/soloshay/">https://sites.google.com/site/soloshay/</a><br/>
Email: shayso@tauex.tau.ac.il</p></div>
    </content>
    <updated>2022-03-10T21:00:32Z</updated>
    <published>2022-03-10T21:00:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-03-16T05:37:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/10/differential-privacy-research-scientist-at-harvard-university-apply-by-april-30-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/10/differential-privacy-research-scientist-at-harvard-university-apply-by-april-30-2022/" rel="alternate" type="text/html"/>
    <title>Differential Privacy Research Scientist at Harvard University (apply by April 30, 2022)</title>
    <summary>The OpenDP project at Harvard University seeks to hire a Research Scientist to work with faculty directors Gary King and Salil Vadhan and the OpenDP Community to formulate and advance the scientific goals of OpenDP and solve research problems that are needed for its success. Website: https://academicpositions.harvard.edu/postings/11093 Email: opendp@g.harvard.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The OpenDP project at Harvard University seeks to hire a Research Scientist to work with faculty directors Gary King and Salil Vadhan and the OpenDP Community to formulate and advance the scientific goals of OpenDP and solve research problems that are needed for its success.</p>
<p>Website: <a href="https://academicpositions.harvard.edu/postings/11093">https://academicpositions.harvard.edu/postings/11093</a><br/>
Email: opendp@g.harvard.edu</p></div>
    </content>
    <updated>2022-03-10T20:56:55Z</updated>
    <published>2022-03-10T20:56:55Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-03-16T05:37:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/037</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/037" rel="alternate" type="text/html"/>
    <title>TR22-037 |  Robust Radical Sylvester-Gallai Theorem for Quadratics | 

	Rafael Mendes de Oliveira, 

	Abhibhav Garg, 

	Akash Sengupta</title>
    <summary>We prove a robust generalization of a Sylvester-Gallai type theorem for quadratic polynomials, generalizing the result in [S'20].
More precisely, given a parameter $0 &lt; \delta \leq 1$ and a finite collection $\mathcal{F}$ of irreducible and pairwise independent polynomials of degree at most 2, we say that $\mathcal{F}$ is a $(\delta, 2)$-radical Sylvester-Gallai configuration if for any polynomial $F_i \in \mathcal{F}$, there exist $\delta(|\mathcal{F}| -1)$ polynomials $F_j$ such that $|\mathrm{rad}(F_i, F_j) \cap \mathcal{F}| \geq 3$, that is, the radical of $F_i, F_j$ contains a third polynomial in the set.

In this work, we prove that any $(\delta, 2)$-radical Sylvester-Gallai configuration $\mathcal{F}$ must be of low dimension: that is 
$$\dim \mathrm{span}(\mathcal{F}) = \mathrm{poly}(1/\delta).$$</summary>
    <updated>2022-03-10T18:30:53Z</updated>
    <published>2022-03-10T18:30:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-03-16T05:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/10/visiting-position-at-williams-college-apply-by-march-28-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/10/visiting-position-at-williams-college-apply-by-march-28-2022/" rel="alternate" type="text/html"/>
    <title>Visiting Position at Williams College (apply by March 28, 2022)</title>
    <summary>The Department of Computer Science at Williams College invites applications for a one-year visiting faculty position beginning in the fall of 2022. Candidates should have a commitment to excellence in teaching and should have a Ph.D., or made significant progress towards completing a Ph.D., in computer science or a closely related discipline by September 2022. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at Williams College invites applications for a one-year visiting faculty position beginning in the fall of 2022. Candidates should have a commitment to excellence in teaching and should have a Ph.D., or made significant progress towards completing a Ph.D., in computer science or a closely related discipline by September 2022.</p>
<p>Website: <a href="https://apply.interfolio.com/103820">https://apply.interfolio.com/103820</a><br/>
Email: hiring@cs.williams.edu</p></div>
    </content>
    <updated>2022-03-10T14:57:32Z</updated>
    <published>2022-03-10T14:57:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-03-16T05:37:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/036</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/036" rel="alternate" type="text/html"/>
    <title>TR22-036 |  Classes of Hard Formulas for QBF Resolution | 

	Agnes Schleitzer, 

	Olaf Beyersdorff</title>
    <summary>To date, we know only a few handcrafted quantified Boolean formulas (QBFs) that are hard for central QBF resolution systems such as Q and QU, and only one specific QBF family to separate Q and QU. 

Here we provide a general method to construct hard formulas for Q and  QU. The construction uses simple propositional formulas (e.g. minimally unsatisfiable formulas) in combination with easy QBF gadgets (Sigma_2^b formulas without constant winning strategies). 
This leads to a host of new hard formulas, including new classes of hard random QBFs. 

We further present generic constructions for formulas separating Q and QU, and for separating Q and LDQ.</summary>
    <updated>2022-03-10T13:16:27Z</updated>
    <published>2022-03-10T13:16:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-03-16T05:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=118</id>
    <link href="https://dstheory.wordpress.com/2022/03/10/wednesday-march-16th-2022-eric-tchetgen-tchetgen-from-penn/" rel="alternate" type="text/html"/>
    <title>Wednesday March 16th 2022 — Eric Tchetgen Tchetgen from Penn</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The second Foundations of Data Science virtual talk of this year will take place on Wednesday, March 16th at 11:00 PM Pacific Time (16:00 Eastern Time, 21:00 Central European Time, 20:00 UTC). Eric Tchetgen Tchetgen from The Wharton School, University of Pennsylvania will speak about “An Introduction to Proximal Causal Learning”. Please register here to join<a class="more-link" href="https://dstheory.wordpress.com/2022/03/10/wednesday-march-16th-2022-eric-tchetgen-tchetgen-from-penn/">Continue reading <span class="screen-reader-text">"Wednesday March 16th 2022 — Eric Tchetgen Tchetgen from Penn"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="has-text-align-justify">The second <a href="https://sites.google.com/view/dstheory/home" rel="noreferrer noopener" target="_blank">Foundations of Data Science</a> virtual talk of this year will take place on <strong>Wednesday, March 16th</strong> at <strong>11:00 PM Pacific Time</strong> (16:00 Eastern Time, 21:00 Central European Time, 20:00 UTC). <a href="https://statistics.wharton.upenn.edu/profile/ett/">Eric Tchetgen Tchetgen</a> from<strong> The Wharton School, University of Pennsylvania</strong> will speak about “An Introduction to Proximal Causal Learning<em>”</em>.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>:  A standard assumption for causal inference from observational data is that one has measured a sufficiently rich set of covariates to ensure that within covariates strata, subjects are exchangeable across observed treatment values. Skepticism about the exchangeability assumption in observational studies is often warranted because it hinges on one’s ability to accurately measure covariates capturing all potential sources of confounding. Realistically, confounding mechanisms can rarely if ever, be learned with certainty from measured covariates. One can therefore only ever hope that covariate measurements are at best proxies of true underlying confounding mechanisms operating in an observational study, thus invalidating causal claims made on basis of standard exchangeability conditions.</p>



<p>Causal learning from proxies is a challenging inverse problem which has to date remained unresolved. In this paper, we introduce a formal potential outcome framework for proximal causal learning, which while explicitly acknowledging covariate measurements as imperfect proxies of confounding mechanisms, offers an opportunity to learn about causal effects in settings where exchangeability based on measured covariates fails. Sufficient conditions for nonparametric identification are given, leading to the proximal g-formula and corresponding proximal g-computation algorithm for estimation, both generalizations of Robins’ foundational g-formula and g-computation algorithm, which account explicitly for bias due to unmeasured confounding. Both point treatment and time-varying treatment settings are considered, and an application of proximal g-computation of causal effects is given for illustration.</p>



<p class="has-text-align-justify"> The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2022-03-10T06:57:04Z</updated>
    <published>2022-03-10T06:57:04Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2022-03-16T05:38:51Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2022-03-10-eip1559/</id>
    <link href="https://decentralizedthoughts.github.io/2022-03-10-eip1559/" rel="alternate" type="text/html"/>
    <title>EIP-1559 In Retrospect</title>
    <summary>On August 5, 2021, Ethereum implemented Ethereum Improvement Proposal 1559 (EIP-1559) on its mainnet as part of the London Hardfork, which modified the transaction fee mechanism on Ethereum from a first price auction to one that involves blocks of varying sizes, separating transaction fees as history-dependent base fees and tips,...</summary>
    <updated>2022-03-10T05:00:00Z</updated>
    <published>2022-03-10T05:00:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2022-03-15T22:46:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/09/postdoc-at-university-of-texas-at-austin-apply-by-march-23-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/09/postdoc-at-university-of-texas-at-austin-apply-by-march-23-2022/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Texas at Austin (apply by March 23, 2022)</title>
    <summary>This Postdoctoral Fellowship in theoretical computer science is for the 2022-23 academic year to work with David Zuckerman. Research interests should overlap with his: pseudorandomness, computational complexity, coding theory, and more. Applications will be accepted until the position is filled. To apply, send a CV and research statement, and arrange for three letters of recommendation. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This Postdoctoral Fellowship in theoretical computer science is for the 2022-23 academic year to work with David Zuckerman. Research interests should overlap with his: pseudorandomness, computational complexity, coding theory, and more. Applications will be accepted until the position is filled. To apply, send a CV and research statement, and arrange for three letters of recommendation.</p>
<p>Website: <a href="https://www.cs.utexas.edu/~diz/Sub%20Websites/Postdoc.html">https://www.cs.utexas.edu/~diz/Sub%20Websites/Postdoc.html</a><br/>
Email: maguilar@cs.utexas.edu</p></div>
    </content>
    <updated>2022-03-09T21:59:19Z</updated>
    <published>2022-03-09T21:59:19Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-03-16T05:37:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/09/postdocs-at-aalto-university-apply-by-april-1-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/09/postdocs-at-aalto-university-apply-by-april-1-2022/" rel="alternate" type="text/html"/>
    <title>Postdocs at Aalto University (apply by April 1, 2022)</title>
    <summary>Our research group at Aalto University (Helsinki, Finland) is hiring postdoctoral researchers to work on the foundations of distributed and parallel computing. Website: https://research.cs.aalto.fi/da/jobs/ Email: jukka.suomela@aalto.fi</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Our research group at Aalto University (Helsinki, Finland) is hiring postdoctoral researchers to work on the foundations of distributed and parallel computing.</p>
<p>Website: <a href="https://research.cs.aalto.fi/da/jobs/">https://research.cs.aalto.fi/da/jobs/</a><br/>
Email: jukka.suomela@aalto.fi</p></div>
    </content>
    <updated>2022-03-09T18:43:28Z</updated>
    <published>2022-03-09T18:43:28Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-03-16T05:37:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/03/08/mathematics-books-by-women</id>
    <link href="https://11011110.github.io/blog/2022/03/08/mathematics-books-by-women.html" rel="alternate" type="text/html"/>
    <title>Mathematics books by women, in need of more reviews</title>
    <summary>The last couple of years, for International Women’s Day, I’ve posted about mathematics books by women covered by recently added Wikipedia articles. I have a few more of those that I could list today, and a few more that I plan to add, but instead, I thought I’d list books on mathematics and related areas that look interesting enough to me that I want to add them to Wikipedia, but haven’t, because I haven’t found enough reviews to use as the basis for an article.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The last couple of years, for International Women’s Day, I’ve posted about <a href="https://11011110.github.io/blog/2021/03/08/more-mathematics-books.html">mathematics books by women covered by recently added Wikipedia articles</a>. I have a few more of those that I could list today, and a few more that I plan to add, but instead, I thought I’d list books on mathematics and related areas that look interesting enough to me that I want to add them to Wikipedia, but haven’t, because I haven’t found enough reviews to use as the basis for an article.</p>

<p>If you know of more reviews for these, please let me know! I need at least three in-depth reviews, published in journals or by major societies rather than just on a web site or someone’s blog, by three different reviewers, to be comfortable writing a Wikipedia article. It would also help convince other Wikipedians that the topic is a worthy one for Wikipedia if it had at least one review in an actual journal, not just in the MAA, EMS, AMS, and zbMATH review collections, for which one could argue that a review is “routine coverage” that doesn’t suggest any particular significance for a book. Even if you can’t find more reviews, you might find some of these titles and topics intriguing enough to read the books.</p>

<ul>
  <li>
    <p><em>A Course on Tug-of-War Games with Random Noise</em> (2020), <a href="https://en.wikipedia.org/wiki/Marta_Lewicka">Marta Lewicka</a>. Research monograph concerning a game-theoretic twist on Brownian motion. Reviewed by <a href="https://www.maa.org/press/maa-reviews/a-course-on-tug-of-war-games-with-random-noise">MAA</a> and <a href="https://zbmath.org/?q=an%3A1452.91002">zbMATH</a>. Listed by MathSciNet as “pending” so maybe there’ll be a third review soon.</p>
  </li>
  <li>
    <p><em>Alfonso’s Rectifying the Curved</em> (2021), Ruth Glasner and Avinoam Baraness. A history of medieval Spanish circle-squaring. Reviewed by <a href="https://www.maa.org/press/maa-reviews/alfonsos-rectifying-the-curved">MAA</a>.</p>
  </li>
  <li>
    <p><em>Bodies of Constant Width: An Introduction to Convex Geometry with Applications</em> (2019), Horst Martini, Luis Montejano, and Déborah Oliveros. A very thorough reference on <a href="https://en.wikipedia.org/wiki/Surface_of_constant_width">surfaces of constant width</a> and related topics, one that I’ve used as a reference in several other Wikipedia articles. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=3930585">MathSciNet</a> and <a href="https://zbmath.org/?q=an:06999635">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Bubbles</em> (2018), <a href="https://en.wikipedia.org/wiki/Helen_Czerski">Helen Czerski</a>. A children’s book on the science of bubbles and foam. I haven’t found any reviews.</p>
  </li>
  <li>
    <p><em>Certificates of Positivity for Real Polynomials</em> (2021), <a href="https://en.wikipedia.org/wiki/Victoria_Powers">Victoria Powers</a>. <a href="https://en.wikipedia.org/wiki/Sum-of-squares_optimization">Sum-of-squares optimization</a> and the related Lasserre hierarchy has been a hot topic in theoretical computer science in recent years. A polynomial that can be represented as a sum of squares of other polynomials automatically takes only positive values, although the converse is untrue. This book concerns both the mathematical foundations and computational aspects of this topic. MathSciNet lists it only as “preliminary” and zbMATH gives only a publisher blurb.</p>
  </li>
  <li>
    <p><em>The Classification of Quadrilaterals: A Study in Definition</em> (2008), Zalman Usiskin and Jennifer Griffin. This calls itself a “micro-curricular analysis” in mathematics education, on the best way to organize basic definitions of four-sided shapes in elementary geometry. The mathematics education community is usually pretty good about reviewing their books, but I haven’t found any reviews for this one.</p>
  </li>
  <li>
    <p><em>Combinatorial Rigidity</em> (1993), Jack Graver, <a href="https://en.wikipedia.org/wiki/Brigitte_Servatius">Brigitte Servatius</a>, and Herman Servatius. Combinatorial and matroid-theoretic analysis of when a spatial structure made from rigid bars connected by flexible ball joints has some freedom of motion, and when its bars constrain it to a single configuration. I have three reviews for this one, in <em><a href="https://doi.org/10.1090%2FS0273-0979-96-00670-2">Bull. AMS</a></em>, <a href="https://www.ams.org/mathscinet-getitem?mr=1251062">MathSciNet</a>, and <a href="https://zbmath.org/?q=an:0788.05001">zbMATH</a>, but they only count as two because the first two are by the same reviewer.</p>
  </li>
  <li>
    <p><em>Computability Theory</em> (2012), Rebecca Weber. An undergraduate textbook on what can and cannot be computed by algorithms. Reviewed by <a href="https://www.maa.org/press/maa-reviews/computability-theory">MAA</a> and <a href="https://zbmath.org/?q=an:1246.03001">zbMATH</a>. It’s listed on MathSciNet, but only with a copy of the publisher’s blurb, not an actual review.</p>
  </li>
  <li>
    <p><em>Configurations from a Graphical Viewpoint</em> (2013), Tomo Pisanski and Brigitte Servatius. A <a href="https://en.wikipedia.org/wiki/Configuration_(geometry)">configuration</a>, in this context, means a finite system of points and lines with uniform numbers of points per line and lines per point, or more abstractly a <a href="https://en.wikipedia.org/wiki/Biregular_graph">biregular graph</a>. Reviewed in <a href="https://www.ams.org/mathscinet-getitem?mr=2978043">MathSciNet</a> and <a href="https://zbmath.org/?q=an:1277.05001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Do Not Erase</em> (2021), Jessica Wynne. I have this one in hardcopy. It’s a photo-essay of mathematicians’ blackboards, set side-by-side with personal reflections from each mathematician. Some of the boards are obviously set up with pretty diagrams specifically for the photo, while others show the blackboards as tools for recording work-in-progress. Reviewed by <a href="https://www.maa.org/press/maa-reviews/do-not-erase">MAA</a> and <em><a href="https://journals.uic.edu/ojs/index.php/fm/article/view/12284">First Monday</a></em>. MathSciNet has publisher’s blurb (surprising me a little that it was listed at all; usually they only cover graduate and research level mathematics). zbMATH lists it but with a blank review (they didn’t follow the instructions in the title).</p>
  </li>
  <li>
    <p><em>A Field Guide to Digital Color</em> (2003), <a href="https://en.wikipedia.org/wiki/Maureen_C._Stone">Maureen C. Stone</a>. When I used to work at Xerox PARC, Stone was the go-to researcher there on anything related to color. I suspect that, despite its age, there’s still plenty of useful material in her book. But I haven’t found any reviews.</p>
  </li>
  <li>
    <p><em>Fixed Point Theory and Applications</em> (2001), Ravi P. Agarwal, Maria Meehan, and Donal O’Regan. A well-cited reference work on the <a href="https://en.wikipedia.org/wiki/Brouwer_fixed-point_theorem">Brouwer fixed-point theorem</a> and its relatives. Reviewed in <a href="https://www.ams.org/mathscinet-getitem?mr=1825411">MathSciNet</a> and <a href="https://zbmath.org/?q=an%3A0960.54027">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Geometric Group Theory: An Introduction</em> (2017), Clara Löh. A graduate-level textbook in group theory, studying groups by their actions as symmetries of metric spaces. Reviewed in <a href="https://zbmath.org/?q=an%3A1426.20001">zbMATH</a>, but MathSciNet has only the publisher’s blurb.</p>
  </li>
  <li>
    <p><em>Geometry of Cuts and Metrics</em> (1997), Michel Deza and <a href="https://en.wikipedia.org/wiki/Monique_Laurent">Monique Laurent</a>. The polyhedral combinatorics of cut polytopes, with connections to the Kahn–Kalai counterexample to <a href="https://en.wikipedia.org/wiki/Borsuk%27s_conjecture">Borsuk’s conjecture</a> on the existence of partitions of \(d\)-dimensional convex bodies into \(d+1\) pieces of smaller diameter. Reviewed in <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=1460488">MathSciNet</a> and <a href="https://zbmath.org/?q=an:0885.52001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Geometry: The Line and the Circle</em> (2018),  Maureen T. Carroll and Elyn Rykken. An undergraduate textbook on axiomatic geometry and non-Euclidean geometry. Reviewed by <a href="https://www.maa.org/press/maa-reviews/geometry-the-line-and-the-circle">MAA</a> and <em><a href="https://doi.org/10.1007/s00605-021-01541-9">Monatsch. Math.</a></em> Publisher blurb on zbMATH. Nothing on MathSciNet.</p>
  </li>
  <li>
    <p><em>Geometry Through History: Euclidean, Hyperbolic, and Projective Geometries</em> (2018), Meighan Dillon. The title is a bit generic and makes this sound like a generic undergraduate textbook on non-Euclidean geometry. It could be used as that, but I think it goes beyond that as a thorough exploration of the history of thought that went into the development of non-Euclidean geometry. Reviewed by <a href="https://www.maa.org/press/maa-reviews/geometry-through-history-euclidean-hyperbolic-and-projective-geometries">MAA</a> and <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=3753698">MathSciNet</a> but only very briefly by <a href="https://zbmath.org/?q=an%3A1395.51002">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Gröbner Bases in Commutative Algebra</em> (2011), Viviana Ene and Jürgen Herzog. <a href="https://en.wikipedia.org/wiki/Gr%C3%B6bner_basis">Gröbner bases</a> are the key structure in a computational method for triangularizing systems of polynomial equations. In the worst case, this method has high complexity but in practice it often works. They’ve been rediscovered multiple times by multiple people, Gröbner not among them. This is one of several graduate textbooks on the subject. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=2850142">MathSciNet</a> and <a href="https://zbmath.org/?q=an%3A1242.13001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>A Guide to Experimental Algorithmics</em> (2012), <a href="https://en.wikipedia.org/wiki/Catherine_McGeoch">Catherine McGeoch</a>. If you’re going to be doing experimental research on algorithms and their implementation, or guiding undergraduate research projects in this area, this is essential reading. I only know of the <a href="https://doi.org/10.1145/2744447.2744453">SIGACT News</a> review.</p>
  </li>
  <li>
    <p><em>Hyperbolic Knot Theory</em> (2020), <a href="https://en.wikipedia.org/wiki/Jessica_Purcell">Jessica Purcell</a>. The space surrounding a knot can often be given a uniform geometric structure, in which the knot itself recedes to infinity, as depicted in the well-known video <em><a href="https://en.wikipedia.org/wiki/Not_Knot">Not Knot</a></em>. For most knots the resulting geometry is hyperbolic. This book brings this material to the level of an undergraduate text. Reviewed by <a href="https://www.maa.org/press/maa-reviews/hyperbolic-knot-theory">MAA</a> and <a href="https://zbmath.org/?q=an%3A7258506">zbMATH</a>. Listed as pending by MathSciNet.</p>
  </li>
  <li>
    <p><em>Impossibility Results for Distributed Computing</em> (2014), <a href="https://en.wikipedia.org/wiki/Hagit_Attiya">Hagit Attiya</a> and <a href="https://en.wikipedia.org/wiki/Faith_Ellen">Faith Ellen</a>. Research monograph on applications of both information theory and combinatorics in showing that certain computations are impossible for distributed computing systems to perform. I only know of the <a href="https://zbmath.org/?q=an:1396.68004">zbMATH</a> review.</p>
  </li>
  <li>
    <p><em>Information and Coding Theory</em> (2000), Gareth Jones and J. Mary Jones. An undergraduate textbook on information theory, algebraic coding theory techniques, and their interrelations. Reviewed by <em><a href="https://www.jstor.org/stable/3622076">Math. Gaz.</a></em> and <a href="https://zbmath.org/?q=an%3A0956.68052">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>An Introduction to Lorentz Surfaces</em> (1996), <a href="https://en.wikipedia.org/wiki/Tilla_Weinstein">Tilla Weinstein</a>. The Wikipedia article on <a href="https://en.wikipedia.org/wiki/Lorentz_surface">Lorentz surfaces</a> is not very informative but apparently they’re two-dimensional analogues of four-dimensional relativistic spacetime. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=1405166">MathSciNet</a> and <a href="https://zbmath.org/?q=an:0881.53001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Leonardo’s Knots</em> (2019), Caroline Cocciardi. A general audience book on the mathematical and aesthetic analysis of the knotwork visible in the artworks of Leonardo Da Vinci. Reviewed by <a href="https://www.maa.org/press/maa-reviews/leonardos-knots">MAA</a>.</p>
  </li>
  <li>
    <p><em>Nonstandard Analysis in Practice</em> (1995), edited by Francine Diener and Marc Diener. The topics in this edited volume on <a href="https://en.wikipedia.org/wiki/Nonstandard_analysis">nonstandard analysis</a> range from pedagogical to researchy. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=1396794">MathSciNet</a> and briefly by <a href="https://zbmath.org/?q=an%3A0848.26015">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Paradoxes and Sophisms in Calculus</em> (2013), Sergiy Klymchuk and Susan Staples. Published several years earlier in New Zealand? Possibly most useful as a collection of examples for instructors. Reviewed by <a href="https://www.maa.org/press/maa-reviews/paradoxes-and-sophisms-in-calculus">MAA</a> and <em><a href="https://www.jstor.org/stable/10.5951/mathteacher.108.5.0398">The Mathematics Teacher</a></em>. MathSciNet and zbMATH both only have the publisher’s blurb.</p>
  </li>
  <li>
    <p><em>Ptolemy’s Philosophy: Mathematics as a Way of Life</em> (2018), Jacqueline Feke. The context and motivation behind Ptolemy’s general philosophical system. Reviewed by <em><a href="https://doi.org/10.1111/meta.12397">Metaphilosophy</a></em> and <em><a href="https://doi.org/10.1163/18725473-12341470">Int. J. Platonic Trad.</a></em></p>
  </li>
  <li>
    <p><em>Selected Topics in Convex Geometry</em> (2006), Maria Moszyńska. This appears to be a graduate textbook in convex analysis, translated from a 2001 Polish version. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=2169492">MathSciNet</a> and <a href="https://zbmath.org/?q=an%3A1093.52001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>The Theory of Remainders</em> (1995), Andrea Rothbart. A playfully-written introduction to number theory, aimed at middle school teachers. Reviewed in <em><a href="https://www.periodicos.rc.biblioteca.unesp.br/index.php/bolema/article/view/10652/7040">Bolema</a></em> (a Brazilian mathematics education journal) and telegraphically in <em><a href="https://www.jstor.org/stable/2975256">Amer. Math. Monthly</a></em>.</p>
  </li>
  <li>
    <p><em>Tolerance Graphs</em> (2004), Martin Charles Golumbic and <a href="https://en.wikipedia.org/wiki/Ann_Trenk">Ann Trenk</a>. Monograph on <a href="https://en.wikipedia.org/wiki/Tolerance_graph">tolerance graphs</a>, a class of perfect graphs generalizing interval graphs by requiring intervals to have a certain length of overlap in order to become adjacent. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=2051713">MathSciNet</a> and <a href="https://zbmath.org/?q=an:1091.05001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Topics from One-Dimensional Dynamics</em> (2004), <a href="https://en.wikipedia.org/wiki/Karen_Brucks">Karen Brucks</a> and Henk Bruin. Advanced undergraduate or introductory graduate text on dynamical systems theory. Reviewed by <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=2080037">MathSciNet</a> and <a href="https://zbmath.org/?q=an:1074.37022">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Unitals in Projective Planes</em> (2008), Susan Barwick and Gary Ebert. Research monograph on structures in finite geometries. Reviewed in <a href="https://zbmath.org/?q=an%3A1156.51006">zbMATH</a>. MathSciNet has only the publisher’s blurb.</p>
  </li>
</ul>

<p>(<a href="https://mathstodon.xyz/@11011110/107920020659204328">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2022-03-08T00:30:00Z</updated>
    <published>2022-03-08T00:30:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-03-16T05:27:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=6414</id>
    <link href="https://francisbach.com/von-neumann-entropy/" rel="alternate" type="text/html"/>
    <title>Playing with positive definite matrices – II: entropy edition</title>
    <summary>Symmetric positive semi-definite (PSD) matrices come up in a variety of places in machine learning, statistics, and optimization, and more generally in most domains of applied mathematics. When estimating or optimizing over the set of such matrices, several geometries can be used. The most direct one is to consider PSD matrices as a convex set...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Symmetric positive semi-definite (PSD) matrices come up in a variety of places in machine learning, statistics, and optimization, and more generally in most domains of applied mathematics. When estimating or optimizing over the set of such matrices, several geometries can be used. The most direct one is to consider PSD matrices as a convex set in the vector space of all symmetric matrices and thus to inherit the Euclidean geometry. Like for the positive orthant (which corresponds to diagonal PSD matrices), this is not natural for a variety of reasons. In this post we will consider <a href="https://en.wikipedia.org/wiki/Bregman_divergence">Bregman divergences</a> to define our notion of geometry.</p>



<p class="justify-text">A first natural and common geometry is to consider PSD matrices as <a href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrices</a> (or their inverses, often referred to a precision or concentration matrices). They can thus be seen as parameters of zero-mean Gaussian random vectors, and classical notions from information theory, such as the Kullback-Leibler divergence, can be brought to bear to define a specific geometry. This happens to be equivalent to using the negative log determinant to define the Bregman divergence, and leads to the usual Stein divergence $$ – \log \det A + \log \det B \ – {\rm tr}[ – B^{\, -1} ( A \, – B) ] =  -\log \det (AB^{\, -1}) + {\rm tr}( A B^{\, -1}) + d, $$ which is the <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback%E2%80%93Leibler_divergence">Kullback Leibler divergence</a> between two Gaussian random vectors with zero means and covariance matrices \(B\) and \(A\). </p>



<p class="justify-text">In this blog post, we consider a different but related matrix function, which will be \(A \mapsto {\rm tr }  [ A \log A]\) instead of \(– {\rm tr} [ \log A ]\). This will have stronger links with the Shannon entropy of discrete random variables [1]. In particular, we will be able to extend two classical notions related to discrete entropy:</p>



<ul class="justify-text"><li>The entropy function \(H(p) = \ – \sum_{i=1}^d p_i \log p_i\), defined on the simplex \(\Delta \subset \mathbb{R}^d\) (vectors with non-negative components that sum to one), is concave, while the associated Bregman divergence, here the usual <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler (KL) divergence</a>, is $$ \sum_{i=1}^d p_i \log \frac{p_i}{q_i}. $$ It is jointly convex in the vectors \(p\) and \(q\) (see this nice article [<a href="http://carma.newcastle.edu.au/resources/jon/Preprints/Books/CUP/CUPold/MaterialI/bregman.pdf">6</a>] for other jointly convex Bregman divergences). Note that the joint convexity is here a direct application of classical results regarding the convexity of the perspective of a convex function [<a href="http://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">10</a>, section 3.2.6].</li><li>The Fenchel-Legendre conjugate of \(-H\) (with the simplex as domain) is $$\sup_{p \in \Delta} \ \ p^\top z\ – \sum_{i=1}^d p_i \log p_i = \log \Big( \sum_{i=1}^d \exp(z_i) \Big).$$ With the positive orthant as the domain, we have $$\sup_{p \in \mathbb{R}_+^d }\ \ p^\top z \ – \sum_{i=1}^d p_i \log p_i = \sum_{i=1}^d \exp(z_i-1) .$$ This has many consequences in probabilistic modelling, notably the duality between maximum likelihood and maximum entropy [1, <a href="http://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">7</a>].</li></ul>



<p class="justify-text">The extension to matrices will have several interesting consequences:</p>



<ul class="justify-text"><li>This defines a Bregman divergence that has interesting applications to mirror descent and smoothing techniques in optimization.</li><li>The non-trivial joint convexity of the associated Bregman divergence will lead to elegant methods for deriving concentration inequalities for symmetric matrices [<a href="http://arxiv.org/pdf/1501.01571">2</a>].</li><li>Application to covariance operators in infinite-dimensional spaces will lead to a whole range of information-theoretic results [<a href="http://arxiv.org/pdf/2202.08545.pdf">5</a>], but this is for next month!</li></ul>



<h2 id="von-neumann-entropy-and-relative-entropy">Von Neumann entropy and relative entropy</h2>



<p class="justify-text">Given a positive semi-definite symmetric matrix \(A \in \mathcal{S}_d\), the von Neumann entropy is defined as $$H(A) =\ – {\rm tr} [ A \log A],$$ as the trace of the matrix function \(– A \log A\), or equivalently as \(– \sum_{i=1}^d \! \lambda_i \log \lambda_i\) where \(\lambda_1,\dots,\lambda_d \geqslant 0\) are the \(d\) eigenvalues of \(A\) (note that we can have a zero eigenvalue and still be finite). </p>



<p class="justify-text">From <a href="https://francisbach.com/matrix-monotony-and-convexity/">last month blog post</a>, we get that \(H\) is concave. We can then naturally define on the <a href="https://en.wikipedia.org/wiki/Spectrahedron">spectrahedron</a> (set of positive semi-definite matrices with unit trace), the <em>relative entropy</em>, which is the Bregman divergence associated with the function \(A \mapsto {\rm tr} [ A \log A]\), whose gradient is \(\log A + I \), leading to: $$D(A\| B) = {\rm tr} [ A \log A ] \, – {\rm tr} [ B \log B] \, – {\rm tr} \big[ ( A \, – B) ( \log B + I ) \big],$$ which is equal to $$ D(A\| B) = {\rm tr} \big[ A ( \log A \,  – \log B ) \big] – {\rm tr}(A) + {\rm tr}(B).$$ This time we have to be careful to avoid infinite values: it is finite if and only if the null space of \(B\) is included in the one of \(A\), or, equivalently if \(B^{\, -1/2} A^{1/2} B^{\, -1/2}\) is finite. Note that when \(A\) and \(B\) have unit traces, the term in \(– {\rm tr}(A) + {\rm tr}(B)\) goes away.</p>



<p class="justify-text">As a Bregman divergence of a strictly convex function, \(D(A\|B)\) is non-negative and equal to zero if and only if \(A = B\).</p>



<p class="justify-text">When \(A = {\rm Diag}(p)\) and \(B = {\rm Diag}(q)\) are diagonal, we get $$D(A\| B) = \sum_{i=1}^d \Big\{ p_i \log \frac{p_i}{q_i} \, – p_i + q_i \Big\},$$ and we recover the traditional relative entropy. </p>



<p class="justify-text">Please refrain from writing \(D(A\| B) = {\rm tr} \big[ A \log ( A B^{\,-1} ) \big] – {\rm tr}(A) + {\rm tr}(B)\), which is not true unless \(A\) and \(B\) commute. See however below how using Kronecker products can make a similar equality always true.</p>



<h2>A geometry on matrices</h2>



<p class="justify-text">The von Neumann relative entropy (also referred to as matrix KL divergence) defines a new notion of geometry, which is different from the Euclidean geometry based on the squared Frobenius norm \(\| A – B \|_{\rm F}^2 = {\rm tr} [ ( A -B)^2]\). Below, we plot and compare balls for these various geometries. We first consider Euclidean balls, defined as $$\mbox{ Euclidean : } \  \big\{ B \in \mathcal{S}_d,  \| A – B \|_{\rm F}^2 \leqslant r \big\},$$ as well as the two KL balls (one for each side of the KL divergence): $$ \mbox{ Left KL : } \ \big\{ B \in \mathcal{S}_d,  D(B\|A)  \leqslant r \big\},$$ $$\mbox{ Right KL : } \big\{ B \in \mathcal{S}_d,  D(A\|B) \leqslant r \big\}.$$</p>



<p class="justify-text"><strong>Diagonal matrices with unit trace (isomorphic to the simplex).</strong> We consider diagonal matrices \(A = {\rm Diag}(p)\) with \(p\) in the simplex. We compare the three balls below (in yellow), with \(r = 1/16\) for the three of them.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img alt="" class="wp-image-6794" height="340" src="https://francisbach.com/wp-content/uploads/2022/03/simplex_geom.gif" width="1480"/></figure></div>



<p class="justify-text">While the Euclidean balls have the same shapes at all positions, the KL balls adapt their shapes to their centers.</p>



<p class="justify-text"><strong>Unit trace PSD matrices in dimension two (isomorphic to the Euclidean ball).</strong> We consider matrices \(A = \Big( \begin{array}{cc} x &amp; y \\[-.15cm] y &amp; \!\!\!1-x\!\end{array} \Big)\) with the PSD constraint being equivalent to \(y^2 \leqslant x(1-x)\), that is, \((x,y)\) is in the disk of center \((\frac{1}{2},0)\) and radius \(\frac{1}{2}\). We compare the three balls below (in yellow), with \(r = \frac{1}{32}\) for the Euclidean ball, and \(r =\frac{1}{16}\) for the KL balls, with a similar difference between geometries.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img alt="" class="wp-image-6798" height="422" src="https://francisbach.com/wp-content/uploads/2022/03/ellipsoid_geom.gif" width="1486"/></figure></div>



<h2 id="pinsker-inequality-1">Pinsker inequality</h2>



<p class="justify-text">In order to use the relative entropy within mirror descent, we need an extension of the <a href="https://en.wikipedia.org/wiki/Pinsker%27s_inequality">Pinsker inequality</a> $$\sum_{i=1}^d p_i \log \frac{p_i}{q_i} \geqslant \frac{1}{2} \Big(\sum_{i=1}^d | p_i – q_i| \Big)^2,$$ which is, in optimization terms, the \(1\)-strong-concavity of the entropy with respect to the \(\ell_1\)-norm. It turns out that the natural extension is true, that is, for PSD matrices with unit trace, $$D(A\| B) =  {\rm tr} \big[ A ( \log A\,   – \log B ) \big] \geqslant \frac{1}{2} \| A \ – B \|_\ast^2,$$ where \(\| \cdot \|_\ast\) denotes the trace norm, a.k.a. the nuclear norm, that is, the \(\ell_1\)-norm of eigenvalues. See [<a href="http://www.cs.cmu.edu/~yaoliang/mynotes/sc.pdf">3</a>] for a nice and simple proof.</p>



<p class="justify-text">This allows to use the relative entropy when optimizing over the spectrahedron using mirror descent [<a href="https://www2.isye.gatech.edu/~nemirovs/MLOptChapterI.pdf">11</a>]. Note that the diameter \(\sup_{A} {\rm tr} [ A \log A] \ –   \inf_{A} {\rm tr} [ A \log A] \) over PSD matrices with unit trace is equal to \(\log d\). Using duality, the relative entropy can also be used for smoothing the maximal eigenvalue of a matrix [<a href="https://link.springer.com/content/pdf/10.1007/s10107-006-0001-8.pdf">12</a>].</p>



<h2 id="joint-convexity-of-matrix-relative-entropy">Joint convexity of matrix relative entropy</h2>



<p class="justify-text">While the convexity with respect to \(A\) is straightforward, the relative entropy \(D(A \| B)\) is also <em>jointly</em> convex in \((A,B)\), which is crucial in the developments below. There exist several proofs, but one is particularly elegant [<a href="https://arxiv.org/pdf/quant-ph/0604206.pdf">4</a>] and will use the matrix-monotonicity that I presented in the <a href="https://francisbach.com/matrix-monotony-and-convexity/">last post</a>.</p>



<p class="justify-text"><strong>Kronecker products.</strong> We need to make a clever use of <a href="https://en.wikipedia.org/wiki/Kronecker_product">Kronecker products</a>. Given two matrices \(A\) and \(B\) of any sizes, \(A \otimes B\) is the matrix defined by blocks \(A_{ij} B\), thus of size the products of the number of rows and columns of \(A\) and \(B\). In the derivations below, both \(A\) and \(B\) will be of size \(d \times d\), and thus \(A \otimes B\) is then of size \(d^2 \times d^2\). </p>



<p class="justify-text">Kronecker products have very nice properties (they would probably deserve their own post!), such as (when dimensions are compatible), \((A \otimes B) ( C\otimes D) = (AC) \otimes (BD)\) or \((A \otimes B)^{\, -1} = A^{-1} \otimes B^{\, -1}\). What we will leverage is their use in characterizing linear operations on matrices. For a rectangular matrix \(X \in \mathbb{R}^{d_1 \times d_2}\), \({\rm vec}(X) \in \mathbb{R}^{d_1 d_2 \times 1}\) denotes the <em>vectorization</em> of \(X\) obtained by stacking all columns of \(X\) one after the other in a single column vector. Then, we have $$ (B^\top \otimes A){\rm vec}(X) = {\rm vec}(AXB)$$ for any \(A, X, B\) with compatible dimensions. </p>



<p class="justify-text"><strong>Reformulation of relative entropy.</strong> We can now use Kronecker products to get: $$ {\rm tr} [A \log A] = {\rm tr}[ A^{1/2}( \log A) A^{1/2} ] = {\rm vec}(A^{1/2})^\top [ I \otimes \log A ] {\rm vec}(A^{1/2}),$$ and $$ {\rm tr} [A \log B] = {\rm tr} [ A^{1/2}( \log B) A^{1/2} ] = {\rm vec}(A^{1/2})^\top [    \log B \otimes I  ] {\rm vec}(A^{1/2}),$$ leading to $$D(A\|B) = {\rm vec}(A^{1/2})^\top \big[ I \otimes \log A \ – \log B \otimes I \big]  {\rm vec}(A^{1/2}) \, – {\rm tr}(A) + {\rm tr}(B).$$ The key element is then to use the identity for PSD matrices $$\log(C \otimes D) = \log C \otimes I + I \otimes \log D$$ (which can be shown using the fact that eigenvalue decomposition of a Kronecker product of symmetric matrices can be obtained from the decompositions of the factors), leading to $$D(A\|B) = {\rm vec}(A^{1/2})^\top \big[ \log ( I \otimes A) \ – \log (B \otimes I) \big]  {\rm vec}(A^{1/2})\, – {\rm tr}(A) + {\rm tr}(B).$$ It seems we have not achieved much, but now, the matrices \( I \otimes A\) and \(B \otimes I\) commute! Therefore, we can now write $$D(A\|B) = {\rm vec}(A^{1/2})^\top \Big[ \log \big( ( I \otimes A) (B \otimes I)^{-1} \big) \Big]  {\rm vec}(A^{1/2})- {\rm tr}(A) + {\rm tr}(B).$$ This can then be rewritten as $$D(A\|B)  =\, – {\rm vec}(A^{1/2})^\top \big[ \log ( B \otimes A^{-1}) \big] {\rm vec}(A^{1/2}) \, – {\rm tr}(A) + {\rm tr}(B).$$ </p>



<p class="justify-text"><strong>Using matrix-convexity.</strong> We can now use the matrix-convexity of \(\lambda \mapsto \ – \log \lambda\), and the representation from <a href="http://francisbach.com/matrix-monotony-and-convexity/">last post</a> as $$ -\log \lambda = 1-\lambda + (\lambda-1)^2 \int_0^{+\infty} \!\!\frac{1}{\lambda +u} \frac{du}{(1+u)^2},$$ leading to $$D(A\|B) =  \int_0^{+\infty}  {\rm vec}(A^{1/2})^\top \Big[ ( B\otimes A^{-1} – I)^2 (B\otimes A^{-1} + u I)^{-1} \Big]  {\rm vec}(A^{1/2}) \frac{du}{(1+u)^2},$$ and then, by using  \(( B\otimes A^{-1} – I) ( I \otimes A^{1/2})  {\rm vec}(A^{1/2})  =  {\rm vec}(B-A)\), $$D(A\|B) =   \int_0^{+\infty}  {\rm vec}(A-B)^\top  (B\otimes I+ u I \otimes A)^{-1}   {\rm vec}(A-B)   \frac{du}{(1+u)^2}.$$ We can then use the joint convexity of the function \((C,D) \mapsto {\rm tr} \big[  C^\top D^{-1} C \big] \) to get the  joint convexity of \(D(A\|B)\).</p>



<p class="justify-text">This extends to all matrix-convex functions beyond the logarithm, such as \(\lambda \mapsto (\lambda-1)^2\), \(\lambda \mapsto 1 \, – \lambda^{1/2}\) or \(\displaystyle \lambda \mapsto \frac{1}{\lambda}(\lambda-1)^2\), leading to other <a href="https://en.wikipedia.org/wiki/F-divergence">\(f\)-divergences</a> between matrices. See [<a href="http://arxiv.org/pdf/2202.08545.pdf">5</a>, Appendix A.4] for details.</p>



<h2 id="lieb-concavity-theorems">Lieb concavity theorems</h2>



<p class="justify-text">Now that \((A,B) \mapsto D(A\|B)\) is jointly convex on all PSD matrices (not only unit trace), for any fixed symmetric matrix \(M\), the function $$ \varphi: B \mapsto \sup_{ A \succcurlyeq 0} \ {\rm tr}(AM) + {\rm tr}(A) \ – D(A\|B), $$ as the partial maximization (with respect to \(B\)) of a jointly concave function of \((A,B)\), is concave. We can maximize in closed form with respect to \(A\) by computing the gradient \(M \ – \log A  + \log B\), leading to \(A = \exp( M + \log B)\), and $$\varphi(B) = {\rm tr} \exp ( M + \log B),$$ which defines a concave function. This is one version of the classical Lieb concavity theorems [<a href="http://sciencedirect.com/science/article/pii/000187087390011X/pdf?md5=09b7ceaa543358cef395fa6af8de62aa&amp;pid=1-s2.0-000187087390011X-main.pdf">8</a>]. </p>



<p class="justify-text">As a consequence, we can obtain the “subadditivity of the matrix cumulant generative function”. That is, for any random <em>independent</em> symmetric matrices \(X_1,\dots,X_n\) of the same sizes, $$ \mathbb{E} \Big[  {\rm tr} \exp \big(   \sum_{i=1}^n X_i \big) \Big] \leqslant {\rm tr} \exp \big( \sum_{i=1}^n \log \big( \mathbb{E} [ \exp( X_i ) ] \big),$$ which can be shown by recursion using Jensen’s inequality for the function \(\varphi\) above with a well-chosen matrix \(M\). See [<a href="http://arxiv.org/pdf/1501.01571">2</a>] for details.</p>



<p class="justify-text">We are now ready to consider concentration inequalities for random matrices.</p>



<h2 id="application-to-concentration-inequalities">Application to concentration inequalities</h2>



<p class="justify-text">We consider random independent random matrices \(X_1,\dots,X_n\) such that \(\mathbb{E} [ X_i ] = 0\) for all \(i \in \{1,\dots,n\}\). Let \(S = \frac{1}{n} \sum_{i=1}^n \! X_i\) (which has zero mean). Our goal is to provide upper bounds on \(\mathbb{E} \big[ \lambda_{\max}(S) \big]\) and \(\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big]\). We follow the outstanding work and presentation of <a href="http://users.cms.caltech.edu/~jtropp/">Joel Tropp</a> [<a href="http://arxiv.org/pdf/1501.01571">2</a>]: using the proper matrix tools presented above, we can get for matrix-valued random variables the almost exact same proofs of the usual concentration inequalities for real-valued random <em>variables</em>, such as <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding</a>, <a href="https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)">Bernstein</a>, <a href="https://en.wikipedia.org/wiki/Azuma%27s_inequality">Azuma</a>, or Mac-Diarmid inequalities. </p>



<p class="justify-text"><strong>Expectation. </strong>We have, for any \(u &gt; 0\), by homogeneity of the largest eigenvalue: $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] = \frac{1}{u}  \mathbb{E} \big[ \lambda_{\max}(uS) \big] =  \frac{1}{u}  \mathbb{E} \big[  \log \exp ( \lambda_{\max}(uS) ) \big].$$ We can then use Jensen’s inequality for the logarithm to get (together with the fact the largest eigenvalue of the exponential is the exponential of the largest eigenvalue): $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] \leqslant \frac{1}{u}  \log \mathbb{E} \big[ \exp ( \lambda_{\max}(uS) ) \big]=\frac{1}{u}  \log \mathbb{E} \big[ ( \lambda_{\max}( \exp ( uS) ) \big], $$ and finally that for a PSD matrix the trace is larger than the largest eigenvalue, to get $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] \leqslant \frac{1}{u}  \log \mathbb{E} \big[ {\rm tr} \exp (  uS ) \big].$$ Experts in concentration inequalities have probably recognized the usual argument to bound the expectation of the maximum of \(d\) random <em>real-valued</em> (non necessarily independent) variables \(X_1,\dots,X_d\), as $$ \mathbb{E} \big[ \max \{Y_1,\dots,Y_d\} \big] \leqslant \frac{1}{u} \log \mathbb{E} \big[ \exp(uY_1)+ \cdots + \exp(u Y_d) \big].$$</p>



<p class="justify-text"><strong>Tail bound.</strong> We have, using Markov’s inequality, for any \(u &gt; 0\): $$\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big] = \mathbb{P} \big[ \exp(\lambda_{\max}(uS)) \geqslant \exp(ut) \big] \leqslant e^{-ut} \mathbb{E} \big[ \exp ( \lambda_{\max}(uS) ) \big],$$ thus leading to $$\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big] \leqslant e^{-ut} \mathbb{E} \big[ {\rm tr} \exp (  uS ) \big].$$ The same experts have recognized $$ \mathbb{P} \big[ \max \{Y_1,\dots,Y_d\} \geqslant t \big] \leqslant  e^{-ut} \mathbb{E} \big[ \exp(uY_1)+ \cdots + \exp(u Y_d) \big].$$ </p>



<p class="justify-text">We can now consider random matrices such that \(a I \preccurlyeq X_i \preccurlyeq b I\) almost surely for all \(i=1,\dots,n\) (and still with zero mean). We first need a lemma whose proof is given at the end of the post, with the exact same proof as for the uni-dimensional case.</p>



<p class="justify-text"><strong>Lemma.</strong> If \(\mathbb{E} [ X ] = 0\) and \(a I \preccurlyeq X \preccurlyeq b I\) almost surely, then for all \(u &gt; 0\), $$ \mathbb{E} \big[ \exp( u X) \big] \preccurlyeq \exp\big( \frac{u^2}{8}(b-a)^2 \big) I.$$ </p>



<p class="justify-text">Together with the consequence of Lieb’s result presented above and the matrix-monotonicity of the logarithm, we get: $$ \mathbb{E} \big[ {\rm tr} \exp (  uS ) \big] \leqslant {\rm tr} \exp \Big( n \big( \frac{u^2}{8n^2}(b-a)^2 I \big) \Big) = d  \exp \big(   \frac{u^2}{8n}(b-a)^2 \big).$$ We can then get immediately, by optimizing with respect to \(u\) (with optimal value \(u=4 n t /(b-a)^2\)), $$\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big] \leqslant d \exp\big( – \frac{2 n t^2 }{(b-a)^2} \big),$$ and, with optimal value \(u= {2 \sqrt{2 n \log d }}/ { (b-a) }\): $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] \leqslant \frac{b-a}{\sqrt{2n}} \sqrt{ \log d}.$$</p>



<p class="justify-text">Note that these are the exact same results as when \(d=1\)! </p>



<p class="justify-text"><strong>Extensions.</strong> As nicely detailed in [<a href="https://arxiv.org/pdf/1501.01571">2</a>], there are extensions to all classical concentration inequalities. Moreover, this can be applied to rectangular matrices, where the largest eigenvalue is replaced by the largest singular value. Finally, for those who like to work in infinite dimensions and cannot bear the explicit dependence in dimension, it is possible to derive results that depend only on a well-defined notion of “intrinsic dimension” (the associated results can then be used for the study of kernel methods, see [<a href="http://di.ens.fr/~fbach/ltfp_book.pdf">13</a>, chapter 9]).</p>



<h2 id="conclusion">Conclusion</h2>



<p class="justify-text">Now that we know everything about the matrix relative entropy and its convexity properties, we will explore next month a new link between eigenvalues of covariance operators and the regular Shannon entropy of the generating probability distribution [<a href="http://arxiv.org/pdf/2202.08545.pdf">5</a>], thus mixing my favorite recent mathematical topic with my probably all-time favorite one: positive definite kernels!</p>



<h2 id="references">References</h2>



<p class="justify-text">[1] Thomas M. Cover and Joy A. Thomas. <em>Elements of Information Theory</em>. John Wiley &amp; Sons, 1999.<br/>[2] Joel A. Tropp. <a href="https://arxiv.org/pdf/1501.01571">An introduction to matrix concentration inequalities</a>. <em>Foundations and Trends in Machine Learning</em>, 8(1-2):1–230, 2015.<br/>[3] Yao-Liang Yu. <a href="http://www.cs.cmu.edu/~yaoliang/mynotes/sc.pdf">The strong convexity of von Neumann’s entropy</a>. Unpublished note, 2013.<br/>[4] Mary Beth Ruskai. <a href="https://arxiv.org/pdf/quant-ph/0604206.pdf">Another short and </a><a href="https://arxiv.org/pdf/quant-ph/0604206.pdf" rel="noreferrer noopener" target="_blank">elementary</a><a href="https://arxiv.org/pdf/quant-ph/0604206.pdf"> proof of strong subadditivity of quantum entropy</a>. Reports on Mathematical Physics, 60(1):1–12, 2007.<br/>[5] Francis Bach. <a href="https://arxiv.org/pdf/2202.08545.pdf">Information Theory with Kernel Methods</a>. Technical Report, arXiv-2202.08545, 2022.<br/>[6] Heinz H. Bauschke, and Jonathan M. Borwein. <a href="https://carma.newcastle.edu.au/resources/jon/Preprints/Books/CUP/CUPold/MaterialI/bregman.pdf">Joint and separate convexity of the Bregman distance</a>. <em>Studies in Computational Mathematics</em>, 8:23-36, 2001.<br/>[7] Martin J. Wainwright and Michael I. Jordan. <a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference</a>. <em>Foundations and Trends in Machine Learning</em>, 1 (1–2):1–305, 2008. <br/>[8] Elliott H. Lieb. <a href="https://www.sciencedirect.com/science/article/pii/000187087390011X/pdf?md5=09b7ceaa543358cef395fa6af8de62aa&amp;pid=1-s2.0-000187087390011X-main.pdf">Convex trace functions and the Wigner-Yanase-Dyson conjecture</a>.<br/><em>Advances in Mathematics</em>, 11(3):267–288, 1973.<br/>[9] Stéphane Boucheron, Gabor Lugosi, and Pascal Massart. <em>Concentration Inequalities: A Nonasymptotic Theory of Independence</em>. Oxford University Press, 2013. <br/>[10] Stephen Boyd, Lieven Vandenberghe. <em><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></em>. Cambridge University Press, 2004.<br/>[11] Anatoli Juditsky, Arkadi Nemirovski. <a href="http://www2.isye.gatech.edu/~nemirovs/MLOptChapterI.pdf">First order methods for nonsmooth convex large-scale optimization, i: general purpose methods</a>. <em>Optimization for Machine Learning</em>, 121-148, 2012.<br/>[12] Yurii Nesterov. <a href="https://link.springer.com/content/pdf/10.1007/s10107-006-0001-8.pdf">Smoothing technique and its applications in Semidefinite Optimization</a>. <em>Mathematical Programming</em>, 110(2):245-259, 2007.<br/>[13] Francis Bach. <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf">Learning Theory From First Principles</a>. Book draft, 2021.</p>



<h2 id="proof-of-lemma-on-matrix-cumulant-generating-functions">Proof of lemma on matrix cumulant generating functions</h2>



<p class="justify-text"><strong>Lemma.</strong> If \(\mathbb{E} [ X ] = 0\) and \(a I \preccurlyeq X \preccurlyeq b I\) almost surely, then for all \(u &gt; 0\), $$ \mathbb{E} \big[ \exp( u X) \big] \preccurlyeq \exp\big( \frac{u^2}{8}(b-a)^2 I \big) =  \exp\big( \frac{u^2}{8}(b-a)^2 \big) I .$$ Note that we have \(a \leqslant 0 \leqslant b\).</p>



<p class="justify-text"><strong>Proof</strong>. We follow the standard proof [9] for real-valued random variables, simply transferred to the matrix case. On the interval \([a,b]\), by convexity of the exponential function $$ \exp(u\lambda) \leqslant \exp(ub) \frac{\lambda-a}{b-a} + \exp(ua) \frac{b \, – \lambda}{b-a}.$$ Thus (check why we can do this), almost surely: $$ \exp(uX) \preccurlyeq \exp(ub) \frac{X-a I }{b-a} + \exp(ua) \frac{bI  – X}{b-a},$$ leading to \(\displaystyle \mathbb{E} [ \exp(uX) ]  \preccurlyeq   \frac{b \exp(ua)\ – a \exp(ub)}{b-a} I\). We can then show that \(\displaystyle \frac{b \exp(ua)\ – a \exp(ub)}{b-a}  \leqslant \exp \Big(  \frac{u^2}{8}(b-a)^2 \Big)\) to conclude the proof. We thus need to show that \(\displaystyle \psi(u) = \log \Big( \frac{b \exp(ua)\ – a \exp(ub)}{b-a}  \Big) \leqslant \frac{u^2}{8}(b-a)^2\) for all \(u \geqslant 0\). We can simply express \(\psi(u)\) as a rescaled logistic function as $$\psi(u) =  ua + \log \frac{b}{b-a} + \log\Big[ 1 + \exp\Big( u(b-a) + \log\frac{-a}{b-a} \Big) \Big].$$ We have \(\psi(0) = 0\), and $$\psi'(u) =  \frac{ba \exp(ua)\ – ab \exp(ub)}{b \exp(ua)\ – a \exp(ub)}, $$ thus \(\psi'(0) = 0\), and through the link with the logistic function, the second-order derivarive is less than \((b-a)^2 / 4\). This leads to the desired result by Taylor’s formula.</p>



<p/>



<p><br/></p></div>
    </content>
    <updated>2022-03-07T21:26:18Z</updated>
    <published>2022-03-07T21:26:18Z</published>
    <category term="Machine learning"/>
    <category term="Optimization"/>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2022-03-16T05:38:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2022-03-07-colordag-from-always-almost-to-almost-always-50-percent-selfish-mining-resilience/</id>
    <link href="https://decentralizedthoughts.github.io/2022-03-07-colordag-from-always-almost-to-almost-always-50-percent-selfish-mining-resilience/" rel="alternate" type="text/html"/>
    <title>Colordag: From always-almost to almost-always 50% selfish mining resilience</title>
    <summary>The Selfish mining attack against blockchain protocols was discovered and formalized in 2013 by Eyal and Sirer (also see our blog post). The Bitcoin community has mentioned similar types of attacks in 2010. This attack remains a vulnerability of all operational blockchains we are aware of. For Bitcoin’s blockchain algorithm...</summary>
    <updated>2022-03-07T15:19:00Z</updated>
    <published>2022-03-07T15:19:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2022-03-15T22:46:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/035</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/035" rel="alternate" type="text/html"/>
    <title>TR22-035 |  A Characterization of Multiclass Learnability | 

	Nataly Brukhim, 

	Daniel Carmon, 

	Irit Dinur, 

	Shay Moran, 

	Amir Yehudayoff</title>
    <summary>A seminal result in learning theory characterizes the PAC learnability of binary classes through the Vapnik-Chervonenkis dimension. Extending this characterization to the general multiclass setting has been open since the pioneering works on multiclass PAC learning in the late 1980s. This work resolves this problem: we characterize multiclass PAC learnability through the DS dimension, a combinatorial dimension defined by Daniely and Shalev-Shwartz (2014). 

The classical characterization of the binary case boils down to empirical risk minimization. In contrast, our characterization of the multiclass case involves a variety of algorithmic ideas; these include a natural setting we call list PAC learning. In the list learning setting, instead of predicting a single outcome for a given unseen input, the goal is to provide a short menu of predictions. 

Our second main result concerns the Natarajan dimension, which has been a central candidate for characterizing multiclass learnability. This dimension was introduced by Natarajan (1988) as a barrier for PAC learning. Whether the Natarajan dimension characterizes PAC learnability in general has been posed as an open question in several papers since. This work provides a negative answer: we construct a non-learnable class with Natarajan dimension one. 

For the construction, we identify a fundamental connection between concept classes and topology (i.e., colorful simplicial complexes). We crucially rely on a deep and involved construction of hyperbolic pseudo-manifolds by Januszkiewicz and Swiatkowski. It is interesting that hyperbolicity is directly related to learning problems that are difficult to solve although no obvious barriers exist. This is another demonstration of the fruitful links machine learning has with different areas in mathematics.</summary>
    <updated>2022-03-07T08:42:20Z</updated>
    <published>2022-03-07T08:42:20Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-03-16T05:37:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5954313589876443289</id>
    <link href="http://blog.computationalcomplexity.org/feeds/5954313589876443289/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/random-thoughts-on-russian-invasion-of.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/5954313589876443289" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/5954313589876443289" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/random-thoughts-on-russian-invasion-of.html" rel="alternate" type="text/html"/>
    <title>Random thoughts on the Russian Invasion of Ukraine</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> 1) My first thought was: Doesn't Putin know that his army (and his society) is corrupt and people are promoted on loyalty rather than talent, hence the invasion is not going to work? But then note</p><p>a) He invaded Georgia (the country not the state)</p><p>b) He annexed Crimea</p><p>c) NATO agreeing on sanctions? They can't even agree on what to have for lunch.  </p><p>d) The above question ASSUMES that he will lose. This is not clear yet.</p><p>2) The US and other countries are imposing sanctions. Visa, MC, and Netflix have withdrawn from Russia. Will these cause Putin to stop, or will they just impose hardships on the citizens without having an affect? I ask non-rhetorically. Foreign Policy is hard, though I suspect Biden has hired experts and is listening to them. Doesn't mean they are right. </p><p>3) This seems like a case where its obvious that Putin is in the wrong so I was surprised to see some Pro-Putin arguments. To be fair, some were more that we should not get involved, which is not quite Pro Putin. Here are some and my thoughts on them.</p><p>a) Putin never called me a racist and never made my kids learn Critical Race Theory. This is meant as a GOTCHA comment. To take it seriously one would have to (1) examine what American's are learning in grade school, (2) examine what Russians are learning in grade school, (3) see which country is giving their students a more honest view of things (particularly history), and (4) decide that whichever school is teaching more honest material deserves are support. </p><p>b) Putin is a strong leader and I admire strong leaders. Really? If  Biden was a strong leader who could push through the Build-Back-Better plan, would you admire that? You can't just admire strong leaders in the abstract, you have to see where they lead. (Same with admiring people who have principles.)</p><p>c) Putin stands for good Christian values. OH- so Putin likes to help the poor and does not approve of divorce or pre-marital sex. Oh, that NOT what you mean? Oh, you just mean that he does not like gay people or abortion. OH, thats not correct- Russia has the second most number of abortions of any country (see <a href="https://www.worldblaze.in/countries-with-highest-abortion-rates/">here</a>) So the only issue where Russia shares your values is in not liking gay people. Is that enough  commonality is enough to justify invading a country? This does raise the serious question: who do you support in a war? The country that shrares your values? The country that didn't start it (sometimes hard to tell, though not in this case)? The country you have a treaty with? These are good and serious questions. </p><p>d) How come the world does not like Putin's war based on lies and had no problem with W's war on Iraq that was based on lies? This is actually a good question. Being an academic by first impulse is to write a paper on it. There are differences between the invasion of Ukraine and Gulf War II, but still, lets assume that they really are somewhat equivalent. The person asking the question was probably AGAINST Gulf War II. Hence they should be against the invasion of Ukraine as well. So this is a good question about the America and NATO's Double standard and hypocrisy but is not a reason to be pro-Putin. (I was emailed that there are many conflicts that American and NATO do not care about that are as bad as Putin's invasion.) </p><p>e) Russia had real concerns about security. The last time Russia was invaded was WW II (if I am wrong then please correct me in the comments). Even so, I've heard analogies made to America freaking out when Cuba had Nuclear missiles. There were no nukes in Ukraine. Ukraine was not going to join NATO anytime soon. I wonder if this was a pretense on Putin's part and not a real reason. </p><p>f) See <a href="https://www.dailykos.com/stories/2022/3/4/2083726/-This-week-in-QAnon-Chronicles-Ukraine-enters-the-conspiracy">here</a>. I give an excerpt (I am not making this up). </p><p>But as its Covid mission has become less clear, the group’s channels have turned to Russia’s invasion of Ukraine, where conspiracy-minded thinking has flourished. While some group members have admonished Russian President Vladimir Putin for the invasion, QAnon and anti-vaccine contingents within the groups have seized on a false conspiracy theory that the war is a cover for a military operation backed by former President Donald Trump in Ukraine.</p><p>The conspiracy theory, which is baseless and has roots in QAnon mythology, alleges that Trump and Putin are secretly working together to stop bioweapons from being made by Dr. Anthony Fauci in Ukraine and that shelling in Ukraine has targeted the secret laboratories. Fauci, director of the National Institute of Allergy and Infectious Diseases, has emerged in the past year as a main target for far-right conspiracy theories.</p><p>3) I was somewhat surprised when Hungary came out against Putin. But Putin's arrangments with his allies are transactional- there is no real love there. I was much more surprised when Switzerland came out against Putin. Switzerland has been neutral since 1515 (I think I read that some place). </p><div><br/></div><p><br/></p></div>
    </content>
    <updated>2022-03-07T00:07:00Z</updated>
    <published>2022-03-07T00:07:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-03-15T21:37:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8290</id>
    <link href="https://windowsontheory.org/2022/03/06/halg-2022-call-for-submissions-guest-post-by-keren-censor-hillel/" rel="alternate" type="text/html"/>
    <title>HALG 2022 Call for submissions: Guest post by Keren Censor Hillel</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The 7th Highlights of Algorithms conference (HALG 2022)London, June 1-3, 2022https://www.lse.ac.uk/HALG-2022 The Highlights of Algorithms conference is a forum for presenting the highlights of recent developments in algorithms and for discussing potential further advances in this area. The conference will provide a broad picture of the latest research in algorithms through a series of invited talks, … <a class="more-link" href="https://windowsontheory.org/2022/03/06/halg-2022-call-for-submissions-guest-post-by-keren-censor-hillel/">Continue reading <span class="screen-reader-text">HALG 2022 Call for submissions: Guest post by Keren Censor Hillel</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>The 7th Highlights of Algorithms conference (HALG 2022)</strong><br/>London, June 1-3, 2022<br/><a href="https://www.lse.ac.uk/HALG-2022" rel="noreferrer noopener" target="_blank">https://www.lse.ac.uk/HALG-2022</a></p>



<p>The Highlights of Algorithms conference is a forum for presenting the highlights of recent developments in algorithms and for discussing potential further advances in this area. The conference will provide a broad picture of the latest research in algorithms through a series of invited talks, as well as the possibility for all researchers and students to present their recent results through a series of short talks and poster presentations. Attending the Highlights of Algorithms conference will also be an opportunity for networking and meeting leading researchers in algorithms.</p>



<p><strong>Call For Submissions of Short Contributed Presentations:</strong></p>



<p>The HALG 2022 conference seeks submissions for contributed presentations. Each presentation is expected to consist of a poster and a short talk (serving as an invitation to the poster). There will be no conference proceedings, hence presenting work already published at a different venue or journal (or to be submitted there) is welcome.</p>



<p>If you would like to present your results at HALG 2022, please submit their details: the abstract, the paper and the speaker of the talk via EasyChair: <a href="https://easychair.org/conferences/?conf=halg2022" rel="noreferrer noopener" target="_blank">https://easychair.org/conferences/?conf=halg2022</a></p>



<p>The abstract should include (when relevant) information where the results have been published/accepted (e.g., conference), and where they are publicly available (e.g., arXiv). All submissions will be reviewed by the program committee, giving priority to work accepted or published in 2021 or later.</p>



<p>Submissions deadline: <strong>March 27, 2022.</strong><br/>Acceptance/rejection notifications will be sent in early April.</p></div>
    </content>
    <updated>2022-03-06T16:10:55Z</updated>
    <published>2022-03-06T16:10:55Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2022-03-16T05:37:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-5529941464304334711</id>
    <link href="http://processalgebra.blogspot.com/feeds/5529941464304334711/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=5529941464304334711" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/5529941464304334711" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/5529941464304334711" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2022/03/halg-2022-call-for-submissions-of-short.html" rel="alternate" type="text/html"/>
    <title>HALG 2022: Call For Submissions of Short Contributed Presentations</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> <i>On behalf of <a href="https://ckeren.net.technion.ac.il/" target="_blank">Keren Censor-Hillel</a>,  PC chair for HALG 2022, I am happy to post the call for </i><i>submissions of short contributed presentations for that event. I encourage all members of the algorithms  community to submit their contributed presentations. HALG has rapidly become a meeting  point for that community in a relaxed workshop-style setting.</i> </p><p style="text-align: center;"><b>The 7th Highlights of Algorithms conference (HALG 2022) </b></p><p style="text-align: center;"><b>London, June 1-3, 2022 </b></p><p style="text-align: center;"><a href="https://www.lse.ac.uk/">https://www.lse.ac.uk/</a></p><p>HALG-2022    The Highlights of Algorithms conference is a forum for presenting the highlights of recent developments in algorithms and for discussing potential further advances in this area. The conference will provide a broad picture of the latest research in algorithms through a series of invited talks, as well as the possibility for all researchers and students to present their recent results through a series of short talks and poster presentations. Attending the Highlights of Algorithms conference will also be an opportunity for networking and meeting leading researchers in algorithms.  </p><p>Call For Submissions of Short Contributed Presentations: The HALG 2022 conference seeks submissions for contributed presentations. Each presentation is expected to consist of a poster and a short talk (serving as an invitation to the poster). There will be no conference proceedings, hence presenting work already published at a different venue or journal (or to be submitted there) is welcome. </p><p>If you would like to present your results at HALG 2022, please submit their details: the abstract, the paper and the speaker of the talk via EasyChair: <a href="https://easychair.org/conferences/?conf=halg2022">https://easychair.org/conferences/?conf=halg2022</a> </p><p>The abstract should include (when relevant) information where the results have been published/accepted (e.g., conference), and where they are publicly available (e.g., arXiv). All submissions will be reviewed by the program committee, giving priority to work accepted or published in 2021 or later. </p><p>Submissions deadline: March 27, 2022. </p><p>Acceptance/rejection notifications will be sent in early April. </p></div>
    </content>
    <updated>2022-03-06T15:26:00Z</updated>
    <published>2022-03-06T15:26:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2022-03-12T20:51:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/034</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/034" rel="alternate" type="text/html"/>
    <title>TR22-034 |  Fourier Growth of Regular Branching Programs | 

	Chin Ho Lee, 

	Edward Pyne, 

	Salil Vadhan</title>
    <summary>We analyze the Fourier growth, i.e. the $L_1$ Fourier weight at level $k$ (denoted $L_{1,k}$),  of read-once regular branching programs.
We prove that every read-once regular branching program $B$ of width $w \in [1,\infty]$ with $s$ accepting states on $n$-bit inputs must have its $L_{1,k}$ bounded by
$$
  \min\left\{ \Pr[B(U_n)=1] (w-1)^k , s \cdot O\left((n \log n)/k\right)^{\frac{k-1}{2}} \right\}.
$$
For any constant $k$, our result is tight up to constant factors for the AND function on $w-1$ bits, and is tight up to polylogarithmic factors for unbounded width programs. In particular, for $k=1$ we have $L_{1,1}(B)\leq s$, with no dependence on the width $w$ of the program.

Our result gives new bounds on the coin problem and new pseudorandom generators (PRGs). Furthermore, we obtain an explicit generator for unordered permutation branching programs of unbounded width with a constant factor stretch, where no PRG was previously known.

Applying a composition theorem of B{\l}asiok, Ivanov, Jin, Lee, Servedio and Viola (RANDOM 2021), we extend our results to ``generalized group products,'' a generalization of modular sums and product tests.</summary>
    <updated>2022-03-06T03:07:32Z</updated>
    <published>2022-03-06T03:07:32Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-03-16T05:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/033</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/033" rel="alternate" type="text/html"/>
    <title>TR22-033 |  A better-than-$3\log{n}$ depth lower bound for De Morgan formulas with restrictions on top gates | 

	Ivan Mihajlin, 

	Anastasia Sofronova</title>
    <summary>We prove that a modification of Andreev's function is not computable by $(3 + \alpha - \varepsilon) \log{n}$ depth De Morgan formula with $(2\alpha - \varepsilon)\log{n}$ layers of AND gates at the top for any $1/5 &gt; \alpha &gt; 0$ and any constant $\varepsilon &gt; 0$. In order to do this, we prove a weak variant of Karchmer-Raz-Wigderson conjecture. To be more precise, we prove the existence of two functions $f \colon \{0,1\}^n \rightarrow \{0,1\}$ and $g \colon \{0,1\}^n \rightarrow \{0,1\}^n$ such that $f(g(x) \oplus y)$ is not computable by depth $(1 + \alpha - \varepsilon) n$ formulas with $(2 \alpha - \varepsilon) n$ layers of AND gates at the top. We do this by a top-down approach, which was only used before for depth-$3$ model. 

Our technical contribution includes combinatorial insights into structure of composition with random boolean function, which led us to introducing a notion of well-mixed sets. A set of functions is well-mixed if, when composed with a random function, it does not have subsets that agree on large fractions of inputs. We use probabilistic method to prove the existence of well-mixed sets.</summary>
    <updated>2022-03-06T03:05:43Z</updated>
    <published>2022-03-06T03:05:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-03-16T05:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19711</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/03/05/i-am-now-a-cyberman/" rel="alternate" type="text/html"/>
    <title>I Am Now A Cyberman</title>
    <summary>Cybermen: You will be upgraded. Cyberman: Upgrading is compulsory. I am now carrying a full power CPU embedded in my chest. If I could use it directly I would have new powers, but it is dedicated to monitor my heart function. Alas. The Procedure My heart has not been beating well. So they had to […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Cybermen: You will be upgraded.<br/>
Cyberman: Upgrading is compulsory.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p>
I am now carrying a full power CPU embedded in my <a href="https://en.wikipedia.org/wiki/Cyberman">chest</a>. If I could use it directly I would have new powers, but it is dedicated to monitor my heart function. Alas.<br/>
<a href="https://rjlipton.wpcomstaging.com/2022/03/05/i-am-now-a-cyberman/bed/" rel="attachment wp-att-19722"><img alt="" class="aligncenter size-medium wp-image-19722" height="300" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/bed.png?resize=285%2C300&amp;ssl=1" width="285"/></a></p>
<p/><h2> The Procedure </h2><p/>
<p/><p>
My heart has not been beating well. So they had to implant a device that monitors the heart and also can restart the beating if needed. It was not a long operation—about one hour—but took two days recovering in the hospital. </p>
<p>
The worst part of the recovery is that it is not restful. And it is boring. I am now home and resting much better.</p>
<p>
</p><p/><h2> Am I Turing Universal? </h2><p/>
<p/><p>
Well, according to Alan Turing, I was universal as soon as I began to use my brain. But can I leverage my new processing power?</p>
<p>
The doctors told Kathryn that some ideas from DNA computing went into my <a href="https://en.wikipedia.org/wiki/Implantable_cardioverter-defibrillator">ICD</a> device. Of course, I cannot implement any DNA computing algorithms on my device. </p>
<p>
I wonder, however, whether I can get useful outputs by varying the input—such as my pulse. Maybe I do not want to experiment on myself there. But it is not too far-fetched to imagine that in the not too distant future many of us could be controlling computations with our heartbeats. Or at least powering them while controlling with brainwaves. Call it “heartbeat computing”—?</p>
<p>
Ken has a more immediate concern: could the computing power in the device be used to cheat at chess? Already many chess tournament prohibit electronic physio devices, even watches, because of this potential. A cognitive <em>implant</em>, however, could bend the rules on what it means to be a human player.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I want to thank all who sent some best wishes. I really thank you all for your thoughts. </p>
<p>
I must also especially thank my dear wife Dr.Kathryn Farley for her tremendous support. And also thank Ken for helping her make some calls on our behalf. Thank you both.</p>
<p/></font></font></div>
    </content>
    <updated>2022-03-05T17:06:11Z</updated>
    <published>2022-03-05T17:06:11Z</published>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="cognitive implants"/>
    <category term="Cyberman"/>
    <category term="ICD"/>
    <category term="personal news"/>
    <category term="Richard Lipton"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-03-16T05:37:32Z</updated>
    </source>
  </entry>
</feed>
