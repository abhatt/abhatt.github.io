<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-07-15T04:39:05Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06876</id>
    <link href="http://arxiv.org/abs/2107.06876" rel="alternate" type="text/html"/>
    <title>Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klicpera:Johannes.html">Johannes Klicpera</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lienen:Marten.html">Marten Lienen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=uuml=nnemann:Stephan.html">Stephan Günnemann</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06876">PDF</a><br/><b>Abstract: </b>The current best practice for computing optimal transport (OT) is via entropy
regularization and Sinkhorn iterations. This algorithm runs in quadratic time
as it requires the full pairwise cost matrix, which is prohibitively expensive
for large sets of objects. In this work we propose two effective log-linear
time approximations of the cost matrix: First, a sparse approximation based on
locality-sensitive hashing (LSH) and, second, a Nystr\"om approximation with
LSH-based sparse corrections, which we call locally corrected Nystr\"om (LCN).
These approximations enable general log-linear time algorithms for
entropy-regularized OT that perform well even for the complex, high-dimensional
spaces common in deep learning. We analyse these approximations theoretically
and evaluate them experimentally both directly and end-to-end as a component
for real-world applications. Using our approximations for unsupervised word
embedding alignment enables us to speed up a state-of-the-art method by a
factor of 3 while also improving the accuracy by 3.1 percentage points without
any additional model changes. For graph distance regression we propose the
graph transport network (GTN), which combines graph neural networks (GNNs) with
enhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales
log-linearly in the number of nodes.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06817</id>
    <link href="http://arxiv.org/abs/2107.06817" rel="alternate" type="text/html"/>
    <title>Efficient Set of Vectors Search</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leybovich:Michael.html">Michael Leybovich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shmueli:Oded.html">Oded Shmueli</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06817">PDF</a><br/><b>Abstract: </b>We consider a similarity measure between two sets $A$ and $B$ of vectors,
that balances the average and maximum cosine distance between pairs of vectors,
one from set $A$ and one from set $B$. As a motivation for this measure, we
present lineage tracking in a database. To practically realize this measure, we
need an approximate search algorithm that given a set of vectors $A$ and sets
of vectors $B_1,...,B_n$, the algorithm quickly locates the set $B_i$ that
maximizes the similarity measure. For the case where all sets are singleton
sets, essentially each is a single vector, there are known efficient
approximate search algorithms, e.g., approximated versions of tree search
algorithms, locality-sensitive hashing (LSH), vector quantization (VQ) and
proximity graph algorithms. In this work, we present approximate search
algorithms for the general case. The underlying idea in these algorithms is
encoding a set of vectors via a "long" single vector.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06800</id>
    <link href="http://arxiv.org/abs/2107.06800" rel="alternate" type="text/html"/>
    <title>Signed Barcodes for Multi-Parameter Persistence via Rank Decompositions and Rank-Exact Resolutions</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Botnan:Magnus_Bakke.html">Magnus Bakke Botnan</a>, Steffen Oppermann, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oudot:Steve.html">Steve Oudot</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06800">PDF</a><br/><b>Abstract: </b>In this paper we introduce the signed barcode, a new visual representation of
the global structure of the rank invariant of a multi-parameter persistence
module or, more generally, of a poset representation. Like its unsigned
counterpart in one-parameter persistence, the signed barcode encodes the rank
invariant as a $\mathbb{Z}$-linear combination of rank invariants of indicator
modules supported on segments in the poset. It can also be enriched to encode
the generalized rank invariant as a $\mathbb{Z}$-linear combination of
generalized rank invariants in fixed classes of interval modules. In the paper
we develop the theory behind these rank invariant decompositions, showing under
what conditions they exist and are unique -- so the signed barcode is
canonically defined. We also connect them to the line of work on generalized
persistence diagrams via M\"obius inversions, deriving explicit formulas to
compute a rank decomposition and its associated signed barcode. Finally, we
show that, similarly to its unsigned counterpart, the signed barcode has its
roots in algebra, coming from a projective resolution of the module in some
exact category. To complete the picture, we show some experimental results that
illustrate the contribution of the signed barcode in the exploration of
multi-parameter persistence modules.
</p></div>
    </summary>
    <updated>2021-07-15T00:55:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06715</id>
    <link href="http://arxiv.org/abs/2107.06715" rel="alternate" type="text/html"/>
    <title>ETH Tight Algorithms for Geometric Intersection Graphs: Now in Polynomial Space</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovach:Petr_A=.html">Petr A. Golovach</a>, Tanmay Inamdar, Saket Saurabh <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06715">PDF</a><br/><b>Abstract: </b>De Berg et al. in [SICOMP 2020] gave an algorithmic framework for
subexponential algorithms on geometric graphs with tight (up to ETH) running
times. This framework is based on dynamic programming on graphs of weighted
treewidth resulting in algorithms that use super-polynomial space. We introduce
the notion of weighted treedepth and use it to refine the framework of de Berg
et al. for obtaining polynomial space (with tight running times) on geometric
graphs. As a result, we prove that for any fixed dimension $d \ge 2$ on
intersection graphs of similarly-sized fat objects many well-known graph
problems including Independent Set, $r$-Dominating Set for constant $r$, Cycle
Cover, Hamiltonian Cycle, Hamiltonian Path, Steiner Tree, Connected Vertex
Cover, Feedback Vertex Set, and (Connected) Odd Cycle Transversal are solvable
in time $2^{O(n^{1-1/d})}$ and within polynomial space.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06649</id>
    <link href="http://arxiv.org/abs/2107.06649" rel="alternate" type="text/html"/>
    <title>Polynomial Time Algorithms to Find an Approximate Competitive Equilibrium for Chores</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boodaghians:Shant.html">Shant Boodaghians</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chaudhury:Bhaskar_Ray.html">Bhaskar Ray Chaudhury</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mehta:Ruta.html">Ruta Mehta</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06649">PDF</a><br/><b>Abstract: </b>Competitive equilibrium with equal income (CEEI) is considered one of the
best mechanisms to allocate a set of items among agents fairly and efficiently.
In this paper, we study the computation of CEEI when items are chores that are
disliked (negatively valued) by agents, under 1-homogeneous and concave utility
functions which includes linear functions as a subcase. It is well-known that,
even with linear utilities, the set of CEEI may be non-convex and disconnected,
and the problem is PPAD-hard in the more general exchange model. In contrast to
these negative results, we design FPTAS: A polynomial-time algorithm to compute
$\epsilon$-approximate CEEI where the running-time depends polynomially on
$1/\epsilon$.
</p>
<p>Our algorithm relies on the recent characterization due to Bogomolnaia et
al.~(2017) of the CEEI set as exactly the KKT points of a non-convex
minimization problem that have all coordinates non-zero. Due to this non-zero
constraint, naive gradient-based methods fail to find the desired local minima
as they are attracted towards zero. We develop an exterior-point method that
alternates between guessing non-zero KKT points and maximizing the objective
along supporting hyperplanes at these points. We show that this procedure must
converge quickly to an approximate KKT point which then can be mapped to an
approximate CEEI; this exterior point method may be of independent interest.
When utility functions are linear, we give explicit procedures for finding the
exact iterates, and as a result show that a stronger form of approximate CEEI
can be found in polynomial time. Finally, we note that our algorithm extends to
the setting of un-equal incomes (CE), and to mixed manna with linear utilities
where each agent may like (positively value) some items and dislike (negatively
value) others.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06626</id>
    <link href="http://arxiv.org/abs/2107.06626" rel="alternate" type="text/html"/>
    <title>Optimality of the Johnson-Lindenstrauss Dimensionality Reduction for Practical Measures</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bartal:Yair.html">Yair Bartal</a>, Ora Nova Fandina, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Larsen:Kasper_Green.html">Kasper Green Larsen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06626">PDF</a><br/><b>Abstract: </b>It is well known that the Johnson-Lindenstrauss dimensionality reduction
method is optimal for worst case distortion. While in practice many other
methods and heuristics are used, not much is known in terms of bounds on their
performance. The question of whether the JL method is optimal for practical
measures of distortion was recently raised in \cite{BFN19} (NeurIPS'19). They
provided upper bounds on its quality for a wide range of practical measures and
showed that indeed these are best possible in many cases. Yet, some of the most
important cases, including the fundamental case of average distortion were left
open. In particular, they show that the JL transform has $1+\epsilon$ average
distortion for embedding into $k$-dimensional Euclidean space, where
$k=O(1/\eps^2)$, and for more general $q$-norms of distortion, $k =
O(\max\{1/\eps^2,q/\eps\})$, whereas tight lower bounds were established only
for large values of $q$ via reduction to the worst case.
</p>
<p>In this paper we prove that these bounds are best possible for any
dimensionality reduction method, for any $1 \leq q \leq O(\frac{\log (2\eps^2
n)}{\eps})$ and $\epsilon \geq \frac{1}{\sqrt{n}}$, where $n$ is the size of
the subset of Euclidean space.
</p>
<p>Our results imply that the JL method is optimal for various distortion
measures commonly used in practice, such as {\it stress, energy} and {\it
relative error}. We prove that if any of these measures is bounded by $\eps$
then $k=\Omega(1/\eps^2)$, for any $\epsilon \geq \frac{1}{\sqrt{n}}$, matching
the upper bounds of \cite{BFN19} and extending their tightness results for the
full range moment analysis.
</p>
<p>Our results may indicate that the JL dimensionality reduction method should
be considered more often in practical applications, and the bounds we provide
for its quality should be served as a measure for comparison when evaluating
the performance of other methods and heuristics.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06615</id>
    <link href="http://arxiv.org/abs/2107.06615" rel="alternate" type="text/html"/>
    <title>Oblivious sketching for logistic regression</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munteanu:Alexander.html">Alexander Munteanu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Omlor:Simon.html">Simon Omlor</a>, David Woodruff <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06615">PDF</a><br/><b>Abstract: </b>What guarantees are possible for solving logistic regression in one pass over
a data stream? To answer this question, we present the first data oblivious
sketch for logistic regression. Our sketch can be computed in input sparsity
time over a turnstile data stream and reduces the size of a $d$-dimensional
data set from $n$ to only $\operatorname{poly}(\mu d\log n)$ weighted points,
where $\mu$ is a useful parameter which captures the complexity of compressing
the data. Solving (weighted) logistic regression on the sketch gives an $O(\log
n)$-approximation to the original problem on the full data set. We also show
how to obtain an $O(1)$-approximation with slight modifications. Our sketches
are fast, simple, easy to implement, and our experiments demonstrate their
practicality.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06582</id>
    <link href="http://arxiv.org/abs/2107.06582" rel="alternate" type="text/html"/>
    <title>Towards a Decomposition-Optimal Algorithm for Counting and Sampling Arbitrary Motifs in Sublinear Time</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biswas:Amartya_Shankha.html">Amartya Shankha Biswas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eden:Talya.html">Talya Eden</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rubinfeld:Ronitt.html">Ronitt Rubinfeld</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06582">PDF</a><br/><b>Abstract: </b>We consider the problem of sampling an arbitrary given motif $H$ in a graph
$G$, where access to $G$ is given via queries: degree, neighbor, and pair, as
well as uniform edge sample queries. Previous algorithms for the uniform
sampling task were based on a decomposition of $H$ into a collection of odd
cycles and stars, denoted $\mathcal{D}^*(H)=\{O_{k_1}, \ldots, O_{k_q},
S_{p_1}, \ldots, S_{p_\ell}\}$. These algorithms were shown to be optimal for
the case where $H$ is a clique or an odd-length cycle, but no other lower
bounds were known.
</p>
<p>We present a new algorithm for sampling arbitrary motifs which, up to
$\poly(\log n)$ factors, for any motif $H$ whose decomposition contains at
least two components or at least one star, is always preferable. The main
ingredient leading to this improvement is an improved uniform algorithm for
sampling stars, which might be of independent interest, as it allows to sample
vertices according to the $p$-th moment of the degree distribution. We further
show how to use our sampling algorithm to get an approximate counting
algorithm, with essentially the same complexity.
</p>
<p>Finally, we prove that this algorithm is \emph{decomposition-optimal} for
decompositions that contain at least one odd cycle. That is, we prove that for
any decomposition $D$ that contains at least one odd cycle, there exists a
motif $H_{D}$ with decomposition $D$, and a family of graphs $\mathcal{G}$, so
that in order to output a uniform copy of $H$ in a uniformly chosen graph in
$\mathcal{G}$, the number of required queries matches our upper bound. These
are the first lower bounds for motifs $H$ with a nontrivial decomposition,
i.e., motifs that have more than a single component in their decomposition.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06571</id>
    <link href="http://arxiv.org/abs/2107.06571" rel="alternate" type="text/html"/>
    <title>A QPTAS for stabbing rectangles</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eisenbrand:Friedrich.html">Friedrich Eisenbrand</a>, Martina Gallato, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svensson:Ola.html">Ola Svensson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Venzin:Moritz.html">Moritz Venzin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06571">PDF</a><br/><b>Abstract: </b>We consider the following geometric optimization problem: Given $ n $
axis-aligned rectangles in the plane, the goal is to find a set of horizontal
segments of minimum total length such that each rectangle is stabbed. A segment
stabs a rectangle if it intersects both its left and right edge. As such, this
stabbing problem falls into the category of weighted geometric set cover
problems for which techniques that improve upon the general ${\Theta}(\log
n)$-approximation guarantee have received a lot of attention in the literature.
Chan at al. (2018) have shown that rectangle stabbing is NP-hard and that it
admits a constant-factor approximation algorithm based on Varadarajan's
quasi-uniform sampling method. In this work we make progress on rectangle
stabbing on two fronts. First, we present a quasi-polynomial time approximation
scheme (QPTAS) for rectangle stabbing. Furthermore, we provide a simple
$8$-approximation algorithm that avoids the framework of Varadarajan. This
settles two open problems raised by Chan et al. (2018).
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06490</id>
    <link href="http://arxiv.org/abs/2107.06490" rel="alternate" type="text/html"/>
    <title>Greedy Spanners in Euclidean Spaces Admit Sublinear Separators</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Le:Hung.html">Hung Le</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Than:Cuong.html">Cuong Than</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06490">PDF</a><br/><b>Abstract: </b>The greedy spanner in a low dimensional Euclidean space is a fundamental
geometric construction that has been extensively studied over three decades as
it possesses the two most basic properties of a good spanner: constant maximum
degree and constant lightness. Recently, Eppstein and Khodabandeh showed that
the greedy spanner in $\mathbb{R}^2$ admits a sublinear separator in a strong
sense: any subgraph of $k$ vertices of the greedy spanner in $\mathbb{R}^2$ has
a separator of size $O(\sqrt{k})$. Their technique is inherently planar and is
not extensible to higher dimensions. They left showing the existence of a small
separator for the greedy spanner in $\mathbb{R}^d$ for any constant $d\geq 3$
as an open problem. In this paper, we resolve the problem of Eppstein and
Khodabandeh by showing that any subgraph of $k$ vertices of the greedy spanner
in $\mathbb{R}^d$ has a separator of size $O(k^{1-1/d})$. We introduce a new
technique that gives a simple characterization for any geometric graph to have
a sublinear separator that we dub $\tau$-lanky: a geometric graph is
$\tau$-lanky if any ball of radius $r$ cuts at most $\tau$ edges of length at
least $r$ in the graph. We show that any $\tau$-lanky geometric graph of $n$
vertices in $\mathbb{R}^d$ has a separator of size $O(\tau n^{1-1/d})$. We then
derive our main result by showing that the greedy spanner is $O(1)$-lanky. We
indeed obtain a more general result that applies to unit ball graphs and point
sets of low fractal dimensions in $\mathbb{R}^d$. Our technique naturally
extends to doubling metrics. We use the $\tau$-lanky characterization to show
that there exists a $(1+\epsilon)$-spanner for doubling metrics of dimension
$d$ with a constant maximum degree and a separator of size
$O(n^{1-\frac{1}{d}})$; this result resolves an open problem posed by Abam and
Har-Peled a decade ago.
</p></div>
    </summary>
    <updated>2021-07-15T00:56:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06406</id>
    <link href="http://arxiv.org/abs/2107.06406" rel="alternate" type="text/html"/>
    <title>A Theoretical Framework for Learning from Quantum Data</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heidari:Mohsen.html">Mohsen Heidari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Padakandla:Arun.html">Arun Padakandla</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Szpankowski:Wojciech.html">Wojciech Szpankowski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06406">PDF</a><br/><b>Abstract: </b>Over decades traditional information theory of source and channel coding
advances toward learning and effective extraction of information from data. We
propose to go one step further and offer a theoretical foundation for learning
classical patterns from quantum data. However, there are several roadblocks to
lay the groundwork for such a generalization. First, classical data must be
replaced by a density operator over a Hilbert space. Hence, deviated from
problems such as state tomography, our samples are i.i.d density operators. The
second challenge is even more profound since we must realize that our only
interaction with a quantum state is through a measurement which -- due to
no-cloning quantum postulate -- loses information after measuring it. With this
in mind, we present a quantum counterpart of the well-known PAC framework.
Based on that, we propose a quantum analogous of the ERM algorithm for learning
measurement hypothesis classes. Then, we establish upper bounds on the quantum
sample complexity quantum concept classes.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06399</id>
    <link href="http://arxiv.org/abs/2107.06399" rel="alternate" type="text/html"/>
    <title>The Perfect Matching Cut Problem Revisited</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Le:Van_Bang.html">Van Bang Le</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Telle:Jan_Arne.html">Jan Arne Telle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06399">PDF</a><br/><b>Abstract: </b>In a graph, a perfect matching cut is an edge cut that is a perfect matching.
Perfect Matching Cut (PMC) is the problem of deciding whether a given graph has
a perfect matching cut, and is known to be NP-complete. We revisit the problem
and show that PMC remains NP-complete when restricted to bipartite graphs of
maximum degree 3 and arbitrarily large girth. Complementing this hardness
result, we give two graph classes in which PMC is polynomial time solvable. The
first one includes claw-free graphs and graphs without an induced path on five
vertices, the second one properly contains all chordal graphs. Assuming the
Exponential Time Hypothesis, we show there is no $O^*(2^{o(n)})$-time algorithm
for PMC even when restricted to $n$-vertex bipartite graphs, and also show that
PMC can be solved in $O^*(1.2721^n)$ time by means of an exact branching
algorithm.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06259</id>
    <link href="http://arxiv.org/abs/2107.06259" rel="alternate" type="text/html"/>
    <title>Robust Learning of Optimal Auctions</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guo:Wenshuo.html">Wenshuo Guo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jordan:Michael_I=.html">Michael I. Jordan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zampetakis:Manolis.html">Manolis Zampetakis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06259">PDF</a><br/><b>Abstract: </b>We study the problem of learning revenue-optimal multi-bidder auctions from
samples when the samples of bidders' valuations can be adversarially corrupted
or drawn from distributions that are adversarially perturbed. First, we prove
tight upper bounds on the revenue we can obtain with a corrupted distribution
under a population model, for both regular valuation distributions and
distributions with monotone hazard rate (MHR). We then propose new algorithms
that, given only an ``approximate distribution'' for the bidder's valuation,
can learn a mechanism whose revenue is nearly optimal simultaneously for all
``true distributions'' that are $\alpha$-close to the original distribution in
Kolmogorov-Smirnov distance. The proposed algorithms operate beyond the setting
of bounded distributions that have been studied in prior works, and are
guaranteed to obtain a fraction $1-O(\alpha)$ of the optimal revenue under the
true distribution when the distributions are MHR. Moreover, they are guaranteed
to yield at least a fraction $1-O(\sqrt{\alpha})$ of the optimal revenue when
the distributions are regular. We prove that these upper bounds cannot be
further improved, by providing matching lower bounds. Lastly, we derive sample
complexity upper bounds for learning a near-optimal auction for both MHR and
regular distributions.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06236</id>
    <link href="http://arxiv.org/abs/2107.06236" rel="alternate" type="text/html"/>
    <title>An FPT algorithm for the embeddability of graphs into two-dimensional simplicial complexes</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Verdi=egrave=re:=Eacute=ric_Colin_de.html">Éric Colin de Verdière</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Magnard:Thomas.html">Thomas Magnard</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06236">PDF</a><br/><b>Abstract: </b>We consider the embeddability problem of a graph G into a two-dimensional
simplicial complex C: Given G and C, decide whether G admits a topological
embedding into C. The problem is NP-hard, even in the restricted case where C
is homeomorphic to a surface.
</p>
<p>It is known that the problem admits an algorithm with running time
f(c).n^{O(c)}, where n is the size of the graph G and c is the size of the
two-dimensional complex C. In other words, that algorithm is polynomial when C
is fixed, but the degree of the polynomial depends on C. We prove that the
problem is fixed-parameter tractable in the size of the two-dimensional
complex, by providing a deterministic f(c).n^3-time algorithm. We also provide
a randomized algorithm with expected running time 2^{c^{O(1)}}.n^{O(1)}.
</p>
<p>Our approach is to reduce to the case where G has bounded branchwidth via an
irrelevant vertex method, and to apply dynamic programming. We do not rely on
any component of the existing linear-time algorithms for embedding graphs on a
fixed surface; the only elaborated tool that we use is an algorithm to compute
grid minors.
</p></div>
    </summary>
    <updated>2021-07-15T00:09:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06232</id>
    <link href="http://arxiv.org/abs/2107.06232" rel="alternate" type="text/html"/>
    <title>Maintaining $\mathsf{CMSO}_2$ properties on dynamic structures with bounded feedback vertex number</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Majewski:Konrad.html">Konrad Majewski</a>, Michał Pilipczuk, Marek Sokołowski <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06232">PDF</a><br/><b>Abstract: </b>Let $\varphi$ be a sentence of $\mathsf{CMSO}_2$ (monadic second-order logic
with quantification over edge subsets and counting modular predicates) over the
signature of graphs. We present a dynamic data structure that for a given graph
$G$ that is updated by edge insertions and edge deletions, maintains whether
$\varphi$ is satisfied in $G$. The data structure is required to correctly
report the outcome only when the feedback vertex number of $G$ does not exceed
a fixed constant $k$, otherwise it reports that the feedback vertex number is
too large. With this assumption, we guarantee amortized update time ${\cal
O}_{\varphi,k}(\log n)$.
</p>
<p>By combining this result with a classic theorem of Erd\H{o}s and P\'osa, we
give a fully dynamic data structure that maintains whether a graph contains a
packing of $k$ vertex-disjoint cycles with amortized update time ${\cal
O}_{k}(\log n)$. Our data structure also works in a larger generality of
relational structures over binary signatures.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06130</id>
    <link href="http://arxiv.org/abs/2107.06130" rel="alternate" type="text/html"/>
    <title>Scalable Surface Reconstruction with Delaunay-Graph Neural Networks</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Raphael Sulzer, Loic Landrieu, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marlet:Renaud.html">Renaud Marlet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vallet:Bruno.html">Bruno Vallet</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06130">PDF</a><br/><b>Abstract: </b>We introduce a novel learning-based, visibility-aware, surface reconstruction
method for large-scale, defect-laden point clouds. Our approach can cope with
the scale and variety of point cloud defects encountered in real-life
Multi-View Stereo (MVS) acquisitions. Our method relies on a 3D Delaunay
tetrahedralization whose cells are classified as inside or outside the surface
by a graph neural network and an energy model solvable with a graph cut. Our
model, making use of both local geometric attributes and line-of-sight
visibility information, is able to learn a visibility model from a small amount
of synthetic training data and generalizes to real-life acquisitions. Combining
the efficiency of deep learning methods and the scalability of energy based
models, our approach outperforms both learning and non learning-based
reconstruction algorithms on two publicly available reconstruction benchmarks.
</p></div>
    </summary>
    <updated>2021-07-15T00:10:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06111</id>
    <link href="http://arxiv.org/abs/2107.06111" rel="alternate" type="text/html"/>
    <title>Towards exact structural thresholds for parameterized complexity</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hegerfeld:Falko.html">Falko Hegerfeld</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kratsch:Stefan.html">Stefan Kratsch</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06111">PDF</a><br/><b>Abstract: </b>Parameterized complexity seeks to use input structure to obtain faster
algorithms for NP-hard problems. This has been most successful for graphs of
low treewidth: Many problems admit fast algorithms relative to treewidth and
many of them are optimal under SETH. Fewer such results are known for more
general structure such as low clique-width and more restrictive structure such
as low deletion distance to a sparse graph class.
</p>
<p>Despite these successes, such results remain "islands'' within the realm of
possible structure. Rather than adding more islands, we seek to determine the
transitions between them, that is, we aim for structural thresholds where the
complexity increases as input structure becomes more general. Going from
deletion distance to treewidth, is a single deletion set to a graph with simple
components enough to yield the same lower bound as for treewidth or does it
take many disjoint separators? Going from treewidth to clique-width, how much
more density entails the same complexity as clique-width? Conversely, what is
the most restrictive structure that yields the same lower bound?
</p>
<p>For treewidth, we obtain both refined and new lower bounds that apply already
to graphs with a single separator $X$ such that $G-X$ has treewidth $r=O(1)$,
while $G$ has treewidth $|X|+O(1)$. We rule out algorithms running in time
$O^*((r+1-\epsilon)^{k})$ for Deletion to $r$-Colorable parameterized by
$k=|X|$. For clique-width, we rule out time $O^*((2^r-\epsilon)^k)$ for
Deletion to $r$-Colorable, where $X$ is now allowed to consist of $k$
twinclasses. There are further results on Vertex Cover, Dominating Set and
Maximum Cut. All lower bounds are matched by existing and newly designed
algorithms.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05972</id>
    <link href="http://arxiv.org/abs/2107.05972" rel="alternate" type="text/html"/>
    <title>Polynomial delay algorithm for minimal chordal completions</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brosse:Caroline.html">Caroline Brosse</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Limouzy:Vincent.html">Vincent Limouzy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mary:Arnaud.html">Arnaud Mary</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05972">PDF</a><br/><b>Abstract: </b>Motivated by the problem of enumerating all tree decompositions of a graph,
we consider in this article the problem of listing all the minimal chordal
completions of a graph. In \cite{carmeli2020} (\textsc{Pods 2017}) Carmeli
\emph{et al.} proved that all minimal chordal completions or equivalently all
proper tree decompositions of a graph can be listed in incremental polynomial
time using exponential space. The total running time of their algorithm is
quadratic in the number of solutions and the existence of an algorithm whose
complexity depends only linearly on the number of solutions remained open. We
close this question by providing a polynomial delay algorithm to solve this
problem which, moreover, uses polynomial space.
</p>
<p>Our algorithm relies on \emph{Proximity Search}, a framework recently
introduced by Conte \emph{et al.} \cite{conte-uno2019} (\textsc{Stoc 2019})
which has been shown powerful to obtain polynomial delay algorithms, but
generally requires exponential space. In order to obtain a polynomial space
algorithm for our problem, we introduce a new general method called
\emph{canonical path reconstruction} to design polynomial delay and polynomial
space algorithms based on proximity search.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05895</id>
    <link href="http://arxiv.org/abs/2107.05895" rel="alternate" type="text/html"/>
    <title>A Theory of Spherical Diagrams</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Viglietta:Giovanni.html">Giovanni Viglietta</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05895">PDF</a><br/><b>Abstract: </b>We introduce the axiomatic theory of Spherical Occlusion Diagrams as a tool
to study certain combinatorial properties of polyhedra in $\mathbb R^3$, which
are of central interest in the context Art Gallery problems for polyhedra and
other visibility-related problems in discrete and computational geometry.
</p></div>
    </summary>
    <updated>2021-07-15T00:10:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05793</id>
    <link href="http://arxiv.org/abs/2107.05793" rel="alternate" type="text/html"/>
    <title>A Parallel Approximation Algorithm for Maximizing Submodular $b$-Matching</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>S M Ferdous, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pothen:Alex.html">Alex Pothen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khan:Arif.html">Arif Khan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panyala:Ajay.html">Ajay Panyala</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Halappanavar:Mahantesh.html">Mahantesh Halappanavar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05793">PDF</a><br/><b>Abstract: </b>We design new serial and parallel approximation algorithms for computing a
maximum weight $b$-matching in an edge-weighted graph with a submodular
objective function. This problem is NP-hard; the new algorithms have
approximation ratio $1/3$, and are relaxations of the Greedy algorithm that
rely only on local information in the graph, making them parallelizable. We
have designed and implemented Local Lazy Greedy algorithms for both serial and
parallel computers. We have applied the approximate submodular $b$-matching
algorithm to assign tasks to processors in the computation of Fock matrices in
quantum chemistry on parallel computers. The assignment seeks to reduce the run
time by balancing the computational load on the processors and bounding the
number of messages that each processor sends. We show that the new assignment
of tasks to processors provides a four fold speedup over the currently used
assignment in the NWChemEx software on $8000$ processors on the Summit
supercomputer at Oak Ridge National Lab.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05753</id>
    <link href="http://arxiv.org/abs/2107.05753" rel="alternate" type="text/html"/>
    <title>Noisy searching: simple, fast and correct</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dereniowski:Dariusz.html">Dariusz Dereniowski</a>, Aleksander Łukasiewicz, Przemysław Uznański <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05753">PDF</a><br/><b>Abstract: </b>This work revisits the multiplicative weights update technique (MWU) which
has a variety of applications, especially in learning and searching algorithms.
In particular, the Bayesian update method is a well known version of MWU that
is particularly applicable for the problem of searching in a given domain. An
ideal scenario for that method is when the input distribution is known a priori
and each single update maximizes the information gain. In this work we consider
two search domains - linear orders (sorted arrays) and graphs, where the aim of
the search is to locate an unknown target by performing as few queries as
possible. Searching such domains is well understood when each query provides a
correct answer and the input target distribution is uniform. Hence, we consider
two generalizations: the noisy search both with arbitrary and adversarial
(i.e., unknown) target distributions. We obtain several results providing full
characterization of the query complexities in the three settings: adversarial
Monte Carlo, adversarial Las Vegas and distributional Las Vegas. Our algorithms
either improve, simplify or patch earlier ambiguities in the literature - see
the works of Emamjomeh-Zadeh et al. [STOC 2016], Dereniowski et. al. [SOSA@SODA
2019] and Ben-Or and Hassidim [FOCS 2008]. In particular, all algorithms give
strategies that provide the optimal number of queries up to lower-order terms.
Our technical contribution lies in providing generic search techniques that are
able to deal with the fact that, in general, queries guarantee only suboptimal
information gain.
</p></div>
    </summary>
    <updated>2021-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05746</id>
    <link href="http://arxiv.org/abs/2107.05746" rel="alternate" type="text/html"/>
    <title>Computational Hardness of the Hylland-Zeckhauser Scheme</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Thomas.html">Thomas Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Xi.html">Xi Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Binghui.html">Binghui Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yannakakis:Mihalis.html">Mihalis Yannakakis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05746">PDF</a><br/><b>Abstract: </b>We study the complexity of the classic Hylland-Zeckhauser scheme [HZ'79] for
one-sided matching markets. We show that the problem of finding an
$\epsilon$-approximate equilibrium in the HZ scheme is PPAD-hard, and this
holds even when $\epsilon$ is polynomially small and when each agent has no
more than four distinct utility values. Our hardness result, when combined with
the PPAD membership result of [VY'21], resolves the approximation complexity of
the HZ scheme. We also show that the problem of approximating the optimal
social welfare (the weight of the matching) achievable by HZ equilibria within
a certain constant factor is NP-hard.
</p></div>
    </summary>
    <updated>2021-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05717</id>
    <link href="http://arxiv.org/abs/2107.05717" rel="alternate" type="text/html"/>
    <title>Sparsifying, Shrinking and Splicing for Minimum Path Cover in Parameterized Linear Time</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/C=aacute=ceres:Manuel.html">Manuel Cáceres</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cairo:Massimo.html">Massimo Cairo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mumey:Brendan.html">Brendan Mumey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rizzi:Romeo.html">Romeo Rizzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tomescu:Alexandru_I=.html">Alexandru I. Tomescu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05717">PDF</a><br/><b>Abstract: </b>A minimum path cover (MPC) of a directed acyclic graph (DAG) $G = (V,E)$ is a
minimum-size set of paths that together cover all the vertices of the DAG.
Computing an MPC is a basic polynomial problem, dating back to Dilworth's and
Fulkerson's results in the 1950s. Since the size $k$ of an MPC (also known as
the width) can be small in practical applications, research has also studied
algorithms whose complexity is parameterized on $k$. We obtain two new MPC
parameterized algorithms for DAGs running in time $O(k^2|V|\log{|V|} + |E|)$
and $O(k^3|V| + |E|)$. We also obtain a parallel algorithm running in $O(k^2|V|
+ |E|)$ parallel steps and using $O(\log{|V|})$ processors (in the PRAM model).
Our latter two algorithms are the first solving the problem in parameterized
linear time. Finally, we present an algorithm running in time $O(k^2|V|)$ for
transforming any MPC to another MPC using less than $2|V|$ distinct edges,
which we prove to be asymptotically tight. As such, we also obtain edge
sparsification algorithms preserving the width of the DAG with the same running
time as our MPC algorithms. At the core of all our algorithms we interleave the
usage of three techniques: transitive sparsification, shrinking of a path
cover, and the splicing of a set of paths along a given path.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05690</id>
    <link href="http://arxiv.org/abs/2107.05690" rel="alternate" type="text/html"/>
    <title>Worst-Case Welfare of Item Pricing in the Tollbooth Problem</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Zihan.html">Zihan Tan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Teng:Yifeng.html">Yifeng Teng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Mingfei.html">Mingfei Zhao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05690">PDF</a><br/><b>Abstract: </b>We study the worst-case welfare of item pricing in the tollbooth problem. The
problem was first introduced by Guruswami et al, and is a special case of the
combinatorial auction in which (i) each of the $m$ items in the auction is an
edge of some underlying graph; and (ii) each of the $n$ buyers is single-minded
and only interested in buying all edges of a single path. We consider the
competitive ratio between the hindsight optimal welfare and the optimal
worst-case welfare among all item-pricing mechanisms, when the order of the
arriving buyers is adversarial. On the one hand, we prove an $\Omega(m^{1/8})$
lower bound of the competitive ratio for general graphs. We show that an
$m^{\Omega(1)}$ competitive ratio is unavoidable even if the graph is a grid,
or if the capacity of every edge is augmented by a constant $c$. On the other
hand, we study the competitive ratio for special families of graphs. In
particular, we improve the ratio when the input graph $G$ is a tree, from 8
(proved by Cheung and Swamy) to 3. We prove that the ratio is $2$ (tight) when
$G$ is a cycle and $O(\log^2 m)$ when $G$ is an outerplanar graph.
</p>
<p>All positive results above require that the seller can choose a proper
tie-breaking rule to maximize the welfare. In the paper we also consider the
setting where the tie-breaking power is on the buyers' side, i.e. the buyer can
choose whether or not to buy her demand path when the total price of edges in
the path equals her value. We show that the gap between the two settings is at
least a constant even when the underlying graph is a single path (this special
case is also known as the highway problem). Meanwhile, in this setting where
buyers have the tie-breaking power, we also prove an $O(1)$ upper bound of
competitive ratio for special families of graphs.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06261</id>
    <link href="http://arxiv.org/abs/2107.06261" rel="alternate" type="text/html"/>
    <title>Tight running times for minimum $\ell_q$-norm load balancing: beyond exponential dependencies on $1/\epsilon$</title>
    <feedworld_mtime>1626220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Lin.html">Lin Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tao:Liangde.html">Liangde Tao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Verschae:Jos=eacute=.html">José Verschae</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06261">PDF</a><br/><b>Abstract: </b>We consider a classical scheduling problem on $m$ identical machines. For an
arbitrary constant $q&gt;1$, the aim is to assign jobs to machines such that
$\sum_{i=1}^m C_i^q$ is minimized, where $C_i$ is the total processing time of
jobs assigned to machine $i$. It is well known that this problem is strongly
NP-hard.
</p>
<p>Under mild assumptions, the running time of an $(1+\epsilon)$-approximation
algorithm for a strongly NP-hard problem cannot be polynomial on $1/\epsilon$,
unless $\text{P}=\text{NP}$. For most problems in the literature, this
translates into algorithms with running time at least as large as
$2^{\Omega(1/\varepsilon)}+n^{O(1)}$. For the natural scheduling problem above,
we establish the existence of an algorithm which violates this threshold. More
precisely, we design a PTAS that runs in
$2^{\tilde{O}(\sqrt{1/\epsilon})}+n^{O(1)}$ time. This result is in sharp
contrast to the closely related minimum makespan variant, where an exponential
lower bound is known under the exponential time hypothesis (ETH). We complement
our result with an essentially matching lower bound on the running time,
showing that our algorithm is best-possible under ETH. The lower bound proof
exploits new number-theoretical constructions for variants of progression-free
sets, which might be of independent interest.
</p>
<p>Furthermore, we provide a fine-grained characterization on the running time
of a PTAS for this problem depending on the relation between $\epsilon$ and the
number of machines $m$. More precisely, our lower bound only holds when
$m=\Theta(\sqrt{1/\epsilon})$. Better algorithms, that go beyond the lower
bound, exist for other values of $m$. In particular, there even exists an
algorithm with running time polynomial in $1/\epsilon$ if we restrict ourselves
to instances with $m=\Omega(1/\epsilon\log^21/\epsilon)$.
</p></div>
    </summary>
    <updated>2021-07-14T22:45:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06216</id>
    <link href="http://arxiv.org/abs/2107.06216" rel="alternate" type="text/html"/>
    <title>Bag-of-Tasks Scheduling on Related Machines</title>
    <feedworld_mtime>1626220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Amit.html">Amit Kumar</a>, Sahil Singla <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06216">PDF</a><br/><b>Abstract: </b>We consider online scheduling to minimize weighted completion time on related
machines, where each job consists of several tasks that can be concurrently
executed. A job gets completed when all its component tasks finish. We obtain
an $O(K^3 \log^2 K)$-competitive algorithm in the non-clairvoyant setting,
where $K$ denotes the number of distinct machine speeds. The analysis is based
on dual-fitting on a precedence-constrained LP relaxation that may be of
independent interest.
</p></div>
    </summary>
    <updated>2021-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06156</id>
    <link href="http://arxiv.org/abs/2107.06156" rel="alternate" type="text/html"/>
    <title>Parallel Repetition for the GHZ Game: A Simpler Proof</title>
    <feedworld_mtime>1626220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Girish:Uma.html">Uma Girish</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Holmgren:Justin.html">Justin Holmgren</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mittal:Kunal.html">Kunal Mittal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raz:Ran.html">Ran Raz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhan:Wei.html">Wei Zhan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06156">PDF</a><br/><b>Abstract: </b>We give a new proof of the fact that the parallel repetition of the
(3-player) GHZ game reduces the value of the game to zero polynomially quickly.
That is, we show that the value of the $n$-fold GHZ game is at most
$n^{-\Omega(1)}$. This was first established by Holmgren and Raz [HR20]. We
present a new proof of this theorem that we believe to be simpler and more
direct. Unlike most previous works on parallel repetition, our proof makes no
use of information theory, and relies on the use of Fourier analysis.
</p>
<p>The GHZ game [GHZ89] has played a foundational role in the understanding of
quantum information theory, due in part to the fact that quantum strategies can
win the GHZ game with probability 1. It is possible that improved parallel
repetition bounds may find applications in this setting.
</p>
<p>Recently, Dinur, Harsha, Venkat, and Yuen [DHVY17] highlighted the GHZ game
as a simple three-player game, which is in some sense maximally far from the
class of multi-player games whose behavior under parallel repetition is well
understood. Dinur et al. conjectured that parallel repetition decreases the
value of the GHZ game exponentially quickly, and speculated that progress on
proving this would shed light on parallel repetition for general multi-player
(multi-prover) games.
</p></div>
    </summary>
    <updated>2021-07-14T22:37:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06121</id>
    <link href="http://arxiv.org/abs/2107.06121" rel="alternate" type="text/html"/>
    <title>The Dynamic Complexity of Acyclic Hypergraph Homomorphisms</title>
    <feedworld_mtime>1626220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vortmeier:Nils.html">Nils Vortmeier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kokkinis:Ioannis.html">Ioannis Kokkinis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06121">PDF</a><br/><b>Abstract: </b>Finding a homomorphism from some hypergraph $\mathcal{Q}$ (or some relational
structure) to another hypergraph $\mathcal{D}$ is a fundamental problem in
computer science. We show that an answer to this problem can be maintained
under single-edge changes of $\mathcal{Q}$, as long as it stays acyclic, in the
DynFO framework of Patnaik and Immerman that uses updates expressed in
first-order logic. If additionally also changes of $\mathcal{D}$ are allowed,
we show that it is unlikely that existence of homomorphisms can be maintained
in DynFO.
</p></div>
    </summary>
    <updated>2021-07-14T22:44:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05886</id>
    <link href="http://arxiv.org/abs/2107.05886" rel="alternate" type="text/html"/>
    <title>Promise Constraint Satisfaction and Width</title>
    <feedworld_mtime>1626220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Atserias:Albert.html">Albert Atserias</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dalmau:V=iacute=ctor.html">Víctor Dalmau</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05886">PDF</a><br/><b>Abstract: </b>We study the power of the bounded-width consistency algorithm in the context
of the fixed-template Promise Constraint Satisfaction Problem (PCSP). Our main
technical finding is that the template of every PCSP that is solvable in
bounded width satisfies a certain structural condition implying that its
algebraic closure-properties include weak near unanimity polymorphisms of all
large arities. While this parallels the standard (non-promise) CSP theory, the
method of proof is quite different and applies even to the regime of sublinear
width. We also show that, in contrast with the CSP world, the presence of weak
near unanimity polymorphisms of all large arities does not guarantee
solvability in bounded width. The separating example is even solvable in the
second level of the Sherali-Adams (SA) hierarchy of linear programming
relaxations. This shows that, unlike for CSPs, linear programming can be
stronger than bounded width. A direct application of these methods also show
that the problem of $q$-coloring $p$-colorable graphs is not solvable in
bounded or even sublinear width, for any two constants $p$ and $q$ such that $3
\leq p \leq q$. Turning to algorithms, we note that Wigderson's algorithm for
$O(\sqrt{n})$-coloring $3$-colorable graphs with $n$ vertices is implementable
in width $4$. Indeed, by generalizing the method we see that, for any $\epsilon
&gt; 0$ smaller than $1/2$, the optimal width for solving the problem of
$O(n^\epsilon)$-coloring $3$-colorable graphs with $n$ vertices lies between
$n^{1-3\epsilon}$ and $n^{1-2\epsilon}$. The upper bound gives a simple
$2^{\Theta(n^{1-2\epsilon}\log(n))}$-time algorithm that, asymptotically, beats
the straightforward $2^{\Theta(n^{1-\epsilon})}$ bound that follows from
partitioning the graph into $O(n^\epsilon)$ many independent parts each of size
$O(n^{1-\epsilon})$.
</p></div>
    </summary>
    <updated>2021-07-14T22:43:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05822</id>
    <link href="http://arxiv.org/abs/2107.05822" rel="alternate" type="text/html"/>
    <title>Markov Game with Switching Costs</title>
    <feedworld_mtime>1626220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jian.html">Jian Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Daogao.html">Daogao Liu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05822">PDF</a><br/><b>Abstract: </b>We study a general Markov game with metric switching costs: in each round,
the player adaptively chooses one of several Markov chains to advance with the
objective of minimizing the expected cost for at least $k$ chains to reach
their target states. If the player decides to play a different chain, an
additional switching cost is incurred. The special case in which there is no
switching cost was solved optimally by Dumitriu, Tetali, and Winkler [DTW03] by
a variant of the celebrated Gittins Index for the classical multi-armed bandit
(MAB) problem with Markovian rewards [Gittins 74, Gittins79]. However, for
multi-armed bandit (MAB) with nontrivial switching cost, even if the switching
cost is a constant, the classic paper by Banks and Sundaram [BS94] showed that
no index strategy can be optimal.
</p>
<p>In this paper, we complement their result and show there is a simple index
strategy that achieves a constant approximation factor if the switching cost is
constant and $k=1$. To the best of our knowledge, this is the first index
strategy that achieves a constant approximation factor for a general MAB
variant with switching costs. For the general metric, we propose a more
involved constant-factor approximation algorithm, via a nontrivial reduction to
the stochastic $k$-TSP problem, in which a Markov chain is approximated by a
random variable. Our analysis makes extensive use of various interesting
properties of the Gittins index.
</p></div>
    </summary>
    <updated>2021-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05810</id>
    <link href="http://arxiv.org/abs/2107.05810" rel="alternate" type="text/html"/>
    <title>The Element Extraction Problem and the Cost of Determinism and Limited Adaptivity in Linear Queries</title>
    <feedworld_mtime>1626220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakrabarti:Amit.html">Amit Chakrabarti</a>, Manuel Stoeckl <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05810">PDF</a><br/><b>Abstract: </b>Two widely-used computational paradigms for sublinear algorithms are using
linear measurements to perform computations on a high dimensional input and
using structured queries to access a massive input. Typically, algorithms in
the former paradigm are non-adaptive whereas those in the latter are highly
adaptive. This work studies the fundamental search problem of
\textsc{element-extraction} in a query model that combines both: linear
measurements with bounded adaptivity.
</p>
<p>In the \textsc{element-extraction} problem, one is given a nonzero vector
$\mathbf{z} = (z_1,\ldots,z_n) \in \{0,1\}^n$ and must report an index $i$
where $z_i = 1$. The input can be accessed using arbitrary linear functions of
it with coefficients in some ring. This problem admits an efficient nonadaptive
randomized solution (through the well known technique of $\ell_0$-sampling) and
an efficient fully adaptive deterministic solution (through binary search). We
prove that when confined to only $k$ rounds of adaptivity, a deterministic
\textsc{element-extraction} algorithm must spend $\Omega(k (n^{1/k} -1))$
queries, when working in the ring of integers modulo some fixed $q$. This
matches the corresponding upper bound. For queries using integer arithmetic, we
prove a $2$-round $\widetilde{\Omega}(\sqrt{n})$ lower bound, also tight up to
polylogarithmic factors. Our proofs reduce to classic problems in
combinatorics, and take advantage of established results on the {\em zero-sum
problem} as well as recent improvements to the {\em sunflower lemma}.
</p></div>
    </summary>
    <updated>2021-07-14T22:44:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05672</id>
    <link href="http://arxiv.org/abs/2107.05672" rel="alternate" type="text/html"/>
    <title>In-Database Regression in Input Sparsity Time</title>
    <feedworld_mtime>1626220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jayaram:Rajesh.html">Rajesh Jayaram</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Samadian:Alireza.html">Alireza Samadian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Ye:Peng.html">Peng Ye</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05672">PDF</a><br/><b>Abstract: </b>Sketching is a powerful dimensionality reduction technique for accelerating
algorithms for data analysis. A crucial step in sketching methods is to compute
a subspace embedding (SE) for a large matrix $\mathbf{A} \in \mathbb{R}^{N
\times d}$. SE's are the primary tool for obtaining extremely efficient
solutions for many linear-algebraic tasks, such as least squares regression and
low rank approximation. Computing an SE often requires an explicit
representation of $\mathbf{A}$ and running time proportional to the size of
$\mathbf{A}$. However, if $\mathbf{A}= \mathbf{T}_1 \Join \mathbf{T}_2 \Join
\dots \Join \mathbf{T}_m$ is the result of a database join query on several
smaller tables $\mathbf{T}_i \in \mathbb{R}^{n_i \times d_i}$, then this
running time can be prohibitive, as $\mathbf{A}$ itself can have as many as
$O(n_1 n_2 \cdots n_m)$ rows.
</p>
<p>In this work, we design subspace embeddings for database joins which can be
computed significantly faster than computing the join. For the case of a two
table join $\mathbf{A} = \mathbf{T}_1 \Join \mathbf{T}_2$ we give
input-sparsity algorithms for computing subspace embeddings, with running time
bounded by the number of non-zero entries in $\mathbf{T}_1,\mathbf{T}_2$. This
results in input-sparsity time algorithms for high accuracy regression,
significantly improving upon the running time of prior FAQ-based methods for
regression. We extend our results to arbitrary joins for the ridge regression
problem, also considerably improving the running time of prior methods.
Empirically, we apply our method to real datasets and show that it is
significantly faster than existing algorithms.
</p></div>
    </summary>
    <updated>2021-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18952</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/" rel="alternate" type="text/html"/>
    <title>Socially Reproduced Experiments</title>
    <summary>We must avoid becoming one Cropped from USA Today source José Altuve hit a game-winning home run in the bottom of the ninth against the Yankees on Sunday. He thereby reproduced the conditions and the outcome of baseball’s most dramatic cheating accusation of 2019. Today, at baseball’s All-Star break, we review this and other social […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>We must avoid becoming one</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/altuvehr/" rel="attachment wp-att-18954"><img alt="" class="alignright size-full wp-image-18954" height="151" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AltuveHR.jpg?resize=151%2C151&amp;ssl=1" width="151"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from USA Today <a href="https://www.usatoday.com/story/sports/mlb/2021/07/11/astros-jose-altuve-walk-off-hr-shirt-ripped-off-yankees/7933018002/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
José Altuve hit a game-winning home run in the bottom of the ninth against the Yankees on Sunday. He thereby reproduced the conditions and the outcome of baseball’s most dramatic cheating accusation of 2019.</p>
<p>
Today, at baseball’s All-Star break, we review this and other social experiments that have quite a bit more data.</p>
<p>
Altuve won the 2019 American League Championship series with a pinch-hit homer in the bottom of the ninth against the Yankees’ closer, Aroldis Chapman. As he approached home plate, he was <a href="https://www.youtube.com/watch?v=MTNBnk1dz6g">seen</a> telling his teammates waiting to mob him at the plate not to rip off his jersey in celebration. He subsequently <a href="https://youtu.be/-ryvOya4PoE?t=247">scooted</a> into the corridor behind the dugout, then re-emerged into the on-field celebration. This fed accusations that he had been wired with a buzzer to know what kind of pitch was coming from Chapman, in line with <a href="https://en.wikipedia.org/wiki/Houston_Astros_sign_stealing_scandal">sign-stealing</a> by other means from the Astros’ 2017 championship season and 2018 that was proven and punished by Major League Baseball. </p>
<p>
Almost the same scenario was reproduced Sunday: Houston down 7-5 against the Yankees with two out and two on base in the ninth, Altuve up against the Yankees’ closer (Chad Green recently supplanting Chapman). Altuve <a href="https://www.youtube.com/watch?v=OHrc6OdYOpo">socked</a> a homer to the same part of the ballpark to complete a shocking six-run comeback. Immediately upon touching home, he had his shirt ripped off to reveal nothing but the top half of his birthday suit. This was the most direct way possible to witness that he could have hit the other homer without illegal information.</p>
<p>
</p><p/><h2> Examples and Non-Examples </h2><p/>
<p/><p>
I have dealt with chess-cheating cases in which electronic buzzing has been specifically alleged, including the two <a href="https://en.wikipedia.org/wiki/Borislav_Ivanov#Retirement_from_competitive_chess_and_brief_return_to_chess-related_activities">most</a> prominent <a href="https://www.nytimes.com/2013/08/18/crosswords/chess/a-master-is-disqualified-over-suspicions-of-cheating.html">cases</a> of 2013. I will not take this post further in this direction, however, but rather to pose this question:</p>
<blockquote><p><b> </b> <em> What is considered a “social proof” of an assertion—especially when there are elements of scientific control and reproduction? </em>
</p></blockquote>
<p/><p>
A simple example is a police lineup. This tries to control for whether a witness has previously seen the accused by including the accused among usually four or five similarly represented people. Picking the right person is considered to prove the previous encounter. Statistically, however, this is a <a href="https://en.wikipedia.org/wiki/P-value"><img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-value</a> of only 0.20 or 0.167, which are not considered significant at even the weakest level of “statistical proof.” Allowing <a href="https://www.apa.org/monitor/julaug04/lineups">null</a> lineups does not change the statistics much.</p>
<p>
Baseball gives a non-example that surprises me. One of the bad performances that cost Chapman his closer role was losing an 8-4 lead against the Los Angeles Angels on June 30. As a fantasy-baseball player, I’ve regularly observed poor pitching (by the closers on my “fantasy team”) when the lead is too large to earn credit for a coveted <a href="https://en.wikipedia.org/wiki/Save_(baseball)">save</a>. Does the data reproduce a phenomenon of closers bearing down less when way ahead, with no “save” to gain? A <a href="https://www.beyondtheboxscore.com/2014/1/27/5344580/the-closer-mentality-part-1-closers-in-non-save-situations">study</a> after the 2013 season, which cleverly represented performances by the same <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scores I use in chess, found none. “Meltdowns” like Chapman’s are offset by cases where closers pitched better. The <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scores in the study are all in the range -1.25 to +1.50 anyway, which count as statistically random.</p>
<p>
This study used a reasonably large data set, one that is well-defined and admits controlling factors such as normalizing for game circumstances and the quality of the opposing hitters. At least it is more than the two instances of Altuve. In-between would be an attempt to determine whether certain national soccer teams are consistently worse at penalty-kick tiebreaks. England’s and Italy’s teams brought their long tortured histories together in the tiebreak of Sunday’s European Cup final. The Italians missed two of five kicks, a score that often spells doom, but the English missed three.</p>
<p>
</p><p/><h2> Larger Scale </h2><p/>
<p/><p>
Dick and I are really interested in “experiments” that have spilled into society, with minimal controls but large data. One sphere of this is cybersecurity. </p>
<p>
It seems to us that only in the past decade have security experts begun formalizing their research as experimental science with repeatability and reproducibility as explicit criteria. The NSA devoted a special 2012 <a href="https://www.nsa.gov/Portals/70/documents/resources/everyone/digital-media-center/publications/the-next-wave/TNW-19-2.pdf">issue</a> of their <em>Next Wave</em> series to what they titled as “Developing a blueprint for a science of cybersecurity.” Among the contents are:</p>
<ul>
<li>
An introductory essay by Carl Landwehr titled, “Cybersecurity: From engineering to science.” <p/>
</li><li>
A linchpin <a href="https://www.cs.dartmouth.edu/~ccpalmer/teaching/cs55/Resources/Papers/TNW_19_2_BlueprintScienceCybersecurity_Schneider.pdf">paper</a> by Fred Schneider titled, “Blueprint for a science of cybersecurity.” <p/>
</li><li>
A <a href="https://www.cs.cmu.edu/~maxion/pubs/Maxion12.pdf">paper</a> by Roy Maxion titled, “Making experiments dependable,” which came from a 2011 Springer LNCS <a href="https://link.springer.com/book/10.1007/978-3-642-24541-1">Festschrift</a>.
</li></ul>
<p>
Maxion’s main example is <a href="https://en.wikipedia.org/wiki/Keystroke_dynamics">keystroke biometrics</a>. This covers inferences made from typing style on a computer keyboard or mouse or similar handheld input device. This can be used to verify identity or screen for malfeasant activities. Online chess playing platforms collect data of this nature—okay we could not resist adding chess example. </p>
<p>
Another area is experiments designed to simulate attacks and test defenses against them. Schneider’s paper begins with a contrast between <em>predictive</em> modeling versus <em>reactive</em> handling of them. About the latter, he draws an analogy with health care:</p>
<blockquote><p><b> </b> <em> “Some health problems are best handled in a reactive manner. We know what to do when somebody breaks a finger, and each year we create a new influenza vaccine in anticipation of the flu season to come. But only after making significant investments in basic medical sciences are we starting to understand the mechanisms by which cancers grow, and a cure seems to require that kind of deep understanding.” </em>
</p></blockquote>
<p/><p>
He goes on to outline the kind of scientific foundation that could hopefully underlie a ‘cure’ for intrusion and malware and the like. </p>
<p>
What we have seen happen especially in the past months, however—in both health and security—is uncontrolled experiments with society as the domain. Large-scale ransomware attacks are becoming as frequent as hurricanes and heat waves. And of course, the pandemic. These share with Altuve the property of being one-off instances, but have large data on the receiving end.</p>
<p>
</p><p/><h2> Summer Pandemic Update </h2><p/>
<p/><p>
The following chart updates our June 20 <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">post</a> on the state of the pandemic and its projection for the summer—for Florida and the United Kingdom in particular:</p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/flukcases071321/" rel="attachment wp-att-18955"><img alt="" class="aligncenter wp-image-18955" height="440" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/FLUKCases071321.png?resize=450%2C440&amp;ssl=1" width="450"/></a></p>
<p>
The vertical line shows about where the charts were on June 20. The past few days are the first where we can point to a significant rise in Florida, though Missouri had a similar rise last week and it is showing up in some other states.  The charts are taken from the <em>Worldometer</em> coronavirus <a href="https://www.worldometers.info/coronavirus/">pages</a>. </p>
<p>
The UK rise looks ghastly. It was a subtext of our previous post to worry that allowing the large dense soccer crowds at London’s Wembley Stadium for the semis and final—and anything similar in baseball—would stoke the rise in our respective countries even more. However, the rate of hospitalizations in the UK has remained largely flat. This Fortune <a href="https://fortune.com/2021/07/08/kids-vulnerable-covid-delta-variant-vaccinated-europe/">article</a> last week is one of several attesting that the new cases are mostly in children or in vaccinated people with enough immunity to contain the “breakthrough” positive. The UK is going ahead with large-scale re-openings later this month, with the portion of those 18 and older who have had one dose approaching 90% and those fully vaccinated coming past 70%. The latter number in relation to the whole population is about 52%.</p>
<p>
The US looks like becoming an experiment in how the local vaccination rate affects the numbers. The rates of those fully vaccinated by state are currently eerily <a href="https://www.cnn.com/2021/07/08/politics/electoral-map-vaccine-map-covid-19/index.html">similar</a> to Joe Biden’s vote percentage in the state. One aspect of scientific reproducibility is the size of the simplest classifier of the results. For a presidential vote to have simpler explaining power than any factors of biology or other life circumstances would make a strange experiment indeed.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Dick and I tried to come up with other examples—from computer security in particular—to sustain what has been occupying our thoughts about standards of proof for policy. We would welcome some examples from you, our readers. </p>
<p>
And of course, we have been concerned about the present course of the pandemic amid re-openings since the referenced post last month. In the meantime, if it is your taste, please enjoy the All-Star Game, which Altuve is, ironically, <a href="https://calltothepen.com/2021/07/11/houston-astros-officially-skipping-star-game/">skipping</a>.</p>
<p/><p><br/>
[some word fixes and changes]</p></font></font></div>
    </content>
    <updated>2021-07-13T19:47:02Z</updated>
    <published>2021-07-13T19:47:02Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="baseball"/>
    <category term="coronavirus"/>
    <category term="Jose Altuve"/>
    <category term="pandemic"/>
    <category term="reproducibility"/>
    <category term="science"/>
    <category term="security"/>
    <category term="social process"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-07-15T04:37:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/102</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/102" rel="alternate" type="text/html"/>
    <title>TR21-102 |  Tight bounds on the Fourier growth of bounded functions on the hypercube | 

	Siddharth Iyer, 

	Anup Rao, 

	Victor Reis, 

	Thomas Rothvoss, 

	Amir Yehudayoff</title>
    <summary>We give tight bounds on the degree $\ell$  homogenous parts $f_\ell$ of a bounded function $f$ on the cube. We show that if $f: \{\pm 1\}^n \rightarrow [-1,1]$ has degree $d$, then $\| f_\ell \|_\infty$ is bounded by $d^\ell/\ell!$, and $\| \hat{f}_\ell \|_1$ is bounded by $d^\ell e^{{\ell+1 \choose 2}} n^{\frac{\ell-1}{2}}$. We describe applications to pseudorandomness and learning theory. We use similar methods to generalize the classical Pisier's inequality from convex analysis. Our analysis  involves properties of real-rooted polynomials that may be useful elsewhere.</summary>
    <updated>2021-07-13T17:55:40Z</updated>
    <published>2021-07-13T17:55:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-15T04:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/101</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/101" rel="alternate" type="text/html"/>
    <title>TR21-101 |  A Parallel Repetition Theorem for the GHZ Game: A Simpler Proof | 

	Uma Girish, 

	Justin Holmgren, 

	Kunal Mittal, 

	Ran Raz, 

	Wei Zhan</title>
    <summary>We give a new proof of the fact that the parallel repetition of the (3-player) GHZ game reduces the value of the game to zero polynomially quickly. That is, we show that the value of the $n$-fold GHZ game is at most $n^{-\Omega(1)}$. This was first established by Holmgren and Raz [HR20]. We present a new proof of this theorem that we believe to be simpler and more direct. Unlike most previous works on parallel repetition, our proof makes no use of information theory, and relies on the use of Fourier analysis.

The GHZ game [GHZ89] has played a foundational role in the understanding of quantum information theory, due in part to the fact that quantum strategies can win the GHZ game with probability $1$. It is possible that improved parallel repetition bounds may find applications in this setting.

Recently, Dinur, Harsha, Venkat, and Yuen [DHVY17] highlighted the GHZ game as a simple three-player game, which is in some sense maximally far from the class of multi-player games whose behavior under parallel repetition is well understood. Dinur et al. conjectured that parallel repetition decreases the value of the GHZ game exponentially quickly, and speculated that progress on proving this would shed light on parallel repetition for general multi-player (multi-prover) games.</summary>
    <updated>2021-07-13T14:48:27Z</updated>
    <published>2021-07-13T14:48:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-15T04:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://sarielhp.org/blog/?p=9420</id>
    <link href="https://sarielhp.org/blog/?p=9420" rel="alternate" type="text/html"/>
    <title>FSTTCS 2021 deadline is this Monday…</title>
    <summary>The link is here. Due to the pandemic it is going to be virtual. Quoting Bob Dylan, this blog is not dead, it is just asleep.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The link is <a href="https://www.fsttcs.org.in/2021/" rel="noreferrer noopener" target="_blank">here</a>. Due to the pandemic it is going to be virtual.</p>



<p>Quoting Bob Dylan,  this blog is not dead, it is just asleep.</p></div>
    </content>
    <updated>2021-07-13T03:38:25Z</updated>
    <published>2021-07-13T03:38:25Z</published>
    <category term="Research"/>
    <category term="cs theory"/>
    <author>
      <name>Sariel</name>
    </author>
    <source>
      <id>https://sarielhp.org/blog</id>
      <link href="https://sarielhp.org/blog/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://sarielhp.org/blog" rel="alternate" type="text/html"/>
      <subtitle>Sariel's blog</subtitle>
      <title>Vanity of Vanities, all is Vanity</title>
      <updated>2021-07-15T04:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4532</id>
    <link href="https://lucatrevisan.wordpress.com/2021/07/12/what-a-difference-a-few-months-can-make/" rel="alternate" type="text/html"/>
    <title>What a difference a few months can make</title>
    <summary>Piazza Duomo, in Milan, on December 26, 2020 Piazza Duomo, in Milan, on July 11, 2021</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Piazza Duomo, in Milan, on December 26, 2020</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure>



<p>Piazza Duomo, in Milan, on July 11, 2021</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure></div>
    </content>
    <updated>2021-07-12T17:04:51Z</updated>
    <published>2021-07-12T17:04:51Z</published>
    <category term="Milan"/>
    <category term="things that are excellent"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2021-07-15T04:37:12Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/exponential-mechanism-bounded-range/</id>
    <link href="https://differentialprivacy.org/exponential-mechanism-bounded-range/" rel="alternate" type="text/html"/>
    <title>A Better Privacy Analysis of the Exponential Mechanism</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A basic and frequent task in data analysis is <em>selection</em> – given a set of options \(\mathcal{Y}\), output the (approximately) best one, where “best” is defined by some loss function \(\ell : \mathcal{Y} \times \mathcal{X}^n \to \mathbb{R}\) and a dataset \(x \in \mathcal{X}^n\). That is, we want to output some \(y \in \mathcal{Y}\) that approximately minimizes \(\ell(y,x)\). Naturally, we are interested in <em>private selection</em> – i.e., the output should be differentially private in terms of the dataset \(x\).
This post discusses algorithms for private selection – in particular, we give an improved privacy analysis of the popular exponential mechanism.</p>

<h2 id="the-exponential-mechanism">The Exponential Mechanism</h2>

<p>The most well-known algorithm for private selection is the <a href="https://en.wikipedia.org/wiki/Exponential_mechanism_(differential_privacy)"><em>exponential mechanism</em></a> <a href="https://doi.org/10.1109/FOCS.2007.66" title="Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007."><strong>[MT07]</strong></a>. The exponential mechanism \(M : \mathcal{X}^n \to \mathcal{Y} \) is a randomized algorithm given by \[\forall x \in \mathcal{X}^n ~ \forall y \in \mathcal{Y} ~~~~~ \mathbb{P}[M(x) = y] = \frac{\exp(-\frac{\varepsilon}{2\Delta} \ell(y,x))}{\sum_{y’ \in \mathcal{Y}} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x)) }, \tag{1}\] where \(\Delta\) is the sensitivity of the loss function \(\ell\) given by \[\Delta = \sup_{x,x’ \in \mathcal{X}^n : d(x,x’) \le 1} \max_{y\in\mathcal{Y}} |\ell(y,x) - \ell(y,x’)|,\tag{2}\] where the supremum is taken over all datasets \(x\) and \(x’\) differing on the data of a single individual (which we denote by \(d(x,x’)\le 1\)).</p>

<p>In terms of utility, we can easily show that <a href="https://arxiv.org/abs/1511.02513" title="Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, Jonathan Ullman. Algorithmic Stability for Adaptive Data Analysis. STOC 2016."><strong>[BNSSSU16]</strong></a> \[\mathbb{E}[\ell(M(x),x)] \le \min_{y \in \mathcal{Y}} \ell(y,x) + \frac{2\Delta}{\varepsilon} \log |\mathcal{Y}|\] for all \(x \in \mathcal{X}^n\) (and we can also give high probability bounds).</p>

<p>It is easy to show that the exponential mechanism satisfies \(\varepsilon\)-differential privacy.
But there is more to this story! We’re going to look at a more refined privacy analysis.</p>

<h2 id="bounded-range">Bounded Range</h2>

<p>The privacy guarantee of the exponential mechanism is more precisely characterized by <em>bounded range</em>. This was observed and defined by David Durfee and Ryan Rogers <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a> and further analyzed later <a href="https://arxiv.org/abs/1909.13830" title="Jinshuo Dong, David Durfee, Ryan Rogers. Optimal Differential Privacy Composition for Exponential Mechanisms. ICML 2020."><strong>[DDR20]</strong></a>.</p>

<blockquote>
  <p><strong>Definition 1 (Bounded Range).</strong><sup id="fnref:1"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:1" rel="footnote">1</a></sup> 
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\eta\)-bounded range if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, there exists some \(t \in \mathbb{R}\) such that \[\forall y \in \mathcal{Y} ~~~~~ \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]}\right) \in [t, t+\eta].\] Here \(t\) may depend on the pair of input datasets \(x,x’\), but not on the output \(y\).</p>
</blockquote>

<p>To interpret this definition, we <a href="https://differentialprivacy.org/flavoursofdelta/">recall the definition of the privacy loss random variable</a>: Define \(f : \mathcal{Y} \to \mathbb{R}\) by \[f(y) = \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]}\right).\] Then the privacy loss random variable \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) is given by \(Z = f(M(x))\).</p>

<p>Pure \(\varepsilon\)-differential privacy is equivalent to demanding that the privacy loss is bounded by \(\varepsilon\) – i.e., \(\mathbb{P}[|Z|\le\varepsilon]=1\). Approximate \((\varepsilon,\delta)\)-differential privacy is, roughly, equivalent to demanding that \(\mathbb{P}[Z\le\varepsilon]\ge1-\delta\).<sup id="fnref:2"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:2" rel="footnote">2</a></sup></p>

<p>Now \(\eta\)-bounded range is simply demanding that the privacy loss \(Z\) is supported on some interval of length \(\eta\). This interval \([t,t+\eta]\) may depend on the pair \(x,x’\).</p>

<p>Bounded range and pure differential privacy are equivalent up to a factor of 2 in the parameters:</p>

<blockquote>
  <p><strong>Lemma 2 (Bounded Range versus Pure Differential Privacy).</strong></p>
  <ul>
    <li>\(\varepsilon\)-differential privacy implies \(\eta\)-bounded range with \(\eta \le 2\varepsilon\).</li>
    <li>\(\eta\)-bounded range implies \(\varepsilon\)-differential privacy with \(\varepsilon \le \eta\).</li>
  </ul>
</blockquote>

<p><em>Proof.</em> The first part of the equivalence follows from the fact that pure \(\varepsilon\)-differential privacy implies the privacy loss is supported on the interval \([-\varepsilon,\varepsilon]\). Thus, if we set \(t=-\varepsilon\) and \(\eta=2\varepsilon\), then \([t,t+\eta] = [-\varepsilon,\varepsilon]\).
The second part follows from the fact that the support of the privacy loss \([t,t+\eta]\) must straddle \(0\). That is, the privacy loss cannot be always positive nor always negative, so \(0 \in [t,t+\eta]\) and, hence, \([t,t+\eta] \subseteq [-\eta,\eta]\). Otherwise \(\forall y ~ f(y)&gt;0\) or \(\forall y ~ f(y)&lt;0\)  would imply \(\forall y ~ \mathbb{P}[M(x)=y]&gt;\mathbb{P}[M(x’)=y]\) or \(\forall y ~ \mathbb{P}[M(x)=y]&lt;\mathbb{P}[M(x’)=y]\), contradicting the fact that \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(x)=y] = 1\) and \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(x’)=y] = 1\). ∎</p>

<p>OK, back to the exponential mechanism:</p>

<blockquote>
  <p><strong>Lemma 3 (The Exponential Mechanism is Bounded Range).</strong>
The exponential mechanism (given in Equation 1 above) satisfies \(\varepsilon\)-bounded range .<sup id="fnref:3"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:3" rel="footnote">3</a></sup></p>
</blockquote>

<p><em>Proof.</em>
We have \[e^{f(y)} = \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,x))}{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,x’))} \cdot \frac{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x’))}{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x))}.\]
Setting \(t = \log\left(\frac{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x’))}{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x))}\right) - \frac{\varepsilon}{2}\), we have \[ f(y) = \frac{\varepsilon}{2\Delta} (\ell(y,x’)-\ell(y,x)+\Delta) + t.\]
By the definition of sensitivity (given in Equation 2), we have \( 0 \le \ell(y,x’)-\ell(y,x)+\Delta \le 2\Delta\), whence \(t \le f(y) \le t + \varepsilon\). ∎</p>

<p>Bounded range is not really a useful privacy definition on its own. Thus we’re going to relate it to a relaxed version of differential privacy next.</p>

<h2 id="concentrated-differential-privacy">Concentrated Differential Privacy</h2>

<p>Concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a> and its variants <a href="https://arxiv.org/abs/1603.01887" title="Cynthia Dwork, Guy N. Rothblum. Concentrated Differential Privacy. 2016."><strong>[DR16]</strong></a> <a href="https://arxiv.org/abs/1702.07476" title="Ilya Mironov. R&#xE9;nyi Differential Privacy. CCS 2017."><strong>[M17]</strong></a> are relaxations of pure differential privacy with many nice properties. In particular, it composes very cleanly.</p>

<blockquote>
  <p><strong>Definition 4 (Concentrated Differential Privacy).</strong>
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, 
\[\forall \lambda &gt; 0 ~~~~~ \mathbb{E}[\exp( \lambda Z)] \le \exp(\lambda(\lambda+1)\rho),\tag{3}\]
where \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) is the privacy loss random variable.<sup id="fnref:4"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:4" rel="footnote">4</a></sup></p>
</blockquote>

<p>Intuitively, concentrated differential privacy requires that the privacy loss is subgaussian. Specifically, the bound on the moment generating function of \(\rho\)-concentrated differential privacy is tight if the privacy loss \(Z\) follows the distribution \(\mathcal{N}(\rho,2\rho)\). Indeed, the privacy loss random variable of the Gaussian mechanism has such a distribution.<sup id="fnref:5"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:5" rel="footnote">5</a></sup></p>

<p>OK, back to the exponential mechanism:
We know that \(\varepsilon\)-differential privacy implies \(\frac12 \varepsilon^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a>.
This, of course, applies to the exponential mechaism. A cool fact – that we want to draw more attention to – is that we can do better! 
Specifically, \(\eta\)-bounded range implies \(\frac18 \eta^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/2004.07223" title="Mark Cesar, Ryan Rogers. Bounding, Concentrating, and Truncating: Unifying Privacy Loss Composition for Data Analytics. ALT 2021."><strong>[CR21]</strong></a>.
What follows is a proof of this fact following that of Mark Cesar and Ryan Rogers, but with some simplification.</p>

<blockquote>
  <p><strong>Theorem 5 (Bounded Range implies Concentrated Differential Privacy).</strong>
If \(M\) is \(\eta\)-bounded range, then it is \(\frac18\eta^2\)-concentrated differentially private.</p>
</blockquote>

<p><em>Proof.</em>
Fix datasets \(x,x’ \in \mathcal{X}^n\) differing on a single individual’s data.
Let \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) be the privacy loss random variable of the mechanism \(M\) on this pair of datasets.
By the definition of bounded range (Definition 1), there exists some \(t \in \mathbb{R}\) such that \(Z \in [t, t+\eta]\) with probability 1.
Now we employ <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_lemma">Hoeffding’s Lemma</a> <a href="https://doi.org/10.1080%2F01621459.1963.10500830" title="Wassily Hoeffding. Probability inequalities for sums of bounded random variables. JASA 1963."><strong>[H63]</strong></a>:</p>
<blockquote>
  <p><strong>Lemma 6 (Hoeffding’s Lemma).</strong>
Let \(X\) be a random variable supported on the interval \([a,b]\). Then, for all \(\lambda \in \mathbb{R}\), we have \[\mathbb{E}[\exp(\lambda X)] \le \exp \left( \mathbb{E}[X] \cdot \lambda + \frac{(b-a)^2}{8} \cdot \lambda^2 \right).\]</p>
</blockquote>

<p>Applying the lemma to the privacy loss gives \[\forall \lambda \in \mathbb{R} ~~~~~  \mathbb{E}[\exp(\lambda Z)] \le \exp \left( \mathbb{E}[Z] \cdot \lambda + \frac{\eta^2}{8} \cdot \lambda^2 \right).\]
The only remaining thing we need is to show is that \(\mathbb{E}[Z] \le \frac18 \eta^2\).<sup id="fnref:6"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:6" rel="footnote">6</a></sup></p>

<p>If we set \(\lambda = -1 \), then we get \( \mathbb{E}[\exp( - Z)] \le \exp \left( -\mathbb{E}[Z] + \frac{\eta^2}{8} \right)\), which rearranges to \(\mathbb{E}[Z] \le \frac18 \eta^2 - \log \mathbb{E}[\exp( - Z)]\). 
Now we have \[ \mathbb{E}[\exp( - Z)] \!=\! \sum_y \mathbb{P}[M(x)\!=\!y] \exp(-f(y)) \!=\! \sum_y \mathbb{P}[M(x)\!=\!y]  \!\cdot\! \frac{\mathbb{P}[M(x’)\!=\!y]}{\mathbb{P}[M(x)\!=\!y]} \!=\! 1.\]
∎</p>

<p>This brings us to the TL;DR of this post:</p>

<blockquote>
  <p><strong>Corollary 7.</strong> The exponential mechanism (given by Equation 1) is \(\frac18 \varepsilon^2\)-concentrated differentially private.</p>
</blockquote>

<p>This is great news. The standard analysis only gives \(\frac12 \varepsilon^2\)-concentrated differential privacy. Constants matter when applying differential privacy, and we save a factor of 4 in the concentrated differential privacy analysis of the exponential mechanism for free with this improved analysis.</p>

<p>Combining Lemma 2 with Theorem 5 also gives a simpler proof of the conversion from pure differential privacy to concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a>:</p>

<blockquote>
  <p><strong>Corollary 8.</strong> \(\varepsilon\)-differential privacy implies \(\frac12 \varepsilon^2\)-concentrated differential privacy.</p>
</blockquote>

<h2 id="beyond-the-exponential-mechanism">Beyond the Exponential Mechanism</h2>

<p>The exponential mechanism is not the only algorithm for private selection. A closely-related algorithm is <em>report noisy max/min</em>:<sup id="fnref:7"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:7" rel="footnote">7</a></sup> Draw independent noise \(\xi_y\) from some distribution for each \(y \in \mathcal{Y}\) then output \[M(x) = \underset{y \in \mathcal{Y}}{\mathrm{argmin}} ~ \ell(y,x) - \xi_y.\]</p>

<p>If the noise distribution is an appropriate <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution</a>, then report noisy max is exactly the exponential mechanism. (This equivalence is known as the “Gumbel max trick.”)</p>

<p>We can also use the Laplace distribution or the exponential distribution. Report noisy max with the exponential distribution is equivalent to the <em>permute and flip</em> algorithm <a href="https://arxiv.org/abs/2010.12603" title="Ryan McKenna, Daniel Sheldon. Permute-and-Flip: A new mechanism for differentially private selection . NeurIPS 2020."><strong>[MS20]</strong></a> <a href="https://arxiv.org/abs/2105.07260" title="Zeyu Ding, Daniel Kifer, Sayed M. Saghaian N. E., Thomas Steinke, Yuxin Wang, Yingtai Xiao, Danfeng Zhang. The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with Exponential Noise. 2021."><strong>[DKSSWXZ21]</strong></a>. However, these algorithms don’t enjoy the same improved bounded range and concentrated differential privacy guarantees as the exponential mechanism.</p>

<p>There are also other variants of the selection problem. For example, in some cases we can assume that only a few options have low loss and the rest of the options have high loss – i.e., there is a gap between the minimum loss and the second-lowest loss (or, more generally, the \(k\)-th lowest loss). In this case there are algorithms that attain better accuracy than the exponential mechanism under relaxed privacy definitions <a href="https://arxiv.org/abs/1409.2177" title="Kamalika Chaudhuri, Daniel Hsu, Shuang Song. The Large Margin Mechanism for Differentially Private Maximization. NIPS 2014."><strong>[CHS14]</strong></a> <a href="https://dl.acm.org/doi/10.1145/3188745.3188946" title=" Mark Bun, Cynthia Dwork, Guy N. Rothblum, Thomas Steinke. Composable and versatile privacy via truncated CDP. STOC 2018."><strong>[BDRS18]</strong></a> <a href="https://arxiv.org/abs/1905.13229" title="Mark Bun, Gautam Kamath, Thomas Steinke, Zhiwei Steven Wu. Private Hypothesis Selection. NeurIPS 2019."><strong>[BKSW19]</strong></a>.</p>

<p>There are a lot of interesting aspects of private selection, including questions for further research! We hope to have further posts about some of these topics.</p>

<hr/>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For simplicity, we restrict our discussion here to finite sets of outputs, although the definitions, algorithms, and results can be extended to infinite sets. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>To be more precise, \((\varepsilon,\delta)\)-differential privacy is equivalent to demanding that \(\mathbb{E}[\max\{0,1-\exp(\varepsilon-Z)\}]\le\delta\) <a href="https://arxiv.org/abs/2004.00010" title="Cl&#xE9;ment L. Canonne, Gautam Kamath, Thomas Steinke. The Discrete Gaussian for Differential Privacy. NeurIPS 2020."><strong>[CKS20]</strong></a>. (To be completely precise, we must appropriately deal with the \(Z=\infty\) case, which we ignore in this discussion for simplicity.) <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:2">↩</a></p>
    </li>
    <li id="fn:3">
      <p>This proof actually gives <a href="https://dongjs.github.io/2020/02/10/ExpMech.html">a slightly stronger result</a>: We can replace the sensitivity \(\Delta\) (defined in Equation 2) by half the range \[\hat\Delta = \frac12 \sup_{x,x’ \in \mathcal{X}^n : d(x,x’) \le 1} \left( \max_{\overline{y}\in\mathcal{Y}} \ell(\overline{y},x) - \ell(\overline{y},x’) - \min_{\underline{y}\in\mathcal{Y}} \ell(\underline{y},x) - \ell(\underline{y},x’) \right).\] We always have \(\hat\Delta \le \Delta\) but it is possible that \(\hat\Delta &lt; \Delta\) and the privacy analysis of the exponential mechanism still works if we replace \(\Delta\) by \(\hat\Delta\). <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:3">↩</a></p>
    </li>
    <li id="fn:4">
      <p>Equivalently, a randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, \[\forall \lambda &gt; 0 ~~~~~ \mathrm{D}_{\lambda+1}(M(x)\|M(x’)) \le \lambda(\lambda+1)\rho,\] where \(\mathrm{D}_{\lambda+1}(M(x)\|M(x’)))\) is the order \(\lambda+1\) Rényi divergence of \(M(x)\) from \(M(x’)\). <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:4">↩</a></p>
    </li>
    <li id="fn:5">
      <p>To be precise, if \(M(x) = q(x) + \mathcal{N}(0,\sigma^2I)\), then \(M : \mathcal{X}^n \to \mathbb{R}^d\) satisfies \(\frac{\Delta_2^2}{2\sigma^2}\)-concentrated differential privacy, where \(\Delta_2 = \sup_{x,x’\in\mathcal{X}^n : d(x,x’)\le1} \|q(x)-q(x’)\|_2\) is the 2-norm sensitivity of \(q:\mathcal{X}^n \to \mathbb{R}^d\). Furthermore, the privacy loss of the Gaussian mechanism is itself a Gaussian and it makes the inequality defining concentrated differential privacy (Equation 3) an equality for all \(\lambda\) <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:5">↩</a></p>
    </li>
    <li id="fn:6">
      <p>Note that the expectation of the privacy loss is simply the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a>: \(\mathbb{E}[Z] = \mathrm{D}_1( M(x) \| M(x’) )\). <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:6">↩</a></p>
    </li>
    <li id="fn:7">
      <p>We have presented selection here in terms of minimization, but most of the literature is in terms of maximization. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:7">↩</a></p>
    </li>
  </ol>
</div></div>
    </summary>
    <updated>2021-07-12T17:00:00Z</updated>
    <published>2021-07-12T17:00:00Z</published>
    <author>
      <name>Thomas Steinke</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-07-15T00:58:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/100</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/100" rel="alternate" type="text/html"/>
    <title>TR21-100 |  Karchmer-Wigderson Games for Hazard-free Computation | 

	Christian Ikenmeyer, 

	Balagopal Komarath, 

	Nitin Saurabh</title>
    <summary>We present a Karchmer-Wigderson game to study the complexity of hazard-free formulas. This new game is both a generalization of the monotone Karchmer-Wigderson game and an analog of the classical Boolean Karchmer-Wigderson game. Therefore, it acts as a bridge between the existing monotone and general games.

Using this game, we prove hazard-free formula size and depth lower bounds that are provably stronger than those possible by the standard technique of transferring results from monotone complexity in a black-box fashion.
For the multiplexer function
we give (1) a hazard-free formula of optimal size and (2) an improved low-depth hazard-free formula of almost optimal size and (3) a hazard-free formula with alternation depth $2$ that has optimal depth.
We then use our optimal constructions to obtain an improved universal worst-case hazard-free formula size upper bound.
We see our results as a significant step towards establishing hazard-free computation as an independent missing link between Boolean complexity and monotone complexity.</summary>
    <updated>2021-07-12T07:35:35Z</updated>
    <published>2021-07-12T07:35:35Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-15T04:37:33Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6138760775046652444</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6138760775046652444/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6138760775046652444" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6138760775046652444" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html" rel="alternate" type="text/html"/>
    <title>Would you take this bet (Part 2) ?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Recall from my last post (<a href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html">here</a>)</p><p><br/></p><p>I offer you the following bet: </p><p>I will flip a coin.</p><p>If  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>If  HEADS you get 2 dollars and we end there.</p><p>If  TAILS I flip again</p><p><br/></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>The expected value is infinity.</p><p><br/></p><p>Would you pay $1000 to play this game?</p><p>Everyone who responded said NO. Most gave reasons similar to what I have below. </p><p>This is called The St Petersburg Paradox. Not sure it's a paradox, but it is odd. The concrete question of <i>would you pay $1000 to play</i> might be a paradox since most people would say NO even though the expected value is infinity.  See <a href="https://en.wikipedia.org/wiki/St._Petersburg_paradox">here</a> for more background.</p><p>Shapley (see <a href="https://www.sciencedirect.com/science/article/abs/pii/0022053177901429">here</a>) gives a good reason why you would not  pay $1000 to play the game, and also how much you should pay to play the game (spoiler alert: not much). I will summarize his argument and then add to it. </p><p><br/></p><p>1) Shapley's argument: Lets say the game goes for 40 rounds. Then you are owed 2^{40} dollars. </p><p>The amount of money in the world is, according to <a href="https://bibloteka.com/how-much-money-is-there-in-the-world/#:~:text=Short%20Answer%3A%20Money%20in%20circulation%20in%20the%20world,the%20medium%20of%20trade%20for%20goods%20and%20services.">this article</a> around 1.2 quadrillion dollars  which is roughly 2^{40} dollars. </p><p>So the expected value calculation has to be capped at (say) 40 rounds. This means you expect to get 20 dollars! So pay 19 to play. </p><p><br/></p><p>2) My angle which is very similar: at what point is more money not going to change your life at all? For me it is way less than 2^{40} dollars. Hence I would not pay 1000. Or even 20. </p><p><i>Exercise</i>: If you think the game will go at most R rounds and you only wand D dollars, how much should you pay to play? You can also juggle more parameters - the bias of the coin, how much they pay out when you win. </p><p>Does Shapley's discussions  <i>resolve</i> the paradox? It depends on what you consider paradoxical. If the paradox is that people would NOT pay 1000 even though the expected value is infinity, then Shapley  resolves the paradox  by contrasting the real world to the math world. </p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-07-12T00:53:00Z</updated>
    <published>2021-07-12T00:53:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-14T15:39:19Z</updated>
    </source>
  </entry>
</feed>
