<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-02-09T09:22:16Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-9121434197807183435</id>
    <link href="http://processalgebra.blogspot.com/feeds/9121434197807183435/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=9121434197807183435" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/9121434197807183435" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/9121434197807183435" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/02/whence-do-research-collaborations-in.html" rel="alternate" type="text/html"/>
    <title>Whence do research collaborations (in TCS) arise?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>About ten days ago, I gave a <a href="http://icetcs.ru.is/slides/intro2icetcs2021.pdf" target="_blank">talk</a> to my colleagues at the <a href="https://en.ru.is/st/dcs/" target="_blank">Department of Computer Science at Reykjavik University</a>, introducing my personal (and admittedly very biased) view of the past, present and future of <a href="http://icetcs.ru.is/" target="_blank">ICE-TCS</a>. </p><p>After my presenta­tion, a colleague asked me how she could engage mathematicians and theoretical computer scientists in joint research. I gave her an answer off the top of my head, but it was clear that she was unconvinc­ed and felt that I was avoid­ing answering her question. (For the record, I basically told her that she should knock on our door, discuss with us the problems she was interested in solving and hope that they are of interest to us. I feel that many research collaborations arise from serendipity and that there is no recipe that is guaranteed to work.) </p><p>The thought that she felt that I might have dodged her question prompted me to look back at my own research collabo­rations and how they came about. The rest of this post is the result of that quick-and-dirty reflection. Let me state right away that my list isn't meant to be exhausti­ve and that I won't mention many of the collaborations in which I have been lucky to be involved and that I have played a crucial role in shaping my academic development.  </p><p><b>Reading papers.</b> One of my long-term research collaborations arose from reading a paper written by a colleague. His paper prompted my companion and me to ask ourselves whether we could prove a similar result to the one our colleague had shown in a different setting. We succeeded and sent him our paper. Subsequently, we invited him to visit us in Aalborg. That visit marked the start of a collaboration and friendship that has lasted for over 20 years. <br/><br/><b>Approaching a colleague via email for help in solving a problem.</b> At some point, my companion and I were thinking about a research problem that had frustrated us for a while. I remembered reading a number of papers by a colleague on related topics, so I wrote to him, describing the problem, our attempts at solving it and where we had hit a brick wall. I asked him whether he would be interested in working with us on solving it. He did and that was one of the lucky breaks I have had in my research career. Once more, that collaboration offer via email led to mutual visits, other joint papers and, IMHO even more importantly, a long-term friendship that extended beyond work. <br/> </p><p><b>Available funding and building on one's mistakes.</b> One day in 2009, an email in my mailbox alerted me to the availability of substantial funding for research collabo­ration between universities in country X and those locat­ed in Norway, Iceland and Lichtenstein. This opportunity was enticing, as I had never visited country X, so I asked myself: "Is there anyone there we might conceivably work with?'' Mulling over that question, I recalled that a colleague from country X had spotted an imprecision in a paper I had coauthored. </p><p>I wrote to him, we applied for that funding jointly and got it. That successful grant application provided the funds for many research visits involving several people in our research groups. Those visits resulted in joint papers, another successful grant application and a number of friendships. </p><p><b>Coffee breaks at conferences.</b> I have at least two exhibits under this heading. The first belongs to a previous geologic­al era (1991). I was attend­ing a conference at CMU and asked a colleague what he was working on. He told me<br/>about a problem he was tackling, which I knew was also on the radar of a fellow researcher and on which I had started working independently. Eventually, after some email exchanges, that chat over coffee turned into a three-way collaboration that, thanks to my coauthors, produced one of my best papers. <br/><br/>Fast forward to 2017 and I'm in Rome to deliver an invited talk at a small conference. During the coffee break follow­ing my presentation, I was approached by a young research­er, with whom I had a number of pleasant conversations during the conference. Some time later, she sent me a draft paper dealing with a topic related to the content of my invited talk. I invited her to visit our research group in Reykjavik and to join the team working on a research project for which we had funding at the time. Those coffee-break conversations led to a collaboration and friendship that I hope will last for a long time. Meeting that colleague has been another of my lucky breaks. <br/> </p><p><b>Reading groups.</b> Last, but by no means least, let me mention that my first research collaboration that did not involve my thesis supervisors arose when I read Gordon Plotkin's famous "<a href="http://homepages.inf.ed.ac.uk/gdp/publications/Domains_a4.ps">Pisa Notes (On Domain Theory)</a>" with a fellow PhD student. Reading that work led to our first joint paper in 1991 and a companionship that has lasted to this day. I heard <a href="https://www.youtube.com/watch?v=KYfmXpLCiy4&amp;list=PLEyo0HIOhDCSR8os82H_2p5M0dE70K6KL&amp;index=2" target="_blank">Orna Kupferman</a> give the following, tongue-in-cheek advice to young researchers: "Write papers with your twin-sister!" Mine might be: "Write papers with your companion in life!" </p><p>Let me conclude by saying that serendipity and an actual friendship that extends beyond the confines of scientific work were the key aspects in my most pleasant and enduring collaborations. I apologise to the colleagues from whom I have learnt much over the years (former students and postdocs, as well as others) who were the prime movers in research collaborations I did not mention in this post. </p><p> I guess that this note provides much more information than my colleague was intending to receive, but I thought I should put it out for the benefit of the young researchers at Reykjavik University and at the Gran Sasso Science Institute, and of any reader I might have. </p><p>How did your research collaborations arise? If you have anything to add to what I wrote above, and I am sure you do, add your contributions as comments to this post. <br/><br/><br/></p></div>
    </content>
    <updated>2021-02-08T17:42:00Z</updated>
    <published>2021-02-08T17:42:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-02-08T17:42:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21161</id>
    <link href="https://gilkalai.wordpress.com/2021/02/08/to-cheer-you-up-in-difficult-times-20-ben-green-presents-super-polynomial-lower-bounds-for-off-diagonal-van-der-waerden-numbers-w3k/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 20: Ben Green presents super-polynomial lower bounds for off-diagonal van der Waerden numbers W(3,k)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">What will be the next polymath project? click here for our post about it.  New lower bounds for van der Waerden numbers by Ben Green Abstract: We show that there is a red-blue colouring of [N] with no blue 3-term … <a href="https://gilkalai.wordpress.com/2021/02/08/to-cheer-you-up-in-difficult-times-20-ben-green-presents-super-polynomial-lower-bounds-for-off-diagonal-van-der-waerden-numbers-w3k/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/">What will be the next polymath project? click here for our post about it. </a></p>
<h2 class="title mathjax"><a href="https://arxiv.org/abs/2102.01543">New lower bounds for van der Waerden numbers</a> by Ben Green</h2>
<p><span style="color: #0000ff;"><strong>Abstract:</strong> We show that there is a red-blue colouring of <em>[N]</em> with no blue 3-term arithmetic progression and no red arithmetic progression of length <img alt="e^{C(\log N)^{3/4}(\log \log N)^{1/4}}." class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7BC%28%5Clog+N%29%5E%7B3%2F4%7D%28%5Clog+%5Clog+N%29%5E%7B1%2F4%7D%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="e^{C(\log N)^{3/4}(\log \log N)^{1/4}}."/> Consequently, the two-colour van der Waerden number w(3,k) is bounded below by <img alt="k^{b(k)}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7Bb%28k%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k^{b(k)}"/>, where <img alt="b(k)=c(\frac{\log k}{\log \log k})^{1/3}" class="latex" src="https://s0.wp.com/latex.php?latex=b%28k%29%3Dc%28%5Cfrac%7B%5Clog+k%7D%7B%5Clog+%5Clog+k%7D%29%5E%7B1%2F3%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="b(k)=c(\frac{\log k}{\log \log k})^{1/3}"/>. Previously it had been speculated, supported by data, that <img alt="w(3,k)=O(k^2)" class="latex" src="https://s0.wp.com/latex.php?latex=w%283%2Ck%29%3DO%28k%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(3,k)=O(k^2)"/>.</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/gsm.png"><img alt="" class="alignnone size-full wp-image-21182" height="180" src="https://gilkalai.files.wordpress.com/2021/02/gsm.png?w=640&amp;h=180" width="640"/></a></p>
<p><span style="color: #ff0000;">The left side of the picture shows the world record holders for W(3,k). On the left Ben Green (LB) and in the centre Tomasz Schoen (UB). The pictures on the right shows protective mittens for people who make bold mathematical conjectures (<a href="https://gilkalai.wordpress.com/2021/01/19/what-if-they-are-all-wrong/">see Igor Pak’s post</a>) </span></p>
<p>The two-colour van der Waerden number <img alt="w(m,k)" class="latex" src="https://s0.wp.com/latex.php?latex=w%28m%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(m,k)"/> is the smallest  N such that however [N] = {1, . . . , N} is coloured blue and red, there is either a blue m-term arithmetic progression or a red k-term arithmetic progression. The celebrated theorem of van der Waerden implies that <img alt="w(m, k)" class="latex" src="https://s0.wp.com/latex.php?latex=w%28m%2C+k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(m, k)"/> is finite.</p>
<p>The van der Waerden number <img alt="w(m,k)" class="latex" src="https://s0.wp.com/latex.php?latex=w%28m%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(m,k)"/> is analogous to the Ramsey number <img alt="R(m,k)" class="latex" src="https://s0.wp.com/latex.php?latex=R%28m%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="R(m,k)"/>. Finding the behaviors of <img alt="R(m,k)" class="latex" src="https://s0.wp.com/latex.php?latex=R%28m%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="R(m,k)"/> is an important problem in Ramsey theory and much attention is given to <img alt="R(k,k)" class="latex" src="https://s0.wp.com/latex.php?latex=R%28k%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="R(k,k)"/> and of <img alt="R(3,k)" class="latex" src="https://s0.wp.com/latex.php?latex=R%283%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="R(3,k)"/>. Similarly, understanding the values of <img alt="W(m,k)" class="latex" src="https://s0.wp.com/latex.php?latex=W%28m%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="W(m,k)"/>, and especially of <img alt="W(k,k)" class="latex" src="https://s0.wp.com/latex.php?latex=W%28k%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="W(k,k)"/> and <img alt="W(3,k)" class="latex" src="https://s0.wp.com/latex.php?latex=W%283%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="W(3,k)"/>  are also  central problems in Ramsey theory. A big difference between van der Waerden number and Ramsey numbers is that there are density theorems for the existence of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k"/>-terms arithmetic progressions. (Roth’s theorem for <img alt="k=3" class="latex" src="https://s0.wp.com/latex.php?latex=k%3D3&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k=3"/> and Szemeredi’s theorem for general <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k"/>.) There are several important methods to derive those density theorems (including Fourier methods, ergodic methods, and Szemeredi-type regularity) and these methods, as far as I know, do not apply for ordinary Ramsey numbers. (But correct me if I am wrong here, and if I am right and you have some insights as to why ergodic methods or Fourier methods do not apply to “ordinary” Ramsey, please share.)</p>
<p>Green’s paper  studies the values of <img alt="w(3,k)" class="latex" src="https://s0.wp.com/latex.php?latex=w%283%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(3,k)"/>. The best known upper bound is of Tomasz Schoen, <img alt="w(3, k)&lt;e^{k^{1-c}}" class="latex" src="https://s0.wp.com/latex.php?latex=w%283%2C+k%29%3Ce%5E%7Bk%5E%7B1-c%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(3, k)&lt;e^{k^{1-c}}"/> for some constant <img alt="c &gt; 0." class="latex" src="https://s0.wp.com/latex.php?latex=c+%3E+0.&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="c &gt; 0."/> The best known lower bound until the new paper, was by Li and Shu: <img alt="w(3, k) \gg (k/ log k)^2" class="latex" src="https://s0.wp.com/latex.php?latex=w%283%2C+k%29+%5Cgg+%28k%2F+log+k%29%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(3, k) \gg (k/ log k)^2"/>. (This result, as the earlier bound by Robertson, used a probabilistic argument and relied on Lovasz’s local lemma.)</p>
<p>Several people conjectured, also based on empirical data, that <img alt="w(3,k) = O(k^2)" class="latex" src="https://s0.wp.com/latex.php?latex=w%283%2Ck%29+%3D+O%28k%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(3,k) = O(k^2)"/> but now Green proved a super-polynomial lower bound! This is amazing! Congratulations, Ben!</p>
<p>The proof is rather involved and long, so, naturally, there is  little I can say about it, which only slightly exceeds the little I actually know about it. The overview and other fragments of the paper I looked at are very illuminating. Here are a few things that caught my eyes.</p>
<p>1) A word about Tomasz Schoen’s upper bound and important paper: <a href="https://arxiv.org/abs/2006.02877">A subexponential upper bound for van der Waerden numbers W(3,k).</a>  Among other things Schoen’s proof relies on a lemma developed by Schoen for improved Roth bound. This relies on a structure theory of Bateman and Katz.  The paper gives a nice description of the state of the art regarding the diagonal values <img alt="w(k,k)" class="latex" src="https://s0.wp.com/latex.php?latex=w%28k%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(k,k)"/>.</p>
<p>2) Among the people that speculated that <img alt="w(3,k)" class="latex" src="https://s0.wp.com/latex.php?latex=w%283%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(3,k)"/> behaves like <img alt="k^2" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k^2"/> is Ben Green himself. This is recorded in reference [9] of the paper. However, Green’s first reaction to this possibility was that it must be false. But he realized that some ideas for showing that it is false are themselves false.</p>
<p>3) Reference [9] in the paper is: <strong>B. J. Green, 100 open problems, manuscript, available on request</strong>. If you are curious about the list, request it!</p>
<p>4) New lower bounds in Ramsey theory are nor frequent. Thirteen years ago I described <a href="https://gilkalai.wordpress.com/2008/07/10/pushing-behrend-around/">Elkin’s improvement</a> to Behrend’s bound and a few days ago I mentioned <a href="https://gilkalai.wordpress.com/2021/02/03/to-cheer-you-up-in-difficult-times-19-nati-linial-and-adi-shraibman-construct-larger-corner-free-sets-from-better-numbers-on-the-forehead-protocols/">Linial and Shraibman’s new lower bounds</a> for the corner problem.  Green’s study started by looking at complements of 3-AP free sets. An example by Green and Julia Wolf (that followed Elkin’s result) turned out to be important for reaching some parts of Green’s strategy.</p>
<p>5) In some sense, something about the <img alt="k^2" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k^2"/> prediction is not entirely lost. The construction gives a sort of a multi-scale behaviour where in the <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="r"/>th scale the example’s cardinality is <img alt="k^r" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Er&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k^r"/>.  (So all the empirical data comes from the <img alt="r=2" class="latex" src="https://s0.wp.com/latex.php?latex=r%3D2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="r=2"/> regime.) Ben Green asks if the true values of <img alt="w(3,k)" class="latex" src="https://s0.wp.com/latex.php?latex=w%283%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(3,k)"/> exhibit such multi-scale behaviour and conjectures that the true value of <img alt="w(3, k)" class="latex" src="https://s0.wp.com/latex.php?latex=w%283%2C+k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="w(3, k)"/> is quasi-polynomial, namely it lies somewhere in between the bound given by his construction and  something like <img alt="k^{c\log k}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7Bc%5Clog+k%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k^{c\log k}"/>.</p>
<p>6) Until Ben’s list of 100 problems becomes available to you, you may find interest in Francis Su’s <a href="https://www.francissu.com/post/100-questions-about-mathematics">100 questions about mathematics for discussion and reflection</a>.</p>
<p>7) In connection with Linial and Shraibman’s new lower bounds for the corner problem, let me mention that the best upper bound is by I.D. Shkredov. The paper is:  On a two-dimensional analog of Szemeredi’s Theorem in Abelian groups, Izvestiya of Russian Academy of Sciences, 73 (2009), 455–505.</p></div>
    </content>
    <updated>2021-02-08T17:00:33Z</updated>
    <published>2021-02-08T17:00:33Z</published>
    <category term="Combinatorics"/>
    <category term="Number theory"/>
    <category term="Ben Green"/>
    <category term="Tomasz Schoen"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-02-09T09:20:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8474138395715737050</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8474138395715737050/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/the-victoria-delfino-problems-example.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8474138395715737050" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8474138395715737050" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/the-victoria-delfino-problems-example.html" rel="alternate" type="text/html"/>
    <title>The Victoria Delfino Problems: an example of math problems named after a non-mathematician</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> If you Google <b>Victoria Delfino </b>you will find that she is a real estate agent in LA (well, one of the Victoria Delfino's you find is such).  After this blog is posted you may well get this post on the first Google page. </p><p>If you Google <b>Victoria Delfino Problems</b> you will find a paper:</p><p><a href="https://andrescaicedo.files.wordpress.com/2008/04/vdp-finalversion-withreferences.pdf">The fourteen Victoria Delfino Problems and their Status in the year 2015</a></p><p>(ADDED LATER: a comment pointed me to an updated version, so  you can see that- I got to a pay wall.) </p><p>How did a real estate agent get honored by having 14 problems in descriptive set theory named after her?</p><p>Possibilities before I tell you which one.</p><p>1) Real estate is her day job. Her hobby is Descriptive Set Theory. Recall that Fermat was a lawyer (or something like that- see <a href="https://en.wikipedia.org/wiki/Pierre_de_Fermat">his Wikipedia page</a>) so perhaps she is similar. Doubtful- I think math is too hard for that now.  Or at least descriptive  set theory is too hard for that now. </p><p>2) She just happened to remark one day,<i> Gee, I wonder if</i></p><p><i> ZFC + SEP(Sigma_3^1) + #   implies DET(Delta_2^1).</i> </p><p>Its just the kind of thing someone might just say. That was problem 4 of the 14. </p><p>3) There are two Victoria Delfino's- one is a realtor, one is a mathematician. While plausible, that would not be worth blogging about. </p><p>4) And now the truth: Victoria was the realtor who helped Moschovakis (a descriptive set theorist who I will henceforth describe as M) buy his house. When Tony Martin (another Desc. Set Theorist) moved to UCLA, M referred him to Victoria and she did indeed help Tony find a house. Victoria gave M a large commission which he tried to turn down. She did not want it returned, so M used the money to fund five problems. Later problems were added, but for no money. The article <i>The Fourteen... </i>linked to above has the full story. It also has the curious line: </p><p><i>Contrary to popular belief, no monetary prize is attached to further problems. </i></p><p>I didn't think any of this was so well known as to have popular believes. </p><p>ANYWAY, this is an example of a math problem named after a non-math person. Are there others? Will the name stick? Probably not- already 12 of the 14 are solved. I have noted in a prior blog (<a href="https://blog.computationalcomplexity.org/2009/08/how-much-credit-should-conjecturer-get_14.html">here</a>) once a conjecture gets proven, the one who made the conjecture gets forgotten. Or in this case the person who the conjectures is named after. </p><p>So are there other open problems in math named after non-math people? How about Theorems?</p><p>Near Misses: </p><p>Pythagoras: Not clear what he had to do with the theorem that bears his name. </p><p>L'hopital's Rule: the story could be a blog in itself, and in fact it is! Not mind, but someone else: <a href="https://andrescaicedo.wordpress.com/2013/11/05/credit/">here</a>. However L'hopital was a mathematician. </p><p>Sheldon's conjecture (see <a href="https://blog.computationalcomplexity.org/2019/10/the-sheldon-conjecture-too-late-for.html">here</a>) was named after a FICTIONAL physicist. Note that Sheldon inspired the conjecture but did not make it. It has been solved. </p><p>The Governor's  Theorem (see <a href="https://blog.computationalcomplexity.org/2013/08/how-much-trig-does-your-governor-know.html">here</a>) was named because Jeb Bush was asked for the angles of a 3-4-5 right triangle (not a fair question). </p><p>The Monty Hall Paradox.</p><p>SO- are there Open Problems, Theorems, Lemmas, any math concepts, named after non-math people? I really mean non-STEM people. If a Physicist or an Engineer or a Chemist or Biologist or...  has their name on something, that would not really be what I want.</p><p>(ADDED LATER - someone emailed me two oddly-named math things:</p><p>Belphegor's prime, <span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">see </span><a href="https://en.wikipedia.org/wiki/Belphegor%27s_prime">here</a></p><p>Morrie's law- odd since Morrie is the FIRST name of who the name is honoring, see <a href="https://en.wikipedia.org/wiki/Morrie%27s_law">here</a> </p><p>)</p><p><br/></p><p>Are there any other open problems in descriptive set theory  named after realtors?</p><p><i><br/></i></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-02-08T04:55:00Z</updated>
    <published>2021-02-08T04:55:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-02-09T06:25:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.03311</id>
    <link href="http://arxiv.org/abs/2102.03311" rel="alternate" type="text/html"/>
    <title>Online Bin Packing with Predictions</title>
    <feedworld_mtime>1612742400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Spyros Angelopoulos, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamali:Shahin.html">Shahin Kamali</a>, Kimia Shadkami <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.03311">PDF</a><br/><b>Abstract: </b>Bin packing is a classic optimization problem with a wide range of
applications from load balancing in networks to supply chain management. In
this work we study the online variant of the problem, in which a sequence of
items of various sizes must be placed into a minimum number of bins of uniform
capacity. The online algorithm is enhanced with a (potentially erroneous)
prediction concerning the frequency of item sizes in the sequence. We design
and analyze online algorithms with efficient tradeoffs between their
consistency (i.e., the competitive ratio assuming no prediction error) and
their robustness (i.e., the competitive ratio under adversarial error), and
whose performance degrades gently as a function of the error. Previous work on
this problem has only addressed the extreme cases with respect to the
prediction error, and has relied on overly powerful and error-free prediction
oracles.
</p></div>
    </summary>
    <updated>2021-02-08T22:42:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.03304</id>
    <link href="http://arxiv.org/abs/2102.03304" rel="alternate" type="text/html"/>
    <title>A $2$-Approximation Algorithm for Flexible Graph Connectivity</title>
    <feedworld_mtime>1612742400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boyd:Sylvia.html">Sylvia Boyd</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheriyan:Joseph.html">Joseph Cheriyan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haddadan:Arash.html">Arash Haddadan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ibrahimpur:Sharat.html">Sharat Ibrahimpur</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.03304">PDF</a><br/><b>Abstract: </b>We present a $2$-approximation algorithm for the Flexible Graph Connectivity
problem [AHM20] via a reduction to the minimum cost $r$-out $2$-arborescence
problem.
</p></div>
    </summary>
    <updated>2021-02-08T22:39:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.03303</id>
    <link href="http://arxiv.org/abs/2102.03303" rel="alternate" type="text/html"/>
    <title>Length of a Full Steiner Tree as a Function of Terminal Coordinates</title>
    <feedworld_mtime>1612742400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uteshev:Alexei_Yu=.html">Alexei Yu. Uteshev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Semenova:Elizaveta_A=.html">Elizaveta A. Semenova</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.03303">PDF</a><br/><b>Abstract: </b>Given the coordinates of the terminals $ \{(x_j,y_j)\}_{j=1}^n $ of the full
Euclidean Steiner tree, its length equals $$ \left| \sum_{j=1}^n z_j U_j
\right| \, , $$ where $ \{z_j:=x_j+ \mathbf i y_j\}_{j=1}^n $ and $
\{U_j\}_{j=1}^n $ are suitably chosen $ 6 $th roots of unity. We also extend
this result for the cost of the optimal Weber networks which are topologically
equivalent to some full Steiner trees.
</p></div>
    </summary>
    <updated>2021-02-08T22:48:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-02-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.03277</id>
    <link href="http://arxiv.org/abs/2102.03277" rel="alternate" type="text/html"/>
    <title>Minimum projective linearizations of trees in linear time</title>
    <feedworld_mtime>1612742400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alemany=Puig:Llu=iacute=s.html">Lluís Alemany-Puig</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esteban:Juan_Luis.html">Juan Luis Esteban</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferrer=i=Cancho:Ramon.html">Ramon Ferrer-i-Cancho</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.03277">PDF</a><br/><b>Abstract: </b>The minimum linear arrangement problem (MLA) consists of finding a mapping
$\pi$ from vertices of a graph to integers that minimizes $\sum_{uv\in
E}|\pi(u) - \pi(v)|$. For trees, various algorithms are available to solve the
problem in polynomial time; the best known runs in subquadratic time in
$n=|V|$. There exist variants of the MLA in which the arrangements are
constrained to certain classes of projectivity. Iordanskii, and later Hochberg
and Stallmann (HS), put forward $O(n)$-time algorithms that solve the problem
when arrangements are constrained to be planar. We also consider linear
arrangements of rooted trees that are constrained to be projective. Gildea and
Temperley (GT) sketched an algorithm for the projectivity constraint which, as
they claimed, runs in $O(n)$ but did not provide any justification of its cost.
In contrast, Park and Levy claimed that GT's algorithm runs in $O(n \log
d_{max})$ where $d_{max}$ is the maximum degree but did not provide sufficient
detail. Here we correct an error in HS's algorithm for the planar case, show
its relationship with the projective case, and derive an algorithm for the
projective case that runs undoubtlessly in $O(n)$-time.
</p></div>
    </summary>
    <updated>2021-02-08T22:40:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.03173</id>
    <link href="http://arxiv.org/abs/2102.03173" rel="alternate" type="text/html"/>
    <title>Reconstructing Arbitrary Trees from Traces in the Tree Edit Distance Model</title>
    <feedworld_mtime>1612742400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Thomas Maranzatto, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reyzin:Lev.html">Lev Reyzin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.03173">PDF</a><br/><b>Abstract: </b>In this paper, we consider the problem of reconstructing trees from traces in
the tree edit distance model. Previous work by Davies et al. (2019) analyzed
special cases of reconstructing labeled trees. In this work, we significantly
expand our understanding of this problem by giving general results in the case
of arbitrary trees. Namely, we give: a reduction from the tree trace
reconstruction problem to the more classical string reconstruction problem when
the tree topology is known, a lower bound for learning arbitrary tree
topologies, and a general algorithm for learning the topology of any tree using
techniques of Nazarov and Peres (2017). We conclude by discussing why arbitrary
trees require exponentially many samples under the left propagation model.
</p></div>
    </summary>
    <updated>2021-02-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.03117</id>
    <link href="http://arxiv.org/abs/2102.03117" rel="alternate" type="text/html"/>
    <title>Twin-width IV: low complexity matrices</title>
    <feedworld_mtime>1612742400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonnet:=Eacute=douard.html">Édouard Bonnet</a>, Ugo Giocanti, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mendez:Patrice_Ossona_de.html">Patrice Ossona de Mendez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thomass=eacute=:St=eacute=phan.html">Stéphan Thomassé</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.03117">PDF</a><br/><b>Abstract: </b>We establish a list of characterizations of bounded twin-width for
hereditary, totally ordered binary structures. This has several consequences.
First, it allows us to show that a (hereditary) class of matrices on a finite
alphabet either contains at least $n!$ matrices of size $n \times n$, or at
most $c^n$ for some constant $c$. This generalizes the celebrated Stanley-Wilf
conjecture/Marcus-Tardos theorem from permutation classes to any matrix class
on a finite alphabet, answers our small conjecture [SODA '21] in the case of
ordered graphs, and with more work, settles a question first asked by Balogh,
Bollob\'as, and Morris [Eur. J. Comb. '06] on the growth of hereditary classes
of ordered graphs. Second, it gives a fixed-parameter approximation algorithm
for twin-width on ordered graphs. Third, it yields a full classification of
fixed-parameter tractable first-order model checking on hereditary classes of
ordered binary structures. Fourth, it provides an alternative proof to a
model-theoretic characterization of classes with bounded twin-width announced
by Simon and Toru\'nczyk.
</p></div>
    </summary>
    <updated>2021-02-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.03069</id>
    <link href="http://arxiv.org/abs/2102.03069" rel="alternate" type="text/html"/>
    <title>Foldover-free maps in 50 lines of code</title>
    <feedworld_mtime>1612742400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Vladimir Garanzha, Igor Kaporin, Liudmila Kudryavtseva, François Protais, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ray:Nicolas.html">Nicolas Ray</a>, Dmitry Sokolov <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.03069">PDF</a><br/><b>Abstract: </b>Mapping a triangulated surface to 2D space (or a tetrahedral mesh to 3D
space) is the most fundamental problem in geometry processing.In computational
physics, untangling plays an important role in mesh generation: it takes a mesh
as an input, and moves the vertices to get rid of foldovers.In fact, mesh
untangling can be considered as a special case of mapping where the geometry of
the object is to be defined in the map space and the geometric domain is not
explicit, supposing that each element is regular.In this paper, we propose a
mapping method inspired by the untangling problem and compare its performance
to the state of the art.The main advantage of our method is that the untangling
aims at producing locally injective maps, which is the major challenge of
mapping.In practice, our method produces locally injective maps in very
difficult settings, and with less distortion than the previous work, both in 2D
and 3D. We demonstrate it on a large reference database as well as on more
difficult stress tests.For a better reproducibility, we publish the code in
Python for a basic evaluation, and in C++ for more advanced applications.
</p></div>
    </summary>
    <updated>2021-02-08T22:47:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-02-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.03004</id>
    <link href="http://arxiv.org/abs/2102.03004" rel="alternate" type="text/html"/>
    <title>The Critical Mean-field Chayes-Machta Dynamics</title>
    <feedworld_mtime>1612742400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanca:Antonio.html">Antonio Blanca</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sinclair:Alistair.html">Alistair Sinclair</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Xusheng.html">Xusheng Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.03004">PDF</a><br/><b>Abstract: </b>The random-cluster model is a unifying framework for studying random graphs,
spin systems and random networks. The model is closely related to the classical
ferromagnetic Ising and Potts models and is often viewed as a generalization of
these models. In this paper, we study a natural non-local Markov chain known as
the Chayes-Machta dynamics for the mean-field case of the random-cluster model,
where the underlying graph is the complete graph on $n$ vertices. The
random-cluster model is parametrized by an edge probability $p$ and a cluster
weight $q$. Our focus is on the critical regime: $p = p_c(q)$ and $q \in
(1,2)$, where $p_c(q)$ is the threshold corresponding to the order-disorder
phase transition of the model. We show that the mixing time of the
Chayes-Machta dynamics is $O(\log n \cdot \log \log n)$ in this parameter
regime, which reveals that the dynamics does not undergo an exponential
slowdown at criticality, a surprising fact that had been predicted (but not
proved) by statistical physicists. This provides a nearly optimal bound (up to
the $\log\log n$ factor) for the mixing time of the mean-field Chayes-Machta
dynamics in the only regime of parameters where no previous bound was known.
Our proof consists of a multi-phased coupling argument that combines several
key ingredients, including a new local limit theorem, a precise bound on the
maximum of symmetric random walks with varying step sizes, and tailored
estimates for critical random graphs.
</p></div>
    </summary>
    <updated>2021-02-08T22:44:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.02931</id>
    <link href="http://arxiv.org/abs/2102.02931" rel="alternate" type="text/html"/>
    <title>Cutoff stability under distributional constraints with an application to summer internship matching</title>
    <feedworld_mtime>1612742400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Haris Aziz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Baychkov:Anton.html">Anton Baychkov</a>, Peter Biro <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.02931">PDF</a><br/><b>Abstract: </b>We introduce a new two-sided stable matching problem that describes the
summer internship matching practice of an Australian university. The model is a
case between two models of Kamada and Kojima on matchings with distributional
constraints. We study three solution concepts, the strong and weak stability
concepts proposed by Kamada and Kojima, and a new one in between the two,
called cutoff stability. Kamada and Kojima showed that a strongly stable
matching may not exist in their most restricted model with disjoint regional
quotas. Our first result is that checking its existence is NP-hard. We then
show that a cutoff stable matching exists not just for the summer internship
problem but also for the general matching model with arbitrary heredity
constraints. We present an algorithm to compute a cutoff stable matching and
show that it runs in polynomial time in our special case of summer internship
model. However, we also show that finding a maximum size cutoff stable matching
is NP-hard, but we provide a Mixed Integer Linear Program formulation for this
optimisation problem.
</p></div>
    </summary>
    <updated>2021-02-08T22:43:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.02873</id>
    <link href="http://arxiv.org/abs/2102.02873" rel="alternate" type="text/html"/>
    <title>Optimal Construction of Hierarchical Overlap Graphs</title>
    <feedworld_mtime>1612742400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Shahbaz Khan <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.02873">PDF</a><br/><b>Abstract: </b>Genome assembly is a fundamental problem in Bioinformatics, where for a given
set of overlapping substrings of a genome, the aim is to reconstruct the source
genome. The classical approaches to solving this problem use assembly graphs,
such as de Bruijn graphs or overlap graphs, which maintain partial information
about such overlaps. For genome assembly algorithms, these graphs present a
trade-off between overlap information stored and scalability. Thus,
Hierarchical Overlap Graph (HOG) was proposed to overcome the limitations of
both these approaches.
</p>
<p>For a given set $P$ of $n$ strings, the first algorithm to compute HOG was
given by Cazaux and Rivals [IPL20] requiring $O(||P||+n^2)$ time using
superlinear space, where $||P||$ is the cummulative sum of the lengths of
strings in $P$. This was improved by Park et al. [SPIRE20] to $O(||P||\log n)$
time and $O(||P||)$ space using segment trees, and further to
$O(||P||\frac{\log n}{\log \log n})$ for the word RAM model. Both these results
described an open problem to compute HOG in optimal $O(||P||)$ time and space.
In this paper, we achieve the desired optimal bounds by presenting a simple
algorithm that does not use any complex data structures.
</p></div>
    </summary>
    <updated>2021-02-08T22:43:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1475</id>
    <link href="https://ptreview.sublinear.info/?p=1475" rel="alternate" type="text/html"/>
    <title>News for January 2021</title>
    <summary>The first month of 2021 has brought with it 5 papers, covering graph testing, Boolean function testing, and distribution testing — as well as database theory. Let’s dive into it. Random walks and forbidden minors III: \(\mathrm{poly}(d/\varepsilon)\)-time partition oracles for minor-free graph classes, by Akash Kumar, C. Seshadhri, and Andrew Stolman (arXiv). Minor-closed bounded-degree graphs […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The first month of 2021 has brought with it 5 papers, covering graph testing, Boolean function testing, and distribution testing — as well as database theory. Let’s dive into it.</p>



<p><strong>Random walks and forbidden minors III: \(\mathrm{poly}(d/\varepsilon)\)-time partition oracles for minor-free graph classes</strong>, by Akash Kumar, C. Seshadhri, and Andrew Stolman (<a href="https://arxiv.org/abs/2102.00556">arXiv</a>). Minor-closed bounded-degree graphs have a very nice property: denoting by \(d\) the degree bound and \(n\) the number of edges, it is always possible to partition any such graph into components of <em>constant</em> size, \(O_\varepsilon(1)\), just by removing a linear number of edges, merely \(\varepsilon dn\) (\(\varepsilon\) being a small parameter). This is a crucial result in many graph property testing algorithms, those which rely on something called a “partition oracle”: loosely speaking, a routine which makes “few” queries to the graph, and is able to indicate which component of an underlying partition any given vertex belongs to. But what is “few” here? The first partition oracles made \(d^{\mathrm{poly}(d,1/\varepsilon)}\) queries to the graph to answer any such request. This later got significantly improved to \(d^{\mathrm{log}(d/\varepsilon)}\). Using spectral graph theory techniques previously developed by the authors (hence the “III” in the title), this work settles the question, achieving partition oracles which as good as it gets: making only \(\mathrm{poly}(d,1/\varepsilon)\) queries to the graph! This in turns has immediate consequences for graph property testing, which the paper details.</p>



<p>And since we are on the topic of oracles… more exciting news on that front:</p>



<p><strong>Spectral Clustering Oracles in Sublinear Time</strong>, by Grzegorz Gluch, Michael Kapralov, Silvio Lattanzi, Aida Mousavifar, Christian Sohler (<a href="https://arxiv.org/abs/2101.05549">arXiv</a>). Given a graph \(G\) and an integer \(k\), one often want to partition the graph into components \(C_1,C_2,\dots,C_k\), such that each \(C_i\) is well-connected and has few edges to the other \(C_j\)’s. But can we get <em>ultra</em> efficient algorithms for such spectral clusterings? Specifically, can we design oracles for them: sublinear-time algorithms which provide implicit access to an underlying “good” spectral clustering \(\hat{C}_1,\hat{C}_2,\dots,\hat{C}_k\), by returning on any query vertex \(v\) the index of the cluster \(\hat{C}_i\) to which \(v\) belongs? This paper introduces the question, and answers it in the affirmative: in more detail, it provides a spectral clustering oracle which, for any \(\varepsilon&gt;0\), has preprocessing time \(2^{\mathrm{poly}(k/\varepsilon)}n^{1/2+O(\varepsilon)}\), query time \(n^{1/2+O(\varepsilon)}\), and space \(n^{1/2+O(\varepsilon)}\); and provides access to a clustering with relative error \(O(\varepsilon\log k)\) per cluster. The paper also allows tradeoffs between query time and space, and discusses applications to the Local Computation Algorithms (LCA) model.</p>



<p>Next stop, distribution testing…</p>



<p><strong>The Sample Complexity of Robust Covariance Testing</strong>, by Ilias Diakonikolas and Daniel M. Kane (<a href="https://arxiv.org/abs/2012.15802">arXiv</a>). Suppose you have i.i.d. samples from some high-dimensional Gaussian \(\mathcal{N}(0,\Sigma)\) in \(d\) dimensions, and want to test whether the unknown covariance matrix \(\Sigma\) is the identity, versus \(\varepsilon\)-far from it (in Frobenius norm). Good news: we know how to do that, and \(\Theta(d/\varepsilon^2)\) samples are necessary and sufficient. (To <em>learn</em> \(\Sigma\), that’d be \(\Theta(d^2/\varepsilon^2)\).) Bad news: you don’t have i.i.d. samples from some high-dimensional Gaussian \(\mathcal{N}(0,\Sigma)\); what you have is i.i.d. samples from a noisy version of it, \((1-\alpha)\mathcal{N}(0,\Sigma) + \alpha B\), where \(B\) is an arbitrary “bad” distribution (not necessarily Gaussian itself). You still want to test whether the covariance \(\Sigma\) is the identity, but now you have that extra \(\alpha\) fraction of noisy samples, and you need to do that testing robustly… The good news is, you can still do that by learning the covariance matrix \(\Sigma\), robustly, with \(O(d^2/\varepsilon^2)\) samples. The bad news is the main result of this paper: that’s also the best you can do. That is, \(\Omega(d^2)\) samples are necessary: if you have to be robust to noise, testing is no longer easier than learning….</p>



<p>Onto Boolean functions!</p>



<p><strong>Junta Distance Approximation with Sub-Exponential Queries</strong>, by Vishnu Iyer, Avishay Tal, and Michael Whitmeyer (<a href="https://eccc.weizmann.ac.il/report/2021/004/">ECCC</a>). If you follow this blog, you may have seen over the past couple years a flurry of results about <em>tolerant junta testing</em>: “given query access to some Boolean function \(f\colon\{-1,1\}^n\to\{-1,1\}\), how close is \(f\) to only depending on \(k \ll n\) of its variables?”<br/>This paper contains several results on this problem, including an improved bicriteria tolerant testing algorithm: an efficient algorithm to distinguish between \(\varepsilon\)-close to \(k\)-junta and \(1.1\varepsilon\)-far from \(k’\)-junta making \(\mathrm{poly}(k,1/\varepsilon)\) queries (and \(k’ = O(k/\varepsilon^2)\)). But the main result of the paper, and the one giving it its name, is for the non-relaxed version where \(k’=k\): while all previous works had a query complexity \(2^{O(k)}\), here the authors show how to break that exponential barrier, giving a fully tolerant testing algorithm with query complexity \(2^{\tilde{O}(\sqrt{k})}\)!</p>



<p>And finally, a foray into database theory:</p>



<p><strong>Towards Approximate Query Enumeration with Sublinear Preprocessing Time</strong>, by  Isolde Adler and Polly Fahey (<a href="https://arxiv.org/abs/2101.06240">arXiv</a>). In this paper, the authors are concerned with the task of (approximate) query enumeration on databases, aiming for ultra efficient (i.e., sublinear-time) algorithms. Leveraging techniques from property testing (specifically, in the bounded-degree graph model), they show the following:<br/><em>On input databases of bounded degree and bounded tree-width, every (fixed) first-order definable query can be enumerated approximately in time linear in the output size, after only a sublinear-time preprocessing phase</em>.</p>



<p>That’s all for this month! If you noticed a paper we missed, please let us know in the comments.</p></div>
    </content>
    <updated>2021-02-07T10:52:38Z</updated>
    <published>2021-02-07T10:52:38Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-02-08T22:50:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=83</id>
    <link href="https://dstheory.wordpress.com/2021/02/05/thursday-feb-18-costis-daskalakis-from-mit/" rel="alternate" type="text/html"/>
    <title>Thursday Feb 18 — Costis Daskalakis from MIT</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Welcome to the Spring 2021 edition of Foundations of Data Science Virtual Talks. Our first talk for the season will take place on Thursday, Feb 18th at 11:00 AM Pacific Time (14:00 Eastern Time, 20:00 Central European Time, 19:00 UTC).  Costis Daskalakis from MIT will speak about “Equilibrium Computation and the Foundations of Deep Learning.” Please<a class="more-link" href="https://dstheory.wordpress.com/2021/02/05/thursday-feb-18-costis-daskalakis-from-mit/">Continue reading <span class="screen-reader-text">"Thursday Feb 18 — Costis Daskalakis from MIT"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Welcome to the Spring 2021 edition of Foundations of Data Science Virtual Talks. </p>



<p>Our first talk for the season will take place on <strong>Thursday, Feb 18</strong>th at <strong>11:00 AM Pacific Time</strong> (14:00 Eastern Time, 20:00 Central European Time, 19:00 UTC).  <strong>Costis Daskalakis</strong> from <strong>MIT</strong> will speak about “<strong>Equilibrium Computation and the Foundations of Deep Learning</strong>.”</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: Deep Learning has recently yielded important advances in single-agent learning challenges, much of that progress being fueled by the empirical success of gradient descent and its variants in computing local optima of non-convex optimization problems. In multi-agent learning applications, the role of single-objective optimization is played by equilibrium computation, yet our understanding of its complexity in settings that are relevant for Deep Learning remains sparse. In this talk we focus on min-max optimization of nonconvex-nonconcave objectives, which has found applications in GANs, and other adversarial learning problems. Here, not only are there no known gradient-descent based methods converging to even local and approximate min-max equilibria, but the computational complexity of identifying them remains poorly understood. We show that finding approximate local min-max equilibria of Lipschitz and smooth objectives requires a number of queries to the function and its gradient that is exponential in the relevant parameters, in sharp contrast to the polynomial number of queries required to find approximate local minima of non convex objectives. Our oracle lower bound is a byproduct of a complexity-theoretic result showing that finding approximate local min-max equilibria is computationally equivalent to finding Brouwer fixed points, and Nash equilibria in non zero-sum games, and thus PPAD-complete.</p>



<p>Minimal complexity theory knowledge will be assumed in the talk. Joint work with Stratis Skoulakis and Manolis Zampetakis.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2021-02-05T15:50:20Z</updated>
    <published>2021-02-05T15:50:20Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2021-02-09T09:22:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-7077954644615536585</id>
    <link href="http://processalgebra.blogspot.com/feeds/7077954644615536585/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=7077954644615536585" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7077954644615536585" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7077954644615536585" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/02/support-research-in-foundations-of.html" rel="alternate" type="text/html"/>
    <title>Support research in the Foundations of Computing at the University of Leicester!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In an ideal world, university administrators would support the work of the top-class academics employed by their institution, especially if they attract students, have a high research standing within their communities and bring in substantial funding from competitive research funds. After all, to quote <a href="https://en.wikipedia.org/wiki/Isidor_Isaac_Rabi" target="_blank">Isidor Isaac Rabi</a>, "<a href="https://academicanchor.wordpress.com/2012/08/09/dwight-eisenhower-and-university-faculty/" target="_blank">the faculty are the university</a>" and the most valuable currency for an academic institution is reputation. </p><p>Unfortunately, university administrations the world over repeatedly surprise me by making structural changes that affect some of their very best academics and actually <i>reduce</i> the reputation of their institutions in the eyes of the community at large. </p><p>The latest example comes from the University of Leicester, where, as stated <a href="https://www.ipetitions.com/petition/foco-is-not-redundant" target="_blank">here</a>, </p><p style="margin-left: 40px; text-align: left;">[the] University VC proposes to merge Informatics and Mathematics into a  combined school focussed exclusively on AI, data science, computational  modelling and "digitalisation". This includes the proposal to cease  research in Foundations of Computer Science (FoCo) where research is  "highly theoretical and not directly linked with applications",  retaining staff only if the research they have published in the past (!)  aligns well with the new desired focus on foundations of AI,  computational modelling, data science and digitalisation. Staff have  been given no opportunity to alter their research to fit with the  proposed new direction. The plan is <u>to make redundant (in the middle of a pandemic) all (up to 10) staff in foundations of computer science</u> whose past research is deemed not to be a good enough fit with the new strategic priorities. </p><p>See also <a href="https://www.uculeicester.org.uk/ucu/first-statement-on-threatened-compulsory-redundancies/" target="_blank">this statement</a> by the University and College Union of the University of Leicester. </p><p>I might be biased, but I find it inconceivable that one can think of building a world-class research programme in AI, data science and computational modelling without building on existing strengths in the Foundations of Computer Science and Mathematics. What my crystal ball tells me is that the strong Leicester academics who might be affected by the planned restructuring will find positions elsewhere and that the University of Leicester is shooting itself in the foot. Which high-profile academic would be enticed to join a university that has shown so little consideration for its existing areas of strength and where one's job might be in danger when the buzzwords du jour change, as they undoubtedly will? </p><p>I encourage you to sign the <a href="https://www.ipetitions.com/petition/foco-is-not-redundant" target="_blank">petition</a> in support of our Leicester colleagues. Kudos to <a href="https://en.wikipedia.org/wiki/Isobel_Armstrong" target="_blank">Isobel Armstrong</a>, FBA, for <a href="https://twitter.com/leicesterucu/status/1355277601980489729" target="_blank">returning her honorary doctorate</a> to the University of Leicester upon hearing of their plans!<br/></p></div>
    </content>
    <updated>2021-02-05T11:30:00Z</updated>
    <published>2021-02-05T11:30:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-02-08T17:42:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=528</id>
    <link href="https://tcsplus.wordpress.com/2021/02/04/tcs-talk-wednesday-february-17-william-hoza-ut-austin/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, February 17 — William Hoza, UT Austin</title>
    <summary>Welcome back for a new season of TCS+! Our talks are back to a fortnightly schedule, with an exciting slate of speakers ahead of us. The first TCS+ talk will take place in two weeks, Wednesday, February 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). William Hoza […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Welcome back for a new season of TCS+! Our talks are back to a fortnightly schedule, with an exciting slate of speakers ahead of us.</p>
<p>The first TCS+ talk will take place in two weeks, Wednesday, February 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>William Hoza</strong> from UT Austin will speak about “<em>Fooling Constant-Depth Threshold Circuits</em>” (abstract below). You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The talk will be recorded and posted on our website and <a href="https://www.youtube.com/user/TCSplusSeminars/videos">YouTube channel</a> afterwards, so people who did not sign up will still be able to watch the talk.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: We present the first non-trivial pseudorandom generator (PRG) for linear threshold (LTF) circuits of arbitrary constant depth and super-linear size. This PRG fools circuits with depth <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="d"/> and <img alt="n^{1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1+%2B+%5Cdelta%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n^{1 + \delta}"/> wires, where <img alt="\delta = \exp(-O(d))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cexp%28-O%28d%29%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\delta = \exp(-O(d))"/>, using seed length <img alt="O(n^{1 - \delta})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B1+-+%5Cdelta%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="O(n^{1 - \delta})"/> and with error <img alt="\exp(-n^{\delta})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-n%5E%7B%5Cdelta%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\exp(-n^{\delta})"/>. This tightly matches the best known lower bounds for this circuit class. As a consequence of our result, all the known hardness for LTF circuits has now effectively been translated into pseudorandomness. This brings the extensive effort in the last decade to construct PRGs and deterministic circuit-analysis algorithms for this class to the point where any subsequent improvement would yield breakthrough lower bounds.</p>
<p>A key ingredient in our construction is a pseudorandom restriction procedure that has tiny failure probability, but simplifies the function to a non-natural “hybrid computational model” that combines decision trees and LTFs. As part of our proof we also construct an “extremely low-error” PRG for the class of functions computable by an arbitrary function of s linear threshold functions that can handle even the extreme setting of parameters <img alt="s = n/\mathrm{polylog}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+n%2F%5Cmathrm%7Bpolylog%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="s = n/\mathrm{polylog}(n)"/> and <img alt="\epsilon = \exp(-n/\mathrm{polylog}(n))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+%5Cexp%28-n%2F%5Cmathrm%7Bpolylog%7D%28n%29%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\epsilon = \exp(-n/\mathrm{polylog}(n))"/>.</p>
<p>Joint work with Pooya Hatami, Avishay Tal, and Roei Tell.</p></blockquote></div>
    </content>
    <updated>2021-02-05T03:51:52Z</updated>
    <published>2021-02-05T03:51:52Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-02-09T09:21:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18070</id>
    <link href="https://rjlipton.wordpress.com/2021/02/04/graph-products/" rel="alternate" type="text/html"/>
    <title>Graph Products</title>
    <summary>The power of definitions and notations Leopold Kronecker was one of the great mathematicians of the 19th century. He thought about foundational issues deeply. We highlighted him before—well not deeply. Today I thought we would talk about some core math ideas arising out of Kronecker’s work. Kronecker’s angle as an early leader in the modern […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>The power of definitions and notations</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2021/02/04/graph-products/ikronec001p1/" rel="attachment wp-att-18075"><img alt="" class="alignright  wp-image-18075" src="https://rjlipton.files.wordpress.com/2021/02/kro.jpg?w=120" width="120"/></a></p>
<p>
Leopold Kronecker was one of the great mathematicians of the <a href="https://en.wikipedia.org/wiki/Leopold_Kronecker">19th century</a>. He thought about foundational issues deeply. We highlighted him <a href="https://rjlipton.wordpress.com/2012/08/31/high-school-trig-is-powerful/">before</a>—well not deeply.</p>
<p>
Today I thought we would talk about some core math ideas arising out of Kronecker’s work.</p>
<p>
Kronecker’s angle as an early leader in the modern <a href="https://en.wikipedia.org/wiki/Foundations_of_mathematics">foundations</a> of mathematics was on which aspects are helpful in concrete analysis. He no doubt would be comfortable with complexity theory, with our interest in not just existence proofs, but in concrete algorithms for the construction of objects. He famously said:</p>
<blockquote><p><b> </b> <em> <i>God made the integers, all else is the work of man.</i> </em>
</p></blockquote>
<p/><p>
His was a philosophy that would agree with our view of complexity theory. Maybe he would say now:</p>
<blockquote><p><b> </b> <em> <i>God made the binary strings, all else is the work of people.</i> </em>
</p></blockquote>
<p>
</p><p/><h2> Operations </h2><p/>
<p/><p>
In any part of mathematics we are often interested in operations that take objects and make new objects. These operations are important as they allow us to build new interesting objects. </p>
<ul>
<li>
In strings we can take two and concatenate them to make a new one. <p/>
</li><li>
In matrices we can add or multiply them. <p/>
</li><li>
More relevant to today’s topic we can take two matrices and make the <a href="https://en.wikipedia.org/wiki/Kronecker_product">Kronecker </a> product: <p/>
<p align="center"> <a href="https://rjlipton.wordpress.com/2021/02/04/graph-products/prod/" rel="attachment wp-att-18079"><img alt="" class="aligncenter size-full wp-image-18079" src="https://rjlipton.files.wordpress.com/2021/02/prod.png?w=600"/></a> </p>
</li><li>
In graph theory we can take two graphs <img alt="{G_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G_1}"/> and <img alt="{G_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G_2}"/> and make a new one <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{H}"/>.
</li></ul>
<p>
There are a wealth of such operations on graphs. One is called Cartesian product, one the strong product, one the direct product. A trouble, in my opinion, is that the names and the notation for these operations is not uniform. There are alternative names for almost all the major operations: For example,</p>
<blockquote><p><b> </b> <em> The Cartesian product of graphs is sometimes called the box product of graphs—see <a href="https://en.wikipedia.org/wiki/Cartesian_product_of_graphs">here</a>. The notation that <img alt="{G \times H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%5Ctimes+H%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G \times H}"/> has often been used for Cartesian products of graphs, but is now more commonly used for another construction known as the tensor product of graphs. </em>
</p></blockquote>
<p/><p>
Confusing, no? </p>
<p>
Ken notes that it gets even more confusing when one teaches the Cartesian product construction of finite automata. Each automaton has a state graph. In general the state graph is directed and the edges are labeled by characters, but one can make cases where the graph is undirected and there is only one character. The state graph of the product machine is in general <em>not</em> the Cartesian product of the graphs of the individual machines. What product is it? Let’s look at graph operations.</p>
<p>
</p><p/><h2> Towards a Uniform Definition </h2><p/>
<p/><p>
Let us give a uniform definition of three basic graph <a href="https://en.wikipedia.org/wiki/Graph_product">products</a>. Let <img alt="{G_1,\dots,G_m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG_1%2C%5Cdots%2CG_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G_1,\dots,G_m}"/> be undirected graphs. Then 	</p>
<p align="center"><img alt="\displaystyle  H = \square(d,G_1,\dots,G_m) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++H+%3D+%5Csquare%28d%2CG_1%2C%5Cdots%2CG_m%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  H = \square(d,G_1,\dots,G_m) "/></p>
<p>is the graph with vertices <img alt="{(x_1,\dots,x_m)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x_1%2C%5Cdots%2Cx_m%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(x_1,\dots,x_m)}"/> so that each <img alt="{x_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x_k}"/> is a vertex in <img alt="{G_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G_k}"/>, and the vertices <img alt="{(x_1,\dots,x_m)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x_1%2C%5Cdots%2Cx_m%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(x_1,\dots,x_m)}"/> and <img alt="{(y_1,\dots,y_m)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28y_1%2C%5Cdots%2Cy_m%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(y_1,\dots,y_m)}"/> are connected provided for exactly <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d}"/> indices <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{k}"/>, <img alt="{x_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x_k}"/> and <img alt="{y_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{y_k}"/> are an edge in <img alt="{G_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G_k}"/> and for the rest <img alt="{x_k=y_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_k%3Dy_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x_k=y_k}"/>.</p>
<ul>
<li>
The <a href="https://en.wikipedia.org/wiki/Cartesian_product_of_graphs">Cartesian</a> product requires exactly one edge: 	<p/>
<p align="center"><img alt="\displaystyle  \square(1,G_1,\dots,G_m). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csquare%281%2CG_1%2C%5Cdots%2CG_m%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \square(1,G_1,\dots,G_m). "/></p>
<p>It is usually written as 	 	</p>
<p align="center"><img alt="\displaystyle  G_1 \square\cdots \square G_m." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++G_1+%5Csquare%5Ccdots+%5Csquare+G_m.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  G_1 \square\cdots \square G_m."/></p>
</li><li>
The <a href="https://en.wikipedia.org/wiki/Strong_product_of_graphs">strong</a> product requires at least one edge: 	<p/>
<p align="center"><img alt="\displaystyle  \bigcup_{d \ge 1}\ \square(d,G_1,\dots,G_m). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbigcup_%7Bd+%5Cge+1%7D%5C+%5Csquare%28d%2CG_1%2C%5Cdots%2CG_m%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \bigcup_{d \ge 1}\ \square(d,G_1,\dots,G_m). "/></p>
<p>It is usually written as 	</p>
<p align="center"><img alt="\displaystyle  G_1 \fbox{x} \cdots \fbox{x} G_m." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++G_1+%5Cfbox%7Bx%7D+%5Ccdots+%5Cfbox%7Bx%7D+G_m.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  G_1 \fbox{x} \cdots \fbox{x} G_m."/></p>
</li><li>
The <a href="https://en.wikipedia.org/wiki/Tensor_product_of_graphs">tensor</a> product requires all edges: 	<p/>
<p align="center"><img alt="\displaystyle  \square(m,G_1,\dots,G_m). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csquare%28m%2CG_1%2C%5Cdots%2CG_m%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \square(m,G_1,\dots,G_m). "/></p>
<p>It is usually written as 	 	</p>
<p align="center"><img alt="\displaystyle  G_1 \times \cdots \times G_m." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++G_1+%5Ctimes+%5Ccdots+%5Ctimes+G_m.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  G_1 \times \cdots \times G_m."/></p>
</li></ul>
<p>
Here is a comparison of four graph products.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/04/graph-products/prod2/" rel="attachment wp-att-18076"><img alt="" class="aligncenter size-full wp-image-18076" src="https://rjlipton.files.wordpress.com/2021/02/prod2.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
The answer to Ken’s question is that you get the tensor product, which is Kronecker’s product on the adjacency matrices of the graphs. This is because each step of the product requires an action by each constituent machine.</p>
<p>
Of course then we get other types of products for other values of <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d}"/>. Are any of these interesting? We get intermediate notions up to Kronecker’s product. Can the be put to any natural use?</p>
<p>
</p><p/><h2> Coloring Products—Conjecture </h2><p/>
<p/><p>
An application of graph products is that they yield some quite compelling conjectures. The <a href="https://en.wikipedia.org/wiki/Hedetniemi%27s_conjecture">conjecture</a> due to Stephen Hedetniemi in 1966 is one example. This states that 	</p>
<p align="center"><img alt="\displaystyle  \chi (G \times H) = min( \chi (G), \chi (H)). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cchi+%28G+%5Ctimes+H%29+%3D+min%28+%5Cchi+%28G%29%2C+%5Cchi+%28H%29%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \chi (G \times H) = min( \chi (G), \chi (H)). "/></p>
<p>Here <img alt="{\chi(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\chi(G)}"/> is the number of colors needed to color <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G}"/>. The conjecture is false–see <a href="https://arxiv.org/abs/1905.02167">Counterexamples to Hedetniemi’s conjecture</a> by Yaroslav Shitov. Also see Gil Kalai’s <a href="https://gilkalai.wordpress.com/2019/05/10/sansation-in-the-morning-news-yaroslav-shitov-counterexamples-to-hedetniemis-conjecture/">post</a> about this news.</p>
<p>
</p><p/><h2> Coloring Products—Proofs </h2><p/>
<p/><p>
A potential application of graph products is to the famous Four Color Theorem (4CT)—see <a href="https://en.wikipedia.org/wiki/Four_color_theorem">here</a>. In a <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.368.4392&amp;rep=rep1&amp;type=pdf">paper</a> of mine with Atish Das Sarma, Amita Gajewar, and Danupon Nanongkai we show:</p>
<blockquote><p><b>Theorem 1 (An Approximate Restatement of the Four-Color Theorem)</b> <em> Suppose every two-edge connected, cubic, planar graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G}"/> can be edge 3-colored with <img alt="{o(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bo%28n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{o(n)}"/> errors. Then the Four Color Theorem is true. </em>
</p></blockquote>
<p/><p>
The proof is simple. We assume that some graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G}"/> is a counterexample to the 4CT. Then form a kind of product <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{H}"/> of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G}"/>. To be honest we did not see the proof exactly as this, but is essentially what we did. The <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{H}"/> is a huge product of many of the copies of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G}"/>, with some small modifications. Then we show if we could almost four color <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{H}"/> then we would be able to exactly color <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G}"/>. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Meanwhile, the paper showing that Hedetniemi’s conjecture is false contains an important lesson for mathematicians, Noga Alon says, </p>
<blockquote><p><b> </b> <em> <i>Sometimes, the reason that a conjecture is very hard to prove is simply that it is false.</i> </em>
</p></blockquote>
<p/><p><br/>
[restored missing line in second sentence, other fixes]</p></font></font></div>
    </content>
    <updated>2021-02-04T22:22:49Z</updated>
    <published>2021-02-04T22:22:49Z</published>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="direct"/>
    <category term="Four color theorem"/>
    <category term="planar"/>
    <category term="products"/>
    <category term="strong product"/>
    <category term="tensor"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-02-09T09:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://nisheethvishnoi.wordpress.com/?p=103</id>
    <link href="https://nisheethvishnoi.wordpress.com/2021/02/03/focs-2021/" rel="alternate" type="text/html"/>
    <title>FOCS 2021</title>
    <summary>A (very) preliminary call for papers for FOCS 2021 now available: https://cs.yale.edu/homes/vishnoi/focs-2021-cfp.html… This year, the submission deadline has been delayed to early June. This is in order to maximize the chances of a physical conference (sometime in early 2022).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A (very) preliminary call for papers for FOCS 2021 now available: </p>



<p><a href="https://t.co/1SAg6jInlH?amp=1" rel="noreferrer noopener" target="_blank">https://cs.yale.edu/homes/vishnoi/focs-2021-cfp.html…</a> </p>



<figure class="wp-block-image"><img alt="Image" src="https://pbs.twimg.com/media/EtQ2VzPXIAIOFiE?format=jpg&amp;name=large"/></figure>



<p>This year,  the submission deadline has been delayed to early June. This is in order to maximize the chances of a physical conference (sometime in early 2022). </p>



<p/></div>
    </content>
    <updated>2021-02-03T16:07:03Z</updated>
    <published>2021-02-03T16:07:03Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>nisheethvishnoi</name>
    </author>
    <source>
      <id>https://nisheethvishnoi.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://nisheethvishnoi.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://nisheethvishnoi.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Algorithms, Nature, and Society</title>
      <updated>2021-02-09T09:21:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/03/postdoc-in-dynamic-graph-algorithms-at-technical-university-of-denmark-copenhagen-denmark-apply-by-march-21-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/03/postdoc-in-dynamic-graph-algorithms-at-technical-university-of-denmark-copenhagen-denmark-apply-by-march-21-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc in Dynamic Graph Algorithms at Technical University of Denmark, Copenhagen, Denmark (apply by March 21, 2021)</title>
    <summary>Together, we will study different hypotheses, problems, and ideas concerning dynamic graph algorithms, in the pursuit of new, efficient algorithms. Here, the fun challenge is to find just the right partial answers to maintain as the graph changes, and often, the road to efficient dynamic algorithms goes via new graph theoretic insights. Contact: Eva Rotenberg. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Together, we will study different hypotheses, problems, and ideas concerning dynamic graph algorithms, in the pursuit of new, efficient algorithms.<br/>
Here, the fun challenge is to find just the right partial answers to maintain as the graph changes, and often, the road to efficient dynamic algorithms goes via new graph theoretic insights.<br/>
Contact: Eva Rotenberg.</p>
<p>Website: <a href="https://www.dtu.dk/english/About/JOB-and-CAREER/vacant-positions/job?id=9903f461-7515-4f33-84bc-843f749b3d21">https://www.dtu.dk/english/About/JOB-and-CAREER/vacant-positions/job?id=9903f461-7515-4f33-84bc-843f749b3d21</a><br/>
Email: erot@dtu.dk</p></div>
    </content>
    <updated>2021-02-03T15:29:15Z</updated>
    <published>2021-02-03T15:29:15Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-02-09T09:20:48Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7991231367461036724</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7991231367461036724/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/a-blood-donation-puzzle.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7991231367461036724" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7991231367461036724" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/a-blood-donation-puzzle.html" rel="alternate" type="text/html"/>
    <title>A Blood Donation Puzzle</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the US you can donate whole blood every eight weeks. Suppose Elvira does exactly that. Will she hit every date of the year? For example, if Elvira gave blood today, will she in some future year give blood on the 4th of July? Can we figure it out without having to rely on a computer simulation or even a calculator?</p><p>Let's make the assumptions that the blood center is open every day and that Elvira gives blood exactly every 56 days for eternity. </p><p>A year has 365 days which is relatively prime to 56=2<sup>3</sup>*7 since 365 mod 2 =1 and 365 mod 7 = 1. By the <a href="https://en.wikipedia.org/wiki/Chinese_remainder_theorem">Chinese remainder theorem</a> her next 365 blood donations will be on 365 distinct dates. If Elvira started giving blood at age 17, she will have hit every date at age 73.</p><p>That was easy but wrong. We have to account for those pesky leap years.</p><p>In a four year span, there will be one leap day. The total days in four years (using modular arithmetic) will still be odd and 5 mod 7, so still relatively prime to 56. So Elvira will donate on every day on the calendar exactly four times, except February 29th which she will hit once, over a period 56*4=224 years. </p><p>Alas not quite. Years ending 00 are not leap years, unless the year is divisible by 400. 2000 was a leap year but 2100 won't be. Any stretch of 224 years will hit at least one of those 00 non-leap years.</p><p>In 400 years, there will be 97 leap years. Since a regular year is 1 mod 7 days and a leap year is 2 mod 7 days, 400 years will be 497 mod 7 days. Since 497=71*7, 400 years has a multiple of 7 days. Every 400 years we have exactly the same calendar. February 3, 2421 is also a Wednesday.</p><p>The cycle of blood donations will repeat every 3200 years, the number of years in the least common multiple of 56 and the odd multiple of seven number of days in 400 years. But we can no longer directly apply the Chinese remainder theorem and argue that every day of the year will be hit. In those 3200 years Elvira will have over 20,000 blood donations. If the dates were chosen randomly the expected number to hit all dates would be 2372 by <a href="https://en.wikipedia.org/wiki/Coupon_collector%27s_problem">coupon collector</a>. So one would expect Elvira would hit every day, but that's not a proof.</p><p>So I had to dust off my Python skills and do the computer simulation after all. No matter what day Elvira starts donating she will eventually hit every date of the year. If Elvira starts donating today, she would give blood on the 4th of July for the first time in 2035 and <a href="https://docs.google.com/document/d/10aTFv5UUWfsrkKmVeud_r2VObfS1_J1tJflMeWld2w4/edit?usp=sharing">hit all dates</a> on January 8, 2087 after 431 donations. The longest sequence is 3235 donations starting April 25, 2140 and hitting all dates on February 29, 2636.</p></div>
    </content>
    <updated>2021-02-03T13:26:00Z</updated>
    <published>2021-02-03T13:26:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-02-09T06:25:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21146</id>
    <link href="https://gilkalai.wordpress.com/2021/02/03/to-cheer-you-up-in-difficult-times-19-nati-linial-and-adi-shraibman-construct-larger-corner-free-sets-from-better-numbers-on-the-forehead-protocols/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 19: Nati Linial and Adi Shraibman construct larger corner-free sets from better numbers-on-the-forehead protocols</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">What will be the next polymath project? click here for our previous post.  Number on the forehead, communication complexity, and additive combinatorics Larger Corner-Free Sets from Better NOF Exactly-N Protocols, by Nati Linial and Adi Shraibman Abstract: A subset of … <a href="https://gilkalai.wordpress.com/2021/02/03/to-cheer-you-up-in-difficult-times-19-nati-linial-and-adi-shraibman-construct-larger-corner-free-sets-from-better-numbers-on-the-forehead-protocols/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/">What will be the next polymath project? click here for our previous post. </a></p>
<h2>Number on the forehead, communication complexity, and additive combinatorics</h2>
<p class="title mathjax"><a href="https://arxiv.org/abs/2102.00421">Larger Corner-Free Sets from Better NOF Exactly-<span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3">N</span></span></span></span> Protocols</a>, by Nati Linial and Adi Shraibman</p>
<p><span style="color: #0000ff;"><strong>Abstract:</strong> A subset of the integer planar grid <em><span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-4"><span class="mrow" id="MathJax-Span-5"><span class="mo" id="MathJax-Span-6">[</span><span class="mi" id="MathJax-Span-7">N</span><span class="mo" id="MathJax-Span-8">]</span><span class="mo" id="MathJax-Span-9">×</span><span class="mo" id="MathJax-Span-10">[</span><span class="mi" id="MathJax-Span-11">N</span><span class="mo" id="MathJax-Span-12">]</span></span></span></span></em> is called <strong>corner-free</strong> if it contains no triple of the form <em><span class="MathJax" id="MathJax-Element-3-Frame"><span class="math" id="MathJax-Span-13"><span class="mrow" id="MathJax-Span-14"><span class="mo" id="MathJax-Span-15">(</span><span class="mi" id="MathJax-Span-16">x</span><span class="mo" id="MathJax-Span-17">,</span><span class="mi" id="MathJax-Span-18">y</span><span class="mo" id="MathJax-Span-19">)</span><span class="mo" id="MathJax-Span-20">,</span><span class="mo" id="MathJax-Span-21">(</span><span class="mi" id="MathJax-Span-22">x</span><span class="mo" id="MathJax-Span-23">+</span><span class="mi" id="MathJax-Span-24">δ</span><span class="mo" id="MathJax-Span-25">,</span><span class="mi" id="MathJax-Span-26">y</span><span class="mo" id="MathJax-Span-27">)</span><span class="mo" id="MathJax-Span-28">,</span><span class="mo" id="MathJax-Span-29">(</span><span class="mi" id="MathJax-Span-30">x</span><span class="mo" id="MathJax-Span-31">,</span><span class="mi" id="MathJax-Span-32">y</span><span class="mo" id="MathJax-Span-33">+</span><span class="mi" id="MathJax-Span-34">δ</span><span class="mo" id="MathJax-Span-35">)</span></span></span></span></em>. It is known that such a set has a vanishingly small density, but how large this density can be remains unknown. The best previous construction was based on Behrend’s large subset of <em><span class="MathJax" id="MathJax-Element-4-Frame"><span class="math" id="MathJax-Span-36"><span class="mrow" id="MathJax-Span-37"><span class="mo" id="MathJax-Span-38">[</span><span class="mi" id="MathJax-Span-39">N</span><span class="mo" id="MathJax-Span-40">]</span></span></span></span></em> with no <span class="MathJax" id="MathJax-Element-5-Frame"><span class="math" id="MathJax-Span-41"><span class="mrow" id="MathJax-Span-42"><span class="mn" id="MathJax-Span-43">3</span></span></span></span>-term arithmetic progression. Here we provide the first substantial improvement to this lower bound in decades. Our approach to the problem is based on the theory of communication complexity.</span></p>
<p><span style="color: #0000ff;">In the <span class="MathJax" id="MathJax-Element-6-Frame"><span class="math" id="MathJax-Span-44"><span class="mrow" id="MathJax-Span-45"><span class="mn" id="MathJax-Span-46">3</span></span></span></span>-players exactly-<span class="MathJax" id="MathJax-Element-7-Frame"><span class="math" id="MathJax-Span-47"><span class="mrow" id="MathJax-Span-48"><span class="mi" id="MathJax-Span-49">N</span></span></span></span> problem the players need to decide whether <em><span class="MathJax" id="MathJax-Element-8-Frame"><span class="math" id="MathJax-Span-50"><span class="mrow" id="MathJax-Span-51"><span class="mi" id="MathJax-Span-52">x</span><span class="mo" id="MathJax-Span-53">+</span><span class="mi" id="MathJax-Span-54">y</span><span class="mo" id="MathJax-Span-55">+</span><span class="mi" id="MathJax-Span-56">z</span><span class="mo" id="MathJax-Span-57">=</span><span class="mi" id="MathJax-Span-58">N</span></span></span></span> </em>for inputs <em><span class="MathJax" id="MathJax-Element-9-Frame"><span class="math" id="MathJax-Span-59"><span class="mrow" id="MathJax-Span-60"><span class="mi" id="MathJax-Span-61">x</span><span class="mo" id="MathJax-Span-62">,</span><span class="mi" id="MathJax-Span-63">y</span><span class="mo" id="MathJax-Span-64">,</span><span class="mi" id="MathJax-Span-65">z</span></span></span></span> </em>and fixed <em><span class="MathJax" id="MathJax-Element-10-Frame"><span class="math" id="MathJax-Span-66"><span class="mrow" id="MathJax-Span-67"><span class="mi" id="MathJax-Span-68">N</span></span></span></span></em>. This is the first problem considered in the multiplayer Number On the Forehead (NOF) model. Despite the basic nature of this problem, no progress has been made on it throughout the years. Only recently have explicit protocols been found for the first time, yet no improvement in complexity has been achieved to date. The present paper offers the first improved protocol for the exactly-<em><span class="MathJax" id="MathJax-Element-11-Frame"><span class="math" id="MathJax-Span-69"><span class="mrow" id="MathJax-Span-70"><span class="mi" id="MathJax-Span-71">N</span></span></span></span></em> problem. This is also the first significant example where algorithmic ideas in communication complexity bear fruit in additive combinatorics.</span></p>
<p>This is remarkable for various reasons. On the additive combinatorics side, improved constructions are rare. For example, <a href="https://gilkalai.wordpress.com/2008/07/10/pushing-behrend-around/">we reported</a> here in 2008 Elkin’s (small) improvements of Behrend’s bound. For the corner-free problem the paper of Nati and Adi goes beyond the Behrend’s (and Elkin’s) constructions. On the communication complexity side this is significant progress on a classical 1983 problem of Chandra, Furst and Lipton. The connection that goes from improved result on communication complexity to additive combinatorics is exciting — certainly a <a href="https://gilkalai.wordpress.com/2013/11/01/natifest-is-coming/">new frontier for Nati and Adi</a>. On the blogging side, I cannot compete with the beautifully written introduction. <a href="https://arxiv.org/abs/2102.00421">Click here to read the paper</a>.</p>
<p><strong>Remark 1:</strong> The number of the forehead problem is  related to Levine’s hat problem that we discussed in <a href="https://gilkalai.wordpress.com/2020/12/12/open-problem-session-of-huji-combsem-problem-3-ehud-friedgut-independent-sets-and-lionel-levins-infamous-hat-problem/">this post</a>.</p>
<p><strong>Remark 2: </strong>Ryan Alweiss just told me about Ben Green’s new paper <a href="https://arxiv.org/abs/2102.01543">New lower bounds for van der Waerden numbers.</a> It gives a construction of a red blue colouring of {1,2,…,N} with no 3 term blue or a k-term red arithmetic progression, where N is super-polynomial! Stay tune for a fuller report.</p></div>
    </content>
    <updated>2021-02-03T11:49:47Z</updated>
    <published>2021-02-03T11:49:47Z</published>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Adi Shraibman"/>
    <category term="Nati Linial"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-02-09T09:20:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-25562705.post-1310669999043503597</id>
    <link href="http://aaronsadventures.blogspot.com/feeds/1310669999043503597/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=1310669999043503597" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/1310669999043503597" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/1310669999043503597" rel="self" type="application/atom+xml"/>
    <link href="http://aaronsadventures.blogspot.com/2021/02/forc-2021-call-for-papers.html" rel="alternate" type="text/html"/>
    <title>FORC 2021 Call for Papers</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Reminder to anyone who has forgotten about FORC 2021 --- its a very nice venue --- and also a nice place to highlight recent work that is published or submitted elsewhere, via the non-archival track.</p><p><br/></p><p><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Symposium on Foundations of Responsible Computing (FORC) 2021 Call for Papers - Deadline February 15, 2021 AOE (anywhere on Earth)</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">The second annual Symposium on Foundations of Responsible Computing (FORC) is planned to be held on June 9-11, 2021, *online*. FORC is a forum for mathematically rigorous research in computation and society writ large.  The Symposium aims to catalyze the formation of a community supportive of the application of theoretical computer science, statistics, economics, and other relevant analytical fields to problems of pressing and anticipated societal concern.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Topics that fall in scope include, but are not restricted to, formal approaches to privacy, including differential privacy; theoretical approaches to fairness in machine learning, including the investigation of definitions, algorithms and lower bounds, tradeoffs, and economic incentives; computational and mathematical social choice (including apportionment and redistricting); theoretical foundations of sustainability; mechanism design for social good; mathematical approaches to bridging computer science, law and ethics; and theory related to modeling and mitigating the spread of epidemics. The Program Committee also warmly welcomes mathematically rigorous work on societal problems that have not traditionally received attention in the theoretical computer science literature. Whatever the topic, submitted papers should communicate their contributions towards responsible computing, broadly construed.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">The symposium itself will feature a mixture of talks by authors of accepted papers and invited talks. At least one author of each accepted paper should be present at the symposium to present the work (with an option for virtual attendance, as needed).</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Dual Submission Policy. Authors must indicate at the time of submission whether they are submitting to the archival-option track or the non-archival track.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* For submissions to the non-archival track, it is permitted to submit papers that have appeared in a peer-reviewed conference or journal since the last FORC. It is also permitted to simultaneously or subsequently submit substantially similar work to another conference or to a journal. Accepted papers in the non-archival track will receive talks at the symposium and will appear as one-page abstracts on the symposium website. They will not appear in the proceedings.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* For submissions to the archival-option track, papers that are substantially similar to papers that have been previously published, accepted for publication, or submitted in parallel to other peer-reviewed conferences with proceedings may not be submitted. Also, submissions that are substantially similar to papers that are already published in a journal at the time of submission may not be submitted to the archival-option track. Accepted papers in the archival-option track will receive talks at the symposium. Authors of papers accepted to the archival-option track will be given the option to choose whether to convert to a one-page abstract (which will not appear in the proceedings) or publish a 10-page version of their paper in the proceedings. The proceedings of FORC 2021 will be published by LIPIcs.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Authors are also responsible for ensuring that submitting to FORC would not be in violation of other journals’ or conferences’ submission policies.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">PC members and reviewers will be aware during the review process of whether papers have been submitted as archival-option or non-archival. The PC reserves the right to hold non-archival papers to a different standard than archival-option papers.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Submission Instructions.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Authors should upload a PDF of the paper here: </span><a href="https://easychair.org/conferences/?conf=forc2021" style="background-color: white; color: #1155cc; font-family: Arial, Helvetica, sans-serif; font-size: small;" target="_blank">https://easychair.org/conferences/?conf=forc2021</a><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* A footnote on the title of the paper should indicate whether the paper is a submission to the archival-option track or the non-archival track. Submissions to the non-archival track should also indicate in this footnote any archival venues (conferences or journals) at which the paper has appeared, a link to the publication, and the date on which it was published.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* The font size should be at least 11 point and the format should be single-column.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Author names and affiliations should appear at the top of the paper (reviewing for FORC is single, not double blind).</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Beyond these, there are no formatting or length requirements, but reviewers will only be asked to read the first 10 pages of the submission. It is the authors’ responsibility that the main results of the paper and their significance be clearly stated within the first 10 pages. For both the archival-option track and the non-archival track, submissions should include proofs of all central claims, and the committee will put a premium on writing that conveys clearly and in the simplest possible way what the paper is accomplishing.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Authors are free to post their submissions on arXiv or other online repositories.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">All questions about submissions should be emailed to the PC chair, Katrina Ligett, at </span><a href="mailto:katrina@cs.huji.ac.il" style="background-color: white; color: #1155cc; font-family: Arial, Helvetica, sans-serif; font-size: small;" target="_blank">katrina@cs.huji.ac.il</a><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">FORC Steering Committee</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Avrim Blum</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Cynthia Dwork      </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Shafi Goldwasser  </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Sampath Kannan</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Jon Kleinberg</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kobbi Nissim  </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Toni Pitassi</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Omer Reingold</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Guy Rothblum  </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Salvatore Ruggieri</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Salil Vadhan</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Adrian Weller</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">FORC 2021 Program Committee</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Borja Balle</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Raef Bassily</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Mark Bun</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Elisa Celis</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Aloni Cohen</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Moon Duchin</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Vitaly Feldman</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kira Goldner</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Krishna Gummadi</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Swati Gupta</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Gautam Kamath</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Michael Kearns</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Scott Kominers</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Himabindu Lakkaraju</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Katrina Ligett (chair)</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Jamie Morgenstern</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Seth Neel</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kobbi Nissim</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Adam Smith</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kunal Talwar</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Salil Vadhan</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Important Dates</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Submission deadline: February 15, 2021 AOE (anywhere on Earth)</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Author notification: March 31, 2021</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;"/><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Conference: June 9-11, 2021</span></p></div>
    </content>
    <updated>2021-02-03T02:44:00Z</updated>
    <published>2021-02-03T02:44:00Z</published>
    <author>
      <name>Aaron</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/09952936358739421126</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-25562705</id>
      <category term="game theory"/>
      <category term="news"/>
      <author>
        <name>Aaron</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/09952936358739421126</uri>
      </author>
      <link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <title>Adventures in Computation</title>
      <updated>2021-02-03T02:44:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/03/postdoc-at-puc-chile-millennium-institute-for-foundational-research-on-data-apply-by-march-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/03/postdoc-at-puc-chile-millennium-institute-for-foundational-research-on-data-apply-by-march-31-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at PUC Chile &amp; Millennium Institute for Foundational Research on Data (apply by March 31, 2021)</title>
    <summary>IMFD Chile, http://www.imfd.cl offers an open position for a postdoc to advance the understanding of theoretical aspects of neural networks. IMFD is a joint initiative held by several universities in Chile. It is a vibrant and truly interdisciplinary environment, which gathers together over 40 researchers and more than 100 students working on theoretical and applied […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>IMFD Chile, <a href="http://www.imfd.cl" rel="nofollow">http://www.imfd.cl</a> offers an open position for a postdoc to advance the understanding of theoretical aspects of neural networks.<br/>
IMFD is a joint initiative held by several universities in Chile. It is a vibrant and truly interdisciplinary environment, which gathers together over 40 researchers and more than 100 students working on theoretical and applied aspects of data science.</p>
<p>Website: <a href="https://docs.google.com/document/d/1PyHp-MRAPWg_0aeinpDGmzJGZbwMtsqC6BqE_4T3KFc/edit?usp=sharing">https://docs.google.com/document/d/1PyHp-MRAPWg_0aeinpDGmzJGZbwMtsqC6BqE_4T3KFc/edit?usp=sharing</a><br/>
Email: pbarcelo@uc.cl</p></div>
    </content>
    <updated>2021-02-03T01:30:54Z</updated>
    <published>2021-02-03T01:30:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-02-09T09:20:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://kamathematics.wordpress.com/?p=212</id>
    <link href="https://kamathematics.wordpress.com/2021/02/02/learning-theory-alliance-and-mentoring-workshop/" rel="alternate" type="text/html"/>
    <title>Learning Theory Alliance and Mentoring Workshop</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Surbhi Goel, Nika Haghtalab, and Ellen Vitercik are the organizers of an excellent new initiative called the Learning Theory Alliance. They have the following inspiring mission statement: Our mission is to develop a strong, supportive learning theory community and ensure its healthy growth by fostering inclusive community engagement and encouraging active contributions from researchers at … <a class="more-link" href="https://kamathematics.wordpress.com/2021/02/02/learning-theory-alliance-and-mentoring-workshop/">Continue reading<span class="screen-reader-text"> "Learning Theory Alliance and Mentoring Workshop"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://www.cs.utexas.edu/~surbhi/">Surbhi Goel</a>, <a href="https://people.eecs.berkeley.edu/~nika/">Nika Haghtalab</a>, and <a href="https://vitercik.github.io/">Ellen Vitercik</a> are the organizers of an excellent new initiative called the <a href="https://www.let-all.com/">Learning Theory Alliance</a>. They have the following inspiring mission statement:</p>



<p><em>Our mission is to develop a strong, supportive learning theory community and ensure its healthy growth by fostering inclusive community engagement and encouraging active contributions from researchers at all stages of their careers.</em></p>



<p>Their first event is a mentoring workshop, to be held at ALT 2021. I’ll be helping out by mentoring the creation of some written ALT highlights. Read on for more details from the organizers.</p>



<hr class="wp-block-separator"/>



<p>We are pleased to announce the first<strong> <a href="https://www.let-all.com/alt.html">Learning Theory Mentorship Workshop</a></strong> in collaboration with the <a href="http://algorithmiclearningtheory.org/alt2021/">Conference on Algorithmic Learning Theory (ALT) 2021</a> to be held virtually on <strong>March 4-5, 2021</strong>. The workshop will focus on building technical and networking skills while giving participants an opportunity to interact with fellow researchers in the field. </p>



<p>The workshop is intended for upper-level undergraduate and all-level graduate students as well as postdoctoral researchers who are excited about the possibility of learning theory research. No prior research experience in the field is expected.</p>



<p>We have several planned events including:</p>



<ul><li><strong>How-to talks </strong>which will provide general advice about giving talks, structuring papers, writing reviews, networking, and attending conferences.</li><li>A small group discussion <strong>dissecting a short talk</strong> with feedback from a senior researcher.</li><li>An informal and interactive<strong> “Ask Me Anything” </strong>sessionwith a senior member of the learning theory community.</li><li><strong>General audience talks</strong> about recent learning theory research which will be accessible to new researchers.</li><li><strong>Social events </strong>such as board games.</li></ul>



<p>Our lineup includes Jacob Abernethy, Kamalika Chaudhuri, Nadav Cohen, Rafael Frongillo, Shafi Goldwasser, Zhiyi Huang, Robert Kleinberg, Pravesh Kothari, Po-Ling Loh, Lester Mackey, Jamie Morgenstern, Praneeth Netrapalli, Vatsal Sharan and Mary Wootters.</p>



<p>Together with Gautam Kamath, we will also organize a written account of ALT titled <strong>“ALT Highlights”</strong> which will summarize the research presented at ALT. We will assist students and postdocs to set up interviews with presenters and keynote speakers as part of the highlights.</p>



<p>A short application<a href="https://forms.gle/v8b8aeJMgWxJ1Bbx9" rel="noreferrer noopener" target="_blank"> form</a> is required to participate with an <strong>application deadline of Friday, Feb. 19, 2021</strong>. Students with backgrounds that are underrepresented or underserved in related fields are especially encouraged to apply. <strong>We will be accommodating all time zones.</strong> More information can be found on the event’s website:<a href="http://let-all.com/alt.html" rel="noreferrer noopener" target="_blank"> http://let-all.com/alt.html</a>.</p>



<p>This workshop is part of our broader community building initiative called the Learning Theory Alliance (advised by Peter Bartlett, Avrim Blum, Stefanie Jegelka, Po-Ling Loh and Jenn Wortman Vaughan). Check out <a href="http://let-all.com/" rel="noreferrer noopener" target="_blank">http://let-all.com/</a> for more details and to sign up to volunteer.</p>



<p>Best,<br/>Surbhi Goel, Nika Haghtalab and Ellen Vitercik</p></div>
    </content>
    <updated>2021-02-02T00:00:39Z</updated>
    <published>2021-02-02T00:00:39Z</published>
    <category term="Events"/>
    <author>
      <name>Gautam</name>
    </author>
    <source>
      <id>https://kamathematics.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://kamathematics.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://kamathematics.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://kamathematics.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://kamathematics.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Kamathematics</title>
      <updated>2021-02-09T09:22:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/009</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/009" rel="alternate" type="text/html"/>
    <title>TR21-009 |  One-way Functions and Partial MCSP | 

	Eric Allender, 

	Mahdi Cheraghchi, 

	Dimitrios Myrisiotis, 

	Harsha Tirumala, 

	Ilya Volkovich</title>
    <summary>One-way functions (OWFs) are central objects of study in cryptography and computational complexity theory. In a seminal work, Liu and Pass (FOCS 2020) proved that the average-case hardness of computing time-bounded Kolmogorov complexity is equivalent to the existence of OWFs. It remained an open problem to establish such an equivalence for the average-case hardness of some NP-complete problem. In this paper, we make progress on this question by studying a polynomially-sparse variant of Partial Minimum Circuit Size Problem (Partial MCSP), which we call Sparse Partial MCSP, as follows.

1. First, we prove that if Sparse Partial MCSP is zero-error average-case hard on a polynomial fraction of its instances, then there exist OWFs.
2. Then, we observe that Sparse Partial MCSP is NP-complete under polynomial-time deterministic reductions. That is, there are NP-complete problems whose average-case hardness implies the existence of OWFs.
3. Finally, we prove that the existence of OWFs implies the nontrivial zero-error average-case hardness of Sparse Partial MCSP.

Thus the existence of OWFs is inextricably linked to the average-case hardness of this NP-complete problem.</summary>
    <updated>2021-02-01T21:54:03Z</updated>
    <published>2021-02-01T21:54:03Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-09T09:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=5433</id>
    <link href="https://francisbach.com/self-concordant-analysis-newton/" rel="alternate" type="text/html"/>
    <title>Going beyond least-squares – I : self-concordant analysis of Newton method</title>
    <summary>Least-squares is a workhorse of optimization, machine learning, statistics, signal processing, and many other scientific fields. I find it particularly appealing (too much, according to some of my students and colleagues…), because all algorithms, such as stochastic gradient [1], and analyses, such as for kernel ridge regression [2], are much simpler and rely on reasonably...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Least-squares is a workhorse of optimization, machine learning, statistics, signal processing, and many other scientific fields. I find it particularly appealing (too much, according to some of my students and colleagues…), because all algorithms, such as stochastic gradient [<a href="https://proceedings.neurips.cc/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf">1</a>], and analyses, such as for kernel ridge regression [<a href="https://link.springer.com/article/10.1007/s10208-006-0196-8">2</a>], are much simpler and rely on reasonably simple linear algebra.</p>



<p class="justify-text">Despite the unique appeal of least-squares, most interesting optimization or machine problems go beyond quadratic functions. While there are many algorithms and analyses dedicated to more general situations, it is tempting to use least-squares for analysis or algorithms by considering the functions at hand to be approximately quadratic.</p>



<p class="justify-text">Using bounds on the third-order derivatives and <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor expansions</a> are the usual ways to go, but they are not ideal for sharp non-asymptotic results where the deviation from quadratic functions has to be precisely quantified. In many situations, more finer structures can be used. In a series of posts, I will describe <em>self-concordance</em> properties, that relate the third order derivatives to second order ones.</p>



<p class="justify-text">There are many great books that cover this topic where the material below is taken from [<a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791">3</a>, <a href="https://www2.isye.gatech.edu/~nemirovs/LecIPM.pdf">4</a>, <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-91578-4.pdf">5</a>, <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">6</a>].</p>



<h2>Self-concordance</h2>



<p class="justify-text">A function \(f: C \subset \mathbb{R} \to \mathbb{R}\) is said self-concordant on the open interval \(C\) if and only if it is convex, three-times differentiable on \(C\), and $$\tag{1}\forall x \in C, \  |f^{\prime\prime\prime}(x)| \leqslant 2 f^{\prime\prime}(x)^{3/2}.$$ You may wonder why the power \(3/2\) or why the constant \(2\). The constant is just a convention (multiplying the function \(f\) by \(c\) would replace \(2\) by \(2/\sqrt{c}\)), while the power \(3/2\) is fundamental, as it makes the definition “affine-invariant”, that is, if \(f\) is self-concordant, so is \(y \mapsto f(ay)\) for any \(a \in \mathbb{R}\).</p>



<p class="justify-text">For a convex function defined on a convex subset \(C\) of \(\mathbb{R}\), this has to be true along all rays, or equivalently, if \(f^{\prime\prime\prime}(x)[h,h,h]= \sum_{i,j,k=1}^d h_i h_j h_k \frac{\partial^3 f}{\partial x_i \partial x_j \partial x_k}(x)\) is the symmetric third-order tensor and \(f^{\prime\prime}(x)[h,h] = \sum_{i,j=1}^d h_i h_j  \frac{\partial^2 f}{\partial x_i \partial x_j}(x)\) the second-order one, then $$\tag{2} \forall x \in C, \ \forall h \in \mathbb{R}^d , \ |f^{\prime\prime\prime}(x)[h,h,h]| \leqslant 2 f^{\prime\prime}(x)^{3/2}[h,h].$$</p>



<p class="justify-text"><strong>Examples. </strong>One can check that if \(f\) and \(g\) are self-concordant, then so is \(f+g\) (but not their average). Moreover, if \(f\) is self-concordant, so is \(y\mapsto f(Ay)\) for any matrix \(A\). The property is also preserved by Fenchel conjugation. Classical examples are all linear and quadratic functions, the negative logarithm, the negative log-determinant, or the negative logarithm of quadratic functions. The three previous examples are particularly important because they are “barrier functions”, with non-full domains, and are instrumental to interior-point methods (see below).</p>



<p class="justify-text"><strong>Properties in one dimension.</strong>  A nice reformulation of Eq. (1) (which is one-dimensional) is $$ \big| \frac{d}{dx} \big( f^{\prime\prime}(x)^{-1/2} \big) \big| = \big| \frac{1}{2} f^{\prime\prime\prime}(x)  f^{\prime \prime}(x)^{-3/2} \big| \leqslant 1,$$ which allows to define upper and lower bounds on \(f^{\prime \prime}(x)\) by integration, as, for \(x &gt; 0\), $$ – x \leqslant f^{\prime\prime}(x)^{-1/2} \, – f^{\prime\prime}(0)^{-1/2} \leqslant x,$$ which can be transformed into (by isolating \(f^{\prime\prime}(x)\)): $$ \tag{3} \frac{f^{\prime\prime}(0)}{\big(1 + x f^{\prime\prime}(0)^{1/2}\big)^2} \leqslant f^{\prime\prime}(x) \leqslant \frac{f^{\prime\prime}(0)}{\big(1 – x f^{\prime\prime}(0)^{1/2}\big)^2}.$$ We thus obtain global upper and lower bounds on \(f^{\prime\prime}(x)\).</p>



<p class="justify-text">We can then integrate Eq. (3) twice between \(0\) and \(x\) to obtain lower and upper bounds on \(f^\prime\) and then \(f\): $$-f^{\prime\prime}(0)^{1/2} + \frac{f^{\prime\prime}(0)^{1/2}}{1+x f^{\prime\prime}(0)^{1/2}} \leqslant f^\prime(x)-f^\prime(0) \leqslant -f^{\prime\prime}(0)^{1/2} + \frac{f^{\prime\prime}(0)^{1/2}}{1-x f^{\prime\prime}(0)^{1/2}},$$ and  $$ \tag{4} \rho \big( – f^{\prime\prime}(0)^{1/2} x \big) \leqslant f(x) \ – f(0) \ – f^\prime(0) x \leqslant   \rho \big( f^{\prime\prime}(0)^{1/2} x \big),$$ with \(\displaystyle \rho(u) =\  – \log(1-u) \ – u \sim \frac{u^2}{2} \) when \(u\to 0\), that is, the second-order expansion is tight at \(x =0\), but leads to global lower and upper bounds. This upper-bound is valid as long as \(\delta = f^{\prime\prime}(0)^{1/2} x \in [0,1]\), while the lower-bound on \(f\) is always true. The function \(\rho\) is plotted below.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5631" height="233" src="https://francisbach.com/wp-content/uploads/2021/01/rho.png" width="303"/></figure></div>



<p class="justify-text"><strong>Properties in multiple dimensions.</strong> The properties above in Eq. (3) and (4) directly extend to multiple dimensions. For any \(x \in C\), then for any \(h \in \mathbb{R}^d\) such that \(\delta^2 = \Delta^\top f^{\prime\prime}(x) \Delta &lt; 1\), we have upper and lower bounds for the Hessian, the gradient and the functions value at \(x + \Delta\), that is, denoting by \(\| \cdot \|\) the standard Euclidean norm (see detailed proofs at the end of the post) $$\tag{5}(1-\delta)^2 f^{\prime \prime}(x) \preccurlyeq  f^{\prime \prime}(x+\Delta) \preccurlyeq  \frac{1}{(1-\delta)^2}  f^{\prime \prime}(x),$$ $$ \tag{6}\big\| f^{\prime\prime}(x)^{-1/2} \big(f^\prime(x+\Delta)-f^\prime(x) -f^{\prime \prime}(x)\Delta \big) \big\|  \leqslant \frac{\delta^2}{1-\delta},$$ and $$\tag{7} \rho(-\delta) \leqslant f(x+\Delta)\ -f(x) \ – f^\prime(x)^\top \Delta \leqslant \rho(\delta).$$ A nice consequence is that if \(\delta &lt; 1\), then \(x+\Delta \in C\), that is, we get “for free” a feasible point. Moreover, these approximations are “second-order tight” at \(\Delta=0\), that is, the term in \(f^{\prime\prime}(x)\) in Taylor expansion around \(x\) is exact.</p>



<p class="justify-text"><strong>Dikin ellipsoid.</strong> The condition that \(\delta^2 = \Delta^\top f^{\prime\prime}(x) \Delta &lt; 1\) defines an ellipsoid around \(x\) which is always strictly inside the domain of \(f\) (see example in the plot below). The results above essentially state that when inside the Dikin ellipsoid, the locally quadratic approximation can be used. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-5642" height="307" src="https://francisbach.com/wp-content/uploads/2021/01/dykin.gif" width="388"/>Dikin ellipsoids for the function \(f(x) = \sum_{i=1}^k \log (b_i – a_i^\top x)\), which is self-concordant with a domain which is a polytope.</figure></div>



<p class="justify-text">In this post, I will focus primarily on the use self-concordant functions in optimization through the analysis of Newton method.</p>



<h2>Why should you care about Newton method?</h2>



<ol class="justify-text"><li>For fun: Newton method is one of the classics of optimization!</li><li>For high precisions: as we will see, it is quadratically convergent and attains machine precision after solving a few linear systems.</li><li>Even in high dimensions where the linear system can be expensive, Newton method may still be the method of choice for severely ill-conditioned problems where even accelerated first-order methods are too slow to obtain low precision solutions.</li><li>It sometimes comes for free in situations where gradients are expensive to evaluate compared to \(d\).</li></ol>



<h2>Classical analysis of Newton method</h2>



<p class="justify-text">Given a function \(f: \mathbb{R}^d \to \mathbb{R}\), Newton method is an iterative optimization algorithm consisting in locally approximating the function \(f\) around the iterate \(x_{t}\) by a second-order Taylor expansion $$f(x_t) + f^\prime(x_t)^\top(x-x_t) + \frac{1}{2} (x-x_t)^\top f^{\prime \prime}(x_t) ( x – x_t),$$ whose minimum can be found in closed form as $$\tag{8} x_{t+1} = x_t \ – f^{\prime \prime}(x_t)^{-1} f^{\prime}(x_t).$$</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-5705" height="275" src="https://francisbach.com/wp-content/uploads/2021/02/approx_taylor.gif" width="323"/>Quadratic approximation and Newton step (in green) for varying starting points (in red). When the starting point is far from the global minimizer (in 0), the Newton step totally overshoots the global minimizer.</figure></div>



<p class="justify-text">Newton method is classically analyzed for three times differentiable convex functions with bounded Hessians and third-order derivatives. The method is only locally convergent, that is, far away from the global minimizer \(x_\ast\) (even for very regular convex function), the method may diverge. In the non-convex setting, this leads to nice <a href="https://en.wikipedia.org/wiki/Newton_fractal">fractal plots</a>, but even for convex functions, the method can be unstable (see plot above).</p>



<p class="justify-text">Locally, it is quadratically convergent, that is, there exists \(c&gt;0\), such that if \(\| x_t \ – x_\ast\| \leqslant c\), then \(\| x_{t+1} \ – x_\ast \| /c  \leqslant \big( \|x_t\  – x_\ast\| / c \big)^2\). Roughly, the number of significant digits doubles at every iteration.</p>



<p class="justify-text">This leads to $$ \| x_{t} \ – x_\ast \| \leqslant c \big( \|x_{t_0} \ – x_\ast\| / c \big)^{2^{t-t_0}} ,$$ for \(t \geqslant t_0\) and \(t_0\) an index for which \(\| x_{t_0} \ – x_\ast\| \leqslant c\). It is less than \(\varepsilon\), as soon as \(2^{t-t_0} \log ( c / \| x_{t_0} – x_\ast \| ) \geqslant \log ( c / \varepsilon)\), that is, $$ t \geqslant t_0 + \frac{ \log \log ( c / \varepsilon)}{ \log 2} \  –  \frac{\log \log ( c / \| x_{t_0} – x_\ast \| )}{\log 2}.$$</p>



<p class="justify-text">That is, once we enter the quadratic phase, we obtain a number of iterations in \( \log \log ( 1/ \varepsilon)\), that is, only very few iterations. For example, for \(\varepsilon = 10^{-16}\), \(\log \log ( 1/ \varepsilon) \leqslant 4\).</p>



<p class="justify-text">Two major issues may be solved elegantly using self-concordant analysis: </p>



<ol class="justify-text"><li>Dealing with the two phases of Newton method, the quadratically convergent final phase, as well as the initial phase.</li><li>Obtaining convergence rates which are affine-invariant, that is, minimizing \(f(x)\) of \(f(Ax+b)\) for \(A\) an invertible matrix should lead to exactly the same convergence rate (this is not the case for the classical analysis, where for example the constant \(c\) depends on non affine-invariant quantities).</li></ol>



<h2>Self-concordant analysis of Newton method</h2>



<p class="justify-text">Consider \(f: \mathcal{C} \to \mathbb{R}\) which is self-concordant. Since Newton method in Eq. (8) is not globally convergent, we need to study a version where the Newton step is performed partially. There are several possible strategies. Here I present the so-called “damped Newton” iteration and thus study the iteration $$ x^+ = x\  – \frac{1}{1+\lambda(x)} f^{\prime \prime}(x)^{-1} f^\prime(x),$$ where we define the “Newton decrement” \(\lambda(x)\) at \(x \in C\), as $$ \lambda^2 = \lambda(x)^2 =f^\prime(x)^\top f^{\prime \prime}(x)^{-1} f^\prime(x) = \|f^{\prime \prime}(x)^{-1/2} f^\prime(x) \|^2.$$</p>



<p class="justify-text">The Newton decrement is a key quantity in the analysis of Newton method, as \(\frac{1}{2} \lambda(x)^2\) is exactly the decrease in the quadratic approximation obtained by a full Newton step. Moreover, </p>



<ol class="justify-text"><li>If \(\lambda(x) &lt; 1\), then \(x^+\) is in the Dikin ellipsoid where we can expect the local quadratic approximation to be relevant.</li><li>If \(\lambda(x) &lt; 1\), then one can show that \(f(x) \ – f(x_\ast) \leqslant \rho( \lambda(x)) \sim \frac{1}{2} \lambda(x)^2\) when \(\lambda(x)\) is close to zero, that is, the Newton decrement provides an upper bound on the distance to optimum.</li></ol>



<p class="justify-text">The update corresponds to \(\Delta = \ – \frac{1}{1+\lambda(x)} f^{\prime \prime}(x)^{-1} f^\prime(x)\), and \(\delta  = \frac{\lambda(x)}{1+\lambda(x)}  \in [0,1]\), thus \(x^+\) is automatically feasible (which is important for constrained case, see below).</p>



<p class="justify-text">Moreover, using Eq. (7), we get $$f(x^+)-f(x) \leqslant \ – \frac{f^\prime(x)^\top f^{\prime \prime}(x)^{-1} f^\prime(x)}{1+\lambda(x)} + \rho \Big( \frac{\lambda(x)}{1+\lambda(x)} \Big) = \log (1+ \lambda(x)) \ – \lambda(x).$$ This immediately leads to a fixed decrease of \(\frac{1}{4} \, – \log \frac{5}{4} \geqslant 0.0268\) if \(\lambda(x) \geqslant \frac{1}{4}\). </p>



<p class="justify-text">We can now compute the Newton decrement at \(x^+\), to see how it decreases, by first bounding $$\lambda(x^+) =\|f^{\prime \prime}(x^+)^{-1/2} f^\prime(x^+) \| \leqslant \frac{1}{1-\delta} \|f^{\prime \prime}(x)^{-1/2} f^\prime(x^+) \|,$$ using Eq. (5). We then have, using Eq. (6): $$ \|f^{\prime \prime}(x)^{-1/2} f^\prime(x^+) \| \leqslant \big\| f^{\prime \prime}(x)^{-1/2} \big( f^\prime(x) + f^{\prime\prime}(x) \Delta  \big) \big\| + \frac{\delta^2}{ 1-\delta } \leqslant \frac{\lambda(x)^2}{1+\lambda(x)} + \frac{\delta^2}{ 1-\delta  }.$$ This exactly leads to $$\lambda(x^+) \leqslant 2  \lambda(x)^2, $$ which leads to quadratic convergence if \(\lambda(x)\) is small enough.</p>



<p class="justify-text">We can then divide the analysis in two phases: before \(\lambda(x) \leqslant 1/4\) and after. The first integer \(t_0\) such that \(\lambda(x) \leqslant 1/4\) is less than \(\frac{ f(x_0) – f(x_\ast)}{0.0268} \leqslant 38 [ f(x_0) – f(x_\ast) ]\). Then, for the second phase, \(2\lambda(x_t) \leqslant (1/2)^{2^{t-t_0}}\). Given that for \(\lambda \leqslant 1/4\), \(\rho(\lambda) \leqslant 2\lambda\), we reach precision \(\varepsilon\) as soon as \(2^{t-t_0} \log 2 \geqslant \log \frac{1}{\varepsilon}\), that is, \(t \geqslant t_0 + \frac{1}{\log 2} \log \log \frac{1}{\varepsilon} -1\). This leads to number of iterations to reach a precision \(\varepsilon\)  which is less than $$38[ f(x_0) \ – f(x_\ast)]  +2 \log \log \frac{1}{\varepsilon}.$$</p>



<h2>Interior point methods</h2>



<p class="justify-text">Self-concordant functions are also key in the analysis of interior point methods. Consider a function \(f\) defined on \(\mathbb{R}^d\) and the constrained optimization problem $$\min_{ x \in C} f(x),$$ where \(C\) is a convex set. Barrier methods are appending a so-called “barrier function” \(g(x)\) to the objective function. A function \(g\) is a barrier function if \(g\) is convex and with domain containing the relative interior of \(C\), with gradients that explode when reaching the boundary of \(C\). We then solve instead $$\tag{9} \min_{x \in \mathbb{R}^d}  \varepsilon^{-1} f(x) +  g(x), $$ where \(\varepsilon &gt; 0\). Typically, the minimizer \(x_\varepsilon\) is in the relative interior of \(C\) (hence the name interior point method), and, when \(\varepsilon\) tends to zero, \(x_\varepsilon\) tends to the minimizer of \(f\) on \(C\).</p>



<p class="justify-text">When both the original function \(f\) and the barrier function \(g\) are self-concordant, the (damped) Newton method is particularly useful as it ensures feasibility of the iterates. Moreover, the interplay between the progressive reduction of \(\varepsilon\) towards zero and the approximate resolution of Eq. (9) can be completely characterized (see [<a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791">3</a>, <a href="https://www2.isye.gatech.edu/~nemirovs/LecIPM.pdf">4</a>, <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-91578-4.pdf">5</a>]). This applies directly to linear programming, second-order cone programming and semidefinite programming.</p>



<h2>Applications in machine learning</h2>



<p class="justify-text">If you have reached this point, you are probably a big fan of self-concordance. While this property is crucial in optimization, is it really relevant for machine learning or statistics? The sad truth is that most of the non-quadratic functions within machine learning are <em>not</em> self-concordant in the sense of Eq. (1) or Eq. (2). In particular, log-sum-exp functions, such that the logistic loss \(f(t) = \log( 1 + \exp(-t) )\),  do satisfy a relationship between third and second-order derivatives, but of the form $$| f^{\prime \prime \prime}(t)| \leqslant f^{\prime \prime }(t),$$ without the power \(3/2\). This seemingly small difference leads to several variations [7] which will the topic of next month blog post. Meanwhile, it is worth mentioning two applications in machine learning of classical self-concordance.</p>



<p class="justify-text"><strong>Maximum likelihood estimation for covariance matrices.</strong> Beyond its use in interior point methods, self-concordant functions arise naturally when estimating the covariance matrix using maximum likelihood estimation with a Gaussian model. Indeed, the negative log-likelihood can be written as $$ – \log p (x| \mu ,\Sigma) =\frac{d}{2} \log(2\pi) +  \frac{1}{2} \log \det \Sigma + \frac{1}{2} ( x -\mu)^\top \Sigma^{-1} ( x – \mu),$$  which leads to a negative log-determinant of the inverse \(\Sigma^{-1}\) of the covariance matrix \(\Sigma\), which is a self-concordant function, on which the guarantees discussed above apply.</p>



<p class="justify-text"><strong>Self-concordant losses.</strong> One can also design losses which are self-concordant. For example, a self-concordant version of the <a href="https://en.wikipedia.org/wiki/Huber_loss">Huber loss</a> is $$f(t) = \sqrt{1+t^2} \ – 1 \ –  \log \frac{ \sqrt{1+t^2} +1 }{2} .$$ It can be seen as the Fenchel-conjugate of \(– \log(1-t^2)\), and the proximity with a quadratic problem can be leveraged to obtain generalization performances using this loss function which are essentially the same as for the square loss. It can also be used for binary classification (see [<a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1609902192">8</a>] for details, and the two nice <a href="https://ostrodmit.github.io/blog/2018/11/12/self-concordance-part-1/">blog posts</a> of Dmitrii Ostrovskii).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5654" height="251" src="https://francisbach.com/wp-content/uploads/2021/01/loss_selfc.png" width="359"/></figure></div>



<p class="justify-text"><strong>One-step-estimation.</strong> Given the focus of this post on Newton method, I cannot resist mentioning a great technique coming from statistics that relies on a single Newton step. We consider a classical empirical risk minimization problem (statisticians would call it an M-estimation problem), with empirical risk \(\displaystyle \widehat{R}(\theta) = \frac{1}{n} \sum_{i=1}^n \ell(y_i, f_\theta(x_i) )\). Given an estimator \(\hat{\theta}\), obtained by any means, then if \(\hat{\theta}\) is \(\frac{1}{\sqrt{n}}\) away from the optimal parameter (with optimal performance on unseen data), then one Newton step on the function \(\widehat{R}\) started from \(\hat{\theta}\) will lead to an estimator achieving asymptotically the usual <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">Cramer-Rao</a> lower bound. In a nutshell, a single Newton step on the empirical risk transforms a good estimator into a very good estimator. See [9, Section 5.7] for more details.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Adrien Taylor and Dmitrii Ostrovskii for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] F. Bach and E. Moulines. <a href="https://proceedings.neurips.cc/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate</a> \(O(1/n)\). Advances in Neural Information Processing Systems (NIPS), 2013.<br/>[2] Andrea Caponnetto, Ernesto De Vito. <a href="https://link.springer.com/article/10.1007/s10208-006-0196-8">Optimal rates for the regularized least-squares algorithm</a>. Foundations of Computational Mathematics 7(3):331-368, 2007.<br/>[3] Yurii Nesterov, and Arkadii Nemirovskii. <em><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791">Interior</a><a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611970791.bm">-Point Polynomial Algorithms in Convex Programming</a></em>, SIAM, 1994.<br/>[4] Arkadii Nemirovski. <em><a href="https://www2.isye.gatech.edu/~nemirovs/LecIPM.pdf">Interior Point Polynomial Time Methods in Convex Programming</a></em>. Lecture notes, 1996.<br/>[5] Yurii Nesterov. <em><a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-91578-4.pdf">Lectures on Convex Optimization</a></em> (Vol. 137). Springer, 2018.<br/>[6] Stephen P. Boyd, and Lieven Vandenberghe. <em><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></em>. Cambridge University Press, 2004.<br/>[7] Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1271941980">Self-Concordant Analysis for Logistic Regression</a>. Electronic Journal of Statistics, 4, 384-414, 2010<br/>[8] Dmitrii Ostrovskii, and Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1609902192">Finite-sample Analysis of M-estimators using Self-concordance</a>. Electronic Journal of Statistics, 15(1):326-391, 2021.<br/>[9] Aad W. Van der Vaart. <em><a href="http://Aad W. Van der Vaart. Asymptotic Statistics">Asymptotic Statistics</a></em>, volume 3. Cambridge University Press, 2000.</p>



<h2>Detailed proofs for self-concordance properties</h2>



<p class="justify-text">We first show Eq. (7), by considering the function \(a(t) = f(x+t\Delta)\), which is a one-dimensional self-concordant function, for which \(a^\prime(t) = \Delta^\top f^\prime(x+t\Delta)\), and \(a^{\prime\prime}(t) = \Delta^\top f^{\prime\prime}(x+t\Delta) \Delta\). Then \(a^{\prime\prime}(0) = \delta^2\), and Eq. (4) for \(x=1\) and \(a\) exactly leads to Eq. (7).</p>



<p class="justify-text">In order to show Eq. (5), we consider \(h \in \mathbb{R}^d\), and the function \(b(t) = h^\top f^{\prime\prime}(x+t\Delta) h\). We have, \(b'(t) = f^{\prime\prime\prime}(x+t\Delta)[h,h,\Delta]\), which can be bounded using Eq. (2) as $$ |b'(t) | \leqslant 2 f^{\prime\prime}(x+t\Delta)[h,h] f^{\prime\prime}(x+t\Delta)[\Delta,\Delta]^{1/2} = 2 b(t) a^{\prime \prime}(t)^{1/2} \leqslant 2 b(t) \frac{\delta}{1-t\delta}, $$ using Eq. (3). This implies that for \(\delta t \in [0,1)\): $$\frac{d}{dt} \big[ (1-\delta t)^2 b(t) \big] = -2\delta (1-\delta t) b(t) + (1-\delta t)^2 b'(t) \leqslant 0, $$  which implies \(b(t) \leqslant \frac{b(0)}{(1-\delta t)^2} = \frac{h^\top f^{\prime\prime}(x) h}{(1-\delta t)^2}\), which leads to the right-hand side of Eq. (5) since this is true for all \(h \in \mathbb{R}^d\). The left-hand side is proved similarly.</p>



<p class="justify-text">In order to show Eq. (6), we consider the function \(g(t) = h^\top f^\prime(x+t\Delta)\), for which, \(g^\prime(t) = h^\top f^{\prime\prime}(x+t\Delta) \Delta\), and \(g^{\prime\prime}(t) =   f^{\prime\prime\prime}(x+t\Delta) [h,\Delta,\Delta]\), which satisfies: $$|g^{\prime\prime}(t)| \leqslant 2 f^{\prime\prime}(x+t\Delta)[\Delta,\Delta] f^{\prime\prime}(x+t\Delta)[h,h]^{1/2} \leqslant 2 b(t)^{1/2} a^{\prime \prime}(t). $$ This leads to $$ |g^{\prime\prime}(t)| \leqslant 2 \big( h^\top f^{\prime\prime}(x) h\big)^{1/2} \frac{\delta^2}{(1-\delta t)^3}.$$ We can then integrate twice, using \(g(0) = h^\top f^\prime(x)\) and \(g^\prime(0) = h^\top f^{\prime\prime}(x) \Delta\), to get: $$h^\top \big(f^\prime(x+t\Delta)-f^\prime(x) \ – f^{\prime \prime}(x)\Delta \big) \leqslant    \big( h^\top f^{\prime\prime}(x) h\big)^{1/2} \frac{\delta^2}{1-\delta},$$ which leads to Eq. (5) after maximizing with respect to \(h\).</p></div>
    </content>
    <updated>2021-02-01T16:16:32Z</updated>
    <published>2021-02-01T16:16:32Z</published>
    <category term="Optimization"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2021-02-09T09:22:14Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-6639458808942063282</id>
    <link href="http://processalgebra.blogspot.com/feeds/6639458808942063282/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=6639458808942063282" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6639458808942063282" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6639458808942063282" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/02/two-phd-positions-at-department-of.html" rel="alternate" type="text/html"/>
    <title>Two PhD positions at the Department of Computer Science, Reykjavik University: Model-driven SE for blockchain and smart contracts</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div align="justify"><span style="background-color: white;"><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">The<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">Software and Emerging Technology Lab at the Department of Computer Science,</span></span><span style="font-size: x-small;"><span style="font-size: 9pt;"><span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">Reykjavik University</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">, is looking for two PhD candidates to work on an ongoing research project on the application of Model Driven Software Engineering principles, methodologies, technologies and abstractions to Blockchain and Smart Contracts. While both positions require strong software development skills and familiarity with the<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">model-driven software engineering</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;"><span> </span>approach, the first position will focus on domain analysis and code generation, while the second position is concerned with contract safety and validity and requires knowledge in model<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">verification</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;"> and validation. The project is based in Iceland. It will be directed by Mohammad Hamdaqa in collaboration with Luca Aceto and Gísli Hjálmtýsson and in close collaboration with Polytechnique Montréal in Canada. The positions</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;"><span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">are fully funded and<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">include full tuition waiver and a salary in accordance with the Icelandic<span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">Research Fund guidelines. Particularly the funding is covering full tuition as well as a stipend of 383,000 ISK</span></span><span style="font-size: x-small;"><span style="font-size: 9pt;"><span> </span>per month before taxes for a minimum of three years. </span></span></span></div><div align="justify"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">If you are interested to apply, please send the documents below to<span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">the following email addresses:<span> </span></span></span><a href="mailto:mhamdaqa@ru.is" rel="noopener noreferrer" target="_blank"><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">mhamdaqa@ru.is</span></span></a><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">; </span></span><a href="mailto:luca@ru.is" rel="noopener noreferrer" target="_blank">luca@ru.is</a>; <a href="mailto:gisli@ru.is" rel="noopener noreferrer" target="_blank">gisli@ru.is</a></span></div><ul><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A copy of your CV and research interests</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A copy of all your transcripts</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A sample publication</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A maximum of one page research statement of your plans for research in your PhD.</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">Your intended starting date / and if you need a visa</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">For more information about the position and the research topics, do not hesitate to send your enquiries<span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">to any of the project collaborators.</span></span></span></li></ul><div><span style="background-color: white;">Mohammad Hamdaqa  (<a href="https://en.ru.is/cress/" rel="noopener noreferrer" target="_blank">https://en.ru.is/cress/</a>)<br/><a href="mailto:mhamdaqa@ru.is" rel="noopener noreferrer" target="_blank">mhamdaqa@ru.is</a></span></div><div><span style="background-color: white;">Luca Aceto (<a href="http://icetcs.ru.is/" rel="noopener noreferrer" target="_blank">http://icetcs.ru.is/</a>)<br/><a href="mailto:luca@ru.is" rel="noopener noreferrer" target="_blank">email: luca@ru.is</a><br/><br/>Gísli Hjálmtýsson (https://en.ru.is/fintech/)<br/>email: gisli@ru.is</span></div><div><br/></div><div>Informal inquiries about the project and the conditions of work are very welcome. We will start reviewing applications as soon as they arrive and will continue to accept applications until each position is filled. We strongly encourage interested applicants to send their applications as soon as possible and no later than 28 February 2021.</div></div>
    </content>
    <updated>2021-02-01T09:10:00Z</updated>
    <published>2021-02-01T09:10:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-02-08T17:42:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/01/postdoc-at-georgia-institute-of-technology-apply-by-february-25-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/01/postdoc-at-georgia-institute-of-technology-apply-by-february-25-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at Georgia Institute of Technology (apply by February 25, 2021)</title>
    <summary>The Algorithms and Randomness Center (ARC) at Georgia Tech is recruiting postdocs for a 1-year position with the possibility of extension by another year starting August 1, 2021. Area of expertise include algorithms, discrete mathematics, optimization, theoretical machine learning. Candidates with a PhD in Computer Science, Math, Operations Research are encouraged to apply by February […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Algorithms and Randomness Center (ARC) at Georgia Tech is recruiting postdocs for a 1-year position with the possibility of extension by another year starting August 1, 2021. Area of expertise include algorithms, discrete mathematics, optimization, theoretical machine learning. Candidates with a PhD in Computer Science, Math, Operations Research are encouraged to apply by February 25.</p>
<p>Website: <a href="http://arc.gatech.edu/postdoc21">http://arc.gatech.edu/postdoc21</a><br/>
Email: lisa.cox@isye.gatech.edu</p></div>
    </content>
    <updated>2021-02-01T00:46:09Z</updated>
    <published>2021-02-01T00:46:09Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-02-09T09:20:48Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8809394001786387491</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8809394001786387491/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/01/grading-policies-during-covid-no-easy.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8809394001786387491" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8809394001786387491" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/01/grading-policies-during-covid-no-easy.html" rel="alternate" type="text/html"/>
    <title>Grading policies during Covid-No easy answers</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Because of COVID  (my spellecheck says covid and Covid are not works, but COVID is) various schools have done various things to make school less traumatic. Students already have problems, either getting COVID or having their friends got family get it (I've had four relatives get it, and one died) . Some do not adjust to learning online.  Some do not have good computer connection to learn on line. So what is a good policy? Here are some things I have either seen schools do or heard that they might do.</p><p><br/></p><p>1) Be more generous with Tuition-Refunds if a student has to withdraw. </p><p>2) Be more generous with Housing-Refunds if a students comes to campus thinking it will be courses on campus and there are no courses on campus. Or if a student has to withdraw. </p><p>3) Make the deadlines for dropping-without-a-W, or taking-it-pass-fail, later in the semester. </p><p>4) Tell the teachers to `just teach them the bare min they need for the next course.'</p><p>5) Allow students to take courses P/F in their major and still allow them to count, so a student might get a D in Discrete Math and be able to go on in the major. </p><p>6) How far to extend deadlines? How is this: extend deadline to make it P/F until the last day of classes (but before the final) and then after the final is given, the school changes its mind and says - OH, you can change to P/F now if you want to.</p><p>7) Allow either an absolute number (say 7) or a fraction (say 1/3) of the courses to be changed to P/F by the last day of class.</p><p>8) Combine 6 or 7 with saying NO- a D is an F for a P/F course. Perhaps only if its in the major, but that maybe hard to work out. since majors can change. Some schools do A-B-C-NO CREDIT, where the NC grade does not go into the GPA.</p><p>9) Give standard letter grades and tell the students to tough it out. Recall the following inspirational quotes</p><p>When the going gets tough, the tough go shopping</p><p>When the going gets tough, the tough take a nap</p><p>If at first you don't succeed, quit. Why make a damn fool of yourself. </p><p>If at first you don't succeed, then skydiving is not for you. </p><p>10) Decide later in the term what to do depending on who yells the loudest. </p><p>11) Any combination of the above that makes sense, and even some that don't. </p><p><br/></p><p>On the one hand, there are students who are going through very hard times because of covid and should be given a break. On the other hand, we want to give people a good education and give grades that are meaningful (the logic of how to give grades in normal times is another issue for another blog post). </p><p>What is your school doing? Is it working? What does it mean to be working?</p><p>The problems I am talking about are first-world problems or even champagne-problems. I know there are people who have far worse problems then getting a bad grade or dropping courses.</p><p><br/></p></div>
    </content>
    <updated>2021-01-31T21:50:00Z</updated>
    <published>2021-01-31T21:50:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-02-09T06:25:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7972</id>
    <link href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/" rel="alternate" type="text/html"/>
    <title>A blitz through classical statistical learning theory</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Previous post: ML theory with bad drawings Next post: TBD, see also all seminar posts and course webpage. Lecture video (starts in slide 2 since I hit record button 30 seconds too late – sorry!) These are rough notes for the first lecture in my advanced topics in machine learning seminar. See the previous post … <a class="more-link" href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">Continue reading <span class="screen-reader-text">A blitz through classical statistical learning theory</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/01/15/ml-theory-with-bad-drawings/">ML theory with bad drawings</a> <strong>Next post:</strong> TBD,  see also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=5c6a9e86-bca7-42df-a04a-acc200ed2c2d">Lecture video</a> (starts in slide 2 since I hit record button 30 seconds too late – sorry!)</p>



<p>These are rough notes for the first lecture in <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">my advanced topics in machine learning seminar</a>. See the <a href="https://windowsontheory.org/2021/01/15/ml-theory-with-bad-drawings/">previous post</a> for the introduction.</p>



<p>This lecture’s focus was on <strong>“classical” learing theory</strong>. The distinction between “classical learning” and “deep learning” is semantic/philosophical, and doesn’t matter much for this seminar. I personally view this difference as follows:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/dzDbio7.png"/></figure>



<p>That is, deep learning is a framework that allows you to translate more resources (data and computation) into bettter performance. “Classical” methods often have a “threshold effect” where a certain amount of data and computation is needed, and more would not really help. For example, in parametric methods there will typically be a sharp threshold for the amount of data required for saturating the potential performance. Even in non-parametric models such as nearest neighbors or kernel methods, the computational cost is fixed for a fixed amount of data, and there is no way to profitably trade more computation for better performance.</p>



<p>In contrast, for deep learning, we often can get better performance using the same data by using bigger models or more computation. For example, I doubt this <a href="http://karpathy.github.io/2019/04/25/recipe/">story of Andrej Karpathy</a> could have happened with a non deep-learning method:</p>



<p><em>“One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”).”</em></p>



<h2>Leaky pipelines</h2>



<p>We can view machine learning (deep or not) as a series of “leaky pipelines”:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/0dpRqYa.png"/></figure>



<p>We want to create an adaptive system that performs well in the wild, but to do so, we:</p>



<ol><li>Set up a benchmark of a test distribution, so we have some way to compare different systems.</li><li>We typically can’t optimize directly on the benchmark, both because losses like accuracy are not differentiable and because we don’t have access to an unbounded number of samples from the distribution. (Though there are exceptions, such as when optimizing for playing video games.) Hence we set up the task of optimizing some proxy loss function <img alt="\mathcal{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{L}"/> on some finite samples of training data.</li><li>We then run an optimization algorithm whose ostensible goal is to find the <img alt="f \in \mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=f+%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f \in \mathcal{F}"/> that minimizes the loss function over the training data. (<img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{F}"/> is a set of models, sometimes known as <em>architecture</em>, and sometimes we also add other restrictions such norms of weights, which is known as <em>regularization</em>)</li></ol>



<p>All these steps are typically “leaky.” Test performance on benchmarks is not the same as real-world performance. Minimizing the loss over the training set is not the same as test performance. Moreover, we typically can’t solve the loss minimization task optimally, and there isn’t a unique minimizer, so the choice of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> depends on the algorithm.</p>



<p>Much of machine learning theory is about obtaining guarantees bounding the “leakiness” of the various steps. These are often easier to do in “classical” contexts of statistical learning theory than for deep learning. In this lecture, we will make a short blitz through classical learning theory. This material is covered in several sources, including the excellent book <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/">understanding machine learning</a> and the upcoming Hardt-Recht text.</p>



<p>We will be very rough, using proofs by picture and making some simplifications (e.g., working in one dimension, assuming functions are always differentiable, etc.)</p>



<h2>Convexity</h2>



<p>A (nice) function <img alt="f:\mathbb{R} \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathbb%7BR%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f:\mathbb{R} \rightarrow \mathbb{R}"/> is (strongly) <em>convex</em> if it satisfies one of the following three equivalent conditions:</p>



<ol><li>For every two points <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x,y"/>, the line between <img alt="(x,f(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,f(x))"/> and <img alt="(y,f(y))" class="latex" src="https://s0.wp.com/latex.php?latex=%28y%2Cf%28y%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(y,f(y))"/> is above the curve of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>.</li><li>For every point <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, the tangent line at <img alt="(x,f(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,f(x))"/> with slope <img alt="f'(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f'(x)"/> is below the curve of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>.</li><li>For every <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, <img alt="f''(x)&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f''(x)&gt;0"/>.</li></ol>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/0p5cY35.png"/></figure>



<p>To see that for example, 2 implies 3, we can use the contrapositive. If 3 does not hold and <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is such that <img alt="f''(x) &lt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29+%3C+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f''(x) &lt; 0"/> (should really assume <img alt="f''(x) \leq 0" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29+%5Cleq+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f''(x) \leq 0"/> but we’re being rough) then by Taylor, around <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> we get</p>



<p><img alt="f(x + \delta) = f(x) + \delta f'(x) + \delta^2 f''(x) /2 + O(\delta^3)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x+%2B+%5Cdelta%29+%3D+f%28x%29+%2B+%5Cdelta+f%27%28x%29+%2B+%5Cdelta%5E2+f%27%27%28x%29+%2F2+%2B+O%28%5Cdelta%5E3%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x + \delta) = f(x) + \delta f'(x) + \delta^2 f''(x) /2 + O(\delta^3)"/></p>



<p>For <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/> small enough, <img alt="O(\delta^3)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Cdelta%5E3%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="O(\delta^3)"/> is negligible and so we see that the curve of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> near <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> equals the tangent line <img alt="f(x) + \delta f'(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29+%2B+%5Cdelta+f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x) + \delta f'(x)"/> plus a negative term, and hence it is below the line, contradicting 2.</p>



<p>To show that 2 implies 1, we can again use the contrapositive and show by a “proof by picture” that if there is some point in which <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> is above the line between <img alt="(x,f(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,f(x))"/> and <img alt="(y,f(y))" class="latex" src="https://s0.wp.com/latex.php?latex=%28y%2Cf%28y%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(y,f(y))"/>, then there must be a point <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> in which the tangent line at <img alt="(z,f(z))" class="latex" src="https://s0.wp.com/latex.php?latex=%28z%2Cf%28z%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(z,f(z))"/> is above <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/84qFxfN.png"/></figure>



<p>Some tips on convexity:</p>



<ol><li>The function <img alt="f(x)=x^2" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3Dx%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)=x^2"/> is convex (proof: <a href="https://www.google.com/search?q=plot+x%5E2&amp;oq=plot+x%5E2">Google</a>)</li><li>If <img alt="f:\mathbb{R}^k \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathbb%7BR%7D%5Ek+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f:\mathbb{R}^k \rightarrow \mathbb{R}"/> is convex and <img alt="L:\mathbb{R}^d \rightarrow \mathbb{R}^k" class="latex" src="https://s0.wp.com/latex.php?latex=L%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Ek&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L:\mathbb{R}^d \rightarrow \mathbb{R}^k"/> is linear then <img alt="x \mapsto f(L(x))" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+f%28L%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x \mapsto f(L(x))"/> is convex (lines are still lines). </li><li>If <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> is convex and <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/> is convex then <img alt="a\cdot f + b \cdot g" class="latex" src="https://s0.wp.com/latex.php?latex=a%5Ccdot+f+%2B+b+%5Ccdot+g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="a\cdot f + b \cdot g"/> is convex for every positive <img alt="a,b" class="latex" src="https://s0.wp.com/latex.php?latex=a%2Cb&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="a,b"/>.</li></ol>



<h2>Gradient descent</h2>



<p>The gradient descent algorithm minimizes a function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> by starting at some point <img alt="x_0" class="latex" src="https://s0.wp.com/latex.php?latex=x_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_0"/> and repeating the following operation:</p>



<p><img alt="x_{t+1} = x_t - \eta \cdot f'(x_t)" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+%3D+x_t+-+%5Ceta+%5Ccdot+f%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_{t+1} = x_t - \eta \cdot f'(x_t)"/></p>



<p>for some small <img alt="\eta&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta&gt;0"/>.</p>



<p>By Taylor, <img alt="f(x+\delta) \approx f(x) + \delta \cdot f'(x) + \delta^2 f''(x)/2" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%2B%5Cdelta%29+%5Capprox+f%28x%29+%2B+%5Cdelta+%5Ccdot+f%27%28x%29+%2B+%5Cdelta%5E2+f%27%27%28x%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x+\delta) \approx f(x) + \delta \cdot f'(x) + \delta^2 f''(x)/2"/>, and so setting <img alt="\delta = -\eta \cdot f'(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+-%5Ceta+%5Ccdot+f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta = -\eta \cdot f'(x)"/>, we can see that</p>



<p><img alt="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right]" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x_%7Bt%2B1%7D%29+-+f%28x_t%29+%5Capprox+-%5Ceta+f%27%28x_t%29%5E2+%2B+%5Ceta%5E2+f%27%28x_t%29%5E2+f%27%27%28x_t%29%2F2+%3D+-%5Ceta+f%27%28x_t%29%5E2+%5Cleft%5B+1+-+%5Ceta+f%27%27%28x_t%29%2F2+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right]"/></p>



<p>Since <img alt="f''(x_t)&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%27%28x_t%29%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f''(x_t)&gt;0"/>, we see that as long as <img alt="\eta &lt; 2/ f''(x_t)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta+%3C+2%2F+f%27%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta &lt; 2/ f''(x_t)"/> we make progress. If we set <img alt="\eta \sim const / f''(x_t)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta+%5Csim+const+%2F+f%27%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta \sim const / f''(x_t)"/> then we reduce in each step the value of the function by roughly <img alt="f'(x_t)^2/f''(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%28x_t%29%5E2%2Ff%27%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f'(x_t)^2/f''(x)"/>.</p>



<p>In the high dimensional case, we replace <img alt="f'(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f'(x)"/> with the gradient <img alt="\nabla f(x) = ( df(x)/dx_1 , \ldots, df(x)/dx_d)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%29+%3D+%28+df%28x%29%2Fdx_1+%2C+%5Cldots%2C+df%28x%29%2Fdx_d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla f(x) = ( df(x)/dx_1 , \ldots, df(x)/dx_d)"/> and <img alt="f''(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f''(x)"/> with the Hessian which is the matrix <img alt="(df(x)/dx_i dx_j)_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%28df%28x%29%2Fdx_i+dx_j%29_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(df(x)/dx_i dx_j)_{i,j}"/>. The progress we can make is controlled by the ratio of the smallest to largest eigenvalues of the Hessian, which is one over its _condition number_.</p>



<p>In <strong>stochastic gradient descent</strong>, instead of performing the step <img alt="x_{t+1} = x_t - \eta f'(x_t)" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+%3D+x_t+-+%5Ceta+f%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_{t+1} = x_t - \eta f'(x_t)"/> we use <img alt="x_{t+1} - \eta \hat{f'}(x_t)" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+-+%5Ceta+%5Chat%7Bf%27%7D%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_{t+1} - \eta \hat{f'}(x_t)"/>, where <img alt="\hat{f'}(x_t)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%27%7D%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\hat{f'}(x_t)"/> is a random variable satisfying:</p>



<ul><li><img alt="\mathbb{E} \hat{f'}(x) = f'(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+%5Chat%7Bf%27%7D%28x%29+%3D+f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E} \hat{f'}(x) = f'(x)"/></li><li><img alt="Var \hat{f'}(x) = \sigma^2" class="latex" src="https://s0.wp.com/latex.php?latex=Var+%5Chat%7Bf%27%7D%28x%29+%3D+%5Csigma%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Var \hat{f'}(x) = \sigma^2"/> for some <img alt="\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sigma"/>.</li></ul>



<p>Let’s define <img alt="N_t = \hat{f'}(x_t)- f'(x_t)" class="latex" src="https://s0.wp.com/latex.php?latex=N_t+%3D+%5Chat%7Bf%27%7D%28x_t%29-+f%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N_t = \hat{f'}(x_t)- f'(x_t)"/>. Then <img alt="N_t" class="latex" src="https://s0.wp.com/latex.php?latex=N_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N_t"/> is a mean zero and variance <img alt="\sigma^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sigma^2"/> random variable, and let’s heuristically imagine that <img alt="N_1,N_2,\ldots" class="latex" src="https://s0.wp.com/latex.php?latex=N_1%2CN_2%2C%5Cldots&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N_1,N_2,\ldots"/> are independent. If we plug in this into the Taylor approximation, then since <img alt="\mathbb{E} N_t = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+N_t+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E} N_t = 0"/>, only the terms with <img alt="N_t^2" class="latex" src="https://s0.wp.com/latex.php?latex=N_t%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N_t^2"/> survive.</p>



<p>So by plugging <img alt="\delta = -\eta (f'(x) + N_t)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+-%5Ceta+%28f%27%28x%29+%2B+N_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta = -\eta (f'(x) + N_t)"/> to the Taylor approximation, we get that in expectation</p>



<p><img alt="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 + \eta^2 \sigma^2 f''(x_t)^2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right] + \eta^2 \sigma^2 f''(x_t)^2" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x_%7Bt%2B1%7D%29+-+f%28x_t%29+%5Capprox+-%5Ceta+f%27%28x_t%29%5E2+%2B+%5Ceta%5E2+f%27%28x_t%29%5E2+f%27%27%28x_t%29%2F2+%2B+%5Ceta%5E2+%5Csigma%5E2+f%27%27%28x_t%29%5E2+%3D+-%5Ceta+f%27%28x_t%29%5E2+%5Cleft%5B+1+-+%5Ceta+f%27%27%28x_t%29%2F2+%5Cright%5D+%2B+%5Ceta%5E2+%5Csigma%5E2+f%27%27%28x_t%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 + \eta^2 \sigma^2 f''(x_t)^2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right] + \eta^2 \sigma^2 f''(x_t)^2"/></p>



<p>We see that now to make progress, we need to ensure that <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta"/> is sufficiently smaller than <img alt="f'(x_t)^2/(\sigma^2 f''(x_t))" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%28x_t%29%5E2%2F%28%5Csigma%5E2+f%27%27%28x_t%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f'(x_t)^2/(\sigma^2 f''(x_t))"/>. We note that in the beginning, when <img alt="f'(x_t)^2" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%28x_t%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f'(x_t)^2"/> is large, we can use a larger learning rate <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta"/>, while when we get closer to the optimum, then we need to use a smaller learning rate.</p>



<h2>Generalization bounds</h2>



<p>The <em>supervised learning problem</em> is the task, given labeled training inputs <img alt="S = { (x_1,y_1),\ldots,(x_n,y_n) }" class="latex" src="https://s0.wp.com/latex.php?latex=S+%3D+%7B+%28x_1%2Cy_1%29%2C%5Cldots%2C%28x_n%2Cy_n%29+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="S = { (x_1,y_1),\ldots,(x_n,y_n) }"/> of obtaining a classifier/regressor <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> that will satisfy <img alt="f(x) \approx y" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29+%5Capprox+y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x) \approx y"/> for future samples <img alt="(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,y)"/> from the same distribution.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/ZVTtBmW.png"/></figure>



<p>Let’s assume that our goal is to minimize some quantity <img alt="LOSS(f)= \mathbb{E}{x,y} \mathcal{L}(y,f(x))" class="latex" src="https://s0.wp.com/latex.php?latex=LOSS%28f%29%3D+%5Cmathbb%7BE%7D%7Bx%2Cy%7D+%5Cmathcal%7BL%7D%28y%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="LOSS(f)= \mathbb{E}{x,y} \mathcal{L}(y,f(x))"/> where <img alt="\mathcal{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{L}"/> is a <em>loss function</em> (that we will normalize to <img alt="[0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="[0,1]"/> for convenience). We call the quantity <img alt="LOSS" class="latex" src="https://s0.wp.com/latex.php?latex=LOSS&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="LOSS"/> the population loss (and abuse notation by denoting it as <img alt="\mathcal{L}(f)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{L}(f)"/>) and the corresponding quantity over the training set <img alt="\hat{\mathcal{L}}_S(f) = \tfrac{1}{n}\sum_{i=1..n} \mathcal{L}(y_i,f(x_i))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29+%3D+%5Ctfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1..n%7D+%5Cmathcal%7BL%7D%28y_i%2Cf%28x_i%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\hat{\mathcal{L}}_S(f) = \tfrac{1}{n}\sum_{i=1..n} \mathcal{L}(y_i,f(x_i))"/> the empirical loss.</p>



<p>The <strong>generalization gap</strong> is the difference <img alt="\mathbb{E}{x,y} \mathcal{L}(y,f(x)) - \tfrac{1}{n}\sum_{i=1..n}\mathcal{L}(y_i,f(x_i))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%7Bx%2Cy%7D+%5Cmathcal%7BL%7D%28y%2Cf%28x%29%29+-+%5Ctfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1..n%7D%5Cmathcal%7BL%7D%28y_i%2Cf%28x_i%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}{x,y} \mathcal{L}(y,f(x)) - \tfrac{1}{n}\sum_{i=1..n}\mathcal{L}(y_i,f(x_i))"/> between the population and empirical losses. (We could add an absolute value though we expect that the loss over the training set would be smaller than the population loss; the population loss can be approximated by the “test loss” and so these terms are sometimes used interchangibly.)</p>



<p><strong>Why care about the generalization gap?</strong> You might argue that we only care about the population loss and not the gap between population and empirical loss. However, as mentioned before, we don’t even care about the population loss but about a more nebulous notion of “real-world performance.” We want the relations between our different abstractions to be as minimally “leaky” as possible and so bound the difference between train and test performance.</p>



<h3>Bias-variance tradeoff</h3>



<p>Suppose that our algorithm performs <em>empirical risk minimization (ERM)</em> which means that on input <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="S"/>, we output <img alt="f = \arg\min_{f \in \mathcal{F}} \hat{\mathcal{L}}_S(f)" class="latex" src="https://s0.wp.com/latex.php?latex=f+%3D+%5Carg%5Cmin_%7Bf+%5Cin+%5Cmathcal%7BF%7D%7D+%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f = \arg\min_{f \in \mathcal{F}} \hat{\mathcal{L}}_S(f)"/>. Let’s assume that we have a collection of classifiers <img alt="{ f_1,f_2,\ldots }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+f_1%2Cf_2%2C%5Cldots+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ f_1,f_2,\ldots }"/> and define <img alt="\mathcal{F}_K = { f_1,\ldots, f_K }" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D_K+%3D+%7B+f_1%2C%5Cldots%2C+f_K+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{F}_K = { f_1,\ldots, f_K }"/>. For every <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>, <img alt="\hat{\mathcal{L}}_S(f)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\hat{\mathcal{L}}_S(f)"/> is an estimator for <img alt="\mathcal{L}(f)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{L}(f)"/> and so we can write <img alt="\hat{\mathcal{L}}_S(f) = \mathcal{L}(f) + N_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29+%3D+%5Cmathcal%7BL%7D%28f%29+%2B+N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\hat{\mathcal{L}}_S(f) = \mathcal{L}(f) + N_i"/> where <img alt="N_i" class="latex" src="https://s0.wp.com/latex.php?latex=N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N_i"/> is a random variable with mean zero and variance roughly <img alt="1/n" class="latex" src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/n"/> (because we have <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> samples).</p>



<p>The ERM algorithm outputs the <img alt="f_i" class="latex" src="https://s0.wp.com/latex.php?latex=f_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_i"/> which minimizes <img alt="\mathcal{L}(f_i) + N_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28f_i%29+%2B+N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{L}(f_i) + N_i"/>. As <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="K"/> grows, the quantity <img alt="\min_{i \in [K]}\mathcal{L}(f_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin_%7Bi+%5Cin+%5BK%5D%7D%5Cmathcal%7BL%7D%28f_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\min_{i \in [K]}\mathcal{L}(f_i)"/> (which is known as the <strong>bias</strong> term) shrinks. The quantity <img alt="\max_{i \in [K]} N_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bi+%5Cin+%5BK%5D%7D+N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\max_{i \in [K]} N_i"/> (which is known as the <strong>variance</strong> term) grows. When the variance term dominates the bias term, we could potentially start outputting classifiers that don’t perform better on the population. This is known as the “bias-variance tradeoff.”</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/B56EjDS.png"/></figure>



<h3>Counting generalization gap</h3>



<p>The most basic generalization gap is the following:</p>



<p><strong>Thm (counting gap):</strong> With high probability over <img alt="S \in (X,Y)^n" class="latex" src="https://s0.wp.com/latex.php?latex=S+%5Cin+%28X%2CY%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="S \in (X,Y)^n"/>, <img alt="\max_{f \in \mathcal{F}}\left| \mathcal{L}(f) - \hat{\mathcal{L}}_S(f) \right| \leq O\left( \sqrt{\tfrac{\log |\mathcal{F}|}{n}}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bf+%5Cin+%5Cmathcal%7BF%7D%7D%5Cleft%7C+%5Cmathcal%7BL%7D%28f%29+-+%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29+%5Cright%7C+%5Cleq+O%5Cleft%28+%5Csqrt%7B%5Ctfrac%7B%5Clog+%7C%5Cmathcal%7BF%7D%7C%7D%7Bn%7D%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\max_{f \in \mathcal{F}}\left| \mathcal{L}(f) - \hat{\mathcal{L}}_S(f) \right| \leq O\left( \sqrt{\tfrac{\log |\mathcal{F}|}{n}}\right)"/>.</p>



<p><strong>Proof:</strong> By standard bounds such as Chernoff etc.., the random variable <img alt="N_i" class="latex" src="https://s0.wp.com/latex.php?latex=N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N_i"/> behaves like a Normal/Gaussian of mean zero and standard deviation at most <img alt="1/\sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\sqrt{n}"/>, which means that the probability that <img alt="|N_i| \geq k/\sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7CN_i%7C+%5Cgeq+k%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="|N_i| \geq k/\sqrt{n}"/> is at most <img alt="\exp(-c \cdot k^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-c+%5Ccdot+k%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\exp(-c \cdot k^2)"/>. If we set <img alt="k = 10 \sqrt{10 \log |\mathcal{F}|/c}" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3D+10+%5Csqrt%7B10+%5Clog+%7C%5Cmathcal%7BF%7D%7C%2Fc%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k = 10 \sqrt{10 \log |\mathcal{F}|/c}"/> then for every <img alt="f_i" class="latex" src="https://s0.wp.com/latex.php?latex=f_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_i"/>, <img alt="\Pr[ |N_i| \geq k/\sqrt{n} ] \leq e^{-ck^2} = e^{-10 \log |\mathcal{F}|} &lt; |\mathcal{F}|^{-10}" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B+%7CN_i%7C+%5Cgeq+k%2F%5Csqrt%7Bn%7D+%5D+%5Cleq+e%5E%7B-ck%5E2%7D+%3D+e%5E%7B-10+%5Clog+%7C%5Cmathcal%7BF%7D%7C%7D+%3C+%7C%5Cmathcal%7BF%7D%7C%5E%7B-10%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Pr[ |N_i| \geq k/\sqrt{n} ] \leq e^{-ck^2} = e^{-10 \log |\mathcal{F}|} &lt; |\mathcal{F}|^{-10}"/>. Hence by the union bound, the probability that there <em>exists</em> <img alt="f_i\in \mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=f_i%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_i\in \mathcal{F}"/> such that <img alt="|N_i| \geq k/\sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7CN_i%7C+%5Cgeq+k%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="|N_i| \geq k/\sqrt{n}"/> is at most <img alt="|\mathcal{F}|/|\mathcal{F}|^{10} \rightarrow 0" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cmathcal%7BF%7D%7C%2F%7C%5Cmathcal%7BF%7D%7C%5E%7B10%7D+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="|\mathcal{F}|/|\mathcal{F}|^{10} \rightarrow 0"/>. QED</p>



<h3>Other generalization bounds</h3>



<p>One way to count the number of classifiers in a family <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{F}"/> is by the bits to represent a member of the family– there are at most <img alt="2^k" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2^k"/> functions that can be represented using <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k"/> bits. But this bound can be quite loose – for example, it can make a big difference if we use <img alt="32" class="latex" src="https://s0.wp.com/latex.php?latex=32&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="32"/> or <img alt="64" class="latex" src="https://s0.wp.com/latex.php?latex=64&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="64"/> bits to specify numbers, and some natural families (e.g., linear functions) are <em>infinite</em>. There are many bounds in the literature of the form</p>



<p><img alt="\text{Generalization gap} \leq O\left(\sqrt{\frac{d}{n}} \right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BGeneralization+gap%7D+%5Cleq+O%5Cleft%28%5Csqrt%7B%5Cfrac%7Bd%7D%7Bn%7D%7D+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\text{Generalization gap} \leq O\left(\sqrt{\frac{d}{n}} \right)"/></p>



<p>with values of <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> other than <img alt="\log |\mathcal{F}|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+%7C%5Cmathcal%7BF%7D%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log |\mathcal{F}|"/>.<br/>Intuitively <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> corresponds to the “capacity” of the classifier family/algorithm – the number of samples it can fit/memorize. Some examples (very roughly stated) include:</p>



<ul><li><strong>VC dimension:</strong> <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> is the maximum number such that for every set of <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> points and labels, there is a classifier in the family that fits the points to the labels. That is for every <img alt="x_1,\ldots,x_d" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_d"/> and <img alt="y_1,\ldots,y_d" class="latex" src="https://s0.wp.com/latex.php?latex=y_1%2C%5Cldots%2Cy_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y_1,\ldots,y_d"/> there is <img alt="f\in\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Cin%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f\in\mathcal{F}"/> with <img alt="\forall_i f(x_i)=y_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cforall_i+f%28x_i%29%3Dy_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\forall_i f(x_i)=y_i"/>.</li><li><strong>Rademacher Complexity:</strong> <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> is the maximum number such that for random <img alt="x_1,\ldots,x_d" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_d"/> from <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X"/> and <img alt="y_1,\ldots,y_d" class="latex" src="https://s0.wp.com/latex.php?latex=y_1%2C%5Cldots%2Cy_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y_1,\ldots,y_d"/> uniform (assume say over <img alt="{ \pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ \pm 1 }"/>) with high probability there exists <img alt="f\in \mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f\in \mathcal{F}"/> with <img alt="f(x_i)\approx y_i" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x_i%29%5Capprox+y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x_i)\approx y_i"/> for most <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/>.</li><li><strong>PAC Bayes:</strong> <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> is the mutual information between the training set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="S"/> that the learning algorithm is given as input and the classifier that it outputs. This requires some conditions on the learning algorithm and some prior distribution on the classifier. To get bounds on this quantity when the weights are continuous, we can add <em>noise</em> to them.</li><li><strong>Margin bounds:</strong> <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> is the “effective dimensionality” as measured by some margin. For example, for random unit vectors <img alt="w,x" class="latex" src="https://s0.wp.com/latex.php?latex=w%2Cx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w,x"/> in <img alt="\mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{R}^d"/>, <img alt="|\langle w,x \rangle| \sim 1/\sqrt{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Clangle+w%2Cx+%5Crangle%7C+%5Csim+1%2F%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="|\langle w,x \rangle| \sim 1/\sqrt{d}"/>. For linear classifiers, the margin bound is the minimum <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> such that correct labels over the training set are classified with at least <img alt="1/\sqrt{d}" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\sqrt{d}"/> margin.</li></ul>



<p>A recent empirical study of generalization bounds is <a href="https://arxiv.org/abs/1912.02178">“fantastic generalization measures and where to find them”</a>by Jiang, Neyshabur, Mobahi, Krishnan, and Bengio, and <a href="https://arxiv.org/abs/2010.11924">“In Search of Robust Measures of Generalization”</a> by Dziugaite,  Drouin, Neal, Rajkumar, Caballero, Wang, Mitliagkas, and Roy.</p>



<h2>Limitations of generalization bounds</h2>



<p>The generalization gap <img alt="\mathbb{E}_{f=A(S)}\left[ \mathcal{L}(f) - \hat{\mathcal{L}}(f) \right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bf%3DA%28S%29%7D%5Cleft%5B+%5Cmathcal%7BL%7D%28f%29+-+%5Chat%7B%5Cmathcal%7BL%7D%7D%28f%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_{f=A(S)}\left[ \mathcal{L}(f) - \hat{\mathcal{L}}(f) \right]"/> depends on several quantities:</p>



<ul><li>The family <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{F}"/> of functions.</li><li>The algorithm <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> used to map the training set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="S"/> to <img alt="f\in\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Cin%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f\in\mathcal{F}"/>.</li><li>The distribution <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X"/> of datapoints</li><li>The distribution <img alt="Y|X" class="latex" src="https://s0.wp.com/latex.php?latex=Y%7CX&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Y|X"/> of labels.</li></ul>



<p>A <strong>generalization bound</strong> is an upper bound on the gap that only depends on some of these quantities. In an influential paper, <a href="https://arxiv.org/abs/1611.03530">Zhang, Bengio, Hardt, Recht, Vinyals</a> showed significant barriers to obtaining such results that are meaningful for practical deep networks. They showed that in many natural settings, we cannot get such bounds even if we allow them to be based arbitrarily on the first three factors. That is, they showed that for natural families of functions (modern deep nets), natural algorithms (gradient descent on the empirical loss), natural distributions (CIFAR 10 and ImageNet), if we replace <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Y"/> by the uniform distribution, then we can get arbitrarily large generalization gap.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/QqP2qnc.png"/></figure>



<p>We can also interpolate between the Zhang et al. experiment and the plain CIFAR-10 distribution. If we consider a distribution <img alt="(X,\tilde{Y})" class="latex" src="https://s0.wp.com/latex.php?latex=%28X%2C%5Ctilde%7BY%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(X,\tilde{Y})"/> where we take <img alt="(X,Y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28X%2CY%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(X,Y)"/> from CIFAR-10 with probability <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> we replace the <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Y"/> with a random label (one of the 10 CIFAR-10 classes) then the test/population performance (fraction of correct classifications) will be at most <img alt="(1-p) + p/10" class="latex" src="https://s0.wp.com/latex.php?latex=%281-p%29+%2B+p%2F10&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(1-p) + p/10"/> (not surprising), but the training/empirical accuracy will remain at roughly 100%. The left-hand side of the gif below demonstrates this (this comes from <a href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/">this paper with Bansal and Kaplun</a> which shows that, as the right side demonstrates, certain self-supervised learning algorithms do not suffer from this phenomenon; here the noise level is the fraction of wrong labels so <img alt="0.9" class="latex" src="https://s0.wp.com/latex.php?latex=0.9&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0.9"/> is perfect noise):</p>



<figure class="wp-block-image"><img alt="" src="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif"/></figure>



<h2>“Double descent.”</h2>



<p>While classical learning theory predicts a “bias-variance tradeoff” whereby as we increase the model class size, we get worse and worse performance, this is not what happens in modern deep learning systems. <a href="https://arxiv.org/abs/1812.11118">Belkin, Hsu, Ma, and Mandal</a> posited that such systems undergo a “double descent” whereby performance behaves according to the classical bias/variance curve up to the point in which we achieve <img alt="\approx 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\approx 0"/> training error and then starts improving again. This <a href="https://windowsontheory.org/2019/12/05/deep-double-descent/">actually happens</a> in real deep networks.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/PpyW6HW.png"/></figure>



<p>To get some intuition for the double descent phenomenon, consider the case of fitting a univariate polynomial of degree <img alt="d^0" class="latex" src="https://s0.wp.com/latex.php?latex=d%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d^0"/> to <img alt="n &gt; d^0" class="latex" src="https://s0.wp.com/latex.php?latex=n+%3E+d%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n &gt; d^0"/> samples of the form <img alt="(x,f(x)+noise)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%2Bnoise%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,f(x)+noise)"/> where <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> is a degree <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> polynomial. When <img alt="d&lt;d^0" class="latex" src="https://s0.wp.com/latex.php?latex=d%3Cd%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d&lt;d^0"/> we are “under-fitting” and will not get good performance. As <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> trends between <img alt="d^0" class="latex" src="https://s0.wp.com/latex.php?latex=d%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d^0"/> and <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/>, we fit more and more of the noise, until for <img alt="d=n" class="latex" src="https://s0.wp.com/latex.php?latex=d%3Dn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d=n"/> we have a perfect interpolating polynomial that will have perfect train but very poor test performance. When <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> grows beyond <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/>, more than one polynomial can fit the data, and (under certain conditions) SGD will select the minimal norm one, which will make the interpolation smoother and smoother and actually result in better performance.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/ZhPtS0Y.gif"/></figure>



<h2>Approximation and representation</h2>



<p>Consider the task of distinguishing between the speech of an adult and a child. In the time domain, this may be hard, but by switching to representation in the Fourier domain, the task becomes much easier. (See this cartoon)</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/8SKSB9q.png"/></figure>



<p>The Fourier transform is based on the following theorem: for every continuous <img alt="f:[0,1] \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5B0%2C1%5D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f:[0,1] \rightarrow \mathbb{R}"/>, we can arbitrarily well approximate <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> as a linear combination of functions of the form <img alt="e^{2\pi i \alpha x}" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B2%5Cpi+i+%5Calpha+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="e^{2\pi i \alpha x}"/>. Another way to say it is that if we use the embedding <img alt="\varphi:\mathbb{R} \rightarrow \mathbb{R}^N" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi%3A%5Cmathbb%7BR%7D+%5Crightarrow+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi:\mathbb{R} \rightarrow \mathbb{R}^N"/> which maps <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> into (sufficiently large) <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N"/> coordinates of the form <img alt="e^{2 \pi i \alpha_j x}" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B2+%5Cpi+i+%5Calpha_j+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="e^{2 \pi i \alpha_j x}"/> then <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> becomes linear.</p>



<p>The wave functions are not the only ones that can approximate an arbitrary function. A <em>ReLU</em> is a function <img alt="r:\mathbb{R}^d \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=r%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r:\mathbb{R}^d \rightarrow \mathbb{R}"/> of the form <img alt="r(x) = max \{ w \cdot x + b , 0\}" class="latex" src="https://s0.wp.com/latex.php?latex=r%28x%29+%3D+max+%5C%7B+w+%5Ccdot+x+%2B+b+%2C+0%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r(x) = max \{ w \cdot x + b , 0\}"/>. We can approximate every continuous function arbitrarily well as a combination of ReLUs:</p>



<p><strong>Theorem:</strong> For every continous <img alt="f:[0,1]^d \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5B0%2C1%5D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f:[0,1]^d \rightarrow \mathbb{R}"/> and <img alt="\epsilon&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\epsilon&gt;0"/> there is a function <img alt="g:[0,1]^d \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=g%3A%5B0%2C1%5D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g:[0,1]^d \rightarrow \mathbb{R}"/> such that <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/> is a linear combination of ReLUs and <img alt="\int |f-g| &lt; \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint+%7Cf-g%7C+%3C+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\int |f-g| &lt; \epsilon"/>.</p>



<p>In one dimension, this follows from the facts that:</p>



<ul><li>ReLUs can give an arbitrarily good approximation to bump functions of the form<br/><img alt="I_{a,b}(x) = \begin{cases} 1 &amp; a \leq x \leq b \\ 0 &amp; \text{otherwise}\end{cases}" class="latex" src="https://s0.wp.com/latex.php?latex=I_%7Ba%2Cb%7D%28x%29+%3D+%5Cbegin%7Bcases%7D+1+%26+a+%5Cleq+x+%5Cleq+b+%5C%5C+0+%26+%5Ctext%7Botherwise%7D%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I_{a,b}(x) = \begin{cases} 1 &amp; a \leq x \leq b \\ 0 &amp; \text{otherwise}\end{cases}"/></li><li>Every continuous function on a bounded domain can be arbitrarily well approximated by the sum of bump functions.</li></ul>



<p>The second fact is well known, and here is a “proof by picture” for the first one:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/sKhHEap.png"/></figure>



<p>For higher dimensions, we need to create higher dimension bump functions. For example, in two dimensions, we can create a “noisy circle” by summing over all rotations of our bump. We can then add many such circles to create a two-dimensional bump. The same construction extends to an arbitrary number of dimensions.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/V8Wm5X1.png"/></figure>



<p><strong>How many ReLUs?</strong> The above shows that a linear combination of ReLUs can approximate every function on <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> variables, but how many ReLUs are needed? Every ReLU <img alt="r:\mathbb{R}^d \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=r%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r:\mathbb{R}^d \rightarrow \mathbb{R}"/> is specified by <img alt="d+1" class="latex" src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d+1"/> numbers for the weights and bias. Intuitively, we could discretize each coordinate to a constant number of choices, and so there would be <img alt="O(1)^d = 2^{O(d)}" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%29%5Ed+%3D+2%5E%7BO%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="O(1)^d = 2^{O(d)}"/> choices for such ReLUs. Indeed, it can be shown that every continuous function can be approximated by a linear combination of <img alt="2^{O(d)}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7BO%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2^{O(d)}"/> ReLUs. It turns out that some functions <em>require</em> an exponential number of ReLUS.</p>



<p>The above discussion doesn’t apply just for ReLUs but virtually any non-linear function.</p>



<h3>Representation summary</h3>



<p>By embedding our input <img alt="x\in \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\in \mathbb{R}^d"/> as a vector <img alt="\varphi(x) \in \mathbb{R}^N" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29+%5Cin+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi(x) \in \mathbb{R}^N"/>, we can often make many “interesting” functions <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> become much simpler to compute (e.g., linear). In learning, we typically search for an <em>embedding</em> or <em>representation</em> that is “good” in one or more of the following senses:</p>



<ul><li>The dimension of embedding <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N"/> is not too large for many “interesting” functions.</li><li>Two inputs <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x,y"/> are “semantically similar” if and only if <img alt="\varphi(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi(x)"/> and <img alt="\varphi(y)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi(y)"/> are correlated (e.g., <img alt="\langle \varphi(x),\varphi(y) \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x%29%2C%5Cvarphi%28y%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle \varphi(x),\varphi(y) \rangle"/> is large).</li><li>We can efficiently compute <img alt="\varphi(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi(x)"/> and (sometimes) can compute <img alt="\langle \varphi(x), \varphi(y) \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x%29%2C+%5Cvarphi%28y%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle \varphi(x), \varphi(y) \rangle"/> without needing to explicitly compute <img alt="\varphi(x),\varphi(y)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29%2C%5Cvarphi%28y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi(x),\varphi(y)"/>.</li><li>For “interesting” functions <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>, <img alt="f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)"/> can be approximated by a linear function in the embedding <img alt="\varphi(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi(x)"/> with “structured” coefficients (for example, sparse combination, or combination of coefficients of certain types, such as low frequency coefficients in Fourier domain)</li><li>…</li></ul>



<h2>Kernels and nearest neighbors</h2>



<p>Suppose that we have some notion <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="K"/> of “similarity” between inputs, where <img alt="K(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=K%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="K(x,y)"/> being large means that <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is “close” to <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/> and <img alt="K(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=K%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="K(x,y)"/> being small means that <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is “far” from <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/>.</p>



<p>This suggests that we can use one of the following methods approximating a function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> given inputs of the form <img alt="{(x_i, y_i \approx f(x_i))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x_i%2C+y_i+%5Capprox+f%28x_i%29%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{(x_i, y_i \approx f(x_i))}"/>. On input <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, any of the following can be reasonable approximations to <img alt="f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)"/> depending on context:</p>



<ul><li><img alt="y_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y_i"/> where <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/> is the closest to <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> in <img alt="{ x_1,\ldots, x_n }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+x_1%2C%5Cldots%2C+x_n+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ x_1,\ldots, x_n }"/>. (This is known as the <em>nearest neighbor</em> algorithm.)</li><li>The mean (or other combining function) of <img alt="y_{i_1},\ldots, y_{i_k}" class="latex" src="https://s0.wp.com/latex.php?latex=y_%7Bi_1%7D%2C%5Cldots%2C+y_%7Bi_k%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y_{i_1},\ldots, y_{i_k}"/> where <img alt="{ x_{i_1},\ldots, x_{i_k} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+x_%7Bi_1%7D%2C%5Cldots%2C+x_%7Bi_k%7D+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ x_{i_1},\ldots, x_{i_k} }"/> are the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k"/> nearest inputs to <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. (This is known as the <em><img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k"/> nearest neighbor</em> algorithm.)</li><li>Some linear combination of <img alt="y_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y_i"/> where the coefficients depend on <img alt="K(x,x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=K%28x%2Cx_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="K(x,x_i)"/>. (This is known as the <em>kernel</em> algorithm.)</li></ul>



<p>All of these algorithms are _non-parametric methods_ in the sense that the final regressor/classifier is specified by the full training set <img alt="{ (x_i,y_i) }_{i=1..n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%28x_i%2Cy_i%29+%7D_%7Bi%3D1..n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ (x_i,y_i) }_{i=1..n}"/>.</p>



<p><strong>Kernel algorithms</strong> can also be described as follows. Given some embedding <img alt="\varphi:\mathcal{X} \rightarrow \mathbb{R}^N" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi%3A%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi:\mathcal{X} \rightarrow \mathbb{R}^N"/>, where <img alt="\mathcal{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{X}"/> is our input space, a Kernel regression approximates a function <img alt="f:\mathcal{X} \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f:\mathcal{X} \rightarrow \mathbb{R}"/> by a linear function in <img alt="\varphi(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi(x)"/>.</p>



<p>The key observation is that to solve linear equations or least-square minimization in <img alt="w \in \mathbb{R}^N" class="latex" src="https://s0.wp.com/latex.php?latex=w+%5Cin+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w \in \mathbb{R}^N"/> of the form <img alt="\langle \varphi(x_i) , w \rangle \approx y_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x_i%29+%2C+w+%5Crangle+%5Capprox+y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle \varphi(x_i) , w \rangle \approx y_i"/>, we don’t need to know the vectors <img alt="\varphi(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi(x_i)"/>. Rather, it is enough to know the inner products <img alt="\langle \varphi(x_i), \varphi(x_j) \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x_i%29%2C+%5Cvarphi%28x_j%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle \varphi(x_i), \varphi(x_j) \rangle"/>. In Kernel methods we are often not given the embedding explicitly (indeed <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N"/> might even be infinite) but rather the function <img alt="K:\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=K%3A%5Cmathcal%7BX%7D+%5Ctimes+%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="K:\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}"/> such that <img alt="K(x_i,x_j) = \langle \varphi(x_i),\varphi(x_j) \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=K%28x_i%2Cx_j%29+%3D+%5Clangle+%5Cvarphi%28x_i%29%2C%5Cvarphi%28x_j%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="K(x_i,x_j) = \langle \varphi(x_i),\varphi(x_j) \rangle"/>. The only thing to verify is that <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="K"/> actually defines an inner product by checking that the matrix <img alt="( K(x_i,x_j))_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%28+K%28x_i%2Cx_j%29%29_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="( K(x_i,x_j))_{i,j}"/> is positive semi-definite.</p>



<p>In general, Kernels and neural networks look quite similar – both ultimately involve composing a linear function <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L"/> on top of a non-linear embedding <img alt="\varphi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi"/>. It is not always clear cut whether an algorithm is a kernel or deep neural net method. Some characteristics of kernels are:</p>



<ul><li>The embedding <img alt="\varphi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi"/> is not learned from the data. However, if <img alt="\varphi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\varphi"/> was learned from some other data, or was inspired by representations that were learned from data, then it becomes a fuzzier distinction.</li><li>There is a “shortcut” to compute the inner product <img alt="\langle \varphi(x),\varphi(x') \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x%29%2C%5Cvarphi%28x%27%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle \varphi(x),\varphi(x') \rangle"/> using significantly smaller than <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N"/> steps.</li></ul>



<p>Generally, the distinction between a kernel and deep nets depends on the application (is it to apply some analysis such as generalization bounds for kernels? is it to use kernel methods with shortcuts for the inner product?) and is more a spectrum than a binary partition.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/jeHi32z.png"/></figure>



<h2>Conclusion</h2>



<p>The above was a very condensed and rough survey of generalization, representation, approximation, and kernel methods. All of these are covered much better in the understanding machine learning book and the upcoming Hardt and Recht book.</p>



<p>In the next lecture, we will discuss the algorithmic bias of gradient descent, including the cases of linear regression and deep linear networks. We will discuss the “simplicity bias” of SGD and what can we say about what is learned at different layers of a deep network.</p>



<p><strong>Acknowledgements:</strong>  Thanks to Manos Theodosis and Preetum Nakkiran for pointing out several typos in a previous version.</p>



<p/></div>
    </content>
    <updated>2021-01-31T19:07:34Z</updated>
    <published>2021-01-31T19:07:34Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-02-09T09:21:09Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/01/31/linkage</id>
    <link href="https://11011110.github.io/blog/2021/01/31/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Hilbert’s 13th, unsolved (\(\mathbb{M}\)). You can solve polynomials of degree at most four using one-argument algebraic functions like \(\sqrt x\). If \(RD(n)\) denotes the number of arguments needed for degree-\(n\) polynomials, then \(RD(4)=1\). Hilbert asked whether \(RD(7)=2\). Vladimir Arnold showed in the 1950s that you can solve all polynomials with two-variable continuous (but not algebraic) functions, but mathematicians are only now catching on that the algebraic problem is still open. See also some recent bounds on \(RD\).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-probe-unsolved-hilbert-polynomial-problem-20210114/">Hilbert’s 13th, unsolved</a> (<a href="https://mathstodon.xyz/@11011110/105569819827303922">\(\mathbb{M}\)</a>). You can solve polynomials of degree at most four using one-argument algebraic functions like \(\sqrt x\). If \(RD(n)\) denotes the number of arguments needed for degree-\(n\) polynomials, <span style="white-space: nowrap;">then \(RD(4)=1\).</span> Hilbert asked whether \(RD(7)=2\). Vladimir Arnold showed in the 1950s that you can solve all polynomials with two-variable continuous (but not algebraic) functions, but mathematicians are only now catching on that the algebraic problem is still  open. See also <a href="https://arxiv.org/abs/2001.06515">some recent bounds on \(RD\)</a>.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/joshmillard/status/1349979253937381379">Fogleworms</a> (<a href="https://mastodon.social/@joshmillard/105572806531271932">\(\mathbb{M}\)</a>), partitions of \(n\times n\) grids into \(n\)-vertex grid paths, their enumeration, and a crafty project to visualize them.</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1007/s00283-020-10034-w">A figure with Heesch number 6: Pushing a two-decade-old boundary</a> (<a href="https://mathstodon.xyz/@11011110/105579308071467320">\(\mathbb{M}\)</a>), Bojan Bašić in the <em>Mathematical Intelligencer</em>. When a shape cannot tile the plane, its <a href="https://en.wikipedia.org/wiki/Heesch%27s_problem">Heesch number</a> measures how far you can tile before getting stuck: if you surround the shape by layers of the same shape, how many layers can you make? Casey Mann’s previous record of five looked like a row of five hexagons with extra crenellations. This one uses six hexagons, with simpler crenellations.</p>
  </li>
  <li>
    <p><a href="https://blogs.scientificamerican.com/roots-of-unity/computation-in-service-of-poetry/">Exponentiation by squaring, in somewhat cryptic form, in a work by Pingala from India from over 2000 years ago</a> (<a href="https://mathstodon.xyz/@11011110/105586527250804920">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>I’ve been trying to understand the structure of the Kurilpa Bridge in Brisbane, supposedly “the world’s largest tensegrity bridge” (<a href="https://mathstodon.xyz/@11011110/105592704371209563">\(\mathbb{M}\)</a>). The clearest description I’ve found is from <a href="http://tadashidesign.com/kurilpa-bridge">Tadashi Design</a>. <a href="https://en.wikipedia.org/wiki/Kurilpa_Bridge">Wikipedia</a> is more cagy, calling it a “hybrid tensegrity bridge”, as it also includes features of cable-stayed bridges where the deck hangs from cables attached to tower piers. But the Tadashi Design site shows long sections far from the piers, so it seems the tensegrity is not just for show.</p>
  </li>
  <li>
    <p><a href="https://www.nationalgeographic.com/science/2021/01/we-need-better-face-masks-and-origami-might-help/">How origami folding patterns might help in the design of better face masks</a> (<a href="https://mathstodon.xyz/@11011110/105595412289181842">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>More US universities using covid as an excuse to treat faculty badly (<a href="https://mathstodon.xyz/@11011110/105609583164670586">\(\mathbb{M}\)</a>): <a href="https://www.chronicle.com/article/kansas-regents-allow-sped-up-dismissals-of-tenured-faculty-members">the Kansas state university system guts tenure</a>, and <a href="https://www.insidehighered.com/news/2021/01/21/u-florida-asks-students-report-professors-who-arent-teaching-person">the University of Florida asks students to snitch on faculty who refuse to endanger themselves by teaching in person</a>.</p>
  </li>
  <li>
    <p><a href="https://www.departures.com/lifestyle/architecture/hayri-atak-design-sarcostyle-building-manhattan-skyline">Proposed New York waterfront tower is a handlebody of high genus</a> (<a href="https://mathstodon.xyz/@11011110/105612316794720508">\(\mathbb{M}\)</a>, <a href="https://mastodon.social/@sarielhp/105612251833052747">via</a>, <a href="https://twitter.com/MathematicsUCL/status/1353328317781467137">via2</a>).</p>
  </li>
  <li>
    <p>Although it also <a href="https://en.wikipedia.org/wiki/Book_(graph_theory)">has other names</a>, the graph \(K_{1,1,n}\)  has been called the “thagomizer graph”, and its associated graphic matroid has been called the “thagomizer matroid” (<a href="https://mathstodon.xyz/@11011110/105620864011377814">\(\mathbb{M}\)</a>). The term appears to have been introduced by Katie Gedeon in  <a href="https://arxiv.org/abs/1610.05349">arXiv:1610.05349</a> in honor of the famous Far Side cartoon, whose terminology has <a href="https://en.wikipedia.org/wiki/Thagomizer">also been adopted by some paleontologists</a>.</p>
  </li>
  <li>
    <p>This new preprint looks interesting: <a href="https://arxiv.org/abs/2101.09592">Point-hyperplane incidence geometry and the log-rank conjecture, Noah Singer and Madhu Sudan, arXiv:2101.09592</a> (<a href="https://mathstodon.xyz/@11011110/105626816229116967">\(\mathbb{M}\)</a>). In the plane, \(n\) points and \(m\) lines can only touch \(\Theta\bigl((mn)^{2/3}+m+n\bigr)\) times. In 3d, points and planes can have mn incidences but only by sharing a common line. This paper connects similar problems in high dimensions to the <a href="https://en.wikipedia.org/wiki/Log-rank_conjecture">log-rank conjecture</a>, a famous unsolved problem in communication complexity.</p>
  </li>
  <li>
    <p><a href="https://www.math.ucdavis.edu/research/seminars/?talk_id=6082">Unknot recognition in quasi-polynomial time</a> (<a href="https://mathstodon.xyz/@11011110/105630455655140054">\(\mathbb{M}\)</a>, <a href="https://www.scottaaronson.com/blog/?p=5270">via</a>). Title of talk announcement by Marc Lackenby. No details or preprint yet but judging solely from the title and non-fringe status of the author this sounds like big news.</p>
  </li>
  <li>
    <p><a href="https://mathcs.clarku.edu/~fgreen/bookreviews/51-4.pdf">Frederic Green has published another review of my book “Forbidden Configurations in Discrete Geometry” in the latest <em>SIGACT News</em></a> (<a href="https://mathstodon.xyz/@11011110/105636589457644955">\(\mathbb{M}\)</a>, <a href="https://doi.org/10.1145/3444815.3444817">official but paywalled url</a>). Thanks to Joe O’Rourke for the heads-up: I last checked my mail at the office, where my physical copies of <em>SIGACT News</em> would go if they went anywhere, months ago, and even then it looked like magazines weren’t getting through.</p>
  </li>
  <li>
    <p>Mathematics on the cutting block at Leicester again: <a href="https://gowers.wordpress.com/2021/01/30/leicester-mathematics-under-threat-again/">Gowers</a>, <a href="https://golem.ph.utexas.edu/category/2021/01/problems_at_the_university_of.html">nCat</a>, <a href="https://www.ipetitions.com/petition/mathematics-is-not-redundant">petition</a> (<a href="https://mathstodon.xyz/@11011110/105646674167288349">\(\mathbb{M}\)</a>). The plan is to eliminate research in pure mathematics at the University of Leicester, fire eight professors, and hire three back in purely teaching positions. I’m not sure who the eight are – the <a href="https://le.ac.uk/mathematics/people/academic-and-research">staff list</a> includes some other disciplines – but Leicester mathematicians in Wikipedia include <a href="https://en.wikipedia.org/wiki/Katrin_Leschke">Katrin Leschke</a> and <a href="https://en.wikipedia.org/wiki/Sergei_Petrovskii">Sergei Petrovskii</a>.</p>
  </li>
  <li>
    <p>Two newly-listed Good Articles on Wikipedia: <a href="https://en.wikipedia.org/wiki/Curve_of_constant_width">Curve of constant width</a> and <a href="https://en.wikipedia.org/wiki/Ronald_Graham">Ronald Graham</a> (<a href="https://mathstodon.xyz/@11011110/105652461392545754">\(\mathbb{M}\)</a>).</p>
  </li>
</ul></div>
    </content>
    <updated>2021-01-31T15:57:00Z</updated>
    <published>2021-01-31T15:57:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-02-06T21:11:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/008</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/008" rel="alternate" type="text/html"/>
    <title>TR21-008 |  Random walks and forbidden minors III: poly(d/?)-time partition oracles for minor-free graph classes | 

	Akash Kumar, 

	C. Seshadhri, 

	Andrew Stolman</title>
    <summary>Consider the family of bounded degree graphs in any minor-closed family (such as planar graphs). Let d be the degree bound and n be the number of vertices of such a graph. Graphs in these classes have hyperfinite decompositions, where, for a sufficiently small ? &gt; 0, one removes
?dn edges to get connected components of size independent of n. An important tool for sublinear
algorithms and property testing for such classes is the partition oracle, introduced by the seminal
work of Hassidim-Kelner-Nguyen-Onak (FOCS 2009). A partition oracle is a local procedure
that gives consistent access to a hyperfinite decomposition, without any preprocessing. Given a
query vertex v, the partition oracle outputs the component containing v in time independent of
n. All the answers are consistent with a single hyperfinite decomposition.
The partition oracle of Hassidim et al. runs in time d^poly(d/?)-per query. They pose the
open problem of whether poly(d/?)-time partition oracles exist. Levi-Ron (ICALP 2013) give
a refinement of the previous approach, to get a partition oracle that runs in time d^log(d/?)-per
query.
In this paper, we resolve this open problem and give poly(d/?)-time partition oracles for
bounded degree graphs in any minor-closed family. Unlike the previous line of work based on
combinatorial methods, we employ techniques from spectral graph theory. We build on a recent
spectral graph theoretical toolkit for minor-closed graph families, introduced by the authors to
develop efficient property testers. A consequence of our result is a poly(d/?)-query tester for
any property of minor-closed families (such as bipartite planar graphs). Our result also gives
poly(d/?)-query algorithms for additive ?n-approximations for problems such as maximum
matching, minimum vertex cover, maximum independent set, and minimum dominating set for
these graph families.</summary>
    <updated>2021-01-30T11:24:19Z</updated>
    <published>2021-01-30T11:24:19Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-09T09:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5253</id>
    <link href="https://www.scottaaronson.com/blog/?p=5253" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5253#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5253" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Once we can see them, it’s too late</title>
    <summary xml:lang="en-US">[updates: here’s the paper, and here’s Robin’s brief response to some of the comments here] This month Robin Hanson, the famous and controversy-prone George Mason University economics professor who I’ve known since 2004, was visiting economists here in Austin for a few weeks. So, while my fear of covid considerably exceeds Robin’s, I met with […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[updates: <a href="https://arxiv.org/abs/2102.01522">here’s the paper</a>, and <a href="https://www.scottaaronson.com/blog/?p=5253#comment-1877426">here’s Robin’s brief response</a> to some of the comments here]</em></p>



<p>This month <a href="http://mason.gmu.edu/~rhanson/home.html">Robin Hanson</a>, the famous and <a href="https://www.scottaaronson.com/blog/?p=3766">controversy-prone</a> George Mason University economics professor who I’ve known since 2004, was visiting economists here in Austin for a few weeks.  So, while my fear of covid considerably exceeds Robin’s, I met with him a few times in the mild Texas winter in an outdoor, socially-distanced way.  It took only a few minutes for me to remember why I enjoy talking to Robin so much.</p>



<p>See, while I’d been moping around depressed about covid, the vaccine rollout, the insurrection, my inability to focus on work, and a dozen other things, Robin was bubbling with excitement about a brand-new mathematical model he was working on to understand the growth of civilizations across the universe—a model that, Robin said, explained lots of cosmic mysteries in one fell swoop and also made striking predictions.  My cloth facemask was, I confess, unable to protect me from Robin’s infectious enthusiasm.</p>



<p>As I listened, I went through the classic stages of reaction to a new Hansonian proposal: first, bemusement over the sheer weirdness of what I was being asked to entertain, as well as Robin’s failure to acknowledge that weirdness in any way whatsoever; then, confusion about the unstated steps in his radically-condensed logic; next, the raising by me of numerous objections (each of which, it turned out, Robin had already thought through at length); finally, the feeling that I <em>must have</em> seen it this way all along, because isn’t it kind of obvious?</p>



<p>Robin has been explaining his model in a <a href="https://www.overcomingbias.com/2020/12/how-far-aggressive-aliens.html">sequence</a> <a href="https://www.overcomingbias.com/2020/12/how-far-aggressive-aliens-part-2.html">of</a> <em><a href="https://www.overcomingbias.com/2020/12/the-long-term-future-of-history.html">Overcoming</a> <a href="https://www.overcomingbias.com/2021/01/try-menu-combo-filter-steps.html">Bias</a></em> <a href="https://www.overcomingbias.com/2021/01/why-we-cant-see-grabby-aliens.html">posts</a>, and <s>will apparently have a paper out about the model soon</s> <a href="https://arxiv.org/abs/2102.01522">the paper is here!</a>  In this post, I’d like to offer my own take on what Robin taught me.  Blame for anything I mangle lies with me alone.</p>



<p>To cut to the chase, Robin is trying to explain the famous <a href="https://en.wikipedia.org/wiki/Fermi_paradox">Fermi Paradox</a>: why, after 60+ years of looking, and despite the periodic excitement around <a href="https://en.wikipedia.org/wiki/Tabby%27s_Star">Tabby’s star</a> and <a href="https://en.wikipedia.org/wiki/%CA%BBOumuamua">‘Oumuamua</a> and the like, have we not seen a single undisputed sign of an extraterrestrial civilization?  Why all this nothing, even though the observable universe is vast, even though (as we now know) organic molecules and planets in Goldilocks zones are everywhere, and even though there have been billions of years for aliens someplace to get a technological head start on us, expanding across a galaxy to the point where they’re easily seen?</p>



<p>Traditional answers to this mystery include: maybe the extraterrestrials quickly annihilate themselves in nuclear wars or environmental cataclysms, just like we soon will; maybe the extraterrestrials don’t <em>want</em> to be found (whether out of self-defense or a cosmic Prime Directive); maybe they spend all their time playing video games.  Crucially, though, all answers of that sort founder against the realization that, given a million alien civilizations, each perhaps more different from the others than kangaroos are from squid, <em>it would only take one</em>, spreading across a billion light-years and transforming everything to its liking, for us to have noticed it.</p>



<p>Robin’s answer to the puzzle is as simple as it is terrifying.  Such civilizations might well exist, he says, but if so, by the time we noticed one, it would already be nearly too late.  Robin proposes, plausibly I think, that if you give a technological civilization 10 million or so years—i.e., an eyeblink on cosmological timescales—then <em>either</em></p>



<ol><li>the civilization wipes itself out, <em>or else</em></li><li>it reaches some relatively quiet steady state, <em>or else</em></li><li>if it’s serious about spreading widely, then it “maxes out” the technology with which to do so, approaching the limits set by physical law.</li></ol>



<p>In cases 1 or 2, the civilization will of course be hard for us to detect, unless it happens to be close by.  But what about case 3?  There, Robin says, the “civilization” should look from the outside like a sphere expanding at nearly the speed of light, transforming everything in its path.</p>



<p>Now think about it: when could we, on earth, detect such a sphere with our telescopes?  Only when the sphere’s thin outer shell had reached the earth—perhaps carrying radio signals from the extraterrestrials’ early history, <em>before</em> their rapid expansion started.  By that point, though, the expanding sphere itself would be nearly upon us!</p>



<p>What would happen to us once we were inside the sphere?  Who knows?  The expanding civilization might obliterate us, it might preserve us as zoo animals, it might merge us into its hive-mind, it might do something else that we can’t imagine, but in any case, <em>detecting</em> the civilization would presumably no longer be the relevant concern!</p>



<p>(Of course, one could also wonder what happens when two of these spheres collide: do they fight it out?  do they reach some agreement?  do they merge?  Whatever the answer, though, it doesn’t matter for Robin’s argument.)</p>



<p>On the view described, there’s only a tiny cosmic window in which a SETI program could be expected to succeed: namely, when the thin surface of the first of these expanding bubbles has just hit us, and when that surface hasn’t yet passed us by.  So, given our “selection bias”—meaning, the fact that we apparently <em>haven’t</em> yet been swallowed up by one of the bubbles—it’s no surprise if we don’t right now happen to find ourselves in the tiny detection window!</p>



<p>This basic proposal, it turns out, is not original to Robin.  Indeed, an <em>Overcoming Bias</em> reader named Daniel X. Varga <a href="http://disq.us/p/2eczhce">pointed out to Robin</a> that he (Daniel) <a href="https://www.scottaaronson.com/blog/?p=334#comment-10766">shared the same idea right here</a>—in a <em>Shtetl-Optimized</em> comment thread—back in 2008!  I must have read Daniel Varga’s comment then, but (embarrassingly) it didn’t make enough of an impression for me to have remembered it.  I probably thought the same as <em>you</em> probably thought while reading this post:</p>



<blockquote class="wp-block-quote"><p>“Sure, whatever.  This is an amusing speculation that could make for a fun science-fiction story.  Alas, like with virtually <em>every</em> story about extraterrestrials, there’s no good reason to favor this over a hundred other stories that a fertile imagination could just as easily spin.  Who the hell knows?”</p></blockquote>



<p>This is where Robin claims to take things further.  Robin would say that he takes them further by developing a mathematical model, and fitting the parameters of the model to the known facts of cosmic history.  Read <a href="https://www.overcomingbias.com/">Overcoming Bias</a>, or Robin’s forthcoming paper, if you want to know the details of his model.  Personally, I confess I’m less interested in those details than I am in the qualitative points, which (unless I’m mistaken) are easy enough to explain in words.</p>



<p>The key realization is this: when we contemplate the Fermi Paradox, we know more than the mere fact that we look and look and we don’t see any aliens.  There are other relevant data points to fit, having to do with the one sample of a technological civilization that we <em>do</em> have.</p>



<p>For starters, there’s the fact that life on earth has been evolving for at least ~3.5 billion years—for most of the time the earth has existed—<em>but</em> life has a mere billion more years to go, until the expanding sun boils away the oceans and makes the earth barely habitable.  In other words, at least on <em>this</em> planet, we’re already relatively close to the end.  Why should that be?</p>



<p>It’s an excellent fit, Robin says, to a model wherein there are a few incredibly difficult, improbable steps along the way to a technological civilization like ours—steps that might include the origin of life, of multicellular life, of consciousness, of language, of something else—and wherein, having achieved some step, evolution basically just does a random search until it either stumbles onto the next step or else runs out of time.</p>



<p>Of course, given that we’re here to talk about it, we necessarily find ourselves on a planet where all the steps necessary for blog-capable life happen to have succeeded.  There might be vastly more planets where evolution got stuck on some earlier step.</p>



<p>But here’s the interesting part: conditioned on all the steps having succeeded, we <em>should</em> find ourselves near the end of the useful lifetime of our planet’s star—simply because the more time is available on a given planet, the better the odds there.  I.e., look around the universe and you should find that, on <em>most</em> of the planets where evolution achieves all the steps, it nearly runs out the planet’s clock in doing so.  Also, as we look back, we should find the hard steps roughly evenly spaced out, with each one having taken a good fraction of the whole available time.  All this is an excellent match for what we see.</p>



<p>OK, but it leads to a second puzzle.  Life on earth is at least ~3.5 billion years old, while the observable universe is ~13.7 billion years old.  Forget for a moment about the oft-stressed enormity of these two timescales and concentrate on their <em>ratio</em>, which is merely ~4. <em> </em><strong>Life on earth stretches a full quarter of the way back in time to the Big Bang.</strong>  Even as an adolescent, I remember finding that striking, and not at all what I would’ve guessed <em>a priori</em>.  It seemed like obviously a clue to <em>something</em>, if I could only figure out what.</p>



<p>The puzzle is compounded once you realize that, even though the sun will boil the oceans in a billion years (and then die in a few billion more), other stars, primarily dwarf stars, will continue shining brightly for <em>trillions</em> more years.  Granted, the dwarf stars don’t seem quite as hospitable to life as sun-like stars, but they do seem <em>somewhat</em> hospitable, and there will be <em>lots</em> of them—indeed, more than of sun-like stars.  And they’ll last orders of magnitude longer.</p>



<p>To sum up, our temporal position relative to the lifetime of the sun makes it look as though life on earth was just a lucky draw from a gigantic cosmic <a href="https://en.wikipedia.org/wiki/Poisson_point_process">Poisson process</a>.  By contrast, our position relative to the lifetime of <em>all</em> the stars makes it look as though we arrived crazily, freakishly early—not at all what you’d expect under a random model.  So what gives?</p>



<p>Robin contends that <em>all</em> of these facts are explained under his bubble scenario.  If we’re to have an experience remotely like the human one, he says, then we <em>have to be</em> relatively close to the beginning of time—since hundreds of billions of years from now, the universe will likely be dominated by near-light-speed expanding spheres of intelligence, and a little upstart civilization like ours would no longer stand a chance.  I.e., even though our existence is down to some lucky accidents, and even though those same accidents probably recur throughout the cosmos, we shouldn’t yet <em>see</em> any of the other accidents, since if we did see them, it would already be nearly too late for us.</p>



<p>Robin admits that his account leaves a huge question open: namely, why should our experience have been a “merely human,” “pre-bubble” experience at all?  If you buy that these expanding bubbles are coming, it seems likely that there will be trillions of times more sentient experiences inside them than outside.  So experiences like ours would be rare and anomalous—like finding yourself at the dawn of human history, with Hammurabi et al., and realizing that almost every interesting thing that will ever happen is still to the future.  So Robin simply takes as a brute fact that our experience is “earth-like” or “human-like”; he then tries to explain the other observations from that starting point.</p>



<p>Notice that, in Robin’s scenario, the present epoch of the universe is extremely special: it’s when civilizations are just forming, when perhaps a few of them will achieve technological liftoff, but <em>before</em> one or more of the civilizations has remade the whole of creation for its own purposes.  Now is the time when the early intelligent beings like us can still look out and see quadrillions of stars shining to no apparent purpose, just <em>wasting</em> all that nuclear fuel in a near-empty cosmos, waiting for someone to come along and put the energy to good use.  In that respect, we’re sort of like the Maoris having just landed in New Zealand, or Bill Gates surveying the microcomputer software industry in 1975.  We’re ridiculously lucky.  The situation is way out of equilibrium.  The golden opportunity in front of us can’t possibly last forever.</p>



<p>If we accept the above, then a major question I had was the role of cosmology.  In 1998, astronomers discovered that the present cosmological epoch is special for a completely<em> different</em> reason than the one Robin talks about.  Namely, right now is when matter and <a href="https://en.wikipedia.org/wiki/Dark_energy">dark energy</a> contribute roughly similarly to the universe’s energy budget, with ~30% the former and ~70% the latter.  Billions of years hence, the universe will become more and more dominated by dark energy.  Our observable region will get sparser and sparser, as the dark energy pushes the galaxies further and further away from each other and from us, with more and more galaxies receding past the horizon where we could receive signals from them at the speed of light.  (Which means, in particular, that if you want to visit a galaxy a few billion light-years from here, you’d better start out while you still can!)</p>



<p>So here’s my question: is it just a coincidence that the time—right now—when the universe is “there for the taking,” potentially poised between competing spacefaring civilizations, is <em>also</em> the time when it’s poised between matter and dark energy?  Note that, in 2007, Bousso et al. tried to give a <a href="https://arxiv.org/abs/hep-th/0702115">sophisticated anthropic argument</a> for the value of the cosmological constant Λ, which measures the density of dark energy, and hence the eventual size of the observable universe.  <a href="https://www.scottaaronson.com/blog/?p=328">See here</a> for my blog post on what they did (“The array size of the universe”).  Long story short, for reasons that I explain in the post, it turns out to be essential to their anthropic explanation for Λ that civilizations flourish <em>only</em> (or mainly) in the present epoch, rather than trillions of years in the future.  If we had to count civilizations that far into the future, then the calculations would favor values of Λ much smaller than what we actually observe.  This, of course, seems to dovetail nicely with Robin’s account.</p>



<p>Let me end with some “practical” consequences of Robin’s scenario, supposing as usual that we take it seriously.  The most immediate consequence is that the prospects for <a href="https://en.wikipedia.org/wiki/Search_for_extraterrestrial_intelligence">SETI</a> are dimmer than you might’ve thought before you’d internalized all this.  (Even <em>after</em> having interalized it, I’d still like at least an order of magnitude more resources devoted to SETI than what our civilization currently spares.  Robin’s assumptions might be wrong!)</p>



<p>But a second consequence is that, if we want human-originated sentience to spread across the universe, then the sooner we get started the better!  Just like Bill Gates in 1975, we should expect that there will soon be competitors out there.  Indeed, there are likely competitors out there “already” (where “already” means, let’s say, in the rest frame of the cosmic microwave background)—it’s just that the light from them hasn’t yet reached us.  So if we want to determine our own cosmic destiny, rather than having post-singularity extraterrestrials determine it for us, then it’s way past time to get our act together as a species.  We might have only a few hundred million more years to do so.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> For more discussion of this post, see the <a href="https://www.reddit.com/r/slatestarcodex/comments/l8l4np/once_we_can_see_them_its_too_late/">SSC Reddit thread</a>.  I especially liked a <a href="https://www.reddit.com/r/slatestarcodex/comments/l8l4np/once_we_can_see_them_its_too_late/glel0lq/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3">beautiful comment by “Njordsier,”</a> which fills in some important context for the arguments in this post:</p>



<blockquote class="wp-block-quote"><p>Suppose you’re an alien anthropologist that sent a probe to Earth a million years ago, and that probe can send back one high-resolution image of the Earth every hundred years. You’d barely notice humans at first, though they’re there. Then, circa 10,000 years ago (99% of the way into the stream) you begin to see plots of land turned into farms. Houses, then cities, first in a few isolated places in river valleys, then exploding across five or six continents. Walls, roads, aqueducts, castles, fortresses. Four frames before the end of the stream, the collapse of the population on two of the continents as invaders from another continent bring disease. At T-minus three frames, a sudden appearance of farmland and cities on the coasts those continents. At T-minus two frames, half the continent. At the second to last frame, a roaring interconnected network of roads, cities, farms, including skyscrapers in the cities that were just trying villas three frames ago. And in the last frame, nearly 80 percent of all wilderness converted to some kind of artifice, and the sky is streaked with the trails of flying machines all over the world.</p><p>Civilizations rose and fell, cultures evolved and clashed, and great and terrible men and women performed awesome deeds. But what the alien anthropologist sees is a consistent, rapid, exponential explosion of a species bulldozing everything in its path.</p><p>That’s what we’re doing when we talk about the far future, or about hypothetical expansionist aliens, on long time scales. We’re zooming out past the level where you can reason about individuals or cultures, but see the strokes of much longer patterns that emerge from that messy, beautiful chaos that is civilization.</p></blockquote>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Jan. 31):</span></strong> Reading the reactions here, <a href="https://news.ycombinator.com/item?id=25972111">on Hacker News</a>, and elsewhere underscored for me that a lot of people get off Robin’s train well before it’s even left the station.  Such people think of extraterrestrial civilizations as things that you either <em>find</em> or, if you haven’t found one, you just <em>speculate</em> or <em>invent stories</em> about.  They’re not even in the category of things that you have any serious hope to <em>reason</em> about.  For myself, I’d simply observe that trying to reason about matters far beyond current human experience, based on the microscopic shreds of fact available to us (e.g., about the earth’s spatial and temporal position within the universe), has led to some of our species’ embarrassing failures but also to some of its greatest triumphs.  Since even the failures tend to be relatively cheap, I feel like we ought to be “venture capitalists” about such efforts to reason beyond our station, encouraging them collegially and mocking them only gently.</p></div>
    </content>
    <updated>2021-01-30T08:54:50Z</updated>
    <published>2021-01-30T08:54:50Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Metaphysical Spouting"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-02-03T18:27:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18041</id>
    <link href="https://rjlipton.wordpress.com/2021/01/29/alan-selman-1941-2021/" rel="alternate" type="text/html"/>
    <title>Alan Selman, 1941–2021</title>
    <summary>Keeping a promise to structure the field of complexity 2017 Who’s Who award Alan Selman, my longtime friend and colleague, passed away last Friday, from the one illness that is most cruel for someone who has excelled by brainpower and effervescent joie de vivre alike. Today, Dick and I join others in mourning and also […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Keeping a promise to structure the field of complexity</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/29/alan-selman-1941-2021/alanwhoswho/" rel="attachment wp-att-18043"><img alt="" class="aligncenter wp-image-18043" height="180" src="https://rjlipton.files.wordpress.com/2021/01/alanwhoswho.jpg?w=160&amp;h=180" width="160"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">2017 <em>Who’s Who <a href="https://www.24-7pressrelease.com/press-release/439185/dr-alan-louis-selman-named-a-lifetime-achiever-by-marquis-whos-who">award</a></em></font></td>
</tr>
</tbody>
</table>
<p>
Alan Selman, my longtime friend and colleague, passed away last Friday, from the one illness that is most cruel for someone who has excelled by brainpower and effervescent <em>joie de vivre</em> alike.</p>
<p>
Today, Dick and I join others in mourning and also in appreciation.</p>
<p>
I last saw Alan with his wife Sharon in May 2019 at the dinner for the <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">NP50</a> celebration of Steve Cook in Toronto. There were inklings of his affliction but nothing that kept him from engaging in dinner talk about how our field has progressed, about reunited friends, about the convivial atmosphere and culture on display during the celebration, and even my own goings-on with chess and quantum complexity and all our grown children. Alan was still going out to concerts last March until the pandemic stopped all that kind of activity. </p>
<p>
I’ve known Alan in person for almost 40 years, starting a decade before he became my Chair for 6 years and colleague for 18 years more of them before his retirement and move to New Jersey. This was all through my graduate study at Oxford. It began at ICALP 1982 in Aarhus after my first year. During a December 1983 workshop at Brooklyn College he shared a problem with me whose story I told <a href="https://rjlipton.wordpress.com/2012/07/14/it-dont-come-easy/">here</a>. He visited me in Oxford in early 1986 accompanied by his son Jeffrey, whose eulogy in this past Sunday’s burial <a href="https://www.facebook.com/162731740423576/videos/756739685268316">service</a> was especially moving. Alan gave me other helps in 1985–86 that aided my “repatriation,” so to speak. My point is that he took great interest in builders of complexity theory even as students—Uwe Schöning noted that he visited him in 1982 while he was a student as well. My 2014 <a href="https://rjlipton.wordpress.com/2014/05/13/nexp-and-the-next-generation/">post</a> on Alan’s retirement party expressed how this continued throughout. </p>
<p>
</p><p/><h2> P NE NP </h2><p/>
<p/><p>
As others have remarked, Alan’s cars had a license plate that declared <b>P NE NP</b>. Over a quarter century I regularly saw it in the parking lot serving Bell Hall, and after 2012, the same lot for our department’s new digs in Davis Hall. My colleague Sargur Srihari relates seeing someone approach Alan in the lot and say, “Yes, but can you prove it?”—adding that it was “probably the only thing in complexity that Alan couldn’t do.” An irony I will say here now is that the Dean of Engineering and Applied Sciences whom Alan worked with during much of his time as Chair, Mark Karwan, <a href="https://arxiv.org/abs/1610.00353">claims</a> the <a href="https://arxiv.org/abs/1902.03549">opposite</a>. It would have been fun to see a dueling license plate saying <b>P EQ NP</b>.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/29/alan-selman-1941-2021/license_20210126153121_34106/" rel="attachment wp-att-18044"><img alt="" class="aligncenter size-full wp-image-18044" src="https://rjlipton.files.wordpress.com/2021/01/license_20210126153121_34106.jpg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Synthesized via <a href="https://www.acme.com/licensemaker/licensemaker.cgi?state=New+York&amp;text=P+NE+NP&amp;plate=1986&amp;r=53091344&amp;">ACME</a> license-plate composing app</font>
</td>
</tr>
</tbody></table>
<p>
Long followers of this blog know that Dick and I have embodied this <a href="https://rjlipton.wordpress.com/2020/01/12/our-thoughts-on-pnp/">duel</a> between ourselves. Only recently has Dick <a href="https://rjlipton.wordpress.com/2017/12/08/pnp-perhaps-i-change-my-mind/">swung</a> toward <a href="https://rjlipton.wordpress.com/2020/06/16/pnp/">P <img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{&lt;}"/> NP</a>, while <a href="https://emanueleviola.wordpress.com/2018/02/16/i-believe-pnp/">some</a> have <a href="https://www.informit.com/articles/article.aspx?p=2213858">moved</a> the other <a href="https://www.cs.rice.edu/~vardi/comp409/pvnp10.pdf">way</a>. </p>
<p>
But what we think everyone would agree on is that we cannot point to any definite progress in the past dozen-plus years, at least not directly on the question. This struck me particularly from the Cook workshop, and this would have been the lead theme of a post on it, had I not been sprinting toward an effective June deadline to complete a major upgrade to my statistical chess model which I <a href="https://rjlipton.wordpress.com/2019/08/15/predicting-chess-and-horses/">achieved</a> two months later. No one attested real progress in comments to our P=NP status <a href="https://rjlipton.wordpress.com/2020/01/12/our-thoughts-on-pnp/">post</a> a year ago. I wrote to Sharon two weeks ago that I wished I could tell Alan of progress but no.</p>
<p>
Where Alan, Dick, I, and many of us are completely aligned is on these two points:</p>
<ol>
<li>
The importance of unwavering focus on P versus NP and other fundamental questions about computational problems, despite our field’s lack of scientific achievement in classifying them. <p/>
</li><li>
Emphasis on what our field has positively, scientifically, and perhaps surprisingly achieved, which is showing deep and crisp mathematical relations among thousands of diverse problems. These relations can be phrased not only in the common term <em>reductions</em> but in terms that arose in structural complexity, such as <a href="https://en.wikipedia.org/wiki/Berman-Hartmanis_conjecture">isomorphisms</a>, <a href="https://en.wikipedia.org/wiki/Truth-table_reduction">(non-)adaptivity</a>, <a href="https://en.wikipedia.org/wiki/Parsimonious_reduction">parsinomiousness</a>, and <a href="http://people.cs.uchicago.edu/~fortnow/papers/insep.pdf">(in-)separability</a>.
</li></ol>
<p>
We put in the mouth of a fictional <a href="https://rjlipton.wordpress.com/2014/05/23/stoc-1500/">STOC 1500</a> keynote the position that relations between problems should be the field’s fundamental objects. There is incredible beauty in these objects, as was the theme of Lane Hemaspaandra’s <a href="https://arxiv.org/pdf/1406.4106.pdf">tribute</a> to Alan on his retirement surveying gems of Alan’s work, and it was my elation to be captivated by them beginning in my graduate years of the 1980s.</p>
<p>
</p><p/><h2> Elijah </h2><p/>
<p/><p>
Alan’s Hebrew name was <em>Eliyahu</em>, Elijah. In the Bible, Elijah is a traveling prophet: he not only goes from <a href="https://en.wikipedia.org/wiki/Sarepta">Zarephath</a> to <a href="https://en.wikipedia.org/wiki/Beersheba">Beersheba</a> but all the way south to Mount Sinai, where he is commanded to return all the way north to Damascus. The mission from Sinai is conveyed not in cloud and fire as with Moses, nor in wind or earthquake, but <a href="https://www.biblegateway.com/passage/?search=1+Kings+19:9-16&amp;version=KJV">famously</a> in a still small voice. </p>
<p>
That was the kind of voice in which Alan during his many travels spread the love of complexity theory. I was glad to experience his kindness, good crisp advice, and also the giving of freedom to pursue things with zeal—a trait that Elijah had and recognized. I was not so fast to recognize good food and fine culture. I remember once when Alan was especially happy about securing tickets to see the 86-year-old legendary jazz violinist Stéphane Grappelli perform in Buffalo and I did not know who that was. I was, however, able to reply that I’d once seen Chet Baker live while standing in a small room in London. He and Sharon and my wife Debbie and I shared other musical interests. All of what I implied about whole-person education in my <a href="https://rjlipton.wordpress.com/2021/01/01/peter-m-neumann-1940-2020/">memorial</a> for Peter Neumann applies to Alan. In Bell Hall we had only a mid-size seminar room for gatherings, yet Alan right away recognized the importance of making it like Oxford’s Mathematical Institute tea room—but with wine included—in weekly Friday afternoon “TGIF” gatherings open to faculty and graduate students, for which I did the shopping those mornings at Wegmans.</p>
<p>
Elijah is also known as a keeper of promises. I won’t go into the gruesome biblical details here, but rather note that Alan shepherded a promise of a different kind: the notion of a <a href="https://en.wikipedia.org/wiki/Promise_problem">promise problem</a>, following on from his famous 1984 <a href="https://www.sciencedirect.com/science/article/pii/S001999588480056X">paper</a> with Shimon Even and Yacov Yacobi. This is a structural complexity concept whose usage has grown, besides the original technical challenges which I recounted <a href="https://rjlipton.wordpress.com/2012/07/14/it-dont-come-easy/">here</a>. I’ve sometimes felt that wider uses have run risk of getting burned by inexactitudes, such as in the notion of promise-completeness.  Alan’s exactness was one trait that made me feel at home.</p>
<p>
Elijah also founds a school <a href="https://www.biblegateway.com/passage/?search=2+Kings+2:1-9&amp;version=KJV">called</a> the “sons of the prophets,” passing his mantle to Elisha before being taken up in a chariot of fire. It is not for me to say who might pick up Alan’s mantle in structural complexity. But many in his train have said wonderful things in tribute over the weekend, and I wish to pass along some of them, with quotes from those who have given permission or have already written in public, in particular as comments to Lance Fortnow’s <a href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html">tribute</a> to Alan.</p>
<p>
</p><p/><h2> Some Tributes From Colleagues </h2><p/>
<p/><p>
The news was passed around our department in Buffalo. Our Chair, <b>Chunming Qiao</b>, mourned the loss of a great colleague, a former Chair, and a respected scholar. <b>Sargur Srihari</b> told the parking lot story above and and noted how Alan recruited a star to the department in the person of Jin-Yi Cai. <b>Stuart Shapiro</b>, who preceded and succeeded Alan as Chair, noted Alan’s higher personal standards for research and publishing. <b>Russ Miller</b>, writing earlier this month when we had news of Alan’s being in hospice care, hailed him as “a terrific person,” meeting the standard of a “true gentleman and scholar,” and hailed his communication skills with colleagues and the university administration alike. <b>Jinhui Xu</b> expressed thanks for Alan’s thoughtfulness and role as a mentor. I can say all this from my own experience. Many of us also took part in the earlier outpouring of affection, including photos and memories of Alan’s time in Buffalo, that was communicated to Sharon. </p>
<p>
<b>Mark Karwan</b> adds, “Alan continued to win awards and accolades throughout my 12 years as Dean which ended in 2006. All of his professional accomplishments provided a great sense of pride and international recognition for UB. Alan was truly a senior leader and mentor for so many. He spoke to me as an advocate for his department and for the young rising faculty and the need to nourish and retain them. His wise counsel was always given in the most gentlemanly manner. When I picture Alan today, I see the twinkle in his eyes and his contagious joy of life. I only started working relatively recently with a colleague in our field of Operations Research on matters in complexity with implications to support P = NP. It would have been such fun to bring Alan into our conversations! We will all miss Alan and will always appreciate the great legacy he created here at UB.” </p>
<p>
<b>Jin-Yi</b>, of course, is well known to all who knew Alan in the theory community and readers of this blog. He called Alan a great structural complexity theorist, and wrote in one of many e-mails I’ve been copied on: “Alan [was] a wonderful colleague of mine, and had a lifetime contribution to complexity theory that will certainly live on. He had a dry wit, enigmatic smile, and most of all, a truly kind heart. We will always remember him in our fond memory.”</p>
<p>
<b>Atri Rudra</b> worked with Alan on many projects including organizing and securing funding for the Eastern Great Lakes (EaGL) Theory Workshops along with Bobby Kleinberg of Cornell. Here are two photos from the first one in 2008:</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/29/alan-selman-1941-2021/cookalaneagl2008/" rel="attachment wp-att-18046"><img alt="" class="aligncenter wp-image-18046" height="270" src="https://rjlipton.files.wordpress.com/2021/01/cookalaneagl2008.jpg?w=400&amp;h=270" width="400"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite from 2008 EaGL photo <a href="https://cse.buffalo.edu/events/theory/images.html">page</a>; Kleinberg to Alan’s right and Atri (obscured) behind.</font>
</td>
</tr>
</tbody></table>
<p>
Atri writes, “I still remember being taken directly from the airport to Alan’s favorite restaurant, Trattoria Aroma, during my interview at Buffalo. I was a bit intimidated by Alan during the dinner but we bonded over the fact that we were both married to epidemiologists. After I joined Buffalo, Alan’s sage advice helped me throughout my tenure process. Alan was a giant in the department and having him in my corner did not hurt. [H]e had a wicked sense of humor as well.”</p>
<p>
<b>Mitsunori Ogihara</b> succeeded Alan as Editor-in-Chief of the <a href="https://www.springer.com/journal/224">journal</a> <em>Theory of Computer Systems</em> and blossomed from being a postdoc recruited by Alan at UB to being Chair of Computer Science at the University of Rochester in a few short years. The story of my taking Mitsu from the airport on a snowy night will be told another time. He is writing his own tribute to appear in the journal, from which I quote just a few words: “While being a scholar of uncanny vision and incredible ingenuity, Alan was a lovely human being. He was generous with his time for his students and colleagues, always willing to offer consultation and advice. The field of theoretical computer science has lost its giant. Alan is no longer here to lead us, but his legacy will live.”</p>
<p>
<b>Lane Hemaspaandra</b>, Mitsu’s colleague at UR, writes: “Very briefly put, Alan’s research handiwork and vision is ingrained in the shape of the field. I was lucky enough to through his generosity know Alan across decades: we co-edited a book, wrote two research papers together, even wrote—Alan had a real love of writing and language—a note on writing in theoretical computer science, and had a long tradition of research seminars and theory days that brought together the University at Buffalo, RIT, and University of Rochester theory groups. Alan was also my wife’s wonderful postdoctoral advisor. And, throughout all that, and coexisting with Alan’s technical artistry and amazing taste in theory research, Alan’s love of and expertise in the ‘real’ world was quiet yet luminous. Alan loved the theater, and spoke warmly of his beloved Shaw Festival at Niagara-on-the-Lake. He loved and knew food; any restaurant commended by Alan was going to be an experience.”</p>
<p>
<b>Ashish Naik</b> <a href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html?showComment=1611460983996#c8100240511053878034">noted</a> that he was Alan’s first student in Buffalo: “[I] still remember going to his office to ask if he would take me as a student, somewhat intimidated because of his stature in the field. Alan he put me at ease immediately with his sense of humor (I still remember the cartoon clippings on his office door) and his generosity. Despite a busy schedule as the Chair, Alan always had time for me — introducing me to the field, sharing interesting problems and his ideas, and teaching me how to write well and asking the right questions, skills that have stayed with me forever.”</p>
<p>
<b>D. Sivakumar</b> was my student at the same time as Ashish, and began his own <a href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html?showComment=1611529242421#c422919304502293363">comment</a> in Lance’s tribute by noting how Alan’s early work with Neil Jones on a logical characterization of nondeterministic exponential time was a precursor to Ron Fagin’s 1974 <a href="https://researcher.watson.ibm.com/researcher/files/us-fagin/genspec.pdf">characterization</a> of NP. He continued how at the end of his first year in Buffalo, “Alan encouraged us students to attend the Structures conference in Boston even though only one of us a theory-committed student. Alan drummed up some money to pay our registration, and we rented a car and stayed with friends in Boston. Structures’92 was magical for me—my first academic conference—everything about the atmosphere, the ideas, the people was fascinating and I decided to work in that field. All the work in my thesis (sparse sets, measure theory,…) were on topics I heard about for the first time at Structures’92—without Alan’s nudge, my life would likely have taken an entirely different trajectory. … Alan’s wit, dry humor, and his generosity are the things I’ll remember forever.”</p>
<p>
<b>Pavan Aduri</b> was also Alan’s student in the 1990s and, speaking during a <em>shiv’ah</em> service with the family on Monday (via Zoom), noted Alan’s quest for <em>simplicity</em> in complexity theory. “Whenever I started to present a proof, Alan would ask, ‘Can you simplify the proof?’ ” Now at Iowa State, he recalled that he would frequently reach out to Alan for advice on various matters, even on what would be a more appropriate title for a research proposal. “Alan was always very generous with his time and advice.” <b>Frederic Green</b> related at the <em>shiv’ah</em> how Alan helped him migrate from physics to complexity and once reviewed every single step of four pages of notes on a complicated proof to optimize it for presentation. <b>Steve Fenner</b> told of Alan’s hosting him in Buffalo right after Steve’s PhD and putting him at ease with small talk about the permanently temporary quarters in Bell Hall. <b>Steve Homer</b> and his wife told of being travel partners exploring castles and flower markets, and the long gestation of the Homer-Selman <a href="https://www.springer.com/gp/book/9781461406815">textbook</a>, <em>Computability and Complexity Theory</em>.</p>
<p>
<b>Harry Buhrman</b> writes: “Alan has been an inspiration for me from the time I started working as a PhD student in complexity theory. My first encounter was his beautiful work on reductions and P-selective sets. I jokingly used to call them P-Selman-sets. I consider myself very lucky that he came over to my PhD defense in 1993 in Amsterdam. It was a joyous gathering.” I remember the photos Alan showed of that defence as well as Edith Hemaspaandra’s: grand pageantry on a scale I did not see even at Oxford, and led by Harry’s co-advisor <b>Peter van Emde Boas</b>, whose <a href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html?showComment=1611483268043#c1171975184441035945">note</a> in Lance’s tribute recalled the genesis of the “Structures” conferences in 1985–86 and the 1994 edition in Amsterdam and both hosting and visiting Alan and Sharon for dinners.</p>
<p>
My retired colleague <b>Mike Buckley</b> <a href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html?showComment=1611419108397#c7225101539941877122">wrote</a>, “I taught with Alan at UB, and he taught me how to teach. Students sought him out until the day he left. He put as much into the undergrads as the PhDs. His presence at faculty meetings meant that no decision was made until he had his last comment. He was a giant in the field, but also the world’s nicest man. I kept in touch and we talked stereos and jazz. I miss him already.”</p>
<p>
<b>Arnold Rosenberg</b> and <b>Paul Spirakis</b> hailed Alan as “a true pioneer and leader in the area of logic-based CS” and noted their long cooperation on the editorial board of the <em>Theory of Computing Systems</em> journal. Arnold continued: “He played vital roles in both research and education in CS. He approached every professional challenge with vision and unswerving devotion to the highest standards of our field. Our journal and our field have lost a great person.” <b>Rod Downey</b> included special thanks for Alan’s early support of parameterized complexity and the journal’s support, for instance in special issues.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Readers are welcome to place words of grace in comments; more may be added above as well. </p>
<p>
Our embraces go to Sharon, to Jeffrey, to their daughter Heather—who also works in medicine—and to all the family. </p>
<p>
[Cantor for the service was not Heather Wargo but Sandra Messinger Aguilar; some word fixes; linked license-plate app]</p></font></font></div>
    </content>
    <updated>2021-01-29T22:27:36Z</updated>
    <published>2021-01-29T22:27:36Z</published>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Teaching"/>
    <category term="Alan Selman"/>
    <category term="community"/>
    <category term="mathematical beauty"/>
    <category term="memorial"/>
    <category term="P&#x2260;NP"/>
    <category term="promise problem"/>
    <category term="structural complexity"/>
    <category term="University at Buffalo"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-02-09T09:20:41Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-8353632136896364991</id>
    <link href="http://processalgebra.blogspot.com/feeds/8353632136896364991/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=8353632136896364991" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/8353632136896364991" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/8353632136896364991" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/01/one-phd-and-one-postdoc-position-at.html" rel="alternate" type="text/html"/>
    <title>One PhD and one postdoc position at Reykjavik University</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div dir="ltr"><div><div style="text-align: center;"><b>Mode(l)s of Verification and Monitorability</b></div><div style="text-align: center;"><br/></div><b><div style="text-align: center;"><b>Department of Computer Science, Reykjavik University</b></div></b><div style="text-align: center;"><br/></div><b><div style="text-align: center;"><b>One PhD and one postdoc position</b></div></b><br/><br/>We  invite applications for a total of two positions: one PhD position and  one postdoc position, at the Department of Computer Science of Reykjavik  University.<br/><br/>The position is part of a research project funded by  the Icelandic Research Fund, under the direction of Antonis Achilleos  (Reykjavik University), Luca Aceto (Reykjavik University), and Anna  Ingolfsdottir (Reykjavik University) in cooperation with Adrian  Francalanza (University of Malta) and Karoliina Lehtinen (LIS,  Aix-Marseille).<br/><br/>The project continues previous work in the  theoretical foundations of runtime verification and its overarching goal  is to better understand the properties and push the limits of  monitorability in different settings. For more information on the  project, please visit <br/><br/><a href="https://sites.google.com/view/antonisachilleos/movemnt" target="_blank">https://sites.google.com/view/antonisachilleos/movemnt</a> <br/><br/>or contact Antonis Achilleos (email: <a href="mailto:antonios@ru.is" target="_blank">antonios@ru.is</a>)<br/><br/>The successful candidates will benefit from, and contribute to, the<br/>research environment at the Icelandic Centre of Excellence in<br/>Theoretical Computer Science (ICE-TCS), with research groups on<br/>concurrency, logic and semantics, algorithms, combinatorics. For<br/>information about ICE-TCS and its activities, see<br/><br/><a href="http://icetcs.ru.is/" target="_blank">http://icetcs.ru.is/</a> .<br/><br/>Moreover, they will cooperate with Adrian Francalanza and Karoliina </div><div>Lehtinen during the project work and will benefit from the interaction </div><div>with their research groups at the University of Malta and LIS, Aix-Marseille.<br/><br/></div><blockquote style="border: medium none; margin: 0px 0px 0px 40px; padding: 0px;"><div><i>*Qualification requirements*</i></div></blockquote><div><br/>Applicants for the postdoctoral position should have, or be about to<br/>defend, a PhD degree in computer science or a closely related<br/>field. Moreover, previous knowledge in logic, concurrency theory, or any </div><div>of the project's related areas, and mathematical competence are desirable.<br/><br/>Applicants for the PhD fellowship should have, or be about to obtain, an </div><div>MSc degree in Computer Science, or closely related fields. Some </div><div>background in logic, concurrency theory, or some other of the project's </div><div>related areas, and mathematical competence are desirable.<br/><br/></div><blockquote style="border: medium none; margin: 0px 0px 0px 40px; padding: 0px;"><div><i>*Remuneration*</i></div></blockquote><div><br/>The PhD position provides a stipend of 383,000 ISK per month before </div><div>taxes, and the salary for the postdoc position is 460,000 ISK per month </div><div>before taxes.<br/><br/></div><blockquote style="border: medium none; margin: 0px 0px 0px 40px; padding: 0px;"><div><i>*Start date and duration*</i></div></blockquote><div><br/>The PhD position is for three years, to start as soon as possible.<br/><br/>The  postdoc position is for one year, to start in July or August 2021 (the  start date is negotiable), and can be renewed for one more year, based  on mutual agreement.<br/><br/></div><blockquote style="border: medium none; margin: 0px 0px 0px 40px; padding: 0px;"><div><i>*Application details*</i></div></blockquote><div><br/>Interested  applicants should send their CV, including a list of publications, in  PDF to all the addresses below, together with a statement outlining  their suitability for the project and the names of at least two  referees.<br/><br/>Antonis Achilleos<br/>email: <a href="mailto:antonios@ru.is" target="_blank">antonios@ru.is</a><br/><br/>Luca Aceto<br/>email: <a href="mailto:luca@ru.is" target="_blank">luca@ru.is</a><br/><br/>Anna Ingolfsdottir<br/>email: <a href="mailto:annai@ru.is" target="_blank">annai@ru.is</a><br/><br/>Adrian Francalanza<br/>email: <a href="mailto:adrian.francalanza@um.edu.mt" target="_blank">adrian.francalanza@um.edu.mt</a><br/><br/>Karoliina Lehtinen<br/>email: <a href="mailto:lehtinen@lis-lab.fr" target="_blank">lehtinen@lis-lab.fr</a><br/><br/>Informal inquiries about the project and the conditions of work are very welcome.<br/><br/>We  will start reviewing applications as soon as they arrive and will  continue to accept applications until each position is filled. We  strongly encourage interested applicants to send their applications as  soon as possible and no later than 20 February 2021.</div></div></div>
    </content>
    <updated>2021-01-29T18:25:00Z</updated>
    <published>2021-01-29T18:25:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-02-08T17:42:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18030</id>
    <link href="https://rjlipton.wordpress.com/2021/01/29/happy-un-birthday-rich/" rel="alternate" type="text/html"/>
    <title>Happy Un-Birthday Rich</title>
    <summary>Still going strong Richard DeMillo just turned 74 years old the other day. Happy Birthday Rich, and many more. Today I want to wish him also a happy un-birthday. Recall an un-birthday is celebrated on any day that is not your birthday. It was created by Lewis Carroll in his 1871 novel Through the Looking-Glass. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Still going strong</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p>
Richard DeMillo just turned 74 years old the other day. Happy Birthday Rich, and many more. </p>
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/29/happy-un-birthday-rich/rich/" rel="attachment wp-att-18032"><img alt="" class="alignright wp-image-18032" src="https://rjlipton.files.wordpress.com/2021/01/rich.jpg?w=200" width="200"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Today I want to wish him also a happy <a href="https://en.wikipedia.org/wiki/Unbirthday">un-birthday</a>.</p>
<p>
Recall an un-birthday is celebrated on any day that is not your birthday. It was created by Lewis Carroll in his 1871 novel Through the Looking-Glass. </p>
<p>I aways liked the concept of un-birthdays. Probably only a mathematician like Carroll could think of this—every set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{S}"/> has a complement <img alt="{\bar{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbar%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bar{S}}"/>.</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/29/happy-un-birthday-rich/un-2/" rel="attachment wp-att-18033"><img alt="" class="aligncenter  wp-image-18033" src="https://rjlipton.files.wordpress.com/2021/01/un.jpg?w=200" width="200"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p/><h2> Quest for Correctness </h2><p/>
<p/><p>
Rich and I started our work together over four decades ago. A central theme of our work was <i>correctness</i>. We were concerned That programs might not work as planned. At the time it was not obvious that this was a major theme of our joint work. But looking back now I can see that it was. </p>
<p>
We wrote several papers on correctness, from various angles.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> Our work on practical approaches to program testing. The main paper was <a href="https://ieeexplore.ieee.org/document/1646911">Hints on test data selection: Help for the practicing programmer</a> by Rich DeMillo, Fred Sayward in 1978. This was part of our work on the mutation program testing <a href="https://en.wikipedia.org/wiki/Mutation_testing">method</a>. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> Our <a href="https://link.springer.com/chapter/10.1007/3-540-69053-0_4">On the importance of checking cryptographic protocols for faults</a> by Dan Boneh, Rich DeMillo in 1997. This showed that incorrect crypto systems could be attacked much easier than correct ones. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> Our <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">Social processes and proofs of theorems and programs</a> by Rich DeMillo, Alan Perlis. This was paper argued against verification technology as the main way to make programs correct. Was attacked as wrong then, and probably still viewed as wrong by many. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> Our <a href="https://apps.dtic.mil/sti/pdfs/ADA050745.pdf">Probabilistic Remark on Algebraic Program Testing</a> 1977 By Rich DeMillo. And in <a href="https://www.sciencedirect.com/science/article/abs/pii/0020019078900674">Information Processing Letters</a>. See how we <a href="https://rjlipton.wordpress.com/2009/11/30/the-curious-history-of-the-schwartz-zippel-lemma/">missed the boat</a> on this important result. </p>
<p>
</p><p/><h2> Other Quests </h2><p/>
<p/><p>
Rich has done much besides his research, that covered much more than just the above papers. He has had a long successful career that has two paths. He was a leader in industry and in government. At Hewlett-Packard Company he served as the company’s first Chief Technology Officer. He also held executive positions with Telcordia Technologies (formerly known as Bell Communications Research) and the National Science Foundation.</p>
<p>
He was and still is a leader in academia. He is currently Distinguished Professor of Computing and Professor of Management at the Georgia Institute of Technology. He was the John P. Imlay Dean of Computing at Georgia Tech for six years. He is now the Chair of the new School of Cybersecurity and Privacy in the College of Computing.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
See <a href="https://rjlipton.wordpress.com/2009/02/18/insider-baseball-and-pnp/">this</a> for some earlier remarks on Rich. </p>
<p/></font></font></div>
    </content>
    <updated>2021-01-29T15:54:42Z</updated>
    <published>2021-01-29T15:54:42Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="correctness"/>
    <category term="DeMillo"/>
    <category term="Rich"/>
    <category term="unbirthday"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-02-09T09:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20946</id>
    <link href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/" rel="alternate" type="text/html"/>
    <title>Possible future Polymath projects (2009, 2021)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">What will be our next polymath project? A polymath project (Wikipedia) is a collaboration among mathematicians to solve important and difficult mathematical problems by coordinating many mathematicians to communicate with each other on finding the best route to the solution. … <a href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2021/01/polymath.png"><img alt="" class="alignnone size-full wp-image-21120" height="269" src="https://gilkalai.files.wordpress.com/2021/01/polymath.png" width="640"/></a></p>
<p style="text-align: center;"><strong><span style="color: #ff0000;">What will be our next polymath project?</span></strong></p>
<p>A <a href="https://en.wikipedia.org/wiki/Polymath_Project">polymath project</a> (<a href="https://en.wikipedia.org/wiki/Polymath_Project">Wikipedia</a>) is a collaboration among mathematicians to solve important and difficult mathematical problems by coordinating many mathematicians to communicate with each other on finding the best route to the solution. The project began in January 2009 on Timothy Gowers’s blog when he posted a problem and asked his readers to post partial ideas and partial progress toward a solution. This experiment resulted in a new answer to a difficult problem, and since then the Polymath Project has grown to describe a particular process of using an online collaboration to solve any math problem.</p>



<p>After the success of Polymath1 and the launching of Polymath3 and Polymath4, Tim Gowers wrote a blog post <a href="https://gowers.wordpress.com/2009/09/16/possible-future-polymath-projects">“Possible future Polymath projects”</a> for planning the next polymath project on his blog. The post mentioned 9 possible projects. (Three of them later turned  to polymath projects, and one turned into a project of a different nature.) Following the post and separate posts describing some of the proposed projects, a few polls were taken and a problem – the Erdős discrepancy problem, was selected for the next project polymath5. </p>
<p>One of our next posts will have the same title and similar purpose as Tim’s 2009 post.  I will describe several possibilities for my next polymath project. <span style="color: #ff0000;"><strong>(A quick rather vague and tentative preview can be found at the end of this post.</strong></span>) Today we go back to Tim’s 2009 post and the problems posed there.</p>
<p><strong><span style="color: #0000ff;"> Comments on the 2009 proposed projects, the new proposed projects, other proposed projects, and on the polymath endeavor, are most welcome. (At the end of the post I also mention a few “meta” questions.)</span></strong></p>
<p>Let me also mention <a href="https://polytcs.wordpress.com/" rel="home">The PolyTCS Project</a> aimed for proposing projects in theoretical computer science. There are so far three very interesting proposals there, and the first proposal is the Friedgut-Kalai <a href="https://terrytao.wordpress.com/2007/08/16/gil-kalai-the-entropyinfluence-conjecture/">Entropy/Influence conjecture</a>. For various proposals, see also <a href="http://polymathprojects.org/">the polymath blog</a> administered by Tim Gowers, Michael Nielsen, Terry Tao, and me, and <a href="https://mathoverflow.net/questions/219638/proposals-for-polymath-projects">this MO question</a>.</p>
<h2>The proposed projects in Gowers’s 2009 post and updates regarding these projects.</h2>
<p><strong>1. Littlewood’s conjecture and related problems.</strong></p>
<p>The conjecture states that if <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\alpha"/> and <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\beta"/> are any two real numbers, and <img alt="\epsilon&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" style="padding: 0; border: none; vertical-align: middle; color: #333333; font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 12.6px; font-style: normal; font-weight: 400; letter-spacing: normal; text-align: justify; text-indent: 0; white-space: normal;" title="\epsilon&gt;0"/>, then there exists a positive integer <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="n"/> such that <img alt="||\alpha n||\,||\beta n||\leq\epsilon/n" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%7C%5Calpha+n%7C%7C%5C%2C%7C%7C%5Cbeta+n%7C%7C%5Cleq%5Cepsilon%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="||\alpha n||\,||\beta n||\leq\epsilon/n"/>. Famously, Einsiedler, Katok and Lindenstrauss proved that the Hausdorff dimension of the set of counterexamples to the conjecture is zero. Gowers had ideas for an elementary approach, and his ideas  <a href="https://gowers.wordpress.com/2009/11/17/problems-related-to-littlewoods-conjecture-2/">are described in this later post.</a> This project was not launched and I am also not aware of progress related to the problem (but I am not an expert). </p>
<p><strong>2. A DHJ-related project.</strong></p>
<p>DHJ stands for “density Hales Jewett” which was the topic of polymath1. The second proposed project was to build on the success of polymath1 and at<a href="https://gowers.wordpress.com/2009/11/14/the-first-unknown-case-of-polynomial-dhj/"> a later post</a> the following problem was proposed.</p>
<p><strong>Conjecture:  </strong><em>For every <img alt="\delta&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" style="padding: 0; border: none; vertical-align: middle;" title="\delta&gt;0"/> and every positive integer <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k"/> there exists <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="n"/> such that if <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="A"/> is any subset of <img alt="[k]^{[n]^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bk%5D%5E%7B%5Bn%5D%5E2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="[k]^{[n]^2}"/> of density at least <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\delta"/>, then <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="A"/> contains a <a href="http://michaelnielsen.org/polymath1/index.php?title=Combinatorial_line">combinatorial line</a> such that the wildcard set is of the form <img alt="X\times X" class="latex" src="https://s0.wp.com/latex.php?latex=X%5Ctimes+X&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="X\times X"/> for some subset <img alt="X\subset[n]" class="latex" src="https://s0.wp.com/latex.php?latex=X%5Csubset%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="X\subset[n]"/>.</em></p>
<p>As far as I know, this conjecture is still open.</p>
<p>Both questions “what kind of forbidden patters in <img alt="[k]^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bk%5D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="[k]^n"/> force exponentially small density” and “what kind of forbidden patters in <img alt="[k]^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bk%5D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="[k]^n"/> force vanishing density” are fascinating. Let me recommend again the <a href="https://gilkalai.wordpress.com/2009/05/18/the-cap-set-problem-and-frankl-rodl-theorem-c/">Frankl-Rodl theorem</a> and its proof as a role model.</p>
<p><strong>3. Four Erdős-style combinatorial problems.</strong></p>
<p><strong>3a. Erdős’s discrepancy problem</strong></p>
<p>This was the problem that was eventually chosen for <strong>polymath5</strong>. This was a very nice story. The problem was presented<a href="https://gowers.wordpress.com/2009/12/17/erdoss-discrepancy-problem/"> in this blog post</a> and selected as polymath5 after some polls among readers. The <a href="https://gowers.wordpress.com/category/polymath5/">polymath project</a> was the longest in polymath history. There were six preliminary discussion posts with more than 600 comments followed by 21 official posts EDP1-EDP21. There was some short revival of the project (EDP22-EDP27) in 2012 where I contributed three posts. Famously, in 2015 Terry Tao solved the problem. The paper <a href="https://discreteanalysisjournal.com/article/609">appeared in the journal Discrete Analysis</a>. Tao’s solution relies on some insights from the polymath project including a crucial reduction that Terry himself contributed. It also crucially relied on a (then) new theory by Kaisa Matomaki and Maksym Radziwill. The solution was triggered  by a blog comment by Uwe Stroinski who pointed to a possible connection to EDP, and a subsequent one by Kodlu who seconded Uwe’s suggestion. This is reported in <a href="https://gowers.wordpress.com/2015/09/20/edp28-problem-solved-by-terence-tao/">EDP28</a>, and here on my blog we celebrated the solution in <a href="https://gilkalai.wordpress.com/2015/10/22/edp-reflections-and-celebrations/">this post</a>.  </p>
<p><strong>3b. The Erdős-Hajnal conjecture.</strong></p>
<p>Let <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k"/> be a positive integer and let <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="H"/> be a graph. Erdös and Hajnal conjectured that there is a constant <img alt="C=C(H)" class="latex" src="https://s0.wp.com/latex.php?latex=C%3DC%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="C=C(H)"/> such that if <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="G"/> is any graph with at least <img alt="k^C" class="latex" src="https://s0.wp.com/latex.php?latex=k%5EC&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k^C"/> vertices that does not contain any induced copy of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="H"/>, then either <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="G"/> or <img alt="G^c" class="latex" src="https://s0.wp.com/latex.php?latex=G%5Ec&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="G^c"/> contain a clique of size <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k"/>.</p>
<p>Tim asserted that the simplest open case is where <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="H"/> is a pentagon. This special case was recently settled by Maria Chudnovsky<span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql rrkovp55 a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id hzawbc8m" dir="auto">, </span>Alex Scott<span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql rrkovp55 a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id hzawbc8m" dir="auto">, Paul Seymour and Sophie Spirkl. They rely on recent results by </span>Janos Pach and Istvan Tomon. See <a href="https://youtu.be/nz-fnYvsuN8">this videotaped lecture</a> by Paul Seymour at IBS Discrete Mathematics Group, South Korea.</p>
<p><strong>3c. Frankl’s union-closed conjecture.</strong></p>
<p><span id="more-20946"/>Let <img alt="\mathcal{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BA%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\mathcal{A}"/> be a collection of sets that is closed under taking unions. Must there be an element that is contained in at least half the sets?</p>
<p>Tim wrote:<em> This is a notorious question, and possibly the least likely to yield to a Polymath approach (it feels as though there might be a burst of ideas, none of which would work, followed by disillusionment, or else, if we were very lucky, a single bright idea from one person that essentially solved the problem, but I could be wrong).</em></p>
<p><a href="https://gowers.wordpress.com/category/polymath11/"><strong>Polymath11</strong></a> on Tim Gowers’s blog (Launched January, 2016) was devoted to Frankl’s conjecture. The problem is still open.</p>
<p><strong>3d. The delta-systems problem.</strong></p>
<p>This question is due to Erdős and Rado. A <em>delta system</em> is a collection of sets <img alt="A_1,\dots,A_m" class="latex" src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cdots%2CA_m&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="A_1,\dots,A_m"/> such that all the sets <img alt="A_i\cap A_j" class="latex" src="https://s0.wp.com/latex.php?latex=A_i%5Ccap+A_j&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="A_i\cap A_j"/> (with <img alt="i\ne j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cne+j&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="i\ne j"/>) are equal. Equivalently, there exists some set <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="X"/> such that <img alt="X\subset A_i" class="latex" src="https://s0.wp.com/latex.php?latex=X%5Csubset+A_i&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="X\subset A_i"/> for every <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="i"/>, and the sets <img alt="A_i\setminus X" class="latex" src="https://s0.wp.com/latex.php?latex=A_i%5Csetminus+X&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="A_i\setminus X"/> are disjoint.</p>
<p>Erdös offered 1000 dollars for a solution to the following problem: does there exist a constant <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="C"/> such that for every <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k"/> and every system of at least <img alt="C^k" class="latex" src="https://s0.wp.com/latex.php?latex=C%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="C^k"/> sets of size <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="k"/>, there must exist three of them that form a delta system?</p>
<p>I devoted <a href="https://gilkalai.wordpress.com/category/polymath10/"><strong>polymath10</strong></a> to this problem. I tried to promote certain  homological approach. This has not led to progress but did lead to some interesting observations on the problem and also some refinement and better understanding of my approach.</p>
<p>While still open since 2009, there were major breakthroughs regarding the problem that we described <a href="https://gilkalai.wordpress.com/2019/08/23/amazing-ryan-alweiss-shachar-lovett-kewen-wu-jiapeng-zhang-made-dramatic-progress-on-the-sunflower-conjecture/">here</a> and <a href="https://gilkalai.wordpress.com/2016/05/17/polymath-10-emergency-post-5-the-erdos-szemeredi-sunflower-conjecture-is-now-proven/">here,</a> most notably the problem was <a href="https://gilkalai.wordpress.com/2019/08/23/amazing-ryan-alweiss-shachar-lovett-kewen-wu-jiapeng-zhang-made-dramatic-progress-on-the-sunflower-conjecture/"><strong>nearly solved</strong> by Ryan Alweiss, Shachar Lovett, Kewen Wu,  and Jiapeng Zhang</a>.</p>
<p><strong>4. Non-mathematical projects.</strong></p>
<p><strong>4a. Developing a new type of chess-playing program.</strong></p>
<p>The precise formulation was: “how good a chess-playing program is possible if the amount of memory space allowed is very restricted, and the amount of calculation is also limited?” In <a href="https://gowers.wordpress.com/2009/09/16/possible-future-polymath-projects/#comment-4000">a comment</a>, Tim explained that one would like to find a method of programming that applied to all games of a certain type (things like Othello, Go, etc.) and not just chess. And this could be taken as the aim. </p>
<p><a href="https://gowers.wordpress.com/2009/09/16/possible-future-polymath-projects/#comment-4006">One comment</a> mentioned the then very recent computer programs for playing Go (based on machine learning). Let me mention that deep learning led to a revolution in this area around 2015.  (And also that we had <a href="https://gilkalai.wordpress.com/2008/06/25/amir-ban-on-deep-junior/">a guest post by Amir Ban</a> on chess playing computer programs.)</p>
<p><strong>4b. The origin of life.</strong></p>
<p>This rather tentative suggestion, was to try to come up with a model that would show convincingly how life could emerge from non-life by purely naturalistic processes. <a href="https://gowers.wordpress.com/2009/11/07/polymath-and-the-origin-of-life/">It was further discussed in this post.</a> There was a lot of excitement around it but it did not lead to a polymath project, and I am not sure about related progress after 2009.</p>
<p>As an aside let me mention that Aubrey de Gray, who famously improved the lower bound on the chromatic number of the plane (that led to <a href="https://dustingmixon.wordpress.com/2018/04/14/polymath16-first-thread-simplifying-de-greys-graph/">polymath16</a>), is very famous for his works and ideas on aging. I suggested to Aubrey, that he should have a “polymath style” project on the issue of aging and his approach to the problem. However Aubrey’s response was that  various attempts to do things like that have failed over the years, across many biological fields. </p>
<p><strong>5. A tentative approach to complexity lower bounds.</strong></p>
<p>This turned into a series of posts (<a href="https://gowers.wordpress.com/2009/09/22/a-conversation-about-complexity-lower-bounds/">first one here</a>, <a href="https://gowers.wordpress.com/category/complexity/">whole “complexity category” here</a>) where Tim tried to develop several ideas related to the <img alt="NP \ne P" class="latex" src="https://s0.wp.com/latex.php?latex=NP+%5Cne+P&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="NP \ne P"/> problem.</p>
<p>________</p>
<p>OK, let me now briefly and tentatively preview some suggestions for my future polymath project. </p>
<h2>Possible future Polymath projects (2021)  </h2>
<p>1. <strong><a href="https://gilkalai.wordpress.com/2021/01/10/open-problem-session-of-huji-combsem-problem-5-gil-kalai-the-3%e1%b5%88-problem/">The  3ᵈ conjecture</a> and the flag conjecture for centrally symmetric polytopes</strong>. </p>
<p><strong>2. Mathematical questions regarding social welfare functions. </strong> Here is a  <a href="https://gilkalai.wordpress.com/2009/06/02/social-choice-preview/">related 2009 post</a> and <a href="https://arxiv.org/abs/2012.10352">a survey by Elchanan Mossel</a>.</p>
<p><strong>3. Developing a theory polymath-style</strong></p>



<p><span style="color: #0000ff;">Can a polymath project be used to develop a theory rather than solving a problem? </span>This question was raised by <strong>Peter Sarnak</strong> in a <a href="https://polymathprojects.files.wordpress.com/2011/03/polymathias.jpg">2010 IAS debate about polymath projects</a>.</p>
<p><strong>3.a A theory of convex hulls of real algebraic varieties</strong>.  One project in this spirit that <a href="https://polymathprojects.org/2014/01/20/two-polymath-of-a-sort-proposed-projects/">I already proposed</a> is to try to develop polymathly a theory of convex hulls of real algebraic varieties.</p>
<p><strong>3.b  Extending algebraic shifting to a wide variety of combinatorial structures.</strong> This is a project with Hélène Barcelo from the mid 90s that at the time did not get off the ground and it could be very nice to explore it collectively.</p>
<p>What  other theories would you like to see developed openly and collectively?<span style="color: #ff0000;">*</span></p>
<p><strong>4.</strong> <strong>Touching non overlapping simplices – <a href="https://gilkalai.wordpress.com/2017/08/27/touching-simplices-and-polytopes-perles-argument/">the 2ᵈ-conjecture</a>.</strong></p>
<p><strong>5.</strong>  <strong>Erdős-style combinatorial problems.</strong> Two possibilities are</p>
<p><strong>5.a</strong> <strong>Simonyi’s conjecture;</strong> and  <strong>5.b</strong> <strong>Chvatal’s conjecture</strong>.</p>
<p>Both can be found at the end of <a href="https://gilkalai.wordpress.com/2008/09/28/extremal-combinatorics-iii-some-basic-theorems/">this post</a>. More links, <a href="https://holzman.technion.ac.il/files/2012/09/cancel.pdf">SC1</a>, <a href="https://arxiv.org/abs/1510.07597">SC2</a>, <a href="http://users.encs.concordia.ca/~chvatal/conjecture.html">CC1</a>, <a href="https://arxiv.org/abs/1608.08954">CC2</a></p>
<p><strong>6.</strong> <strong>Enumeration – weights to the rescue  </strong>(sorry for being vague)</p>
<p><strong>7.</strong> <strong>Mathematics with computers </strong>(this is even more vague for now; but see this <a href="https://mathoverflow.net/questions/12085/experimental-mathematics-leading-to-major-advances">MO question; </a>  Here I am thinking about an experimental project. Most of the other projects may have large ingredients of computer experimentation.)</p>
<p><strong>8. Is there a statistical methodology for detecting biased data selection and related statistical follies.</strong>   See <a href="https://gilkalai.wordpress.com/2020/06/26/to-cheer-you-up-in-difficult-times-6-play-rani-sharims-two-player-games-of-life-read-maya-bar-hillel-presentation-on-catching-lies-with-statistics-and-more/">this post</a> and <a href="https://gilkalai.files.wordpress.com/2020/06/ester.ppt">Maya Bar Hillel’s slides,</a> and also <a href="https://windowsontheory.org/2012/05/31/rigged-lottery-bible-codes-and-spinning-globes-what-would-kolmogorov-say/">this post by Omer Reingold</a> on Windows in Theory. </p>
<p><strong>9.</strong> <strong>A problem posed by Oded Schramm:</strong> Is there some <img alt="\epsilon &gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\epsilon &gt;0"/> so that for every <img alt="n&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=n%3E1&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="n&gt;1"/> there exist a set <img alt="K_n" class="latex" src="https://s0.wp.com/latex.php?latex=K_n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="K_n"/> of constant width 1 in dimension <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="n"/> whose volume satisfies <img alt="VOL(K_n) \le (1-\epsilon)^n V_n" class="latex" src="https://s0.wp.com/latex.php?latex=VOL%28K_n%29+%5Cle+%281-%5Cepsilon%29%5En+V_n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="VOL(K_n) \le (1-\epsilon)^n V_n"/>. (<img alt="V_n" class="latex" src="https://s0.wp.com/latex.php?latex=V_n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="V_n"/> is the volume of the <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="n"/>-dimensional ball of width one.) See <a href="https://mathoverflow.net/questions/29000/volumes-of-sets-of-constant-width-in-high-dimensions">this MO question</a>.</p>
<h3><span style="color: #0000ff;">Meta questions</span></h3>
<p>There are many meta questions regarding polymath projects: </p>
<ol>
<li>One important one is: What is the ideal platform for a polymath project?</li>
<li>Could a polymath project be moderated by a computer?</li>
<li> What are the potential of polymath projects? The limitations? Are polymath projects a good way to do mathematics.</li>
<li>What are the incentives to participate?</li>
<li>Are polymath projects inviting in terms of diversity of participants?  </li>
</ol>
<p><span style="color: #ff0000;">*</span> I wonder if a question “What mathematical theory would you like to see developed” on Math Overflow could stay alive and whether it may lead to nice responses. (There was a nice question about <a href="https://mathoverflow.net/questions/53036/books-you-would-like-to-read-if-somebody-would-just-write-them">what book would you like to see written.</a>) </p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2021-01-29T09:03:13Z</updated>
    <published>2021-01-29T09:03:13Z</published>
    <category term="Combinatorics"/>
    <category term="Mathematics over the Internet"/>
    <category term="Open discussion"/>
    <category term="polymath"/>
    <category term="Polymath proposals"/>
    <category term="Tim Gowers"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-02-09T09:20:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5382973292753625821</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5382973292753625821/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/01/phds-and-green-cards.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5382973292753625821" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5382973292753625821" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/01/phds-and-green-cards.html" rel="alternate" type="text/html"/>
    <title>PhDs and Green Cards</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Joe Biden's <a href="https://joebiden.com/immigration/">immigration policy</a> has some interesting policies for PhDs. </p><p/><blockquote>Biden will exempt from any cap recent graduates of PhD programs in STEM fields in the U.S. who are poised to make some of the most important contributions to the world economy. Biden believes that foreign graduates of a U.S. doctoral program should be given a green card with their degree and that losing these highly trained workers to foreign economies is a disservice to our own economic competitiveness. </blockquote><p>Biden will submit an immigration plan to congress soon but it is not clear if the above will be in the bill, whether it will survive negotiations or even whether an immigration bill will be passed at all. </p><p>Nevertheless I worry about the unintended consequences of this policy. It will encourage students to apply and come for a PhD who have no interest in research, but want the green card. It gives too much power to professors who may abuse their students who need the PhD. Conversely it will pressure professors and thesis committees to grant PhDs because there would be a big difference between graduating with a PhD and leaving early with a Masters. By making the PhD so valuable, we may devalue it.</p><p>The solution is to give green cards to Masters students as well. We shouldn't limit talented researchers and developers who can help the United States keep its technology edge. They don't take jobs away from Americans, but instead help create new ones.</p><p/></div>
    </content>
    <updated>2021-01-28T14:23:00Z</updated>
    <published>2021-01-28T14:23:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-02-09T06:25:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5270</id>
    <link href="https://www.scottaaronson.com/blog/?p=5270" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5270#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5270" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Research (by others) proceeds apace</title>
    <summary xml:lang="en-US">At age 39, I already feel more often than not like a washed-up has-been in complexity theory and quantum computing research. It’s not intelligence that I feel like I’ve lost, so much as two other necessary ingredients: burning motivation and time. But all is not lost: I still have students and postdocs to guide and […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>At age 39, I already feel more often than not like a washed-up has-been in complexity theory and quantum computing research.  It’s not intelligence that I feel like I’ve lost, so much as two other necessary ingredients: <em>burning motivation</em> and <em>time</em>.  But all is not lost: I still have students and postdocs to guide and inspire!  I still have the people who email me every day—journalists, high-school kids, colleagues—asking this and that!  Finally, I still have this blog, with which to talk about all the exciting research that <em>others</em> are doing!</p>



<p>Speaking of blogging about research: I know I ought to do more of it, so let me start right now.</p>



<ul><li>Last night, Renou et al. posted a striking paper on the arXiv entitled <a href="https://arxiv.org/pdf/2101.10873.pdf">Quantum physics needs complex numbers</a>.  One’s immediate reaction to the title might be “well duh … who ever thought it didn’t?”  (See <a href="https://www.scottaaronson.com/blog/?p=4021">this post of mine</a> for a survey of explanations for why quantum mechanics “should have” involved complex numbers.)  Renou et al., however, are interested in ruling out a subtler possibility: namely, that our universe is secretly based on a version of quantum mechanics with real amplitudes only, and that it uses extra Hilbert space dimensions that we don’t see in order to <em>simulate</em> complex quantum mechanics.  Strictly speaking, such a possibility can <em>never</em> be ruled out, any more than one can rule out the possibility that the universe is a classical computer that simulates quantum mechanics.  In the latter case, though, the whole point of Bell’s Theorem is to show that <em>if</em> the universe is secretly classical, then it also needs to be radically nonlocal (relying on faster-than-light communication to coordinate measurement outcomes).  Renou et al. claim to show something analogous about real quantum mechanics: there’s an experiment—as it happens, one involving three players and two entangled pairs—for which conventional QM predicts an outcome that can’t be explained using any variant of QM that’s both local and secretly based on real amplitudes.  Their experiment seems eminently doable, and I imagine it will be done in short order.</li></ul>



<ul><li>A bunch of people from PsiQuantum posted a <a href="https://arxiv.org/pdf/2101.09310.pdf">paper on the arXiv</a> introducing “fusion-based quantum computation” (FBQC), a variant of <a href="https://en.wikipedia.org/wiki/One-way_quantum_computer">measurement-based quantum computation</a> (MBQC) and apparently a new approach to fault-tolerance, which the authors say can handle a ~10% rate of lost photons.  PsiQuantum is the large, Palo-Alto-based startup trying to build scalable quantum computers based on photonics.  They’ve been notoriously secretive, to the point of not having a website.  I’m delighted that they’re sharing details of the sort of thing they hope to build; I hope and expect that the FBQC proposal will be evaluated by people more qualified than me.</li></ul>



<ul><li>Since this is already on social media: apparently, Marc Lackenby from Oxford will be <a href="https://www.math.ucdavis.edu/research/seminars/?talk_id=6082">giving a Zoom talk at UC Davis next week</a>, about a quasipolynomial-time algorithm to decide whether a given knot is the unknot.  A preprint doesn’t seem to be available yet, but this is a big deal if correct, on par with Babai’s quasipolynomial-time algorithm for graph isomorphism from four years ago (see <a href="https://www.scottaaronson.com/blog/?p=2521">this post</a>).  I can’t wait to see details!  (Not that I’ll understand them well.)</li></ul>



<p/></div>
    </content>
    <updated>2021-01-27T22:45:06Z</updated>
    <published>2021-01-27T22:45:06Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-02-03T18:27:57Z</updated>
    </source>
  </entry>
</feed>
