<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-01-14T14:21:37Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3458</id>
    <link href="https://agtb.wordpress.com/2020/01/14/ec20-workshops-and-tutorials-call-for-proposals/" rel="alternate" type="text/html"/>
    <title>EC20 workshops and tutorials: call for proposals</title>
    <summary>The ACM EC20 conference to be held on July 13-17, 2020 in Budapest is now calling for proposals for tutorials and workshops.  The deadline for submission of such proposals is March 2nd, 2020.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="http://ec20.sigecom.org/">ACM EC20 conference</a> to be held on July 13-17, 2020 in Budapest is now <a href="http://ec20.sigecom.org/call-for-contributions/workshops-tutorials/">calling for proposals for tutorials and workshops</a>.  The deadline for submission of such proposals is March 2nd, 2020.</p></div>
    </content>
    <updated>2020-01-14T09:56:02Z</updated>
    <published>2020-01-14T09:56:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Noam Nisan</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2020-01-14T14:20:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://mycqstate.wordpress.com/?p=1234</id>
    <link href="https://mycqstate.wordpress.com/2020/01/14/a-masters-project/" rel="alternate" type="text/html"/>
    <title>A Masters project</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In a previous post I reported on the beautiful recent result by Natarajan and Wright showing the astounding power of multi-prover interactive proofs with quantum provers sharing entanglement: in letters, . In this post I want to report on follow-up … <a href="https://mycqstate.wordpress.com/2020/01/14/a-masters-project/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In a <a href="https://mycqstate.wordpress.com/2019/04/14/randomness-and-interaction-entanglement-ups-the-game/">previous post</a> I reported on the beautiful <a href="https://arxiv.org/abs/1904.05870">recent result</a> by Natarajan and Wright showing the astounding power of multi-prover interactive proofs with quantum provers sharing entanglement: in letters, <img alt="{\text{NEEXP} \subseteq \text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BNEEXP%7D+%5Csubseteq+%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{NEEXP} \subseteq \text{MIP}^\star}"/>. In this post I want to report on follow-up work with Ji, Natarajan, Wright, and Yuen, that we just posted to <a href="https://arxiv.org/abs/2001.04383">arXiv</a>. This time however I will tell the story from a personal point of view, with all the caveats that this implies: the “hard science” will be limited (but there could be a hint as to how “science”, to use a big word, “progresses”, to use an ill-defined one), the story is far too long, and it might be mostly of interest to me only. It’s a one-sided story, but that has to be. (In particular below I may at times attribute credit in the form “X had this idea”. This is my recollection only, and it is likely to be inaccurate. Certainly I am ignoring a lot of important threads.) I wrote this because I enjoyed recollecting some of the best moments in the story just as much as some the hardest; it is fun to look back and find meanings in ideas that initially appeared disconnected. Think of it as an example of how different lines of work can come together in unexpected ways; a case for open-ended research. It’s also an antidote against despair that I am preparing for myself: whenever I feel I’ve been stuck on a project for far too long, I’ll come back to this post and ask myself if it’s been 14 years yet — if not, then press on.</p>
<p>It likely comes as a surprise to me only that I am no longer fresh out of the cradle. My academic life started in earnest some 14 years ago, when in the Spring of 2006 I completed my Masters thesis in Computer Science under the supervision of Julia Kempe, at Orsay in France. I had met Julia the previous term: her class on quantum computing was, by far, the best-taught and most exciting course in the Masters program I was attending, and she had gotten me instantly hooked. Julia agreed to supervise my thesis, and suggested that I look into some interesting recent result by Stephanie Wehner that linked the study of entanglement and nonlocality in quantum mechanics to complexity-theoretic questions about interactive proof systems (specifically, this was Stephanie’s <a href="https://arxiv.org/abs/quant-ph/0508201">paper</a> showing that <img alt="{\text{XOR-MIP}^\star \subseteq \text{QIP}(2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BXOR-MIP%7D%5E%5Cstar+%5Csubseteq+%5Ctext%7BQIP%7D%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{XOR-MIP}^\star \subseteq \text{QIP}(2)}"/>).</p>
<p>At the time the topic was very new. It had been initiated the previous year with a beautiful <a href="https://arxiv.org/abs/quant-ph/0404076">paper</a> by Cleve et al. (that I have recommended to many a student since!). It was a perfect fit for me: the mathematical aspects of complexity theory and quantum computing connected to my undergraduate background, while the relative concreteness of quantum mechanics (it is a physical theory after all) spoke to my desire for real-world connection (not “impact” or even “application” — just “connection”). Once I got myself up to speed in the area (which consisted of three papers: the two I already mentioned, together with a <a href="https://arxiv.org/abs/cs/0102013">paper</a> by Kobayashi and Matsumoto where they studied interactive proofs with quantum messages), Julia suggested looking into the the “entangled-prover” class <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> introduced in the aforementioned paper by Cleve et al. Nothing was known about this class! Nothing besides the trivial inclusion of single-prover interactive proofs, IP, and the containment in…ALL, the trivial class that contains all languages.<br/>
Yet the characterization MIP=NEXP of its classical counterpart by Babai et al. in the 1990s had led to one of the most productive lines of work in complexity of the past few decades, through the PCP theorem and its use from hardness of approximation to efficient cryptographic schemes. Surely, studying <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> had to be a productive direction? In spite of its well-established connection to classical complexity theory, via the formalism of interactive proofs, this was a real gamble. The study of entanglement from the complexity-theoretic perspective was entirely new, and bound to be fraught with difficulty; very few results were available and the existing lines of works, from the foundations of nonlocality to more recent endeavors in device-independent cryptography, provided little other starting point than strong evidence that even the simplest examples came with many unanswered questions. But my mentor was fearless, and far from a novice in terms of defraying new areas, having done pioneering work in areas ranging from quantum random walks to Hamiltonian complexity through adiabatic computation. Surely this would lead to something?</p>
<p>It certainly did. More sleepless nights than papers, clearly, but then the opposite would only indicate dullness. Julia’s question led to far more unexpected consequences than I, or I believe she, could have imagined at the time. I am writing this post to celebrate, in a personal way, the latest step in 15 years of research by dozens of researchers: today my co-authors and I uploaded to the quant-ph arXiv what we consider a complete characterization of the power of entangled-prover interactive proof systems by proving the equality <img alt="{\text{MIP}^\star = \text{RE}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar+%3D+%5Ctext%7BRE%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star = \text{RE}}"/>, the class of all recursively enumerable languages (a complete problem for RE is the halting problem). Without goign too much into the result itself (if you’re interested, we have a long introduction waiting for you), and since this is a personal blog, I will continue on with some personal thoughts about the path that got us there.</p>
<p>When Julia &amp; I started working on the question, our main source of inspiration were the results by Cleve et al. showing that the nonlocal correlations of entanglement had interesting consequences when seen through the lens of interactive proof systems in complexity theory. Since the EPR paper a lot of work in understanding entanglement had already been accomplished in the Physics community, most notably by Mermin, Peres, Bell, and more recently the works in device-indepent quantum cryptography by Acin, Pironio, Scarani and many others stimulated by Ekert’s proposal for quantum key distribution and Mayers and Yao’s idea for “device-independent cryptography”. By then we certainly knew that “spooky action-at-a-distance” did not entail any faster-than-light communication, and indeed was not really “action-at-a-distance” in the first place but merely “correlation-at-a-distance”. What Cleve et al. recognized is that these “spooky correlations-at-a-distance” were sufficiently special so as to not only give numerically different values in “Bell inequalities”, the tool invented by Bell to evidence nonlocality in quantum mechanics, but also have some potentially profound consequences in complexity theory. In particular, examples such as the “Magic Square game” demonstrated that enough correlation could be gained from entanglement so as to defeat basic proof systems whose soundness relied only on the absence of communication between the provers, an assumption that until then had been wrongly equated with the assumption that any computation performed by the provers could be modeled entirely locally. I think that the fallacy of this implicit assumption came as a surprise to complexity theorists, who may still not have entirely internalized it. Yet the perfect quantum strategy for the Magic Square game provides a very concrete “counter-example” to the soundness of the “clause-vs-variable” game for 3SAT. Indeed this game, a reformulation by Aravind and Cleve-Mermin of a Bell Inequality discovered by Mermin and Peres in 1990, can be easily re-framed as a 3SAT system of equations that is <em>not</em> satisfiable and yet is such that the associated two-player clause-vs-variable game has a <em>perfect</em> quantum strategy. It is this observation, made in the paper by Cleve et al., that gave the first strong hint that the use of entanglement in interactive proof systems could make many classical results in the area go awry.</p>
<p>By importing the study of non-locality into complexity theory Cleve et al. immediately brought it into the realm of asymptotic analysis. Complexity theorists don’t study fixed objects, they study families of objects that tend to have a uniform underlying structure and whose interesting properties manifest themselves “in the limit”. As a result of this new perspective focus shifted from the study of single games or correlations to infinite families thereof. Some of the early successes of this translation include the “unbounded violations” that arose from translating asymptotic separations in communication complexity to the language of Bell inequalities and correlations (e.g. this <a href="https://arxiv.org/abs/1012.5043">paper</a>). These early successes attracted the attention of some physicists working in foundations as well as some mathematical physicists, leading to a productive exploration that combined tools from quantum information, functional analysis and complexity theory.</p>
<p>The initial observations made by Cleve et al. had pointed to <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> as a possibly interesting complexity class to study. Rather amazingly, nothing was known about it! They had shown that under strong restrictions on the verifier’s predicate (it should be an XOR of two answer bits), a collapse took place: by the work of Hastad, XOR-MIP equals NEXP, but <img alt="{\text{XOR-MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BXOR-MIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{XOR-MIP}^\star}"/> is included in EXP. This seemed very fortuitous (the inclusion is proved via a connection with semidefinite programming that seems tied to the structure of XOR-MIP protocols): could entanglement induce a collapse of the entire, unrestricted class? We thought (at this point mostly Julia thought, because I had no clue) that this ought not to be the case, and so we set ourselves to show that the equality <img alt="{\text{MIP}^\star=\text{NEXP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%3D%5Ctext%7BNEXP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star=\text{NEXP}}"/>, that would directly parallel Babai et al.’s characterization MIP=NEXP, holds. We tried to show this by introducing techniques to “immunize” games against entanglement: modify an interactive proof system so that its structure makes it “resistant” to the kind of “nonlocal powers” that can be used to defeat the clause-vs-variable game (witness the Magic Square). This was partially successful, and led to one of the papers I am most proud of — I am proud of it because I think it introduced elementary techniques (such as the use of the Cauchy-Schwarz inequality — inside joke — more seriously, basic things such as “prover-switching”, “commutation tests”, etc.) that are now routine manipulations in the area. The paper was a hard sell! It’s good to remember the first rejections we received. They were not unjustified: the main point of criticism was that we were only able to establish a hardness result for exponentially small completeness-soundness gap. A result for such a small gap in the classical setting follows directly from a very elementary analysis based on the Cook-Levin theorem. So then why did we have to write so many pages (and so many applications of Cauchy-Schwarz!) to arrive at basically the same result (with a <img alt="{^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{^\star}"/>)?</p>
<p>Eventually we got lucky and the paper was accepted to a conference. But the real problem, of establishing any non-trivial lower bound on the class <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> with constant (or, in the absence of any parallel repetition theorem, inverse-polynomial) completeness-soundness gap, remained. By that time I had transitioned from a Masters student in France to a graduate student in Berkeley, and the problem (pre-)occupied me during some of the most difficult years of my Ph.D. I fully remember spending my first year entirely thinking about this (oh and sure, that systems class I had to pass to satisfy the Berkeley requirements), and then my second year — yet, getting nowhere. (I checked the arXiv to make sure I’m not making this up: two full years, no posts.) I am forever grateful to my fellow student Anindya De for having taken me out of the cycle of torture by knocking on my door with one of the most interesting questions I have studied, that led me into quantum cryptography and quickly resulted in an enjoyable <a href="https://arxiv.org/abs/0911.4680">paper</a>. It was good to feel productive again! (Though the paper had fun reactions as well: after putting it on the arXiv we quickly heard from experts in the area that we had solved an irrelevant problem, and that we better learn about information theory — which we did, eventually leading to another <a href="https://arxiv.org/abs/0912.5514">paper</a>, etc.) The project had distracted me and I set interactive proofs aside; clearly, I was stuck.</p>
<p>About a year later I visited IQC in Waterloo. I don’t remember in what context the visit took place. What I do remember is a meeting in the office of Tsuyoshi Ito, at the time a postdoctoral scholar at IQC. Tsuyoshi asked me to explain our result with Julia. He then asked a very pointed question: the bedrock for the classical analysis of interactive proof systems is the “linearity test” of Blum-Luby-Rubinfeld (BLR). Is there any sense in which we could devise a quantum version of that test?</p>
<p>What a question! This was great. At first it seemed fruitless: in what sense could one argue that quantum provers apply a “linear function”? Sure, quantum mechanics is linear, but that is besides the point. The linearity is a property of the prover’s answers as a function of their question. So what to make of the quantum state, the inherent randomness, etc.?</p>
<p>It took us a few months to figure it out. Once we got there however, the answer was relatively simple — the prover should be making a question-independent measurement that returns a linear function that it applies to its question in order to obtain the answer returned to the verifier — and it opened the path to our subsequent <a href="https://arxiv.org/abs/1207.0550">paper</a> showing that the inclusion of NEXP in <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> indeed holds. Tsuyoshi’s question about linearity testing had allowed us to make the connection with PCP techniques; from there to MIP=NEXP there was only one step to make, which is to analyze multi-linearity testing. That step was suggested by my Ph.D. advisor, Umesh Vazirani, who was well aware of the many pathways towards the classical PCP theorem, since the theorem had been obtained in great part by his former student Sanjeev Arora. It took a lot of technical work, yet conceptually a single question from my co-author had sufficed to take me out of a 3-year slumber.</p>
<p>This was in 2012, and I thought we were done. For some reason the converse inclusion, of <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> in NEXP, seemed to resist our efforts, but surely it couldn’t resist much longer. Navascues et al. had introduced a hierarchy of semidefinite programs that seemed to give the right answer (technically they could only show convergence to a relaxation, the commuting value, but that seemed like a technicality; in particular, the values coincide when restricted to finite-dimensional strategies, which is all we computer scientists cared about). There were no convergence bounds on the hierarchy, yet at the same time commutative SDP hierarchies were being used to obtain very strong results in combinatorial optimization, and it seemed like it would only be a matter of time before someone came up with an analysis of the quantum case. (I had been trying to solve a related “dimension reduction problem” with Oded Regev for years, and we were making no progress; yet it seemed <em>someone</em> ought to!)</p>
<p>In Spring 2014 during an open questions session at a <a href="https://simons.berkeley.edu/workshops/qhc2014-1">workshop</a> at the Simons Institute in Berkeley Dorit Aharonov suggested that I ask the question of the possible inclusion of QMA-EXP, the exponential-sized-proofs analogue of QMA, in <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/>. A stronger result than the inclusion of NEXP (under assumptions), wouldn’t it be a more natural “fully quantum” analogue of MIP=NEXP? Dorit’s suggestion was motivated by research on the “quantum PCP theorem”, that aims to establish similar hardness results in the realm of the local Hamiltonian problem; see e.g. <a href="https://mycqstate.wordpress.com/2014/10/31/quantum-pcp-conjectures/">this post</a> for the connection. I had no idea how to approach the question — I also didn’t really believe the answer could be positive — but what can you do, if Dorit asks you something… So I reluctantly went to the board and asked the question. Joe Fitzsimons was in the audience, and he immediately picked it up! Joe had the fantastic ideas of using quantum error-correction, or more specifically secret-sharing, to distribute a quantum proof among the provers. His enthusiasm overcame my skepticism, and we eventually <a href="https://arxiv.org/abs/1409.0260">showed</a> the desired inclusion. Maybe <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> <em>was</em> bigger than <img alt="{\text{NEXP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BNEXP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{NEXP}}"/> after all.</p>
<p>Our result, however, had a similar deficiency as the one with Julia, in that the completeness-soundness gap was exponentially small. Obtaining a result with a constant gap took 3 years of couple more years of work and the fantastic energy and insights of a Ph.D. student at MIT, Anand Natarajan. Anand is the first person I know of to have had the courage to dive in to the most technical aspects of the analysis of the aforementioned results, while also bringing in the insights of a “true quantum information theorist” that were supported by Anand’s background in Physics and upbringing in the group of Aram Harrow at MIT. (In contrast I think of myself more as a “raw” mathematician; I don’t really understand quantum states other than as psd matrices…not that I understand math either of course; I suppose I’m some kind of a half-baked mish-mash.) Anand had many ideas but one of the most beautiful ones led to what he poetically called the “Pauli braiding test”, a “truly quantum” analogue of the BLR linearity test that amounts to doing <em>two</em> linearity tests in conjugate bases and piecing the results together into a robust test for <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-qubit entanglement (I wrote about our work on this <a href="https://mycqstate.wordpress.com/2017/06/28/pauli-braiding/">here</a>).</p>
<p>At approximately the same time Zhengfeng Ji had another wonderful idea, that was in some sense orthogonal to our work. (My interpretation of) Zhengfeng’s idea is that one can see an interactive proof system as a computation (verifier-prover-verifier) and use Kitaev’s circuit-to-Hamiltonian construction to transform the entire computation into a “quantum CSP” (in the same sense that the local Hamiltonian problem is a quantum analogue of classical constraint satisfaction problems (CSP)) that could then itself be verified by a quantum multi-prover interactive proof system…with exponential gains in efficiency! Zhengfeng’s result implied an exponential improvement in complexity compared to the result by Julia and myself, showing inclusion of NEEXP, instead of NEXP, in <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/>. However, Zhengfeng’s technique suffered from the same exponentially small completeness-soundness gap as we had, so that the best lower bound on <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> per se remained NEXP.</p>
<p>Both works led to follow-ups. With Natarajan we promoted the Pauli braiding test into a “<a href="https://arxiv.org/abs/1801.03821">quantum low-degree test</a>” that allowed us to show the inclusion of QMA-EXP into <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/>, with constant gap, thereby finally answering the question posed by Aharonov 4 years after it was asked. (I should also say that by then all results on <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> started relying on a sequence of parallel repetition results shown by Bavarian, Yuen, and others; I am skipping this part.) In parallel, with Ji, Fitzsimons, and Yuen we showed that Ji’s compression technique could be “iterated” an arbitrary number of times. In fact, by going back to “first principles” and representing verifiers uniformly as Turing machines we realized that the compression technique could be used iteratively to (up to small caveats) give a new proof of the fact (first <a href="https://arxiv.org/abs/1703.08618">shown</a> by Slofstra using an embedding theorem for finitely presented group) that the zero-gap version of <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> contains the halting problem. In particular, the entangled value is uncomputable! This was not the first time that uncomputability crops in to a natural problem in quantum computing (e.g. the <a href="https://arxiv.org/abs/1502.04573">spectral gap paper</a>), yet it still surprises when it shows up. Uncomputable! How can anything be uncomputable!</p>
<p>As we were wrapping up our paper Henry Yuen realized that our “iterated compression of interactive proof systems” was likely optimal, in the following sense. Even a mild improvement of the technique, in the form of a slower closing of the completeness-soundness gap through compression, would yield a much stronger result: undecidability of the constant-gap class <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/>. It was already known by work of Navascues et al., Fritz, and others, that such a result would have, if not surprising, certainly consequences that seemed like they would be taking us out of our depth. In particular, undecidability of any language in <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> would imply a negative resolution to a series of equivalent conjectures in functional analysis, from Tsirelson’s problem to Connes’ Embedding Conjecture through Kirchberg’s QWEP conjecture. While we liked our result, I don’t think that we believed it could resolve any conjecture(s) in functional analysis.</p>
<p>So we moved on. At least I moved on, I did some cryptography for a change. But Anand Natarajan and his co-author John Wright did not stop there. They had the last major insight in this story, which underlies their recent STOC best paper described in the previous <a href="https://mycqstate.wordpress.com/2019/04/14/randomness-and-interaction-entanglement-ups-the-game/">post</a>. Briefly, they were able to combine the two lines of work, by Natarajan &amp; myself on low-degree testing and by Ji et al. on compression, to obtain a compression that is specially tailored to the existing <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> protocol for NEXP and compresses that protocol without reducing its completeness-soundness gap. This then let them show Ji’s result that <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/> contains NEEXP, but this time with constant gap! The result received well-deserved attention. In particular, it is the first in this line of works to not suffer from any caveats (such as a closing gap, or randomized reductions, or some kind of “unfair” tweak on the model that one could attribute the gain in power to), and it implies an unconditional separation between MIP and <img alt="{\text{MIP}^\star}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star}"/>.</p>
<p>As they were putting the last touches on their result, suddenly something happened, which is that a path towards a much bigger result opened up. What Natarajan &amp; Wright had achieved is a one-step gapless compression. In our iterated compression paper we had observed that iterated gapless compression would lead to <img alt="{\text{MIP}^\star=\text{RE}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%3D%5Ctext%7BRE%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star=\text{RE}}"/>, implying negative answers to the aforementioned conjectures. So then?</p>
<p>I suppose it took some more work, but in some way all the ideas had been laid out in the previous 15 years of work in the complexity of quantum interactive proof systems; we just had to put it together. And so a decade after the characterization <a href="https://arxiv.org/abs/0907.4737">QIP = PSPACE</a> of single-prover quantum interactive proof systems, we have arrived at a characterization of quantum multiprover interactive proof systems, <img alt="{\text{MIP}^\star = \text{RE}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar+%3D+%5Ctext%7BRE%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{MIP}^\star = \text{RE}}"/>. With one author in common between the two papers: congratulations Zhengfeng!</p>
<p>Even though we just posted a paper, in a sense there is much more left to do. I am hopeful that our complexity-theoretic result will attract enough interest from the mathematicians’ community, and especially operator algebraists, for whom CEP is a central problem, that some of them will be willing to devote time to understanding the result. I also recognize that much effort is needed on our own side to make it accessible in the first place! I don’t doubt that eventually complexity theory will not be needed to obtain the purely mathematical consequences; yet I am hopeful that some of the ideas may eventually find their way into the construction of interesting mathematical objects (such as, who knows, a non-hyperlinear group).</p>
<p>That was a good Masters project…thanks Julia!</p></div>
    </content>
    <updated>2020-01-14T01:32:43Z</updated>
    <published>2020-01-14T01:32:43Z</published>
    <category term="meta"/>
    <category term="QPCP"/>
    <category term="Quantum"/>
    <category term="Science"/>
    <category term="interactive proofs"/>
    <category term="qpcp"/>
    <category term="science"/>
    <author>
      <name>Thomas</name>
    </author>
    <source>
      <id>https://mycqstate.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://mycqstate.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://mycqstate.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://mycqstate.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://mycqstate.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>In superposition</subtitle>
      <title>MyCQstate</title>
      <updated>2020-01-14T14:21:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3743356831833676109</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3743356831833676109/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/01/what-would-you-do-if-you-showed-pnp-i.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3743356831833676109" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3743356831833676109" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/01/what-would-you-do-if-you-showed-pnp-i.html" rel="alternate" type="text/html"/>
    <title>What would you do if you showed P=NP? I would reread Factor Man by Matt Ginsberg</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Lance has often said (and also in <a href="https://blog.computationalcomplexity.org/2020/01/silicon-valley-ethics.html">this</a>) that if P=NP that would be great for the world: much more efficient ways to build things, science could be done better, etc, and that is much more important than that modern crypto would no longer work. We now have the technology to do private key really well--- like a thumb drive that has a billion bits for 1-time pads.<br/>
<br/>
I agree that the world would be better off in some ways, I wonder how much damage would be done in the transition period from public to private key. Would the world recover enough to reap the benefits of P=NP?<br/>
<br/>
First think of what YOU would do if you showed P=NP (and lets assume your algorithm is either reasonable or could be made reasonable with some time and effort).<br/>
<br/>
The novel <i>Factor Man  </i>is about what someone who has solved P=NP does. I won't tell you how it goes, but they deal with the issue intelligently. So if I solved P=NP then I would first re-read it, and think through if I would do that, or modify what is done, or what.  Its a good start.<br/>
<br/>
I reviewed the book in SIGACT News or you can read my review <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/factorman.pdf">here</a><br/>
<br/>
On a slightly diff note, here is the latest argument I've heard for why P=NP:<br/>
<br/>
Planar 2-coloring is in P<br/>
<br/>
Planar 4-coloring is in P<br/>
<br/>
So<br/>
<br/>
Planar 3-coloring should be in P.<br/>
<br/>
This was said by a very good math/cs ugrad at UMCP. I do not know if he was kidding.<br/>
<br/>
<br/></div>
    </content>
    <updated>2020-01-14T01:28:00Z</updated>
    <published>2020-01-14T01:28:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-01-14T11:01:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/01/14/socal-theory-day-2020/</id>
    <link href="https://cstheory-events.org/2020/01/14/socal-theory-day-2020/" rel="alternate" type="text/html"/>
    <title>SoCal Theory Day 2020</title>
    <summary>January 20, 2020 UC Riverside https://www.cs.ucr.edu/~silas/ An all day event to celebrate the TCS research community in Southern California</summary>
    <updated>2020-01-14T00:51:47Z</updated>
    <published>2020-01-14T00:51:47Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-01-14T14:21:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.04447</id>
    <link href="http://arxiv.org/abs/2001.04447" rel="alternate" type="text/html"/>
    <title>Scattering and Sparse Partitions, and their Applications</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Arnold.html">Arnold Filtser</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.04447">PDF</a><br/><b>Abstract: </b>A partition $\mathcal{P}$ of a weighted graph $G$ is
$(\sigma,\tau,\Delta)$-sparse if every cluster has diameter at most $\Delta$,
and every ball of radius $\Delta/\sigma$ intersects at most $\tau$ clusters.
Similarly, $\mathcal{P}$ is $(\sigma,\tau,\Delta)$-scattering if instead for
balls we require that every shortest path of length at most $\Delta/\sigma$
intersects at most $\tau$ clusters. Given a graph $G$ that admits a
$(\sigma,\tau,\Delta)$-sparse partition for all $\Delta&gt;0$, Jia et al. [STOC05]
constructed a solution for the Universal Steiner Tree problem (and also
Universal TSP) with stretch $O(\tau\sigma^2\log_\tau n)$. Given a graph $G$
that admits a $(\sigma,\tau,\Delta)$-scattering partition for all $\Delta&gt;0$,
we construct a solution for the Steiner Point Removal problem with stretch
$O(\tau^3\sigma^3)$. We then construct sparse and scattering partitions for
various different graph families, receiving many new results for the Universal
Steiner Tree and Steiner Point Removal problems.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.04430</id>
    <link href="http://arxiv.org/abs/2001.04430" rel="alternate" type="text/html"/>
    <title>Evaluating the snappability of bar-joint frameworks</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nawratil:Georg.html">Georg Nawratil</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.04430">PDF</a><br/><b>Abstract: </b>It is well-known that there exist bar-joint frameworks (without continuous
flexions) whose physical models can snap between different realizations due to
non-destructive elastic deformations of material. We present a method to
measure these snapping capability -- shortly called snappability -- based on
the total elastic strain energy of the framework by computing the deformation
of all bars using Hook's law. The presented theoretical results give further
connections between shakiness and snapping beside the well-known technique of
averaging and deaveraging.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.04413</id>
    <link href="http://arxiv.org/abs/2001.04413" rel="alternate" type="text/html"/>
    <title>Backward Feature Correction: How Deep Learning Performs Deep Learning</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Allen=Zhu:Zeyuan.html">Zeyuan Allen-Zhu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yuanzhi.html">Yuanzhi Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.04413">PDF</a><br/><b>Abstract: </b>How does a 110-layer ResNet learn a high-complexity classifier using
relatively few training examples and short training time? We present a theory
towards explaining this in terms of $\textit{hierarchical learning}$. We refer
hierarchical learning as the learner learns to represent a complicated target
function by decomposing it into a sequence of simpler functions to reduce
sample and time complexity. This paper formally analyzes how multi-layer neural
networks can perform such hierarchical learning efficiently and automatically
simply by applying stochastic gradient descent (SGD). On the conceptual side,
we present, to the best of our knowledge, the FIRST theory result indicating
how very deep neural networks can still be sample and time efficient on certain
hierarchical learning tasks, when NO KNOWN non-hierarchical algorithms (such as
kernel method, linear regression over feature mappings, tensor decomposition,
sparse coding) are efficient. We establish a new principle called "backward
feature correction", which we believe is the key to understand the hierarchical
learning in multi-layer neural networks. On the technical side, we show for
regression and even for binary classification, for every input dimension $d &gt;
0$, there is a concept class consisting of degree $\omega(1)$ multi-variate
polynomials so that, using $\omega(1)$-layer neural networks as learners, SGD
can learn any target function from this class in $\mathsf{poly}(d)$ time using
$\mathsf{poly}(d)$ samples to any $\frac{1}{\mathsf{poly}(d)}$ error, through
learning to represent it as a composition of $\omega(1)$ layers of quadratic
functions. In contrast, we present lower bounds stating that several
non-hierarchical learners, including any kernel methods, neural tangent
kernels, must suffer from $d^{\omega(1)}$ sample or time complexity to learn
functions in this concept class even to any $d^{-0.01}$ error.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.04383</id>
    <link href="http://arxiv.org/abs/2001.04383" rel="alternate" type="text/html"/>
    <title>MIP*=RE</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Ji:Zhengfeng.html">Zhengfeng Ji</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Natarajan:Anand.html">Anand Natarajan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vidick:Thomas.html">Thomas Vidick</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wright:John.html">John Wright</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yuen:Henry.html">Henry Yuen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.04383">PDF</a><br/><b>Abstract: </b>We show that the class MIP* of languages that can be decided by a classical
verifier interacting with multiple all-powerful quantum provers sharing
entanglement is equal to the class RE of recursively enumerable languages. Our
proof builds upon the quantum low-degree test of (Natarajan and Vidick, FOCS
2018) by integrating recent developments from (Natarajan and Wright, FOCS 2019)
and combining them with the recursive compression framework of (Fitzsimons et
al., STOC 2019).
</p>
<p>An immediate byproduct of our result is that there is an efficient reduction
from the Halting Problem to the problem of deciding whether a two-player
nonlocal game has entangled value $1$ or at most $\frac{1}{2}$. Using a known
connection, undecidability of the entangled value implies a negative answer to
Tsirelson's problem: we show, by providing an explicit example, that the
closure $C_{qa}$ of the set of quantum tensor product correlations is strictly
included in the set $C_{qc}$ of quantum commuting correlations. Following work
of (Fritz, Rev. Math. Phys. 2012) and (Junge et al., J. Math. Phys. 2011) our
results provide a refutation of Connes' embedding conjecture from the theory of
von Neumann algebras.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.04333</id>
    <link href="http://arxiv.org/abs/2001.04333" rel="alternate" type="text/html"/>
    <title>A Universal Attractor Decomposition Algorithm for Parity Games</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Marcin Jurdziński, Rémi Morvan <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.04333">PDF</a><br/><b>Abstract: </b>An attractor decomposition meta-algorithm for solving parity games is given
that generalizes the classic McNaughton-Zielonka algorithm and its recent
quasi-polynomial variants due to Parys (2019), and to Lehtinen, Schewe, and
Wojtczak (2019). The central concepts studied and exploited are attractor
decompositions of dominia in parity games and the ordered trees that describe
the inductive structure of attractor decompositions.
</p>
<p>The main technical results include the embeddable decomposition theorem and
the dominion separation theorem that together help establish a precise
structural condition for the correctness of the universal algorithm: it
suffices that the two ordered trees given to the algorithm as inputs embed the
trees of some attractor decompositions of the largest dominia for each of the
two players, respectively.
</p>
<p>The universal algorithm yields McNaughton-Zielonka, Parys's, and
Lehtinen-Schewe-Wojtczak algorithms as special cases when suitable universal
trees are given to it as inputs. The main technical results provide a unified
proof of correctness and deep structural insights into those algorithms.
</p>
<p>A symbolic implementation of the universal algorithm is also given that
improves the symbolic space complexity of solving parity games in
quasi-polynomial time from $O(d \lg n)$---achieved by Chatterjee,
Dvo\v{r}\'{a}k, Henzinger, and Svozil (2018)---down to $O(\lg d)$, where $n$ is
the number of vertices and $d$ is the number of distinct priorities in a parity
game. This not only exponentially improves the dependence on $d$, but it also
entirely removes the dependence on $n$.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.04286</id>
    <link href="http://arxiv.org/abs/2001.04286" rel="alternate" type="text/html"/>
    <title>Nonparametric Continuous Sensor Registration</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Clark:William.html">William Clark</a>, Maani Ghaffari, Anthony Bloch <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.04286">PDF</a><br/><b>Abstract: </b>This paper develops a new mathematical framework that enables nonparametric
joint semantic/appearance and geometric representation of continuous functions
using data. The joint semantic and geometric embedding is modeled by
representing the processes in a reproducing kernel Hilbert space. The framework
allows the functions to be defined on arbitrary smooth manifolds where the
action of a Lie group is used to align them. The continuous functions allow the
registration to be independent of a specific signal resolution and the
framework is fully analytical with a closed-form derivation of the Riemannian
gradient and Hessian. We study a more specialized but widely used case where
the Lie group acts on functions isometrically. We solve the problem by
maximizing the inner product between two functions defined over data, while the
continuous action of the rigid body motion Lie group is captured through the
integration of the flow in the corresponding Lie algebra. Low-dimensional cases
are derived with numerical examples to show the generality of the proposed
framework. The high-dimensional derivation for the special Euclidean group
acting on the Euclidean space showcases the point cloud registration and
bird's-eye view map registration abilities. A specific derivation and
implementation of this framework for RGB-D cameras outperform the
state-of-the-art robust visual odometry and performs well in texture and
structure-scares environments.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.04219</id>
    <link href="http://arxiv.org/abs/2001.04219" rel="alternate" type="text/html"/>
    <title>Structural Decompositions of Epistemic Logic Programs</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hecher:Markus.html">Markus Hecher</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morak:Michael.html">Michael Morak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woltran:Stefan.html">Stefan Woltran</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.04219">PDF</a><br/><b>Abstract: </b>Epistemic logic programs (ELPs) are a popular generalization of standard
Answer Set Programming (ASP) providing means for reasoning over answer sets
within the language. This richer formalism comes at the price of higher
computational complexity reaching up to the fourth level of the polynomial
hierarchy. However, in contrast to standard ASP, dedicated investigations
towards tractability have not been undertaken yet. In this paper, we give first
results in this direction and show that central ELP problems can be solved in
linear time for ELPs exhibiting structural properties in terms of bounded
treewidth. We also provide a full dynamic programming algorithm that adheres to
these bounds. Finally, we show that applying treewidth to a novel dependency
structure---given in terms of epistemic literals---allows to bound the number
of ASP solver calls in typical ELP solving procedures.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.04191</id>
    <link href="http://arxiv.org/abs/2001.04191" rel="alternate" type="text/html"/>
    <title>Exploiting Database Management Systems and Treewidth for Counting</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Johannes K. Fichte, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hecher:Markus.html">Markus Hecher</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thier:Patrick.html">Patrick Thier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woltran:Stefan.html">Stefan Woltran</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.04191">PDF</a><br/><b>Abstract: </b>Bounded treewidth is one of the most cited combinatorial invariants, which
was applied in the literature for solving several counting problems
efficiently. A canonical counting problem is #SAT, which asks to count the
satisfying assignments of a Boolean formula. Recent work shows that
benchmarking instances for #SAT often have reasonably small treewidth. This
paper deals with counting problems for instances of small treewidth. We
introduce a general framework to solve counting questions based on
state-of-the-art database management systems (DBMS). Our framework takes
explicitly advantage of small treewidth by solving instances using dynamic
programming (DP) on tree decompositions (TD). Therefore, we implement the
concept of DP into a DBMS (PostgreSQL), since DP algorithms are already often
given in terms of table manipulations in theory. This allows for elegant
specifications of DP algorithms and the use of SQL to manipulate records and
tables, which gives us a natural approach to bring DP algorithms into practice.
To the best of our knowledge, we present the first approach to employ a DBMS
for algorithms on TDs. A key advantage of our approach is that DBMS naturally
allow to deal with huge tables with a limited amount of main memory (RAM),
parallelization, as well as suspending computation.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.04120</id>
    <link href="http://arxiv.org/abs/2001.04120" rel="alternate" type="text/html"/>
    <title>NP-complete variants of some classical graph problems</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alexandersson:Per.html">Per Alexandersson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.04120">PDF</a><br/><b>Abstract: </b>Some classical graph problems such as finding minimal spanning tree, shortest
path or maximal flow can be done efficiently. We describe slight variations of
such problems which are shown to be NP-complete. Our proofs use straightforward
reduction from $3$-SAT.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.04020</id>
    <link href="http://arxiv.org/abs/2001.04020" rel="alternate" type="text/html"/>
    <title>LinearFold: linear-time approximate RNA folding by 5'-to-3' dynamic programming and beam search</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Liang.html">Liang Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:He.html">He Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deng:Dezhong.html">Dezhong Deng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Kai.html">Kai Zhao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Kaibo.html">Kaibo Liu</a>, David A. Hendrix, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mathews:David_H=.html">David H. Mathews</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.04020">PDF</a><br/><b>Abstract: </b>Motivation: Predicting the secondary structure of an RNA sequence is useful
in many applications. Existing algorithms (based on dynamic programming) suffer
from a major limitation: their runtimes scale cubically with the RNA length,
and this slowness limits their use in genome-wide applications.
</p>
<p>Results: We present a novel alternative $O(n^3)$-time dynamic programming
algorithm for RNA folding that is amenable to heuristics that make it run in
$O(n)$ time and $O(n)$ space, while producing a high-quality approximation to
the optimal solution. Inspired by incremental parsing for context-free grammars
in computational linguistics, our alternative dynamic programming algorithm
scans the sequence in a left-to-right (5'-to-3') direction rather than in a
bottom-up fashion, which allows us to employ the effective beam pruning
heuristic. Our work, though inexact, is the first RNA folding algorithm to
achieve linear runtime (and linear space) without imposing constraints on the
output structure. Surprisingly, our approximate search results in even higher
overall accuracy on a diverse database of sequences with known structures. More
interestingly, it leads to significantly more accurate predictions on the
longest sequence families in that database (16S and 23S Ribosomal RNAs), as
well as improved accuracies for long-range base pairs (500+ nucleotides apart),
both of which are well known to be challenging for the current models.
</p>
<p>Availability: Our source code is available at
https://github.com/LinearFold/LinearFold, and our webserver is at
<a href="http://linearfold.org">this http URL</a> (sequence limit: 100,000nt).
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.03962</id>
    <link href="http://arxiv.org/abs/2001.03962" rel="alternate" type="text/html"/>
    <title>Computational Hardness of Multidimensional Subtraction Games</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gurvich:Vladimir.html">Vladimir Gurvich</a>, Michael Vyalyi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.03962">PDF</a><br/><b>Abstract: </b>We study algorithmic complexity of solving subtraction games in a~fixed
dimension with a finite difference set. We prove that there exists a game in
this class such that any algorithm solving the game runs in exponential time.
Also we prove an existence of a game in this class such that solving the game
is PSPACE-hard. The results are based on the construction introduced by Larsson
and W\"astlund. It relates subtraction games and cellular automata.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.03924</id>
    <link href="http://arxiv.org/abs/2001.03924" rel="alternate" type="text/html"/>
    <title>An improvement of the upper bound for GKS communication game</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ivan Petrenko <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.03924">PDF</a><br/><b>Abstract: </b>The GKS game was formulated by Justin Gilmer, Michal Koucky, and Michael Saks
in their research of the sensitivity conjecture. Mario Szegedy invented a
protocol for the game with the cost of $O(n^{0.4732})$. Then a protocol with
the cost of $O(n^{0.4696})$ was obtained by DeVon Ingram who used a bipartite
matching. We propose a slight improvement of Ingram's method and design a
protocol with cost of $O(n^{0.4693})$.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.03794</id>
    <link href="http://arxiv.org/abs/2001.03794" rel="alternate" type="text/html"/>
    <title>Grundy Coloring &amp; friends, Half-Graphs, Bicliques</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aboulker:Pierre.html">Pierre Aboulker</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonnet:=Eacute=douard.html">Édouard Bonnet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Eun_Jung.html">Eun Jung Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sikora:Florian.html">Florian Sikora</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.03794">PDF</a><br/><b>Abstract: </b>The first-fit coloring is a heuristic that assigns to each vertex, arriving
in a specified order $\sigma$, the smallest available color. The problem Grundy
Coloring asks how many colors are needed for the most adversarial vertex
ordering $\sigma$, i.e., the maximum number of colors that the first-fit
coloring requires over all possible vertex orderings. Since its inception by
Grundy in 1939, Grundy Coloring has been examined for its structural and
algorithmic aspects. A brute-force $f(k)n^{2^{k-1}}$-time algorithm for Grundy
Coloring on general graphs is not difficult to obtain, where $k$ is the number
of colors required by the most adversarial vertex ordering. It was asked
several times whether the dependency on $k$ in the exponent of $n$ can be
avoided or reduced, and its answer seemed elusive until now. We prove that
Grundy Coloring is W[1]-hard and the brute-force algorithm is essentially
optimal under the Exponential Time Hypothesis, thus settling this question by
the negative.
</p>
<p>The key ingredient in our W[1]-hardness proof is to use so-called half-graphs
as a building block to transmit a color from one vertex to another. Leveraging
the half-graphs, we also prove that b-Chromatic Core is W[1]-hard, whose
parameterized complexity was posed as an open question by Panolan et al. [JCSS
'17]. A natural follow-up question is, how the parameterized complexity changes
in the absence of (large) half-graphs. We establish fixed-parameter
tractability on $K_{t,t}$-free graphs for b-Chromatic Core and Partial Grundy
Coloring, making a step toward answering this question. The key combinatorial
lemma underlying the tractability result might be of independent interest.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.03788</id>
    <link href="http://arxiv.org/abs/2001.03788" rel="alternate" type="text/html"/>
    <title>Minimum $2$-vertex-twinless connected spanning subgraph problem</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaberi:Raed.html">Raed Jaberi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.03788">PDF</a><br/><b>Abstract: </b>Given a $2$-vertex-twinless connected directed graph $G=(V,E)$, the minimum
$2$-vertex-twinless connected spanning subgraph problem is to find a minimum
cardinality edge subset $E^{t} \subseteq E$ such that the subgraph $(V,E^{t})$
is $2$-vertex-twinless connected. Let $G^{1}$ be a minimal $2$-vertex-connected
subgraph of $G$. In this paper we present a $(2+a_{t}/2)$-approximation
algorithm for the minimum $2$-vertex-twinless connected spanning subgraph
problem, where $a_{t}$ is the number of twinless articulation points in
$G^{1}$.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.03743</id>
    <link href="http://arxiv.org/abs/2001.03743" rel="alternate" type="text/html"/>
    <title>Accelerating Forward and Backward Private Searchable Encryption Using Trusted Execution</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vo:Viet.html">Viet Vo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lai:Shangqi.html">Shangqi Lai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yuan:Xingliang.html">Xingliang Yuan</a>, Shi-Feng Sun, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nepal:Surya.html">Surya Nepal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Joseph_K=.html">Joseph K. Liu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.03743">PDF</a><br/><b>Abstract: </b>Searchable encryption (SE) is one of the key enablers for building encrypted
databases. It allows a cloud server to search over encrypted data without
decryption. Dynamic SE additionally includes data addition and deletion
operations to enrich the functions of encrypted databases. Recent attacks
exploiting the leakage in dynamic operations drive rapid development of new SE
schemes revealing less information while performing updates; they are also
known as forward and backward private SE. Newly added data is no longer
linkable to queries issued before, and deleted data is no longer searchable in
queries issued later. However, those advanced SE schemes reduce the efficiency
of SE, especially in the communication cost between the client and server. In
this paper, we resort to the hardware-assisted solution, aka Intel SGX, to ease
the above bottleneck. Our key idea is to leverage SGX to take over the most
tasks of the client, i.e., tracking keyword states along with data addition and
caching deleted data. However, handling large datasets is non-trivial due to
the I/O and memory constraints of the SGX enclave. We further develop batch
data processing and state compression technique to reduce the communication
overhead between the SGX and untrusted server, and minimise the memory
footprint in the enclave. We conduct a comprehensive set of evaluations on both
synthetic and real-world datasets, which confirm that our designs outperform
the prior art.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.03741</id>
    <link href="http://arxiv.org/abs/2001.03741" rel="alternate" type="text/html"/>
    <title>On Polynomial Modular Number Systems over $\mathbb{Z}/p\mathbb{Z}$</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Jean Claude Bajard, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marrez:J=eacute=r=eacute=my.html">Jérémy Marrez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Plantard:Thomas.html">Thomas Plantard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/V=eacute=ron:Pascal.html">Pascal Véron</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.03741">PDF</a><br/><b>Abstract: </b>Polynomial Modular Number System (PMNS) is a convenient number system for
modular arithmetic, introduced in 2004. The main motivation was to accelerate
arithmetic modulo an integer $p$. An existence theorem of PMNS with specific
properties was given.
</p>
<p>The construction of such systems relies on sparse polynomials whose roots
modulo $p$ can be chosen as radices of this kind of positional representation.
However, the choice of those polynomials and the research of their roots are
not trivial.
</p>
<p>In this paper, we introduce a general theorem on the existence of PMNS and we
provide bounds on the size of the digits used to represent an integer modulo
$p$.
</p>
<p>Then, we present classes of suitable polynomials to obtain systems with an
efficient arithmetic. Finally, given a prime $p$, we evaluate the number of
roots of polynomials modulo $p$ in order to give a number of PMNS bases we can
reach. Hence, for a fixed prime $p$, it is possible to get numerous PMNS, which
can be used efficiently for different applications based on large prime finite
fields, such as those we find in cryptography, like RSA, Diffie-Hellmann key
exchange and ECC (Elliptic Curve Cryptography).
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.03664</id>
    <link href="http://arxiv.org/abs/2001.03664" rel="alternate" type="text/html"/>
    <title>Obtaining a Canonical Polygonal Schema from a Greedy Homotopy Basis with Minimal Mesh Refinement</title>
    <feedworld_mtime>1578960000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Livesu:Marco.html">Marco Livesu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.03664">PDF</a><br/><b>Abstract: </b>Any closed manifold of genus g can be cut open to form a topological disk and
then mapped to a regular polygon with 4g sides. This construction is called the
canonical polygonal schema of the manifold, and is a key ingredient for many
applications in graphics and engineering, where a parameterization between two
shapes with same topology is often needed. The sides of the 4g-gon define on
the manifold a system of loops, which all intersect at a single point and are
disjoint elsewhere. Computing a shortest system of loops of this kind is
NP-hard. A computationally tractable alternative consists in computing a set of
shortest loops that are not fully disjoint in polynomial time, using the greedy
homotopy basis algorithm proposed by Erickson and Whittlesey, and then detach
them in post processing via mesh refinement. Despite this operation is
conceptually simple, known refinement strategies do not scale well for high
genus shapes, triggering a mesh growth that may exceed the amount of memory
available in modern computers, leading to failures. In this paper we study
various local refinement operators to detach cycles in a system of loops, and
show that there are important differences between them, both in terms of mesh
complexity and preservation of the original surface. We ultimately propose two
novel refinement approaches: the former minimizes the number of new elements in
the mesh, possibly at the cost of a deviation from the input geometry. The
latter allows to trade mesh complexity for geometric accuracy, bounding
deviation from the input surface. Both strategies are trivial to implement, and
experiments confirm that they allow to realize canonical polygonal schemas even
for extremely high genus shapes where previous methods fail.
</p></div>
    </summary>
    <updated>2020-01-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-01-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://bit-player.org/?p=2222</id>
    <link href="http://bit-player.org/2020/the-teetering-towers-of-abstraction" rel="alternate" type="text/html"/>
    <link href="http://bit-player.org/2020/the-teetering-towers-of-abstraction#comments" rel="replies" type="text/html"/>
    <link href="http://bit-player.org/2020/the-teetering-towers-of-abstraction/feed/atom" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Teetering Towers of Abstraction</title>
    <summary type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml">Abstraction is an abstraction. You can’t touch it or taste it or photograph it. You can barely talk about it without resorting to metaphors and analogies. Yet this ghostly concept is an essential tool in both mathematics and computer science. … <a href="http://bit-player.org/2020/the-teetering-towers-of-abstraction">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Abstraction is an abstraction. You can’t touch it or taste it or photograph it. You can barely talk about it without resorting to metaphors and analogies. Yet this ghostly concept is an essential tool in both mathematics and computer science. Oddly, it seems to inspire quite different feelings and responses in those two fields. I’ve been wondering why.</p>
<p>In mathematics abstraction serves as a kind of stairway to heaven—as well as a test of stamina for those who want to get there. <img alt="West stairs to Grandview Park 2017-10-28" border="0" class="alignright" height="" src="http://bit-player.org/wp-content/uploads/2019/11/west-stairs-to-Grandview-Park-2017-10-28.jpg" width="340"/>West stairs to Grand View Park, San Francisco, October 2017. You begin the climb at an early age, at ground level, with things that are not at all abstract. Jelly beans, for example.  You learn the important life lesson that if you have <img alt="five jelly beans" border="0" height="14" src="http://bit-player.org/wp-content/uploads/2019/11/five-jelly-beans-32-by-89.png" width="39"/> and you eat <img alt="three jelly beans" border="0" height="14" src="http://bit-player.org/wp-content/uploads/2019/11/three-jelly-beans-32-by-53.png" width="23"/>, you will have only <img alt="two jelly beans" border="0" height="14" src="http://bit-player.org/wp-content/uploads/2019/11/two-jelly-beans-32-by-36.png" width="16"/> left. After absorbing this bitter truth, you are invited to climb the stairs of ab­straction as far as the first landing, where you replace the tasty tangible jelly beans with sugar-free symbols: \(5 - 3 = 2\).</p>
<p>Some years later you reach higher ground. The sym­bols represent­ing par­tic­ular numbers give way to the \(x\)s and \(y\)s that stand for quantities yet to be determined. They are symbols for sym­bols. Later still you come to realize that this algebra business is not just about “solving for \(x\),” for finding a specific number that corresponds to a specific letter. It’s a magical device that allows you to make blanket statements encompassing <em>all</em> numbers: \(x^2 - 1 = (x + 1)(x - 1)\) is true for any value of \(x\).</p>
<p>Continuing onward and upward, you learn to manipulate symbolic expressions in various other ways, such as differentiating and integrating them, or constructing functions of functions of functions. Keep climbing the stairs and eventually you’ll be introduced to areas of mathematics that openly boast of their abstractness. There’s <em>abstract algebra</em>, where you build your own collections of numberlike things: groups, fields, rings, vector spaces. <img alt="Ben Orlin cartoon: 'Sorry, I only do abstractions, not numbers.' 'But numbers are abstractions.' 'Let me clarify: I only do abstractions of abstractions of abstractions'.jpg" border="0" class="alignleft" height="301" src="http://bit-player.org/wp-content/uploads/2019/11/Orlin-abstractions-of-abstractions.jpg" width="400"/>Cartoon by Ben Orlin, <a href="https://mathwithbaddrawings.com/2017/01/11/why-are-mathematicians-so-bad-at-arithmetic/">mathwithbaddrawings.com</a>, reprinted under <a href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons license.</a>Another route up the stairway takes you to <em>category theory</em>, where you’ll find a collection of ideas with the disarming label <em>ab­stract nonsense</em>.</p>
<p>Not everyone is filled with admiration for this Jenga tower of abstrac­tions teetering atop more abstrac­tions. Con­sider Andrew Wiles’s proof of Fermat’s last theorem, and its reception by the public. The theorem, first stated by Pierre de Fermat in the 1630s, makes a simple claim about powers of integers: If \(x, y, z, n\) are all integers greater than \(0\), then \(x^n + y^n = z^n\) has solutions only if \(n \le 2\). The proof of this claim, published in the 1990s, is not nearly so simple. Wiles (with contributions from Richard Taylor) went on a scavenger hunt through much of modern mathematics, collecting a truckload of tools and spare parts needed to make the proof work: elliptic curves, modular forms, Galois groups, functions on the complex plane, <em>L</em>-series. It is truly a <em>tour de force</em>.</p>
<p>Diagram (borrowed from Kenneth A. Ribet and Brian Hayes, “<a href="http://bit-player.org/wp-content/extras/bph-publications/AmSci-1994-03-Ribet-Hayes-Fermats-Last-Theorem.pdf">Fermat’s Last Theorem and Modern Arithmetic</a>“) outlines the overall strategy of the Wiles proof. If you had a counterexample to FLT, you could construct an elliptic curve <em>E</em> with certain properties. But the properties deduced on the left and right branches of the diagram turn out to be inconsistent, implying that <em>E</em> does not exist, nor does the counter­example that gave rise to it.<img alt="Outline of the Wiles-Taylor proof of Fermat's last theorem" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2019/11/1994-03-Fermat-F11-L-series-updated-2019.svg" width="550"/></p>
<p class="indent">Is all that heavy machinery really needed to prove such an innocent-looking state­ment? Many people yearn for a simpler and more direct proof, ideally based on methods that would have been available to Fermat himself. Ken Ribet will be presenting “A 2020 View of Fermat’s Last Theorem” at the Joint Mathematics Meetings later this week. In a <a href="https://www.ams.org/journals/notices/202001/rnoti-p82.pdf">preview</a> of the talk, he notes that advances made since 1994 allow a more succinct statement of the proof. But those recent advances are no easier to understand than the original proof.At least nine attempts to construct an elementary proof have been posted on the arXiv in the past 20 years, and there are lots more elsewhere. I think the sentiment motivating much of this work is, “You shouldn’t be allowed to prove a theorem I care about with methods I don’t understand.” Marilyn vos Savant, the <em>Parade</em> columnist, takes an even more <a href="https://us.macmillan.com/books/9780312106577">extreme position</a>, arguing that Wiles strayed so far from the subject matter of the theorem as to make his proof invalid. (For a critique of her critique, see <a href="https://www.jstor.org/stable/2975048?seq=1">Boston and Granville</a>.)</p>
<p>Almost all of this grumbling about illegimate methods and excess complexity comes from outside the  community of research mathematicians. Insiders see the Wiles proof differently. For them, the wide-ranging nature of the proof is actually what’s most important. The main accomp­lishment, in this view, was cementing a connection between those far-flung areas of mathematics; resolving FLT was just a bonus.</p>
<p>Yet even mathematicians can have misgivings about the intricacy of math­ematical arguments and the ever-taller skyscrapers of abstraction. <a href="http://oro.open.ac.uk/2818/">Jeremy Gray</a>, a historian of mathematics, believes anxiety over abstraction was already rising in the 19th century, when mathematics seemed to be “moving away from reality, into worlds of arbitrary dimension, for example, and into the habit of supplanting intuitive concepts (curves that touch, neighboring points, velocity) with an opaque language of mathematical analysis that bought rigor at a high cost in intelligibility.”</p>
<p>Quite apart from these comments on abstraction, the thesis is well worth reading. It offers alternating sections of “mathsplaining” and “laysplaining.” See also a <a href="http://digitaleditions.walsworthprintgroup.com/publication/?m=7656&amp;l=1#{%22issue_id%22:%22566588%22,%22page%22:%2250%22}">review in <em>MAA Focus</em></a> by Adriana Salerno. The thesis was to be published in book form last fall by Birkhäuser, but the book doesn’t seem to be available yet.For a view of abstraction in contemporary mathematics, we have a vivid image from Piper Harron, a young mathematician who wrote an <a href="http://www.theliberatedmathematician.com/math/">extraordinarily candid PhD thesis in 2016</a>. The introductory chapter begins, “The hardest part about math is the level of abstraction required.” She goes on to explain:</p>
<blockquote class="undent"><p>I like to imagine abstraction (abstractly ha ha ha) as pulling the strings on a marionette. The marionette, being “real life,” is easily accessible. Everyone understands the marionette whether it’s walking or dancing or fighting. We can see it and it makes sense. But watch instead the hands of the puppeteers. Can you look at the hand movements of the puppeteers and know what the marionette is doing?… Imagine it gets worse. Much, much worse. Imagine that the marionettes we see are controlled by marionettoids we don’t see which are in turn controlled by pre-puppeteers which are finally controlled by actual puppeteers.</p></blockquote>
<p>Keep all those puppetoids in mind. I’ll be coming back to them, but first I want to shift my attention to computer science, where the towers of abstraction are just as tall and teetery, but somehow less scary.</p>
<hr/>
<p>Suppose your computer is about to add two numbers…. No, wait, there’s no need to suppose or imagine. In the orange panel below, type some numbers into the \(a\) and \(b\) boxes, then press the “+” button to get the sum in box \(c\). Now, please describe what’s happening inside the machine as that computation is performed.</p>
<div id="adder-box" style="width: 230px; height: 97px; border: 1px solid black; margin: 10px auto; background-color: coral;">
	<input id="aInput" style="width: 150px; height: 18px; text-align: right; font-family: monospace; font-size: 12pt;" type="text"/><p/>
<div style="font-size: 14pt;">a</div>
<p>	<button id="plus-button">+</button><br/>
	<input id="bInput" style="width: 150px; height: 18px; text-align: right; font-family: monospace; font-size: 12pt;" type="text"/></p>
<div style="font-size: 14pt;">b</div>
<div id="rule" style="width: 185px; height: 0px; border: 0.5px solid black;"/>
<div id="output" style="width: 163px; height: 22px; border: 1px solid #0000002e; text-align: right; background-color: white; font-size: 12pt; font-family: monospace; line-height: 18pt; padding-right: 1px;"/>
<div style="font-size: 14pt;">c</div>
</div>
<p class="indent">You can probably guess that somewhere behind the curtains there’s a fragment of code that looks like <code>c = a + b</code>. And, indeed, that statement appears verbatim in the JavaScript program that’s triggered when you click on the plus button. But if you were to go poking around among the circuit boards under the keyboard of your laptop, you wouldn’t find anything resembling that sequence of symbols. The program statement is a high-level abstraction. If you really want to know what’s going on inside the computing engine, you need to dig deeper—down to something as tangible as a jelly bean.</p>
<p>How about an electron? In truth, electrons are not so tangible. The proper mental image is not a hard sphere like a BB but a diffuse probability distribution. In other words, the electron itself is an abstraction.During the computation, clouds of electrons drift through the machine’s circuitry, like swarms of migrating butterflies. Their movements are regulated by the switching action of transistors, and the transistors in turn are controlled by the moving electrons. It is this dance of the electrons that does the arithmetic and produces an answer. Yet it would be madness to describe the evaluation of <code>c = a + b</code> by tracing the motions of all the electrons (perhaps \(10^{23}\) of them) through all the transistors (perhaps \(10^{11}\)).</p>
<p>To understand how electrons are persuaded to do arithmetic for us, we need to introduce a whole sequence of abstractions.</p>
<ul>
<li>First, step back from the focus on individual electrons, and reformulate the problem in terms of continuous quantities: voltage, current, capacitance, inductance.</li>
<li>Replace the physical transistors, in which voltages and currents change smoothly, with idealized devices that instantly switch from totally off to fully on.</li>
<li>Interpret the two states of a transistor as logical values (<em>true</em> and <em>false</em>) or as numerical values (\(1\) and \(0\)).</li>
<li>Organize groups of transistors into “gates” that carry out basic functions of Boolean logic, such as <span style="font-variant: small-caps;">and</span>, <span style="font-variant: small-caps;">or</span>, and <span style="font-variant: small-caps;">not</span>.</li>
<li>Assemble the gates into larger functional units, including adders, multipliers, comparators, and other components for doing base-\(2\) arithmetic.</li>
<li>Build higher-level modules that allow the adders and such to be operated under the control of a program. This is the conceptual level of the instruction-set architecture, defining the basic operation codes (<em>add, shift, jump</em>, etc.) recognized by the computer hardware.</li>
<li>Graduating from hardware to software, design an operating system, a collection of services and interfaces for abstract objects such as files, input and output channels, and concurrent processes.</li>
<li>Create a compiler or interpreter that knows how to translate programming language statements such as <code>c = a + b</code> into sequences of machine instructions and operating-system requests.</li>
</ul>
<p class="indent">From the point of view of most programmers, the abstractions listed above represent computational <em>infrastructure</em>: They lie beneath the level where you do most of your thinking—the level where you describe the algorithms and data structures that solve your problem. But computational abstractions are also a tool for building <em>superstructure</em>, for creating new functions beyond what the operating system and the programming language provide. For example, if your programming language handles only numbers drawn from the real number line, you can write procedures for doing arithmetic with complex numbers, such as \(3 + 5i\). (Go ahead, try it in the orange box above.) And, in analogy with the mathematical practice of defining functions of functions, we can build compiler compilers and schemes for metaprogramming—programs that act on other programs.</p>
<p>In both mathematics and computation, rising through the various levels of abstraction gives you a more elevated view of the landscape, with wider scope but less detail. Even if the process is essentially the same in the two fields, however, it doesn’t feel that way, at least to me. In mathematics, abstraction can be a source of anxiety; in computing, it is nothing to be afraid of. In math, you must take care not to tangle the puppet strings; in computing, abstractions are a defense against such confusion. For the mathematician, abstraction is an intellectual challenge; for the programmer, it is an aid to clear thinking.</p>
<p>Why the difference? How can abstraction have such a friendly face in computation and such a stern mien in math? One possible answer is that computation is just plain easier than mathematics. In speaking of “computation,” what I have in mind is the design of algorithms and data structures suitable for a machine we can build out of material components. If you are playing with Turing machines and other toys of theoretical computer science, the game is altogether different. But in my view theoretical computer science is just a funny-looking branch of mathematics. (With apologies to those of my friends who grimace to hear me say it.) Anything that fits into the computer is necessarily discrete and finite. In principle, any computer program could be reduced to a big table mapping all possible inputs to the corresponding outputs. Mathematics is invulnerable to this kind of trivialization by brute force. It has infinities hiding under the bed and lurking behind the closet door, and that’s what makes it both fun and frightening.</p>
<p>Another possible explanation is that computer systems are engineered artifacts; we can build them to our own specifications. If a concept is just too hairy for the human mind to master, we can break it down into simpler pieces. Math is not so complaisant—not even for those who hold that mathematical objects are invented rather than discovered. We can’t just design number theory so that the Riemann hypothesis will be true.</p>
<p>But I think the crucial distinction between math abstractions and computer abstractions lies elsewhere. It’s not in the abstractions themselves but in the boundaries between them.</p>
<hr/>
<p>Warning from the abstraction police on the office door of Radhika Nagpal, Harvard University. (Photographed November 2013.)<img alt="Abstraction barrier doorway 3402" border="0" class="centered" height="472" src="http://bit-player.org/wp-content/uploads/2019/12/abstraction-barrier-doorway-3402.jpg" width="640"/></p>
<p>I believe I first encountered the term <em>abstraction barrier</em> in Abelson and Sussman’s <a href="https://web.mit.edu/alexmv/6.037/sicp.pdf">Structure and Inter­pretation of Computer Programs</a>, circa 1986. The underlying idea is surely older; it’s implicit in the “structured programming” literature of the 1960s and 70s. But <em>SICP</em> still offers the clearest and most compelling introduction.In building computer systems, we are urged to compartmentalize, to create self-contained and sealed-off modules—black boxes whose inner workings are concealed from outside observers. In this world, <em>information hiding</em> is considered a virtue, not an impeachable offense. If a design has a layered structure, with abstractions piled one atop the other, the layers are separated by  <em>abstraction barriers</em>. A high-level module can reach across the barrier to make use of procedures from lower levels, but it won’t know anything about the implementation of those procedures. When you are writing programs in Lisp or Python, you shouldn’t need to think about how the  operating system carries out its chores; and when you’re writing routines for the operating system, you needn’t think about the physics of electrons meandering through the crystal lattice of a semiconductor. Each level of the hierarchy can be treated (almost) independently.</p>
<p>Mathematics also has its abstraction barriers, although I’ve never actually heard the term used by mathematicians. A notable example comes from Giuseppe Peano’s formulation of the foundations of arithmetic, circa 1900. Peano posits the existence of a number \(0\), and a function called <em>successor</em>, \(S(n)\), which takes a number \(n\) and returns the next number in the counting sequence. Thus the natural numbers begin \(0, S(0), S(S(0)), S(S(S(0)))\), and so on. Peano deliberately refrains from saying anything more about what these numbers look like or how they work. They might be implemented as sets, with \(0\) being the empty set and successor the operation of adjoining an element to a set. Or they could be unary lists: (), (|), (||), (|||), . . . The most direct approach is to use <a href="https://en.wikipedia.org/wiki/Church_encoding">Church numerals</a>, in which the successor function itself serves as a counting token, and the number \(n\) is represented by \(n\) nested applications of \(S\).</p>
<p>From these minimalist axioms we can define the rest of arithmetic, starting with addition. In calculating \(a + b\), if \(b\) happens to be \(0\), the problem is solved: \(a + 0 = a\). If \(b\) is <em>not</em> \(0\), then it must be the successor of some number, which we can call \(c\). Then \(a + S(c) = S(a + c)\). Notice that this definition doesn’t depend in any way on how the number \(0\) and the successor function are represented or implemented. Under the hood, we might be working with sets or lists or abacus beads; it makes no difference. An abstraction barrier separates the levels. From addition you can go on to define multiplication, and then exponentiation, and again abstraction barriers protect you from the lower-level details. There’s never any need to think about how the successor function works, just as the computer programmer doesn’t think about the flow of electrons.</p>
<p>The importance of not thinking was stated eloquently by Alfred North Whitehead, more than a century ago:</p>
<blockquote><p>Alfred North Whitehead, <em><a href="http://www.gutenberg.org/ebooks/41568">An Introduction of Mathematics</a></em>, 1911, pp. 45–46.It is a profoundly erroneous truism, repeated by all copybooks and by eminent people when they are making speeches, that we should cultivate the habit of thinking of what we are doing. The precise opposite is the case. Civilisation advances by extending the number of important operations which we can perform without thinking about them. Operations of thought are like cavalry charges in a battle—they are strictly limited in number, they require fresh horses, and must only be made at decisive moments.</p></blockquote>
<hr/>
<p>If all of mathematics were like the Peano axioms, we would have a watertight structure, compartmentalized by lots of leakproof abstraction barriers. And abstraction would probably not be considered “the hardest part about math.” But, of course, Peano described only the tiniest corner of mathematics. We also have the puppet strings.</p>
<p>In Piper Harron’s unsettling vision, the puppeteers high above the stage pull strings that control the pre-puppeteers, who in turn operate the marionettoids, who animate the marionettes. Each of these agents can be taken as representing a level of abstraction. The problem is, we want to follow the action at both the top and the bottom of the hierarchy, and possibly at the middle levels as well. The commands coming down from the puppeteers on high embody the abstract ideas that are needed to build theorems and proofs, but the propositions to be proved lie at the level of the marionettes. There’s no separating these levels; the puppet strings tie them together.</p>
<p>In the case of Fermat’s Last Theorem, you might choose to view the Wiles proof as nothing more than an elevated statement about elliptic curves and modular forms, but the proof is famous for something else—for what it tells us about the elementary equation \(x^n + y^n = z^n\). Thus the master puppeteers work at the level of algebraic geometry, but our eyes are on the dancing marionettes of simple number theory. What I’m suggesting, in other words, is that abstraction barriers in mathematics sometimes fail because events on both sides of the barrier make simultaneous claims on our interest. </p>
<p>In computer science, the programmer can ignore the trajectories of the electrons because those details really are of no consequence. Indeed, the electronic guts of the computing machinery could be ripped out and replaced by fluidic devices or fiber optics or hamsters in exercise wheels, and that brain transplant would have no effect on the outcome of the computation. Few areas of mathematics can be so cleanly floated away and rebuilt on a new foundation.</p>
<p>Can this notion of leaky abstraction barriers actually explain why higher mathematics looks so intimidating to most of the human population? It’s surely not the whole story, but maybe it has a role.</p>
<p>In closing I would like to point out an analogy with a few other areas of science, where problems that cross abstraction barriers seem to be particularly difficult. Physics, for example, deals with a vast range of spatial scales. At one end of the spectrum are the quarks and leptons, which rattle around comfortably inside a particle with a radius of \(10^{-15}\) meter; at the other end are galaxy clusters spanning \(10^{24}\) meters. In most cases, effective abstraction barriers separate these levels. When you’re studying celestial mechanics, you don’t have to think about the atomic composition of the planets. Conversely, if you are looking at the interactions of elementary particles, you are allowed to assume they will behave the same way anywhere in the universe. But there are a few areas where the barriers break down. For example, near a critical point where liquid and gas phases merge into an undifferentiated fluid, forces at all scales from molecular to macroscopic become equally important. Turbulent flow is similar, with whirls upon whirls upon whirls. It’s not a coincidence that critical phenomena and turbulence are notoriously difficult to describe.</p>
<p>Biology also covers a wide swath of territory, from molecules and single cells to whole organisms and ecosystems on a planetary scale. Again, abstraction barriers usually allow the biologist to focus on one realm at a time. To understand a predator-prey system you don’t need to know about the structure of cytochrome <em>c</em>. But the barriers don’t always hold. Evolution spans all these levels. It depends on molecular events (mutations in DNA), and determines the shape and fate of the entire tree of life. We can’t fully grasp what’s going on in the biosphere without keeping all these levels in mind at once.</p>

<p/></div>
    </content>
    <updated>2020-01-13T23:00:40Z</updated>
    <published>2020-01-13T23:00:40Z</published>
    <category scheme="http://bit-player.org" term="computing"/>
    <category scheme="http://bit-player.org" term="mathematics"/>
    <author>
      <name>Brian Hayes</name>
      <uri>http://bit-player.org</uri>
    </author>
    <source>
      <id>http://bit-player.org/feed/atom</id>
      <link href="http://bit-player.org" rel="alternate" type="text/html"/>
      <link href="http://bit-player.org/feed/atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">An amateur's outlook on computation and mathematics</subtitle>
      <title xml:lang="en-US">bit-player</title>
      <updated>2020-01-13T23:01:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/01/13/postdoctoral-fellow-at-university-of-texas-at-austin-apply-by-june-1-2020/</id>
    <link href="https://cstheory-jobs.org/2020/01/13/postdoctoral-fellow-at-university-of-texas-at-austin-apply-by-june-1-2020/" rel="alternate" type="text/html"/>
    <title>Postdoctoral Fellow at University of Texas at Austin (apply by June 1, 2020)</title>
    <summary>The Computer Science Department at UT Austin invites applications for a Postdoctoral Fellow in theoretical computer science for the 2020-21 academic year. The Fellow will work with Dana Moshkovitz and David Zuckerman on pseudorandomness and computational complexity. Review of applicants will begin on January 15, but applications will be accepted until the position is filled. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science Department at UT Austin invites applications for a Postdoctoral Fellow in theoretical computer science for the 2020-21 academic year. The Fellow will work with Dana Moshkovitz and David Zuckerman on pseudorandomness and computational complexity. Review of applicants will begin on January 15, but applications will be accepted until the position is filled.</p>
<p>Website: <a href="https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/UT-MAIN-CAMPUS/Postdoctoral-Fellow_R_00006957">https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/UT-MAIN-CAMPUS/Postdoctoral-Fellow_R_00006957</a><br/>
Email: maguilar@cs.utexas.edu</p></div>
    </content>
    <updated>2020-01-13T14:40:52Z</updated>
    <published>2020-01-13T14:40:52Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-01-14T14:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.03252</id>
    <link href="http://arxiv.org/abs/2001.03252" rel="alternate" type="text/html"/>
    <title>The Very Best of Perfect Non-crossing Matchings</title>
    <feedworld_mtime>1578873600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mantas:Ioannis.html">Ioannis Mantas</a>, Marko Savić, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schrezenmaier:Hendrik.html">Hendrik Schrezenmaier</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.03252">PDF</a><br/><b>Abstract: </b>Given a set of points in the plane, we are interested in matching them with
straight line segments. We focus on perfect (all points are matched)
non-crossing (no two edges intersect) matchings. Apart from the well known
MinMax variation, where the length of the longest edge is minimized, we extend
work by looking into different optimization variants such as MaxMin, MinMin,
and MaxMax. We consider both the monochromatic and bichromatic versions of
these problems and provide efficient algorithms for various input point
configurations.
</p></div>
    </summary>
    <updated>2020-01-13T23:20:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-01-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2001.03196</id>
    <link href="http://arxiv.org/abs/2001.03196" rel="alternate" type="text/html"/>
    <title>Assignment-based Path Choice Estimation for Metro Systems Using Smart Card Data</title>
    <feedworld_mtime>1578873600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Baichuan Mo, Zhenliang Ma, Haris N. Koutsopoulosc, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Jinhua.html">Jinhua Zhao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2001.03196">PDF</a><br/><b>Abstract: </b>Urban rail services are the principal means of public transportation in many
cities. To understand the crowding patterns and develop efficient operation
strategies in the system, obtaining path choices is important. This paper
proposed an assignment-based path choice estimation framework using automated
fare collection (AFC) data. The framework captures the inherent correlation of
crowding among stations, as well as the interaction between path choice and
left behind. The path choice estimation is formulated as an optimization
problem. The original problem is intractable because of a non-analytical
constraint and a non-linear equation constraint. A solution procedure is
proposed to decompose the original problem into three tractable sub-problems,
which can be solved efficiently. The model is validated using both synthetic
data and real-world AFC data in Hong Kong Mass Transit Railway (MTR) system.
The synthetic data test validates the model's effectiveness in estimating path
choice parameters, which can outperform the purely simulation-based
optimization methods in both accuracy and efficiency. The test results using
actual data show that the estimated path shares are more reasonable than
survey-derived path shares and uniform path shares. Model robustness in terms
of different initial values and different case study dates are also verified.
</p></div>
    </summary>
    <updated>2020-01-13T23:20:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-01-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16548</id>
    <link href="https://rjlipton.wordpress.com/2020/01/12/our-thoughts-on-pnp/" rel="alternate" type="text/html"/>
    <title>Our Thoughts on P=NP</title>
    <summary>The Clay prize anniversary is soon. SME keynote lecture source Evelyn Lamb is a mathematician who is also a journalist. She has a blog called Roots of Unity on the Scientific American blog network. Also impressive is that she designed the AMS Page-a-Day Calendar on behalf of the American Mathematical Society. It is available from […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>The Clay prize anniversary is soon.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/01/evelynlambsme.jpg"><img alt="" class="alignright wp-image-16553" height="150" src="https://rjlipton.files.wordpress.com/2020/01/evelynlambsme.jpg?w=120&amp;h=150" width="120"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">SME keynote lecture <a href="https://www.youtube.com/watch?v=z8rWJkEDJTk">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Evelyn Lamb is a mathematician who is also a journalist. She has a blog called <a href="http://blogs.scientificamerican.com/roots-of-unity/">Roots of Unity</a> on the <em>Scientific American</em> blog network. Also impressive is that she designed the <em>AMS Page-a-Day Calendar</em> on behalf of the American Mathematical Society. It is <a href="https://bookstore.ams.org/mbk-128/">available</a> from the AMS with member discounts and is filled with wonderful tidbits on math.</p>
<p>
The other day she interviewed Ken and me on P=NP.</p>
<p>
Ken just happened to be visiting me that afternoon in New York and we spoke to Evelyn about P=NP. The call was delayed because of equipment issues on both ends. Perhaps the fundamental problem is not P=NP after all, but making computerized equipment work. Oh well.</p>
<p>
Evelyn is writing an article for the <em>New Scientist</em> magazine about P=NP. She said it was driven by the near 20th anniversary of the Clay <a href="https://en.wikipedia.org/wiki/Millennium_Prize_Problems">Prizes</a>. Recall there were seven of these, each with a million dollar prize. One, the Poincaré conjecture, was already solved. The others are still open—the million dollars is still there waiting for a brilliant researcher. </p>
<p>
</p><p/><h2> Questions and Answers </h2><p/>
<p/><p>
Here is our own rendition of some questions that came up. We did not keep a recording or notes on our end, and we have paraphrased and expanded some things that we said:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>What is the update on P=NP?</i> </p>
<p>
Ken: The update is that here is no update. There is no recent progress on resolving P=NP—seemingly none this decade, I’d say. <a href="https://rjlipton.wordpress.com/2018/01/23/progress-on-the-frontier/">This</a> is still light-years away from it and you could even say the difficulty needed for yea-much progress is discouraging. There are some conjectures that elaborate on P <img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/> NP, including <a href="https://en.wikipedia.org/wiki/Unique_games_conjecture">Unique Games</a> and <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">(S)ETH</a>, but those two have gone less clear.</p>
<p/><p><br/>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>Does the Clay prize help researchers?</i></p>
<p>
Dick: I do not see that the prize gets anyone to work on P=NP. <br/>
Ken: I disagree. The prize does help people explain quickly what they are working on to others and why. This is quite valuable. </p>
<p/><p><br/>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>Could P=NP be undecidable?</i></p>
<p>
Dick: Who knows. I note that known proofs that some combinatorial problem is unprovable in Peano arithmetic somehow rely on a function that grows too fast. The famous Ramsey problem is a perfect example. I do not see any way to get such a function in the P=NP area. Of course I could easily be wrong.</p>
<p>
Ken: This is the subject by which Dick and I first came to know each other in the 1980s, for instance this <a href="https://dl.acm.org/doi/10.1145/800141.804652">paper</a> of Dick’s with Rich DeMillo vis-à-vis <a href="https://www.mendeley.com/catalogue/topology-provability-complexity-theory/">one</a> of mine. I now believe it is resolvable but will need deep and complex techniques. </p>
<p/><p><br/>
Here I, Ken, went into the topic of “<a href="https://www.sciencedirect.com/science/article/pii/S002200009791494X">Natural</a> <a href="https://en.wikipedia.org/wiki/Natural_proof">Proofs</a>,” as Dick did again later.</p>
<p/><p><br/>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>When will P=NP be solved?</i></p>
<p>
Ken: We just <a href="https://rjlipton.wordpress.com/2019/12/22/predicting-when-pnp-is-resolved/">discussed</a> this on our blog. The upshot is that the conjecture has been open long enough—coming to its 50th anniversary if you date it by Steve Cook’s famous 1970–71 paper, 64 years if you date it by Kurt Gödel’s 1956 <a href="https://rjlipton.wordpress.com/the-gdel-letter/">letter</a> to John von Neumann—that it is going outside the range where data on other conjectures has any predictive value.</p>
<p>
Dick: There is a related issue with P=NP. Perhaps we have guessed the wrong direction. Most believe that P is not equal to NP. But many conjectures were finally resolved when someone worked on the right direction.</p>
<p/><p><br/>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>Who believes P=NP vs P <img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/> NP?</i></p>
<p>
Ken: Our friend Bill Gasarch has polled this three times since 2002. His <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/pollpaper3.pdf">article</a> finds 80% support for P <img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/> NP, which he <a href="https://blog.computationalcomplexity.org/2019/03/third-poll-on-p-vs-np-and-related.html">says</a> goes higher among those who have worked more on it. I believe unequal, but Dick’s opinion is <a href="https://rjlipton.wordpress.com/2017/12/08/pnp-perhaps-i-change-my-mind/">fluid</a> and Don Knuth recently said he takes the “equal” possibility seriously. </p>
<p/><p><br/>
I (Ken) started hunting for how we’ve covered Knuth’s opinion on GLL—it seems only a brief mention <a href="https://rjlipton.wordpress.com/2016/04/09/missing-mate-in-ten/">here</a> and in comments <a href="https://rjlipton.wordpress.com/2017/02/05/a-panel-on-p-vs-np/">here</a>—but Dick related hearing it in person from Knuth.</p>
<p/><p><br/>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>Why so Hard?</i></p>
<p>
Ken: If you believe P <img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/> NP, then it is hard because Nature—mathematical nature—does a bang-up job of making it seem like P <img alt="{=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{=}"/> NP. Most instances of NP-complete problems are easy; so called SAT-solvers have had much practical success. The larger issue is that nature has frustrated us from proving any nontrivial general lower bounds at all. You can allow a <img alt="{2^{O(n)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7BO%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{O(n)}}"/> exponential-time algorithm to make exponentially-long queries to NP and yet no one has proved that the resulting language cannot be decided by <em>linear</em>-sized Boolean circuits. Ryan Williams needed a lot of effort to <a href="https://rjlipton.wordpress.com/2012/11/13/the-ryan-wiliams-combine/">prove</a> that this class does not have constant-depth poly-size modular-counting circuits, but those could be weaker in relevant ways than general linear-size circuits. But this class is a <em>lot</em> bigger than NP. </p>
<p/><p><br/>
I then said another factor is that sometimes algorithms seem to make no visible progress until at the very end when they suddenly come up with a correct answer. Dick and I had <a href="https://rjlipton.wordpress.com/2010/08/28/lower-bounds-and-progressive-algorithms/">tried</a> to <a href="https://rjlipton.wordpress.com/2012/11/17/progress-on-progressive-algorithms/">quantify</a> a notion of progress. I then started talking about the “hardness versus randomness” phenomenon and the “Natural Proofs” barrier again (for which this 2016 <a href="https://people.csail.mit.edu/rrw/natural-journal.pdf">paper</a> by Ryan is a good reference) but Dick cut in with a nub of all these matters.</p>
<p>
Dick: A key issue is what I call “Bait-and-Switch” (indeed, in a <a href="https://rjlipton.wordpress.com/2009/02/12/bait-and-switch-why-lower-bounds-are-so-hard/">post</a> on the first day of GLL). Suppose an output <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is believed to be hard. Add a random <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> to it. The result <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> is also random. One branch of an algorithm computing <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> and another working on <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> seem to have nothing to do with <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Yet when you do <img alt="{y+z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%2Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y+z}"/> bitwise you have <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. This destroys any lower-bound argument that would be based on metering progress toward <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. </p>
<p/><p><br/>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>Guessing wrong way?</i></p>
<p>
Dick continued saying that this issue only affects the “not equal” position and maybe it’s a hint of people guessing the wrong way. This went back into some things that were said before, and then the call started winding up. </p>
<p>
</p><p/><h2> Things We Didn’t Say </h2><p/>
<p/><p>
We had made mental notes while walking back from lunch across the street in time for the call, but forgot some of them. To recycle an old <a href="https://quoteinvestigator.com/2014/01/06/verbal-contract/">saying</a>, a mental note isn’t worth the paper it’s written on. </p>
<p>
One of them was to remark on Gerhard Woeginger’s P Versus NP claims <a href="https://www.win.tue.nl/~gwoegi/P-versus-NP.htm">page</a> and the relative balance of claims of “Equal” and “Not equal.” As of its last update in September 2016, the 116 listed claims are (by Ken’s count) divided 62 for “Equal,” 49 for “Not equal,” 3 for unprovable/undecidable, 1 claiming no claim, and 1 for <em>both</em> “Equal” and “Not equal.” It may be that “Equal” predominates because its logical formula begins with <img alt="{\exists}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cexists%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\exists}"/> and it seems easier to imagine one has found a single <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> that works rather than to exclude all <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>—an infinite number of them. </p>
<p>
I (Ken) had intended to connect this and the P=NP poll results to our <a href="https://rjlipton.wordpress.com/2019/11/11/goldbach-a-curious-conjecture/">post</a> two months ago about cases of open questions where one direction seems overwhelmingly supported both by evidence and sentiment. Whatever one thinks about the value of all the P-vs.-NP claims, they witness that P-vs.-NP is certainly not one of those cases. </p>
<p>
Last, I had intended to mention the deepest evidence in favor of “not equal.” This is the sinuous thin line between hard and easy cases of problems. Going back at least to Thomas Schaefer’s great 1978 <a href="https://en.wikipedia.org/wiki/Schaefer's_dichotomy_theorem">work</a> classifying cases of SAT, we’ve been aware of a surprisingly sharp <em>dichotomy</em> between “hard” and “in P” with seemingly no buffer zone. And the line sometimes seems to fall capriciously. In the second half of this <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit/">post</a> we mentioned Jin-Yi Cai’s work on dichotomy and a case relevant also to quantum where counting solutions to a fixed family of quadratic mod-4 polynomials in <img alt="{\{0,1,2,3\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%2C2%2C3%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{0,1,2,3\}^n}"/> is easy, but counting those in <img alt="{\{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{0,1\}^n}"/> to the same polynomial is hard. For another instance of this widely-appreciated point, see Scott Aaronson’s discussion of 3SAT <a href="https://www.scottaaronson.com/blog/?p=1720">here</a>.</p>
<p>
The point is that if P=NP (or counting is in P) then all of this finely filigreed structure would vanish—poof—an illusion all along. Like if the Mandelbrot Set became a smeary blob. But then we’d be left with: why did we have this illusion in the first place?</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We suggested that she speak to some others—people more expert than we are. For this and other reasons the article may be quite different. We hope giving our own summary here is a help. What points would you add to it?</p>
<p>
We also forgot to ask her about her own work in Teichmüller theory. Here are a <a href="https://blogs.scientificamerican.com/roots-of-unity/do-you-know-your-abcs/">couple</a> of her <a href="https://blogs.scientificamerican.com/roots-of-unity/contrasts-in-number-theory/">articles</a> on the <a href="https://en.wikipedia.org/wiki/Abc_conjecture">ABC</a> <a href="https://www.quantamagazine.org/titans-of-mathematics-clash-over-epic-proof-of-abc-conjecture-20180920">Conjecture</a>. But that is not a Clay problem and is a subject for another decade.</p>
<p>[Edited 50 to 20]</p></font></font></div>
    </content>
    <updated>2020-01-12T22:01:30Z</updated>
    <published>2020-01-12T22:01:30Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="Clay Prizes"/>
    <category term="Evelyn Lamb"/>
    <category term="interview"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-01-14T14:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/01/12/associate-professor-docent-at-st-petersburg-state-university-apply-by-february-14-2020/</id>
    <link href="https://cstheory-jobs.org/2020/01/12/associate-professor-docent-at-st-petersburg-state-university-apply-by-february-14-2020/" rel="alternate" type="text/html"/>
    <title>Associate Professor (Docent) at St.Petersburg State University (apply by February 14, 2020)</title>
    <summary>St. Petersburg State University invites applications for four associate professorships within the new Department of Mathematics and Computer Science, where students are some of the best in the world. All areas of mathematics and TCS will be considered, with preference given to candidates working in probability, mathematical physics, mathematical logic or TCS. Website: https://math-cs.spbu.ru/en/news/news-2019-12-11/ Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>St. Petersburg State University invites applications for four associate professorships within the new Department of Mathematics and Computer Science, where students are some of the best in the world.</p>
<p>All areas of mathematics and TCS will be considered, with preference given to candidates working in probability, mathematical physics, mathematical logic or TCS.</p>
<p>Website: <a href="https://math-cs.spbu.ru/en/news/news-2019-12-11/">https://math-cs.spbu.ru/en/news/news-2019-12-11/</a><br/>
Email: chebyshev.msc@gmail.com</p></div>
    </content>
    <updated>2020-01-12T19:26:19Z</updated>
    <published>2020-01-12T19:26:19Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-01-14T14:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/01/12/counting-grid-polygonalizations</id>
    <link href="https://11011110.github.io/blog/2020/01/12/counting-grid-polygonalizations.html" rel="alternate" type="text/html"/>
    <title>Counting grid polygonalizations</title>
    <summary>A polygonalization of a point set is a simple polygon having all the points as its vertices. Another way to think about it is that it’s a non-crossing Hamiltonian cycle in a complete geometric graph. My recent work on the hardness of counting polygon triangulations was motivated in large part by the problem of counting polygonalizations. Counting polygonalizations should also be hard, but I don’t know how to prove it, and I was hoping that proving other similar problems hard might either lead to more insights or at least provide something to prove a reduction from.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A <em>polygonalization</em> of a point set is a simple polygon having all the points as its vertices. Another way to think about it is that it’s a non-crossing Hamiltonian cycle in a complete geometric graph. <a href="https://11011110.github.io/blog/2019/03/12/counting-polygon-triangulations.html">My recent work on the hardness of counting polygon triangulations</a> was motivated in large part by the problem of counting polygonalizations. Counting polygonalizations should also be hard, but I don’t know how to prove it, and I was hoping that proving other similar problems hard might either lead to more insights or at least provide something to prove a reduction from.</p>

<p>More recently I’ve been attacking the problem from a different direction: looking at the number of polygonalizations for special point sets that are structured enough to make it possible to compute the answer easily while not being completely trivial. The point sets I chose were the  integer grids. Smaller grids are uninteresting:  grids don’t have any polygonalizations and  grids have only one, the outer rectangle.</p>

<p>In a  grid, any polygonalization must touch the outer points of the grid in clockwise order. And any subset of the inner  points that are between the same pair of outer points can only be connected in consecutive order. So, to describe a polygonalization, we only need to determine for each inner point which pair of outer points it is between. One way to do this is to assign letters to the consecutive pairs of outer points (for instance  along the upper left, top, and upper right, and  along the lower left, bottom, and lower right).
Then, identify each polygonalization with a sequence of letters describing the positions of its  inner points, in their left-to-right order.</p>

<p style="text-align: center;"><img alt="Polygonalizations of a 2x5 grid labeled by the positions of the three inner points in the cyclic sequence of outer points" src="https://11011110.github.io/blog/assets/2020/gridcycles.svg"/></p>

<p>This gives a one-to-one correspondence between polygonalizations and strings, where:</p>

<ul>
  <li>Each string has length .</li>
  <li>The letters of the strings are drawn from the alphabet .</li>
  <li>If a letter appears in a string, all its appearances are consecutive.</li>
  <li>The subsequence of letters from  appear in that order, as does the subsequence of letters from .</li>
  <li>It is not allowed to have both  and , nor to have both  and .</li>
</ul>

<p>These conditions are a little messy, but it’s possible to set up a recurrence for the number  of strings that don’t use the left and right side symbols, with  choices for the top (lower case) symbols,  choices for the bottom (upper case) symbols, and  labeled points in the middle.
In the recurrence below,  represents the number of symbols in the last contiguous block of equal letters, and  or  represents the number of letters that remain available with values earlier in the alphabetic order than the one that was used for this block. The recurrence is:</p>



<p>with base case . Then the total number of polygonalizations can be found by splitting up the overall length  of the string into substrings of left, top or bottom, and right symbols, using this recurrence to count the number of ways of filling in the middle part, multiplying by two for each nonempty left or right part (because there is a choice of upper or lower case for each of these parts), and summing over all splits. <a href="https://11011110.github.io/blog/assets/2020/gridcycles.py">My code to do this</a> tells me that the sequence of numbers of polygonalizations for  grids, for , is</p>



<p>(not yet in OEIS but I will submit it).</p>

<p>The next question is: how quickly does this sequence grow? It’s singly exponential (because that’s true of all 2d non-crossing geometric graph counting problems) but what is the base of the exponential? We can make a rough estimate by ignoring faxtors that are polynomial or smaller. The first estimate I tried was wrong: it was that there are roughly  ways of choosing whether each letter is upper or lower case, most of which assign roughly half of the letters to each case, and roughly  ways of choosing an upper or lower case substring of length , for a total of  choices. But this is an overestimate, because it doesn’t take into account the requirement that each letter be contiguous. It would allow strings like  which don’t correspond to polygonalizations, and that turns out to matter in the estimation.</p>

<p>To get a better estimate, we need to break down how we choose a string in a different way:</p>

<ul>
  <li>First, choose how many of the lower case letters are actually used, and then which subset of them is used.</li>
  <li>Second, do the same thing with the upper case letters.</li>
  <li>Finally, repeat  times choosing for each letter of the string whether it repeats the previous letter, takes the next lower case letter from the chosen subset, or takes the next upper case letter from the chosen subset, making sure that the first choice is not a repeat and that the total number of next-letter choices matches the number of different letters to choose from.</li>
</ul>

<p>There are polynomially many choices for how many letters of each type to use,
so the total number of choices is within a polynomial factor of the number of choices after we fix these numbers of letters to a single value, the one that maximizes the number of remaining choices. And we can estimate the number of remaining choices using Stirling’s approximation. Estimating the number of polygonalizations in this way shows that it is within a polynomial factor of , where</p>



<p>and where the number of letters we use from each case is . Here, some magic happens: It doesn’t look like  and  should have nice simple formulas, but they do. With  (the golden ratio), we get a maximum at , for which . So our sequence is approximately asymptotic to .</p>

<p>Probably if you go through the same estimation process more carefully, you can recover the polynomial factor as well. I wasn’t so careful; instead I just computed how far off from  the numbers in my sequence are, fit a line to these factors on a log-log scale, and observed that its exponent was approximately 1. So, the polynomial factor is , and numerically the sequence of numbers of polygonalizations above is a very good fit to</p>



<p>Why the golden ratio? I don’t know.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/103472952839621766">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-01-12T13:33:00Z</updated>
    <published>2020-01-12T13:33:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-01-12T23:33:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/002</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/002" rel="alternate" type="text/html"/>
    <title>TR20-002 |  Sensitivity lower bounds from linear dependencies | 

	Sophie Laplante, 

	Reza Naserasr, 

	Anupa Sunny</title>
    <summary>Recently, using spectral techniques, H. Huang proved that every subgraph of the hypercube of dimension n induced on more than half the vertices has maximum degree at least the square root of n. Combined with some earlier work, this completed a proof of the sensitivity conjecture. In this work we show how to derive a proof of Huang’s result using only linear dependency and independence of vectors associated with the vertices of the hypercube. Our approach leads to several improvements of the result. In particular we prove that in any induced subgraph of the hypercube with more than half the number of vertices, there are two vertices, one of odd parity and the other of even parity, each with at least n vertices at distance at most 2. As an application we show that for any Boolean function f, the polynomial degree of f is bounded above by the product of the 0- and 1-sensitivities, a strictly stronger statement which implies the sensitivity conjecture.</summary>
    <updated>2020-01-12T05:01:29Z</updated>
    <published>2020-01-12T05:01:29Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-01-14T14:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/01/09/women-in-theory-2020/</id>
    <link href="https://cstheory-events.org/2020/01/09/women-in-theory-2020/" rel="alternate" type="text/html"/>
    <title>Women in Theory 2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">June 16-19, 2020 Simons Institute, Berkeley, CA https://womenintheory.wordpress.com/ Submission deadline: February 7, 2020 Registration deadline: February 7, 2020 The Women in Theory (WIT) Workshop is intended for graduate and exceptional undergraduate students in the area of theory of computer science. The workshop will feature technical talks and tutorials by senior and junior women in the … <a class="more-link" href="https://cstheory-events.org/2020/01/09/women-in-theory-2020/">Continue reading <span class="screen-reader-text">Women in Theory 2020</span></a></div>
    </summary>
    <updated>2020-01-09T16:45:30Z</updated>
    <published>2020-01-09T16:45:30Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-01-14T14:21:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1547</id>
    <link href="https://theorydish.blog/2020/01/09/women-in-theory-2018-call-for-application-2/" rel="alternate" type="text/html"/>
    <title>Women in Theory 2020 Call for Application</title>
    <summary>The wonderful Women in Theory (WIT) biennial series of workshops started in 2008 and the 7th meeting will take place at   Simons Institute at Berkeley, Jun 16 – 19, 2020. Please see below the call for application. WIT is one of my favorite (if not the favorite) program in the theory community. Many in our community share my enthusiasm (and theory groups fight for the honor of hosting these meetings). The reactions from past participants leave no room for doubt – this is an important a great and experience. So if you fit the workshop’s qualifications – please do yourself a favor and apply!   The Women in Theory (WIT) Workshop is intended for graduate and exceptional undergraduate students in the area of theory of computer science. The workshop will feature technical talks and tutorials by senior and junior women in the field, as well as social events and activities. The motivation for the workshop is twofold. The first goal is to deliver an invigorating educational program; the second is to bring together theory women students from different departments and foster a sense of kinship and camaraderie. The 7th WIT workshop will take place at  Simons Institute at Berkeley, Jun 16 – [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The wonderful Women in Theory (WIT) biennial series of workshops started in 2008 and the 7th meeting will take place at   <a href="https://simons.berkeley.edu/">Simons Institute at Berkeley</a>, Jun 16 – 19, 2020. Please see below the call for application.</p>
<p>WIT is one of my favorite (if not <em>the</em> favorite) program in the theory community. Many in our community share my enthusiasm (and theory groups fight for the honor of hosting these meetings). The reactions from past participants leave no room for doubt – this is an important a great and experience. So if you fit the workshop’s qualifications – please do yourself a favor and apply!</p>
<hr/>
<p> </p>
<p>The Women in Theory (WIT) Workshop is intended for graduate and exceptional undergraduate students in the area of theory of computer science. The workshop will feature technical talks and tutorials by senior and junior women in the field, as well as social events and activities. The motivation for the workshop is twofold. The first goal is to deliver an invigorating educational program; the second is to bring together theory women students from different departments and foster a sense of kinship and camaraderie.</p>
<p>The 7th WIT workshop will take place at  <a href="https://simons.berkeley.edu/">Simons Institute at Berkeley</a>, Jun 16 – 19, 2020.</p>
<p><strong>Confirmed Speakers</strong>: <a href="https://www.cs.tau.ac.il/~mfeldman/">Michal Feldman</a> (Tel-Aviv University), <a href="https://simons.berkeley.edu/people/shafi-goldwasser">Shafi Goldwasser</a> (Simons, UC Berkeley)<br/>
<strong>Organizers</strong>: <a href="http://researcher.ibm.com/view.php?person=us-talr">Tal Rabin</a> (IBM), <a href="http://www.math.ias.edu/~shubhangi/">Shubhangi Saraf </a>(Rutgers) and <a href="mailto:lisa.zhang@nokia-bell-labs.com">Lisa Zhang</a> (Bell Labs).<br/>
<strong>Local Host Institution:  </strong><a href="https://simons.berkeley.edu/">Simons Institute at Berkeley</a>.<br/>
<b>Local Arrangements</b>:<br/>
<strong>Special Guest:</strong>  <a href="https://omereingold.wordpress.com/">Omer Reingold</a> (Stanford).<br/>
<strong>Contact us:</strong> <a href="mailto:womenintheory2016@gmail.com">womenintheory2020@gmail.com</a>.</p>
<p><strong>To apply</strong>: click <a href="https://womenintheory.wordpress.com/apply">here</a>.</p>
<p><strong>Important dates:</strong><br/>
<strong>Application deadline: </strong>Feb 7, 2020<br/>
<strong>Notification of acceptance: </strong>March 15, 2020<br/>
<strong>Workshop: </strong>June 16-19, 2020.</p></div>
    </content>
    <updated>2020-01-09T16:33:15Z</updated>
    <published>2020-01-09T16:33:15Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-01-14T14:21:14Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6812304357684621341</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6812304357684621341/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/01/silicon-valley-ethics.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6812304357684621341" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6812304357684621341" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/01/silicon-valley-ethics.html" rel="alternate" type="text/html"/>
    <title>Silicon Valley Ethics</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><b>Spoiler Alert: </b>This post has details from the final episodes of the HBO television series <a href="https://en.wikipedia.org/wiki/Silicon_Valley_(TV_series)">Silicon Valley</a><br/>
<br/>
A few times I've gotten emails from people claiming they have shown P = NP and asking whether they should keep their algorithm a secret to protect the cryptography out there. My typical response is that they should use their algorithm to mine a few bitcoins and then get back to me.<br/>
<br/>
The fictional characters of Pied Piper faced this dilemma when they AI they created "developed a general solution to discrete log in polynomial time" with some nice complexity class diagrams in the background.<br/>
<br/>
<div class="separator" style="clear: both; text-align: center;">
<a href="https://1.bp.blogspot.com/-0wC_yidITsY/Xg9oKlfZkrI/AAAAAAABu-g/OhzquwGwXbUqNDSDmGlpkU66wrjOcW62QCKgBGAsYHg/s1600/IMG_20200101_200809.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="240" src="https://1.bp.blogspot.com/-0wC_yidITsY/Xg9oKlfZkrI/AAAAAAABu-g/OhzquwGwXbUqNDSDmGlpkU66wrjOcW62QCKgBGAsYHg/s320/IMG_20200101_200809.jpg" width="320"/></a></div>
<br/>
Pied Piper was about to roll out its new internet, a distributed network that communicated between cell phones based on a compression algorithm developed by Pied Piper's CEO. Rolling out the network would reveal even more advanced compression based on breaking discrete log. "If we cancel it or shut it down, then others will try to copy or reverse engineer everything that we've built ... Our launch has to fail, publicly and spectacularly."<br/>
<br/>
But here comes the P v NP dilemma: "And what about all the other stuff we're gonna do? I mean, give internet to underserved communities, students in the homework gap, refugees, genomic research. Pied Piper can help scientists cure cancer."<br/>
<br/>
I'd take broken encryption over cancer any day. You can still do encryption even if P = NP, one-time pads distributed via USB drives or quantum. And cancer sucks.<br/>
<br/>
They should have mined a few bitcoins.</div>
    </content>
    <updated>2020-01-09T01:35:00Z</updated>
    <published>2020-01-09T01:35:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-01-14T11:01:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=19104</id>
    <link href="https://gilkalai.wordpress.com/2020/01/08/the-brown-erdos-sos-1973-conjecture/" rel="alternate" type="text/html"/>
    <title>The Brown-Erdős-Sós 1973 Conjecture</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Greetings from Oberwolfach.  This week, there is a great meeting here on combinatorics. In this post I want to state the Brown-Erdős-Sós conjecture and one of its variants. The trigger was a beautiful talk I heard from Lior Gishboliner on … <a href="https://gilkalai.wordpress.com/2020/01/08/the-brown-erdos-sos-1973-conjecture/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: left;">Greetings from Oberwolfach.  This week, there is a great meeting here on combinatorics. In this post I want to state the Brown-Erdős-Sós conjecture and one of its variants. The trigger was a beautiful talk I heard from <a href="http://www.math.tau.ac.il/~liorgis1/">Lior Gishboliner</a> on some impressive progress toward this very difficult problem.  Yet the solution of the problem seems far, and some people even doubt if the conjecture is true. Here is the paper <a href="https://arxiv.org/abs/1912.08834">“A New Bound for the Brown-Erdős-Sós problem</a>” by David Conlon, Lior Gishboliner, Yevgeny Levanzov, and Asaf Shapira.</p>
<h2>The Brown-Erdős-Sós conjecture and strong versions</h2>
<p><a href="https://gilkalai.files.wordpress.com/2019/01/pv.jpg"><img alt="" class="alignnone size-medium wp-image-16790" height="169" src="https://gilkalai.files.wordpress.com/2019/01/pv.jpg?w=300&amp;h=169" width="300"/></a></p>
<p>Lior’s talk reminded me that Peter Frankl and Voita Rodl told me in the late 80s about strong variants of the Brown-Erdős-Sós problem that implies the Szemeredi theorem. With the kind help of Voita Rodl  who is here I can tell you about two such variants. (But I am solely responsible for all mistakes.)</p>
<p>Let <em>f(n,v,e)</em> be the largest number of edges in an 3-uniform hypergraph with <em>n</em> vertices and no v vertices spanning e edges or more.</p>
<p><strong>Brown-Erdős-Sós Conjecture:</strong> <img alt="f(n,e+3,e)=o(n^2)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%2Ce%2B3%2Ce%29%3Do%28n%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(n,e+3,e)=o(n^2)"/>, for every <img alt="e\ge 3" class="latex" src="https://s0.wp.com/latex.php?latex=e%5Cge+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e\ge 3"/>.</p>
<p>For <img alt="e=3" class="latex" src="https://s0.wp.com/latex.php?latex=e%3D3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e=3"/> the BES conjecture is the famous Ruzsa-Szemeredi theorem (that implies Roth’s theorem on 3-term arithmetic progressions). For <img alt="e=4" class="latex" src="https://s0.wp.com/latex.php?latex=e%3D4&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e=4"/> the conjecture is painfully open (and is stronger than Szemeredi theorem for 4-term APs).</p>
<p>Here is an even stronger conjectures that implies Szemeredi theorem arithmetic progressions of arbitrary length.</p>
<p>Consider the following 3-uniform hypergraph <em>H(v)</em> with<em> v+3</em> vertices and <em>v</em> edges: Start with a path with v+1 vertices (and v edges). Color the edges alternately with red and brown and add a red vertex and a brown vertex. Now form a 3-uniform hypergraph whose edges are the red edges of the path joined with the red vertex and the brown edges of the graph joined with the brown vertex.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/01/szem-voita1.png"><img alt="" class="alignnone size-medium wp-image-19110" height="205" src="https://gilkalai.files.wordpress.com/2020/01/szem-voita1.png?w=300&amp;h=205" width="300"/></a></p>
<p><strong>Conjecture:</strong> Every <em>H(v)</em>-free 3-uniform hypergraph with <em>n</em> vertices has <img alt="o(n^2)" class="latex" src="https://s0.wp.com/latex.php?latex=o%28n%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="o(n^2)"/> vertices.</p>
<p>For another strengthening of the BES-conjecture that implies the Szemeredi theorem see the paper <a href="https://mathscinet.ams.org/mathscinet/search/publdoc.html?arg3=&amp;co4=AND&amp;co5=AND&amp;co6=AND&amp;co7=AND&amp;dr=all&amp;pg4=AUCN&amp;pg5=TI&amp;pg6=PC&amp;pg7=ALLF&amp;pg8=ET&amp;review_format=html&amp;s4=Rodl%20and%20Erdos&amp;s5=&amp;s6=&amp;s7=&amp;s8=All&amp;sort=Newest&amp;vfpref=html&amp;yearRangeFirst=&amp;yearRangeSecond=&amp;yrop=eq&amp;r=2&amp;mx-pid=1393704">A remark of Pisier-type theorems</a> of Erdos, Nesetril and Rodl. Famously, the celebrated hypergraph regularity lemma (and associated counting lemma) implies another Turan type theorem for hypergraphs that in turn implies Szemeredi’s theorem and its high dimensional extensions.</p>
<h2>The new bounds by Conlon, Gishboliner, Levanzov, and Shapira</h2>
<p><a href="https://gilkalai.files.wordpress.com/2020/01/bes2.png"><img alt="" class="alignnone size-full wp-image-19114" height="173" src="https://gilkalai.files.wordpress.com/2020/01/bes2.png?w=640&amp;h=173" width="640"/></a></p>
<p>Although the theorem is for 3-uniform hypergraph the proof relies on the hypergraph regularity lemma for higher dimensional hypergraphs.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/01/img_6303.jpg"><img alt="" class="alignnone size-medium wp-image-19122" height="168" src="https://gilkalai.files.wordpress.com/2020/01/img_6303.jpg?w=300&amp;h=168" width="300"/></a><a href="https://gilkalai.files.wordpress.com/2020/01/img_6302.jpg"><img alt="" class="alignnone size-medium wp-image-19123" height="225" src="https://gilkalai.files.wordpress.com/2020/01/img_6302.jpg?w=300&amp;h=225" width="300"/></a></p>
<p>With Voita in 1985 (or 86) and today (2020)</p></div>
    </content>
    <updated>2020-01-08T19:58:17Z</updated>
    <published>2020-01-08T19:58:17Z</published>
    <category term="Combinatorics"/>
    <category term="Asaf Shapira"/>
    <category term="B. Nagle"/>
    <category term="David Conlon"/>
    <category term="Endre Szemeredi"/>
    <category term="Imre Z. Ruzsa"/>
    <category term="J. Skokan"/>
    <category term="Lior Gishboliner"/>
    <category term="M. Schacht"/>
    <category term="Paul Erdos"/>
    <category term="Peter Frankl"/>
    <category term="Tim Gowers"/>
    <category term="Vera T. S&#xF2;s"/>
    <category term="Voita Rodl"/>
    <category term="W. G. Brown"/>
    <category term="Yevgeny Levanzov"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-01-14T14:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16539</id>
    <link href="https://rjlipton.wordpress.com/2020/01/08/no-password-encryption/" rel="alternate" type="text/html"/>
    <title>No Password Encryption</title>
    <summary>Who can remember passwords anyway? Real Bernie Sanders reaction source Larry David is an American comedian. He was the lead writer and producer of the Seinfeld TV series. During the previous and current election cycles he has played Presidential candidate Bernie Sanders in skits on Saturday Night Live. His “Bernie” rails about issues with passwords. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Who can remember passwords anyway?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/01/larrydavidasbernie-1.jpg"><img alt="" class="alignright wp-image-16542" height="140" src="https://rjlipton.files.wordpress.com/2020/01/larrydavidasbernie-1.jpg?w=200&amp;h=140" width="200"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Real Bernie Sanders reaction <a href="https://abcnews.go.com/ThisWeek/video/sen-bernie-sanders-reacts-larry-david-snl-impersonation-34555440">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Larry David is an American comedian. He was the lead writer and producer of the <em>Seinfeld</em> TV series. During the previous and current election cycles he has played Presidential candidate Bernie Sanders in skits on <em>Saturday Night Live</em>. His “Bernie” rails about issues with passwords.</p>
<p>
Today I want to talk about reducing our dependence on passwords.<br/>
<span id="more-16539"/></p>
<p>
On SNL in October 2015, his “Bernie” branched off the topic of Hillary Clinton’s e-mails to famously <a href="https://www.geekwire.com/2015/this-snl-skit-starring-larry-david-as-bernie-sanders-claiming-to-be-bad-at-tech-is-a-big-youtube-hit/">rant</a>:</p>
<blockquote><p><b> </b> <em> What’s the deal with e-mails anyway? I forgot my password the other day, so they say “We’ll email you a new one.” But I can’t get into my email to get the password. I mean, talk about a ball-buster. </em>
</p></blockquote>
<p/><p>
In the SNL <a href="https://deadline.com/2019/12/saturday-night-live-spoofs-democratic-debate-1202815760/">cold open</a> about the most recent debate three weeks ago, “Bernie” reprised the complaint:</p>
<blockquote><p><b> </b> <em> Apple lies, Amazon lies, even my I-Phone lies. Every time it says it’s at 1 percent battery, it stays on for at least 20 minutes. The other times it’s at 7 percent—it shuts down immediately. Apple, what are you trying to hide? And what’s my password?!! </em>
</p></blockquote>
<p/><p>
Passwords are still needed. But, and this is the key, we wish to reduce our dependency on them and still be safe. Also the method fits nicely with today’s need for access to encrypted information by law enforcement. We have already discussed this recently <a href="https://rjlipton.wordpress.com/2019/12/03/end-to-end-encryption-a-problem/">here</a>. The trick is to make information safe enough to please us, but not have as many passwords that we need to remember. “Bernie” would be happy. </p>
<p/><p>
Let’s next review why we use passwords at all.</p>
<p>
</p><p/><h2> Why We Have Passwords </h2><p/>
<p/><p>
Passwords have been used forever. See our friends at Wikipedia for a <a href="https://en.wikipedia.org/wiki/Password">story</a> by Polybius, an ancient historian, on how eons ago the Roman military used watchwords. Passwords have been used since the beginning of computing, forever in our world, to safeguard computer systems. The MIT computer system, CTSS, used them starting in 1961 to protect users. This is the reason Fernando Corbató is credited with the invention of computer passwords. </p>
<p>
The goal of passwords and watchwords, is simple: Control access. Both the real and fake Bernies use e-mail passwords to keep their communications private. We try to select good passwords, but the management of them can be challenging. It’s not just that people like “Bernie” forget theirs—it’s that our efforts to keep them in our heads often make them too easy to figure out. Jimmy Kimmel showed this in a 2017 <a href="https://www.cnet.com/news/jimmy-kimmel-gets-people-to-give-him-their-passwords/">segment</a> on his own show.</p>
<p>
</p><p/><h2> Passwords are Dead </h2><p/>
<p/><p>
Managing passwords has lead to the theme, “passwords are dead.” If they are dead, then we must have alternative methods. Some are based on biometrics such as fingerprints and eye scans; others are based on additional hardware. Fingerprints are one of the most popular, but have major drawbacks: </p>
<ul>
<li>
They can be unreliable. <p/>
</li><li>
They can be attacked. <p/>
</li><li>
They cannot be changed.
</li></ul>
<p>I can attest that fingerprints are unreliable. I use a Global Pass to speed my re-entry into the US. After my flight arrives at the airport, I use a Global Pass machine that checks my fingerprints. The machine fails to recognize my fingerprints, every time. This forces me to talk to a custom agent and convince them am who I claim to be. Thus defeating the reason for using the Global Pass.</p>
<p>
A 2012 <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-817.html">study</a> by Joseph Bonneau, Cormac Herley, Paul Oorschot, and Frank Stajano tilted “The Quest to Replace Passwords” compared 35 alternatives to passwords on their security, usability, and deployability. They showed that most do better on security, many do better on usability. But all alternatives do worse on deployability. The authors conclude: </p>
<blockquote><p><b> </b> <em> “Marginal gains are often not sufficient to reach the activation energy necessary to overcome significant transition costs, which may provide the best explanation of why we are likely to live considerably longer before seeing the funeral procession for passwords arrive at the cemetery.” </em>
</p></blockquote>
<p>Colorful language, but the point is that passwords are hard to beat.</p>
<p>
</p><p/><h2> How to Remove Them </h2><p/>
<p/><p>
I have a proposal on how to avoid reliance on just passwords. At least for many applications. The idea is: You would use a password and add a secret password that you do not remember. You do not even know this password. You do not write it down. Clearly, this is additional security if you do not even know the secret password. No one can steal it from you or guess it.</p>
<p>
So the issue is: How do you access your own stuff without accessing the secret password? The answer is simple: You run a program that tries all the possibilities. Let this take some time <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>. This point is that for many situations you will not mind if <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is large. For example, when the access that you are protecting occurs rarely. The security that this affords is based on this: An attacker can and will definitely be able to get access by expending <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> time, if they also know you standard password. But this protects against mass attacks. An attacker may be able to get your data, but will not be able to get millions of people’s data.</p>
<p>
Let’s examine a few applications: </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>End-to-end Encryption:</i> The secret password could be protected by a <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> of order years or even decades of computer time. This would allow law enforcement to get information it needs, while stopping causal attackers.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>Recovery of your data:</i> The secret password could be protected by a <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> of order days of computer time. Your computer backup data is secured by such a password. Then it crashes. You need to run such a computation to recover the data. In many situations this would be fine.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>A banking site:</i> You would still use a good password. The extra secret one could have a <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> of an hour say. Then when you need to pay an on-line bill, you would have to wait a reasonable amount of time.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>Not need to place passwords in your will:</i> Omada King, in his 2013 <a href="https://www.amazon.com/Making-Identity-Passwords-Personal-Transformation/dp/1479781177">book</a> <em>The Making Identity</em>, wrote: </p>
<blockquote><p><em>[A]ccording to a recent survey from the University of London, one in ten people are now leaving their passwords in their wills to pass on this important information when they die. </em>
</p></blockquote>
<p/><p>
This could be avoided by using a secret password with <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> of order <em>months</em>. When it is needed the heirs would run the recovery program and get the access they need.</p>
<p>
</p><p/><h2> Ken’s Words </h2><p/>
<p/><p>
Ken points out that merely trying more than a few possibilities will usually generate a suspicious-activity message and usually a shutdown. So a system would have to be set up to permit “self-hacking.” Ken has a small rotation of “password extenders” not written down and often has to try two or three before gaining access to non-financial sites where he is registered.</p>
<p>
Ken is sometimes asked to monitor chess tournaments for possible cheating using the initial “screening” phase of his system. This phase requires minimal effort from Ken as it is supposed to be automated on a server that any chess tournament director would have quick access to, but for various reasons the International Chess Federation (FIDE) has not (yet) erected such a server. So Ken posts the auto-generated reports at a private location known only to him and the arbiter(s) of a particular tournament.</p>
<p>
Ken does not maintain a password scheme for the reports. This would become a headache precisely because of the difficulties people have with passwords. Ken does not want to assume the responsibility for managing them. Instead he includes a “quasi-password” as part of the URL he creates. These are often multi-lingual puns or references to quirky artists or factoids in the home country of the tournament. Being memorable and unusual enables the arbiter’s browser to learn the word and link without collision. </p>
<p>
This maximizes convenience: For Ken to e-mail reports or zipped folders as attachments would be cumbersome under daily updates. With Ken’s way, the arbiter can view updates even without having to pull up Ken’s previous e-mails with links, just by typing some letters of the weird word in the browser address bar. Hiding directory listings and a “no robots” directive completes minimal security for temporary use during the tournament.</p>
<p>
</p><h2> The First and Last Word? </h2><p/>
<p>We mentioned Fernando Corbató having originated passwords.  But before his passing last July 12, he came to regret them.  <a href="https://www.wsj.com/articles/despite-data-thefts-the-password-endures-1400725381">Here</a> he is quoted:</p>
<blockquote><p>
“It’s become a kind of nightmare.  I don’t think anybody can possibly remember all the passwords.”
</p></blockquote>
<p/><h2> Open Problems </h2><p/>
<p/><p>
How practical do you find the “no-password” idea? At least the above suggestion may save us from having to place passwords in our wills. </p>
<p/></font></font></div>
    </content>
    <updated>2020-01-08T05:31:02Z</updated>
    <published>2020-01-08T05:31:02Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Bernie Sanders"/>
    <category term="crypto"/>
    <category term="Fernando Corbato"/>
    <category term="Larry David"/>
    <category term="passwords"/>
    <category term="security"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-01-14T14:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1291</id>
    <link href="https://thmatters.wordpress.com/2020/01/06/tcs-job-market-profiles/" rel="alternate" type="text/html"/>
    <title>TCS job market profiles</title>
    <summary>Earlier this academic year, CATCS started a project to collect the profiles of TCS candidates on the job market this academic year. Those profiles can now be found at this webpage. The webpage offers a limited amount of search functionality, and we are continuing to improve the layout and add features. If you are at […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Earlier this academic year, CATCS <a href="https://thmatters.wordpress.com/2019/10/02/a-solicitation-for-tcs-job-market-profiles/">started a project</a> to collect the profiles of TCS candidates on the job market this academic year. Those profiles can now be found at <a href="http://tcsjobcandidates.web.unc.edu/job-candidates/">this webpage</a>. The webpage offers a limited amount of search functionality, and we are continuing to improve the layout and add features.</p>
<p>If you are at an institution that is hiring in theory, please feel free to use this database and share it with your colleagues as appropriate.</p>
<p>If you are going on the job market and would like to fill out a profile, please use <a href="http://tcsjobcandidates.web.unc.edu/application/">this form</a>. We will periodically update the webpage adding newer profiles.</p>
<p>Many thanks to Nicole Immorlica, Jack Snoeyink, and Chris Umans for putting together the website, and to all those who contributed their profiles. Questions and comments may be emailed to Shuchi Chawla.</p></div>
    </content>
    <updated>2020-01-06T23:23:25Z</updated>
    <published>2020-01-06T23:23:25Z</published>
    <category term="for PhD students"/>
    <category term="postdocs"/>
    <category term="Uncategorized"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2020-01-14T14:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1096</id>
    <link href="http://corner.mimuw.edu.pl/?p=1096" rel="alternate" type="text/html"/>
    <title>We are looking for YOU to join our award-winning scientific team!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Have you just graduated your PhD and are considering a post-doc position in theory of informatics? Science is your passion and you would like to spend most of your post-doc researching on whatever interests you? You are in the right … <a href="http://corner.mimuw.edu.pl/?p=1096">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Have you just graduated your PhD and are considering a post-doc position in theory of informatics? Science is your passion and you would like to spend most of your post-doc researching on whatever interests you? You are in the right place.</p>



<p>At MIM UW we offer you:</p>



<ol><li>Great FREEDOM OF CHOICE related to what to work on;</li><li>Just A FEW or NO teaching duties;</li><li>A lot of TIME FOR RESEARCH;</li><li>Chance to cooperate with VERY EXPERIENCED and TALENTED scientists;</li><li>FRIENDLY environment;</li><li>Excellent SUPPORT from our administrative staff;</li></ol>



<p>If you still hesitate, here are two interviews with former post-docs in ERC GRANT TUgbOAT.</p>



<p>Watch a video and find out more about benefits of working at MIM UW.</p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div>Watch an interview with Krzysztof Fleszar, post-doc at the Faculty of Mathematics, Informatics and Mechanics, University of Warsaw and find out how the participation in the project has changed his life. <br/>For more information about ERC GRANT  TUgbOAT visit our blog <a href="https://duch.mimuw.edu.pl/~tugboat/" rel="noreferrer noopener" target="_blank">https://duch.mimuw.edu.pl/~tugboat/</a>.</figure>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div>Adam Karczmarz told us about his experience as a post-doc at the Faculty of Mathematics, Informatics and Mechanics, University of Warsaw in ERC GRANT „Towards Unification of Algorithmic Tools”. <br/>To find out more about TUgbOAT project visit our blog <a href="https://duch.mimuw.edu.pl/~tugboat/" rel="noreferrer noopener" target="_blank">https://duch.mimuw.edu.pl/~tugboat/</a>.</figure></div>
    </content>
    <updated>2020-01-06T19:39:00Z</updated>
    <published>2020-01-06T19:39:00Z</published>
    <category term="post"/>
    <category term="Uncategorized"/>
    <author>
      <name>Renata Czarniecka</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2020-01-13T23:21:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/01/06/complexity-theory-with-a-human-face/</id>
    <link href="https://cstheory-events.org/2020/01/06/complexity-theory-with-a-human-face/" rel="alternate" type="text/html"/>
    <title>Complexity Theory with a Human Face</title>
    <summary>September 1-4, 2020 Czech Republic http://users.math.cas.cz/talebanfard/workshop2020/ The workshop consists of excellent speakers giving enlightening tutorials on delectable aspects of complexity theory which will take place in Tábor, Czech Republic. The event is co-organized with Krajíček’s Fest celebrating the 60th birthday of Jan Krajíček.</summary>
    <updated>2020-01-06T13:42:39Z</updated>
    <published>2020-01-06T13:42:39Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-01-14T14:21:12Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-32902056.post-8905653443034252450</id>
    <link href="http://paulwgoldberg.blogspot.com/feeds/8905653443034252450/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=32902056&amp;postID=8905653443034252450" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/32902056/posts/default/8905653443034252450" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/32902056/posts/default/8905653443034252450" rel="self" type="application/atom+xml"/>
    <link href="http://paulwgoldberg.blogspot.com/2020/01/on-leaving-eu.html" rel="alternate" type="text/html"/>
    <title>On leaving the EU</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div dir="ltr" style="text-align: left;">For someone based in the UK who works in any kind of international market, it looks reasonable to consider how his business strategy should be affected by leaving the EU. In the case of CS theory research, this perhaps runs counter to an idealised view in which all research is global, and should not be affected by squalid political considerations. The interest inherent in any specific problem or result ought to be independent of where it was studied. On a related note, it may be felt that it’s the research topic that chooses the researcher, not the researcher who chooses the topic. On the other hand, even in CS theory, ones political environment and associated social networks may have a stronger effect than we would like to acknowledge.<br/><br/>When I was a graduate student, Algorithms and Computational Complexity was relatively under-represented in the UK, compared with today. The UK theory community was dominated by so-called “Euro-theory”, which at the time did not seem to exhibit obvious points of contact with algorithms research. People like me had to look west for assurance that our research was of wider interest than what was apparent in our own backyard. To further justify that west-looking approach, it was clear that the USA was, in terms of research, the undisputed world leader. Then as now, it collected the lion’s share of Nobel prizes. It had industrial research labs producing leading CS theory research, such as IBM, AT&amp;T, and NEC, while Europe had nothing similar. For me, this sowed the seeds of a defiantly Atlanticist attitude to CS theory research — appropriate for Brexit Britain, perhaps? — that the best way to pursue high-quality research was via links with colleagues in the USA.<br/><br/>Fast-forward to about five years ago, and my attitude had softened. The UK’s algorithms-and-complexity community steadily grew, and is much larger than it was in the early 90’s. Travel within Europe is relatively quick and cheap, with no visa issues. The European research community became a bit of a comfort-zone, while the USA’s research ethic seemed comparatively intense and high-pressure. The perennial question of “Where’s my next STOC/FOCS paper going to come from?” has always seemed less urgent in Europe.<br/><br/>The EU has attempted to unify Europe’s academic research activity, which is supported by diverse governments, and gives rise to diverse complaints among European colleagues. I am reminded of the “<a href="https://en.wikipedia.org/wiki/Anna_Karenina_principle">unhappy families</a>” quote from <i>Anna Karenina</i>. In an attempt to give it a bit of unity, there’s some EU funding for research, concerning which we have this criticism from <a href="https://www.nobelprize.org/uploads/2018/06/geim_lecture.pdf">Andre Geim’s Nobel prize speech</a>:  “I can offer no nice words for the EU Framework programmes which, except for the  European Research Council, can be praised only by Europhobes for discrediting the whole idea of an effectively working Europe.” For my part, I recently tried to get a grant from the European Research Council but they turned me down. If I were a rational agent, I should at this point be one of Geim’s Europhobes; of course in reality things are not quite so simple. But at this point I reckon the US research funding system looks like the least worst. <a href="http://occamstypewriter.org/athenedonald/2019/12/16/post-election-christmas-reading-list/">A recent article</a> at Athene Donald’s blog discusses the post-EU era and the idea of a DARPA-like research agency for the UK.<br/><br/>So, leaving the EU looks like an opportune point to dust off the above-mentioned “Atlanticist attitude”. These days China is also becoming more important. But I hope that Europe will not give up on us, but will compete strenuously with them for our attention.</div></div>
    </content>
    <updated>2020-01-06T12:21:00Z</updated>
    <published>2020-01-06T12:21:00Z</published>
    <category scheme="http://www.blogger.com/atom/ns#" term="aggregator"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="politics"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="UK"/>
    <author>
      <name>Paul Goldberg</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/10952445127830395305</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-32902056</id>
      <category term="aggregator"/>
      <category term="UK academia"/>
      <category term="politics"/>
      <category term="meetings"/>
      <category term="research"/>
      <category term="research directions"/>
      <category term="conferences"/>
      <category term="funding"/>
      <category term="people"/>
      <category term="rant"/>
      <category term="economics"/>
      <category term="academia"/>
      <category term="forecasts"/>
      <category term="internet"/>
      <category term="advertisement"/>
      <category term="game theory"/>
      <category term="trips"/>
      <category term="University of Liverpool"/>
      <category term="editorial"/>
      <category term="technical"/>
      <category term="times higher"/>
      <category term="announcements"/>
      <category term="publications"/>
      <category term="teaching"/>
      <category term="work"/>
      <category term="postgraduate research"/>
      <category term="technology"/>
      <category term="books"/>
      <category term="social choice"/>
      <category term="talks"/>
      <category term="CACM"/>
      <category term="games"/>
      <category term="liverpool"/>
      <category term="open problems"/>
      <category term="problems"/>
      <category term="research assessment"/>
      <category term="tongue in cheek"/>
      <category term="administration"/>
      <category term="architecture"/>
      <category term="email"/>
      <category term="environment"/>
      <category term="mechanism design"/>
      <category term="puzzles"/>
      <category term="web sites"/>
      <category term="URLs"/>
      <category term="USS"/>
      <category term="education"/>
      <category term="higher education"/>
      <category term="oxford"/>
      <category term="schools"/>
      <category term="UCU"/>
      <category term="UK"/>
      <category term="go"/>
      <category term="intellectual property"/>
      <category term="jobs"/>
      <category term="league table"/>
      <category term="math education"/>
      <category term="money"/>
      <category term="products"/>
      <category term="science"/>
      <category term="students"/>
      <category term="Liberal democrats"/>
      <category term="XJTLU"/>
      <category term="behavioural economics"/>
      <category term="china"/>
      <category term="current affairs"/>
      <category term="diversity"/>
      <category term="epsrc"/>
      <category term="family"/>
      <category term="geography"/>
      <category term="holidays"/>
      <category term="joke"/>
      <category term="misc"/>
      <category term="nerd humour"/>
      <category term="pensions"/>
      <category term="predictions"/>
      <category term="proposals"/>
      <category term="psephology"/>
      <category term="region"/>
      <category term="student finance"/>
      <category term="visits"/>
      <category term="warwick"/>
      <category term="web"/>
      <category term="weekends"/>
      <category term="wikipedia"/>
      <category term="writing"/>
      <author>
        <name>Paul Goldberg</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/10952445127830395305</uri>
      </author>
      <link href="http://paulwgoldberg.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/32902056/posts/default/-/aggregator" rel="self" type="application/atom+xml"/>
      <link href="http://paulwgoldberg.blogspot.com/search/label/aggregator" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/32902056/posts/default/-/aggregator/-/aggregator?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>theoretical computer science, economics, and academic life in general. Writing in personal capacity, not representing my employer or other colleagues</subtitle>
      <title>Paul Goldberg</title>
      <updated>2020-01-10T13:16:01Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8980814517027039027</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8980814517027039027/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/01/the-wikipedia-entry-on-np-intermediary.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8980814517027039027" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8980814517027039027" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/01/the-wikipedia-entry-on-np-intermediary.html" rel="alternate" type="text/html"/>
    <title>The Wikipedia Entry on NP-Intermediary Problems lists one of mine! I'm not bragging about it.</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I recently needed to look at what NP problems were possibly intermediary (neither in P nor NP-complete). So I went to Wikipedia and found <a href="https://en.wikipedia.org/wiki/NP-intermediate">this</a>.<br/>
<br/>
They had many problems, though some I had never heard of. Those that I had never heard of<br/>
<br/>
<i>should they be on the list?</i><br/>
<i><br/>
</i> That is, are they natural? That is hard to define rigorously, but I will take you through my train of thought as I read the first few:<br/>
<br/>
<b>Factoring Integers</b>. Yes, quite possibly intermediary: If  its NPC then PH collapses, and, at least so far, does not seem to be in P.  (the NPC--&gt; PH collapse result: We take<br/>
<br/>
FACT = { (n,x) : n has a nontrivial factor ≤ x }<br/>
<br/>
FACT is clearly in NP:<br/>
a complete factorization of n provides evidence that some nontrivial factor is \le x.<br/>
<br/>
FACT is clearly in coNP:<br/>
a complete factorization of n provides evidence that no nontrivial factor is \le x<br/>
<br/>
so if FACT is NP-complete then SAT is in coNP.<br/>
<br/>
Factoring is clearly an important and well studied problem. It even has its own Wikipedia entry!<br/>
<br/>
<b>Discrete Log</b>. Similar to Factoring. And it is also an important and well studied problem. It even has its own Wikipedia Entry!<br/>
<br/>
<b>Isomorphism Problems</b> They list Group and Ring isomorphism. They don't list Graph, which is odd. (ADDED LATER- my bad, they do mention Graph Isom in the section on Graph Algorithms) Anyway, if Graph Isom is NPC then PH collapses, and, at least so far, there is no algorithm for Graph Isom in P. (I do not think it is know if Group Isom NPC means PH collapses, or if Ring Isom NPC means PH collapses---if you know of such a proof leave a comment and a pointer to it.)<br/>
<br/>
Graph Isomorphism is a well studied problem and seems important and natural (I don't know if Graph Isomorphism has any real applications they way that factoring and DL do).  It even has its own Wikipedia entry! Group and Ring Isomorphism also seem important and natural. And they have their own Wikipedia entry!<br/>
<br/>
<b>Numbers in Boxes Problem </b>My first reaction-Gee, whats that? For the Factoring, DL, and Isomorphism they did not define the problem-- they gave pointers to the Wikipedia entries on them. For this one there was no Wikipedia entry. There was one reference. I went to it. <i>It was a blog entry of mine</i>! Here it is: <a href="https://blog.computationalcomplexity.org/2010/07/what-is-complexity-of-these-problems.html">her</a>e, and to save you time I'll say what it is:<br/>
<br/>
{ (1<sup>n</sup>,1<sup>k</sup>) : you can partition 1,...,n into k boxes so that no box has x,y,z with x + y = z }<br/>
<br/>
Is this problem important? Does it exist anywhere outside of my blog entry? Yes--- a special case of it was in <a href="https://www.amazon.com/Doctor-Eccos-Cyberpuzzles-Dennis-Shasha/dp/0393325415/ref=sr_1_2?keywords=Dr.+Ecco&amp;qid=1577931523&amp;s=books&amp;sr=1-2">Dr. Ecco's Cyperpuzzles by Dennis Shasha</a> (note- Dennis was a classmate of mine in graduate school at Harvard). I think the case was to try to partition {1,...,100} as best you can. Actually I first saw the case of the problem in his book and then generalized it.<br/>
<br/>
The problem is sparse so if it was NP-complete then P = NP, very good evidence that its not NPC. And its been studied for thousands of years, with people looking for poly time algorithms (I think Pythagoras studied it) without success, so its almost surely not in P. OR that last sentence was complete nonsense. Indeed, I don't think anyone has studied the problem computationally, or, for that matter, at all. So the evidence that its not in P is... sparse.<br/>
<br/>
But its worse than that. One could devise MANY sparse problems that are, since spares, likely NOT NPC, and hardly studied, so as-of-now, not in P. Should those count? Only if (a) more people study them so there is an attempt to point to to get it into P, and (b) the problem is natural (which is hard to define).<br/>
<br/>
Note that I can vary the problem: x+2y=z (this relates to lower bounds on VDW numbers)<br/>
or any other combination of x,y,z or more that I like.<br/>
<br/>
<br/>
<br/>
This raises a question:<br/>
<br/>
<b>When is a problem worthy of being put on lists of problems?</b><br/>
<b><br/>
</b> Here are some possibly criteria. One can take ANDS and ORS of them.<br/>
<br/>
1) The problem has a Wikipedia entry. This might fall victim to Goodhearts law: when a measure becomes a target, it ceases to be a measure.  That is, I could make a Wikipedia entry on the Number-in-boxes problem and then say LOOK, its on Wikipedia!<br/>
<br/>
2) More than X people have worked on the problem for some value of X. But here is a reason this might not be a good criteria: look at the problem<br/>
<br/>
{ α : α is a reg expression that allows numbers (so a<sup>1000</sup> is fine, makes reg expressions  VERY succint) such that L(α)=Σ<sup>*</sup> }<br/>
<br/>
This problem looks natural, and was proven by Meyer and Stockmeyer to be EXPSPACE complete.<br/>
That is the only paper on this problem, yet the problem really does look natural, and the result is rightly celebrated as a natural problem that is provably not in P.<br/>
<br/>
3) When people in the field look at the problem they say YEAH, thats a good problem.<br/>
<br/>
4) The problem relates to other problems or other fields.<br/>
<br/>
I doubt the Number-in-boxes problem satisfies any of these criteria. The variant with x+2y=z relates to Ramsey Theory. Great.<br/>
<br/>
NOW, back to the list-- I won't go through any more on the list, but I note that for some of them the only reference seems to be a conversation on stack-exchange.  Some of those end up referring to real papers so are more likely natural, but some do not.<br/>
<br/>
Having said that, is there any harm in the list having on it some problems that are not ... worthy? Is that even the right word to use?<br/>
<br/>
Note that I don't have strong opinions on any of these matters, I am just wondering what criteria Wikipedia, and other sources, uses, when they have lists of problems.<br/>
<b><br/>
</b> <b><br/>
</b> <b><br/>
</b> <b><br/>
</b></div>
    </content>
    <updated>2020-01-06T04:54:00Z</updated>
    <published>2020-01-06T04:54:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-01-14T11:01:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1250</id>
    <link href="https://ptreview.sublinear.info/?p=1250" rel="alternate" type="text/html"/>
    <title>News for December 2019</title>
    <summary>Happy new year! And now for the last post of 2019 papers. We have found a diverse collection of six papers, ranging from classic property testing topics to new perspectives on sublinear computation. Sublinear Optimal Policy Value Estimation in Contextual Bandits by Weihao Kong, Gregory Valiant, Emma Brunskill (arXiv). This isn’t our usual sublinear paper, but it […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Happy new year! And now for the last post of 2019 papers. We have found a diverse collection of six papers, ranging from classic property testing topics to new perspectives on sublinear computation. </p>



<p><strong>Sublinear Optimal Policy Value Estimation in Contextual Bandits</strong> by Weihao Kong, Gregory Valiant, Emma Brunskill (<a href="https://arxiv.org/abs/1912.06111">arXiv</a>). This isn’t our usual sublinear paper, but it is definitely of interest to us sublinear folk. Let’s start with a stripped down definition of the problem (or rather, game). There are \(K\) “arms”, where the \(i\)th arm is represented by an unknown vector in \(\beta_i \in \mathbb{R}^d\). We are presented with a “context”, which is a vector \(x \in \mathbb{R}^d\). Our job is to choose an arm \(i \in [K]\). We get the reward \(x \cdot \beta_i\) (with some noise added). The contexts appears from a known distribution. To aid us, we observe the rewards of \(N\) iid contexts, so we observe a total of \(T = KN\) rewards. There has been much work on figuring out the minimum value of \(T\) required to learn the optimal policy. One requires at least \(d\) (the dimension) samples to estimate any of the arm vectors. This papers shows that one can actually estimate the expected reward of the optimal policy, without being able to describe it, with sublinear in \(d\) (technically, \(\widetilde{O}(\sqrt{d})\)) samples. We see this a lot in property testing, where producing the “optimal” solution for a problem requires linear-in-dimension samples, but estimating the optimal value is much cheaper (consider, for example, the situation of linearity testing, where we wish to find the closest linear function).</p>



<p><strong>Sublinear Time Numerical Linear Algebra for Structured Matrices </strong>by Xiaofei Shi and David P. Woodruff (<a href="https://arxiv.org/pdf/1912.06060.pdf">arXiv</a>). This follows the recent linear of advances in sublinear time linear algebra. Given a matrix \(A \in \mathbb{R}^{n \times d}\), the aim is to get algorithms that only look at \(o(nnz(A))\) entries (where \(nnz(A)\) is the number of non-zeroes, or the support). Consider the classic talk of low rank approximation. Unfortunately, suppose one entry is extremely large, and the others are extremely small. One has to find this large entry for any reasonable approximation, which (in the worst-case) requires \(nnz(A)\) queries into \(A\). Thus, previous papers make structural assumption (such as, \(A\) being a Vandermonde matrix) to get sublinear bounds. This paper gives a clean black box method to get a variety of such results. Basically, one can replace the usual \(nnz(A)\) term in many algorithms, by \(T(A)\), which is the time to compute the matrix-vector product \( Ay\), for \(y \in \mathbb{R}^d\). In many cases \(T(A) = \widetilde{O}(n)\), which can be significantly smaller than \(nnz(A)\). This paper gives such results for low-rank approximations and many regression problems.</p>



<p><strong>Robust and Sample Optimal Algorithms for PSD Low-Rank Approximation</strong> by Ainesh Bakshi, Nadiia Chepurko, and David P. Woodruff. Consider the low rank problem discussed above. As mentioned in the previous paragraph, we need structural assumptions on \(A\). Previous results gave sublinear time low-rank approximations assuming that \(A\) is positive semidefinite (PSD). The aim is to get a rank \(k\) matrix \(B\) such that \(\|A-B\|^2_2\) is at most \((1+\epsilon)\)-times the optimal such approximation. The previous algorithm of <a href="https://arxiv.org/abs/1704.03371">Musco-Woodruff</a> makes \(\widetilde{O}(nk/\epsilon^{2.5})\) queries in to \(A\), while there is a lower bound of \(\Omega(nk/\epsilon)\). This gap between the complexities is resolved in this paper with an upper bound of \(\widetilde{O}(nk/\epsilon)\) queries.</p>



<p><strong>Constructive derandomization of query algorithms </strong>by Guy Blanc, Jane Lange, and Li-Yang Tan (<a href="https://arxiv.org/abs/1912.03042">arXiv</a>). This paper discusses an intriguing angle to sublinear question: when can they be derandomized? Abstractly, consider a randomized algorithm \(R\) that makes \(q\) queries. Think of \(R\) as a function \(R(x,r)\), where \(x\) is the input, and \(r\) is the randomness. We would like to design a deterministic algorithm \(D\) making, ideally, close to \(q\) queries and approximates \(\mathop{E}_r[R(x,r)]\). For starters, consider some distribution over \(x\), and suppose we want \(\mathbb{E}_x[D(x) – \mathbb{E}_r[R(x,r)]] &lt; \epsilon\). By (the easy direction of) Yao’s minimax lemma, one can show the existence of such an algorithm \(D\) that makes \(O(q/\epsilon)\) queries. But how to explicitly construct it? Indeed, the first result of this paper gives a “meta-algorithm” that takes as input the description of \(R\) (which is of size \(N\)), has running time \(poly(N)2^{O(q/\epsilon)}\) and outputs a description of \(D\). When \(R\) satisfies the stronger property of “bounded error”, one can get a \(O(q^3)\)-query algorithm \(D\) that approximates \(\mathop{E}_r[R(x,r)]\) for all \(x\) (again, the existence is proven by a classic theorem of Nisan). Overall, this paper gives a method to derandomize sublinear time algorithms, and I wonder if there could be some applications of this method for proving lower bounds. After all, Yao’s minimax theorem is <em>the</em> tool for property testing lower bounds, and any perspective on Yao’s theorem is likely of relevance. </p>



<p><strong>Testing Membership for Timed Automata</strong> by Richard Lassaigne and Michel de Rougemont (<a href="https://arxiv.org/abs/1912.08277">arXiv</a>). Property testing for regular languages is a classic result in sublinear algorithms. This paper focuses on the more complex notion of timed automata. The technical definition is quite complicated, but here’s an overview. There is a finite automaton and a collection of “clocks”. Imagine a string being processed, where each alphabet symbol appears with a new timestamp. Thus, the input word is called a “timed word”. The transitions of the automaton involve the new symbol read, as well as constraints involving the clock times and the timestamp. Thus, we can enforce conditions like “only transition if another symbol is read within a single time unit”. In general, deciding whether a timed word is accepted by a timed automaton is NP-complete. This papers studies the property testing viewpoint. The paper gives a new definition of “timed edit distance” between timed words. The main result shows that one can distinguish time words accepted by a timed automaton from words that are far (according to timed edit distance), by querying a constant number of word positions.</p>



<p><strong>On the query complexity of estimating the distance to hereditary graph properties</strong> by Carlos Hoppen, Yoshiharu Kohayakawa, Richard Lang, Hanno Lefmann, and Henrique Stagni (<a href="https://arxiv.org/abs/1912.01081">arXiv</a>). This paper concerns the classic setting of property testing of dense graphs. It is well-known that all hereditary graph properties are testable, and are moreover, one can estimate the distance to the property in time that only depends on \(\varepsilon\). Unfortunately, the queries complexities have large tower dependencies on \(\varepsilon\), arising from the use of the Szemeredi regularity lemma. The question of property testing in dense graphs can be reduced to finding “removal” lemmas (such as the classic triangle remove lemma). Such a lemma states that if at least \(\varepsilon n^2\) edges need to be removed from \(G\) to destroy all “forbidden subgraphs”, then there must be “many” forbidden subgraphs in \(G\). There is much recent research on finding families of forbidden subgraphs, where the “many” (in the above statement) is at least \(poly(\varepsilon)\) times  the trivial upper bound. This paper shows that one can also estimate the distance to any hereditary property, in a query complexity that depends directly on the corresponding removal lemma parameters. As a compelling example, one can estimate the distance to being chordal in \(\exp(1/\varepsilon)\) queries, a significant improvement over standard tower bounds.</p></div>
    </content>
    <updated>2020-01-05T23:35:14Z</updated>
    <published>2020-01-05T23:35:14Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-01-13T23:21:54Z</updated>
    </source>
  </entry>
</feed>
