<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-06-10T10:22:00Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/090</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/090" rel="alternate" type="text/html"/>
    <title>TR20-090 |  Tight Quantum Time-Space Tradeoffs for Function Inversion | 

	Kai-Min Chung, 

	Siyao  Guo, 

	Qipeng Liu, 

	Luowen Qian</title>
    <summary>In function inversion, we are given a function $f: [N] \mapsto [N]$, and want to prepare some advice of size $S$, such that we can efficiently invert any image in time $T$. This is a well studied problem with profound connections to cryptography, data structures, communication complexity, and circuit lower bounds. Investigation of this problem in the quantum setting was initiated by Nayebi, Aaronson, Belovs, and Trevisan (2015), who proved a lower bound of $ST^2 = \tilde\Omega(N)$ for random permutations against classical advice, leaving open an intriguing possibility that Grover's search can be sped up to time $\tilde O(\sqrt{N/S})$. Recent works by Hhan, Xagawa, and Yamakawa (2019), and Chung, Liao, and Qian (2019) extended the argument for random functions and quantum advice, but the lower bound remains $ST^2 = \tilde\Omega(N)$. 

In this work, we prove that even with quantum advice, $ST + T^2 = \tilde\Omega(N)$ is required for an algorithm to invert random functions. This demonstrates that Grover's search is optimal for $S = \tilde O(\sqrt{N})$, ruling out any substantial speed-up for Grover's search even with quantum advice. Further improvements to our bounds would imply a breakthrough in circuit lower bounds, as shown by Corrigan-Gibbs and Kogan (2019).

To prove this result, we develop a general framework for establishing quantum time-space lower bounds. We further demonstrate the power of our framework by proving the following results. 

* Yao's box problem: We prove a tight quantum time-space lower bound for classical advice. For quantum advice, we prove a first time-space lower bound using shadow tomography. These results resolve two open problems posted by Nayebi, Aaronson, Belovs, and Trevisan (2015). 

* Salted cryptography: We show that “salting generically provably defeats preprocessing,” a result shown by Coretti, Dodis, Guo, and Steinberger (2018), also holds in the quantum setting. In particular, we prove quantum time-space lower bounds for a wide class of salted cryptographic primitives in the quantum random oracle model. This yields a first quantum time-space lower bound for salted collision-finding, which in turn implies that ${PWPP}^{O} \not\subseteq {FBQP}^{O}{/qpoly}$ relative to a random oracle $O$.</summary>
    <updated>2020-06-10T06:43:41Z</updated>
    <published>2020-06-10T06:43:41Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-10T10:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/089</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/089" rel="alternate" type="text/html"/>
    <title>TR20-089 |  Lower Bounds on  the Time/Memory Tradeoff of Function Inversion | 

	Dror Chawin, 

	Iftach Haitner, 

	Noam Mazor</title>
    <summary>We study time/memory tradeoffs of function inversion: an algorithm, i.e., an inverter, equipped with an $s$-bit advice for a randomly chosen function $f\colon [n] \mapsto [n]$ and using $q$ oracle queries to $f$, tries to invert a randomly chosen output $y$ of $f$ (i.e., to find $x$ such that $f(x)=y$). Much progress was done regarding adaptive function inversion - the inverter is allowed to make adaptive oracle queries. Hellman [IEEE transactions on Information Theory '80] presented an adaptive inverter that inverts with high probability a random $f$. Fiat and Naor [SICOMP '00] proved that for any $s,q$ with $s^2 q = n^2$ (ignoring low-order terms), an $s$-advice, $q$-query variant of Hellman's algorithm inverts a constant fraction of the image points of any function. Yao [STOC '90] proved a lower bound of $sq\ge n$ for  this problem. Closing the gap between the above lower and upper bounds is a long-standing open question.

Very little is known for the non-adaptive variant of the question - the inverter chooses its queries in advance. The only known upper bounds, i.e., inverters, are the trivial ones (with $s+q= n$), and the only lower bound is the above bound of Yao. In a recent work, Corrigan-Gibbs and Kogan [TCC '19] partially justified the difficulty of finding lower bounds on non-adaptive inverters, showing that a lower bound on the time/memory tradeoff of non-adaptive inverters implies a lower bound on low-depth Boolean circuits. Bounds that for a strong enough choice of parameters, are notoriously hard to prove.

We make progress on the above intriguing question, both for the adaptive and the non-adaptive case, proving the following lower bounds on restricted families of inverters:

	- Linear-advice (adaptive inverter): If the advice string is a linear function of $f$ (e.g., $A\times f$, viewing $f$ as  a vector in $[n]^n$), then $s+q \in \Omega(n)$.
	
	- Affine non-adaptive decoders: If the non-adaptive inverter has an affine decoder - it outputs a linear function, determined by the advice string and the element to invert, of the query answers - then $s \in \Omega(n)$ (regardless of $q$).
	
	- Affine non-adaptive decision trees: If the non-adaptive inversion algorithm is a $d$-depth affine decision tree - it outputs the evaluation of a decision tree whose nodes compute a linear function of the answers to the queries - and $q \le cn$ for some universal $c&gt;0$, then $s\in \Omega(n/d \log n)$.</summary>
    <updated>2020-06-09T13:19:02Z</updated>
    <published>2020-06-09T13:19:02Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-10T10:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.04166</id>
    <link href="http://arxiv.org/abs/2006.04166" rel="alternate" type="text/html"/>
    <title>Learning Restricted Boltzmann Machines with Few Latent Variables</title>
    <feedworld_mtime>1591660800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bresler:Guy.html">Guy Bresler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Buhai:Rares=Darius.html">Rares-Darius Buhai</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.04166">PDF</a><br/><b>Abstract: </b>Restricted Boltzmann Machines (RBMs) are a common family of undirected
graphical models with latent variables. An RBM is described by a bipartite
graph, with all observed variables in one layer and all latent variables in the
other. We consider the task of learning an RBM given samples generated
according to it. The best algorithms for this task currently have time
complexity $\tilde{O}(n^2)$ for ferromagnetic RBMs (i.e., with attractive
potentials) but $\tilde{O}(n^d)$ for general RBMs, where $n$ is the number of
observed variables and $d$ is the maximum degree of a latent variable. Let the
MRF neighborhood of an observed variable be its neighborhood in the Markov
Random Field of the marginal distribution of the observed variables. In this
paper, we give an algorithm for learning general RBMs with time complexity
$\tilde{O}(n^{2^s+1})$, where $s$ is the maximum number of latent variables
connected to the MRF neighborhood of an observed variable. This represents an
improvement when $s &lt; \log_2 (d-1)$, which is satisfied by many classes of RBMs
with "few latent variables''. Furthermore, we give a version of this learning
algorithm that recovers a model with small prediction error and whose sample
complexity is independent of the minimum potential in the Markov Random Field
of the observed variables. This is of interest because the sample complexity of
current algorithms scales with the inverse of the minimum potential, which
cannot be controlled in terms of natural properties of the RBM.
</p></div>
    </summary>
    <updated>2020-06-09T23:24:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.04124</id>
    <link href="http://arxiv.org/abs/2006.04124" rel="alternate" type="text/html"/>
    <title>On the Complexity of Branching Proofs</title>
    <feedworld_mtime>1591660800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dadush:Daniel.html">Daniel Dadush</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tiwari:Samarth.html">Samarth Tiwari</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.04124">PDF</a><br/><b>Abstract: </b>We consider the task of proving integer infeasibility of a bounded convex $K$
in $\mathbb{R}^n$ using a general branching proof system. In a general
branching proof, one constructs a branching tree by adding an integer
disjunction $\mathbf{a} \mathbf{x} \leq b$ or $\mathbf{a} \mathbf{x} \geq b+1$,
$\mathbf{a} \in \mathbb{Z}^n$, $b \in \mathbb{Z}$, at each node, such that the
leaves of the tree correspond to empty sets (i.e., $K$ together with the
inequalities picked up from the root to leaf is empty). Recently, Beame et al
(ITCS 2018), asked whether the bit size of the coefficients in a branching
proof, which they named stabbing planes (SP) refutations, for the case of
polytopes derived from SAT formulas, can be assumed to be polynomial in $n$. We
resolve this question by showing that any branching proof can be recompiled so
that the integer disjunctions have coefficients of size at most $(n
R)^{O(n^2)}$, where $R \in \mathbb{N}$ such that $K \in R \mathbb{B}_1^n$,
while increasing the number of nodes in the branching tree by at most a factor
$O(n)$. As our second contribution, we show that Tseitin formulas, an important
class of infeasible SAT instances, have quasi-polynomial sized cutting plane
(CP) refutations, disproving the conjecture that Tseitin formulas are
(exponentially) hard for CP. As our final contribution, we give a simple family
of polytopes in $[0,1]^n$ requiring branching proofs of length $2^n/n$.
</p></div>
    </summary>
    <updated>2020-06-09T23:20:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.04094</id>
    <link href="http://arxiv.org/abs/2006.04094" rel="alternate" type="text/html"/>
    <title>Average Sensitivity of Spectral Clustering</title>
    <feedworld_mtime>1591660800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Pan.html">Pan Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoshida:Yuichi.html">Yuichi Yoshida</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.04094">PDF</a><br/><b>Abstract: </b>Spectral clustering is one of the most popular clustering methods for finding
clusters in a graph, which has found many applications in data mining. However,
the input graph in those applications may have many missing edges due to error
in measurement, withholding for a privacy reason, or arbitrariness in data
conversion. To make reliable and efficient decisions based on spectral
clustering, we assess the stability of spectral clustering against edge
perturbations in the input graph using the notion of average sensitivity, which
is the expected size of the symmetric difference of the output clusters before
and after we randomly remove edges.
</p>
<p>We first prove that the average sensitivity of spectral clustering is
proportional to $\lambda_2/\lambda_3^2$, where $\lambda_i$ is the $i$-th
smallest eigenvalue of the (normalized) Laplacian. We also prove an analogous
bound for $k$-way spectral clustering, which partitions the graph into $k$
clusters. Then, we empirically confirm our theoretical bounds by conducting
experiments on synthetic and real networks. Our results suggest that spectral
clustering is stable against edge perturbations when there is a cluster
structure in the input graph.
</p></div>
    </summary>
    <updated>2020-06-09T23:24:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.04046</id>
    <link href="http://arxiv.org/abs/2006.04046" rel="alternate" type="text/html"/>
    <title>On Suboptimality of Least Squares with Application to Estimation of Convex Bodies</title>
    <feedworld_mtime>1591660800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kur:Gil.html">Gil Kur</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rakhlin:Alexander.html">Alexander Rakhlin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guntuboyina:Adityanand.html">Adityanand Guntuboyina</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.04046">PDF</a><br/><b>Abstract: </b>We develop a technique for establishing lower bounds on the sample complexity
of Least Squares (or, Empirical Risk Minimization) for large classes of
functions. As an application, we settle an open problem regarding optimality of
Least Squares in estimating a convex set from noisy support function
measurements in dimension $d\geq 6$. Specifically, we establish that Least
Squares is mimimax sub-optimal, and achieves a rate of
$\tilde{\Theta}_d(n^{-2/(d-1)})$ whereas the minimax rate is
$\Theta_d(n^{-4/(d+3)})$.
</p></div>
    </summary>
    <updated>2020-06-09T23:26:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.03856</id>
    <link href="http://arxiv.org/abs/2006.03856" rel="alternate" type="text/html"/>
    <title>On the Maximum Cardinality Cut Problem in Proper Interval Graphs and Related Graph Classes</title>
    <feedworld_mtime>1591660800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Arman Boyacı, Tınaz Ekim, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shalom:Mordechai.html">Mordechai Shalom</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.03856">PDF</a><br/><b>Abstract: </b>Although it has been claimed in two different papers that the maximum
cardinality cut problem is polynomial-time solvable for proper interval graphs,
both of them turned out to be erroneous. In this paper, we give FPT algorithms
for the maximum cardinality cut problem in classes of graphs containing proper
interval graphs and mixed unit interval graphs when parameterized by some new
parameters that we introduce. These new parameters are related to a
generalization of the so-called bubble representations of proper interval
graphs and mixed unit interval graphs and to clique-width decompositions.
</p></div>
    </summary>
    <updated>2020-06-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.03840</id>
    <link href="http://arxiv.org/abs/2006.03840" rel="alternate" type="text/html"/>
    <title>Building a Heterogeneous, Large Scale Morphable Face Model</title>
    <feedworld_mtime>1591660800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferrari:Claudio.html">Claudio Ferrari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berretti:Stefano.html">Stefano Berretti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pala:Pietro.html">Pietro Pala</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bimbo:Alberto_Del.html">Alberto Del Bimbo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.03840">PDF</a><br/><b>Abstract: </b>3D Morphable Models (3DMMs) are powerful statistical tools for representing
and modeling 3D faces. To build a 3DMM, a training set of fully registered face
scans is required, and its modeling capabilities directly depend on the
variability contained in the training data. Thus, accurately establishing a
dense point-to-point correspondence across heterogeneous scans with sufficient
diversity in terms of identities, ethnicities, or expressions becomes
essential. In this manuscript, we present an approach that leverages a 3DMM to
transfer its dense semantic annotation across a large set of heterogeneous 3D
faces, establishing a dense correspondence between them. To this aim, we
propose a novel formulation to learn a set of sparse deformation components
with local support on the face that, together with an original non-rigid
deformation algorithm, allow precisely fitting the 3DMM to arbitrary faces and
transfer its semantic annotation. We experimented our approach on three large
and diverse datasets, showing it can effectively generalize to very different
samples and accurately establish a dense correspondence even in presence of
complex facial expressions or unseen deformations. As main outcome of this
work, we build a heterogeneous, large-scale 3DMM from more than 9,000 fully
registered scans obtained joining the three datasets together.
</p></div>
    </summary>
    <updated>2020-06-09T23:26:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.03819</id>
    <link href="http://arxiv.org/abs/2006.03819" rel="alternate" type="text/html"/>
    <title>A generalized expression for filling congruent circles in a circle</title>
    <feedworld_mtime>1591660800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ajeet K. Srivastav <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.03819">PDF</a><br/><b>Abstract: </b>The paper reports a generalized expression for filling the congruent circles
(of radius r) in a circle (of radius R). First, a generalized expression for
the biggest circle (r) inscribed in the nth part of the bigger circle (R) was
developed. Further, it was extended as n such circles (r) touching each other
and the bigger circle (R). To fill the bigger circle (R), the exercise was
further repeated by considering the bigger circle radius as R-2r, R-4r and so
on. In the process, a generalized expression was deduced for the total no. of
such circles (r) which could be inscribed in this way of filling the bigger
circle (R). The approach does not claim the closest packing always though it
could be helpful for practical purposes.
</p></div>
    </summary>
    <updated>2020-06-09T23:26:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.03746</id>
    <link href="http://arxiv.org/abs/2006.03746" rel="alternate" type="text/html"/>
    <title>Distributed Approximation on Power Graphs</title>
    <feedworld_mtime>1591660800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bar=Yehuda:Reuven.html">Reuven Bar-Yehuda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Censor=Hillel:Keren.html">Keren Censor-Hillel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maus:Yannic.html">Yannic Maus</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pai:Shreyas.html">Shreyas Pai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pemmaraju:Sriram_V=.html">Sriram V. Pemmaraju</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.03746">PDF</a><br/><b>Abstract: </b>We investigate graph problems in the following setting: we are given a graph
$G$ and we are required to solve a problem on $G^2$. While we focus mostly on
exploring this theme in the distributed CONGEST model, we show new results and
surprising connections to the centralized model of computation. In the CONGEST
model, it is natural to expect that problems on $G^2$ would be quite difficult
to solve efficiently on $G$, due to congestion. However, we show that the
picture is both more complicated and more interesting.
</p>
<p>Specifically, we encounter two phenomena acting in opposing directions: (i)
slowdown due to congestion and (ii) speedup due to structural properties of
$G^2$.
</p>
<p>We demonstrate these two phenomena via two fundamental graph problems,
namely, Minimum Vertex Cover (MVC) and Minimum Dominating Set (MDS). Among our
many contributions, the highlights are the following.
</p>
<p>- In the CONGEST model, we show an $O(n/\epsilon)$-round
$(1+\epsilon)$-approximation algorithm for MVC on $G^2$, while no
$o(n^2)$-round algorithm is known for any better-than-2 approximation for MVC
on $G$.
</p>
<p>- We show a centralized polynomial time $5/3$-approximation algorithm for MVC
on $G^2$, whereas a better-than-2 approximation is UGC-hard for $G$.
</p>
<p>- In contrast, for MDS, in the CONGEST model, we show an
$\tilde{\Omega}(n^2)$ lower bound for a constant approximation factor for MDS
on $G^2$, whereas an $\Omega(n^2)$ lower bound for MDS on $G$ is known only for
exact computation.
</p>
<p>In addition to these highlighted results, we prove a number of other results
in the distributed CONGEST model including an $\tilde{\Omega}(n^2)$ lower bound
for computing an exact solution to MVC on $G^2$, a conditional hardness result
for obtaining a $(1+\epsilon)$-approximation to MVC on $G^2$, and an $O(\log
\Delta)$-approximation to the MDS problem on $G^2$ in $\mbox{poly}\log n$
rounds.
</p></div>
    </summary>
    <updated>2020-06-09T23:22:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.03684</id>
    <link href="http://arxiv.org/abs/2006.03684" rel="alternate" type="text/html"/>
    <title>Differentially private partition selection</title>
    <feedworld_mtime>1591660800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Desfontaines:Damien.html">Damien Desfontaines</a>, James Voss, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gipson:Bryant.html">Bryant Gipson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.03684">PDF</a><br/><b>Abstract: </b>Many data analysis operations can be expressed as a GROUP BY query on an
unbounded set of partitions, followed by a per-partition aggregation. To make
such a query differentially private, adding noise to each aggregation is not
enough: we also need to make sure that the set of partitions released is also
differentially private.
</p>
<p>This problem is not new, and it was recently formally introduced as
differentially private set union. In this work, we continue this area of study,
and focus on the common setting where each user is associated with a single
partition. In this setting, we propose a simple, optimal differentially private
mechanism that maximizes the number of released partitions. We discuss
implementation considerations, as well as the possible extension of this
approach to the setting where each user contributes to a fixed, small number of
partitions.
</p></div>
    </summary>
    <updated>2020-06-09T23:23:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.03666</id>
    <link href="http://arxiv.org/abs/2006.03666" rel="alternate" type="text/html"/>
    <title>VectorTSP: A Traveling Salesperson Problem with Racetrack-like acceleration constraints</title>
    <feedworld_mtime>1591660800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Casteigts:Arnaud.html">Arnaud Casteigts</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raffinot:Mathieu.html">Mathieu Raffinot</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schoeters:Jason.html">Jason Schoeters</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.03666">PDF</a><br/><b>Abstract: </b>We study a new version of the Euclidean TSP called VectorTSP (VTSP for short)
where a mobile entity is allowed to move according to a set of physical
constraints inspired from the pen-and-pencil game Racetrack (also known as
Vector Racer ). In contrast to other versions of TSP accounting for physical
constraints, such as Dubins TSP, the spirit of this model is that (1) no speed
limitations apply, and (2) inertia depends on the current velocity. As such,
this model is closer to typical models considered in path planning problems,
although applied here to the visit of n cities in a non-predetermined order. We
motivate and introduce the VectorTSP problem, discussing fundamental
differences with previous versions of TSP. In particular, an optimal visit
order for ETSP may not be optimal for VTSP. We show that VectorTSP is NP-hard,
and in the other direction, that VectorTSP reduces to GroupTSP in polynomial
time (although with a significant blow-up in size). On the algorithmic side, we
formulate the search for a solution as an interactive scheme between a
high-level algorithm and a trajectory oracle, the former being responsible for
computing the visit order and the latter for computing the cost (or the
trajectory) for a given visit order. We present algorithms for both, and we
demonstrate and quantify through experiments that this approach frequently
finds a better solution than the optimal trajectory realizing an optimal ETSP
tour, which legitimates the problem itself and (we hope) motivates further
algorithmic developments.
</p></div>
    </summary>
    <updated>2020-06-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/088</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/088" rel="alternate" type="text/html"/>
    <title>TR20-088 |  Eliminating Intermediate Measurements in Space-Bounded Quantum Computation | 

	Bill Fefferman, 

	Zachary Remscrim</title>
    <summary>A foundational result in the theory of quantum computation known as the ``principle of safe storage'' shows that it is always possible to take a quantum circuit and produce an equivalent circuit that makes all measurements at the end of the computation.  While this procedure is time efficient, meaning that it does not introduce a large overhead in the number of gates, it uses extra ancillary qubits and so is not generally space efficient.  It is quite natural to ask whether it is possible to defer measurements to the end of a quantum computation without increasing the number of ancillary qubits.
		
		We give an affirmative answer to this question by exhibiting a procedure to eliminate all intermediate measurements that is simultaneously space-efficient and time-efficient. A key component of our approach, which may be of independent interest, involves showing that the well-conditioned versions of many standard linear-algebraic problems may be solved by a quantum computer in less space than seems possible by a classical computer.</summary>
    <updated>2020-06-08T21:24:06Z</updated>
    <published>2020-06-08T21:24:06Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-10T10:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/087</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/087" rel="alternate" type="text/html"/>
    <title>TR20-087 |  Quantum Logspace Algorithm for Powering Matrices with Bounded Norm | 

	Uma Girish, 

	Ran Raz, 

	Wei Zhan</title>
    <summary>We give a quantum logspace algorithm for powering contraction matrices, that is, matrices with spectral norm at most 1. The algorithm gets as an input an arbitrary $n\times n$ contraction matrix $A$, and a parameter $T \leq poly(n)$ and outputs the entries of $A^T$, up to (arbitrary) polynomially small additive error. The algorithm applies only unitary operators, without intermediate measurements. We show various implications and applications of this result:

First, we use this algorithm to show that the class of quantum logspace algorithms with only quantum memory and with intermediate measurements is equivalent to the class of quantum logspace algorithms with only quantum memory without intermediate measurements. This shows that the deferred-measurement principle, a fundamental principle of quantum computing, applies also for quantum logspace algorithms (without classical memory). More generally, we give a quantum algorithm with space $O(S + \log T)$ that takes as an input the description of a quantum algorithm with quantum space $S$ and time $T$, with intermediate measurements (without classical memory), and simulates it unitarily with polynomially small error, without intermediate measurements.

Since unitary transformations are reversible (while measurements are irreversible) an interesting aspect of this result is that it shows that any quantum logspace algorithm (without classical memory) can be simulated by a reversible quantum logspace algorithm. This proves a quantum analogue of the result of Lange, McKenzie and Tapp that deterministic logspace is equal to reversible logspace.

Finally, we use our results to show non-trivial classical simulations of quantum logspace learning algorithms.</summary>
    <updated>2020-06-08T18:51:13Z</updated>
    <published>2020-06-08T18:51:13Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-10T10:20:33Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-9046421052819875987</id>
    <link href="https://blog.computationalcomplexity.org/feeds/9046421052819875987/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/06/the-committee-for-adv-of-tcs-workshop.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/9046421052819875987" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/9046421052819875987" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/06/the-committee-for-adv-of-tcs-workshop.html" rel="alternate" type="text/html"/>
    <title>The Committee for the Adv. of TCS- workshop coming up SOON!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><span id="docs-internal-guid-06a8f4c9-7fff-916b-0154-0d4b08969907"/><br/>
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span id="docs-internal-guid-06a8f4c9-7fff-916b-0154-0d4b08969907"><span>(Posted by request from Jelani Nelson.)</span></span></div>
<span id="docs-internal-guid-06a8f4c9-7fff-916b-0154-0d4b08969907"><br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>The Committee for the Advancement of Theoretical Computer Science (CATCS)</span></div>
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>is organizing a Visioning workshop.  The primary objective of the workshop</span></div>
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>is for TCS participants to brainstorm directions and talking points for TCS</span></div>
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>program managers at funding agencies to advocate for theory funding.</span></div>
<br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>There was some question of whether or not it would run this summer, but</span></div>
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>YES, it is going to run.</span></div>
<br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>If you are interested then reply (at the link below) by June 15.</span></div>
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>This is SOON so click that link SOON.</span></div>
<br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>The time commitment is 4-5 hours during the week of July 20-July 24 for</span></div>
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>most participants, or roughly 10 hours for those who are willing to</span></div>
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>volunteer to be group leaders.</span></div>
<br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span>The link to sign up is:</span></div>
<br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<span><a href="https://thmatters.wordpress.com/2020/06/05/tcs-visioning-workshop-call-for-participation/">https://thmatters.wordpress.com/2020/06/05/tcs-visioning-workshop-call-for-participation/</a></span></div>
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<br/></div>
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;">
<br/></div>
</span></div>
    </content>
    <updated>2020-06-08T15:03:00Z</updated>
    <published>2020-06-08T15:03:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-06-09T07:09:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4392</id>
    <link href="https://lucatrevisan.wordpress.com/2020/06/08/visioning-workshop-call-for-participation/" rel="alternate" type="text/html"/>
    <title>“Visioning” workshop call for participation</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In 2008, the Committee for the Advancement of Theoretical Computer Science convened a workshop to brainstorm directions and talking points for TCS program managers at funding agencies to advocate for theory funding. The event was quite productive and successful. A … <a href="https://lucatrevisan.wordpress.com/2020/06/08/visioning-workshop-call-for-participation/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In 2008, the <a href="https://thmatters.wordpress.com/catcs/">Committee for the Advancement of Theoretical Computer Science</a> convened a workshop to brainstorm directions and talking points for TCS<br/>
program managers at funding agencies to advocate for theory funding. The event was quite productive and successful. </p>
<p>A second such workshop is going to be held, online, in the third week of July. Applications to participate are due on June 15, a week from today. Organizers expect that participants will have to devote about four hours of their time to the workshop, and those who volunteer to be team leads will have a time commitment of about ten hours.</p>
<p>More information <a href="https://thmatters.wordpress.com/2020/06/05/tcs-visioning-workshop-call-for-participation/">at this link</a></p></div>
    </content>
    <updated>2020-06-08T09:38:07Z</updated>
    <published>2020-06-08T09:38:07Z</published>
    <category term="theory"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-06-10T10:20:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1343</id>
    <link href="https://ptreview.sublinear.info/?p=1343" rel="alternate" type="text/html"/>
    <title>Welcome Akash Kumar!</title>
    <summary>Let’s welcome our latest editor, Akash Kumar. Akash will be taking the place of Gautam Kamath, who has decided to pass the torch on. Let’s also thank Gautam for all the help with PTReview.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Let’s welcome our latest editor, Akash Kumar. Akash will be taking the place of Gautam Kamath, who has decided to pass the torch on. Let’s also thank Gautam for all the help with PTReview.</p></div>
    </content>
    <updated>2020-06-08T03:29:36Z</updated>
    <published>2020-06-08T03:29:36Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-06-09T23:41:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1315</id>
    <link href="https://ptreview.sublinear.info/?p=1315" rel="alternate" type="text/html"/>
    <title>News for May 2020</title>
    <summary>Last month saw activity across a diverse collection of topics in sublinear algorithms. In particular, we had the following five six papers. (Sorry, I missed one) One-Sided Error Testing of Monomials and Affine Subspaces by Oded Goldreich and Dana Ron (ECCC). This work focuses on one-sided testing of two kinds of problems (and their variants): […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Last month saw activity across a diverse collection of topics in sublinear algorithms. In particular, we had the following <s>five</s> six papers. <em>(Sorry, I missed one)</em> </p>



<p><strong>One-Sided Error Testing of Monomials and Affine Subspaces</strong> by Oded Goldreich and Dana Ron (<a href="https://eccc.weizmann.ac.il/report/2020/068/">ECCC</a>). This work focuses on one-sided testing of two kinds of problems (and their variants): <br/><span class="has-inline-color has-blue-color">1.</span> Testing Monomials: Suppose you are given a function \(f \colon \{0,1\}^n \to \{0,1\}\). Is \(f = \wedge_{i \in I} x_i\) (that is, is \(f\) a monotone monomial). <br/><span class="has-inline-color has-blue-color">2.</span> Testing Affine Subspaces: Consider the task of testing whether a \(f \colon \mathcal{F}^n \to \{0,1\}\) is the indicator of an \((n-k)\)-dimensional affine space for some \(k\) (where \(\mathcal{F}\) is a finite field).<br/></p>



<p>The paper shows that the general problem — the one in which the arity of the monomial (resp the co-dimension of the subspace) is not specified has one-sided query complexity \(\widetilde{O}(1/\varepsilon)\). The same holds for testing whether the arity of the monomial is at most \(k\) (resp the co-dimension of the subspace is at most \(k\)). Finally, the exact problem which seeks to test whether the arity of the monomial is exactly \(k\) (resp the co-dimension of the space is exactly \(k\)) has query complexity \(\Omega(\log n)\). For two sided testers however, size oblivious testers are known for this problem. Thus, like the authors remark, two-sided error is inherent in the case of the exact version of the problem.</p>



<p/>



<p><strong>Sampling Arbitrary Subgraphs Exactly Uniformly in Sublinear Time</strong> by Hendrik Fichtenberger, Mingze Gao, Pan Peng (<a href="https://arxiv.org/abs/2005.01861v2">arXiv</a>). Readers of PT Review are no strangers to the problem of counting cliques in sublinear time (with a certain query model). Building on tools from [1], in [2], Eden-Ron-Seshadhri gave the first algorithms for counting number of copies \(K_r\) in a graph \(G\) to within a \((1 \pm \varepsilon)\) multiplicative factor. En route to this result, they also gave a procedure to sample cliques incident to some special set \(S \subseteq V(G)\). The query model in [2] allowed the following queries: a u.a.r vertex query, degree query, \(i^{th}\) neighbor query and a pair query which answers whether a pair \((u,v)\) forms an edge. The work under consideration shows a result which I personally find remarkable: given the additional ability to get a u.a.r edge sample, we can do the following. For any graph \(H\) we can obtain a uniformly random subgraph isomorphic to \(H\) in \(G\). Let that sink in: this work shows that you can sample \(H\) <em>exactly uniformly</em> from the graph \(G\).  </p>



<p><strong>Finding Planted Cliques in Sublinear Time</strong> by Jay Mardia, Hilal Asi, Kabir Aladin Chandrasekher (<a href="https://arxiv.org/abs/2004.12002">arXiv</a>). Planted Clique is a time honored problem in average case complexity. This classic problem asks the following: You are given a \(G \sim \mathcal{G}(n, 1/2)\). Suppose I select a subset of \(k\) vertices in this graph and put a clique on the subgraph they induce. In principle it is possible to recover the clique I planted if \(k &gt; (2 + \varepsilon) \log n\). But it seems you get polynomial time algorithms only when \(k \geq \Omega(\sqrt n)\) even after you throw SDPs at the problem. Moreover, so far, the algorithms which recover the planted \(k\)-clique were known to take \(\widetilde{O}(n^2)\) time. This work shows that you actually get algorithms which take time \(\widetilde{O}(n^{3/2})\) if \(k \geq \Omega(\sqrt{n \log n})\). The key idea is to first obtain a “core” part of the clique of size \(O(\log n)\) in time \(\widetilde{O}(n^2/k)\). This is followed up with a clique completion routine where you mark all vertices connected to the entire core as being potentially in the clique. The paper also shows a conditional lower bound result which shows that given query access to adjacency matrix of the graph, a natural family of non-adaptive algorithms cannot recover a planted \(k\) clique in time \(o\left(\frac{n}{k}\right)^3\) (for \(k \geq \widetilde{\Omega}(\sqrt n))\).</p>



<p><strong>A robust multi-dimensional sparse Fourier transform in the continuous setting</strong> by Yaonan Jin, Daogao Liu and Zhao Song (<a href="https://arxiv.org/abs/2005.06156">arXiv</a>).  Suppose you are given an unknown signal whose Fourier Spectrum is k-sparse (that is, there are at most k dominant Fourier Coefficients and all the others are zero or close to zero). Significant research effort has been devoted to learn these signals leading to works which study this problem for multi-dimensional discrete setting and in the one-dimensional continuous case. The \(d\)-dimensional continuous case \((d = \Theta(1))\) was largely unexplored. This work makes progress on this frontier by making some natural assumptions on the unknown signal. In particular, the paper assumes that the frequencies — which are vectors \(f_i’s \in R^d\) — are well separated and satisfy \(\|f_i – f_j\|_2 \leq \eta\) and that all \({f_i}_{i \in [k]} \subseteq [-F, F]^d\) sit inside a bounded box.<br/>The authors assume sample access to the signal in the sense that at any desired timestep \(\tau\), the algorithm can sample the signal’s value. With this setup, the authors show that all the dominant frequencies can be recovered with a \(O_d(k \cdot \log(F/\eta))\) samples by considering a relatively small time horizon.</p>



<p><strong>Extrapolating the profile of a finite population</strong> by Soham Jana, Yury Polyanskiy, Yihong Wu (<a href="https://arxiv.org/abs/2005.10561">arXiv</a>). Consider the following setup. You are given a universe \(k\) balls. Ball come in up to \(k\) different colors. Say you \(\theta_j\) balls in color \(j\) for each \(j \in [k]\). One of the fundamental problems in statistics considers taking samples \(m\) balls from the universe and attempts estimating “population profile” (that is, the number of balls in each color). Historically, it is known that unless an overwhelming majority of the universe has been seen, one cannot estimate the empirical distribution of colors. This paper shows that in the sublinear regime, with \(m \geq \omega(k/\log k)\), it is possible to consistently estimate the population profile in total variation. And once you have a handle on the empirical distribution of the population, you can go ahead and learn lots of interesting label invariant properties of your universe (things like entropy, number of distinct elements etc). </p>



<p/>



<p><em>(Edit added later)</em></p>



<p><strong>Testing Positive Semi-Definiteness via Random Submatrices</strong> by Ainesh Bakshi, Nadiia Chepurko, Rajesh Jayaram (<a href="https://arxiv.org/abs/2005.06441">arXiv</a>). Suppose I give you a PSD matrix \(A \in R^{n \times n}\). You know that all of its principle submatrices are also PSD. What if \(A\) was \(\varepsilon\)-far from the PSD cone (in a sense I will define soon)? What can you say about the eigenvalues of principle submatrices of \(A\) now? In this paper, the authors tackle precisely this question. The paper defines a matrix \(A\) to be \(\varepsilon\)-far in \(\ell_2^2\) distance from the PSD Cone if you have that \(\min_{B \geq 0: B \in R^{n \times n}}\|A – B\|_F^2 \geq \varepsilon n^2\). You are allowed to randomly sample a bunch of principle submatrices (of order roughly \(O(1/\varepsilon)\) by \(O(1/\varepsilon)\) and check if they are PSD. Armed with this setup, the paper gives a non-adaptive one sided tester for this problem which makes \(\widetilde{O}(1/\varepsilon^4)\) queries. The paper also supplements this result with a lower bound of \(\widetilde{\Omega}(1/\varepsilon^2)\) queries.</p>



<p>If I missed something, please let me know. This is my first post on PT Review and I might have botched up a few things.</p>



<p><strong>References</strong></p>



<p>[1] Talya Eden, Amit Levi, Dana Ron and C. Seshadhri. Approximately Counting Triangles in Sublinear Time. <em>56th Annual Symposium on Foundations of Computer Science, 2015</em></p>



<p>[2] Talya Eden, Dana Ron and C. Seshadhri. On approximating the number of k-cliques in sublinear time. Proceedings of the <em>50th Annual ACM SIGACT Symposium on Theory of Computing 2018</em>.</p></div>
    </content>
    <updated>2020-06-07T22:22:58Z</updated>
    <published>2020-06-07T22:22:58Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>akumar</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-06-09T23:41:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17136</id>
    <link href="https://rjlipton.wordpress.com/2020/06/07/the-doomsday-argument-in-chess/" rel="alternate" type="text/html"/>
    <title>The Doomsday Argument in Chess</title>
    <summary>Framing a controversial conversation piece as a conservation law Snip from Closer to Truth video on DA John Gott III is an emeritus professor of astrophysical sciences at Princeton. He was one of several independent inventors of the controversial Doomsday Argument (DA). He may have been the first to think of it but the last […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Framing a controversial conversation piece as a conservation law</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/06/07/the-doomsday-argument-in-chess/jrichardgottsnip/" rel="attachment wp-att-17138"><img alt="" class="alignright wp-image-17138" height="190" src="https://rjlipton.files.wordpress.com/2020/06/jrichardgottsnip.jpg?w=173&amp;h=190" width="173"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Snip from <em>Closer to Truth</em> <a href="https://www.youtube.com/watch?v=Qlrj4iE7FA4">video</a> on DA</font></td>
</tr>
</tbody>
</table>
<p>
John Gott III is an emeritus professor of astrophysical sciences at Princeton. He was one of several independent inventors of the controversial <a href="https://en.wikipedia.org/wiki/Doomsday_argument">Doomsday</a> <a href="http://www.anthropic-principle.com/?q=anthropic_principle/doomsday_argument">Argument</a> (DA). He may have been the first to think of it but the last to expound it in a <a href="https://www.nature.com/articles/363315a0.epdf">paper</a> or presentation.</p>
<p>
Today we expound DA as a defense against thought experiments that require unreasonable lengths of time.</p>
<p>
Gott thought of the argument when he saw the Berlin Wall as a 22-year-old touring Berlin in 1969. He reasoned that his visit was a uniformly random event in the lifetime <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> of the wall. That assumption gave him a 75% likelihood that he was not observing the wall in the first quarter of its lifetime. Since the wall was then 8 years old, that became a 75% likelihood that the wall would not last beyond 1993. It came down in late 1989.</p>
<p>
The “Doomsday” name comes when one’s birthdate <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is regarded as a uniformly random sample from the sequence of all human births. If you are my age, <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is probably <a href="https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=F7CF2C002ED22C17F221BE1788C43A6E?doi=10.1.1.49.5899&amp;rep=rep1&amp;type=pdf">closer</a> to ordinal 60 billion than 70 or 100 billion. We can then say we are 95% confident that we are not in the initial 5% of this sequence. That <em>entails</em> the sequence stopping before 1.2 trillion births. If our population levels off at 10 billion with 80 years’ life expectancy, that makes <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> extend no further than the year 12,000 AD. The upshot is that a longer <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> entails asserting that our random sample gave a point unusually <em>early</em> in the span. The purer form of DA also argues that <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is not unusually <em>late</em>, giving this picture:</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/06/07/the-doomsday-argument-in-chess/doom/" rel="attachment wp-att-17140"><img alt="" class="aligncenter wp-image-17140" height="120" src="https://rjlipton.files.wordpress.com/2020/06/doom.png?w=290&amp;h=120" width="290"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Modified from Michael Stock <a href="https://michielstock.github.io/doomsday/">source</a></font>
</td>
</tr>
</tbody></table>
<p>This doubles the span of <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> allowed with 95% confidence while giving reason—at the time <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> occurs—to believe that the end is not imminent: at least about 1.75 billion more births will come after <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. For <img alt="{x =}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x =}"/> my birth, however, this is already a given.</p>
<p>
</p><p/><h2> Debating DA </h2><p/>
<p/><p>
The dependence on which observer is taken as the reference point <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is one shiftable parameter of the DA. If you are a preteen reader, then your own birth may be closer to ordinal 70 billion in the sequence, which becomes <em>your</em> reference point. You can then tack on another 2,000 years to <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/>. The earliest human cave painters may have been among the first 3 billion <em>Homo sapiens</em>. With regard to their reference point, <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> has already gone past their 95% limit. </p>
<p>
A more fundamental rebuff to DA comes from the equal reasonableness of an alternate uniformity assumption: that you are a uniformly random element of the set <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> of all <em>possible</em> human beings. Only a subset <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> of <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> will ever be born. The longer <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> is, the higher was your prior probability of belonging to <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>. Thus the fact of your birth can be construed as weighting the odds toward longer <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> in a way that cancels out the short-<img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> reasoning of DA.</p>
<p>
Even when an instance of DA passes these objections, the inference remains controversial. We <a href="https://rjlipton.wordpress.com/2019/12/22/predicting-when-pnp-is-resolved/">wrote</a> about DA last year in connection with estimating the lifespan of open problems remaining open. A clear <em>non</em>-instance is trying to apply DA to estimate the lifespan of the Covid-19 pandemic. We have all been going through the span together and <em>now</em> is not a uniformly random sample. </p>
<p>
The DA assumptions would however hold if an alien tourist with no prior knowledge of events dropped in on Earth today. The delicacy of the assumptions makes it significant to seek scenarios where DA firmly applies—and better, where the inference may be deemed <em>necessary</em> to preserve the validity of established modes of inference against extreme skeptical hypotheses. This is what we will try to argue in regard to inferences of cheating at chess. </p>
<p>
</p><p/><h2> The 1-in-100,000 Question </h2><p/>
<p/><p>
We have <a href="https://rjlipton.wordpress.com/2014/06/18/the-problem-of-catching-chess-cheaters/">posted</a> <a href="https://rjlipton.wordpress.com/2013/09/17/littlewoods-law/">numerous</a> <a href="https://rjlipton.wordpress.com/2013/01/13/the-crown-game-affair/">times</a> <a href="https://rjlipton.wordpress.com/2013/07/27/thirteen-sigma/">about</a> my statistical <a href="https://rjlipton.wordpress.com/2018/10/18/london-calling/">chess</a> <a href="https://rjlipton.wordpress.com/2019/08/15/predicting-chess-and-horses/">model</a>, its giving judgments of odds against null hypotheses of fair play in the form of <a href="https://en.wikipedia.org/wiki/Standard_score"><em>z</em>-scores</a>, and my <a href="https://rjlipton.wordpress.com/2011/10/12/empirical-humility/">means</a> of <a href="https://rjlipton.wordpress.com/2019/08/20/our-trip-to-monte-carlo/">validating</a> them. We will suppose for this argument that the modeling is true in the sense that the distribution of <em>z</em>-scores from testing honest players conforms to the standard normal distribution. </p>
<p>
Now let us talk about chess in the years B.C.—before Covid—when the game was played over-the-board (OTB) in-person across a table. Suppose I obtained a <em>z</em>-score of 4.265 from a test of one player in one tournament. I have chosen this number for all of the following reasons:</p>
<ul>
<li>
It corresponds to what I call “face-value odds” of 100,000-to-1 against the null hypothesis, as one can see from <a href="https://www.fourmilab.ch/rpkp/experiments/analysis/zCalc.html">this</a> or any similar calculator. <p/>
</li><li>
It is close to my number from an actual case in the year 1 B.C., that is, last year. <p/>
</li><li>
It is also typical of <em>z</em>-scores I have been obtaining these past three months since chess went online, at the point where certain online platforms have made their own decisions to impose sanctions. Here I must add that the platforms’ cheating detection systems avail much information about the manner of play that often furnishes much greater statistical evidence, whereas my minimalist model uses only the record of the moves played in the games.
</li></ul>
<p>
Suppose there were no other relevant information about the case. How would one assess the significance of the <em>z</em>-score of 4.265? Here are two different ways of reasoning that—in the case of OTB chess—arrive at similar answers:</p>
<ol>
<li>
The Bayesian prior probability of cheating in OTB chess has been estimated between 1-in-10,000 and 1-in-5,000. Suppose the former, and consider a thought experiment in which 100,000 players are tested. For simplicity, let’s suppose all true instances—that is, cheating players—give above 4.265. We expect there to be ten of them, plus one <em>natural</em> occurrence of 4.265 or more. Thus the odds that our score represents a true positive are only 10-in-11. This is well short of the odds range usually needed to meet the standard of <em>comfortable satisfaction</em> used for example by the Court of Arbitration for Sport. Thus the 4.265 datum alone should not be sufficient grounds for sanction. <p/>
</li><li>
Suppose there were a policy of sanction above a threshold of 4.25. The sum of playing fields in events held under auspices of the International Chess Federation (FIDE) each year exceeds 100,000. Thus we would expect to find at least one <em>z</em>-score over 4.265 per year by natural chance, whose sanctioning would be a serious human-rights error. FIDE cannot afford a rate of one such error per year. Thus it is insufficient for sanction.
</li></ol>
<p>
A 5.0 standard, however, gives a natural frequency of just over 1-in-3.5 million. The resulting error rate of once in 20-to-30 years might be acceptable in prospect. And the Bayesian argument based on a 0.0001 prior leaves about 350-to-1 odds against the null hypothesis, which is comfortably within the comfortable-satisfaction range as it has been applied. </p>
<p>
FIDE nevertheless has maintained a policy that statistical evidence must be accompanied by some other kind of evidence. If a player is caught looking at a chess position in a bathroom, or found to have a buzzing device or wires on his-or-her person, or signaling behavior is observed, then in fact much lower <em>z</em>-scores (to a threshold of 2.50, about 160-to-1 odds, in current FIDE regulations) are deemed to lend strong support to such evidence. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/06/07/the-doomsday-argument-in-chess/cheat/" rel="attachment wp-att-17142"><img alt="" class="aligncenter wp-image-17142" height="180" src="https://rjlipton.files.wordpress.com/2020/06/cheat.png?w=240&amp;h=180" width="240"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">2015 Peter Doggers/Chess.com <a href="https://www.chess.com/news/view/amateur-player-caught-cheating-at-tournament-in-new-delhi-7004?page=5">source</a></font>
</td>
</tr>
</tbody></table>
<p>
I posted a similar <a href="https://cse.buffalo.edu/~regan/chess/fidelity/Golfers.html">rationale</a> on my own website in early 2012, where causal evidence is likened to the “black spot” in the novel <em>Treasure Island</em>.</p>
<p>
</p><p/><h2> One More Datum </h2><p/>
<p/><p>
Now, however, suppose we have the 4.265 and one more piece of “evidence” that is pertinent but not as clearly causal. It could be:</p>
<ul>
<li>
The player wore a hat that covers the ears, or <p/>
</li><li>
An unusually bulky sweater (worn on a hot day), or <p/>
</li><li>
Unusual gestures or movements during the games.
</li></ul>
<p>
Say a search of the player turned up nothing, but this occurred after the sequence of games giving the 4.265, a day after the player had been put on notice of suspicion. So the extra information is not a black spot but instead a “grey spot.” What can we conclude now?</p>
<p>
The Bayesian argument seems to depend on judging how this information affects the prior probability of cheating. Does it make cheating a more likely hypothesis? We don’t actually know. Whereas the 1-in-10,000 global prior estimate was based on knowing dozens of <a href="https://en.wikipedia.org/wiki/Cheating_in_chess#Incidents">cases</a> over the past decade, only a handful conformed to this level of indication—short of more obvious things like making frequent visits to the restroom or being seen with an ear adornment. The most we can say is that the datum is not irrelevant. An example of an irrelevant datum would be if the player were wearing neon green sneakers—not bulky, no wires, just a weird green.</p>
<p>
I would like, however, to argue that the player’s membership in a smaller sample <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> that is <em>pertinent</em> enhances the significance of the <em>z</em>-score. <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> must be defined by criteria that are not only independent of my statistical analysis of the games but also pertinent so as to avoid selection bias. What is needed to quantify this enhancement is:</p>
<blockquote><p>
(a) to collect all (other) kinds of items on a par with the above—say ostentatious bracelets that could camouflage electronic indicators—and</p>
<p>(b) to establish that the frequency of players having <em>any</em> such accoutrement over the global mass of tournaments is at most, say, 1-in-100.<br/>
</p></blockquote>
<p>
Now there are several equivalent ways to continue the reasoning. One is to say that since <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is “at worst” independent, the face-value odds are amplified by a factor of at least 100. The Bayesian mitigation then still leaves about 1,000-to-1 odds against the null hypothesis. Another is to say that in any given year, the natural chance of seeing the conjunction of <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> and the <em>z</em>-score is at most 1-in-100. Thus aside from the frequency of true positives, a policy of sanctioning in such cases would have a prospective error rate of once in 100 years. The conjoined error rate of that and sanctioning on 5.0 in isolation would be acceptable.</p>
<p>
A Bayesian defense attorney might still counter: Consider a thought experiment in which we test 100,000 such “bulky” players. We don’t have any new information on the prior rate of cheating by players in <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>. For all we know, it is still 1-in-10,000. Thus the same terms as before will apply: our experiment will expect to have 10 cheaters in <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> plus the one natural false positive, leaving the odds only 10-to-1 as before. Put another way: without knowing the import of specializing to <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> on the likelihood of cheating, you can’t reach any further conclusion.</p>
<p>
</p><p/><h2> Doomsday to the Rescue </h2><p/>
<p/><p>
The nub of rejecting this counter-argument is that:</p>
<blockquote><p><b> </b> <em> Because there are only about 1,000 players in <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B}"/> per year, the thought experiment of testing 100,000 players in <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B}"/> now takes 100 years. </em>
</p></blockquote>
<p>Moreover, the defense attorney is asserting that the mistaken false positive has occurred <em>unusually early</em> in this span. If this is the first year under consideration, then it is a uniformly random event in the first 1% of <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/>. By the same reasoning of DA, the odds of this are only 1-in-100. Compounded by the 10-1 odds against this particular score in the thought experiment being the false positive, we recover something near the 1,000-to-1 odds of the original reasoning.</p>
<p>
We might allow that we are not in the first year of “the cheating era” in chess. The thicket of high-profile cases with solid grounds for judgment goes back a little over 10 years. The factor from DA then goes down to 1-in-10. But this still leaves the overall odds about 100-to-1 against the null hypothesis, and that is commonly taken as an anchor point for the standard of comfortable satisfaction after all mitigating factors have been addressed. </p>
<p>
Thus I am casting the Doomsday interval argument as a defense against unreasonably long thought experiments. It restores a dimension of time that is ignored by the Bayesian objection. This dimension of time is correctly preserved in the analysis of the expected error rate from a policy of imposing sanctions under this <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>–<em>z</em> combination of circumstances.</p>
<p>
Is my line of reasoning valid? You can be the judge. If so, then it is a class of instances where DA is applied merely to <em>conserve</em> an inference of unlikelihood that was originally made by other means. This supports the validity of DA-type inference in general.</p>
<p>
</p><p/><h2> Online Chess and the Time Warp </h2><p/>
<p/><p>
We are now in the third month of “the online era” in chess. Even though online platforms can process many more kinds of information than I can avail from OTB play, my work has proved highly relevant for global early indications, second opinions, and transparent explanations. Alas, the sanction rate at the new featured tournaments has been well in excess of 1%. We hope this will come down as the playing pool—which has been greatly democratized in massive online events—wises up to the reality of getting caught.</p>
<p>
What I want to discuss here is how this brave new world flips the Bayesian reasoning in a way that may come on too strong <em>for the prosecution</em>, again by its indifference to the element of time. </p>
<p>
Take the 4.265 <em>z</em>-score with a <img alt="{1\%}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1\%}"/> prior. The face-value odds from the <em>z</em>-score are now mitigated only to 1-in-1,000. This gives 99.9% confidence in imposing a sanction. However, the <em>rate of errors</em> would be <em>higher</em> than once-per-year because more players total have been involved per tournament. The tournaments are played at faster Rapid and Blitz paces allowing eight or more games per day, whereas classic OTB tournaments feature one game per day, sometimes two, over a span of a week to ten days. </p>
<p>
This is also set against a vastly higher global sample size. Whereas the entire historical record of OTB chess represented by the ChessBase <a href="https://shop.chessbase.com/en/products/mega_database_2020?ref=RF32-OET9DCF5AK&amp;gclid=EAIaIQobChMIu9P-4-_u6QIVGqSzCh0QQQsyEAAYASAAEgKcevD_BwE">Mega</a> database has yet to hit the 10 million games mark, the online platform <a href="https://lichess.org/">Lichess</a> has now hit 75 million games played <a href="https://database.lichess.org/">per month</a>. Adding in <a href="https://en.wikipedia.org/wiki/Internet_Chess_Club">ICC</a> and <a href="https://www.chess.com/home">chess.com</a> and ChessBase’s and FIDE’s own servers yields an equation that recalls <a href="https://biblehub.com/psalms/90-4.htm">Ps 90:4</a> and <a href="https://biblehub.com/2_peter/3-8.htm">2 Peter 3:8</a>:</p>
<blockquote><p><b> </b> <em> A thousand years of OTB are but a day that passes online. </em>
</p></blockquote>
<p>
For online platforms in isolation, absent anything to distinguish one player’s set of games from any other’s (such as their belonging to a highest-profile tournament), this means that even a 5.0 standard is inadequate for sure judgment. At their volume, online sites can see deviations of by natural chance more than once per day. Thus they either tolerate a higher rate of errors or adopt a standard so high as to let many more guilty <a href="https://en.bab.la/dictionary/french-english/partie-d-echecs">parties</a> through the sieve. </p>
<p>
Such volume means all the more that one should hold a score of 4.265 as insufficient for judgment. This is despite the vastly higher Bayesian likelihood that a sanction based on that score is correct. The greater frequency of actual cheating does mean that the rate of error per positive reading declines, but the rate per absolute time, with regard to the fixed population of honest players, may matter more. This has accompanied deliberations of whether sanctions for online cheating must be given less permanent consequences in order to allow setting thresholds so that a high percentage of actual cheaters are flagged and the error rate can be tolerated.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Does this analysis square with you? Does it help in understanding controversies over the original Doomsday Argument’s paradigm?</p>
<p>
For another pass over the argument, suppose I get a <em>z</em>-score of 4.265 in a narrowly-defined event such as one country’s championship league. Does that limit the sample size, so that the score is more dispositive? The kind of reasoning in point (b) above, where we had to gather all possible indicators that would lead us to constrain the sample, would however mandate widening it at least to include other countries’ leagues. This is an aspect of the “look elsewhere” <a href="https://en.wikipedia.org/wiki/Look-elsewhere_effect">effect</a> where the space of potential tests is widened even before actual tests are considered. Possibly it should be widened to include all tournaments with similar levels of players, in which case we are back to the “square 1” of the 1-in-100,000 section of this post. The point of the analysis of the extra datum about the player is that the sample expansion has an effective pre-defined limit.</p>
<p/><p><br/>
[Added note about online cheating detection to third bullet in section 3.  Clarified: “… the conjunction of these two factors” –&gt; “… the conjunction of B and the z-score” and changed the succeeding sentence.]</p></font></font></div>
    </content>
    <updated>2020-06-07T19:22:02Z</updated>
    <published>2020-06-07T19:22:02Z</published>
    <category term="All Posts"/>
    <category term="chess"/>
    <category term="detection"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Bayesian reasoning"/>
    <category term="cheating"/>
    <category term="Doomsday Argument"/>
    <category term="fraud-detection"/>
    <category term="John Richard Gott"/>
    <category term="online chess"/>
    <category term="statistics"/>
    <category term="time horizons"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-06-10T10:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/086</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/086" rel="alternate" type="text/html"/>
    <title>TR20-086 |  A Survey on Approximation in Parameterized Complexity: Hardness and Algorithms | 

	Andreas Feldmann, 

	Karthik  C. S., 

	Euiwoong Lee, 

	Pasin Manurangsi</title>
    <summary>Parameterization and approximation are two popular ways of coping with NP-hard problems. More recently, the two have also been combined to derive many interesting results. We survey developments in the area both from the algorithmic and hardness perspectives, with emphasis on new techniques and potential future research directions.</summary>
    <updated>2020-06-07T13:52:52Z</updated>
    <published>2020-06-07T13:52:52Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-10T10:20:33Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-6421565733839282178</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/6421565733839282178/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=6421565733839282178" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/6421565733839282178" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/6421565733839282178" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2020/06/catcs-visioning-workshop.html" rel="alternate" type="text/html"/>
    <title>CATCS Visioning Workshop</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div><span class="im">Reposting an important call -- these events have had big impact in the past!</span></div><div><span class="im">The <a href="https://thmatters.wordpress.com/catcs/" style="color: #3f9291;">CATCS</a> will be hosting a virtual <strong>“Visioning Workshop”</strong> the <strong>week of </strong></span><strong>July 20</strong> in order to identify broad research themes within theoretical computer science (TCS) that have potential for a major impact in the future. The goals are similar to the <a href="https://thmatters.wordpress.com/visioning-workshop/" style="color: #3f9291;">workshop of the same name</a> in 2008: to package these themes in a way that can be consumed <span class="im">by the general public, which we would deliver primarily to the Computing Community Consortium and others (e.g. funding agencies) to help them advocate for TCS.</span></div><div>While participation in the workshop is primarily through invitation, we have a few slots available for the broader community. If you are interested in participating, please see details of the application process below. The workshop will be organized according to area-wise breakout groups. Each breakout group will have 1-2 leads. Breakout groups will meet for 4-5 hours spread across several days and will be tasked with brainstorming ideas and preparing materials related to their topic. Leads are further expected to participate in plenary sessions held on Monday July 20 and Friday July 24 (4-5 hrs of additional time) where these materials will be discussed.</div><div>If you are interested in participating in the workshop, please fill out <a href="https://forms.gle/cdCTsLfUs56CDhKS9" style="color: #3f9291;">this Google form</a> by <strong>Monday June 15</strong>. On this form, applicants are asked to contribute one or two <span class="im">major results in the last 10 years whose significance can be explained in layperson terms, and one or two major challenges for theory whose significance can be explained in layperson terms. These descriptions </span>can be very brief. We will just use them to select participants and create breakout groups.</div></div>
    </content>
    <updated>2020-06-06T22:40:00Z</updated>
    <published>2020-06-06T22:40:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2020-06-09T07:46:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4839</id>
    <link href="https://www.scottaaronson.com/blog/?p=4839" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4839#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4839" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Jonathan Dowling (1955-2020)</title>
    <summary xml:lang="en-US">Today I woke up to the sad and shocking news that Jon Dowling (homepage / Twitter / Wikipedia)—physics professor at Louisiana State, guy who got the US government to invest in quantum computing back in the 90s, author of the popular book Schrödinger’s Killer App: Race to Build the World’s First Quantum Computer, investigator of […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><figure class="wp-block-image"><img alt="" src="http://phys.lsu.edu/~jdowling/index_files/Dowling.jpg"/></figure>



<p>Today I woke up to the sad and shocking news that Jon Dowling (<a href="http://phys.lsu.edu/~jdowling/">homepage</a> / <a href="https://twitter.com/jpdowling?lang=en">Twitter</a> / <a href="https://en.wikipedia.org/wiki/Jonathan_Dowling">Wikipedia</a>)—physics professor at Louisiana State, guy who got the US government to invest in quantum computing back in the 90s, author of the popular book <a href="https://www.amazon.com/Schr%C3%B6dingers-Killer-App-Jonathan-Dowling/dp/1439896739">Schrödinger’s Killer App: Race to Build the World’s First Quantum Computer</a>, investigator of BosonSampling among many other topics, owner of a “QUBIT” license plate, and one of my main competitors in the field of quantum computing humor—has passed away at age 65, apparently due to an aortic aneurysm.</p>



<p>Three months ago, right before covid shut down the world, the <em>last</em> travel I did was a seven-hour road trip from Austin to Baton Rouge, together with my postdoc <a href="https://twitter.com/a_rocchetto?lang=en">Andrea Rocchetto</a>, to deliver something called the <a href="https://calendar.lsu.edu/event/the_hearne_eminent_lecture_with_scott_aaronson#.Xtv1HDpKg2w">Hearne Lecture</a> at the Louisiana State physics department.  My topic (unsurprisingly) was Google’s quantum supremacy experiment.</p>



<p>I’d debated whether to cancel the trip, as flying already seemed too dangerous.  Dowling was the one who said “why not just drive here with one of your postdocs?”—which turned into a memorable experience for me and Andrea, complete with a personal tour of <a href="https://en.wikipedia.org/wiki/LIGO">LIGO</a> and a visit to an alligator hatchery.  I had no inkling that it was the last time I’d ever see Jon Dowling, but am now super-glad that we made the visit.</p>



<p>At the dinner after my talk, Dowling was exactly the same as every other time I’d seen him: loud, piss-drunk, obnoxious, and hilarious.  He dominated the conversation with stories and jokes, referring in every other sentence either to his Irishness or my Jewishness.  His efforts to banter with the waitress, to elicit her deepest opinions about each appetizer and bottle of wine, were so over-the-top that I, sitting next to him, blushed, as if to say, “hey, I’m just the visitor here!  I don’t necessarily <em>endorse</em> this routine!”</p>



<p>But Dowling got away with it because, no matter how many taboos he violated per sentence, there was never any hint of malice in it.  He was an equal-opportunity offender, with his favorite target being himself.  He loved to talk, for example, about my pathological obsession with airy-fairy abstractions, like some kind of “polynomial hierarchy” that hopefully wouldn’t “collapse”—with the punchline being that he, the hardheaded laser physicist, then needed to learn what that meant for his own research.</p>



<p>The quantum computing community of the southern US, not to mention of Twitter and Facebook, and indeed of the entire world, will be poorer without this inimitable, louder-than-life presence.</p>



<p>Feel free to share your own Dowling stories in the comments.</p></div>
    </content>
    <updated>2020-06-06T20:37:22Z</updated>
    <published>2020-06-06T20:37:22Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Adventures in Meatspace"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="CS/Physics Deathmatch"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-06-08T04:26:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=441</id>
    <link href="https://tcsplus.wordpress.com/2020/06/06/tcs-talk-wednesday-june-10-cliff-stein-columbia-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, June 10 — Cliff Stein, Columbia University</title>
    <summary>In solidarity with the current protests in the United States (https://www.shutdownstem.com/), our speaker asked that the talk this coming Wednesday be postponed. Cliff’s talk has been rescheduled to Thursday, June 18th; we will keep you updated in case of any further change. We would also take this opportunity to call the TCS community for feedback, […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>In solidarity with the current protests in the United States (<a href="https://www.shutdownstem.com/" rel="nofollow">https://www.shutdownstem.com/</a>), our speaker asked that the talk this coming Wednesday be postponed.</b></p>
<p><strong>Cliff’s talk has been rescheduled to <span style="color: #ff0000;">Thursday, June 18th</span>; we will keep you updated in case of any further change. We would also take this opportunity to <a href="https://sites.google.com/site/plustcs/suggest">call the TCS community for feedback, and for suggestions and comments</a>. As a small team organizing this virtual seminar, we are continuously trying to improve and welcome your input.</strong></p>
<p>The next TCS+ talk<em> (and the last of the season!)</em> will take place this coming Wednesday, June 10th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Cliff Stein</strong> from Columbia University will speak about “<em>Parallel Approximate Undirected Shortest Paths Via Low Hop Emulators</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br/>
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see &gt;the website.</p>
<blockquote><p>Abstract: Although sequential algorithms with (nearly) optimal running time for finding shortest paths in undirected graphs with non-negative edge weights have been known for several decades, near-optimal parallel algorithms have turned out to be a much tougher challenge. In this talk, we present a <img alt="(1+\varepsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=%281%2B%5Cvarepsilon%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="(1+\varepsilon)"/>-approximate parallel algorithm for computing shortest paths in undirected graphs, achieving polylog depth and near-linear work. All prior <img alt="(1+\varepsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=%281%2B%5Cvarepsilon%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="(1+\varepsilon)"/>-algorithms with polylog depth perform at least superlinear work. Improving this long-standing upper bound obtained by Cohen (STOC’94) has been open for 25 years.</p>
<p>Our algorithm uses several new tools. Prior work uses hopsets to introduce shortcuts in the graph. We introduce a new notion that we call low hop emulators. We also introduce compressible preconditioners, which we use in conjunction with Serman’s framework (SODA ’17) for the uncapacitated minimum cost flow problem.</p>
<p>Joint work with Alex Andoni and Peilin Zhong.</p></blockquote></div>
    </content>
    <updated>2020-06-06T18:16:14Z</updated>
    <published>2020-06-06T18:16:14Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-06-10T10:21:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7750</id>
    <link href="https://windowsontheory.org/2020/06/06/tcs-visioning-workshop/" rel="alternate" type="text/html"/>
    <title>TCS visioning workshop</title>
    <summary>[From Jelani Nelson and David Woodruff. This workshop can be very important to ensure TCS is represented in what is likely to be a difficult funding environment in coming years. –Boaz ] The CATCS will be hosting a virtual “Visioning Workshop” the week of July 20 in order to identify broad research themes within theoretical computer science (TCS) that have potential for […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[From Jelani Nelson and David Woodruff. This workshop can be very important to ensure TCS is represented in what is likely to be a difficult funding environment in coming years. –Boaz ]</em> </p>



<p>The <a href="https://thmatters.wordpress.com/catcs/">CATCS</a> will be hosting a virtual <strong>“Visioning Workshop”</strong> the <strong>week of </strong><strong>July 20</strong> in order to identify broad research themes within theoretical computer science (TCS) that have potential for a major impact in the future. The goals are similar to the <a href="https://thmatters.wordpress.com/visioning-workshop/">workshop of the same name</a> in 2008: to package these themes in a way that can be consumed by the general public, which we would deliver primarily to the Computing Community Consortium and others (e.g. funding agencies) to help them advocate for TCS.</p>



<p>While participation in the workshop is primarily through invitation, we have a few slots available for the broader community. If you are interested in participating, please see details of the application process below. The workshop will be organized according to area-wise breakout groups. Each breakout group will have 1-2 leads. Breakout groups will meet for 4-5 hours spread across several days and will be tasked with brainstorming ideas and preparing materials related to their topic. Leads are further expected to participate in plenary sessions held on Monday July 20 and Friday July 24 (4-5 hrs of additional time) where these materials will be discussed.</p>



<p>If you are interested in participating in the workshop, please fill out <a href="https://forms.gle/cdCTsLfUs56CDhKS9">this Google form</a> by <strong>Monday June 15</strong> ( <a href="https://forms.gle/cdCTsLfUs56CDhKS9">https://forms.gle/cdCTsLfUs56CDhKS9</a> ). On this form, applicants are asked to contribute one or two major results in the last 10 years whose significance can be explained in layperson terms, and one or two major challenges for theory whose significance can be explained in layperson terms. <em>[The results need not and generally will not be your own. They just should be easy-to-explain major results in your research area–Boaz] </em>These descriptions can be very brief. We will just use them to select participants and create breakout groups.</p></div>
    </content>
    <updated>2020-06-06T05:24:17Z</updated>
    <published>2020-06-06T05:24:17Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-06-10T10:21:04Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-25562705.post-4113531432946112122</id>
    <link href="http://aaronsadventures.blogspot.com/feeds/4113531432946112122/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=4113531432946112122" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/4113531432946112122" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/4113531432946112122" rel="self" type="application/atom+xml"/>
    <link href="http://aaronsadventures.blogspot.com/2020/06/tcs-visioning-workshop-call-for.html" rel="alternate" type="text/html"/>
    <title>TCS Visioning Workshop — Call for Participation</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Reposting from here: <a href="https://thmatters.wordpress.com/2020/06/05/tcs-visioning-workshop-call-for-participation/">https://thmatters.wordpress.com/2020/06/05/tcs-visioning-workshop-call-for-participation/</a><br/><br/><div><div style="line-height: 22px; padding: 0px 0px 20px;"><span class="im">The <a href="https://thmatters.wordpress.com/catcs/" style="color: #3f9291;">CATCS</a> will be hosting a virtual <strong>“Visioning Workshop”</strong> the <strong>week of </strong></span><strong>July 20</strong> in order to identify broad research themes within theoretical computer science (TCS) that have potential for a major impact in the future. The goals are similar to the <a href="https://thmatters.wordpress.com/visioning-workshop/" style="color: #3f9291;">workshop of the same name</a> in 2008: to package these themes in a way that can be consumed <span class="im">by the general public, which we would deliver primarily to the Computing Community Consortium and others (e.g. funding agencies) to help them advocate for TCS.</span></div><div style="line-height: 22px; padding: 0px 0px 20px;">While participation in the workshop is primarily through invitation, we have a few slots available for the broader community. If you are interested in participating, please see details of the application process below. The workshop will be organized according to area-wise breakout groups. Each breakout group will have 1-2 leads. Breakout groups will meet for 4-5 hours spread across several days and will be tasked with brainstorming ideas and preparing materials related to their topic. Leads are further expected to participate in plenary sessions held on Monday July 20 and Friday July 24 (4-5 hrs of additional time) where these materials will be discussed.</div><div style="line-height: 22px; padding: 0px 0px 20px;">If you are interested in participating in the workshop, please fill out <a href="https://forms.gle/cdCTsLfUs56CDhKS9" style="color: #3f9291;">this Google form</a> by <strong>Monday June 15</strong>. On this form, applicants are asked to contribute one or two <span class="im">major results in the last 10 years whose significance can be explained in layperson terms, and one or two major challenges for theory whose significance can be explained in layperson terms. These descriptions </span>can be very brief. We will just use them to select participants and create breakout groups.</div></div><div/><div><br/></div></div>
    </content>
    <updated>2020-06-05T22:59:00Z</updated>
    <published>2020-06-05T22:59:00Z</published>
    <author>
      <name>Aaron</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/09952936358739421126</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-25562705</id>
      <category term="game theory"/>
      <category term="news"/>
      <author>
        <name>Aaron</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/09952936358739421126</uri>
      </author>
      <link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <title>Adventures in Computation</title>
      <updated>2020-06-06T09:46:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1312</id>
    <link href="https://thmatters.wordpress.com/2020/06/05/tcs-visioning-workshop-call-for-participation/" rel="alternate" type="text/html"/>
    <title>TCS Visioning Workshop — Call for Participation</title>
    <summary>The CATCS will be hosting a virtual “Visioning Workshop” the week of July 20 in order to identify broad research themes within theoretical computer science (TCS) that have potential for a major impact in the future. The goals are similar to the workshop of the same name in 2008: to package these themes in a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>
<p><span class="im">The <a href="https://thmatters.wordpress.com/catcs/">CATCS</a> will be hosting a virtual <strong>“Visioning Workshop”</strong> the <strong>week of </strong></span><strong>July 20</strong> in order to identify broad research themes within theoretical computer science (TCS) that have potential for a major impact in the future. The goals are similar to the <a href="https://thmatters.wordpress.com/visioning-workshop/">workshop of the same name</a> in 2008: to package these themes in a way that can be consumed <span class="im">by the general public, which we would deliver primarily to the Computing Community Consortium and others (e.g. funding agencies) to help them advocate for TCS.</span></p>
<p>While participation in the workshop is primarily through invitation, we have a few slots available for the broader community. If you are interested in participating, please see details of the application process below. The workshop will be organized according to area-wise breakout groups. Each breakout group will have 1-2 leads. Breakout groups will meet for 4-5 hours spread across several days and will be tasked with brainstorming ideas and preparing materials related to their topic. Leads are further expected to participate in plenary sessions held on Monday July 20 and Friday July 24 (4-5 hrs of additional time) where these materials will be discussed.</p>
<p>If you are interested in participating in the workshop, please fill out <a href="https://forms.gle/cdCTsLfUs56CDhKS9">this Google form</a> by <strong>Monday June 15</strong>. On this form, applicants are asked to contribute one or two <span class="im">major results in the last 10 years whose significance can be explained in layperson terms, and one or two major challenges for theory whose significance can be explained in layperson terms. These descriptions </span>can be very brief. We will just use them to select participants and create breakout groups.</p>
</div>
<div/>
<div>(If the embedded link doesn’t work, paste this URL in your browser: <a href="https://forms.gle/cdCTsLfUs56CDhKS9" rel="nofollow">https://forms.gle/cdCTsLfUs56CDhKS9</a>.)</div></div>
    </content>
    <updated>2020-06-05T18:46:13Z</updated>
    <published>2020-06-05T18:46:13Z</published>
    <category term="workshops"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2020-06-10T10:21:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/085</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/085" rel="alternate" type="text/html"/>
    <title>TR20-085 |  Neural Networks with Small Weights and Depth-Separation Barriers | 

	Gal Vardi, 

	Ohad Shamir</title>
    <summary>In studying the expressiveness of neural networks, an important question is whether there are functions which can only be approximated by sufficiently deep networks, assuming their size is bounded. However, for constant depths, existing results are limited to depths $2$ and $3$, and achieving results for higher depths has been an important open question. In this paper, we focus on feedforward ReLU networks, and prove fundamental barriers to proving such results beyond depth $4$, by reduction to open problems and natural-proof barriers in circuit complexity. To show this, we study a seemingly unrelated problem of independent interest: Namely, whether there are polynomially-bounded functions which require super-polynomial weights in order to approximate with constant-depth neural networks. We provide a negative and constructive answer to that question, by showing that if a function can be approximated by a polynomially-sized, constant depth $k$ network with arbitrarily large weights, it can also be approximated by a polynomially-sized, depth $3k+3$ network, whose weights are polynomially bounded.</summary>
    <updated>2020-06-05T00:21:20Z</updated>
    <published>2020-06-05T00:21:20Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-10T10:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17119</id>
    <link href="https://rjlipton.wordpress.com/2020/06/04/the-truth/" rel="alternate" type="text/html"/>
    <title>The Truth</title>
    <summary>What is the truth? Alfred Whitehead was a logician and philosopher, who had a student of some note. The student was Bertrand Russell and together they wrote the famous three-volume Principia Mathematica. It took several hundred pages to get to the result that . Amazing. Today I thought that discussing truth might be an interesting […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>What is the truth?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/06/04/the-truth/whead/" rel="attachment wp-att-17122"><img alt="" class="alignright wp-image-17122" src="https://rjlipton.files.wordpress.com/2020/06/whead.jpg?w=160" width="160"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Alfred Whitehead was a logician and philosopher, who had a student of some note. The student was Bertrand Russell and together they wrote the famous three-volume <a href="https://en.wikipedia.org/wiki/Principia_Mathematica">Principia Mathematica</a>. It took several hundred pages to get to the result that <img alt="{1+1=2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2B1%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1+1=2}"/>.</p>
<p><a href="https://rjlipton.wordpress.com/2020/06/04/the-truth/pm/" rel="attachment wp-att-17124"><img alt="" class="aligncenter size-medium wp-image-17124" height="125" src="https://rjlipton.files.wordpress.com/2020/06/pm.png?w=300&amp;h=125" width="300"/></a></p>
<p>
Amazing. </p>
<p>
Today I thought that discussing truth might be an interesting topic.<br/>
<span id="more-17119"/></p>
<p>
Whitehead said: </p>
<blockquote><p><b> </b> <em> There are no whole truths; all truths are half-truths. It is trying to treat them as whole truths that plays the devil. </em>
</p></blockquote>
<p/><p>
I like this quote. Whitehead was not the best lecturer, however. He gave the prestigious <a href="https://en.wikipedia.org/wiki/Gifford_lectures">Gifford lectures</a> a year after the astronomer Arthur Eddington. As Wikipedia <a href="https://en.wikipedia.org/wiki/Alfred_North_Whitehead">relates</a> quoting Victor Lowe: </p>
<blockquote><p><b> </b> <em> Eddington was a marvellous popular lecturer who had enthralled an audience of 600 for his entire course. The same audience turned up to Whitehead’s first lecture but it was completely unintelligible, not merely to the world at large but to the elect. My father remarked to me afterwards that if he had not known Whitehead well he would have suspected that it was an imposter making it up as he went along … The audience at subsequent lectures was only about half a dozen in all. </em>
</p></blockquote>
<p>Between the pandemic and the unrest in our cities there is debate about what is the “truth”. On cable news—CNN, MSNBC, FOX—one hears statements about the truth. You can also hear statements like “the experts know” or the “model” shows that this is true. Can math shed light on these discussions? What would Whitehead say?</p>
<p>
</p><p/><h2> What is the Truth? </h2><p/>
<p/><p>
Mathematical truth is the one absolute we can count on—right? Math is precise in its own way, but does it yield truth? Not so clear.</p>
<p>
Whitehead’s proof that <img alt="{1+1=2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2B1%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1+1=2}"/> takes 100’s of pages; it may or may not increase your confidence. Here is a short “proof” that <img alt="{2=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2=1}"/>:</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/06/04/the-truth/proof-5/" rel="attachment wp-att-17125"><img alt="" class="aligncenter size-medium wp-image-17125" height="195" src="https://rjlipton.files.wordpress.com/2020/06/proof.png?w=300&amp;h=195" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
Math proofs are only as safe as two elements that are unavoidably social: </p>
<ol>
<li>
The care we use in applying our reasoning; and <p/>
</li><li>
The care we use in choosing our assumptions.
</li></ol>
<p>
In the above proof snippet, one step divided by <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> which is the source of the error that <img alt="{2=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2=1}"/>. A more worrisome issue is reasoning from assumptions. Wrong assumptions are a problem.</p>
<p>
</p><p/><h2> Who are the Experts? </h2><p/>
<p/><p>
One definition of <a href="https://en.wikipedia.org/wiki/Expert">expert</a> is: An expert is somebody who has a broad and deep competence in terms of knowledge, skill and experience through practice and education in a particular field. </p>
<p>
More amusing definitions are: </p>
<blockquote><p><b> </b> <em> Mark Twain defined an expert as “an ordinary fellow from another town.” Will Rogers described an expert as “A man fifty miles from home with a briefcase.” Danish scientist and Nobel laureate Niels Bohr defined an expert as “A person that has made every possible mistake within his or her field.” </em>
</p></blockquote>
<p/><p>
I find the use of the term <i>expert</i> in regard to the pandemic at best puzzling. How can anyone be an expert when the current situation is unique? The last pandemic happened over a hundred years ago. Unfortunately Twain, Rogers, and Bohr are closer to being correct. The situation we find ourselves in today does not lend itself to being an expert. At least in my non-expert opinion.</p>
<p><a href="https://rjlipton.wordpress.com/2020/06/04/the-truth/not/" rel="attachment wp-att-17131"><img alt="" class="aligncenter size-medium wp-image-17131" height="102" src="https://rjlipton.files.wordpress.com/2020/06/not.jpg?w=300&amp;h=102" width="300"/></a></p>
<p/><p>
Yes there are people, for example, who are experts on various viral agents. But there is more we do not know about this agent that we do know. </p>
<ul>
<li>
Can you get the virus twice? <p/>
</li><li>
Can children get the virus? <p/>
</li><li>
Will a vaccine be possible? <p/>
</li><li>
Are there long-term affects even for those who survive? <p/>
</li><li>
And so on <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/>
</li></ul>
<p>
</p><p/><h2> Where are the Models? </h2><p/>
<p/><p>
Models are created by experts, so you probably guess that I am not bullish on models. There are lots of models, for example, on the projection of how many will be infected, and how many will get seriously sick, and sadly how many will succumb. These models are based on various assumptions about how the virus works. Most of these assumptions are not proved in any sense. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I plan on saying more about truth in the future. Take care.</p>
<p/></font></font></div>
    </content>
    <updated>2020-06-04T16:18:49Z</updated>
    <published>2020-06-04T16:18:49Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="experts"/>
    <category term="models"/>
    <category term="Proofs"/>
    <category term="truth"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-06-10T10:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/2020/06/04/itc-2020-program-is-out/</id>
    <link href="https://windowsontheory.org/2020/06/04/itc-2020-program-is-out/" rel="alternate" type="text/html"/>
    <title>ITC 2020 program is out</title>
    <summary>Guest post by Benny Applebaum The ITC 2020 program is out, and this newborn looks healthy and strong! The program contains exciting new works in the area of Information-Theoretic Cryptography, confirming the importance of this new venue. The PC, chaired by Daniel Wichs, also chose an amazing sequence of invited talks by Dachman-Soled, Natarajan, Jafar, […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Guest post by Benny Applebaum</p>



<p>The<a href="https://itcrypto.github.io/2020main.html" rel="noreferrer noopener" target="_blank"> ITC 2020 program</a> is out, and this newborn looks healthy and strong! The program contains exciting new works in the area of Information-Theoretic Cryptography, confirming the importance of this new venue. </p>



<p>The PC, chaired by Daniel Wichs, also chose an amazing sequence of invited talks by Dachman-Soled, Natarajan, Jafar, Kol, Raz, and Vaikuntanathan, so this can also be a good opportunity to hear about the big recent developments in the area.</p>



<p><br/>The conference will be virtual this year. Participation is free but requires registration. We hope to see many of you there</p>



<p/></div>
    </content>
    <updated>2020-06-04T16:12:15Z</updated>
    <published>2020-06-04T16:12:15Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-06-10T10:21:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/06/04/211-0051-20-2k-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020-2/</id>
    <link href="https://cstheory-jobs.org/2020/06/04/211-0051-20-2k-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020-2/" rel="alternate" type="text/html"/>
    <title>211-0051/20-2K Professor of Machine Learning at Department of Computer Science, Faculty of Science, University of Copenhagen (apply by August 9, 2020)</title>
    <summary>The Department of Computer Science at the University of Copenhagen is seeking candidates for a full professorship in Machine Learning. The new professor is expected to make a significant contribution to leadership in the Machine Learning field within the Department of Computer Science. Website: https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151901&amp;DepartmentId=18971&amp;MediaId=4642 Email: c.lioma@di.ku.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Copenhagen is seeking candidates for a full professorship in Machine Learning. The new professor is expected to make a significant contribution to leadership in the Machine Learning field within the Department of Computer Science.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151901&amp;DepartmentId=18971&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151901&amp;DepartmentId=18971&amp;MediaId=4642</a><br/>
Email: c.lioma@di.ku.dk</p></div>
    </content>
    <updated>2020-06-04T09:24:04Z</updated>
    <published>2020-06-04T09:24:04Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-10T10:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/06/04/211-0051-20-2k-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020/</id>
    <link href="https://cstheory-jobs.org/2020/06/04/211-0051-20-2k-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020/" rel="alternate" type="text/html"/>
    <title>211-0051/20-2K Professor of Machine Learning at Department of Computer Science, Faculty of Science, University of Copenhagen (apply by August 9, 2020)</title>
    <summary>The Department of Computer Science at the University of Copenhagen is seeking candidates for a full professorship in Machine Learning. The new professor is expected to make a significant contribution to leadership in the Machine Learning field within the Department of Computer Science. Website: https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151901&amp;DepartmentId=18971&amp;MediaId=4642 Email: c.lioma@di.ku.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Copenhagen is seeking candidates for a full professorship in Machine Learning. The new professor is expected to make a significant contribution to leadership in the Machine Learning field within the Department of Computer Science.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151901&amp;DepartmentId=18971&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151901&amp;DepartmentId=18971&amp;MediaId=4642</a><br/>
Email: c.lioma@di.ku.dk</p></div>
    </content>
    <updated>2020-06-04T09:21:08Z</updated>
    <published>2020-06-04T09:21:08Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-10T10:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/06/04/211-0243-20-2e-associate-professor-of-machine-learning-at-department-of-computer-science-university-of-copenhagen-apply-by-august-9-2020/</id>
    <link href="https://cstheory-jobs.org/2020/06/04/211-0243-20-2e-associate-professor-of-machine-learning-at-department-of-computer-science-university-of-copenhagen-apply-by-august-9-2020/" rel="alternate" type="text/html"/>
    <title>211-0243/20-2E Associate Professor of Machine Learning at Department of Computer Science, University of Copenhagen (apply by August 9, 2020)</title>
    <summary>The Department of Computer Science at the University of Copenhagen is seeking candidates for an associate professorship in Machine Learning. The new associate professor is expected to make a significant contribution to leadership in the Machine Learning field within the Department of Computer Science. Website: https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151903&amp;DepartmentId=18971&amp;MediaId=4642 Email: c.lioma@di.ku.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Copenhagen is seeking candidates for an associate professorship in Machine Learning. The new associate professor is expected to make a significant contribution to leadership in the Machine Learning field within the Department of Computer Science.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151903&amp;DepartmentId=18971&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151903&amp;DepartmentId=18971&amp;MediaId=4642</a><br/>
Email: c.lioma@di.ku.dk</p></div>
    </content>
    <updated>2020-06-04T09:18:39Z</updated>
    <published>2020-06-04T09:18:39Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-10T10:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/06/04/211-0062-20-2n-tenure-track-assistant-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020/</id>
    <link href="https://cstheory-jobs.org/2020/06/04/211-0062-20-2n-tenure-track-assistant-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020/" rel="alternate" type="text/html"/>
    <title>211-0062/20-2N Tenure-track assistant Professor of Machine Learning at Department of Computer Science, Faculty of Science, University of Copenhagen (apply by August 9, 2020)</title>
    <summary>The Department of Computer Science at the University of Copenhagen is seeking candidates for a tenure track assistant professorship in Machine Learning. The tenure track assistant professor’s duties will primarily include research, including obligations with regard to publication/scientific communication and research-based teaching with associated examination obligations within Machine Learning. Website: https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151899&amp;DepartmentId=18971&amp;MediaId=4642 Email: c.lioma@di.ku.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Copenhagen is seeking candidates for a tenure track assistant professorship in Machine Learning. The tenure track assistant professor’s duties will primarily include research, including obligations with regard to publication/scientific communication and research-based teaching with associated examination obligations within Machine Learning.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151899&amp;DepartmentId=18971&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151899&amp;DepartmentId=18971&amp;MediaId=4642</a><br/>
Email: c.lioma@di.ku.dk</p></div>
    </content>
    <updated>2020-06-04T08:39:30Z</updated>
    <published>2020-06-04T08:39:30Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-10T10:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4834</id>
    <link href="https://www.scottaaronson.com/blog/?p=4834" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4834#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4834" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Pooled testing for covid: Guest post by Zeph Landau</title>
    <summary xml:lang="en-US">Scott’s foreword: Zeph Landau, a noted quantum computing theorist at UC Berkeley who’s worked closely with my adviser Umesh Vazirani, recently asked me if he could write a guest post about pooled testing for covid—an old idea that, Zeph argues, could play a crucial role in letting universities safely reopen this fall. Seeing a small […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Scott’s foreword:</span></strong> <a href="https://people.eecs.berkeley.edu/~landau/">Zeph Landau</a>, a noted quantum computing theorist at UC Berkeley who’s worked closely with my adviser Umesh Vazirani, recently asked me if he could write a guest post about <a href="https://en.wikipedia.org/wiki/Group_testing">pooled testing</a> for covid—an old idea that, Zeph argues, could play a crucial role in letting universities safely reopen this fall.  Seeing a small chance to do a great good, I readily agreed.</p>



<p>I should confess that I’m more … fatalistic than Zeph.  Not that I’m proud of it: I think that Zeph’s attitude is superior to mine.  But, like, I’m a theoretical computer scientist with zero expertise in medical testing or statistics, and <em>I</em> knew about pooled testing and its WWII origins—so imagine how thoroughly the actual experts must know the idea.  Just like they know all about variolation, and challenge trials, and copper fixtures, and UV light, and vitamin D supplements, and a dozen other possible tools against covid that future historians might ask why we didn’t try more.</p>



<p>As I’ve written before, I think our fundamental problem is not a lack of good ideas.  It’s that, outside of some isolated pockets of progress, our entire civilization no longer has the will (or ability? is there a difference?) to <em>implement</em> good ideas, or even really to try them.  For anything new that requires coordination, today there are just too many stakeholders who need to be brought on board, too many risks that need further study.  So I see Zeph, and anyone like him, as occupying a tragic position, a bit like that of an Aztec advocating the use of the wheel.  “Sure,” the Aztec elders might calmly reply, “wheeled transport is obvious enough that we’ve <em>all</em> considered it, but a moment’s thought reveals why, in our actually existing empire, it would be reckless, costly, and of at most marginal benefit…”</p>



<p>But I hope I’m wrong!  Better, I hope this post is the one that <em>proves</em> me wrong!  So without further ado, here’s…</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Zeph Landau’s Guest Post</span></strong></p>



<p>This post describes how every university could efficiently use modest testing resources to sensibly and extensively reduce the number of COVID-19 cases on their campus this fall.  It is meant as a call to action to the reader—because without a concerted effort to get the right people the necessary information and take immediate consequential action, a far worse alternative will be implemented almost everywhere. It is my sincere hope, that immediately after reading this post, you will take the following steps:</p>



<p>1) Figure out who is part of the reopening committee at your institution.</p>



<p>2) Find the right people and engage with them either as a fellow faculty member or, better yet, through a connection to get them good information about the information posted here.</p>



<p>3) Then stay engaged and keep pushing.  (See below for links to sample documents.)</p>



<p>OK, here we go.</p>



<p><strong>The Problem</strong></p>



<p>How can we safely open a university or college campus such that we ensure that the number of cases does not drastically increase through the newfound interactions between the population?</p>



<p>One obvious, albeit impractical, solution to opening universities is to test everyone, everyday and isolate those that test positive quickly.  Unfortunately, we can’t do that due to costs ($100 per student per day) and availability of tests (on the order of 1000 tests per day at university testing labs).</p>



<p><em>Turns out there is a solution that uses drastically fewer tests and is commensurate in detecting an outbreak. It is called pooled screening which is a variant of pooled testing.</em></p>



<p><strong>The missing piece: early detection surveillance</strong></p>



<p>So how do we detect most contagious people quickly if we don’t have the resources to test everyone regularly?  The answer is by pooled testing—or to be more accurate (I’ll be clear about why this distinction is important later) <strong>pooled screening. </strong>The idea of pooling is old (attributed to Dorfman in the 40’s), simple, and has been used over and over in all kinds of scenarios. Pooled testing works by mixing samples together from a group and then administering a single test to the mixture. The test is designed to be sensitive enough to come up positive whenever <em>at least one</em> underlying sample is positive. Instead of testing each sample individually, you test the mixture, and then only those groups that test positive undergo a second round of testing of each individual sample. The individuals do not need to deliver a second sample; there is more than enough biological material for multiple tests per sample. When prevalence of a disease is low, most pools come up negative and you save a large amount of testing resources and time.  (For those more visually inclined, here is a one minute <a href="https://www.youtube.com/watch?v=pdC6irz_vys&amp;feature=youtu.be" rel="noreferrer noopener" target="_blank">video</a> on pooled testing.)</p>



<p>So what would a good early detection surveillance system look like?  Here is a reasonable and doable framework:</p>



<ul><li>Divide the campus population into three groups (call them A, B, and C).</li><li>Collect samples from each group twice a week, (e.g. Group A: M/Th, Group B: Tu/Fri, Group C Wed/Sat).</li><li>Pool test the samples in groups of 16.</li></ul>



<p>What kinds of resources would this use?</p>



<ul><li>For a 10,000 person campus, you’d need about 200 tests per day, 6 days a week.  The universities that have implemented testing labs typically have the capacity to do on the order of 1000 tests a day.</li><li>Assuming a rough cost of $100 a test (which should be an overestimate if they are using their own lab), it would amount to a $12 a student/ per week.  </li></ul>



<p>What would it accomplish?  It would quickly find outbreaks and new cases.  Under a few different assumptions of the time-course of the viral load in a person, <em>the expected time for detecting an infectious person in this scheme is under 3 days. </em>Those cases would then need to be fed into an existing contact tracing and quarantine protocol.  The result: an outbreak suppressed before it had a chance to get going.</p>



<p>So why aren’t we already doing this?  Read on…</p>



<p><strong>The fear of false negatives in pooling</strong></p>



<p>The general concern to implementing pooling  for Covid-19 in the US is two-fold. </p>



<ol><li>Without the creation of a better test the dilution effect will make the test less sensitive and in turn produce more false negatives.  </li><li>Even if you could solve the scientific sensitivity issue, navigating the process of getting government approval is a big barrier.</li></ol>



<p>Let’s take each of these concerns in turn.  The first is definitely a concern if the goal is 1:1 medical testing.  If a sample can be barely seen as positive in an individual test, then the risk is that the dilution effect when pooled with others will cause the group test to come out negative—giving a wrong result to the positive individual.  The word for this is “sensitivity”, i.e. if a test has 95% sensitivity it means that it’ll be accurate 95% of the time and produce a false negative 5% of the time.  So how sensitive would a pooled test be where you combined 16 individual samples into 1 and just ran it through an existing 1:1 test?  Lab data suggests it would have at least 70% sensitivity.  For 1:1 testing this is a non-starter, however, <strong>the goal is early detection </strong>of an outbreak, which is different and as we shall see, a 70% sensitivity does fine for this purpose.</p>



<p>Suppose you are doing early detection surveillance and imagine that an outbreak starts.  Imagine 3 people are infected.  Because you are sampling every 3 days, you’ll be getting at least 6 positive samples, and the chances that your 70% screen misses all 6 is tiny.  As soon as it catches one, a contact tracing protocol is initiated and the others will be found.</p>



<p>Another way to formulate what is going on is that you are trading sensitivity for speed (in the form of capacity and cost)—and that is a huge win.  The pooling and more frequent testing gives you that speed versus sensitivity tradeoff.  Sure, Lebron James (a 70% free-throw shooter) won’t make every free throw, but the chance that he misses 6 in a row is tiny.</p>



<p>For some, the above thinking is straightforward.  However, for the medical testing paradigm—where the goal is the most accurate test for an individual using the one sample you have—this point of view is foreign and in many ways almost out of reach.   </p>



<p>OK.  So with the concern of sensitivity laid to rest, what about the second concern?  That the regulations will get in the way.  It turns out that this isn’t an issue though again, it is slightly counterintuitive for those who work in medical testing.  The task is surveillance, and therefore the pooling test is being used as a screen (not a medical test): negative group tests are not reported to the individual as a negative test result.   Positive groups are deconvoluted for individual testing and results returned to the person who is positive individually.  HHS/CLIA has indicated there aren’t regulatory restrictions as long as you don’t return test results due to the pooled test.</p>



<p>It is important to re-emphasize that the above is for pooled screening (where negative results are not returned), which is in contrast to pooled testing (where negative pools are reported as negative test results for each individual).  For pooled testing, which has received a jump of coverage due to its use recently in Wuhan, there are large regulatory hurdles—the CDC is just formulating criteria for clearing those hurdles and the science looks like, for now, that most labs wouldn’t be able to get above pools of size 5 or so.</p>



<p><strong>How do you safely collect so many samples?</strong></p>



<p>A different direction of concern for early detection surveillance is the logistics and feasibility around collecting samples.  To date, the gold standard for sampling is a deep nasal swab that requires a professional to do it, requires PPE equipment, and is not a pleasant experience.  Using this method wouldn’t work logistically on campus.</p>



<p>However, there are other sampling techniques that allow people to self-sample, both in the form of a shallow nasal swab and saliva based techniques.  The stated concern is obvious: there is a worry that these sampling techniques are less sensitive.  There is some evidence that this is not the case (and even the opposite) but regardless, as has been discussed— in early detection surveillance it is OK to take a hit on sensitivity.  The system remains robust because of the frequent testing and the goal of detecting an outbreak, not every individual.</p>



<p>Being able to self-sample removes a huge bottleneck.  The picture is very much simplified.  Students/faculty/staff self-sample on their prescribed days (either in the presence of a medical professional or not depending on the approved protocol) and then drop off their sample at any of various drop-off stations on campus.  Those stations deliver the samples to the testing facility for pooling and testing.</p>



<p><strong>You can help to get this done</strong></p>



<p>Is what I’m describing a new idea?  As far as I can tell, the answer is both no and yes.  Pooled testing is in the news both as a theoretical idea and now as being implemented at some scale—in Israel, in a lab in Nebraska, and most recently in Wuhan.   But using pooling as a screen (not a medical test) within an early detection surveillance system that repeatedly screens everyone is, as far as I know, not in the discussion.</p>



<p>What seems clear is that right now—reopening committees and labs are perhaps aware of the idea of pooling but only as a theoretical idea of a technology that might be coming at some vague time in the future.  They are unaware that in the form of early detection surveillance, it is right in front of them ready to go.  They’d need a matter of weeks to convert a 1:1 lab into a lab that could handle both pooled screening and 1:1 testing (this <a href="https://www.medrxiv.org/content/10.1101/2020.04.17.20069062v2" rel="noreferrer noopener" target="_blank">lab</a> did it, <a href="https://docs.google.com/document/d/1FVp55JAs2heqV5ktyGVVdoE-W_XTKmZgCWBy1SEoo-o/edit?usp=sharing" rel="noreferrer noopener" target="_blank">here</a> is a brief outline of the steps).  In the same timeline, they could develop a system for handling the logistics of sampling large numbers of people.</p>



<p><strong>And that is where each of you come in… </strong>  you can help get these ideas to the right people.  It needs to be done quickly because decisions are being made now as to what to do.  The right people are your colleagues—you just have to find out who they are and reach out to them personally.  You can find out who is on the reopening committee, you can track down faculty members in public health and microbiology.  They are often busy and might be skeptical of what an outsider can offer, but keep trying because my experience has been that if you keep at it and follow up, they will listen and be grateful for the information.</p>



<p>Here is a <a href="https://docs.google.com/document/d/1QZHWZUxrMGzYKycZNpsfuJBCqRphGBzMQf4-ws76YrI/edit?usp=sharing" rel="noreferrer noopener" target="_blank">sample letter</a> you could use.</p>



<p>Here is a crowdsourced <a href="https://docs.google.com/spreadsheets/d/15zXZsGh6W2hoejgp4_wgTQzDPp-qHBrvYYcgOoaitZ4/edit?usp=sharing" rel="noreferrer noopener" target="_blank">spreadsheet</a> for potential contact people at various universities.  If your university isn’t yet there, we ask that you enter the info that you find for your university in this <a href="https://forms.gle/7v13r2qfYiGoVRUM7" rel="noreferrer noopener" target="_blank">form</a> which is linked to the above spreadsheet (or enter it directly into the spreadsheet).</p>



<p>If you want to know more or would like to craft your own letter, here are some relevant links:</p>



<p><a href="https://docs.google.com/document/d/1qdOkIVle8fQtDjQB-EZA_OdMrCbeeAiljhhHRFc05jE/edit?usp=sharing" rel="noreferrer noopener" target="_blank">Covid-19 early detection surveillance on a 240 person facility using 5 tests a day</a></p>



<p><a href="https://docs.google.com/document/d/16fQ91xkL2Evpi7kEYl5FvSM4EAIWf5Lq9unmD1XJQsY/edit?usp=sharing" rel="noreferrer noopener" target="_blank">Covid-19 early detection surveillance for a campus of 24,000 using 500 tests a day</a></p>



<p>And here is a <a href="https://docs.google.com/document/d/1ZHwHGZMGFzqqj6up-9dRda__pzttSwy7B8fx5wBk5cg/edit?usp=sharing" rel="noreferrer noopener" target="_blank">simple analysis of the mean time between contagion</a> and detection that an early detection scheme could accomplish.</p>



<p>If anyone wants to follow up with me, I’m happy to do so.  You can reach me at:  zeph dot landau at gmail dot com </p>



<p>Thanks.</p>



<p>Zeph Landau<br/>Dept. of Computer Science<br/>University of California, Berkeley</p></div>
    </content>
    <updated>2020-06-04T07:27:56Z</updated>
    <published>2020-06-04T07:27:56Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-06-08T04:26:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/084</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/084" rel="alternate" type="text/html"/>
    <title>TR20-084 |  Rate Amplification and Query-Efficient Distance Amplification for Locally Decodable Codes | 

	Gil Cohen, 

	Tal Yankovitz</title>
    <summary>In a seminal work, Kopparty et al. (J. ACM 2017) constructed asymptotically good $n$-bit locally decodable codes (LDC) with $2^{\widetilde{O}(\sqrt{\log{n}})}$ queries. A key ingredient in their construction is a distance amplification procedure by Alon et al. (FOCS 1995) which amplifies the distance $\delta$ of a code to a constant at a  $\mathrm{poly}(1/\delta)$ multiplicative cost in query complexity. Given the pivotal role of the AEL distance amplification procedure in the state-of-the-art constructions of LDC as well as LCC and LTC, one is prompt to ask whether the $\mathrm{poly}(1/\delta)$ factor in query complexity can be reduced.

Our first contribution is a significantly improved distance amplification procedure for LDC. The cost is reduced from $\mathrm{poly}(1/\delta)$ to, roughly, the query complexity of a length $1/\delta$ asymptotically good LDC. We derive several applications, one of which allows us to convert a $q$-query LDC with extremely poor distance $\delta = n^{-(1-o(1))}$ to a constant distance LDC with $q^{\mathrm{poly}(\log\log{n})}$ queries. As another example, amplifying distance $\delta = 2^{-(\log{n})^\alpha}$, for any constant $\alpha &lt; 1$, will require $q^{O(\log\log\log{n})}$ queries using our procedure.

Motivated by the fruitfulness of distance amplification,  we investigate the natural question of rate amplification. Our second contribution is identifying a rich and natural class of LDC and devise two (incomparable) rate amplification procedures for it. These, however, deteriorate the distance, at which point a distance amplification procedure is invoked. Combined, the procedures convert any $q$-query LDC in our class, having rate $\rho$ and, say, constant distance, to an asymptotically good LDC with $q^{\mathrm{poly}(1/\rho)}$ queries.</summary>
    <updated>2020-06-03T13:03:27Z</updated>
    <published>2020-06-03T13:03:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-10T10:20:33Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6696939180299352624</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6696939180299352624/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/06/how-to-handle-grades-during-pandemic.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6696939180299352624" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6696939180299352624" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/06/how-to-handle-grades-during-pandemic.html" rel="alternate" type="text/html"/>
    <title>How to handle grades during the Pandemic</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In March many Colleges sent students home and the rest of the semester was online. This was quite disruptive for the students. Schools, quite reasonably, wanted to make it less traumatic for students.<br/>
<br/>
So what to do about grades? There are two issues. I state the options I have heard.<br/>
<br/>
<br/>
ISSUE ONE  If P/F How to Got About it?<br/>
<br/>
1) Grade as usual.<br/>
<br/>
2) Make all classes P/F.<br/>
<br/>
PRO: Much less pressure on students.<br/>
<br/>
CON: Might be demoralizing for the good students.<br/>
<br/>
3) Make all classes P/F but allow students to opt for letter grades BUT they must decide before the last day of class. Hence teachers must post cutoffs before the final is graded<br/>
<br/>
CON: Complicated and puts (a) teachers in an awkward position of having to post cutoffs before the final, and (b) puts students in an awkward position of having to predict how well they would do.<br/>
<br/>
CON: A student can blow off a final knowing they will still get a D (passing) in the course.<br/>
<br/>
PRO: Good students can still get their A's<br/>
<br/>
CAVEAT: A transcript might look very strange. Say I was looking at a graduate school applicant and I see<br/>
<br/>
Operating Systems: A<br/>
<br/>
Theory of Computation: P<br/>
<br/>
I would likely assume that the Theory course the student got a C. And that might be unfair.<br/>
<br/>
3) Make all classes P/F but allow students to opt for letter grades AFTER seeing their letter grades. <br/>
<br/>
PRO: Less complicated an awkard<br/>
<br/>
PRO: A students blah blah<br/>
<br/>
CAVEAT above still applies.<br/>
<br/>
ISSUE TWO If P/F what about a D in the major<br/>
<br/>
At UMCP COMP SCI (and I expect other depts)<br/>
<br/>
a D is a passing grade for the University<br/>
<br/>
but<br/>
<br/>
a D is not a passing grade for the Major.<br/>
<br/>
So if a s CS Major gets a D in Discrete Math that does not count for the major--- they have to take it over again.<br/>
<br/>
But if classes are P/F what do do about that.<br/>
<br/>
Options<br/>
<br/>
1) Students have to take classes in their major for a letter grade.<br/>
<br/>
CON: The whole point of the P/F is to relieve pressure on the students in these hard times.<br/>
<br/>
PRO: None.<br/>
<br/>
2) Students taking a course in their major who get a D will still get a P on the transcript but will be told that they have to take the class over again.<br/>
<br/>
3) Do nothing, but tell the students<br/>
<br/>
IF you got a D in a course in your major and you are taking a sequel, STUDY HARD OVER THE SUMMER!<br/>
<br/>
4) Do nothing, but tell the teachers<br/>
<br/>
Students in the Fall may have a weak background. Just teach the bare minimum of what they need for the major.<br/>
<br/>
(Could do both 3 and 4)<br/>
<br/>
SO- what is your school doing and how is it working?</div>
    </content>
    <updated>2020-06-03T04:55:00Z</updated>
    <published>2020-06-03T04:55:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-06-09T07:09:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/06/02/post-doctoral-research-fellowship-at-all-souls-college-university-of-oxford-apply-by-september-11-2020/</id>
    <link href="https://cstheory-jobs.org/2020/06/02/post-doctoral-research-fellowship-at-all-souls-college-university-of-oxford-apply-by-september-11-2020/" rel="alternate" type="text/html"/>
    <title>Post-Doctoral Research Fellowship at All Souls College, University of Oxford (apply by September 11, 2020)</title>
    <summary>All Souls is primarily a research institution with particular strengths in the Humanities and Social and Theoretical Sciences, and with strong connections to public life. It is strongly committed to supporting early career scholars. The Fellowships are intended to offer opportunities for outstanding early career researchers to establish a record of independent research. Website: https://www.asc.ox.ac.uk/post-doctoral-research-fellowships-2020-further-particulars […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>All Souls is primarily a research institution with particular strengths in the Humanities and Social and Theoretical Sciences, and with strong connections to public life. It is strongly committed to supporting early career scholars. The Fellowships are intended to offer opportunities for outstanding early career researchers to establish a record of independent research.</p>
<p>Website: <a href="https://www.asc.ox.ac.uk/post-doctoral-research-fellowships-2020-further-particulars">https://www.asc.ox.ac.uk/post-doctoral-research-fellowships-2020-further-particulars</a><br/>
Email: pdrf.admin@all-souls.ox.ac.uk</p></div>
    </content>
    <updated>2020-06-02T11:12:59Z</updated>
    <published>2020-06-02T11:12:59Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-10T10:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4830</id>
    <link href="https://www.scottaaronson.com/blog/?p=4830" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4830#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4830" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The US might die, but P and PSPACE are forever</title>
    <summary xml:lang="en-US">Today, I interrupt the news of the rapid disintegration of the United States of America, on every possible front at once (medical, economic, social…), to bring you something far more important: a long-planned two-hour podcast, where theoretical physicist and longtime friend-of-the-blog Sean Carroll interviews yours truly about complexity theory! Here’s Sean’s description of this historic […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Today, I interrupt the news of the rapid disintegration of the United States of America, on every possible front at once (medical, economic, social…), to bring you something far more important: a long-planned <a href="https://www.preposterousuniverse.com/podcast/2020/06/01/99-scott-aaronson-on-complexity-computation-and-quantum-gravity/">two-hour podcast</a>, where theoretical physicist and longtime friend-of-the-blog <a href="https://www.preposterousuniverse.com/">Sean Carroll</a> interviews yours truly about complexity theory!  Here’s Sean’s description of this historic event:</p>



<blockquote class="wp-block-quote"><p>There are some problems for which it’s very hard to find the answer, but very easy to check the answer if someone gives it to you. At least, we think there are such problems; whether or not they really exist is the famous <a href="https://en.wikipedia.org/wiki/P_versus_NP_problem">P vs NP problem</a>, and actually proving it will win you <a href="https://en.wikipedia.org/wiki/Millennium_Prize_Problems">a million dollars</a>. This kind of question falls under the rubric of “computational complexity theory,” which formalizes how hard it is to computationally attack a well-posed problem. Scott Aaronson is one of the world’s leading thinkers in computational complexity, especially the wrinkles that enter once we consider quantum computers as well as classical ones. We talk about how we quantify complexity, and how that relates to ideas as disparate as creativity, knowledge vs. proof, and what all this has to do with black holes and quantum gravity.</p></blockquote>



<p>So, OK, I guess I should also comment on the national disintegration thing.  As someone who was once himself the victim of a crazy police overreaction (albeit, trivial compared to what African-Americans regularly deal with), I was moved by the scenes of police chiefs in several American towns taking off their helmets and joining protesters to cheers.  Not only is that a deeply moral thing to do, but it serves a practical purpose of quickly defusing the protests.  Right now, of course, is an <em>even worse time than usual</em> for chaos in the streets, with a lethal virus still spreading that doesn’t care whether people are congregating for good or for ill.  If rational discussion of policy still matters, I support the current push to end the “qualified immunity” doctrine, end the provision of military training and equipment to police, and generally spur the nation’s police to rein in their psychopath minority.</p></div>
    </content>
    <updated>2020-06-01T19:00:33Z</updated>
    <published>2020-06-01T19:00:33Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-06-08T04:26:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7741</id>
    <link href="https://windowsontheory.org/2020/06/01/theory-of-machine-learning-summer-seminar/" rel="alternate" type="text/html"/>
    <title>Theory of Machine Learning summer seminar</title>
    <summary>[Note: While I and many others are fortunate to be able to go on with our work, deadlines, and (as mentioned in this post) seminars, this is not the case for many in the U.S. following yet another demonstration that black lives don’t matter as much as they should in this country. I would like […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Note: While I and many others are fortunate to be able to go on with our work, deadlines, and (as mentioned in this post) seminars, this is not the case for many in the U.S. following yet another demonstration that black lives don’t matter as much as they should in this country. I would like to relay <a href="https://twitter.com/red_abebe/status/1267145593991946245">Rediet Abebe’s call</a> to support local organizations. As Rediet says “These problems have been and will be here for a very long time. We’re not solving racism this month.”. –Boaz]</em></p>



<p>For the last year, I have been co organizing a <a href="https://mltheory.org/#talks">theory of machine learning seminar</a> at Harvard. Following the format of our prior Harvard/MSR/MIT theory reading group, these have been extended blackboard talks with plenty of audience interaction.</p>



<p>Following COVID-19, the last three talks in the semester (by Moritz Hardt,  Zico Kolter, and Anima Anandkumar)  were given virtually. Frankly, I was at first unsure whether these seminars can work in the virtual format but was pleasantly surprised. Talks have been very interactive, with plenty of audience participation in the chat channel. In fact, the virtual format has some <em>advantages </em>over physical talks. Sometimes a question will be asked and answered by a co-author over chat, without the speaker needing to interrupt the talk.</p>



<p>Since the seminars were so successful, we decided to continue holding them over the summer. We have an exciting line up of confirmed speakers, and more will come soon. See <a href="https://mltheory.org/#talks">our webpage</a> for more details, which also contains a google calendar and a mailing list you can sign up for to get the Zoom link.</p>



<p>Confirmed speakers so far include:</p>



<ul><li><a href="http://www.sohldickstein.com/">Jascha Sohl-Dickstein</a> – June 11</li><li><a href="https://www.neyshabur.net/">Behnam Neyshabur</a> – June 18</li><li><a href="https://users.ece.utexas.edu/~dimakis/">Alex Dimakis</a> – July 9</li><li><a href="https://www.cohennadav.com/">Nadav Cohen</a> – August 6</li><li><a href="https://maithraraghu.com/">Maithra Raghu</a> – date tbd</li><li><a href="https://research.google/people/HanieSedghi/">Hanie Sedghi</a> – date tbd</li></ul>



<p>More should be confirmed soon – join our <a href="https://groups.google.com/a/seas.harvard.edu/forum/#!forum/ml-theory-seminar/join">mailing list</a> to get updates.</p></div>
    </content>
    <updated>2020-06-01T16:17:12Z</updated>
    <published>2020-06-01T16:17:12Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-06-10T10:21:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=2566</id>
    <link href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/" rel="alternate" type="text/html"/>
    <link href="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m100.mp4" length="434960" rel="enclosure" type="video/mp4"/>
    <title>Gradient descent for wide two-layer neural networks – I : Global convergence</title>
    <summary>Supervised learning methods come in a variety of flavors. While local averaging techniques such as nearest-neighbors or decision trees are often used with low-dimensional inputs where they can adapt to any potentially non-linear relationship between inputs and outputs, methods based on empirical risk minimization are the most commonly used in high-dimensional settings. Their principle is...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Supervised learning methods come in a variety of flavors. While local averaging techniques such as <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">nearest-neighbors</a> or <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision trees</a> are often used with low-dimensional inputs where they can adapt to any potentially non-linear relationship between inputs and outputs, methods based on empirical risk minimization are the most commonly used in high-dimensional settings. Their principle is simple: optimize the (potentially regularized) risk on training data over prediction functions in a pre-defined set of functions.</p>



<p class="justify-text">When the set of a functions is a convex subset of a vector space with a finite-dimensional representation, with standard assumptions, the corresponding optimization problem is convex. This has the benefits of allowing a thorough theoretical understanding of the computational and statistical properties of learning methods, which often come with strong theoretical guarantees, in terms of running time [<a href="https://arxiv.org/pdf/1405.4980">1</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/16M1080173">2</a>, <a href="http://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">3</a>] or prediction performance on unseen data [<a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">4</a>, <a href="http://static.stevereads.com/papers_to_read/all_of_statistics.pdf">5</a>]. In particular, the linear parameterization can be done either explicitly by building a typically large finite set of features, or implicitly through the use of kernel methods and then a series of dedicated algorithms and theories can be leveraged for efficient non-linear predictions [6, <a href="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">7</a>, <a href="http://papers.nips.cc/paper/6978-falkon-an-optimal-large-scale-kernel-method.pdf">8</a>].</p>



<p class="justify-text">However, linearly-parameterized sets of functions do not include neural networks, which lead to state-of-the-art performance in most learning tasks in computer vision, natural language processing, speech processing, in particular through the use of deep and convolutional neural networks [<a href="https://www.deeplearningbook.org/">9</a>].</p>



<h2>Two-layer neural networks with “relu” activations</h2>



<p class="justify-text">The goal of this blog post is to provide some understanding of why supervised machine learning work for the simplest form of such models: $$ h(x) = \frac{1}{m} \sum_{i=1}^m a_i ( b_i^\top x)_+ = \frac{1}{m} \sum_{i=1}^m a_i \max\{ b_i^\top x,0\},$$ where the input \(x\) is a vector in \(\mathbb{R}^d\), and \(m\) is the number of hidden neurons. The weights \(a_i \in \mathbb{R}\), \(i=1,\dots,n\), are the <em>output weights</em>, while the weights \(b_i \in \mathbb{R}^d\), \(i=1,\dots,n\), are the <em>input weights</em>. The rectified linear unit (“relu”) [<a href="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">10</a>] activation is used, and our results will depend heavily on its positive homogeneity (that is, for \(\lambda &gt; 0\), \((\lambda u)_+ = \lambda u_+\)).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3829" height="292" src="https://francisbach.com/wp-content/uploads/2020/05/nn_single_blog.png" width="407"/>Two-layer neural network in dimension \(d = 6\) with \(m=4\) hidden neurons, and a single output.</figure></div>



<p class="justify-text">Note that this is an idealized and much simplified set-up for deep learning, as there is a single hidden-layer, no convolutions, no pooling, etc. As I will show below, this simple set-up is already complex to understand, and I believe it captures some of the core difficulties associated with non-convexity.</p>



<p class="justify-text">The first question that one may come to after decades of research in learning theory is: <em>why is it so hard to analyze?</em>  </p>



<p class="justify-text">There are at least two major difficulties:</p>



<ul class="justify-text"><li><strong>Non-linearity</strong>: the dependence on the input weights \(b_i\)’s is non-linear because of the activation function, typically leading to non-convex optimization problems.</li><li><strong>Overparameterization</strong>: The number \(m\) of hidden neurons is very large (often so large that the number of parameters \(m(d+1)\) exceeds the number of observations), which is hard in terms of optimization and potentially generalization to unseen data. </li></ul>



<p class="justify-text">In this blog post, we will leverage the overparameterization and take \(m\) tending to infinity (without any dependence on the number of observations), which will allow us to derive theoretical results. We will leverage two key properties of the problem:</p>



<ul class="justify-text"><li><strong>Separability</strong> of the model in \(w_i = (a_i,b_i)\), that is, the prediction function \(h(x)\) is the sum of terms which are independently parameterized, as \(h = \frac{1}{m} \sum_{i=1}^m \Phi(w_i)\), where \(\Phi: \mathbb{R}^p \to \mathcal{F}\), with \(\mathcal{F}\) a space of functions. In our situation, \(p = d+1\) and: $$ \Phi(w)(x) = a (b^\top x)_+. $$ In other words,  there is no parameter sharing among hidden neurons. Unfortunately, this does not generalize to more than a single hidden layer.</li><li><strong>Homogeneity</strong>: the relu activation is positively homogeneous so that as as function of \(w = (a,b) \in \mathbb{R} \times \mathbb{R}^d\), \(\Phi(w)(x) = a (b^\top x)_+\) is positively 2-homogeneous, that is, \(\Phi(\lambda w) = \lambda^2 \Phi(w)\) for \(\lambda &gt; 0\).</li></ul>



<p class="justify-text">In this sequence of two blog posts, following a recent trend in optimization and machine learning theory [<a href="http://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">11</a>, <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">12</a>], optimization and statistics cannot be separated and need to be tackled together. I will focus on gradient flows on empirical or expected risks.</p>



<p class="justify-text">In this blog post, I will cover optimization and how over-parameterization leads to global convergence for 2-homogeneous models, a recent result obtained two years ago with <a href="https://lchizat.github.io/">Lénaïc Chizat</a> [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>]. This requires tools from optimal transport which I will briefly describe (for more details, see, e.g., [<a href="https://arxiv.org/abs/1803.00567">14</a>]).</p>



<p class="justify-text">Next month, I will focus on generalization capabilities and the several implicit biases associated with gradient descent in this context [<a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">15</a>, <a href="https://arxiv.org/pdf/2002.04486">16</a>].</p>



<h2>Infinitely wide limit and probability measures</h2>



<p class="justify-text">Following the standard learning set-up, our goal will be to minimize with respect to the prediction function \(h\) the functional \(R\) defined as $$ R(h) = \mathbb{E}_{p(x,y)} \big[ \ell( y, h(x) ) \big],$$ where \(\ell(y,h(x))\) is the loss incurred by outputting \(h(x)\) when \(y\) is the true label. Even within deep learning, this loss is most often convex in its second argument, such as for least-squares or <a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">logistic</a> losses. Thus, I will assume that \(R\) is convex.</p>



<p>The expectation can be considered in two scenarios:</p>



<ul class="justify-text"><li><strong>Empirical risk</strong>: this corresponds to the situation where we have observations \((x_j,y_j)\), \(j=1,\dots,n\), coming from some joint distribution on \((x,y) \in \mathbb{R}^d \times \mathbb{R}\). Minimizing \(R\) then may not lead to any guarantee on unseen data unless some explicit or implicit regularization is used. In next blog post, I will consider the implicit regularization effect of gradient-based algorithms.</li><li><strong>Expected risk (or generalization performance)</strong>: The expectation is taken with respect to unseen data, and thus its value (or a gradient) cannot be computed. However, any training observation \((x_j,y_j)\) can lead to an unbiased estimate, and if single pass stochastic gradient is used, our guarantees will be on the expected risk.</li></ul>



<p class="justify-text">The main and very classical idea is to consider the minimization of $$ G(W) = G(w_1,\dots,w_m) = R \Big( \frac{1}{m} \sum_{i=1}^m \Phi(w_i) \Big),$$ and see it as the minimization of $$ F(\mu) = R \Big( \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \Big),$$ with respect to a probability measure \(\mu\), with the equivalence for $$ \mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i),$$ where \(\delta(w_i)\) is the Dirac measure at \(w_i\). See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3852" height="93" src="https://francisbach.com/wp-content/uploads/2020/05/diracs_measures-1-1024x156.png" width="615"/>Left: discrete probability measure. Right: measure with density.</figure></div>



<p class="justify-text">When \(m\) is large, we can represent any measure in the <a href="https://en.wikipedia.org/wiki/Convergence_of_measures#Weak_convergence_of_measures">weak sense</a> (that is, expectations of any continuous and bounded functions can be approximated). The benefits of considering the space of all measures instead of discrete measures have been used already in variety of contexts in machine learning, statistics and signal processing [<a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">17</a>, <a href="http://papers.nips.cc/paper/2800-convex-neural-networks.pdf">18</a>, <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">19</a>]. In this blog post, the key benefit is that the set of measures in convex and \(h = \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \) is linear in the measure \(\mu\), so that our optimization problem has become convex.</p>



<p class="justify-text">However, (1) It does not buy much in practice, as the set of probability measures is infinite-dimensional. <a href="https://en.wikipedia.org/wiki/Frank&#x2013;Wolfe_algorithm">Frank-Wolfe algorithms</a> can be used, but the choice of new neurons is a difficult optimization problem, NP-hard for the threshold activation function [<a href="https://epubs.siam.org/doi/pdf/10.1137/070685798">20</a>], with polynomial potentially high complexity for the relu activation [<a href="http://proceedings.mlr.press/v65/goel17a/goel17a.pdf">21</a>], and (2) this is not what is used in practice, which is (stochastic) gradient descent.</p>



<h2>Finite-dimensional gradient flow</h2>



<p class="justify-text">In this post, I will consider the gradient flow $$\dot{W} = \ – m \nabla G(W),$$ (where \(m\) is added as a normalization factor to allow a well-defined limit when \(m\) tends to infinity). This is still not exactly what is used in practice, but, as explained in <a href="https://francisbach.com/gradient-flows/">last month post</a>, this is a good approximation of gradient descent (if using the empirical risk, then leading to guarantees of global convergence on the empirical risk only), or stochastic gradient descent (if doing a single pass on the data, then leading to guarantees of global convergence on unseen data). This is a non-convex dynamics, with stationary points and local minima, even when \(m\) is large (see, e.g., [<a href="http://proceedings.mlr.press/v80/safran18a/safran18a.pdf">27</a>]).</p>



<p class="justify-text">Two main questions arise: (1) what does the gradient flow dynamics converge to when the number of neurons \(m\) tends to infinity, and (2) can we get any global convergence guarantees for the limiting dynamics?</p>



<h2>Mean-field limit and Wasserstein gradient flows</h2>



<p class="justify-text">When \(m\) tends to infinity, since we want to use the measure representation, we need to understand the effect of performing the gradient flow jointly on \(w_1,\dots,w_m\) on the measure $$ \mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i),$$ and see if we can take a limit when \(m\) tends to infinity. This process of taking limits is common in physics and often referred to as the “mean-field” limit, and has been considered in a series of recent works [<a href="https://arxiv.org/pdf/1712.05438">22</a>, <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>, <a href="https://www.pnas.org/content/pnas/115/33/E7665.full.pdf">23</a>, <a href="https://arxiv.org/pdf/1805.00915">24</a>]. To avoid too much technicality, I will assume that the map \(\Phi\) is sufficiently differentiable, which unfortunately exclude the relu activation; for dedicated results, see [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>].</p>



<p class="justify-text"><strong>Gradient flows on metric spaces.</strong> In order to understand the dynamics in the space of probability measures, we need to take a step backward and realize that gradient flows can be defined for many functions \(f\) on any metric space \(\mathcal{X}\). Indeed, it can be seen as the limit of taking infinitesimal steps of length \(\gamma\), where each new iterate \(x_{k+1}\) (corresponding to the value at time \(k\gamma\)) is defined recursively from \(x_k\) as $$x_{k+1} \in \arg\min_{x \in \mathcal{X}}\  f(x) + \frac{1}{2\gamma} d(x,x_k)^2.$$ As shown in [<a href="http://www2.stat.duke.edu/~sayan/ambrosio.pdf">25</a>, <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">26</a>], with some form of interpolation, this defines a curve with prescribed values \(x_k\) at each \(\gamma k\), and when the step-size \(\gamma\) goes to zero, this curves “converges” to the gradient flow.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3960" height="212" src="https://francisbach.com/wp-content/uploads/2020/05/euler-1024x520.png" width="419"/>Gradient flow in bold black, with interpolating curve in red from 14 points \(x_0,\dots,x_{13}\).</figure></div>



<p class="justify-text">For the space \(\mathcal{X} = \mathbb{R}^d\) with the Euclidean distance and a continuously differentiable function \(f\), we obtain that $$x_{k+1} = x_k – \gamma f'(x_k) + o(\gamma),$$ and we get the usual gradient flow associated to \(f\), and the scheme above is nothing less than <a href="https://en.wikipedia.org/wiki/Euler_method">Euler discretization</a> that was described <a href="https://francisbach.com/gradient-flows/">last month</a>.</p>



<p class="justify-text"><strong>Vector space gradient flows on probability measures.</strong> Probability measures are a convex subset of measures with finite <a href="https://en.wikipedia.org/wiki/Total_variation#Total_variation_of_probability_measures">total variation</a>, which is equal to the \(\ell_1\)-norm between densities when the two probability measures have densities with respect to the same base measure. It is a normed vector space for which we could derive our first type of gradient flow, which can be seen as a continuous version of Frank-Wolfe algorithm, where atoms are added one by one, until convergence.  </p>



<p class="justify-text">As mentioned above, the fact that atoms are created sequentially seems attractive computationally. However, (1) deciding which one to add is a computationally hard problem, and (2) the flow on measures cannot be approximated by a finite evolving set of “particles” (here hidden neurons each defined by a vector \(w \in \mathbb{R}^{d+1}\)).</p>



<p class="justify-text"><strong>Wasserstein gradient flows on probability measures.</strong> There is another natural distance here, namely the <a href="https://en.wikipedia.org/wiki/Wasserstein_metric">Wasserstein distance</a> (sometimes called the Kantorovich–Rubinstein distance). In order to remain short, I will only define it between empirical measures $$\mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i) \mbox{ and } \nu = \frac{1}{m} \sum_{i=1}^m \delta(v_i)$$ with the same number of points. The squared 2-Wasserstein distance is obtained by minimizing $$\frac{1}{m} \sum_{i=1}^m \| w_j – v_{\sigma(j)} \|_2^2$$ over all permutations \(\sigma: \{1,\dots,m\} \to \{1,\dots,m\}\). See illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3886" height="171" src="https://francisbach.com/wp-content/uploads/2020/05/Wasserstein-2-1024x307.png" width="573"/>Wasserstein distance between two empirical measures: (left) original observations of two empirical measures with \(m = 11\) points, (right) assigning all black points to red points by minimizing the sum of squared distances between assigned points.</figure></div>



<p class="justify-text">This can be extended to any pair of probability measures, and used within gradient flows, it has a very natural decoupling property: if \(\mu\) is fixed, and \(\nu\) is within a small distance of \(\mu\) in Wasserstein distance, then the optimal permutation above will always be the same, that is, locally, the Wasserstein distance is a sum of squared Euclidean distances. Then, the Wasserstein gradient flow will lead to \(m\) independent local regular Euclidean gradient flows, which interact through the gradient term as: $$ \dot{w}_i = \ –  \nabla \Phi(w_i)  \nabla R\Big(\int_{\mathbb{R}^p} \Phi d\mu \Big),$$ where the Jacobian \(\nabla \Phi(w_i)\) is a linear operator from \(\mathcal{F}\) to \(\mathbb{R}^p\), and \( \nabla R: \mathbb{R}^p \to \mathcal{F}\) the gradient operator of \(R\). Since \(\mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i)\), the dynamics of each particle interacts through the gradient of \(R\). </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3890" height="225" src="https://francisbach.com/wp-content/uploads/2020/05/Wasserstein_flows-1024x508.png" width="456"/>Gradient flow for \(m=7\) interacting particles.</figure></div>



<p class="justify-text">The intuitive reasoning above is behind the formal result for the function $$ F(\mu) = R \Big( \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \Big),$$ that the limit of the Euclidean gradient flow on each particle when \(m\) tends to infinity, is exactly the Wasserstein gradient flow of \(F\). While I have proposed an intuitive explanation, this can be made more formal in particular through the use of partial differential equations on the density of the measure [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>] (see also nice <a href="http://web.math.ucsb.edu/~kcraig/math/curriculum_vitae_files/NIPS_120917.pdf">slides</a> from Katy Craig on Wasserstein gradient flows).</p>



<p class="justify-text"><strong>Stationary points.</strong> Since \(R\) is assumed convex over the convex set of probability measures, all local minima of \(R\) are global, and we should expect the gradient flow to converge to global optimum from any initial measure. This is true for the gradient flow associated with the total variation metric. However this is not true for the Wasserstein gradient flow, for which stationary points which are not global minimizers exist (given that for discrete measures this corresponds to classical backpropagation, this is well known to anybody who has ever trained a neural network). Note that there exists a notion of convexity for Wasserstein gradient flows, namely <a href="https://en.wikipedia.org/wiki/Geodesic_convexity">geodesic convexity</a>, but the function \(F\) is not geodesically convex in general.</p>



<h2>Global convergence</h2>



<p class="justify-text">We can now describe the main result from our recent work with Lénaïc [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>]: under assumptions described below, for the function \(F\) defined above, if the Wasserstein gradient flow converges to a measure, this measure has to be a global minimum of \(F\) (note that we cannot prove it is always convergent).</p>



<p class="justify-text">On top of technical regularity assumptions that I will not describe here, we need two crucial broad assumptions:</p>



<ul class="justify-text"><li><strong>Homogeneity</strong> of the function \(\Phi: \mathbb{R}^{d+1} \to \mathcal{F}\). We need a condition of this form, since if \(R\) is a linear function, then \(F(\mu)\) is of the form \(F(\mu) = \int_{\mathbb{R}^p} \psi(w)d\mu(w)\) with \(\psi(w) = R(\Phi(w))\), and the Wasserstein gradient flow will converge to a weighted some of Diracs at all local minimizers of \(\psi\), which is typically not a global minimizer.</li><li><strong>Initialization with positive mass in all directions</strong>. That is, \(w_i\)’s are uniformly distributed on the sphere or Gaussian, which is the de facto choice in practice. </li></ul>



<p class="justify-text"><strong>Illustration</strong>. We illustrate the result above by considering \(R\) as the square loss and \(y\) being generated from \(x\) through a neural network with \(m_0=5\) neurons. When running the gradient flow above, as soon as \(m \geqslant 5\), the model is sufficiently flexible to attain zero loss, which is thus the global optimum of the cost function. However, the gradient flow may not reach it, as it gets trapped in a local optimum. Our theoretical result suggests that when \(m\) is large, we should converge to the original neurons, which we see below. The surprising (and still unexplained) phenomenon is that \(m\) does not need to be much larger than \(m_0\) to see practical global convergence.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3942" height="259" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m5-1024x683.png" width="389"/>Position of \(m = 5\) neurons, plotted as \(|a_i| b_i \in \mathbb{R}^2\) for a two-dimensional problem. The five dotted lines are the directions of the generating neurons. Although \(m\) is large enough to lead to the global optimum, the flow gets stuck in a local optimum.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3943" height="243" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m10-1024x683.png" width="365"/>Position of \(m = 10\) neurons; same setting as above. The flow converges to the global optimum, although \(m\) is not large.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3944" height="252" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m100-1024x683.png" width="379"/>Position of \(m = 100\) neurons; same setting as above. The flow converges to the global optimum, with \(m\) large. See video below.</figure></div>



<figure class="wp-block-video aligncenter justify-text"><video src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m100.mp4"/>Position of \(m = 100\) neurons; exact same setting as above.</figure>



<h2>Discussion and open problems</h2>



<p class="justify-text">In this blog post, I described theoretical results showing the benefits of overparameterization: when the number of hidden neurons \(m\) tends to infinity, then the corresponding gradient flow converges to the global optimum of the cost function. The proof relies notably on homogeneity properties of the relu activation. </p>



<p class="justify-text">The main weakness of  this result is that is only <em>qualitative</em>: we cannot quantify how big \(m\) need to be to be close to the infinite width limit, or how fast the gradient flow converges to the global optimum. These are still open problems. Additional interesting areas of research are to extend these results to convolutional and/or deep networks.</p>



<p class="justify-text">Now that we know that we can obtain global convergence, I will describe next month the generalization properties when interpolating the training data with an overparameterized relu network [<a href="https://arxiv.org/pdf/2002.04486">16</a>].</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Lénaïc Chizat for producing the nice figures and video of neural networks, proofreading this blog post, and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Sébastien Bubeck. <a href="https://arxiv.org/pdf/1405.4980">Convex Optimization: Algorithms and Complexity</a>. <em>Foundations and Trends in Machine Learning</em>, <em>8</em>(3-4), 231-357, 2015.<br/>[2] Léon Bottou, Frank E. Curtis, Jorge Nocedal. <a href="https://epubs.siam.org/doi/pdf/10.1137/16M1080173">Optimization methods for large-scale machine learning</a>. SIAM Review, 60(2):223-311, 2018.<br/>[3] Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski. <a href="http://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">Optimization with sparsity-inducing penalties</a>. <em>Foundations and Trends in Machine Learning, </em>4(1):1-106, 2012.<br/>[4] Shai Shalev-Shwartz, Shai Ben-David. <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding machine learning: From theory to algorithms</a>. Cambridge University Press, 2014.<br/>[5] Larry Wasserman. <a href="http://static.stevereads.com/papers_to_read/all_of_statistics.pdf">All of statistics: a concise course in statistical inference</a>. Springer Science &amp; Business Media, 2013.<br/>[6] Bernhard Schölkopf, Alexander J. Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2002.<br/>[7] Ali Rahimi and Benjamin Recht. <a href="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">Random features for large-scale kernel machines</a>. <em>Advances in neural information processing systems</em>, 2008.<br/>[8] Alessandro Rudi, Luigi Carratino, Lorenzo Rosasco. <a href="http://papers.nips.cc/paper/6978-falkon-an-optimal-large-scale-kernel-method.pdf">Falkon: An optimal large scale kernel method</a>. <em>Advances in Neural Information Processing Systems</em>, 2017.<br/>[9] Ian Goodfellow, Yoshua Bengio, Aaron Courville. <a href="https://www.deeplearningbook.org/">Deep learning</a>. MIT Press, 2016.<br/>[10] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. <a href="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">Deep sparse rectifier neural networks</a>. <em>International Conference on Artificial Intelligence and Statistics</em>, 2011.<br/>[11] Francis Bach and Eric Moulines. <a href="http://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)</a>. <em>Advances in Neural Information Processing Systems</em>, 2013.<br/>[12] MIkhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal. <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">Reconciling modern machine-learning practice and the classical bias–variance trade-off</a>. <em>Proceedings of the National Academy of Sciences</em>, 116(32), 15849-15854, 2019.<br/>[13] Lénaïc Chizat, Francis Bach. <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport</a>. <em>Advances in Neural Information Processing Systems</em>, 2018.<br/>[14] Gabriel Peyré, Marco Cututi. <em><a href="https://arxiv.org/abs/1803.00567">Computational Optimal Transport</a></em>. Foundations and Trends in Machine Learning, 51(1):1–44, 2019.<br/>[15] Lénaïc Chizat, Edouard Oyallon, Francis Bach. <a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">On Lazy Training in Differentiable Programming</a>. <em>Advances in Neural Information Processing Systems</em>, 2019.<br/>[16] Lénaïc Chizat, Francis Bach. <a href="https://arxiv.org/pdf/2002.04486">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss</a>. Technical report, arXiv:2002.04486, 2020.<br/>[17] Andrew R. Barron. <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">Universal approximation bounds for superpositions of a sigmoidal function</a>. <em>IEEE Transactions on Information theory</em>, <em>39</em>(3), 930-945, 1993.<br/>[18] Yoshua Bengio, Nicolas Le Roux, Pascal Vincent, Olivier Delalleau, Patrice Marcotte. <a href="http://papers.nips.cc/paper/2800-convex-neural-networks.pdf">Convex neural networks</a>. <em>Advances in neural information processing systems</em>, 2006.<br/>[19] Francis Bach. <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the Curse of Dimensionality with Convex Neural Networks</a>.<strong> </strong><em>Journal of Machine Learning Research</em>, 18(19):1-53, 2017.<br/>[20] Venkatesan Guruswami, Prasad Raghavendra. <a href="https://epubs.siam.org/doi/pdf/10.1137/070685798">Hardness of learning halfspaces with noise</a>. <em>SIAM Journal on Computing</em>, 39(2):742-765, 2009.<br/>[21] Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler. <a href="http://proceedings.mlr.press/v65/goel17a/goel17a.pdf">Reliably Learning the ReLU in Polynomial Time</a>. <em>Conference on Learning Theory</em>, 2017.<br/>[22] Atsushi Nitanda, Taiji Suzuki. <a href="https://arxiv.org/pdf/1712.05438">Stochastic particle gradient descent for infinite ensembles</a>. Technical report, arXiv:1712.05438, 2017.<br/>[23] Song Mei, Andrea Montanari, Phan-Minh Nguyen. <a href="https://www.pnas.org/content/pnas/115/33/E7665.full.pdf">A mean field view of the landscape of two-layer neural networks</a>. <em>Proceedings of the National Academy of Sciences</em> 115(33):E7665-E7671, 2018.<br/>[24] Grant M. Rotskoff, Eric Vanden-Eijnden. <a href="https://arxiv.org/pdf/1805.00915">Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error</a>. Technical report, arXiv:1805.00915, 2018.<br/>[25] Luigi Ambrosio, Nicola Gigli, Giuseppe Savaré. <a href="http://www2.stat.duke.edu/~sayan/ambrosio.pdf">Gradient flows: in metric spaces and in the space of probability measures</a>. Springer Science &amp; Business Media, 2008<br/>[26] Filippo Santambrogio. <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">{Euclidean, metric, and Wasserstein} gradient flows: an overview</a>. <em>Bulletin of Mathematical Sciences</em>, <em>7</em>(1), 87-154, 2017.<br/>[27] Itay Safran, Ohad Shamir. <a href="http://proceedings.mlr.press/v80/safran18a/safran18a.pdf">Spurious Local Minima are Common in Two-Layer ReLU Neural Networks</a>. <em>International Conference on Machine Learning</em>, 2018.</p></div>
    </content>
    <updated>2020-06-01T07:32:48Z</updated>
    <published>2020-06-01T07:32:48Z</published>
    <category term="Machine learning"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-06-10T10:22:00Z</updated>
    </source>
  </entry>
</feed>
