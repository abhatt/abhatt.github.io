<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-06-25T20:21:42Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-5732832478359389342</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/5732832478359389342/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=5732832478359389342" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/5732832478359389342" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/5732832478359389342" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2020/06/writing-code-for-paper-note-to-students.html" rel="alternate" type="text/html"/>
    <title>Writing Code for a Paper :  A Note to Students</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post both relates to some of the stuff I'll be presenting at Friday's STOC workshop on Algorithms with Predictions, but is also for future students in my classes, who sometimes wonder why I have them write code on theory material.  (Somehow this might be a theme for a lecture in my grad class next semester, so maybe these are a first attempt at notes for the lecture.  Comments  and  suggestions welcome.)<br/><br/>Some of the work I'm doing is looking at how queueing systems perform with various sorts of predictions on the times the jobs take.  This particular work I'm doing on my own.  (While most of my research has been and is with collaborators, and it's one of the things I enjoy about computer science -- we're a very collaborative field!, which seems to surprise many people -- I still sometimes like to do research projects on my own.  I've looked at queueing systems since my PhD thesis, and it's a bit outside the research interest of most of my collaborator pool, and it's "fun" sometimes to do my own thing.  The reason why "fun" is in quotes is described below.)  <br/><br/>Often in my work in queueing I'm looking at mean-field limits (meant to model infinite systems of queues, which provides a good approximation for large finite systems under reasonable assumptions), where I can derive families of differential equations describing the system behavior.  I can also simulate the large finite system directly, and make sure the results match.  I generally do this for all of these types of papers.<br/><br/>Now the numbers I get from simulating the system directly and from simulating the differential equations should match (say within 1% or so).  If they don't, something is wrong.  In an effort to avoid wrongness, I won't consider the paper ready for outside consumption until I get a match.  Unfortunately, there are three ways things can go wrong.<br/><br/>1.  My simulation code for the queueing system might have bugs.<br/>2.  My code to evaluate the differential equations might have bugs.<br/>3.  My equations themselves might have bugs.<br/><br/>And I find there are two main categories of bugs.  Sometimes the bugs are simple/standard coding mistakes -- I'm off by 1 on an index, or I cut and paste and forget to change an i++ to a j++ in one my double loops, or I type x instead of a y.  Usually it's pretty easy to find these things, although I've had times where a hidden typo took hours to find.  But sometimes the bug is a thinking mistake -- I've forgotten a subcase and so my equations aren't complete (and so my code evaluating the equations won't give the right answer), or I've not handled a subcase correctly in my simulation.  That type usually takes longer. <br/><br/>Usually, the first time through, most all of these types of bugs happen -- my math is off, I've typed some stuff wrong, it can all happen.  And then, like coders everywhere, I go through and fix it.  And it's painful.  Sometimes everything goes right, a quick check or two and everything works.  For more complicated stuff, it's more time figuring out what went wrong than setting up the code to begin with.  And being the personality type to not let things sit, that can mean late nights figuring out what went wrong.<br/><br/>For my talk this week, there was one last problem I wanted to include, which meant finally taking the model and writing the equations and code.  I didn't even need it for the talk, but it's also the last bit before I put a paper draft on arxiv, so taking advantage of a deadline, I figured now was the time.  Which means the last 2 days, I've spent many hours (and a late night) trying to remove the disagreements.<br/><br/>On the plus side, when everything finally works, it's a wonderful feeling.  And it always makes me feel better when I have worked to verify my math this way;  this time, what kept me up well past midnight and took several hours to track down was actually a boundary case I had left out of the equations.  (I had looked at the equations over and over again without noticing I had left out the subcase;  I had to step through numbers from the differential equations one time step at a time to track down what was missing, and then the numbers told me what I had done wrong.)<br/><br/>On the down side, it's work, and debugging is never particularly fun.<br/><br/>For students out there, maybe I'm just explaining that I understand the pain that I am putting you through.  You may wonder why I have you do simulations that take a few hours if you do them well, but days if you don't think through the best approach.  But using programming and theory together can be powerful;  it's helped me countless times in my research.<br/><br/>(Related: on theory and experiments that <a href="https://cacm.acm.org/magazines/2015/9/191184-theory-without-experiments/fulltext">I've written on before</a>, along with <a href="https://cacm.acm.org/magazines/2015/9/191183-experiments-as-research-validation/fulltext">a viewpoint by Jeffrey Ullman</a>.)<br/><br/><br/></div>
    </content>
    <updated>2020-06-25T17:35:00Z</updated>
    <published>2020-06-25T17:35:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2020-06-25T17:35:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=775</id>
    <link href="https://emanueleviola.wordpress.com/2020/06/25/we-dont-need-no-education/" rel="alternate" type="text/html"/>
    <title>We don’t need no education</title>
    <summary>Around us, educational systems whose primary emphasis is on social rather than intellectual skills are simply disintegrating. Unequipped for content delivery, teachers spray parents with a mixture of links and credentials whose collection is a complete waste of everybody’s time. Suggestion: What about assigning homework and let teachers provide feedback? To all the school-age kids […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Around us, educational systems whose primary emphasis is on social rather than intellectual skills are simply disintegrating.  Unequipped for content delivery, teachers spray parents with a mixture of links and credentials whose collection is a complete waste of everybody’s time.  Suggestion: What about assigning <em>homework </em>and let teachers provide <em>feedback</em>?</p>



<p>To all the school-age kids stuck at home doing some fun coding, <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwio8MDa4JvqAhWfVhUIHSAIABgQyCkwAHoECBUQBw&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYR5ApYxkU-U&amp;usg=AOvVaw0s6Ai-o5-CNtyqFC7uHT_4">this immortal song is for you</a>.</p></div>
    </content>
    <updated>2020-06-25T10:29:13Z</updated>
    <published>2020-06-25T10:29:13Z</published>
    <category term="Uncategorized"/>
    <category term="Utopia"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2020-06-25T20:20:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/06/25/postdoc-in-experimental-algorithms-at-national-university-of-singapore-apply-by-august-15-2020/</id>
    <link href="https://cstheory-jobs.org/2020/06/25/postdoc-in-experimental-algorithms-at-national-university-of-singapore-apply-by-august-15-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc in Experimental Algorithms at National University of Singapore (apply by August 15, 2020)</title>
    <summary>A one-year postdoc position (with a possibility of extension for another year) in experimental (/practical) algorithms is available at NUS. The position will be jointly hosted by Diptarka Chakraborty and Kuldeep S. Meel. Applications are invited from candidates who have solid background in algorithm design/formal methods, mathematics. A prior experience in programming would be a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A one-year postdoc position (with a possibility of extension for another year) in experimental (/practical) algorithms is available at NUS. The position will be jointly hosted by Diptarka Chakraborty and Kuldeep S. Meel. Applications are invited from candidates who have solid background in algorithm design/formal methods, mathematics. A prior experience in programming would be a plus.</p>
<p>Website: <a href="https://sites.google.com/view/diptarka/postdoc-advertisement">https://sites.google.com/view/diptarka/postdoc-advertisement</a><br/>
Email: diptarka@comp.nus.edu.sg</p></div>
    </content>
    <updated>2020-06-25T06:10:01Z</updated>
    <published>2020-06-25T06:10:01Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-25T20:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/06/25/postdoc-in-tcs-at-national-university-of-singapore-apply-by-august-15-2020-2/</id>
    <link href="https://cstheory-jobs.org/2020/06/25/postdoc-in-tcs-at-national-university-of-singapore-apply-by-august-15-2020-2/" rel="alternate" type="text/html"/>
    <title>Postdoc in TCS at National University of Singapore (apply by August 15, 2020)</title>
    <summary>A one-year postdoc position (with a possibility of extension for another year, subject to availability of fund) in theoretical computer science is available in the research group of Diptarka Chakraborty at NUS. Applications are invited from candidates who have solid background in algorithm design, computational complexity and mathematics. Website: https://sites.google.com/view/diptarka/postdoc-advertisement Email: diptarka@comp.nus.edu.sg</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A one-year postdoc position (with a possibility of extension for another year, subject to availability of fund) in theoretical computer science is available in the research group of Diptarka Chakraborty at NUS. Applications are invited from candidates who have solid background in algorithm design, computational complexity and mathematics.</p>
<p>Website: <a href="https://sites.google.com/view/diptarka/postdoc-advertisement">https://sites.google.com/view/diptarka/postdoc-advertisement</a><br/>
Email: diptarka@comp.nus.edu.sg</p></div>
    </content>
    <updated>2020-06-25T06:05:34Z</updated>
    <published>2020-06-25T06:05:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-25T20:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/06/25/postdoc-in-tcs-at-national-university-of-singapore-apply-by-august-15-2020/</id>
    <link href="https://cstheory-jobs.org/2020/06/25/postdoc-in-tcs-at-national-university-of-singapore-apply-by-august-15-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc in TCS at National University of Singapore (apply by August 15, 2020)</title>
    <summary>A one-year postdoc position (with a possibility of extension for another year, subject to availability of fund) in theoretical computer science is available in the research group of Diptarka Chakraborty at NUS. Applications are invited from candidates who have solid background in algorithm design, computational complexity and mathematics. Website: https://sites.google.com/view/diptarka/postdoc-advertisement Email: diptarka@comp.nus.edu.sg</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A one-year postdoc position (with a possibility of extension for another year, subject to availability of fund) in theoretical computer science is available in the research group of Diptarka Chakraborty at NUS. Applications are invited from candidates who have solid background in algorithm design, computational complexity and mathematics.</p>
<p>Website: <a href="https://sites.google.com/view/diptarka/postdoc-advertisement">https://sites.google.com/view/diptarka/postdoc-advertisement</a><br/>
Email: diptarka@comp.nus.edu.sg</p></div>
    </content>
    <updated>2020-06-25T06:05:03Z</updated>
    <published>2020-06-25T06:05:03Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-25T20:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13911</id>
    <link href="http://arxiv.org/abs/2006.13911" rel="alternate" type="text/html"/>
    <title>Acyclic coloring of special digraphs</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gurski:Frank.html">Frank Gurski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Komander:Dominique.html">Dominique Komander</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rehs:Carolin.html">Carolin Rehs</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13911">PDF</a><br/><b>Abstract: </b>An acyclic r-coloring of a directed graph G=(V,E) is a partition of the
vertex set V into r acyclic sets. The dichromatic number of a directed graph G
is the smallest r such that G allows an acyclic r-coloring. For symmetric
digraphs the dichromatic number equals the well-known chromatic number of the
underlying undirected graph. This allows us to carry over the W[1]-hardness and
lower bounds for running times of the chromatic number problem parameterized by
clique-width to the dichromatic number problem parameterized by directed
clique-width. We introduce the first polynomial-time algorithm for the acyclic
coloring problem on digraphs of constant directed clique-width. From a
parameterized point of view our algorithm shows that the Dichromatic Number
problem is in XP when parameterized by directed clique-width and extends the
only known structural parameterization by directed modular width for this
problem. For directed co-graphs, which is a class of digraphs of directed
clique-width 2, and several generalizations we even show linear time solutions
for computing the dichromatic number. Furthermore, we conclude that directed
co-graphs and the considered generalizations lead to subclasses of perfect
digraphs. For directed cactus forests, which is a set of digraphs of directed
tree-width 1, we conclude an upper bound of 2 for the dichromatic number and we
show that an optimal acyclic coloring can be computed in linear time.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13754</id>
    <link href="http://arxiv.org/abs/2006.13754" rel="alternate" type="text/html"/>
    <title>A Parameterized Family of Meta-Submodular Functions</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghadiri:Mehrdad.html">Mehrdad Ghadiri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Santiago:Richard.html">Richard Santiago</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shepherd:Bruce.html">Bruce Shepherd</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13754">PDF</a><br/><b>Abstract: </b>Submodular function maximization has found a wealth of new applications in
machine learning models during the past years. The related supermodular
maximization models (submodular minimization) also offer an abundance of
applications, but they appeared to be highly intractable even under simple
cardinality constraints. Hence, while there are well-developed tools for
maximizing a submodular function subject to a matroid constraint, there is much
less work on the corresponding supermodular maximization problems.
</p>
<p>We give a broad parameterized family of monotone functions which includes
submodular functions and a class of supermodular functions containing diversity
functions. Functions in this parameterized family are called
\emph{$\gamma$-meta-submodular}. We develop local search algorithms with
approximation factors that depend only on the parameter $\gamma$. We show that
the $\gamma$-meta-submodular families include well-known classes of functions
such as meta-submodular functions ($\gamma=0$), metric diversity functions and
proportionally submodular functions (both with $\gamma=1$), diversity functions
based on negative-type distances or Jensen-Shannon divergence (both with
$\gamma=2$), and $\sigma$-semi metric diversity functions ($\gamma = \sigma$).
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13738</id>
    <link href="http://arxiv.org/abs/2006.13738" rel="alternate" type="text/html"/>
    <title>The Power of Connection: Leveraging Network Analysis to Advance Receivable Financing</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bordino:Ilaria.html">Ilaria Bordino</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gullo:Francesco.html">Francesco Gullo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Legnaro:Giacomo.html">Giacomo Legnaro</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13738">PDF</a><br/><b>Abstract: </b>Receivable financing is the process whereby cash is advanced to firms against
receivables their customers have yet to pay: a receivable can be sold to a
funder, which immediately gives the firm cash in return for a small percentage
of the receivable amount as a fee. Receivable financing has been traditionally
handled in a centralized way, where every request is processed by the funder
individually and independently of one another. In this work we propose a novel,
network-based approach to receivable financing, which enables customers of the
same funder to autonomously pay each other as much as possible, and gives
benefits to both the funder (reduced cash anticipation and exposure risk) and
its customers (smaller fees and lightweight service establishment). Our main
contributions consist in providing a principled formulation of the
network-based receivable-settlement strategy, and showing how to achieve all
algorithmic challenges posed by the design of this strategy. We formulate
network-based receivable financing as a novel combinatorial-optimization
problem on a multigraph of receivables. We show that the problem is NP-hard,
and devise an exact branch-and-bound algorithm, as well as algorithms to
efficiently find effective approximate solutions. Our more efficient algorithms
are based on cycle enumeration and selection, and exploit a theoretical
characterization in terms of a knapsack problem, as well as a refining strategy
that properly adds paths between cycles. We also investigate the real-world
issue of avoiding temporary violations of the problem constraints, and design
methods for handling it. An extensive experimental evaluation is performed on
real receivable data. Results attest the good performance of our methods.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13712</id>
    <link href="http://arxiv.org/abs/2006.13712" rel="alternate" type="text/html"/>
    <title>Disjointness through the Lens of Vapnik-Chervonenkis Dimension: Sparsity and Beyond</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhattacharya:Anup.html">Anup Bhattacharya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakraborty:Sourav.html">Sourav Chakraborty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Arijit.html">Arijit Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mishra:Gopinath.html">Gopinath Mishra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paraashar:Manaswi.html">Manaswi Paraashar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13712">PDF</a><br/><b>Abstract: </b>The disjointness problem - where Alice and Bob are given two subsets of $\{1,
\dots, n\}$ and they have to check if their sets intersect - is a central
problem in the world of communication complexity. While both deterministic and
randomized communication complexities for this problem are known to be
$\Theta(n)$, it is also known that if the sets are assumed to be drawn from
some restricted set systems then the communication complexity can be much
lower. In this work, we explore how communication complexity measures change
with respect to the complexity of the underlying set system. The complexity
measure for the set system that we use in this work is the Vapnik-Chervonenkis
(VC) dimension. More precisely, on any set system with VC dimension bounded by
$d$, we analyze how large can the deterministic and randomized communication
complexities be, as a function of $d$ and $n$.
</p>
<p>In this paper, we construct two natural set systems of VC dimension $d$,
motivated from geometry. Using these set systems we show that the deterministic
and randomized communication complexity can be $\widetilde{\Theta}\left(d\log
\left( n/d \right)\right)$ for set systems of VC dimension $d$ and this matches
the deterministic upper bound for all set systems of VC dimension $d$. We also
study the deterministic and randomized communication complexities of the set
intersection problem when sets belong to a set system of bounded VC dimension.
We show that there exists set systems of VC dimension $d$ such that both
deterministic and randomized (one-way and multi-round) complexity for the set
intersection problem can be as high as $\Theta\left( d\log \left( n/d \right)
\right)$, and this is tight among all set systems of VC dimension $d$.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13684</id>
    <link href="http://arxiv.org/abs/2006.13684" rel="alternate" type="text/html"/>
    <title>Kernelization of Whitney Switches</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovach:Petr_A=.html">Petr A. Golovach</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13684">PDF</a><br/><b>Abstract: </b>A fundamental theorem of Whitney from 1933 asserts that 2-connected graphs G
and H are 2-isomorphic, or equivalently, their cycle matroids are isomorphic,
if and only if G can be transformed into H by a series of operations called
Whitney switches. In this paper we consider the quantitative question arising
from Whitney's theorem: Given two 2-isomorphic graphs, can we transform one
into another by applying at most k Whitney switches? This problem is already
NP-complete for cycles, and we investigate its parameterized complexity. We
show that the problem admits a kernel of size O(k), and thus, is
fixed-parameter tractable when parameterized by k.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13679</id>
    <link href="http://arxiv.org/abs/2006.13679" rel="alternate" type="text/html"/>
    <title>Approximation of the Diagonal of a Laplacian's Pseudoinverse for Complex Network Analysis</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Angriman:Eugenio.html">Eugenio Angriman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Predari:Maria.html">Maria Predari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grinten:Alexander_van_der.html">Alexander van der Grinten</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meyerhenke:Henning.html">Henning Meyerhenke</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13679">PDF</a><br/><b>Abstract: </b>The ubiquity of massive graph data sets in numerous applications requires
fast algorithms for extracting knowledge from these data. We are motivated here
by three electrical measures for the analysis of large small-world graphs $G =
(V, E)$ -- i.e., graphs with diameter in $O(\log |V|)$, which are abundant in
complex network analysis. From a computational point of view, the three
measures have in common that their crucial component is the diagonal of the
graph Laplacian's pseudoinverse, $L^\dagger$. Computing diag$(L^\dagger)$
exactly by pseudoinversion, however, is as expensive as dense matrix
multiplication -- and the standard tools in practice even require cubic time.
Moreover, the pseudoinverse requires quadratic space -- hardly feasible for
large graphs. Resorting to approximation by, e.g., using the
Johnson-Lindenstrauss transform, requires the solution of $O(\log |V| /
\epsilon^2)$ Laplacian linear systems to guarantee a relative error, which is
still very expensive for large inputs.
</p>
<p>In this paper, we present a novel approximation algorithm that requires the
solution of only one Laplacian linear system. The remaining parts are purely
combinatorial -- mainly sampling uniform spanning trees, which we relate to
diag$(L^\dagger)$ via effective resistances. For small-world networks, our
algorithm obtains a $\pm \epsilon$-approximation with high probability, in a
time that is nearly-linear in $|E|$ and quadratic in $1 / \epsilon$. Another
positive aspect of our algorithm is its parallel nature due to independent
sampling. We thus provide two parallel implementations of our algorithm: one
using OpenMP, one MPI + OpenMP. In our experiments against the state of the
art, our algorithm (i) yields more accurate results, (ii) is much faster and
more memory-efficient, and (iii) obtains good parallel speedups, in particular
in the distributed setting.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13673</id>
    <link href="http://arxiv.org/abs/2006.13673" rel="alternate" type="text/html"/>
    <title>Improved Circular $k$-Mismatch Sketches</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golan:Shay.html">Shay Golan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kociumaka:Tomasz.html">Tomasz Kociumaka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kopelowitz:Tsvi.html">Tsvi Kopelowitz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Porat:Ely.html">Ely Porat</a>, Przemysław Uznański <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13673">PDF</a><br/><b>Abstract: </b>The shift distance $\mathsf{sh}(S_1,S_2)$ between two strings $S_1$ and $S_2$
of the same length is defined as the minimum Hamming distance between $S_1$ and
any rotation (cyclic shift) of $S_2$. We study the problem of sketching the
shift distance, which is the following communication complexity problem:
Strings $S_1$ and $S_2$ of length $n$ are given to two identical players
(encoders), who independently compute sketches (summaries) $\mathtt{sk}(S_1)$
and $\mathtt{sk}(S_2)$, respectively, so that upon receiving the two sketches,
a third player (decoder) is able to compute (or approximate)
$\mathsf{sh}(S_1,S_2)$ with high probability.
</p>
<p>This paper primarily focuses on the more general $k$-mismatch version of the
problem, where the decoder is allowed to declare a failure if
$\mathsf{sh}(S_1,S_2)&gt;k$, where $k$ is a parameter known to all parties. Andoni
et al. (STOC'13) introduced exact circular $k$-mismatch sketches of size
$\widetilde{O}(k+D(n))$, where $D(n)$ is the number of divisors of $n$. Andoni
et al. also showed that their sketch size is optimal in the class of linear
homomorphic sketches.
</p>
<p>We circumvent this lower bound by designing a (non-linear) exact circular
$k$-mismatch sketch of size $\widetilde{O}(k)$; this size matches
communication-complexity lower bounds. We also design $(1\pm
\varepsilon)$-approximate circular $k$-mismatch sketch of size
$\widetilde{O}(\min(\varepsilon^{-2}\sqrt{k}, \varepsilon^{-1.5}\sqrt{n}))$,
which improves upon an $\widetilde{O}(\varepsilon^{-2}\sqrt{n})$-size sketch of
Crouch and McGregor (APPROX'11).
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13642</id>
    <link href="http://arxiv.org/abs/2006.13642" rel="alternate" type="text/html"/>
    <title>Online Dense Subgraph Discovery via Blurred-Graph Feedback</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuroki:Yuko.html">Yuko Kuroki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miyauchi:Atsushi.html">Atsushi Miyauchi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Honda:Junya.html">Junya Honda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sugiyama:Masashi.html">Masashi Sugiyama</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13642">PDF</a><br/><b>Abstract: </b>Dense subgraph discovery aims to find a dense component in edge-weighted
graphs. This is a fundamental graph-mining task with a variety of applications
and thus has received much attention recently. Although most existing methods
assume that each individual edge weight is easily obtained, such an assumption
is not necessarily valid in practice. In this paper, we introduce a novel
learning problem for dense subgraph discovery in which a learner queries edge
subsets rather than only single edges and observes a noisy sum of edge weights
in a queried subset. For this problem, we first propose a polynomial-time
algorithm that obtains a nearly-optimal solution with high probability.
Moreover, to deal with large-sized graphs, we design a more scalable algorithm
with a theoretical guarantee. Computational experiments using real-world graphs
demonstrate the effectiveness of our algorithms.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13483</id>
    <link href="http://arxiv.org/abs/2006.13483" rel="alternate" type="text/html"/>
    <title>Provably and Efficiently Approximating Near-cliques using the Tur\'an Shadow: PEANUTS</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jain:Shweta.html">Shweta Jain</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seshadhri:C=.html">C. Seshadhri</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13483">PDF</a><br/><b>Abstract: </b>Clique and near-clique counts are important graph properties with
applications in graph generation, graph modeling, graph analytics, community
detection among others. They are the archetypal examples of dense subgraphs.
While there are several different definitions of near-cliques, most of them
share the attribute that they are cliques that are missing a small number of
edges. Clique counting is itself considered a challenging problem. Counting
near-cliques is significantly harder more so since the search space for
near-cliques is orders of magnitude larger than that of cliques.
</p>
<p>We give a formulation of a near-clique as a clique that is missing a constant
number of edges. We exploit the fact that a near-clique contains a smaller
clique, and use techniques for clique sampling to count near-cliques. This
method allows us to count near-cliques with 1 or 2 missing edges, in graphs
with tens of millions of edges. To the best of our knowledge, there was no
known efficient method for this problem, and we obtain a 10x - 100x speedup
over existing algorithms for counting near-cliques.
</p>
<p>Our main technique is a space-efficient adaptation of the Tur\'an Shadow
sampling approach, recently introduced by Jain and Seshadhri (WWW 2017). This
approach constructs a large recursion tree (called the Tur\'an Shadow) that
represents cliques in a graph. We design a novel algorithm that builds an
estimator for near-cliques, using an online, compact construction of the
Tur\'an Shadow.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13449</id>
    <link href="http://arxiv.org/abs/2006.13449" rel="alternate" type="text/html"/>
    <title>Hardness of Approximation of (Multi-)LCS over Small Alphabet</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhangale:Amey.html">Amey Bhangale</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakraborty:Diptarka.html">Diptarka Chakraborty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Rajendra.html">Rajendra Kumar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13449">PDF</a><br/><b>Abstract: </b>The problem of finding longest common subsequence (LCS) is one of the
fundamental problems in computer science, which finds application in fields
such as computational biology, text processing, information retrieval, data
compression etc. It is well known that (decision version of) the problem of
finding the length of a LCS of an arbitrary number of input sequences (which we
refer to as Multi-LCS problem) is NP-complete. Jiang and Li [SICOMP'95] showed
that if Max-Clique is hard to approximate within a factor of $s$ then Multi-LCS
is also hard to approximate within a factor of $\Theta(s)$. By the NP-hardness
of the problem of approximating Max-Clique by Zuckerman [ToC'07], for any
constant $\delta&gt;0$, the length of a LCS of arbitrary number of input sequences
of length $n$ each, cannot be approximated within an $n^{1-\delta}$-factor in
polynomial time unless {\tt{P}}$=${\NP}. However, the reduction of Jiang and Li
assumes the alphabet size to be $\Omega(n)$. So far no hardness result is known
for the problem of approximating Multi-LCS over sub-linear sized alphabet. On
the other hand, it is easy to get $1/|\Sigma|$-factor approximation for strings
of alphabet $\Sigma$.
</p>
<p>In this paper, we make a significant progress towards proving hardness of
approximation over small alphabet by showing a polynomial-time reduction from
the well-studied \emph{densest $k$-subgraph} problem with {\em perfect
completeness} to approximating Multi-LCS over alphabet of size $poly(n/k)$. As
a consequence, from the known hardness result of densest $k$-subgraph problem
(e.g. [Manurangsi, STOC'17]) we get that no polynomial-time algorithm can give
an $n^{-o(1)}$-factor approximation of Multi-LCS over an alphabet of size
$n^{o(1)}$, unless the Exponential Time Hypothesis is false.
</p></div>
    </summary>
    <updated>2020-06-25T01:20:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13430</id>
    <link href="http://arxiv.org/abs/2006.13430" rel="alternate" type="text/html"/>
    <title>Approximation algorithms for the MAXSPACE advertisement problem</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>M. R. C. da Silva, L. L. C. Pedrosa, R. C. S. Schouery <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13430">PDF</a><br/><b>Abstract: </b>In the MAXSPACE problem, given a set of ads A, one wants to schedule a subset
A' of A into K slots B_1, ..., B_K of size L. Each ad A_i in A has a size s_i
and a frequency w_i. A schedule is feasible if the total size of ads in any
slot is at most L, and each ad A_i in A' appears in exactly w_i slots. The goal
is to find a feasible schedule that maximizes the sum of the space occupied by
all slots. We introduce a generalization called MAXSPACE-R in which each ad A_i
also has a release date r_i &gt;= 1, and may only appear in a slot B_j with j &gt;=
r_i. We also introduce a generalization of MAXSPACE-R called MAXSPACE-RD in
which each ad A_i also has a deadline d_i &lt;= K, and may only appear in a slot
B_j with r_i &lt;= j &lt;= d_i. These parameters model situations where a subset of
ads corresponds to a commercial campaign with an announcement date that may
expire after some defined period. We present a 1/9-approximation algorithm for
MAXSPACE-R and a polynomial-time approximation scheme for MAXSPACE-RD when K is
bounded by a constant. This is the best factor one can expect, since MAXSPACE
is NP-hard, even if K = 2.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13412</id>
    <link href="http://arxiv.org/abs/2006.13412" rel="alternate" type="text/html"/>
    <title>Lower Bounds on Rate of Convergence of Matrix Products in All Pairs Shortest Path of Social Network</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Dezhou Shen <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13412">PDF</a><br/><b>Abstract: </b>With the rapid development of social network applications, social network has
become an important medium for people to interact. For the minimum distance
computation of all pairs in networks, Alon N[4] proposed an algorithm with
matrix multiplication, combining with distance product association law and
block matrix multiplication, all pairs shortest path length algorithm on
networks has time bound O((2n^3)/B logn). In practical applications,
considering the scale-free characteristics of social networks and the precision
limitations of floating-point operations on computer hardware, I found that the
shortest path algorithm has an improved time bound O((14n^3)/B). Based on the
above theory, I propose an all pairs shortest path algorithm that combines
sparseness judgment and convergence judgment, leveraging the distance product
algorithm with matrix multiplication, distance product association law, block
matrix multiplication, scale-free characteristics of social networks, and
limitation of floating-point operations on hardware. Testing on a social
network dataset with 8508 actors, compared to Alon N algorithm, proposed
algorithm has a performance improvement of 39% to 36.2 times on CPU and GPU.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13312</id>
    <link href="http://arxiv.org/abs/2006.13312" rel="alternate" type="text/html"/>
    <title>Robust Gaussian Covariance Estimation in Nearly-Matrix Multiplication Time</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jerry.html">Jerry Li</a>, Guanghao Ye <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13312">PDF</a><br/><b>Abstract: </b>Robust covariance estimation is the following, well-studied problem in high
dimensional statistics: given $N$ samples from a $d$-dimensional Gaussian
$\mathcal{N}(\boldsymbol{0}, \Sigma)$, but where an $\varepsilon$-fraction of
the samples have been arbitrarily corrupted, output $\widehat{\Sigma}$
minimizing the total variation distance between $\mathcal{N}(\boldsymbol{0},
\Sigma)$ and $\mathcal{N}(\boldsymbol{0}, \widehat{\Sigma})$. This corresponds
to learning $\Sigma$ in a natural affine-invariant variant of the Frobenius
norm known as the \emph{Mahalanobis norm}. Previous work of Cheng et al
demonstrated an algorithm that, given $N = \Omega (d^2 / \varepsilon^2)$
samples, achieved a near-optimal error of $O(\varepsilon \log 1 /
\varepsilon)$, and moreover, their algorithm ran in time $\widetilde{O}(T(N, d)
\log \kappa / \mathrm{poly} (\varepsilon))$, where $T(N, d)$ is the time it
takes to multiply a $d \times N$ matrix by its transpose, and $\kappa$ is the
condition number of $\Sigma$. When $\varepsilon$ is relatively small, their
polynomial dependence on $1/\varepsilon$ in the runtime is prohibitively large.
In this paper, we demonstrate a novel algorithm that achieves the same
statistical guarantees, but which runs in time $\widetilde{O} (T(N, d) \log
\kappa)$. In particular, our runtime has no dependence on $\varepsilon$. When
$\Sigma$ is reasonably conditioned, our runtime matches that of the fastest
algorithm for covariance estimation without outliers, up to poly-logarithmic
factors, showing that we can get robustness essentially "for free."
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13266</id>
    <link href="http://arxiv.org/abs/2006.13266" rel="alternate" type="text/html"/>
    <title>OMiCroN -- Oblique Multipass Hierarchy Creation while Navigating</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Vin=iacute=cius_da.html">Vinícius da Silva</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esperan=ccedil=a:Claudio.html">Claudio Esperança</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marroquim:Ricardo.html">Ricardo Marroquim</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13266">PDF</a><br/><b>Abstract: </b>Rendering large point clouds ordinarily requires building a hierarchical data
structure for accessing the points that best represent the object for a given
viewing frustum and level-of-detail. The building of such data structures
frequently represents a large portion of the cost of the rendering pipeline
both in terms of time and space complexity, especially when rendering is done
for inspection purposes only. This problem has been addressed in the past by
incremental construction approaches, but these either result in low quality
hierarchies or in longer construction times. In this work we present OMiCroN --
Oblique Multipass Hierarchy Creation while Navigating -- which is the first
algorithm capable of immediately displaying partial renders of the geometry,
provided the cloud is made available sorted in Morton order. OMiCroN is fast,
being capable of building the entire data structure in memory spending an
amount of time that is comparable to that of just reading the cloud from disk.
Thus, there is no need for storing an expensive hierarchy, nor for delaying the
rendering until the whole hierarchy is read from disk. In fact, a pipeline
coupling OMiCroN with an incremental sorting algorithm running in parallel can
start rendering as soon as the first sorted prefix is produced, making this
setup very convenient for streamed viewing.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13241</id>
    <link href="http://arxiv.org/abs/2006.13241" rel="alternate" type="text/html"/>
    <title>The Bike Sharing Problem</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Czyzowicz:Jurek.html">Jurek Czyzowicz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Georgiou:Konstantinos.html">Konstantinos Georgiou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Killick:Ryan.html">Ryan Killick</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kranakis:Evangelos.html">Evangelos Kranakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krizanc:Danny.html">Danny Krizanc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narayanan:Lata.html">Lata Narayanan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Opatrny:Jaroslav.html">Jaroslav Opatrny</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pankratov:Denis.html">Denis Pankratov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13241">PDF</a><br/><b>Abstract: </b>Assume that $m \geq 1$ autonomous mobile agents and $0 \leq b \leq m$
single-agent transportation devices (called {\em bikes}) are initially placed
at the left endpoint $0$ of the unit interval $[0,1]$. The agents are identical
in capability and can move at speed one. The bikes cannot move on their own,
but any agent riding bike $i$ can move at speed $v_i &gt; 1$. An agent may ride at
most one bike at a time. The agents can cooperate by sharing the bikes; an
agent can ride a bike for a time, then drop it to be used by another agent, and
possibly switch to a different bike.
</p>
<p>We study two problems. In the \BS problem, we require all agents and bikes
starting at the left endpoint of the interval to reach the end of the interval
as soon as possible. In the \RBS problem, we aim to minimize the arrival time
of the agents; the bikes can be used to increase the average speed of the
agents, but are not required to reach the end of the interval.
</p>
<p>Our main result is the construction of a polynomial time algorithm for the
\BS problem that creates an arrival-time optimal schedule for travellers and
bikes to travel across the interval. For the \RBS problem, we give an algorithm
that gives an optimal solution for the case when at most one of the bikes can
be abandoned.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12943</id>
    <link href="http://arxiv.org/abs/2006.12943" rel="alternate" type="text/html"/>
    <title>Learning Based Distributed Tracking</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Hao.html">Hao Wu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gan:Junhao.html">Junhao Gan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Rui.html">Rui Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12943">PDF</a><br/><b>Abstract: </b>Inspired by the great success of machine learning in the past decade, people
have been thinking about the possibility of improving the theoretical results
by exploring data distribution. In this paper, we revisit a fundamental problem
called Distributed Tracking (DT) under an assumption that the data follows a
certain (known or unknown) distribution, and propose a number data-dependent
algorithms with improved theoretical bounds. Informally, in the DT problem,
there is a coordinator and k players, where the coordinator holds a threshold N
and each player has a counter. At each time stamp, at most one counter can be
increased by one. The job of the coordinator is to capture the exact moment
when the sum of all these k counters reaches N. The goal is to minimise the
communication cost. While our first type of algorithms assume the concrete data
distribution is known in advance, our second type of algorithms can learn the
distribution on the fly. Both of the algorithms achieve a communication cost
bounded byO(k log log N) with high probability, improving the state-of-the-art
data-independent bound O(k log N/k). We further propose a number of
implementation optimisation heuristics to improve both efficiency and
robustness of the algorithms. Finally, we conduct extensive experiments on
three real datasets and four synthetic datasets. The experimental results show
that the communication cost of our algorithms is as least as 20% of that of the
state-of-the-art algorithms.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12929</id>
    <link href="http://arxiv.org/abs/2006.12929" rel="alternate" type="text/html"/>
    <title>Approximation algorithms for general cluster routing problem</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Xiaoyan.html">Xiaoyan Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Du:Donglei.html">Donglei Du</a>, Gregory Gutin, Qiaoxia Ming, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Jian.html">Jian Sun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12929">PDF</a><br/><b>Abstract: </b>Graph routing problems have been investigated extensively in operations
research, computer science and engineering due to their ubiquity and vast
applications. In this paper, we study constant approximation algorithms for
some variations of the general cluster routing problem. In this problem, we are
given an edge-weighted complete undirected graph $G=(V,E,c),$ whose vertex set
is partitioned into clusters $C_{1},\dots ,C_{k}.$ We are also given a subset
$V'$ of $V$ and a subset $E'$ of $E.$ The weight function $c$ satisfies the
triangle inequality. The goal is to find a minimum cost walk $T$ that visits
each vertex in $V'$ only once, traverses every edge in $E'$ at least once and
for every $i\in [k]$ all vertices of $C_i$ are traversed consecutively.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12897</id>
    <link href="http://arxiv.org/abs/2006.12897" rel="alternate" type="text/html"/>
    <title>Polynomial Time Approximation Schemes for Clustering in Low Highway Dimension Graphs</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldmann:Andreas_Emil.html">Andreas Emil Feldmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saulpic:David.html">David Saulpic</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12897">PDF</a><br/><b>Abstract: </b>We study clustering problems such as k-Median, k-Means, and Facility Location
in graphs of low highway dimension, which is a graph parameter modeling
transportation networks. It was previously shown that approximation schemes for
these problems exist, which either run in quasi-polynomial time (assuming
constant highway dimension) [Feldmann et al. SICOMP 2018] or run in FPT time
(parameterized by the number of clusters $k$, the highway dimension, and the
approximation factor) [Becker et al. ESA~2018, Braverman et al. 2020]. In this
paper we show that a polynomial-time approximation scheme (PTAS) exists
(assuming constant highway dimension). We also show that the considered
problems are NP-hard on graphs of highway dimension 1.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12881</id>
    <link href="http://arxiv.org/abs/2006.12881" rel="alternate" type="text/html"/>
    <title>BETULA: Numerically Stable CF-Trees for BIRCH Clustering</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lang:Andreas.html">Andreas Lang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schubert:Erich.html">Erich Schubert</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12881">PDF</a><br/><b>Abstract: </b>BIRCH clustering is a widely known approach for clustering, that has
influenced much subsequent research and commercial products. The key
contribution of BIRCH is the Clustering Feature tree (CF-Tree), which is a
compressed representation of the input data. As new data arrives, the tree is
eventually rebuilt to increase the compression. Afterward, the leaves of the
tree are used for clustering. Because of the data compression, this method is
very scalable. The idea has been adopted for example for k-means, data stream,
and density-based clustering.
</p>
<p>Clustering features used by BIRCH are simple summary statistics that can
easily be updated with new data: the number of points, the linear sums, and the
sum of squared values. Unfortunately, how the sum of squares is then used in
BIRCH is prone to catastrophic cancellation.
</p>
<p>We introduce a replacement cluster feature that does not have this numeric
problem, that is not much more expensive to maintain, and which makes many
computations simpler and hence more efficient. These cluster features can also
easily be used in other work derived from BIRCH, such as algorithms for
streaming data. In the experiments, we demonstrate the numerical problem and
compare the performance of the original algorithm compared to the improved
cluster features.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12772</id>
    <link href="http://arxiv.org/abs/2006.12772" rel="alternate" type="text/html"/>
    <title>Combinatorial Pure Exploration of Dueling Bandit</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Wei.html">Wei Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Du:Yihan.html">Yihan Du</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Longbo.html">Longbo Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Haoyu.html">Haoyu Zhao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12772">PDF</a><br/><b>Abstract: </b>In this paper, we study combinatorial pure exploration for dueling bandits
(CPE-DB): we have multiple candidates for multiple positions as modeled by a
bipartite graph, and in each round we sample a duel of two candidates on one
position and observe who wins in the duel, with the goal of finding the best
candidate-position matching with high probability after multiple rounds of
samples. CPE-DB is an adaptation of the original combinatorial pure exploration
for multi-armed bandit (CPE-MAB) problem to the dueling bandit setting.
</p>
<p>We consider both the Borda winner and the Condorcet winner cases. For Borda
winner, we establish a reduction of the problem to the original CPE-MAB setting
and design PAC and exact algorithms that achieve both the sample complexity
similar to that in the CPE-MAB setting (which is nearly optimal for a subclass
of problems) and polynomial running time per round.
</p>
<p>For Condorcet winner, we first design a fully polynomial time approximation
scheme (FPTAS) for the offline problem of finding the Condorcet winner with
known winning probabilities, and then use the FPTAS as an oracle to design a
novel pure exploration algorithm ${\sf CAR}$-${\sf Cond}$ with sample
complexity analysis. ${\sf CAR}$-${\sf Cond}$ is the first algorithm with
polynomial running time per round for identifying the Condorcet winner in
CPE-DB.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12748</id>
    <link href="http://arxiv.org/abs/2006.12748" rel="alternate" type="text/html"/>
    <title>Approximation Algorithms for Sparse Principal Component Analysis</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chowdhury:Agniva.html">Agniva Chowdhury</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Drineas:Petros.html">Petros Drineas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12748">PDF</a><br/><b>Abstract: </b>We present three provably accurate, polynomial time, approximation algorithms
for the Sparse Principal Component Analysis (SPCA) problem, without imposing
any restrictive assumptions on the input covariance matrix. The first algorithm
is based on randomized matrix multiplication; the second algorithm is based on
a novel deterministic thresholding scheme; and the third algorithm is based on
a semidefinite programming relaxation of SPCA. All algorithms come with
provable guarantees and run in low-degree polynomial time. Our empirical
evaluations confirm our theoretical findings.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12670</id>
    <link href="http://arxiv.org/abs/2006.12670" rel="alternate" type="text/html"/>
    <title>An Efficient PTAS for Stochastic Load Balancing with Poisson Jobs</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/De:Anindya.html">Anindya De</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khanna:Sanjeev.html">Sanjeev Khanna</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Huan.html">Huan Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nikpey:Hesam.html">Hesam Nikpey</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12670">PDF</a><br/><b>Abstract: </b>We give the first polynomial-time approximation scheme (PTAS) for the
stochastic load balancing problem when the job sizes follow Poisson
distributions. This improves upon the 2-approximation algorithm due to Goel and
Indyk (FOCS'99). Moreover, our approximation scheme is an efficient PTAS that
has a running time double exponential in $1/\epsilon$ but nearly-linear in $n$,
where $n$ is the number of jobs and $\epsilon$ is the target error. Previously,
a PTAS (not efficient) was only known for jobs that obey exponential
distributions (Goel and Indyk, FOCS'99).
</p>
<p>Our algorithm relies on several probabilistic ingredients including some
(seemingly) new results on scaling and the so-called "focusing effect" of
maximum of Poisson random variables which might be of independent interest.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12608</id>
    <link href="http://arxiv.org/abs/2006.12608" rel="alternate" type="text/html"/>
    <title>Similarity Search with Tensor Core Units</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahle:Thomas_D=.html">Thomas D. Ahle</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silvestri:Francesco.html">Francesco Silvestri</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12608">PDF</a><br/><b>Abstract: </b>Tensor Core Units (TCUs) are hardware accelerators developed for deep neural
networks, which efficiently support the multiplication of two dense
$\sqrt{m}\times \sqrt{m}$ matrices, where $m$ is a given hardware parameter. In
this paper, we show that TCUs can speed up similarity search problems as well.
We propose algorithms for the Johnson-Lindenstrauss dimensionality reduction
and for similarity join that, by leveraging TCUs, achieve a $\sqrt{m}$ speedup
up with respect to traditional approaches.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12589</id>
    <link href="http://arxiv.org/abs/2006.12589" rel="alternate" type="text/html"/>
    <title>Distributional Individual Fairness in Clustering</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Nihesh Anderson, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bera:Suman_K=.html">Suman K. Bera</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Das:Syamantak.html">Syamantak Das</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yang.html">Yang Liu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12589">PDF</a><br/><b>Abstract: </b>In this paper, we initiate the study of fair clustering that ensures
distributional similarity among similar individuals. In response to improving
fairness in machine learning, recent papers have investigated fairness in
clustering algorithms and have focused on the paradigm of statistical
parity/group fairness. These efforts attempt to minimize bias against some
protected groups in the population. However, to the best of our knowledge, the
alternative viewpoint of individual fairness, introduced by Dwork et al. (ITCS
2012) in the context of classification, has not been considered for clustering
so far. Similar to Dwork et al., we adopt the individual fairness notion which
mandates that similar individuals should be treated similarly for clustering
problems. We use the notion of $f$-divergence as a measure of statistical
similarity that significantly generalizes the ones used by Dwork et al. We
introduce a framework for assigning individuals, embedded in a metric space, to
probability distributions over a bounded number of cluster centers. The
objective is to ensure (a) low cost of clustering in expectation and (b)
individuals that are close to each other in a given fairness space are mapped
to statistically similar distributions.
</p>
<p>We provide an algorithm for clustering with $p$-norm objective ($k$-center,
$k$-means are special cases) and individual fairness constraints with provable
approximation guarantee. We extend this framework to include both group
fairness and individual fairness inside the protected groups. Finally, we
observe conditions under which individual fairness implies group fairness. We
present extensive experimental evidence that justifies the effectiveness of our
approach.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12561</id>
    <link href="http://arxiv.org/abs/2006.12561" rel="alternate" type="text/html"/>
    <title>Better approximation algorithms for maximum weight internal spanning trees in cubic graphs and claw-free graphs</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biniaz:Ahmad.html">Ahmad Biniaz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12561">PDF</a><br/><b>Abstract: </b>Given a connected vertex-weighted graph $G$, the maximum weight internal
spanning tree (MaxwIST) problem asks for a spanning tree of $G$ that maximizes
the total weight of internal nodes. This problem is NP-hard and APX-hard, with
the currently best known approximation factor $1/2$ (Chen et al., Algorithmica
2019). For the case of claw-free graphs, Chen et al. present an involved
approximation algorithm with approximation factor $7/12$. They asked whether it
is possible to improve these ratios, in particular for claw-free graphs and
cubic graphs.
</p>
<p>We improve the approximation factors for the MaxwIST problem in cubic graphs
and claw-free graphs. For cubic graphs we present an algorithm that computes a
spanning tree whose total weight of internal vertices is at least
$\frac{3}{4}-\frac{3}{n}$ times the total weight of all vertices, where $n$ is
the number of vertices of $G$. This ratio is almost tight for large values of
$n$. For claw-free graphs of degree at least three, we present an algorithm
that computes a spanning tree whose total internal weight is at least
$\frac{3}{5}-\frac{1}{n}$ times the total vertex weight. The degree constraint
is necessary as this ratio may not be achievable if we allow vertices of degree
less than three.
</p>
<p>With the above ratios, we immediately obtain better approximation algorithms
with factors $\frac{3}{4}-\epsilon$ and $\frac{3}{5}-\epsilon$ for the MaxwIST
problem in cubic graphs and claw-free graphs of degree at least three, for any
$\epsilon&gt;0$. In addition to improving the approximation factors, the new
algorithms are relatively short compared to that of Chen et al.. The new
algorithms are fairly simple, and employ a variant of the depth-first search
algorithm that selects a relatively-large-weight vertex in every branching
step. Moreover, the new algorithms take linear time while previous algorithms
for similar problem instances are super-linear.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.03781</id>
    <link href="http://arxiv.org/abs/2006.03781" rel="alternate" type="text/html"/>
    <title>Attribute-Efficient Learning of Halfspaces with Malicious Noise: Near-Optimal Label Complexity and Noise Tolerance</title>
    <feedworld_mtime>1593043200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shen:Jie.html">Jie Shen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Chicheng.html">Chicheng Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.03781">PDF</a><br/><b>Abstract: </b>This paper is concerned with computationally efficient learning of
homogeneous sparse halfspaces in $\Rd$ under noise. Though recent works have
established attribute-efficient learning algorithms under various types of
label noise (e.g. bounded noise), it remains an open question of when and how
$s$-sparse halfspaces can be efficiently learned under the challenging {\em
malicious noise} model, where an adversary may corrupt both the unlabeled data
distribution and the labels. We answer this question in the affirmative by
designing a computationally efficient algorithm with near-optimal label
complexity $\tilde{O}\big(s \log^3 d \cdot \log^4\frac{1}{\epsilon}\big)$ and
noise tolerance $\eta = \Omega(\epsilon)$, where $\epsilon \in (0, 1)$ is the
target error rate. Our main techniques include attribute-efficient paradigms
for instance reweighting and for empirical risk minimization, and a new
analysis of uniform concentration for unbounded data~--~all of them crucially
take the structure of the underlying halfspace into account. To the best of our
knowledge, this is the first near-optimal result in the setting. As a byproduct
of our analysis, we resolve a long-standing problem in statistics and machine
learning: we show that a global optimum of sparse principal component analysis
can be found in polynomial time without any statistical assumption on the data.
This result might be of independent interest to both communities.
</p></div>
    </summary>
    <updated>2020-06-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/06/24/ideal-workshop-on-computational-vs-statistical-tradeoffs-in-network-inference/</id>
    <link href="https://cstheory-events.org/2020/06/24/ideal-workshop-on-computational-vs-statistical-tradeoffs-in-network-inference/" rel="alternate" type="text/html"/>
    <title>IDEAL Workshop on Computational vs Statistical Tradeoffs in Network Inference</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">June 29, 2020 Virtual https://www.ideal.northwestern.edu/events/workshop-computational-vs-statistical-tradeoffs-in-network-inference/ Network models have been used as a tool to understand the role of interconnections between entities in multiple research areas like sociology, biology, meteorology, economics, and computer science. Moreover emerging technological developments allow collecting data on increasingly larger networks. This leads to both computational and statistical challenges when inferring or … <a class="more-link" href="https://cstheory-events.org/2020/06/24/ideal-workshop-on-computational-vs-statistical-tradeoffs-in-network-inference/">Continue reading <span class="screen-reader-text">IDEAL Workshop on Computational vs Statistical Tradeoffs in Network Inference</span></a></div>
    </summary>
    <updated>2020-06-24T23:53:00Z</updated>
    <published>2020-06-24T23:53:00Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-06-25T20:21:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/096</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/096" rel="alternate" type="text/html"/>
    <title>TR20-096 |  On the asymptotic complexity of sorting | 

	Igor Sergeev</title>
    <summary>We investigate the number of pairwise comparisons sufficient to sort $n$ elements chosen from a linearly ordered set. This number is shown to be $\log_2(n!) + o(n)$ thus improving over the previously known upper bounds of the form $\log_2(n!) + \Theta(n)$. The new bound is achieved by the proposed group insertion sorting algorithm.</summary>
    <updated>2020-06-24T17:41:11Z</updated>
    <published>2020-06-24T17:41:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-25T20:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/095</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/095" rel="alternate" type="text/html"/>
    <title>TR20-095 |  On Basing Auxiliary-Input Cryptography on NP-hardness via Nonadaptive Black-Box Reductions | 

	Mikito Nanashima</title>
    <summary>A black-box (BB) reduction is a central proof technique in theoretical computer science. However, the limitations on BB reductions have been revealed for several decades, and the series of previous work gives strong evidence that we should avoid a nonadaptive BB reduction to base cryptography on NP-hardness (e.g., Akavia et al., 2006). Then should we also give up such a familiar proof technique even for an intermediate step towards cryptography?

In this paper, we continue to explore the capability of nonadaptive BB reductions and extend our knowledge on such a central technique out of the current (worst-to-average) framework. In particular, we investigate the attempt to base weaker cryptographic notions allowed to take auxiliary-input via nonadaptive BB reductions. As a result, we prove the following theorems: (1) if we base an auxiliary-input pseudorandom generator (AIPRG) on NP-hardness via a nonadaptive BB reduction, then the polynomial hierarchy collapses; (2) if we base an auxiliary-input one-way function (AIOWF) or auxiliary-input hitting set generator (AIHSG) on NP-hardness via a nonadaptive BB reduction, then an (i.o.-)one-way function also exists based on NP-hardness (via an adaptive BB reduction).

The first result gives new evidence that nonadaptive BB reductions are insufficient to base AIPRG. The second result also yields a weaker but still surprising consequence of nonadaptive BB reductions, that is, a one-way function based on NP-hardness. In fact, the second result is interpreted as the following two opposite ways. Pessimistically, it shows that basing AIOWF or AIHSG via nonadaptive BB reductions is harder than constructing a one-way function based on NP-hardness, which can be regarded as a negative result. Note that AIHSG is a weak primitive implied even by the hardness of learning; thus, this pessimistic view gives conceptually stronger limitations than the currently known limitations on nonadaptive BB reductions. Optimistically, our result gives a new hope: a breakthrough construction of auxiliary-input primitives might also be useful to construct standard cryptographic primitives. This optimistic view enhances the significance of further investigation on constructing auxiliary-input or other intermediate cryptographic primitives instead of standard cryptographic primitives.</summary>
    <updated>2020-06-24T16:44:42Z</updated>
    <published>2020-06-24T16:44:42Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-25T20:20:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2020/06/24/equilibrium-min-max/</id>
    <link href="http://offconvex.github.io/2020/06/24/equilibrium-min-max/" rel="alternate" type="text/html"/>
    <title>An equilibrium in nonconvex-nonconcave min-max optimization</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>While there has been incredible progress in convex and nonconvex minimization, a multitude of problems in ML today are in need of efficient algorithms to solve min-max optimization problems. 
 Unlike minimization, where algorithms can always be shown to converge to some local minimum, there is no notion of a local equilibrium in min-max optimization that exists for general nonconvex-nonconcave functions.
    In two recent papers, we give  two notions of local equilibria that are guaranteed to exist and efficient algorithms to compute them.
In this post we present the key ideas behind a second-order notion of local min-max equilibrium from <a href="https://arxiv.org/abs/2006.12363">this paper</a> and in the next we will talk about a different notion along with the algorithm and show its implications to GANs from <a href="https://arxiv.org/abs/2006.12376">this paper</a>.</p>

<h2 id="min-max-optimization">Min-max optimization</h2>

<p>Min-max optimization of an objective function $f:\mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$</p>



<p>is a powerful framework in optimization, economics, and ML as it allows one to model learning in the presence of multiple agents with competing objectives.
In ML applications, such as <a href="https://arxiv.org/abs/1406.2661">GANs</a> and <a href="https://adversarial-ml-tutorial.org">adversarial robustness</a>, the min-max objective function may be nonconvex-nonconcave.
We know that min-max optimization is at least as hard as minimization, hence, we cannot hope to find a globally optimal solution to min-max problems for general functions.</p>

<h2 id="approximate-local-minima-for-minimization">Approximate local minima for minimization</h2>

<p>Let us first revisit the special case of minimization, where there is a natural notion of an approximate second-order local minimum.</p>

<blockquote>
  <p>$x$ is a second-order $\varepsilon$-local minimum of $\mathcal{L}:\mathbb{R}^d\rightarrow \mathbb{R}$ if
</p>
</blockquote>

<p>Now suppose we just wanted to minimize a function $\mathcal{L}$, and we start from any point which is <em>not</em> at an $\varepsilon$-local minimum of $\mathcal{L}$.
Then we can always find a direction to travel in along which either $\mathcal{L}$ decreases rapidly, or the second derivative of $\mathcal{L}$ is large.
 By searching in such a direction we can easily find a new point which has a smaller value of $\mathcal{L}$ using only local information about the gradient and Hessian of $\mathcal{L}$.
 This means that we can keep decreasing $\mathcal{L}$ until we reach an $\varepsilon$-local minimum (see <a href="https://www.researchgate.net/profile/Boris_Polyak2/publication/220589612_Cubic_regularization_of_Newton_method_and_its_global_performance/links/09e4150dd2f0320879000000/Cubic-regularization-of-Newton-method-and-its-global-performance.pdf">Nesterov and Polyak</a>,  <a href="https://dl.acm.org/doi/10.1145/3055399.3055464">here</a>,  <a href="http://proceedings.mlr.press/v40/Ge15.pdf">here</a>,  and also an earlier <a href="https://www.offconvex.org/2016/03/22/saddlepoints">blog post</a> for how to do this with only access to gradients of $\mathcal{L}$).
 If $\mathcal{L}$ is Lipschitz smooth and bounded, we will reach an $\varepsilon$-local minimum in polynomial time from any starting point.</p>

<blockquote>
  <p>Is there an analogous definition with similar properties for min-max optimization?</p>
</blockquote>

<h2 id="problems-with-current-local-optimality-notions">Problems with current local optimality notions</h2>
<p>There has been much recent work on extending theoretical results in nonconvex minimization to min-max optimization (see <a href="https://arxiv.org/abs/1906.00331">here</a>, <a href="https://papers.nips.cc/paper/9430-efficient-algorithms-for-smooth-minimax-optimization">here</a>, <a href="https://arxiv.org/pdf/1807.02629.pdf">here</a>,  <a href="https://papers.nips.cc/paper/9631-solving-a-class-of-non-convex-min-max-games-using-iterative-first-order-methods.pdf">here</a>, <a href="https://arxiv.org/abs/1910.07512">here</a>.
One way to extend the notion of local minimum to the min-max setting is to seek a solution point called a “local saddle”–a point $(x,y)$ where 1) $y$ is a local maximum for $f(x, \cdot)$ and 2) $x$ is a local minimum for $f(\cdot, y).$</p>

<p>For instance,
 this is used  <a href="https://arxiv.org/abs/1706.08500">here</a>, <a href="https://arxiv.org/pdf/1901.00838.pdf">here</a>, <a href="https://arxiv.org/pdf/1705.10461.pdf">here</a>, and <a href="http://proceedings.mlr.press/v89/adolphs19a.html">here</a>.
But, there are very simple examples of two-dimensional bounded functions where a local saddle does not exist.</p>

<blockquote>
  <p>For instance, consider $f(x,y) = sin(x+y)$ from <a href="https://arxiv.org/abs/1902.00618">here</a>. Check that none of the points on this function are simultaneously a local minimum for $x$ and local maximum for $y$.</p>
</blockquote>

<p>The fact that no local saddle exists may be surprising, since an $\varepsilon$-global solution to a min-max optimization problem <em>is</em> guaranteed to exist as long as the objective function is uniformly bounded.
Roughly, this is because, in a global min-max setting, the max-player is empowered to globally maximize the function $f(x,\cdot)$, and the min-player is empowered to minimize the “global max” function $\max_y(f(x, \cdot))$.</p>

<p>The ability to compute the global max  allows the min-player to  predict the max-player’s response.
If $x$ is a global minimum of $\max_y(f(x, \cdot))$, the min-player is aware of this fact and will have no incentive to update $x$.
On the other hand, if the min-player can only simulate the max-player’s updates locally (as in local saddle),
then the min-player may try to update her strategy even when it leads to a net increase in $f$.
This can happen because the min-player is not powerful enough to accurately simulate the max-player’s response. (See  a  <a href="https://arxiv.org/abs/1902.00618">related notion</a> of local optimality with similar issues due to vanishingly small updates.)</p>

<p>The fact that players who can only make local predictions are
unable to predict their opponents’ responses can lead to convergence problems in many popular algorithms such as<br/>
gradient descent ascent (GDA). This non-convergence behavior can occur if the function has no local saddle point (e.g. the function $sin(x+y)$  mentioned above), and can even happen on some functions, like $f(x,y) = xy$ which do have a local saddle point.</p>

<div style="text-align: center;">
<img alt="" src="http://www.offconvex.org/assets/GDA_spiral_fast.gif"/>
<br/>
<b>Figure 1.</b> GDA spirals off to infinity from almost every starting point on the objective function $f(x,y) = xy$. 
</div>
<p><br/></p>

<h2 id="greedy-max-a-computationally-tractable-alternative-to-global-max">Greedy max: a computationally tractable alternative to global max</h2>

<p>To allow for a more stable min-player, and a more stable notion of local optimality, we would like to empower the min-player to more effectively simulate the max-player’s response. 
While the notion of global min-max does exactly this by having the min-player compute the global max function $\max_y(f(\cdot,y))$, computing the global maximum may be intractable.</p>

<p>Instead, we replace the global max function $\max_y (f(\cdot ,y))$ with a computationally tractable alternative. 
Towards this end, we restrict the max-player’s response, and the min-player’s simulation of this response, to updates which can be computed using any algorithm from a class of second-order optimization algorithms.
More specifically, we restrict the max-player to updating $y$ by traveling along continuous paths which start at the current value of $y$ and along which either $f$ is increasing or the second derivative of $f$ is positive.  We refer to such paths as greedy paths since they model a class of second-order “greedy” optimization algorithms.</p>

<blockquote>
  <p><strong>Greedy path:</strong> A unit-speed path $\varphi:[0,\tau] \rightarrow \mathbb{R}^d$ is greedy if $f$ is non-decreasing over this path, and for every $t\in[0,\tau]$
</p>
</blockquote>

<p>Roughly speaking, when restricted to updates obtained from greedy paths, the max-player will always be able to reach a point which is an approximate local maximum for $f(x,\cdot)$, although there may not be a greedy path which leads the max-player to a global maximum.</p>

<div style="text-align: center;">
<img alt="" src="http://www.offconvex.org/assets/greedy_region_omega_t.png" style="width: 400px;"/> <img alt="" src="http://www.offconvex.org/assets/global_max_path_no_axes_t.png" style="width: 400px;"/> 
<br/>
 <b>Figure 2.</b> <i>Left:</i> The light-colored region $\Omega$ is reachable from the initial point $A$ by a greedy path; the dark region is not reachable. <i>Right:</i> There is always a greedy path from any point $A$ to a local maximum ($B$), but a global maximum ($C$) may not be reachable by any greedy path.
</div>
<p><br/></p>

<p>To define an alternative to $\max_y(f(\cdot,y))$, we consider the local maximum point with the largest value of $f(x,\cdot)$ attainable from a given starting point $y$ by any greedy path.
We refer to the value of $f$ at this point as the <em>greedy max function</em>, and denote this value by $g(x,y)$.</p>

<blockquote>
  <p><strong>Greedy max function:</strong> 
    $g(x,y) = \max_{z \in \Omega} f(x,z),$
where $\Omega$ is points reachable from $y$ by greedy path.</p>
</blockquote>

<h2 id="our-greedy-min-max-equilibrium">Our greedy min-max equilibrium</h2>
<p>We use the greedy max function to define a new second-order notion of local optimality for min-max optimization, which we refer to as a greedy min-max equilibrium.
Roughly speaking, we say that $(x,y)$ is a greedy min-max equilibrium if 
1) $y$ is a local maximum for $f(x,\cdot)$ (and hence the endpoint of a greedy path), and 
2) if $x$ is a local minimum of the greedy max function $g(\cdot,y)$.</p>

<p>In other words, $x$ is a local minimum of $\max_y f(\cdot, y)$ under the constraint that the maximum is computed only over the set of greedy paths starting at $y$.
Unfortunately, even if $f$ is smooth, the greedy max function may not be differentiable with respect to $x$ and may even be discontinuous.</p>

<div style="text-align: center;">
<img alt="" src="http://www.offconvex.org/assets/discontinuity2_grid_t.png" width="400"/> <img alt="" src="http://www.offconvex.org/assets/discontinuity2g_grid_t.png" width="400"/> 
<br/>
 <b>Figure 3.</b> <i>Left:</i> If we change $x$ from one value $x$ to a very close value $\hat{x}$, the largest value of $f$ reachable by greedy path undergoes a discontinuous change.  <i>Right:</i>  This means the greedy max function $g(x,y)$ is discontinuous in $x$.</div>
<p><br/></p>

<p>This creates a problem, since the definition of $\varepsilon$-local minimum only applies to smooth functions.</p>

<p>To solve this problem we would ideally like to smooth $g$ by convolution with a Gaussian.
Unfortunately, convolution can cause the local minima of a function to “shift”– a point which is a local minimum for $g$ may no longer be a local minimum for the convolved version of $g$ (to see why, try convolving the function $f(x) = x - 3x I(x\leq 0) + I(x \leq 0)$ with a Gaussian $N(0,\sigma^2)$ for any $\sigma&gt;0$).
To avoid this, we instead consider a “truncated” version of $g$, and then convolve this function in the $x$ variable with a Gaussian to obtain our smoothed version of $g$.</p>

<p>This allows us to define a notion of greedy min-max equilibrium.  We say that a point $(x^\star, y^\star)$ is a greedy min-max equilibrium if $y^\star$ is an approximate local maximum of $f(x^\star, \cdot)$, and $x^\star$ is an $\varepsilon$-local minimum of this smoothed version of $g(\cdot, y^\star)$.</p>

<blockquote>
  <p><b>Greedy min-max equilibrium:</b>
$(x^{\star}, y^{\star})$ is a greedy min-max equilibrium if

 
where $S(x,y):= \mathrm{smooth}_x(\mathrm{truncate}(g(x, y))$.</p>
</blockquote>

<p>Any point which is a local saddle point (talked about earlier) also satisifeis our equilibrium conditions. The converse, however, cannot be true as a local saddle point may not always exist. Further, for compactly supported convex-concave functions a point is a greedy min-max equilibrium (in an appropriate sense) if and only if it is a global min-max point. (See Section 7 and Appendix A respectively in <a href="https://arxiv.org/abs/2006.12363">our paper</a>.)</p>

<h2 id="greedy-min-max-equilibria-always-exist-and-can-be-found-efficiently">Greedy min-max equilibria always exist! (And can be found efficiently)</h2>
<p>In <a href="https://arxiv.org/abs/2006.12363">this paper</a> we show: A greedy min-max equilibrium is always guaranteed to exist provided that $f$ is uniformly bounded with Lipschitz Hessian. We do so by providing an algorithm which converges to a greedy min-max equilibrium, and, moreover, we show that it is able to do this in polynomial time from any initial point:</p>

<blockquote>
  <p><b>Main theorem:</b> Suppose that we are given access to a smooth function $f:\mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$ and to its gradient and Hessian.  And suppose that $f$ is unformly bounded by $b&gt;0$ and has $L$-Lipschitz Hessian.
Then given any initial point, our algorithm returns an $\varepsilon$-greedy min-max equilibrium $(x^\star,y^\star)$ of $f$ in $\mathrm{poly}(b, L, d, \frac{1}{\varepsilon})$ time.</p>
</blockquote>

<p>There are a number of difficulties that our algorithm and proof must overcome:
One difficulty in designing an algorithm is that the greedy max function may be discontinuous. 
To find an approximate local minimum of a discontinuous function, our algorithm combines a Monte-Carlo hill climbing algorithm with a <a href="https://arxiv.org/abs/cs/0408007">zeroth-order optimization version</a> of stochastic gradient descent.
Another difficulty is that, while one can easily compute a greedy path from any starting point, there may be many different greedy paths which end up at different local maxima.
Searching for the greedy path which leads to the local maximum point with the largest value of $f$ may be infeasible.
In other words the greedy max function $g$ may be intractable to compute.</p>

<div style="text-align: center;">
<img alt="" src="http://www.offconvex.org/assets/greedy_paths_no_axes_t.png" width="400"/> 
<br/>
 <b>Figure 4.</b>There are many different greedy paths that start at the same point $A$.  They can end up at different local maxima ($B$, $D$), with different values of $f$.  In many cases it may be intractable to search over all these paths to compute the greedy max function.
 </div>
<p><br/></p>

<p>To get around this problem, rather than computing the exact value of $g(x,y)$, we instead compute a lower bound $h(x,y)$ for the greedy max function. Since we are able to obtain this lower bound by computing only a <em>single</em> greedy path, it is much easier to compute than greedy max function.</p>

<p>In our paper, we prove that if 1) $x^\star$ is an approximate local minimum for the this lower bound $h(\cdot, y^\star)$, and  2) $y^\star$ is a an approximate local maximum for $f(x^\star, \cdot)$, then $x^\star$ is also an approximate local minimum for the greedy max $g(\cdot, y^\star)$.
This allows us to design an algorithm which obtains a greedy min-max point by minimizing the computationally tractable lower bound $h$, instead of the greedy max function which may be intractable to compute.</p>

<h2 id="to-conclude">To conclude</h2>

<p>In this post we have shown how to extend a notion of second-order equilibrium for minimization to min-max optimization which is guaranteed to exist for any function which is bounded and Lipschitz, with Lipschitz gradient and Hessian.
We have also shown that our algorithm is able to find this equilibrium in  polynomial time from any initial point.</p>

<blockquote>
  <p>Our results do not require any additional assumptions such as convexity, monotonicity, or sufficient bilinearity.</p>
</blockquote>

<p>In an upcoming blog post we will show how one can use some of the ideas from here to obtain a new min-max optimization algorithm with applications to stably training GANs.</p></div>
    </summary>
    <updated>2020-06-24T10:00:00Z</updated>
    <published>2020-06-24T10:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2020-06-25T13:21:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/094</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/094" rel="alternate" type="text/html"/>
    <title>TR20-094 |  Is it possible to improve Yao’s XOR lemma using reductions that exploit the efficiency of their oracle? | 

	Ronen Shaltiel</title>
    <summary>Yao's XOR lemma states that for every function $f:\set{0,1}^k \ar \set{0,1}$, if $f$ has hardness $2/3$ for $P/poly$ (meaning that for every circuit $C$ in $P/poly$, $\Pr[C(X)=f(X)] \le 2/3$ on a uniform input $X$), then the task of computing $f(X_1) \oplus \ldots \oplus f(X_t)$ for sufficiently large $t$ has hardness $\half +\epsilon$ for $P/poly$.

Known proofs of this lemma cannot achieve $\epsilon=\frac{1}{k^{\omega(1)}}$, and even for $\epsilon=\frac{1}{k}$, we do not know how to replace
$P/poly$ by AC$^0[\textsc{parity}]$ (the class of constant depth circuits with the gates $\set{\textsc{and,or,not,parity}}$ of unbounded fan-in).

Recently, Grinberg, Shaltiel and Viola (FOCS 2018) (building on a sequence of earlier works) showed that these limitations cannot be circumvented by \emph{black-box reductions}. Namely, by reductions $\Red^{(\cdot)}$ that given oracle access to a function $D$ that violates the conclusion of Yao's XOR lemma, implement a circuit that violates the assumption of Yao's XOR lemma.

There are a few known reductions in the related literature on worst-case to average case reductions that are \emph{non-black box}. Specifically, the reductions of Gutfreund, Shaltiel and Ta Shma (Computational Complexity 2007) and  Hirahara (FOCS 2018)) are ``class reductions'' that are only guaranteed to succeed when given oracle access to an oracle $D$ from some efficient class of algorithms. These works seem to circumvent some black-box impossibility results.

In this paper we extend the previous limitations of Grinberg, Shaltiel and Viola to class reductions, giving evidence that class reductions cannot yield the desired improvements in Yao's XOR lemma.  To the best of our knowledge, this is the first limitation on reductions for hardness amplification that applies to class reductions.

Our technique imitates the previous lower bounds for black-box reductions, replacing the inefficient oracle used in that proof, with an efficient one that is based on limited independence, and developing tools to deal with the technical difficulties that arise following this replacement.</summary>
    <updated>2020-06-24T05:25:21Z</updated>
    <published>2020-06-24T05:25:21Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-25T20:20:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13073</id>
    <link href="http://arxiv.org/abs/2006.13073" rel="alternate" type="text/html"/>
    <title>Reduction From Non-Unique Games To Boolean Unique Games</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eldan:Ronen.html">Ronen Eldan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moshkovitz:Dana.html">Dana Moshkovitz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13073">PDF</a><br/><b>Abstract: </b>We reduce the problem of proving a "Boolean Unique Games Conjecture" (with
gap 1-delta vs. 1-C*delta, for any C&gt; 1, and sufficiently small delta&gt;0) to the
problem of proving a PCP Theorem for a certain non-unique game. In a previous
work, Khot and Moshkovitz suggested an inefficient candidate reduction (i.e.,
without a proof of soundness). The current work is the first to provide an
efficient reduction along with a proof of soundness. The non-unique game we
reduce from is similar to non-unique games for which PCP theorems are known.
Our proof relies on a new concentration theorem for functions in Gaussian space
that are restricted to a random hyperplane. We bound the typical Euclidean
distance between the low degree part of the restriction of the function to the
hyperplane and the restriction to the hyperplane of the low degree part of the
function.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12760</id>
    <link href="http://arxiv.org/abs/2006.12760" rel="alternate" type="text/html"/>
    <title>Symmetries, graph properties, and quantum speedups</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Ben=David:Shalev.html">Shalev Ben-David</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Childs:Andrew_M=.html">Andrew M. Childs</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gily=eacute=n:Andr=aacute=s.html">András Gilyén</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kretschmer:William.html">William Kretschmer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Podder:Supartha.html">Supartha Podder</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Daochen.html">Daochen Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12760">PDF</a><br/><b>Abstract: </b>Aaronson and Ambainis (2009) and Chailloux (2018) showed that fully symmetric
(partial) functions do not admit exponential quantum query speedups. This
raises a natural question: how symmetric must a function be before it cannot
exhibit a large quantum speedup?
</p>
<p>In this work, we prove that hypergraph symmetries in the adjacency matrix
model allow at most a polynomial separation between randomized and quantum
query complexities. We also show that, remarkably, permutation groups
constructed out of these symmetries are essentially the only permutation groups
that prevent super-polynomial quantum speedups. We prove this by fully
characterizing the primitive permutation groups that allow super-polynomial
quantum speedups.
</p>
<p>In contrast, in the adjacency list model for bounded-degree graphs (where
graph symmetry is manifested differently), we exhibit a property testing
problem that shows an exponential quantum speedup. These results resolve open
questions posed by Ambainis, Childs, and Liu (2010) and Montanaro and de Wolf
(2013).
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4870</id>
    <link href="https://www.scottaaronson.com/blog/?p=4870" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4870#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4870" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Pseudonymity as a trivial concession to genius</title>
    <summary xml:lang="en-US">Update (6/24): For further thoughts and context about this unfolding saga, see this excellent piece by Tom Chivers (author of The AI Does Not Hate You, so far the only book about the rationalist community, one that I reviewed here). This morning, like many others, I woke up to the terrible news that Scott Alexander—the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Update (6/24):</span></strong> For further thoughts and context about this unfolding saga, see <a href="https://unherd.com/2020/06/slate-star-codex-must-remain-anonymous/">this excellent piece by Tom Chivers</a> (author of <em>The AI Does Not Hate You</em>, so far the only book about the rationalist community, one that I <a href="https://www.scottaaronson.com/blog/?p=4361">reviewed here</a>).</p>



<p/><hr/><p/>



<p>This morning, like many others, I woke up to the terrible news that Scott Alexander—the man I call “the greatest Scott A. of the Internet”—has <a href="https://slatestarcodex.com/">deleted SlateStarCodex in its entirety</a>.  The reason, Scott explains, is that the <em>New York Times</em> was planning to run an article about SSC.  Even though the article was going to be <em>positive</em>, NYT decided that by policy, it would need to include Scott’s real surname (Alexander is his middle name).  Scott felt that revealing his name to the world would endanger himself and his psychiatry patients.  Taking down his entire blog was the only recourse that he saw.</p>



<p>The NYT writer, Cade Metz, was someone who I’d previously known and trusted from his reporting on Google’s quantum supremacy experiment.  So in recent weeks, I’d spent a couple hours on the phone with Cade, answering his questions about the rationality community, the history of my interactions with it, and why I thought SlateStarCodex spoke to so many readers.  Alas, when word got around the rationality community that Cade was writing a story, a huge panic arose that he was planning on some sort of <em>Gawker</em>-style hit piece or takedown.  Trying to tamp down the fire, I told Scott Alexander and others that I knew Cade, his intentions were good, he was only trying to understand the community, and everyone should help him by talking to him openly.</p>



<p>In a year of historic ironies, here’s another one: that it was the decent, reasonable, and well-meaning Cade Metz, rather than any of the SneerClubbers or Twitter-gangsters who despised Scott Alexander for sharing his honest thoughts on hot-button issues, who finally achieved the latter’s dark dream of exiling Scott from the public sphere.</p>



<p>The recent news had already been bad enough: Trump’s “temporary suspension” of J1 and H1B visas (which will deal a body blow to American universities this year, and to all the foreign scientists who planned to work at them), on top of the civil unrest, on top of the economic collapse, on top of the now-resurgent coronavirus.  But with no more SlateStarCodex, now I <em>really</em> feel like my world is coming to an end.</p>



<p>I’ve considered SSC to be the best blog on the Internet since not long after discovering it five years ago.  Of course my judgment is colored by one of the most notorious posts in SSC’s history (“Untitled”) being a ferocious defense of me, when thousands were attacking me and it felt like my life was finished.  But that’s merely what brought me there in the first place.  I stayed because of Scott’s insights about everything else, and because of the humor and humanity and craftsmanship of his prose.  Since then I had the privilege to become friends with Scott, not only virtually but in real life, and to meet dozens of others in the SSC community, in its Bay Area epicenter and elsewhere.</p>



<p>In my view, for SSC to be <em>permanently</em> deleted would be an intellectual loss on the scale of, let’s say, John Stuart Mill or Mark Twain burning their collected works.  That might sound like hyperbole, but not (I don’t think) to the tens of thousands who read Scott’s essays and fiction, particularly during their 2013-2016 heyday, and who went from casual enjoyment to growing admiration to the gradual recognition that they were experiencing, “live,” the works that future generations of teachers will assign their students when they cover the early twenty-first century.  The one thing that mitigates this tragedy is the hope that it will yet be reversed (and, of course, the fact that backups still exist in the bowels of the Internet).</p>



<p>When I discovered Scott Alexander in early 2015, the one issue that gave me pause was his strange insistence on maintaining pseudonymity, even as he was already then becoming more and more of a public figure.  In effect, Scott was trying to erect a firewall between his Internet persona and his personal and professional identities, and was <em>relying on the entire world’s goodwill</em> not to breach that firewall.  I thought to myself, “this can’t <em>possibly</em> last!  Scott simply writes too well to evade mainstream notice forever—and once he’s on the world’s radar, he’ll need to make a choice, about who he is and whether he’s ready to own his gifts to posterity under his real name.”  In retrospect, what astonishes me is that Scott has been able to maintain the “double life” for as long as he has!</p>



<p>In his takedown notice, Scott writes that it’s considered vitally important in psychiatry for patients to know almost nothing about their doctors, beyond their names and their areas of expertise.  That caused me to wonder: OK, but doesn’t the world already have enough psychiatrists who are ciphers to their patients?  Would it be so terrible to have one psychiatrist with a clear public persona—possibly even one who patients sought out <em>because</em> of his public persona, because his writings gave evidence that he’d have sympathy or insight about their conditions?  To become a psychiatrist, does one really need to take a lifelong vow of boringness—a vow never to do or say anything notable enough that one would be “outed” to one’s patients?  What would Freud, or Jung, or any of the other famous therapist-intellectuals of times past have thought about such a vow?</p>



<p>Scott also mentions that he’s gotten death threats, and harassing calls to his workplace, from people who hate him because of his blog (and who found his real name by sleuthing).  I wish I knew a solution to that.  For what it’s worth, my blogging has <em>also</em> earned me a death threat, and threats to sue me, and accusatory letters to the president of my university—although in my case, the worst threats came neither from Jew-hating neo-Nazis nor from nerd-bashing SJWs, but from crackpots enraged that I wouldn’t use my blog to credit their proof of P≠NP or their refutation of quantum mechanics.</p>



<p>When I started <em>Shtetl-Optimized</em> back in 2005, I remember thinking: this is it.  From now on, the only secrets I’ll have in life will be ephemeral and inconsequential ones.  From this day on, every student in my class, every prospective employer, every woman who I ask on a date (I wasn’t married yet), can know whatever they want to know about my political sympathies, my deepest fears and insecurities, any of it, with a five-second Google search.  Am I ready for that?  I decided that I was—partly just because I‘ve never had the mental space to maintain multiple partitioned identities <em>anyway</em>, to remember what each one is or isn’t allowed to know and say!  I won’t pretend that this is the right decision for everyone, but it was my decision, and I stuck with it, and it wasn’t always easy but I’m still here and so evidently are you.</p>



<p>I’d be <em>overjoyed</em> if Scott Alexander were someday to reach a place in his life where he felt comfortable deciding similarly.  That way, not only could he enjoy the full acclaim that he’s earned for what he’s given to the world, but (much more importantly) his tens of thousands of fans would be able to continue benefitting from his insights.</p>



<p>For now, though, the brute fact is that Scott is obviously <em>not</em> comfortable making that choice.  That being so, it seems to me that, if the NYT was able to respect the pseudonymity of Banksy and many others who it’s reported on in the past, when revealing their real names would serve no public interest, then it should also be able to respect Scott Alexander’s pseudonymity.  Especially now that Scott has sent the most credible signal imaginable of how much he values that pseudonymity, a signal that astonished even me.  The world does not exist only to serve its rare geniuses, but surely it can make such trivial concessions to them.</p></div>
    </content>
    <updated>2020-06-23T17:41:53Z</updated>
    <published>2020-06-23T17:41:53Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-06-24T07:33:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/093</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/093" rel="alternate" type="text/html"/>
    <title>TR20-093 |  Reduction From Non-Unique Games To Boolean Unique Games | 

	Dana Moshkovitz, 

	Ronen Eldan</title>
    <summary>We reduce the problem of proving a "Boolean Unique Games Conjecture" (with gap $1-\delta$ vs. $1-C\delta$, for any $C&gt; 1$, and sufficiently small $\delta&gt;0$) to the problem of proving a PCP Theorem for a certain non-unique game.
In a previous work, Khot and Moshkovitz suggested an inefficient candidate reduction (i.e., without a proof of soundness). 
The current work is the first to provide an efficient reduction along with a proof of soundness. 
The non-unique game we reduce from is similar to non-unique games for which PCP theorems are known.
Our proof relies on a new concentration theorem for functions in Gaussian space that are restricted to a random hyperplane. We bound the typical Euclidean distance between the low degree part of the restriction of the function to the hyperplane and the restriction to the hyperplane of the low degree part of the function.</summary>
    <updated>2020-06-23T14:41:45Z</updated>
    <published>2020-06-23T14:41:45Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-25T20:20:26Z</updated>
    </source>
  </entry>
</feed>
