<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-10-30T10:21:46Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=18400</id>
    <link href="https://gilkalai.wordpress.com/2019/10/30/amazing-keith-frankston-jeff-kahn-bhargav-narayanan-jinyoung-park-thresholds-versus-fractional-expectation-thresholds/" rel="alternate" type="text/html"/>
    <title>Amazing! Keith Frankston, Jeff Kahn, Bhargav Narayanan, Jinyoung Park: Thresholds versus fractional expectation-thresholds</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post describes a totally unexpected breakthrough about expectation and thresholds. The result  by Frankston, Kahn, Narayanan, and Park has many startling applications and it builds on the recent breakthrough work of Alweiss, Lovett, Wu and Zhang on the sunflower … <a href="https://gilkalai.wordpress.com/2019/10/30/amazing-keith-frankston-jeff-kahn-bhargav-narayanan-jinyoung-park-thresholds-versus-fractional-expectation-thresholds/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This post describes a <a href="https://arxiv.org/abs/1910.13433">totally unexpected breakthrough</a> about expectation and thresholds. The result  by Frankston, Kahn, Narayanan, and Park has many startling applications and it builds on the <a href="https://gilkalai.wordpress.com/2019/08/23/amazing-ryan-alweiss-shachar-lovett-kewen-wu-jiapeng-zhang-made-dramatic-progress-on-the-sunflower-conjecture/">recent breakthrough work of Alweiss, Lovett, Wu and Zhang on the sunflower conjecture</a>. Warm congratulations to Keith, Jeff, Bhargav, and Jinyoung!</p>
<p><span style="color: #0000ff;">Let me start with an update on the matter of applying the intermediate value theorem for football (or soccer as referred to in the US). You can read about it <a href="https://gilkalai.wordpress.com/2009/04/20/the-intermediate-value-theorem-applied-to-football/">in this 2009 post</a>. As you may recall, Shmuel Weinberger’s <a href="https://gilkalai.wordpress.com/2009/04/20/the-intermediate-value-theorem-applied-to-football/#comment-1010">raised the concern</a> of instability of fixed points. (A partial solution of Gower’s apply the original idea for three foreheads.) Sylvia Serfaty mentioned to me a possible one-player implementation based on <a href="https://en.wikipedia.org/wiki/Inverted_pendulum">inverted pendulum</a> control (See<a href="https://www.youtube.com/watch?v=5oGYCxkgnHQ"> this video</a>, and <a href="https://youtu.be/D3bblng-Kcc?t=1912">this one</a>, and <a href="https://www.youtube.com/watch?v=gnn21smGVrQ">this one</a>, and <a href="https://youtu.be/OCXrXUhJCTI?t=5118">this one</a>, and for the fascinating mathematics  <a href="https://youtu.be/swFwHWMTA4k">this lecture</a> by Jean-Michel Coron. <strong>Please, don’t try it at home.</strong>) <span style="color: #ff0000;">Implement this method in football is a notable remaining challenge <span style="color: #0000ff;">(this you can try at home)</span></span>. On another matter, for readers interested in the Google’s quantum supremacy news, here is a link of my <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">main post on the matter</a>.</span></p>
<h2>Thresholds versus fractional expectation-thresholds</h2>
<p class="title mathjax">This morning the following paper appeared on the arXive: <a href="https://arxiv.org/abs/1910.13433">Thresholds versus fractional expectation-thresholds</a> by Keith Frankston, Jeff Kahn, Bhargav Narayanan, and Jinyoung Park.</p>
<p><strong>Abstract:</strong> Proving a conjecture of Talagrand, a fractional version of the ‘expectation-threshold’ conjecture of Kalai and the second author, we show for any increasing family <strong>F</strong> on a finite set <strong>X</strong> that <img alt="p_c(F)=O(q_f(F) \log \ell ({\bf F}))" class="latex" src="https://s0.wp.com/latex.php?latex=p_c%28F%29%3DO%28q_f%28F%29+%5Clog+%5Cell+%28%7B%5Cbf+F%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_c(F)=O(q_f(F) \log \ell ({\bf F}))"/>, where <img alt="p_c({\bf F})" class="latex" src="https://s0.wp.com/latex.php?latex=p_c%28%7B%5Cbf+F%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_c({\bf F})"/> and <img alt="q_f({\bf F})" class="latex" src="https://s0.wp.com/latex.php?latex=q_f%28%7B%5Cbf+F%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_f({\bf F})"/> are the threshold and ‘fractional expectation-threshold’ of <strong>F</strong>, and ℓ(<strong>F</strong>) is the largest size of a minimal member of <strong>F</strong>. This easily implies various heretofore difficult results in probabilistic combinatorics, e.g. thresholds for perfect hypergraph matchings (Johansson-Kahn-Vu) and bounded-degree spanning trees (Montgomery). We also resolve (and vastly extend) one version of the ‘random multi-dimensional assignment’ problem of Frieze and Sorkin. Our approach builds on recent breakthrough work of Alweiss, Lovett, Wu and Zhang on the Erdős-Rado ‘sunflower’ conjecture.</p>
<h2>The expectation-threshold conjecture</h2>
<p><a href="https://gilkalai.files.wordpress.com/2019/10/kahn-kalai.png"><img alt="" class="alignnone size-full wp-image-18406" height="476" src="https://gilkalai.files.wordpress.com/2019/10/kahn-kalai.png?w=640&amp;h=476" width="640"/></a></p>
<p>The 2006 <a href="https://arxiv.org/abs/math/0603218">expectation threshold conjecture</a> gives a justification for a naive way to estimate the threshold probability of a random graph property. Suppose that you are asked about the critical probability for a random graph in G(n,p) for having a perfect matching (or a Hamiltonian cycle). You compute the expected number of perfect matchings and realize that when p is C/n this expected number equals 1/2. (For Hamiltonian cycles it will be C’/n.) Of course, if the expectation is one half the probability for a perfect matching can be very low, indeed in this case, an isolated vertex is quite likely but when there is no isolated vertices the expected number of perfect matchings is rather large. Our 2006 conjecture boldly asserts that the gap between the value given by such a naive computation and the true threshold value is at most logarithmic in the number of vertices. Jeff and I tried hard to find a counterexample but instead we managed to find more general and stronger forms of the conjecture that we could not disprove.</p>
<h2>Conjectures by Talagrand</h2>
<p>The expectation threshold conjecture had some connections with a 1995 paper of Michel Talagrand entitled <a href="https://link.springer.com/chapter/10.1007%2F978-3-0348-9090-8_25">Are all sets of positive measure essentially convex?</a> In a 2010 STOC paper <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.6973&amp;rep=rep1&amp;type=pdf">Are Many Small Sets Explicitly Small?</a> Michel formulated a weaker fractional version of the expectation threshold conjecture which is sufficient for the various applications of the original conjecture. This conjecture (as well as a stronger form also posed by Talagrand) is now verified in the new paper!</p>
<h2>Connection to isoperimetry</h2>
<p>In our 2006 paper we tried to relate the expectation threshold conjecture to various questions of independent interest related to stability theorems for discrete isoperimetric inequalities. This direction did not play a role in the new paper. Let me note that the isoperimetric problems served as partial motivation for the recent breakthrough results by Peter Keevash, Noam Lifshitz, Eoin Long, and Dor Minzer that are reported in this <a href="https://gilkalai.wordpress.com/2018/10/30/exciting-beginning-of-the-year-activities-and-seminars/">October 2018 post</a>. See their paper <a href="https://arxiv.org/abs/1906.05568">Hypercontractivity for global functions and sharp thresholds</a>.</p>
<h2>A sample from the applications</h2>
<ol>
<li>The threshold value for perfect matching – this was proved already by Erdos and Renyi (1960)  and it follow from the new results. The same goes for the threshold for connectivity.</li>
<li>The threshold value for Hamiltonian circuits – posed as a problem by  Erdos and Renyi it was solved by Korshunov (1976) and by Posa (1976).</li>
<li>The threshold for perfect matching in 3-uniform hypergraphs – was posed by Schmidt and Shamir (1983) and was settled by  Johansson, Kahn, and Vu. (It was one of the motivation for my 2006 paper with Jeff.)</li>
<li>The threshold for bounded degree spanning trees that was open for a long time and was settled by Montgomery (2019).</li>
</ol>
<p>Let me mention that in various cases the gap between the (fractional) expectation threshold and threshold is a smaller power of log <em>n, </em>or is a constant, or has different behavior. Understanding this through a general theory is still unknown.</p>
<h2><strong>Connection to the sunflower breakthrough</strong></h2>
<p>What did play a major role in the new development was the recent <a href="https://arxiv.org/abs/1908.08483">breakthrough work</a> of Alweiss, Lovett, Wu and Zhang on the Erdős-Rado ‘sunflower’ conjecture. (See <a href="https://gilkalai.wordpress.com/2019/08/23/amazing-ryan-alweiss-shachar-lovett-kewen-wu-jiapeng-zhang-made-dramatic-progress-on-the-sunflower-conjecture/">this post</a>.)  I expected that the method of the sunflower paper will have major applications but this application took me by a surprise.</p>
<p> </p></div>
    </content>
    <updated>2019-10-30T08:53:54Z</updated>
    <published>2019-10-30T08:53:54Z</published>
    <category term="Combinatorics"/>
    <category term="Probability"/>
    <category term="Bhargav Narayanan"/>
    <category term="Jeff Kahn"/>
    <category term="Jinyoung Park"/>
    <category term="Keith Frankston"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-10-30T10:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13386</id>
    <link href="http://arxiv.org/abs/1910.13386" rel="alternate" type="text/html"/>
    <title>NC Algorithms for Popular Matchings in One-Sided Preference Systems and Related Problems</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hu:Changyong.html">Changyong Hu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Vijay_K=.html">Vijay K. Garg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13386">PDF</a><br/><b>Abstract: </b>The popular matching problem is of matching a set of applicants to a set of
posts, where each applicant has a preference list, ranking a non-empty subset
of posts in the order of preference, possibly with ties. A matching M is
popular if there is no other matching M' such that more applicants prefer M' to
M. We give the first NC algorithm to solve the popular matching problem without
ties. We also give an NC algorithm that solves the maximum-cardinality popular
matching problem. No NC or RNC algorithms were known for the matching problem
in preference systems prior to this work. Moreover, we give an NC algorithm for
a weaker version of the stable matching problem, that is, the problem of
finding the "next" stable matching given a stable matching.
</p></div>
    </summary>
    <updated>2019-10-30T01:21:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13367</id>
    <link href="http://arxiv.org/abs/1910.13367" rel="alternate" type="text/html"/>
    <title>Derivation and Analysis of Fast Bilinear Algorithms for Convolution</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Caleb Ju, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomonik:Edgar.html">Edgar Solomonik</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13367">PDF</a><br/><b>Abstract: </b>The prevalence of convolution in applications within signal processing, deep
neural networks, and numerical solvers has motivated the development of
numerous fast convolution algorithms. In many of these problems, convolution is
performed on terabytes or petabytes of data, so even constant factors of
improvement can significantly reduce the computation time. We leverage the
formalism of bilinear algorithms to describe and analyze all of the most
popular approaches. This unified lens permits us to study the relationship
between different variants of convolution as well as to derive error bounds and
analyze the cost of the various algorithms. We provide new derivations, which
predominantly leverage matrix and tensor algebra, to describe the Winograd
family of convolution algorithms as well as reductions between 1D and
multidimensional convolution. We provide cost and error bounds as well as
experimental numerical studies. Our experiments for two of these algorithms,
the overlap-add approach and Winograd convolution algorithm with polynomials of
degree greater than one, show that fast convolution algorithms can rival the
accuracy of the fast Fourier transform (FFT) without using complex arithmetic.
These algorithms can be used for convolution problems with multidimensional
inputs or for filters larger than size of four, extending the state-of-the-art
in Winograd-based convolution algorithms.
</p></div>
    </summary>
    <updated>2019-10-30T01:20:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13352</id>
    <link href="http://arxiv.org/abs/1910.13352" rel="alternate" type="text/html"/>
    <title>Equipartitions with Wedges and Cones</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schnider:Patrick.html">Patrick Schnider</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13352">PDF</a><br/><b>Abstract: </b>A famous result about mass partitions is the so called \emph{Ham-Sandwich
theorem}. It states that any $d$ mass distributions in $\mathbb{R}^d$ can be
simultaneously bisected by a single hyperplane. In this work, we study two
related questions.
</p>
<p>The first one is, whether we can bisect more than $d$ masses, if we allow for
bisections with more general objects such as cones, wedges or double wedges. We
answer this question in the affirmative by showing that with all of these
objects, we can simultaneously bisect $d+1$ masses. For double wedges, we prove
a stronger statement, namely that $d$ families of $d+1$ masses each can each by
simultaneously bisected by some double wedge such that all double wedges have
one hyperplane in common.
</p>
<p>The second question is, how many masses we can simultaneously equipartition
with a $k$-fan, that is, $k$ half-hyperplanes in $\mathbb{R}^d$, emanating from
a common $(d-2)$-dimensional apex. This question was already studied in the
plane, our contribution is to extend the planar results to higher dimensions.
</p>
<p>All of our results are proved using topological methods. We use some
well-established techniques, but also some newer methods. In particular, we
introduce a Borsuk-Ulam theorem for flag manifolds, which we believe to be of
independent interest.
</p></div>
    </summary>
    <updated>2019-10-30T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13297</id>
    <link href="http://arxiv.org/abs/1910.13297" rel="alternate" type="text/html"/>
    <title>Flexible Graph Connectivity: Approximating Network Design Problems Between 1- and 2-connectivity</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adjiashvili:David.html">David Adjiashvili</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hommelsheim:Felix.html">Felix Hommelsheim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/M=uuml=hlenthaler:Moritz.html">Moritz Mühlenthaler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13297">PDF</a><br/><b>Abstract: </b>Graph connectivity and network design problems are among the most fundamental
problems in combinatorial optimization. The minimum spanning tree problem, the
two edge-connected spanning subgraph problem (2-ECSS) and the tree augmentation
problem (TAP) are all examples of fundamental well-studied network design tasks
that postulate different initial states of the network and different
assumptions on the reliability of network components. In this paper we motivate
and study \emph{Flexible Graph Connectivity} (FGC), a problem that mixes
together both the modeling power and the complexities of all aforementioned
problems and more. In a nutshell, FGC asks to design a connected network, while
allowing to specify different reliability levels for individual edges. While
this non-uniform nature of the problem makes it appealing from the modeling
perspective, it also renders most existing algorithmic tools for dealing with
network design problems unfit for approximating FGC.
</p>
<p>In this paper we develop a general algorithmic approach for approximating FGC
that yields approximation algorithms with ratios that are very close to the
best known bounds for many special cases, such as 2-ECSS and TAP. Our algorithm
and analysis combine various techniques including a weight-scaling algorithm, a
charging argument that uses a variant of exchange bijections between spanning
trees and a factor revealing non-linear optimization problem.
</p></div>
    </summary>
    <updated>2019-10-30T01:20:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13292</id>
    <link href="http://arxiv.org/abs/1910.13292" rel="alternate" type="text/html"/>
    <title>Real-time Bidding campaigns optimization using attribute selection</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miralles:Luis.html">Luis Miralles</a>, M. Atif Qureshi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Namee:Brian_Mac.html">Brian Mac Namee</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13292">PDF</a><br/><b>Abstract: </b>Real-Time Bidding is nowadays one of the most promising systems in the online
advertising ecosystem. In the presented study, the performance of RTB campaigns
is improved by optimising the parameters of the users' profiles and the
publishers' websites. Most studies about optimising RTB campaigns are focused
on the bidding strategy. In contrast, the objective of our research consists of
optimising RTB campaigns by finding out configurations that maximise both the
number of impressions and their average profitability. The experiments
demonstrate that, when the number of required visits by advertisers is low, it
is easy to find configurations with high average profitability, but as the
required number of visits increases, the average profitability tends to go
down. Additionally, configuration optimisation has been combined with other
interesting strategies to increase, even more, the campaigns' profitability.
Along with parameter configuration the study considers the following
complementary strategies to increase profitability: i) selecting multiple
configurations with a small number of visits instead of a unique configuration
with a large number, ii) discarding visits according to the thresholds of cost
and profitability, iii) analysing a reduced space of the dataset and
extrapolating the solution, and iv) increasing the search space by including
solutions below the required number of visits. The developed campaign
optimisation methodology could be offered by RTB platforms to advertisers to
make their campaigns more profitable.
</p></div>
    </summary>
    <updated>2019-10-30T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13123</id>
    <link href="http://arxiv.org/abs/1910.13123" rel="alternate" type="text/html"/>
    <title>Reconstruction of time-consistent species trees</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lafond:Manuel.html">Manuel Lafond</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hellmuth:Marc.html">Marc Hellmuth</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13123">PDF</a><br/><b>Abstract: </b>The history of gene families -- which are equivalent to event-labeled gene
trees -- can to some extent be reconstructed from empirically estimated
evolutionary event-relations containing pairs of orthologous, paralogous or
xenologous genes. The question then arises as whether inferred event-labeled
gene trees are "biologically feasible" which is the case if one can find a
species tree with which the gene tree can be reconciled in a time-consistent
way.
</p>
<p>In this contribution, we consider event-labeled gene trees that contain
speciation, duplication as well as horizontal gene transfer and we assume that
the species tree is unknown. We provide a cubic-time algorithm to decide
whether a "time-consistent" binary species for a given event-labeled gene tree
exists and, in the affirmative case, to construct the species tree within the
same time-complexity.
</p></div>
    </summary>
    <updated>2019-10-30T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13011</id>
    <link href="http://arxiv.org/abs/1910.13011" rel="alternate" type="text/html"/>
    <title>A Survey on Subgraph Counting: Concepts, Algorithms and Applications to Network Motifs and Graphlets</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ribeiro:Pedro.html">Pedro Ribeiro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paredes:Pedro.html">Pedro Paredes</a>, Miguel E. P. Silva, David Aparicio, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Fernando.html">Fernando Silva</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13011">PDF</a><br/><b>Abstract: </b>Computing subgraph frequencies is a fundamental task that lies at the core of
several network analysis methodologies, such as network motifs and
graphlet-based metrics, which have been widely used to categorize and compare
networks from multiple domains. Counting subgraphs is however computationally
very expensive and there has been a large body of work on efficient algorithms
and strategies to make subgraph counting feasible for larger subgraphs and
networks.
</p>
<p>This survey aims precisely to provide a comprehensive overview of the
existing methods for subgraph counting. Our main contribution is a general and
structured review of existing algorithms, classifying them on a set of key
characteristics, highlighting their main similarities and differences. We
identify and describe the main conceptual approaches, giving insight on their
advantages and limitations, and provide pointers to existing implementations.
We initially focus on exact sequential algorithms, but we also do a thorough
survey on approximate methodologies (with a trade-off between accuracy and
execution time) and parallel strategies (that need to deal with an unbalanced
search space).
</p></div>
    </summary>
    <updated>2019-10-30T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1903.02185</id>
    <link href="http://arxiv.org/abs/1903.02185" rel="alternate" type="text/html"/>
    <title>Stable Noncrossing Matchings</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ruangwises:Suthee.html">Suthee Ruangwises</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Itoh:Toshiya.html">Toshiya Itoh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1903.02185">PDF</a><br/><b>Abstract: </b>Given a set of $n$ men represented by $n$ points lying on a line, and $n$
women represented by $n$ points lying on another parallel line, with each
person having a list that ranks some people of opposite gender as his/her
acceptable partners in strict order of preference. In this problem, we want to
match people of opposite genders to satisfy people's preferences as well as
making the edges not crossing one another geometrically. A noncrossing blocking
pair w.r.t. a matching $M$ is a pair $(m,w)$ of a man and a woman such that
they are not matched with each other but prefer each other to their own
partners in $M$, and the segment $(m,w)$ does not cross any edge in $M$. A
weakly stable noncrossing matching (WSNM) is a noncrossing matching that does
not admit any noncrossing blocking pair. In this paper, we prove the existence
of a WSNM in any instance by developing an $O(n^2)$ algorithm to find one in a
given instance.
</p></div>
    </summary>
    <updated>2019-10-30T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/144</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/144" rel="alternate" type="text/html"/>
    <title>TR19-144 |  An Adaptive Step Toward the Multiphase Conjecture | 

	Omri Weinstein, 

	Young Ko</title>
    <summary>In 2010, Patrascu proposed a dynamic set-disjointness problem, known as the Multiphase problem, as a candidate for proving $polynomial$ lower bounds on the operational time of dynamic data structures. Patrascu conjectured that any data structure for the Multiphase problem must make $n^\epsilon$ cell-probes in either the update or query phase, and showed that this would imply similar $unconditional$ lower bounds on many important dynamic data structure problems. Alas, there has been almost no progress on this conjecture in the past decade since its introduction. 

We show an $\tilde{\Omega}(\sqrt{n})$ cell-probe lower bound on the Multiphase problem for data structures with general (adaptive) updates, and queries with unbounded but "layered" adaptivity. This result captures all known set-intersection data structures  and significantly strengthens previous Multiphase lower bounds, which only captured non-adaptive data structures.
Our main technical result is a communication lower bound on a 4-party variant of Patrascu's Number-On-Forehead  Multiphase game, using information complexity techniques.  We also show that a lower bound on Patrascu's original NOF game would imply a polynomial ($n^{1+\epsilon}$) lower bound on the number of wires of any constant-depth circuit with $arbitrary$ gates computing a random $\tilde{O}(n)\times n$ $linear$ operator $x \mapsto Ax$, a long-standing open problem in circuit complexity. This suggests that the NOF conjecture is much stronger than its data structure counterpart.</summary>
    <updated>2019-10-29T22:20:01Z</updated>
    <published>2019-10-29T22:20:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-30T10:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/29/multiple-postdoctoral-fellowships-in-quantum-information-at-cu-boulder-center-for-theory-of-quantum-matter-apply-by-december-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/29/multiple-postdoctoral-fellowships-in-quantum-information-at-cu-boulder-center-for-theory-of-quantum-matter-apply-by-december-1-2019/" rel="alternate" type="text/html"/>
    <title>Multiple postdoctoral fellowships in quantum information at CU Boulder Center for Theory of Quantum Matter (apply by December 1, 2019)</title>
    <summary>Successful candidates will interact with CTQM faculty (O. DeWolfe, V. Gurarie, M. Hermele, M. Holland, E. Knill, A. Lucas, R. Nandkishore, E. Neil, L. Radzihovsky, A. M. Rey, P. Romatschke, G. Smith) and can also work with faculty throughout the Boulder Physics Department &amp; JILA. Applications esp. encouraged from candidates whose interests and/or expertise span […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Successful candidates will interact with CTQM faculty (O. DeWolfe, V. Gurarie, M. Hermele, M. Holland, E. Knill, A. Lucas, R. Nandkishore, E. Neil, L. Radzihovsky, A. M. Rey, P. Romatschke, G. Smith) and can also work with faculty throughout the Boulder Physics Department &amp; JILA. Applications esp. encouraged from candidates whose interests and/or expertise span traditionally distinct subfields.</p>
<p>Website: <a href="https://jobs.colorado.edu/jobs/JobDetail/?jobId=21755">https://jobs.colorado.edu/jobs/JobDetail/?jobId=21755</a><br/>
Email: Graeme.Smith@colorado.edu</p></div>
    </content>
    <updated>2019-10-29T19:30:15Z</updated>
    <published>2019-10-29T19:30:15Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-30T10:20:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5464</id>
    <link href="https://adamsheffer.wordpress.com/2019/10/29/incidences-open-problems-part-2/" rel="alternate" type="text/html"/>
    <title>Incidences: Open Problems (part 2)</title>
    <summary>We now continue our journey of seeing how we still don’t know much about geometric incidences. So far, we looked at two main problems concerning incidences with curves in the plane (see the first post of the series). It might make sense to move to study incidences in higher dimensions. Instead, we are now regressing […]</summary>
    <updated>2019-10-29T18:59:15Z</updated>
    <published>2019-10-29T18:59:15Z</published>
    <category term="Incidences"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2019-10-30T10:21:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12526</id>
    <link href="http://arxiv.org/abs/1910.12526" rel="alternate" type="text/html"/>
    <title>A* with Perfect Potentials</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Strasser:Ben.html">Ben Strasser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zeitz:Tim.html">Tim Zeitz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12526">PDF</a><br/><b>Abstract: </b>Quickly determining shortest paths in networks is an important ingredient for
many routing applications. While Dijkstra's algorithm can be used to solve
these problems, it is too slow for many practical problems. A* is an extension
to Dijkstra's algorithm. It uses a potential function to estimate the distance
of each node to the target. By adding these estimates to the queue keys, the
search is directed towards the target. The quality of this potential determines
the performance of A*. We introduce a novel way to efficiently calculate
perfect potentials for extended problem settings where a lower bound graph is
available. For example, in the case of routing with live traffic, this could be
the free flow graph.
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12504</id>
    <link href="http://arxiv.org/abs/1910.12504" rel="alternate" type="text/html"/>
    <title>The Multi-level Bottleneck Assignment Problem: Complexity and Solution Methods</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dokka:Trivikram.html">Trivikram Dokka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goerigk:Marc.html">Marc Goerigk</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12504">PDF</a><br/><b>Abstract: </b>We study the multi-level bottleneck assignment problem (MBA), which has
important applications in scheduling and quantitative finance. Given a weight
matrix, the task is to rearrange entries in each column such that the maximum
sum of values in each row is as small as possible. We analyze the complexity of
this problem in a generalized setting, where there are restrictions in how
values in columns can be permuted. We present a lower bound on its
approximability by giving a non-trivial gap reduction from three-dimensional
matching to MBA.
</p>
<p>To solve MBA, a greedy method has been used in the literature. We present new
solution methods based on an extension of the greedy method, an integer
programming formulation, and a column generation heuristic. In computational
experiments we show that it is possible to outperform the standard greedy
approach by around 10% on random instances.
</p></div>
    </summary>
    <updated>2019-10-29T23:20:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12490</id>
    <link href="http://arxiv.org/abs/1910.12490" rel="alternate" type="text/html"/>
    <title>Same-Cluster Querying for Overlapping Clusters</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huleihel:Wasim.html">Wasim Huleihel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mazumdar:Arya.html">Arya Mazumdar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/M=eacute=dard:Muriel.html">Muriel Médard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pal:Soumyabrata.html">Soumyabrata Pal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12490">PDF</a><br/><b>Abstract: </b>Overlapping clusters are common in models of many practical data-segmentation
applications. Suppose we are given $n$ elements to be clustered into $k$
possibly overlapping clusters, and an oracle that can interactively answer
queries of the form "do elements $u$ and $v$ belong to the same cluster?" The
goal is to recover the clusters with minimum number of such queries. This
problem has been of recent interest for the case of disjoint clusters. In this
paper, we look at the more practical scenario of overlapping clusters, and
provide upper bounds (with algorithms) on the sufficient number of queries. We
provide algorithmic results under both arbitrary (worst-case) and statistical
modeling assumptions. Our algorithms are parameter free, efficient, and work in
the presence of random noise. We also derive information-theoretic lower bounds
on the number of queries needed, proving that our algorithms are order optimal.
Finally, we test our algorithms over both synthetic and real-world data,
showing their practicality and effectiveness.
</p></div>
    </summary>
    <updated>2019-10-29T23:34:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12458</id>
    <link href="http://arxiv.org/abs/1910.12458" rel="alternate" type="text/html"/>
    <title>On the Degree of Boolean Functions as Polynomials over $\mathbb{Z}_m$</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Xiaoming.html">Xiaoming Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Yuan.html">Yuan Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Jiaheng.html">Jiaheng Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Kewen.html">Kewen Wu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xia:Zhiyu.html">Zhiyu Xia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zheng:Yufan.html">Yufan Zheng</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12458">PDF</a><br/><b>Abstract: </b>Polynomial representations of Boolean functions over various rings such as
$\mathbb{Z}$ and $\mathbb{Z}_m$ have been studied since Minsky and Papert
(1969). From then on, they have been employed in a large variety of fields
including communication complexity, circuit complexity, learning theory, coding
theory and so on. For any integer $m\ge2$, each Boolean function has a unique
multilinear polynomial representation over ring $\mathbb Z_m$. The degree of
such polynomial is called modulo-$m$ degree, denoted as
$\mathrm{deg}_m(\cdot)$.
</p>
<p>In this paper, we discuss the lower bound of modulo-$m$ degree of Boolean
functions. When $m=p^k$ ($k\ge 1$) for some prime $p$, we give a tight lower
bound that $\mathrm{deg}_m(f)\geq k(p-1)$ for any non-degenerated function
$f:\{0,1\}^n\to\{0,1\}$, provided that $n$ is sufficient large. When $m$
contains two different prime factors $p$ and $q$, we give a nearly optimal
lower bound for any symmetric function $f:\{0,1\}^n\to\{0,1\}$ that
$\mathrm{deg}_m(f) \geq \frac{n}{2+\frac{1}{p-1}+\frac{1}{q-1}}$.
</p>
<p>The idea of the proof is as follows. First we investigate properties of
polynomial representation of $\mathsf{MOD}$ function, then use it to span
symmetric Boolean functions to prove the lower bound for symmetric functions,
when $m$ is a prime power. Afterwards, Ramsey Theory is applied in order to
extend the bound from symmetric functions to non-degenerated ones. Finally, by
showing that $\mathrm{deg}_p(f)$ and $\mathrm{deg}_q(f)$ cannot be small
simultaneously, the lower bound for symmetric functions can be obtained when
$m$ is a composite but not prime power.
</p></div>
    </summary>
    <updated>2019-10-29T23:23:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12414</id>
    <link href="http://arxiv.org/abs/1910.12414" rel="alternate" type="text/html"/>
    <title>Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Lin.html">Lin Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esfandiari:Hossein.html">Hossein Esfandiari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fu:Thomas.html">Thomas Fu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mirrokni:Vahab_S=.html">Vahab S. Mirrokni</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12414">PDF</a><br/><b>Abstract: </b>Computing approximate nearest neighbors in high dimensional spaces is a
central problem in large-scale data mining with a wide range of applications in
machine learning and data science. A popular and effective technique in
computing nearest neighbors approximately is the locality-sensitive hashing
(LSH) scheme. In this paper, we aim to develop LSH schemes for distance
functions that measure the distance between two probability distributions,
particularly for f-divergences as well as a generalization to capture mutual
information loss. First, we provide a general framework to design LHS schemes
for f-divergence distance functions and develop LSH schemes for the generalized
Jensen-Shannon divergence and triangular discrimination in this framework. We
show a two-sided approximation result for approximation of the generalized
Jensen-Shannon divergence by the Hellinger distance, which may be of
independent interest. Next, we show a general method of reducing the problem of
designing an LSH scheme for a Krein kernel (which can be expressed as the
difference of two positive definite kernels) to the problem of maximum inner
product search. We exemplify this method by applying it to the mutual
information loss, due to its several important applications such as model
compression.
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12375</id>
    <link href="http://arxiv.org/abs/1910.12375" rel="alternate" type="text/html"/>
    <title>Towards a theory of non-commutative optimization: geodesic first and second order methods for moment maps and polytopes</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=uuml=rgisser:Peter.html">Peter Bürgisser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Franks:Cole.html">Cole Franks</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Ankit.html">Ankit Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oliveira:Rafael.html">Rafael Oliveira</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Walter:Michael.html">Michael Walter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wigderson:Avi.html">Avi Wigderson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12375">PDF</a><br/><b>Abstract: </b>This paper initiates a systematic development of a theory of non-commutative
optimization. It aims to unify and generalize a growing body of work from the
past few years which developed and analyzed algorithms for natural geodesically
convex optimization problems on Riemannian manifolds that arise from the
symmetries of non-commutative groups. These algorithms minimize the moment map
(a non-commutative notion of the usual gradient) and test membership in moment
polytopes (a vast class of polytopes, typically of exponential vertex and facet
complexity, which arise from this a-priori non-convex, non-linear setting).
This setting captures a diverse set of problems in different areas of computer
science, mathematics, and physics. Several of them were solved efficiently for
the first time using non-commutative methods; the corresponding algorithms also
lead to solutions of purely structural problems and to many new connections
between disparate fields.
</p>
<p>In the spirit of standard convex optimization, we develop two general methods
in the geodesic setting, a first order and a second order method, which
respectively receive first and second order information on the "derivatives" of
the function to be optimized. These in particular subsume all past results. The
main technical work, again unifying and extending much of the previous
literature, goes into identifying the key parameters of the underlying group
actions which control convergence to the optimum in each of these methods.
These non-commutative analogues of "smoothness" are far more complex and
require significant algebraic and analytic machinery. Despite this complexity,
the way in which these parameters control convergence in both methods is quite
simple and elegant. We show how bound these parameters in several general
cases. Our work points to intriguing open problems and suggests further
research directions.
</p></div>
    </summary>
    <updated>2019-10-29T23:25:14Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12359</id>
    <link href="http://arxiv.org/abs/1910.12359" rel="alternate" type="text/html"/>
    <title>Chatter Diagnosis in Milling Using Supervised Learning and Topological Features Vector</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yesilli:Melih_C=.html">Melih C. Yesilli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tymochko:Sarah.html">Sarah Tymochko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khasawneh:Firas_A=.html">Firas A. Khasawneh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munch:Elizabeth.html">Elizabeth Munch</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12359">PDF</a><br/><b>Abstract: </b>Chatter detection has become a prominent subject of interest due to its
effect on cutting tool life, surface finish and spindle of machine tool. Most
of the existing methods in chatter detection literature are based on signal
processing and signal decomposition. In this study, we use topological features
of data simulating cutting tool vibrations, combined with four supervised
machine learning algorithms to diagnose chatter in the milling process.
Persistence diagrams, a method of representing topological features, are not
easily used in the context of machine learning, so they must be transformed
into a form that is more amenable. Specifically, we will focus on two different
methods for featurizing persistence diagrams, Carlsson coordinates and template
functions. In this paper, we provide classification results for simulated data
from various cutting configurations, including upmilling and downmilling, in
addition to the same data with some added noise. Our results show that Carlsson
Coordinates and Template Functions yield accuracies as high as 96% and 95%,
respectively. We also provide evidence that these topological methods are noise
robust descriptors for chatter detection.
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12353</id>
    <link href="http://arxiv.org/abs/1910.12353" rel="alternate" type="text/html"/>
    <title>On the Parameterized Complexity of Sparsest Cut and Small-set Expansion Problems</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Javadi:Ramin.html">Ramin Javadi</a>, Amir Nikabadi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12353">PDF</a><br/><b>Abstract: </b>We study the NP-hard \textsc{$k$-Sparsest Cut} problem ($k$SC) in which,
given an undirected graph $G = (V, E)$ and a parameter $k$, the objective is to
partition vertex set into $k$ subsets whose maximum edge expansion is
minimized. Herein, the edge expansion of a subset $S \subseteq V$ is defined as
the sum of the weights of edges exiting $S$ divided by the number of vertices
in $S$. Another problem that has been investigated is \textsc{$k$-Small-Set
Expansion} problem ($k$SSE), which aims to find a subset with minimum edge
expansion with a restriction on the size of the subset. We extend previous
studies on $k$SC and $k$SSE by inspecting their parameterized complexity. On
the positive side, we present two FPT algorithms for both $k$SSE and 2SC
problems where in the first algorithm we consider the parameter treewidth of
the input graph and uses exponential space, and in the second we consider the
parameter vertex cover number of the input graph and uses polynomial space.
Moreover, we consider the unweighted version of the $k$SC problem where $k \geq
2$ is fixed and proposed two FPT algorithms with parameters treewidth and
vertex cover number of the input graph. We also propose a randomized FPT
algorithm for $k$SSE when parameterized by $k$ and the maximum degree of the
input graph combined. Its derandomization is done efficiently.
</p>
<p>\noindent On the negative side, first we prove that for every fixed integer
$k,\tau\geq 3$, the problem $k$SC is NP-hard for graphs with vertex cover
number at most $\tau$. We also show that $k$SC is W[1]-hard when parameterized
by the treewidth of the input graph and the number~$k$ of components combined
using a reduction from \textsc{Unary Bin Packing}. Furthermore, we prove that
$k$SC remains NP-hard for graphs with maximum degree three and also graphs with
degeneracy two. Finally, we prove that the unweighted $k$SSE is W[1]-hard for
the parameter $k$.
</p></div>
    </summary>
    <updated>2019-10-29T23:21:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12340</id>
    <link href="http://arxiv.org/abs/1910.12340" rel="alternate" type="text/html"/>
    <title>Cilkmem: Algorithms for Analyzing the Memory High-Water Mark of Fork-Join Parallel Programs</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaler:Tim.html">Tim Kaler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuszmaul:William.html">William Kuszmaul</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schardl:Tao_B=.html">Tao B. Schardl</a>, Daniele Vettorel <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12340">PDF</a><br/><b>Abstract: </b>Software engineers designing recursive fork-join programs destined to run on
massively parallel computing systems must be cognizant of how their program's
memory requirements scale in a many-processor execution. Although tools exist
for measuring memory usage during one particular execution of a parallel
program, such tools cannot bound the worst-case memory usage over all possible
parallel executions.
</p>
<p>This paper introduces Cilkmem, a tool that analyzes the execution of a
deterministic Cilk program to determine its $p$-processor memory high-water
mark (MHWM), which is the worst-case memory usage of the program over \emph{all
possible} $p$-processor executions. Cilkmem employs two new algorithms for
computing the $p$-processor MHWM. The first algorithm calculates the exact
$p$-processor MHWM in $O(T_1 \cdot p)$ time, where $T_1$ is the total work of
the program. The second algorithm solves, in $O(T_1)$ time, the approximate
threshold problem, which asks, for a given memory threshold $M$, whether the
$p$-processor MHWM exceeds $M/2$ or whether it is guaranteed to be less than
$M$. Both algorithms are memory efficient, requiring $O(p \cdot D)$ and $O(D)$
space, respectively, where $D$ is the maximum call-stack depth of the program's
execution on a single thread.
</p>
<p>Our empirical studies show that Cilkmem generally exhibits low overheads.
Across ten application benchmarks from the Cilkbench suite, the exact algorithm
incurs a geometric-mean multiplicative overhead of $1.54$ for $p=128$, whereas
the approximation-threshold algorithm incurs an overhead of $1.36$ independent
of $p$. In addition, we use Cilkmem to reveal and diagnose a previously unknown
issue in a large image-alignment program contributing to unexpectedly high
memory usage under parallel executions.
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12310</id>
    <link href="http://arxiv.org/abs/1910.12310" rel="alternate" type="text/html"/>
    <title>Semi-Asymmetric Parallel Graph Algorithms for NVRAMs</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dhulipala:Laxman.html">Laxman Dhulipala</a>, Charlie McGuffey, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kang:Hongbo.html">Hongbo Kang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gu:Yan.html">Yan Gu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blelloch:Guy_E=.html">Guy E. Blelloch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gibbons:Phillip_B=.html">Phillip B. Gibbons</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shun:Julian.html">Julian Shun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12310">PDF</a><br/><b>Abstract: </b>Emerging non-volatile main memory (NVRAM) technologies provide novel features
for large-scale graph analytics, combining byte-addressability, low idle power,
and improved memory-density. Systems are likely to have an order of magnitude
more NVRAM than traditional memory (DRAM), allowing large graph problems to be
solved efficiently at a modest cost on a single machine. However, a significant
challenge in achieving high performance is in accounting for the fact that
NVRAM writes can be significantly more expensive than NVRAM reads.
</p>
<p>In this paper, we propose an approach to parallel graph analytics in which
the graph is stored as a read-only data structure (in NVRAM), and the amount of
mutable memory is kept proportional to the number of vertices. Similar to the
popular semi-external and semi-streaming models for graph analytics, the
approach assumes that the vertices of the graph fit in a fast read-write memory
(DRAM), but the edges do not. In NVRAM systems, our approach eliminates writes
to the NVRAM, among other benefits.
</p>
<p>We present a model, the Parallel Semi-Asymmetric Model (PSAM), to analyze
algorithms in the setting, and run experiments on a 48-core NVRAM system to
validate the effectiveness of these algorithms. To this end, we study over a
dozen graph problems. We develop parallel algorithms for each that are
efficient, often work-optimal, in the model. Experimentally, we run all of the
algorithms on the largest publicly-available graph and show that our PSAM
algorithms outperform the fastest prior algorithms designed for DRAM or NVRAM.
We also show that our algorithms running on NVRAM nearly match the fastest
prior algorithms running solely in DRAM, by effectively hiding the costs of
repeatedly accessing NVRAM versus DRAM.
</p></div>
    </summary>
    <updated>2019-10-29T23:29:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12177</id>
    <link href="http://arxiv.org/abs/1910.12177" rel="alternate" type="text/html"/>
    <title>Computing a Geodesic Two-Center of Points in a Simple Polygon</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oh:Eunjin.html">Eunjin Oh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bae:Sang_Won.html">Sang Won Bae</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahn:Hee=Kap.html">Hee-Kap Ahn</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12177">PDF</a><br/><b>Abstract: </b>Given a simple polygon $P$ and a set $Q$ of points contained in $P$, we
consider the geodesic $k$-center problem where we want to find $k$ points,
called \emph{centers}, in $P$ to minimize the maximum geodesic distance of any
point of $Q$ to its closest center. In this paper, we focus on the case for
$k=2$ and present the first exact algorithm that efficiently computes an
optimal $2$-center of $Q$ with respect to the geodesic distance in $P$.
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12172</id>
    <link href="http://arxiv.org/abs/1910.12172" rel="alternate" type="text/html"/>
    <title>Near-Optimal Bounds for Online Caching with Machine Learned Advice</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohatgi:Dhruv.html">Dhruv Rohatgi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12172">PDF</a><br/><b>Abstract: </b>In the model of online caching with machine learned advice, introduced by
Lykouris and Vassilvitskii, the goal is to solve the caching problem with an
online algorithm that has access to next-arrival predictions: when each input
element arrives, the algorithm is given a prediction of the next time when the
element will reappear. The traditional model for online caching suffers from an
$\Omega(\log k)$ competitive ratio lower bound (on a cache of size $k$). In
contrast, the augmented model admits algorithms which beat this lower bound
when the predictions have low error, and asymptotically match the lower bound
when the predictions have high error, even if the algorithms are oblivious to
the prediction error. In particular, Lykouris and Vassilvitskii showed that
there is a prediction-augmented caching algorithm with a competitive ratio of
$O(1+\min(\sqrt{\eta/OPT}, \log k))$ when the overall $\ell_1$ prediction error
is bounded by $\eta$, and $OPT$ is the cost of the optimal offline algorithm.
</p>
<p>The dependence on $k$ in the competitive ratio is optimal, but the dependence
on $\eta/OPT$ may be far from optimal. In this work, we make progress towards
closing this gap. Our contributions are twofold. First, we provide an improved
algorithm with a competitive ratio of $O(1 + \min((\eta/OPT)/k, 1) \log k)$.
Second, we provide a lower bound of $\Omega(\log \min((\eta/OPT)/(k \log k),
k))$.
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12169</id>
    <link href="http://arxiv.org/abs/1910.12169" rel="alternate" type="text/html"/>
    <title>Computing the Center Region and Its Variants</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oh:Eunjin.html">Eunjin Oh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahn:Hee=Kap.html">Hee-Kap Ahn</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12169">PDF</a><br/><b>Abstract: </b>We present an $O(n^2\log^4 n)$-time algorithm for computing the center region
of a set of $n$ points in the three-dimensional Euclidean space. This improves
the previously best known algorithm by Agarwal, Sharir and Welzl, which takes
$O(n^{2+\epsilon})$ time for any $\epsilon &gt; 0$. It is known that the
combinatorial complexity of the center region is $\Omega(n^2)$ in the worst
case, thus our algorithm is almost tight. We also consider the problem of
computing a colored version of the center region in the two-dimensional
Euclidean space and present an $O(n\log^4 n)$-time algorithm.
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12050</id>
    <link href="http://arxiv.org/abs/1910.12050" rel="alternate" type="text/html"/>
    <title>Facility Location Problem in Differential Privacy Model Revisited</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Yunus Esencayi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gaboardi:Marco.html">Marco Gaboardi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Shi.html">Shi Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Di.html">Di Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12050">PDF</a><br/><b>Abstract: </b>In this paper we study the uncapacitated facility location problem in the
model of differential privacy (DP) with uniform facility cost. Specifically, we
first show that, under the hierarchically well-separated tree (HST) metrics and
the super-set output setting that was introduced in Gupta et. al., there is an
$\epsilon$-DP algorithm that achieves an $O(\frac{1}{\epsilon})$(expected
multiplicative) approximation ratio; this implies an $O(\frac{\log
n}{\epsilon})$ approximation ratio for the general metric case, where $n$ is
the size of the input metric. These bounds improve the best-known results given
by Gupta et. al. In particular, our approximation ratio for HST-metrics is
independent of $n$, and the ratio for general metrics is independent of the
aspect ratio of the input metric. On the negative side, we show that the
approximation ratio of any $\epsilon$-DP algorithm is lower bounded by
$\Omega(\frac{1}{\sqrt{\epsilon}})$, even for instances on HST metrics with
uniform facility cost, under the super-set output setting. The lower bound
shows that the dependence of the approximation ratio for HST metrics on
$\epsilon$ can not be removed or greatly improved. Our novel methods and
techniques for both the upper and lower bound may find additional applications.
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12026</id>
    <link href="http://arxiv.org/abs/1910.12026" rel="alternate" type="text/html"/>
    <title>On the Hardness of Energy Minimisation for Crystal Structure Prediction</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Duncan Adamson, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deligkas:Argyrios.html">Argyrios Deligkas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gusev:Vladimir.html">Vladimir Gusev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potapov:Igor.html">Igor Potapov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12026">PDF</a><br/><b>Abstract: </b>Crystal Structure Prediction (csp) is one of the central and most challenging
problems in materials science and computational chemistry. In csp, the goal is
to find a configuration of ions in 3D space that yields the lowest potential
energy. Finding an efficient procedure to solve this complex optimisation
question is a well known open problem in computational chemistry. Due to the
exponentially large search space, the problem has been referred in several
materials-science papers as ''NP-Hard and very challenging'' without any formal
proof though. This paper fills a gap in the literature providing the first set
of formally proven NP-Hardness results for a variant of csp with various
realistic constraints. In particular, we focus on the problem of removal: the
goal is to find a substructure with minimal potential energy, by removing a
subset of the ions from a given initial structure. Our main contributions are
NP-Hardness results for the csp removal problem, new embeddings of
combinatorial graph problems into geometrical settings, and a more systematic
exploration of the energy function to reveal the complexity of csp. In a wider
context, our results contribute to the analysis of computational problems for
weighted graphs embedded into the three-dimensional Euclidean space.
</p></div>
    </summary>
    <updated>2019-10-29T23:22:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11993</id>
    <link href="http://arxiv.org/abs/1910.11993" rel="alternate" type="text/html"/>
    <title>Selection on $X_1+X_2+\cdots + X_m$ with layer-ordered heaps</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kreitzberg:Patrick.html">Patrick Kreitzberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lucke:Kyle.html">Kyle Lucke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Serang:Oliver.html">Oliver Serang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11993">PDF</a><br/><b>Abstract: </b>Selection on $X_1+X_2+\cdots + X_m$ is an important problem with many
applications in areas such as max-convolution, max-product Bayesian inference,
calculating most probable isotopes, and computing non-parametric test
statistics, among others. Faster-than-na\"{i}ve approaches exist for $m=2$:
Johnson \&amp; Mizoguchi (1978) find the smallest $k$ values in $A+B$ with runtime
$O(n \log(n))$. Frederickson \&amp; Johnson (1982) created a method for finding the
$k$ smallest values in $A+B$ with runtime $O(n +
\min(k,n)\log(\frac{k}{\min(k,n)}))$. In 1993, Frederickson published an
optimal algorithm for selection on $A+B$, which runs in $O(n+k)$. In 2018,
Kaplan \emph{et al.} described another optimal algorithm in terms Chazelle's of
soft heaps. No fast methods exist for $m&gt;2$. Johnson \&amp; Mizoguchi (1978)
introduced a method to compute the minimal $k$ terms when $m&gt;2$, but that
method runs in $O(m\cdot n^{\frac{m}{2}} \log(n))$ and is inefficient when $m
\gg 1$.
</p>
<p>In this paper, we introduce the first efficient methods for problems where
$m&gt;2$. We introduce the ``layer-ordered heap,'' a simple special class of heap
with which we produce a new, fast selection algorithm on the Cartesian product.
Using this new algorithm to perform $k$-selection on the Cartesian product of
$m$ arrays of length $n$ has runtime $\in o(m\cdot n + k\cdot m)$. We also
provide implementations of the algorithms proposed and their performance in
practice.
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11957</id>
    <link href="http://arxiv.org/abs/1910.11957" rel="alternate" type="text/html"/>
    <title>A Deterministic Linear Program Solver in Current Matrix Multiplication Time</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brand:Jan_van_den.html">Jan van den Brand</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11957">PDF</a><br/><b>Abstract: </b>Interior point algorithms for solving linear programs have been studied
extensively for a long time [e.g. Karmarkar 1984; Lee, Sidford FOCS'14; Cohen,
Lee, Song STOC'19]. For linear programs of the form $\min_{Ax=b, x \ge 0}
c^\top x$ with $n$ variables and $d$ constraints, the generic case $d =
\Omega(n)$ has recently been settled by Cohen, Lee and Song [STOC'19]. Their
algorithm can solve linear programs in $\tilde O(n^\omega \log(n/\delta))$
expected time, where $\delta$ is the relative accuracy. This is essentially
optimal as all known linear system solvers require up to $O(n^{\omega})$ time
for solving $Ax = b$. However, for the case of deterministic solvers, the best
upper bound is Vaidya's 30 years old $O(n^{2.5} \log(n/\delta))$ bound
[FOCS'89]. In this paper we show that one can also settle the deterministic
setting by derandomizing Cohen et al.'s $\tilde{O}(n^\omega \log(n/\delta))$
time algorithm. This allows for a strict $\tilde{O}(n^\omega \log(n/\delta))$
time bound, instead of an expected one, and a simplified analysis, reducing the
length of their proof of their central path method by roughly half.
Derandomizing this algorithm was also an open question asked in Song's PhD
Thesis.
</p>
<p>The main tool to achieve our result is a new data-structure that can maintain
the solution to a linear system in subquadratic time. More accurately we are
able to maintain $\sqrt{U}A^\top(AUA^\top)^{-1}A\sqrt{U}\:v$ in subquadratic
time under $\ell_2$ multiplicative changes to the diagonal matrix $U$ and the
vector $v$. This type of change is common for interior point algorithms.
Previous algorithms [e.g. Vaidya STOC'89; Lee, Sidford FOCS'15; Cohen, Lee,
Song STOC'19] required $\Omega(n^2)$ time for this task. [...]
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11921</id>
    <link href="http://arxiv.org/abs/1910.11921" rel="alternate" type="text/html"/>
    <title>Equivalence of Systematic Linear Data Structures and Matrix Rigidity</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramamoorthy:Sivaramakrishnan_Natarajan.html">Sivaramakrishnan Natarajan Ramamoorthy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rashtchian:Cyrus.html">Cyrus Rashtchian</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11921">PDF</a><br/><b>Abstract: </b>Recently, Dvir, Golovnev, and Weinstein have shown that sufficiently strong
lower bounds for linear data structures would imply new bounds for rigid
matrices. However, their result utilizes an algorithm that requires an $NP$
oracle, and hence, the rigid matrices are not explicit. In this work, we derive
an equivalence between rigidity and the systematic linear model of data
structures. For the $n$-dimensional inner product problem with $m$ queries, we
prove that lower bounds on the query time imply rigidity lower bounds for the
query set itself. In particular, an explicit lower bound of
$\omega\left(\frac{n}{r}\log m\right)$ for $r$ redundant storage bits would
yield better rigidity parameters than the best bounds due to Alon, Panigrahy,
and Yekhanin. We also prove a converse result, showing that rigid matrices
directly correspond to hard query sets for the systematic linear model. As an
application, we prove that the set of vectors obtained from rank one binary
matrices is rigid with parameters matching the known results for explicit sets.
This implies that the vector-matrix-vector problem requires query time
$\Omega(n^{3/2}/r)$ for redundancy $r \geq \sqrt{n}$ in the systematic linear
model, improving a result of Chakraborty, Kamma, and Larsen. Finally, we prove
a cell probe lower bound for the vector-matrix-vector problem in the high error
regime, improving a result of Chattopadhyay, Kouck\'{y}, Loff, and
Mukhopadhyay.
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3222567344389850699</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3222567344389850699/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/09/random-non-partisan-thoughts-on-prez.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3222567344389850699" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3222567344389850699" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/09/random-non-partisan-thoughts-on-prez.html" rel="alternate" type="text/html"/>
    <title>Random non-partisan thoughts on the Prez Election</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
This post is non-partisan, but in the interest of full disclosure I disclose that I will almost surely be voting for the Democratic Nominee. And I say <i>almost surely</i> because very weird things could happen.I can imagine a republican saying, in 2015 <i>I will almost surely be voting for the Republican Nominee</i> and then later deciding to not vote for Trump. <br/>
<br/>
<br/>
<i>My Past Predictions</i>: Early on in 2007 I predicted it would be Obama vs McCain and that Obama would win. Was I smart or lucky? Early in 2011 I predicted Paul Ryan would be the Rep. Candidate. Early in 2015 and even into 2016 I predicted  that Trump would not get the nomination. After he got the nomination I predicted  he would not become president. So, in answer to my first question, I was lucky not smart. Having said all of this I predict that the Dem. candidate will be Warren. Note- this is an honest prediction, not one fueled by what I want to see happen. I predict Warren since she seems to be someone who can bridge the so-called establishment and the so-called left (I dislike the terms LEFT and RIGHT since issues and views change over time). Given my past record I would not take me too seriously. Also, since this prediction is not particularly unusual, if I am right this would NOT be impressive (My Obama prediction was impressive, and my Paul Ryan prediction would have been very impressive had I been right.)<br/>
<br/>
<i>Electability</i>: My spell checker doesn't think its a word. Actually it shouldn't be a word. It's a stupid concept. Recall<br/>
<br/>
JFK was unelectable since he was Catholic. <br/>
<br/>
Ronald Reagan was unelectable because he was too conservative.<br/>
<br/>
A draft dodging adulterer named Bill Clinton could not possible beat a sitting president who just won a popular war.<br/>
<br/>
Nobody named Barack Hussein Obama, who is half-black,  could possibly get the nomination, never mind the presidency. And Hillary had the nomination locked up in 2008--- she had no any serious challengers. <br/>
<br/>
(An article in <i>The New Republic</i> in 2007 predicted a brokered convention for the Republicans where Fred Thompson, Mitt Romney, and Rudy Guilliani would split the vote, and at the same time a cake walk for Hillary Clinton with<br/>
Barak Obama winning Illinois in the primaries but not much else. Recall that 2008 was McCain vs Obama.)<br/>
<br/>
Donald Trump will surely be stopped from getting the nomination because, in the end, <a href="https://www.amazon.com/Party-Decides-Presidential-Nominations-American/dp/0226112373/ref=cm_cr_arp_d_product_top?ie=UTF8">The Party Decides</a>.<br/>
<br/>
Republican voters in 2016  will prefer  Rubio to Trump since Marco is more electable AND more conservative. Hence, in the space of Rep. Candidates, Rubio dominates Trump. So, by simple game theory, Trump can't get the nomination.  The more electable Rubio, in the 2016 primaries, won Minnesota, Wash DC,  and Puerto Rico (Puerto Rico has a primary. Really!) One of my friends thought he also won Guam (Guam?) but I could not find evidence of that on the web. Okay, so why did Trump win? <i>Because voters are not game theorists.</i><br/>
<br/>
ANYWAY, my point is that how can anyone take the notion of electability seriously when unelectable people have gotten elected?<br/>
<br/>
<i>Primaries</i>: Dem primary  voters are torn between who they want to be president and who can beat Trump.  Since its so hard to tell who can beat who, I would recommend voting for who you like and not say stupid things like<br/>
<br/>
American would never elect  a 76 year old socialist whose recently had a heart attack.<br/>
<br/>
or<br/>
<br/>
Trump beat a women in 2016 so we can't nominate a women<br/>
<br/>
or<br/>
<br/>
America is not ready to elect a gay president yet. (America is never ready to do X until after it does X and then the pundits ret-con their opinions.For example, of course America is ready for Gay-Marriage. Duh.)<br/>
<br/>
<i>Who won the debate?<br/>
</i> Whoever didn't bother watching it :-). I think the question is stupid and has become who got out a clever sound bite. We need sound policy, not sound bites!</div>
    </content>
    <updated>2019-10-28T14:05:00Z</updated>
    <published>2019-10-28T14:05:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-10-29T21:31:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16320</id>
    <link href="https://rjlipton.wordpress.com/2019/10/27/quantum-supremacy-at-last/" rel="alternate" type="text/html"/>
    <title>Quantum Supremacy At Last?</title>
    <summary>What it takes to understand and verify the claim Cropped from 2014 Wired source John Martinis of U.C. Santa Barbara and Google is the last author of a paper published Wednesday in Nature that claims to have demonstrated a task executed with minimum effort by a quantum computer that no classical computer can emulate without […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>What it takes to understand and verify the claim</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/10/martinis1.png"><img alt="" class="alignright wp-image-16322" height="180" src="https://rjlipton.files.wordpress.com/2019/10/martinis1.png?w=153&amp;h=180" width="153"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from 2014 <i>Wired</i> <a href="https://www.wired.com/2014/09/martinis/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
John Martinis of U.C. Santa Barbara and Google is the <em>last</em> author of a <a href="https://www.nature.com/articles/s41586-019-1666-5">paper</a> published Wednesday in <em>Nature</em> that claims to have demonstrated a task executed with minimum effort by a quantum computer that no classical computer can emulate without expending Herculean—or Sisyphean—effort. </p>
<p>
Today we present a lay understanding of the claim and discuss degrees of establishing it.</p>
<p>
There are 76 other authors of the paper. The first 75 are alphabetical, then comes Hartmut Neven before Martinis. Usually pride of place goes to the first author, but that depends on size. Martinis is also the corresponding author. The cox in a rowing race rides at the rear. We have discussed aspects of papers with a huge number of authors <a href="https://rjlipton.wordpress.com/2014/02/13/seeing-atoms/">here</a>. </p>
<p>
Three planks of a quantum supremacy claim are:</p>
<ol>
<li>
<em>Build a physical device capable of a nontrivial sampling task.</em> <p/>
</li><li>
<em>Prove that it gains advantage over known classical approaches.</em> <p/>
</li><li>
<em>Prove that comparable classical hardware cannot gain such advantage.</em>
</li></ol>
<p>
Scott Aaronson not only has made <a href="https://www.scottaaronson.com/blog/?p=4317">two</a> great <a href="https://www.scottaaronson.com/blog/?p=4372">posts</a> on these and many other aspects of the claim, he independently proposed in 2015 the sampling task that was programmed, and he analyzed it in a foundational <a href="https://arxiv.org/abs/1612.05903">paper</a> with Lijie Chen of MIT. Researchers at Google had already been thinking along those lines, and they anchored the team composed from numerous other institutions as well. As if on cue—just a couple days before Wednesday’s announcement—a group from IBM put out a <a href="https://www.ibm.com/blogs/research/2019/10/on-quantum-supremacy/">post</a> and <a href="https://arxiv.org/abs/1910.09534">paper</a> taking issue with the argument for the third plank.</p>
<p>
We’ll start with the task and go in order 1-3-2.</p>
<p>
</p><p/><h2> The Task </h2><p/>
<p/><p>
Any <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-qubit quantum circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> and input <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> to <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> induces a probability distribution <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> on <img alt="{\{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{0,1\}^n}"/>. Because it will not matter if we prepend up to <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> NOT gates to <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>, we may suppose <img alt="{x = 0^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+0%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x = 0^n}"/>. Then <img alt="{C(0^n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%280%5En%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C(0^n)}"/> is a unit complex vector of length <img alt="{N = 2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N = 2^n}"/> with entries <img alt="{a_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_z}"/> corresponding to possible outputs <img alt="{z \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z \in \{0,1\}^n}"/>. Then the probability of getting <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> by a final measurement of all qubits is </p>
<p align="center"><img alt="\displaystyle  p_z = D_C(z) = |a_z|^2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_z+%3D+D_C%28z%29+%3D+%7Ca_z%7C%5E2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p_z = D_C(z) = |a_z|^2. "/></p>
<p>
Next we consider probability distributions <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/> that are generated uniformly at random by the following process, for some <img alt="{r \geq n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Cgeq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r \geq n}"/> and taking <img alt="{R = 2^r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%3D+2%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R = 2^r}"/>:</p>
<blockquote><p><b> </b> <em> for <img alt="{i = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%3D+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i = 1}"/> to <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{R}"/>:<br/>
   choose a <img alt="{z \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{z \in \{0,1\}^n}"/> uniformly at random;<br/>
   increment its probability <img alt="{D_1(z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%28z%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D_1(z)}"/> by <img alt="{\frac{1}{R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7BR%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\frac{1}{R}}"/>. </em>
</p></blockquote>
<p/><p>
Here we intend <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> to be the number of binary nondeterministic gates in the circuit. In place of Hadamard gates the experimental circuits get their nondeterminism from these three single-qubit gates (ignoring global phase for <img alt="{\mathbf{Y}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BY%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{Y}^{1/2}}"/> in particular): </p>
<p align="center"><img alt="\displaystyle  \mathbf{X}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; 1 - i \\ 1 - i &amp; 1 + i \end{bmatrix},~ \mathbf{Y}^{1/2} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1 \end{bmatrix},~ \mathbf{W}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; - i\sqrt{2} \\ \sqrt{2} &amp; 1 + i \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BX%7D%5E%7B1%2F2%7D+%3D+%5Cfrac%7B1%7D%7B2%7D%5Cbegin%7Bbmatrix%7D+1+%2B+i+%26+1+-+i+%5C%5C+1+-+i+%26+1+%2B+i+%5Cend%7Bbmatrix%7D%2C%7E+%5Cmathbf%7BY%7D%5E%7B1%2F2%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D+1+%26+-1+%5C%5C+1+%26+1+%5Cend%7Bbmatrix%7D%2C%7E+%5Cmathbf%7BW%7D%5E%7B1%2F2%7D+%3D+%5Cfrac%7B1%7D%7B2%7D%5Cbegin%7Bbmatrix%7D+1+%2B+i+%26+-+i%5Csqrt%7B2%7D+%5C%5C+%5Csqrt%7B2%7D+%26+1+%2B+i+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathbf{X}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; 1 - i \\ 1 - i &amp; 1 + i \end{bmatrix},~ \mathbf{Y}^{1/2} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1 \end{bmatrix},~ \mathbf{W}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; - i\sqrt{2} \\ \sqrt{2} &amp; 1 + i \end{bmatrix}. "/></p>
<p>Here <img alt="{\mathbf{W} = \frac{1}{\sqrt{2}}(\mathbf{X} + \mathbf{Y})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%5Cmathbf%7BX%7D+%2B+%5Cmathbf%7BY%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{W} = \frac{1}{\sqrt{2}}(\mathbf{X} + \mathbf{Y})}"/> where <img alt="{\mathbf{Y} = \begin{bmatrix} 0 &amp; -i \\ i &amp; 0 \end{bmatrix}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BY%7D+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+-i+%5C%5C+i+%26+0+%5Cend%7Bbmatrix%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{Y} = \begin{bmatrix} 0 &amp; -i \\ i &amp; 0 \end{bmatrix}}"/> and <img alt="{\mathbf{X}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BX%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{X}}"/> is another name for NOT. The difference from using Hadamard gates matters to technical analysis of the distributions <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> but the interplay between quantum nondeterministic gates and classical random coins remains in force. </p>
<p>
The choice of <img alt="{\mathbf{X}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BX%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{X}^{1/2}}"/>, <img alt="{\mathbf{Y}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BY%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{Y}^{1/2}}"/>, or <img alt="{\mathbf{W}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{W}^{1/2}}"/> is itself uniformly random at each point where a single-qubit gate is used, except for not repeating the same gate on the same qubit, and those choices determine <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. Now we can give an initial statement of the task tailored to what the paper achieves:</p>
<blockquote><p><b> </b> <em> Given randomly-generated quantum circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/> as inputs, distinguish <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D_C}"/> with high probability from any <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D_1}"/>. </em>
</p></blockquote>
<p/><p>
In more detail, the object is to take a number <img alt="{\delta &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta &gt; 0}"/> and moderately large integer <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, both dictated by practical elements of the experiment, and fulfill this task statement:</p>
<blockquote><p><b> </b> <em> Given randomly-generated <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/>, generate samples <img alt="{z_1,...,z_k \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_1%2C...%2Cz_k+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{z_1,...,z_k \in \{0,1\}^n}"/> such that <img alt="{\frac{1}{k}(D_C(z_1) + \cdots D_C(z_k)) \geq 1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Bk%7D%28D_C%28z_1%29+%2B+%5Ccdots+D_C%28z_k%29%29+%5Cgeq+1+%2B+%5Cdelta%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\frac{1}{k}(D_C(z_1) + \cdots D_C(z_k)) \geq 1 + \delta}"/>. </em>
</p></blockquote>
<p/><p>
It’s important to note that there are two <em>stages</em> of randomness: one over <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> which chooses <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/>, and then the stage of measuring after (perhaps imperfectly) executing <img alt="{C(0^n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%280%5En%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C(0^n)}"/>. The latter can be repeated to get a large sample of strings <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> for a given <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. The nature of the former stage matters most to justifying how to interpret tests of the samples and to closing loopholes. Our <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/> does not signify having uniform distribution in the latter sampling, but rather covers classical alternatives in the former stage that (with overwhelming probability) belong to a class we call <img alt="{\mathcal{D}_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_1}"/>. The <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> for random <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> will (again w.o.p.) belong to a class <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/> which we explain next.</p>
<p>
</p><p/><h2> The World Series of Quantum Computing </h2><p/>
<p/><p>
In honor of the baseball World Series, we offer a baseball analogy. To make differences sharper to see, we take <img alt="{r = n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = n}"/>, so <img alt="{R = N = 2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%3D+N+%3D+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R = N = 2^n}"/>. This is not what the experiment does: their biggest instance has 20 layers totaling <img alt="{r = 1,\!113}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+1%2C%5C%21113%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = 1,\!113}"/> nondeterministic single-qubit gates (plus <img alt="{430}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B430%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{430}"/> two-qubit gates) on the <img alt="{n = 53}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+53%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 53}"/> qubits. But let us continue.</p>
<p>
We are distributing <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> units of probability among <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> “batters” <img alt="{z \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z \in \{0,1\}^n}"/>. A batter who gets two units hits a double, three units makes a triple, and so on. The key distinction is between the familiar batting average and the <em>slugging average</em>, which averages all the bases scored with hits:</p>
<ul>
<li>
The chance of making an out—that is, getting no units—is <img alt="{(\frac{N-1}{N})^N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cfrac%7BN-1%7D%7BN%7D%29%5EN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\frac{N-1}{N})^N}"/> which is approximately <img alt="{\frac{1}{e} = 0.367879\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Be%7D+%3D+0.367879%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{e} = 0.367879\dots}"/> <p/>
</li><li>
The chance of hitting a single is also about <img alt="{\frac{1}{e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Be%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{e}}"/>, leaving <img alt="{1 - \frac{2}{e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cfrac%7B2%7D%7Be%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 - \frac{2}{e}}"/> as the frequency of getting an extra-base hit—which makes <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> a “heavy hitter.” <p/>
</li><li>
From <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> batters chosen uniformly at random, their expected batting average will be <img alt="{1 - \frac{1}{e} = 0.632\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cfrac%7B1%7D%7Be%7D+%3D+0.632%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 - \frac{1}{e} = 0.632\dots}"/>. <p/>
</li><li>
Their expected slugging average, however, will just be <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>: they expect <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> units to be distributed among them.
</li></ul>
<p>
Thus with respect to a random <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/>, and without any knowledge of <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/>, a chosen team of <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> hitters cannot expect to have a joint slugging average higher than <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. Moreover, for any fixed <img alt="{\delta &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta &gt; 0}"/>, the chance of getting a slugging average higher than <img alt="{1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 + \delta}"/> tails away exponentially in <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> (provided <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> also grows). </p>
<p>
With respect to <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/>, however, a quantum device can do better. Google’s device programs itself given <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> as the blueprint. So it just executes <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> and measures all qubits to sample the output. Finding its own heavy hitters is what a quantum circuit is good at. The probability of getting a hitter who hits a triple is magnified by <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/> compared to a uniform choice. Moreover, <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> will never output a string with zero hits—a “can’t miss” property denied to a classical reader of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. For large <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> the probability distribution approaches <img alt="{xe^{-x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bxe%5E%7B-x%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{xe^{-x}}"/> and the slugging expectation is approximately </p>
<p align="center"><img alt="\displaystyle  \int_0^\infty x^2 e^{-x} = 2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cint_0%5E%5Cinfty+x%5E2+e%5E%7B-x%7D+%3D+2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \int_0^\infty x^2 e^{-x} = 2. "/></p>
<p>That is, a team <img alt="{z_1,\dots,z_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_1%2C%5Cdots%2Cz_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_1,\dots,z_k}"/> drafted by sampling from random quantum circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> expects to have a slugging average near <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. This defines the class <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/>. If <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> works perfectly, the average will surpass <img alt="{1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 + \delta}"/> whenever <img alt="{0 &lt; \delta &lt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%3C+%5Cdelta+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0 &lt; \delta &lt; 1}"/> with near certainty as <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> grows. </p>
<p>
Google’s circuits have up to <img alt="{r = 20n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+20n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = 20n}"/>, so <img alt="{R \gg N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%5Cgg+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R \gg N}"/>. Then the “can’t miss” aspect of the quantum advantage is less sharp but the <img alt="{xe^{-x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bxe%5E%7B-x%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{xe^{-x}}"/> approximation is closer and the idea of <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/> is the same. The nature of <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/> can actually be <em>seen</em> from point intensities in <a href="https://en.wikipedia.org/wiki/Speckle_pattern">speckle</a> patterns of laser light:</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2019/10/375px-objective_speckle.jpg"><img alt="" class="aligncenter wp-image-16324" height="150" src="https://rjlipton.files.wordpress.com/2019/10/375px-objective_speckle.jpg?w=150&amp;h=150" width="150"/></a></p>
<p>
</p><p/><h2> Real-World Execution </h2><p/>
<p/><p>
The practical challenge is that the implementation of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> is not perfect. The consequence of an error in the final output is severe. The heavy-hitter outputs <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> of a random <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> are generally not bit-wise similar, so sampling their neighbors is like sampling uniform distribution. As the paper says, “A single bit or phase flip over the course of the algorithm will completely shuffle the speckle pattern and result in close to zero fidelity.”</p>
<p>
Their circuits are sufficiently random that effects of sporadic errors over millions of samples can be modeled by a simple equation using quantum mixed states. We shortcut the paper’s physical analysis by drawing on John Preskill’s illustration of a <em>de-polarizing channel</em> in <a href="http://www.theory.caltech.edu/~preskill/ph219/chap3_15.pdf">chapter 3</a> of his wonderful online <a href="http://www.theory.caltech.edu/~preskill/ph219/ph219_2018-19">notes</a> on quantum computation to reach the same equation (<a href="https://rjlipton.wordpress.com/feed/#F">1</a>). The modeling has informative symmetry when the errors of a <b>bit flip</b>, <b>phase flip</b>, or both are considered equally likely with probability <img alt="{\frac{p}{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bp%7D%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{p}{3}}"/>. The action on the entangled pair <img alt="{|\Phi^+\rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5CPhi%5E%2B%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|\Phi^+\rangle}"/> in the <a href="https://en.wikipedia.org/wiki/Bell_state#Bell_basis">Bell basis</a> is given by the density matrix evolution <img alt="{\rho = |\Phi^+\rangle\langle\Phi^+| \mapsto \rho'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho+%3D+%7C%5CPhi%5E%2B%5Crangle%5Clangle%5CPhi%5E%2B%7C+%5Cmapsto+%5Crho%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho = |\Phi^+\rangle\langle\Phi^+| \mapsto \rho'}"/> where </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}  \rho' &amp;=&amp; (1 - p)|\Phi^+\rangle\langle\Phi^+| \;+\; \frac{p}{3}\left(|\Psi^+\rangle\langle\Psi^+| \;+\; |\Phi^-\rangle\langle\Phi^-| \;+\; |\Psi^-\rangle\langle\Psi^-|\right)\\ ~~~\\ &amp;=&amp; (1 - p') |\Phi^+\rangle\langle\Phi^+| \;+\; p'\frac{I}{4}, \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Crho%27+%26%3D%26+%281+-+p%29%7C%5CPhi%5E%2B%5Crangle%5Clangle%5CPhi%5E%2B%7C+%5C%3B%2B%5C%3B+%5Cfrac%7Bp%7D%7B3%7D%5Cleft%28%7C%5CPsi%5E%2B%5Crangle%5Clangle%5CPsi%5E%2B%7C+%5C%3B%2B%5C%3B+%7C%5CPhi%5E-%5Crangle%5Clangle%5CPhi%5E-%7C+%5C%3B%2B%5C%3B+%7C%5CPsi%5E-%5Crangle%5Clangle%5CPsi%5E-%7C%5Cright%29%5C%5C+%7E%7E%7E%5C%5C+%26%3D%26+%281+-+p%27%29+%7C%5CPhi%5E%2B%5Crangle%5Clangle%5CPhi%5E%2B%7C+%5C%3B%2B%5C%3B+p%27%5Cfrac%7BI%7D%7B4%7D%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}  \rho' &amp;=&amp; (1 - p)|\Phi^+\rangle\langle\Phi^+| \;+\; \frac{p}{3}\left(|\Psi^+\rangle\langle\Psi^+| \;+\; |\Phi^-\rangle\langle\Phi^-| \;+\; |\Psi^-\rangle\langle\Psi^-|\right)\\ ~~~\\ &amp;=&amp; (1 - p') |\Phi^+\rangle\langle\Phi^+| \;+\; p'\frac{I}{4}, \end{array} "/></p>
<p>where <img alt="{p' = \frac{4}{3} p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%27+%3D+%5Cfrac%7B4%7D%7B3%7D+p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p' = \frac{4}{3} p}"/> and <img alt="{\frac{I}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BI%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{I}{4}}"/> is the density matrix of the completely mixed two-qubit state which is just a classical distribution. This presumes <img alt="{p \leq \frac{3}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%5Cleq+%5Cfrac%7B3%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p \leq \frac{3}{4}}"/>; note that <img alt="{p = \frac{3}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%3D+%5Cfrac%7B3%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p = \frac{3}{4}}"/> completely mixes the Bell basis already. The <b>fidelity</b> of <img alt="{\vec{\rho}'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7B%5Crho%7D%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{\rho}'}"/> to the original state is then given by </p>
<p align="center"><img alt="\displaystyle  F = \langle\Phi^+|\rho'|\Phi^+\rangle = 1 - p'. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F+%3D+%5Clangle%5CPhi%5E%2B%7C%5Crho%27%7C%5CPhi%5E%2B%5Crangle+%3D+1+-+p%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F = \langle\Phi^+|\rho'|\Phi^+\rangle = 1 - p'. "/></p>
<p>
This modeling already indicates that with <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> serial opportunities for error the fidelity will decay as <img alt="{(1 - p')^m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281+-+p%27%29%5Em%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1 - p')^m}"/>.  The Google team found low ‘crosstalk’ between qubits and they used exactly this expression in the form <img alt="{(1 - \frac{e_1}{1 - 1/D^2})^m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281+-+%5Cfrac%7Be_1%7D%7B1+-+1%2FD%5E2%7D%29%5Em%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1 - \frac{e_1}{1 - 1/D^2})^m}"/>, evidently with <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> being the native gate error rate they call <img alt="{e_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_1}"/> and <img alt="{D = 2^k,}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD+%3D+2%5Ek%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D = 2^k,}"/> where having <img alt="{k=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k=1}"/> for single-qubit gates supplies the factor <img alt="{\frac{2^{2k}}{2^{2k} - 1} = \frac{4}{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B2%5E%7B2k%7D%7D%7B2%5E%7B2k%7D+-+1%7D+%3D+%5Cfrac%7B4%7D%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{2^{2k}}{2^{2k} - 1} = \frac{4}{3}}"/>.<br/>
The error <img alt="{e_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_2}"/> for the two-qubit gates is similarly represented.  (The full modeling in the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1666-5/MediaObjects/41586_2019_1666_MOESM1_ESM.pdf">supplement</a>, section V, is more refined.)</p>
<p>
By observing their benchmarks (discussed below) for varying small <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> they could calculate the decay concretely and hence estimate values of <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> for the vast majority of runs with larger <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/>.  The random nature of the circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> evidently makes covariance of errors that could systematically upset this modeling negligible.  Thus they can conclude that their device effectively samples from the distribution <a name="F"/></p><a name="F">
<p align="center"><img alt="\displaystyle  F|\langle z \;|\; C \;|\; 0^n\rangle|^2 \;\;+\;\; (1 - F)\frac{1}{N}. \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%7C%5Clangle+z+%5C%3B%7C%5C%3B+C+%5C%3B%7C%5C%3B+0%5En%5Crangle%7C%5E2+%5C%3B%5C%3B%2B%5C%3B%5C%3B+%281+-+F%29%5Cfrac%7B1%7D%7BN%7D.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F|\langle z \;|\; C \;|\; 0^n\rangle|^2 \;\;+\;\; (1 - F)\frac{1}{N}. \ \ \ \ \ (1)"/></p>
</a><p><a name="F"/> Such distributions can be said to belong to the class <img alt="{\mathcal{D}_{1 + F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1+%2B+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1 + F}}"/>. The paper reports that their <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> is driven below <img alt="{0.01}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.01%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.01}"/> but stays above <img alt="{0.001}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.001%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.001}"/> in trials. This bounds the range of the <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> they can separate by. That <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> is separated from zero achieves the first plank and starts on the second. The third needs attention first, however. </p>
<p>
</p><p/><h2> The Third Plank </h2><p/>
<p/><p>
Both <em>concrete</em> and <em>asymptotic</em> complexity evidence matter for the third plank, the former for now and the latter for how <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> and everything else may scale up in the future. In asymptotic complexity, we still don’t know that <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/> and <img alt="{\mathsf{PSPACE}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPSPACE%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{PSPACE}}"/>, which sandwich the quantum feasible class <img alt="{\mathsf{BQP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{BQP}}"/>, are different. Thus asymptotic evidence about polynomial bounds must be conditional. Asymptotic evidence about linear time bounds can be sharper but then tends to be conditioned on forms of <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">SETH</a> in ways we still find <a href="https://rjlipton.wordpress.com/2015/06/01/puzzling-evidence/">puzzling</a>.</p>
<p>
Lower bounds in concrete complexity are less known and have a self-defeating aspect: We are trying to say that any program <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> run for less than an infeasible time <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> must fail. But we can’t run <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> for time <img alt="{T-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T-1}"/> to show that it fails because time <img alt="{T-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T-1}"/> is just as infeasible as time <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>. The best we can do is run <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> for a feasible <img alt="{T_0 \ll T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_0+%5Cll+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_0 \ll T}"/>, either (i) on a smaller task size, or (ii) on the original task but argue it doesn’t show <em>progress</em>. Neither is the same; we <a href="https://rjlipton.wordpress.com/2010/08/28/lower-bounds-and-progressive-algorithms/">made</a> some <a href="https://rjlipton.wordpress.com/2012/11/17/progress-on-progressive-algorithms/">attempts</a> on (ii). </p>
<p>
What the paper does instead is argue that a particular classical approach <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> (also from the Aaronson-Chen paper) would take 10,000 years on today’s hardware. This reminds us of a famous 1977 “Mathematical Games” <a href="https://simson.net/ref/1977/Gardner_RSA.pdf">column</a> by Martin Gardner, which quotes an estimate by Ron Rivest that for factoring a 126-digit number on then-current hardware, “the running time required would be about 40 quadrillion years!” It took only until <a href="https://en.wikipedia.org/wiki/The_Magic_Words_are_Squeamish_Ossifrage">1994</a> for this to be broken. Sure enough, IBM calculated that a more-clever implementation of <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> on the <a href="https://en.wikipedia.org/wiki/Summit_(supercomputer)">Summit</a> supercomputer would take under 3 days. The point is not so much that the Summit hardware is comparable as that estimates based on what are currently thought to be the best possible (classical) methods need asterisks.</p>
<p>
On the asymptotic side, the last section (XI) of the paper’s 66-page <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1666-5/MediaObjects/41586_2019_1666_MOESM1_ESM.pdf">supplement</a> proves a theorem toward showing that a classical simulation from <img alt="{\mathcal{D}_{1 + \delta}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1+%2B+%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1 + \delta}}"/> that scales polynomially with <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> would collapse <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{\#P}}"/> to <img alt="{\mathsf{AM}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{AM}}"/>, and similarly for sub-exponential running times. It does not get all the way there, however: improvements would need to be made in upper bounds for approximation and for worst-case to average-case equivalence. Moreover, there is a difference from what their statistical testing achieves that we try to explain next. </p>
<p>
</p><p/><h2> The Statistical Tests </h2><p/>
<p/><p>
We can cast the second plank in the general context of predictive modeling. Consider a forecaster who places estimates <img alt="{\{q_i\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Bq_i%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{q_i\}}"/> on the true probabilities <img alt="{\{p_i\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Bp_i%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{p_i\}}"/> of various events. In the quantum case, the <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> come from distributions in <img alt="{\mathcal{D}_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1+F}}"/>, where the <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> that applies to the latter sampling stage can be estimated based on the size and depth of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. The <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> come from the physical quantum device—that is to say, from the strings <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> that it outputs. What’s needed is to compute the corresponding outcome probability <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> analytically based on the given circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. This must be done <em>classically</em>, and incurs the “<img alt="{T_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_0}"/>-versus-<img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>” issue discussed above.  <b>[See Addendum below.]</b></p>
<p>
But before we get to that issue, let’s say more from the viewpoint of predictive modeling. We measure how well the forecasts <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> conform to the true <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> by applying a prediction scoring <a href="https://en.wikipedia.org/wiki/Scoring_rule">rule</a>. If outcome <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> happens, then the <em>log-likelihood rule</em> assesses a penalty of </p>
<p align="center"><img alt="\displaystyle  L_i = \log(\frac{1}{q_i}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_i+%3D+%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L_i = \log(\frac{1}{q_i}). "/></p>
<p>This is zero if the outcome was predicted with certainty but goes to infinity if the individual <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> is very low—which is an issue in the quantum case. The expected score based on the true probabilities is <a name="XE"/></p><a name="XE">
<p align="center"><img alt="\displaystyle  E[L_i] = \sum_i p_i \log(\frac{1}{q_i}). \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E%5BL_i%5D+%3D+%5Csum_i+p_i+%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  E[L_i] = \sum_i p_i \log(\frac{1}{q_i}). \ \ \ \ \ (2)"/></p>
</a><p><a name="XE"/> The log-likelihood rule is <b>strictly proper</b> insofar as the unique way to minimize <img alt="{E[L_i]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%5BL_i%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E[L_i]}"/> is to set <img alt="{q_i = p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i = p_i}"/> for each <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>. In human contexts this means the model has incentive to be as accurate as possible. For the quantum device, knowing the <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> that applies to its running of circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> suffices to calculate <img alt="{E[L_i]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%5BL_i%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E[L_i]}"/> as “<img alt="{E_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_{1+F}}"/>,” and hence to benchmark how accurately the device is conforming to the target.</p>
<p>
The formula (<a href="https://rjlipton.wordpress.com/feed/#XE">2</a>) is the <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a> between the <img alt="{\vec{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{p}}"/> and <img alt="{\vec{q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{q}}"/> distributions. It is advocated in several predecessor papers on quantum supremacy experiments, but in fact the team shifted to something simpler they call “linear cross-entropy.” They simply show that the <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> from their samples collectively beat the “<img alt="{E_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_1}"/>” that applies to <img alt="{\mathcal{D}_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_1}"/>—more simply put, that when summed over <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>-many trials <img alt="{z_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_t}"/>, </p>
<p align="center"><img alt="\displaystyle  \frac{1}{T} \sum_{t = 1}^T q_{z_t} &gt; \frac{1}{N} + \delta. \ \ \ \ \ (3)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B1%7D%7BT%7D+%5Csum_%7Bt+%3D+1%7D%5ET+q_%7Bz_t%7D+%3E+%5Cfrac%7B1%7D%7BN%7D+%2B+%5Cdelta.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{1}{T} \sum_{t = 1}^T q_{z_t} &gt; \frac{1}{N} + \delta. \ \ \ \ \ (3)"/></p>
<p>This just boils down to giving a <a href="https://en.wikipedia.org/wiki/Standard_score">z-score</a> based on the modeling for <img alt="{\mathcal{D}_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_1}"/>. It is analogous to how I (Ken writing this) test for cheating at chess. We are flagging the physical device as getting surreptitious input from quantum to achieve a strength of <img alt="{1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 + \delta}"/> compared to a “classical player” who is “rated” as having strength <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. </p>
<p>
The difference from showing that the device’s score from (<a href="https://rjlipton.wordpress.com/feed/#XE">2</a>) is within a hair of <img alt="{E_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_{1+F}}"/> is that this is based on <img alt="{E_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_1}"/>. To be sure, the paper shows that their <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-scores conform to those one would expect an “<img alt="{E_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_{1+F}}"/>-rated” device to achieve. But this is still not the same as (<a href="https://rjlipton.wordpress.com/feed/#XE">2</a>). Whether it is tantamount for enough purposes—including the theorem about <img alt="{\mathsf{AM}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{AM}}"/>—is where we’re most unsure, and we note distinctions between fully (classically) sampling and “spoofing” the statistical tests(s) raised by Scott (including directly in reply to me <a href="https://www.scottaaronson.com/blog/?p=4372#comment-1822570">here</a>) and others. The authors say that using “linear cross-entropy” gave sharper results and that they tried other (unspecified) measures. We wonder how much of the space of scoring rules familiar in predictive modeling has been tried, and whether rules having more gentle tail behavior for tiny <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> than <img alt="{L_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_i}"/> might do better.</p>
<p>
Finally, there is the issue that the team were able to verify <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> exactly only for circuits up to <img alt="{43}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B43%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{43}"/> qubits and/or with <img alt="{14}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B14%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{14}"/> levels, not <img alt="{53}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B53%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{53}"/> with <img alt="{20}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B20%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{20}"/> levels. This creates a dilemma in that IBM’s paper may push them toward <img alt="{n = 60}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 60}"/> or <img alt="{70}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{70}"/>, but that increases the gap from instance sizes they can verify. This also pushes away from the possibly of observing the <img alt="{\mathcal{D}_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1+F}}"/> nature of <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> more directly by finding repeated strings <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> in the second-stage sampling of a fixed <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. The “birthday paradox” threshold for repeats is roughly <img alt="{2^{n/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{n/2}}"/> samples, which might be feasible for <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> around <img alt="{50}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B50%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{50}"/> (given the classical work needed for each <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>, which IBM’s cleverness might speed) but not above <img alt="{60}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{60}"/>. The distinguishing power of repeats drops further with <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. We intend to say more about these last few points, and we are sure there are many chapters still to write about supremacy experiments. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Is the evidence so far convincing to you? Is enough being done on the third plank to exclude possible clever classical use of the fact that the circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> are given as “white boxes”? Are there possible loopholes? </p>
<p>
We would also be grateful to know where we may have oversimplified our characterization of the task and our analysis of the issues.</p>
<p/><p><br/>
<b>Addendum 10/28:</b> On further review, the “outcome probability” of a string <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> comes from first exhaustively computing the probability <img alt="{r_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r_z}"/> that would result from error-free operation of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> and plugging that in to make <img alt="{Fr_z + (1 - F)\frac{1}{N}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BFr_z+%2B+%281+-+F%29%5Cfrac%7B1%7D%7BN%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Fr_z + (1 - F)\frac{1}{N}.}"/>  Although derived from the estimate of <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> and taking <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> from the device, this seems better to regard as the “true probability” <img alt="{p_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_z}"/>, rather than “<img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/>” as stated above.  The actual quantity to regard as “<img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/>” is not calculable and estimating it would require observing repeats from the physical device.  Equation (2) remains correct on principle, but as explained in these <a href="https://www.cs.cmu.edu/~odonnell/quantum18/lecture25.pdf">notes</a> by Ryan O’Donnell, the reversed equation is used instead: </p>
<p align="center"><img alt="\displaystyle  E[L'_i] = \sum_z q_z \log(\frac{1}{p_z}). \ \ \ \ \ (2')" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E%5BL%27_i%5D+%3D+%5Csum_z+q_z+%5Clog%28%5Cfrac%7B1%7D%7Bp_z%7D%29.+%5C+%5C+%5C+%5C+%5C+%282%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  E[L'_i] = \sum_z q_z \log(\frac{1}{p_z}). \ \ \ \ \ (2')"/></p>
<p>
The difference is that <img alt="{\log(\frac{1}{p_z})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28%5Cfrac%7B1%7D%7Bp_z%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log(\frac{1}{p_z})}"/> can be calculated, and while <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> still cannot be, the act of sampling from the physical device estimates the idealized sum <img alt="{\sum_i q_i \log(\frac{1}{p_i})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+q_i+%5Clog%28%5Cfrac%7B1%7D%7Bp_i%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_i q_i \log(\frac{1}{p_i})}"/> closely enough.  This switches the roles of “forecaster” and “forecastee,” but the optimality of <img alt="{q_z = p_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z+%3D+p_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z = p_z}"/> remains valid and the target value is the same as before.  O’Donnell calls this inversion “slightly dicey” but (i) it was ultimately not used anyway, (ii) has an interpretation that regards the physical device as the ground truth, and (iii) may be equally amenable to asymptotic conditional hardness results.  Likewise “<img alt="{q_{z_t}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_%7Bz_t%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_{z_t}}"/>” should be re-named as “<img alt="{p_{z_t}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_%7Bz_t%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_{z_t}}"/>” in (3).]</p>
<p/><p><br/>
[Added more error-modeling details to the real-world section; some minor word changes; clarified how X,Y,W are chosen; addendum to clarify modeling issues.]</p></font></font></div>
    </content>
    <updated>2019-10-27T13:19:04Z</updated>
    <published>2019-10-27T13:19:04Z</published>
    <category term="All Posts"/>
    <category term="detection"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="decoherence"/>
    <category term="Google"/>
    <category term="IBM"/>
    <category term="John Martinis"/>
    <category term="Physics"/>
    <category term="quantum"/>
    <category term="quantum computer"/>
    <category term="quantum supremacy"/>
    <category term="Scott Aaronson"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-10-30T10:20:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=18384</id>
    <link href="https://gilkalai.wordpress.com/2019/10/27/starting-today-kazhdan-sunday-seminar-computation-quantumness-symplectic-geometry-and-information/" rel="alternate" type="text/html"/>
    <title>Starting today: Kazhdan Sunday seminar: “Computation, quantumness, symplectic geometry, and information”</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Sunday, 27 October, 2019 – 14:00 to 16:00 Repeats every week every Sunday until Sat Feb 01 2020 Location: Ross 70 See also: Seminar announcement; previous post Symplectic Geometry, Quantization, and Quantum Noise. The Google supremacy claims are discussed (with … <a href="https://gilkalai.wordpress.com/2019/10/27/starting-today-kazhdan-sunday-seminar-computation-quantumness-symplectic-geometry-and-information/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Sunday, 27 October, 2019 – 14:00 to 16:00</p>
<p>Repeats every week every Sunday until Sat Feb 01 2020</p>
<p>Location: Ross 70</p>
<p>See also: <a href="https://mathematics.huji.ac.il/event/kazhdan-sunday-seminar-computation-quantumness-symplectic-geometry-and-information-gil?delta=0">Seminar announcement</a>; previous post <a href="https://gilkalai.wordpress.com/2013/01/01/symplectic-geometry-quantization-and-quantum-noise/" rel="bookmark">Symplectic Geometry, Quantization, and Quantum Noise.</a></p>
<p>The Google supremacy claims are discussed (with updates from time to time) in <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">this earlier post</a>. Don’t miss <a href="https://gilkalai.wordpress.com/2019/10/13/gerard-cornuejolss-bakers-eighteen-5000-dollars-conjectures/">our previous post</a> on combinatorics.</p>
<h3>Tentative syllabus for “Computation, quantumness, symplectic geometry, and information”</h3>
<p>1. Mathematical models of classical and quantum mechanics.</p>
<p>2. Correspondence principle and quantization.</p>
<p>3. Classical and quantum computation: gates, circuits, algorithms (Shor, Grover). Solovay-Kitaev. Some ideas of cryptography</p>
<p>4. Quantum noise and measurement, and rigidity of the Poisson bracket.</p>
<p>5. Noisy classical and quantum computing and error correction, threshold theorem- quantum fault tolerance (small noise is good for quantum computation). Kitaev’s surface code.</p>
<p>6. Quantum speed limit/time-energy uncertainty vs symplectic displacement energy.</p>
<p>7. Time-energy uncertainty and quantum computation (Dorit or her student?)</p>
<p>8. Berezin transform, Markov chains, spectral gap, noise.</p>
<p>9. Adiabatic computation, quantum PCP (probabilistically checkable proofs) conjecture [? under discussion]</p>
<p>10. Noise stability and noise sensitivity of Boolean functions, noisy boson sampling</p>
<p>11. Connection to quantum field theory (Guy?).</p>
<p>Literature: Aharonov, D. Quantum computation, In “Annual Reviews of Computational Physics” VI, 1999 (pp. 259-346). <a href="https://arxiv.org/abs/quant-ph/9812037">https://arxiv.org/abs/quant-ph/9812037</a></p>
<p>Kalai, G., Three puzzles on mathematics computations, and games, Proc. Int Congress Math 2018, Rio de Janeiro, Vol. 1 pp. 551–606. <a href="https://arxiv.org/abs/1801.02602">https://arxiv.org/abs/1801.02602</a></p>
<p>Nielsen, M.A., and Chuang, I.L., Quantum computation and quantum information. Cambridge University Press, Cambridge, 2000.</p>
<p>Polterovich, L., Symplectic rigidity and quantum mechanics, European Congress of Mathematics, 155–179, Eur. Math. Soc., Zürich, 2018. <a href="https://sites.google.com/site/polterov/miscellaneoustexts/symplectic-rigidity-and-quantum-mechanics">https://sites.google.com/site/polterov/miscellaneoustexts/symplectic-rig…</a></p>
<p>Polterovich L., and Rosen D., Function theory on symplectic manifolds. American Mathematical Society; 2014. [Chapters 1,9] <a href="https://sites.google.com/site/polterov/miscellaneoustexts/function-theory-on-symplectic-manifolds">https://sites.google.com/site/polterov/miscellaneoustexts/function-theor…</a></p>
<p>Wigderson, A., Mathematics and computation, Princeton Univ. Press, 2019. <a href="https://www.math.ias.edu/files/mathandcomp.pdf">https://www.math.ias.edu/files/mathandcomp.pdf</a></p></div>
    </content>
    <updated>2019-10-27T06:09:19Z</updated>
    <published>2019-10-27T06:09:19Z</published>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Geometry"/>
    <category term="Physics"/>
    <category term="Teaching"/>
    <category term="Quantization"/>
    <category term="Quantum computation"/>
    <category term="Quantum information"/>
    <category term="Symplectic geometry"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-10-30T10:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/143</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/143" rel="alternate" type="text/html"/>
    <title>TR19-143 |  Equivalence of Systematic Linear Data Structures and Matrix Rigidity | 

	Sivaramakrishnan Natarajan Ramamoorthy, 

	Cyrus Rashtchian</title>
    <summary>Recently, Dvir, Golovnev, and Weinstein have shown that sufficiently strong lower bounds for linear data structures would imply new bounds for rigid matrices. However, their result utilizes an algorithm that requires an $NP$ oracle, and hence, the rigid matrices are not explicit. In this work, we derive an equivalence between rigidity and the systematic linear model of data structures. For the $n$-dimensional inner product problem with $m$ queries, we prove that lower bounds on the query time imply rigidity lower bounds for the query set itself. In particular, an explicit lower bound of $\omega\left(\frac{n}{r}\log m\right)$ for $r$ redundant storage bits would yield better rigidity parameters than the best bounds due to Alon, Panigrahy, and Yekhanin. We also prove a converse result, showing that rigid matrices directly correspond to hard query sets for the systematic linear model. As an application, we prove that the set of vectors obtained from rank one binary matrices is rigid with parameters matching the known results for explicit sets. This implies that the vector-matrix-vector problem requires query time $\Omega(n^{3/2}/r)$ for redundancy $r \geq \sqrt{n}$ in the systematic linear model, improving a result of Chakraborty, Kamma, and Larsen. Finally, we prove a cell probe lower bound for the vector-matrix-vector problem in the  high error regime, improving a result of Chattopadhyay, Koucký, Loff, and Mukhopadhyay.</summary>
    <updated>2019-10-25T19:43:46Z</updated>
    <published>2019-10-25T19:43:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-30T10:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/25/tenure-track-faculties-at-krannert-school-of-management-purdue-university-apply-by-december-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/25/tenure-track-faculties-at-krannert-school-of-management-purdue-university-apply-by-december-1-2019/" rel="alternate" type="text/html"/>
    <title>Tenure track faculties at Krannert School of Management, Purdue University (apply by December 1, 2019)</title>
    <summary>Krannert School of Management invites applicants for two tenure-track faculty positions at the assistant professor level in the Quantitative Methods area, to begin in the fall semester of 2020. We welcome applicants from all research areas represented within the Quantitative Methods area. Website: https://career8.successfactors.com/sfcareer/jobreqcareer?jobId=8013&amp;company=purdueuniv&amp;userna%20me= Email: nguye161@purdue.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Krannert School of Management invites applicants for two tenure-track faculty positions at the assistant professor level in the Quantitative Methods area, to begin in the fall semester of 2020. We welcome applicants from all research areas represented within the Quantitative Methods area.</p>
<p>Website: <a href="https://career8.successfactors.com/sfcareer/jobreqcareer?jobId=8013&amp;company=purdueuniv&amp;userna%20me=">https://career8.successfactors.com/sfcareer/jobreqcareer?jobId=8013&amp;company=purdueuniv&amp;userna%20me=</a><br/>
Email: nguye161@purdue.edu</p></div>
    </content>
    <updated>2019-10-25T18:11:05Z</updated>
    <published>2019-10-25T18:11:05Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-30T10:20:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/24/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/24/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019/" rel="alternate" type="text/html"/>
    <title>Simons-Berkeley Research Fellowship at Simons Institute for the Theory of Computing (apply by December 15, 2019)</title>
    <summary>The Simons Institute for the Theory of Computing invites applications for the Simons-Berkeley Research Fellowships to participate in one or more of the semester-long programs during the 2020-21 academic year: Probability, Geometry, and Computation in High Dimensions; Theory of Reinforcement Learning; Satisfiability: Theory, Practice, and Beyond; and Theoretical Foundations of Computer Systems Website: https://simons.berkeley.edu/fellows2020 Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Simons Institute for the Theory of Computing invites applications for the Simons-Berkeley Research Fellowships to participate in one or more of the semester-long programs during the 2020-21 academic year: Probability, Geometry, and Computation in High Dimensions; Theory of Reinforcement Learning; Satisfiability: Theory, Practice, and Beyond; and Theoretical Foundations of Computer Systems</p>
<p>Website: <a href="https://simons.berkeley.edu/fellows2020">https://simons.berkeley.edu/fellows2020</a><br/>
Email: simonsvisitorservices@berkeley.edu</p></div>
    </content>
    <updated>2019-10-24T23:20:27Z</updated>
    <published>2019-10-24T23:20:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-30T10:20:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1513</id>
    <link href="https://theorydish.blog/2019/10/24/shopping-for-grain-in-the-market-works-a-fine-job/" rel="alternate" type="text/html"/>
    <title>Shopping for Grain in the Market Works –   a Fine Job!</title>
    <summary>I’m excited to share the news of two upcoming workshops…   TCS Early Career Mentoring Workshop Yael Kalai, Matt Weinberg, and I are organizing a TCS mentoring workshop in upcoming FOCS with a focus on demystifying the job market. The program includes a senior panel featuring Shafi Goldwasser, Samir Khuller, Tim Roughgarden, and Eva Tardos, a junior panel starring Inbal Talgam-Cohen, Omri Weinstein, and Henry Yuen, and two exemplary job talks by Eshan Chattopadhyay and Pravesh Kothari. Visit our website to see the full program and most importantly suggest panel questions.   Fine-grained Complexity Workshop Amir Abboud and I are organizing a workshop on fine-grained complexity, to be held Jan 2nd 2020 at Tel-Aviv University, closing the first annual TAU Theory-Fest. The program includes a morning of plenary talks (Karl Bringmann, Seth Pettie, and Barna Saha) and shorter cutting-edge technical talks in the afternoon. (If you have something interesting to share with the fine-grained complexity community, and we haven’t contacted you yet about giving a talk, please let us know.)   Looking forward to seeing you in Baltimore and Tel-Aviv!</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I’m excited to share the news of two upcoming workshops…</p>
<p> </p>
<h3>TCS Early Career Mentoring Workshop</h3>
<p>Yael Kalai, Matt Weinberg, and I are organizing a TCS mentoring workshop in <a href="http://focs2019.cs.jhu.edu/">upcoming FOCS</a> with a focus on <strong>demystifying the job market</strong>.</p>
<p>The program includes a senior panel featuring Shafi Goldwasser, Samir Khuller, Tim Roughgarden, and Eva Tardos, a junior panel starring Inbal Talgam-Cohen, Omri Weinstein, and Henry Yuen, and two exemplary job talks by Eshan Chattopadhyay and Pravesh Kothari.</p>
<p>Visit our <a href="https://www.cs.princeton.edu/~smattw/FOCS19/index.html">website</a> to see the full program and most importantly <a href="https://forms.gle/5kw7Zydo4Cvw2D4r7">suggest panel questions</a>.</p>
<p> </p>
<h3>Fine-grained Complexity Workshop</h3>
<p>Amir Abboud and I are organizing a workshop on fine-grained complexity, to be held Jan 2nd 2020 at Tel-Aviv University, closing the first annual <a href="https://sites.google.com/view/tau-theory-fest/home">TAU Theory-Fest</a>.</p>
<p>The program includes a morning of plenary talks (Karl Bringmann, Seth Pettie, and Barna Saha) and shorter cutting-edge technical talks in the afternoon.</p>
<p>(If you have something interesting to share with the fine-grained complexity community, and we haven’t contacted you yet about giving a talk, please let us know.)</p>
<p> </p>
<p>Looking forward to seeing you in Baltimore and Tel-Aviv!</p></div>
    </content>
    <updated>2019-10-24T17:59:00Z</updated>
    <published>2019-10-24T17:59:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>aviad.rubinstein</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-10-30T10:21:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7561</id>
    <link href="https://windowsontheory.org/2019/10/24/boazs-inferior-classical-inferiority-faq/" rel="alternate" type="text/html"/>
    <title>Boaz’s inferior classical inferiority FAQ</title>
    <summary>(For better info, see Scott’s Supreme Quantum Superiority FAQ and also his latest post on the Google paper; also this is not really an FAQ but was inspired by a question about the Google paper from a former CS 121 student) “Suppose aliens invade the earth and threaten to obliterate it in a year’s time […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>(For better info, see <a href="https://www.scottaaronson.com/blog/?p=4317">Scott’s Supreme Quantum Superiority FAQ</a> and also his <a href="https://www.scottaaronson.com/blog/?p=4372">latest post on the Google paper</a>; also this is not really an FAQ but was inspired by a question about the Google paper from a former <a href="https://cs121.boazbarak.org/schedule/">CS 121 </a>student)</p>



<blockquote class="wp-block-quote"><p><em> “Suppose aliens invade the earth and threaten to obliterate it in a year’s time unless human beings can find the Ramsey number for red five and blue five. We could marshal the world’s best minds and fastest computers, and within a year we could probably calculate the value. If the aliens demanded the Ramsey number for red six and blue six, however, we would have no choice but to launch a preemptive attack.</em>“</p><cite>Paul Erdős (as quoted by Graham and Spencer, 1990, hat tip: <a href="https://windowsontheory.org/2017/10/30/the-different-forms-of-quantum-computing-skepticism/#comment-42659">Lamaze Tishallishmi</a>)</cite></blockquote>



<p>In a <a href="https://www.nature.com/articles/s41586-019-1666-5">Nature paper</a> published this week, a group of researchers from John Martinis’s lab at Google announced arguably the first demonstration of “quantum supremacy” – a computational task carried out by a 53 qubit quantum computer that would require a prohibitive amount of time to simulate classically. </p>



<p>Google’s calculations of the “classical computation time” might have been overly pessimistic (from the classical point of view), and there has been work from <a href="https://arxiv.org/abs/1910.09534">IBM</a> as well as some <a href="https://www.caltech.edu/campus-life-events/master-calendar/iqi-weekly-seminar-2019-10-01">work of Johnnie Gray</a> suggesting that there are significant savings to be made. Indeed, given the lessons that we learned from private key cryptography, where techniques such as linear and differential cryptanalysis were used to “shave factors from exponents”, we know that even if a problem requires exponential time in general, this does not mean that by being very clever we can’t make significant savings over the naive brute force algorithm. This holds doubly  so in this case, where, unlike the designers of block ciphers, the Google researchers were severely constrained by factors of geometry and the kind of gates they can reliably implement.</p>



<p>I would not be terribly surprised if we will see more savings and even an actual classical simulation of the same sampling task that Google achieved. In fact, I very much hope this happens, since it will allow us to independently verify the reliability of Google’s chip and whether it actually did in fact sample from the distribution it is supposed to have sampled from (or at least rule out some “null hypothesis”).  But this would not change the main point that the resources for classical simulation, as far as we know, scale exponentially with the number of qubits and their quality. While we could perhaps with great effort simulate a 53 qubit depth 20 circuit classically, once we reach something like 100 qubits and depth then all current approaches will be hopelessly behind.</p>



<p>In the language of <a href="https://windowsontheory.org/2017/10/30/the-different-forms-of-quantum-computing-skepticism/">my essay on quantum skepticism</a>, I think this latest result, and the rest of the significant experimental progress that has been going on, all but rules out the possibility of “Skepticland”  where there would be some fundamental physical reason why it is not possible to build quantum computers that offer exponential advantage in the amount of resources to achieve certain tasks over classical computers.</p>



<p>While the worlds of “Popscitopia”  (quantum computers can do everything) and “Classicatopia” (there is an efficient classical algorithm to simulate BQP) remain mathematical possiblities (just as P=NP is), most likely we live in <strong>“Superiorita”</strong> where quantum computers do offer exponential advantage for <em>some</em> computational problems.</p>



<p>Some people question whether these kind of “special purpose” devices that might be very expensive to build are worth the investment. First of all (and most importantly for me), as I argued in <a href="https://windowsontheory.org/2017/10/30/the-different-forms-of-quantum-computing-skepticism/">my essay</a>, exploring the limits of physically realizable computation is a grand scientific goal in its own right worthy of investment regardless  of applications. Second, technology is now a <a href="https://www.gartner.com/en/newsroom/press-releases/2019-01-28-gartner-says-global-it-spending-to-reach--3-8-trillio#targetText=Worldwide%20IT%20spending%20is%20projected,latest%20forecast%20by%20Gartner%2C%20Inc.">3.8 trillion dollar </a>per year industry, and quantum computers are in a very real sense the first qualitatively different computing devices since the days of Babbage and Turing. Spending a fraction of a percent of the industry’s worth to the economy on exploring the potential for quantum computing seems like a good investment, even if there will be no practical application in the next decade or two. (By the same token, spending a fraction of a percent on exploring algorithm design and the limitations of <em>classical </em>algorithms is a very good investment as well.)</p></div>
    </content>
    <updated>2019-10-24T13:32:18Z</updated>
    <published>2019-10-24T13:32:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-10-30T10:21:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/142</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/142" rel="alternate" type="text/html"/>
    <title>TR19-142 |  Semi-Algebraic Proofs, IPS Lower Bounds and the $\tau$-Conjecture: Can a Natural Number be Negative? | 

	Yaroslav Alekseev, 

	Dima Grigoriev, 

	Edward Hirsch, 

	Iddo  Tzameret</title>
    <summary>We introduce the `binary value principle' which is a simple subset-sum instance expressing that a natural number written in binary cannot be negative, relating it to central problems in proof and algebraic complexity. We prove conditional superpolynomial lower bounds on the Ideal Proof System (IPS) refutation size of this instance, based on a well-known hypothesis by Shub and Smale about the hardness of computing factorials, where IPS is the strong algebraic proof system introduced by Grochow and Pitassi (2018). Conversely, we show that short IPS refutations of this instance bridge the gap between sufficiently strong algebraic and semi-algebraic proof systems. Our results extend to full-fledged IPS the paradigm introduced in Forbes et al. (2016), whereby lower bounds against subsystems of IPS were obtained using restricted algebraic circuit lower bounds, and demonstrate that the binary value principle captures the advantage of semi-algebraic over algebraic reasoning, for sufficiently strong systems. Specifically, we show the following:

*Conditional IPS lower bounds:* The Shub-Smale hypothesis (1995) implies a superpolynomial lower bound on the size of IPS refutations of the binary value principle over the rationals defined as the unsatisfiable linear equation $\sum_{i=1}^{n} 2^{i-1}x_i = -1$, for boolean $x_i$'s. Further, the related $\tau$-conjecture (1995) implies a superpolynomial lower bound on the size of IPS refutations of a variant of the binary value principle over the ring of rational functions. No prior conditional lower bounds were known for IPS or for apparently much weaker propositional proof systems such as Frege.

*Algebraic vs. semi-algebraic proofs:* Admitting short refutations of the binary value principle is necessary for any algebraic proof system to fully simulate any known semi-algebraic proof system, and for strong enough algebraic proof systems it is also sufficient. In particular, we introduce a very strong proof system that simulates all known semi-algebraic proof systems (and most other known concrete propositional proof systems), under the name Cone Proof System (CPS), as a semi-algebraic analogue of the ideal proof system: CPS establishes the unsatisfiability of collections of polynomial equalities and inequalities over the reals, by representing sum-of-squares proofs (and extensions) as algebraic circuits. We prove that IPS is polynomially equivalent to CPS iff IPS admits polynomial-size refutations of the binary value principle (for the language of systems of equations that have no 0/1-solutions), over both $\mathbb{Z}$ and $\mathbb{Q}$.</summary>
    <updated>2019-10-24T00:45:43Z</updated>
    <published>2019-10-24T00:45:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-30T10:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/141</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/141" rel="alternate" type="text/html"/>
    <title>TR19-141 |  On Rich $2$-to-$1$ Games | 

	Mark Braverman, 

	Subhash Khot, 

	Dor Minzer</title>
    <summary>We propose a variant of the $2$-to-$1$ Games Conjecture that we call the Rich $2$-to-$1$ Games Conjecture and show that it is equivalent to the Unique Games Conjecture. We are motivated by two considerations. Firstly, in light of the recent proof of the $2$-to-$1$ Games Conjecture, we hope to understand how one might make further progress towards a proof of the Unique Games Conjecture. Secondly, the new variant along with perfect completeness in addition, might imply hardness of approximation results that necessarily require perfect completeness and (hence) are not implied by the Unique Games Conjecture.</summary>
    <updated>2019-10-24T00:42:15Z</updated>
    <published>2019-10-24T00:42:15Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-30T10:20:29Z</updated>
    </source>
  </entry>
</feed>
