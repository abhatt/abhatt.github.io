<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-10-29T03:21:56Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12526</id>
    <link href="http://arxiv.org/abs/1910.12526" rel="alternate" type="text/html"/>
    <title>A* with Perfect Potentials</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Strasser:Ben.html">Ben Strasser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zeitz:Tim.html">Tim Zeitz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12526">PDF</a><br/><b>Abstract: </b>Quickly determining shortest paths in networks is an important ingredient for
many routing applications. While Dijkstra's algorithm can be used to solve
these problems, it is too slow for many practical problems. A* is an extension
to Dijkstra's algorithm. It uses a potential function to estimate the distance
of each node to the target. By adding these estimates to the queue keys, the
search is directed towards the target. The quality of this potential determines
the performance of A*. We introduce a novel way to efficiently calculate
perfect potentials for extended problem settings where a lower bound graph is
available. For example, in the case of routing with live traffic, this could be
the free flow graph.
</p></div>
    </summary>
    <updated>2019-10-29T01:32:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12504</id>
    <link href="http://arxiv.org/abs/1910.12504" rel="alternate" type="text/html"/>
    <title>The Multi-level Bottleneck Assignment Problem: Complexity and Solution Methods</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dokka:Trivikram.html">Trivikram Dokka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goerigk:Marc.html">Marc Goerigk</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12504">PDF</a><br/><b>Abstract: </b>We study the multi-level bottleneck assignment problem (MBA), which has
important applications in scheduling and quantitative finance. Given a weight
matrix, the task is to rearrange entries in each column such that the maximum
sum of values in each row is as small as possible. We analyze the complexity of
this problem in a generalized setting, where there are restrictions in how
values in columns can be permuted. We present a lower bound on its
approximability by giving a non-trivial gap reduction from three-dimensional
matching to MBA.
</p>
<p>To solve MBA, a greedy method has been used in the literature. We present new
solution methods based on an extension of the greedy method, an integer
programming formulation, and a column generation heuristic. In computational
experiments we show that it is possible to outperform the standard greedy
approach by around 10% on random instances.
</p></div>
    </summary>
    <updated>2019-10-29T01:20:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12490</id>
    <link href="http://arxiv.org/abs/1910.12490" rel="alternate" type="text/html"/>
    <title>Same-Cluster Querying for Overlapping Clusters</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huleihel:Wasim.html">Wasim Huleihel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mazumdar:Arya.html">Arya Mazumdar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/M=eacute=dard:Muriel.html">Muriel Médard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pal:Soumyabrata.html">Soumyabrata Pal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12490">PDF</a><br/><b>Abstract: </b>Overlapping clusters are common in models of many practical data-segmentation
applications. Suppose we are given $n$ elements to be clustered into $k$
possibly overlapping clusters, and an oracle that can interactively answer
queries of the form "do elements $u$ and $v$ belong to the same cluster?" The
goal is to recover the clusters with minimum number of such queries. This
problem has been of recent interest for the case of disjoint clusters. In this
paper, we look at the more practical scenario of overlapping clusters, and
provide upper bounds (with algorithms) on the sufficient number of queries. We
provide algorithmic results under both arbitrary (worst-case) and statistical
modeling assumptions. Our algorithms are parameter free, efficient, and work in
the presence of random noise. We also derive information-theoretic lower bounds
on the number of queries needed, proving that our algorithms are order optimal.
Finally, we test our algorithms over both synthetic and real-world data,
showing their practicality and effectiveness.
</p></div>
    </summary>
    <updated>2019-10-29T01:33:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12458</id>
    <link href="http://arxiv.org/abs/1910.12458" rel="alternate" type="text/html"/>
    <title>On the Degree of Boolean Functions as Polynomials over $\mathbb{Z}_m$</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Xiaoming.html">Xiaoming Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Yuan.html">Yuan Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Jiaheng.html">Jiaheng Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Kewen.html">Kewen Wu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xia:Zhiyu.html">Zhiyu Xia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zheng:Yufan.html">Yufan Zheng</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12458">PDF</a><br/><b>Abstract: </b>Polynomial representations of Boolean functions over various rings such as
$\mathbb{Z}$ and $\mathbb{Z}_m$ have been studied since Minsky and Papert
(1969). From then on, they have been employed in a large variety of fields
including communication complexity, circuit complexity, learning theory, coding
theory and so on. For any integer $m\ge2$, each Boolean function has a unique
multilinear polynomial representation over ring $\mathbb Z_m$. The degree of
such polynomial is called modulo-$m$ degree, denoted as
$\mathrm{deg}_m(\cdot)$.
</p>
<p>In this paper, we discuss the lower bound of modulo-$m$ degree of Boolean
functions. When $m=p^k$ ($k\ge 1$) for some prime $p$, we give a tight lower
bound that $\mathrm{deg}_m(f)\geq k(p-1)$ for any non-degenerated function
$f:\{0,1\}^n\to\{0,1\}$, provided that $n$ is sufficient large. When $m$
contains two different prime factors $p$ and $q$, we give a nearly optimal
lower bound for any symmetric function $f:\{0,1\}^n\to\{0,1\}$ that
$\mathrm{deg}_m(f) \geq \frac{n}{2+\frac{1}{p-1}+\frac{1}{q-1}}$.
</p>
<p>The idea of the proof is as follows. First we investigate properties of
polynomial representation of $\mathsf{MOD}$ function, then use it to span
symmetric Boolean functions to prove the lower bound for symmetric functions,
when $m$ is a prime power. Afterwards, Ramsey Theory is applied in order to
extend the bound from symmetric functions to non-degenerated ones. Finally, by
showing that $\mathrm{deg}_p(f)$ and $\mathrm{deg}_q(f)$ cannot be small
simultaneously, the lower bound for symmetric functions can be obtained when
$m$ is a composite but not prime power.
</p></div>
    </summary>
    <updated>2019-10-29T01:22:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12414</id>
    <link href="http://arxiv.org/abs/1910.12414" rel="alternate" type="text/html"/>
    <title>Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Lin.html">Lin Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esfandiari:Hossein.html">Hossein Esfandiari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fu:Thomas.html">Thomas Fu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mirrokni:Vahab_S=.html">Vahab S. Mirrokni</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12414">PDF</a><br/><b>Abstract: </b>Computing approximate nearest neighbors in high dimensional spaces is a
central problem in large-scale data mining with a wide range of applications in
machine learning and data science. A popular and effective technique in
computing nearest neighbors approximately is the locality-sensitive hashing
(LSH) scheme. In this paper, we aim to develop LSH schemes for distance
functions that measure the distance between two probability distributions,
particularly for f-divergences as well as a generalization to capture mutual
information loss. First, we provide a general framework to design LHS schemes
for f-divergence distance functions and develop LSH schemes for the generalized
Jensen-Shannon divergence and triangular discrimination in this framework. We
show a two-sided approximation result for approximation of the generalized
Jensen-Shannon divergence by the Hellinger distance, which may be of
independent interest. Next, we show a general method of reducing the problem of
designing an LSH scheme for a Krein kernel (which can be expressed as the
difference of two positive definite kernels) to the problem of maximum inner
product search. We exemplify this method by applying it to the mutual
information loss, due to its several important applications such as model
compression.
</p></div>
    </summary>
    <updated>2019-10-29T01:41:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12375</id>
    <link href="http://arxiv.org/abs/1910.12375" rel="alternate" type="text/html"/>
    <title>Towards a theory of non-commutative optimization: geodesic first and second order methods for moment maps and polytopes</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=uuml=rgisser:Peter.html">Peter Bürgisser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Franks:Cole.html">Cole Franks</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Ankit.html">Ankit Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oliveira:Rafael.html">Rafael Oliveira</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Walter:Michael.html">Michael Walter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wigderson:Avi.html">Avi Wigderson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12375">PDF</a><br/><b>Abstract: </b>This paper initiates a systematic development of a theory of non-commutative
optimization. It aims to unify and generalize a growing body of work from the
past few years which developed and analyzed algorithms for natural geodesically
convex optimization problems on Riemannian manifolds that arise from the
symmetries of non-commutative groups. These algorithms minimize the moment map
(a non-commutative notion of the usual gradient) and test membership in moment
polytopes (a vast class of polytopes, typically of exponential vertex and facet
complexity, which arise from this a-priori non-convex, non-linear setting).
This setting captures a diverse set of problems in different areas of computer
science, mathematics, and physics. Several of them were solved efficiently for
the first time using non-commutative methods; the corresponding algorithms also
lead to solutions of purely structural problems and to many new connections
between disparate fields.
</p>
<p>In the spirit of standard convex optimization, we develop two general methods
in the geodesic setting, a first order and a second order method, which
respectively receive first and second order information on the "derivatives" of
the function to be optimized. These in particular subsume all past results. The
main technical work, again unifying and extending much of the previous
literature, goes into identifying the key parameters of the underlying group
actions which control convergence to the optimum in each of these methods.
These non-commutative analogues of "smoothness" are far more complex and
require significant algebraic and analytic machinery. Despite this complexity,
the way in which these parameters control convergence in both methods is quite
simple and elegant. We show how bound these parameters in several general
cases. Our work points to intriguing open problems and suggests further
research directions.
</p></div>
    </summary>
    <updated>2019-10-29T01:24:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12359</id>
    <link href="http://arxiv.org/abs/1910.12359" rel="alternate" type="text/html"/>
    <title>Chatter Diagnosis in Milling Using Supervised Learning and Topological Features Vector</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yesilli:Melih_C=.html">Melih C. Yesilli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tymochko:Sarah.html">Sarah Tymochko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khasawneh:Firas_A=.html">Firas A. Khasawneh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munch:Elizabeth.html">Elizabeth Munch</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12359">PDF</a><br/><b>Abstract: </b>Chatter detection has become a prominent subject of interest due to its
effect on cutting tool life, surface finish and spindle of machine tool. Most
of the existing methods in chatter detection literature are based on signal
processing and signal decomposition. In this study, we use topological features
of data simulating cutting tool vibrations, combined with four supervised
machine learning algorithms to diagnose chatter in the milling process.
Persistence diagrams, a method of representing topological features, are not
easily used in the context of machine learning, so they must be transformed
into a form that is more amenable. Specifically, we will focus on two different
methods for featurizing persistence diagrams, Carlsson coordinates and template
functions. In this paper, we provide classification results for simulated data
from various cutting configurations, including upmilling and downmilling, in
addition to the same data with some added noise. Our results show that Carlsson
Coordinates and Template Functions yield accuracies as high as 96% and 95%,
respectively. We also provide evidence that these topological methods are noise
robust descriptors for chatter detection.
</p></div>
    </summary>
    <updated>2019-10-29T01:48:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12353</id>
    <link href="http://arxiv.org/abs/1910.12353" rel="alternate" type="text/html"/>
    <title>On the Parameterized Complexity of Sparsest Cut and Small-set Expansion Problems</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Javadi:Ramin.html">Ramin Javadi</a>, Amir Nikabadi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12353">PDF</a><br/><b>Abstract: </b>We study the NP-hard \textsc{$k$-Sparsest Cut} problem ($k$SC) in which,
given an undirected graph $G = (V, E)$ and a parameter $k$, the objective is to
partition vertex set into $k$ subsets whose maximum edge expansion is
minimized. Herein, the edge expansion of a subset $S \subseteq V$ is defined as
the sum of the weights of edges exiting $S$ divided by the number of vertices
in $S$. Another problem that has been investigated is \textsc{$k$-Small-Set
Expansion} problem ($k$SSE), which aims to find a subset with minimum edge
expansion with a restriction on the size of the subset. We extend previous
studies on $k$SC and $k$SSE by inspecting their parameterized complexity. On
the positive side, we present two FPT algorithms for both $k$SSE and 2SC
problems where in the first algorithm we consider the parameter treewidth of
the input graph and uses exponential space, and in the second we consider the
parameter vertex cover number of the input graph and uses polynomial space.
Moreover, we consider the unweighted version of the $k$SC problem where $k \geq
2$ is fixed and proposed two FPT algorithms with parameters treewidth and
vertex cover number of the input graph. We also propose a randomized FPT
algorithm for $k$SSE when parameterized by $k$ and the maximum degree of the
input graph combined. Its derandomization is done efficiently.
</p>
<p>\noindent On the negative side, first we prove that for every fixed integer
$k,\tau\geq 3$, the problem $k$SC is NP-hard for graphs with vertex cover
number at most $\tau$. We also show that $k$SC is W[1]-hard when parameterized
by the treewidth of the input graph and the number~$k$ of components combined
using a reduction from \textsc{Unary Bin Packing}. Furthermore, we prove that
$k$SC remains NP-hard for graphs with maximum degree three and also graphs with
degeneracy two. Finally, we prove that the unweighted $k$SSE is W[1]-hard for
the parameter $k$.
</p></div>
    </summary>
    <updated>2019-10-29T01:20:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12340</id>
    <link href="http://arxiv.org/abs/1910.12340" rel="alternate" type="text/html"/>
    <title>Cilkmem: Algorithms for Analyzing the Memory High-Water Mark of Fork-Join Parallel Programs</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaler:Tim.html">Tim Kaler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuszmaul:William.html">William Kuszmaul</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schardl:Tao_B=.html">Tao B. Schardl</a>, Daniele Vettorel <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12340">PDF</a><br/><b>Abstract: </b>Software engineers designing recursive fork-join programs destined to run on
massively parallel computing systems must be cognizant of how their program's
memory requirements scale in a many-processor execution. Although tools exist
for measuring memory usage during one particular execution of a parallel
program, such tools cannot bound the worst-case memory usage over all possible
parallel executions.
</p>
<p>This paper introduces Cilkmem, a tool that analyzes the execution of a
deterministic Cilk program to determine its $p$-processor memory high-water
mark (MHWM), which is the worst-case memory usage of the program over \emph{all
possible} $p$-processor executions. Cilkmem employs two new algorithms for
computing the $p$-processor MHWM. The first algorithm calculates the exact
$p$-processor MHWM in $O(T_1 \cdot p)$ time, where $T_1$ is the total work of
the program. The second algorithm solves, in $O(T_1)$ time, the approximate
threshold problem, which asks, for a given memory threshold $M$, whether the
$p$-processor MHWM exceeds $M/2$ or whether it is guaranteed to be less than
$M$. Both algorithms are memory efficient, requiring $O(p \cdot D)$ and $O(D)$
space, respectively, where $D$ is the maximum call-stack depth of the program's
execution on a single thread.
</p>
<p>Our empirical studies show that Cilkmem generally exhibits low overheads.
Across ten application benchmarks from the Cilkbench suite, the exact algorithm
incurs a geometric-mean multiplicative overhead of $1.54$ for $p=128$, whereas
the approximation-threshold algorithm incurs an overhead of $1.36$ independent
of $p$. In addition, we use Cilkmem to reveal and diagnose a previously unknown
issue in a large image-alignment program contributing to unexpectedly high
memory usage under parallel executions.
</p></div>
    </summary>
    <updated>2019-10-29T01:32:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12310</id>
    <link href="http://arxiv.org/abs/1910.12310" rel="alternate" type="text/html"/>
    <title>Semi-Asymmetric Parallel Graph Algorithms for NVRAMs</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dhulipala:Laxman.html">Laxman Dhulipala</a>, Charlie McGuffey, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kang:Hongbo.html">Hongbo Kang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gu:Yan.html">Yan Gu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blelloch:Guy_E=.html">Guy E. Blelloch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gibbons:Phillip_B=.html">Phillip B. Gibbons</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shun:Julian.html">Julian Shun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12310">PDF</a><br/><b>Abstract: </b>Emerging non-volatile main memory (NVRAM) technologies provide novel features
for large-scale graph analytics, combining byte-addressability, low idle power,
and improved memory-density. Systems are likely to have an order of magnitude
more NVRAM than traditional memory (DRAM), allowing large graph problems to be
solved efficiently at a modest cost on a single machine. However, a significant
challenge in achieving high performance is in accounting for the fact that
NVRAM writes can be significantly more expensive than NVRAM reads.
</p>
<p>In this paper, we propose an approach to parallel graph analytics in which
the graph is stored as a read-only data structure (in NVRAM), and the amount of
mutable memory is kept proportional to the number of vertices. Similar to the
popular semi-external and semi-streaming models for graph analytics, the
approach assumes that the vertices of the graph fit in a fast read-write memory
(DRAM), but the edges do not. In NVRAM systems, our approach eliminates writes
to the NVRAM, among other benefits.
</p>
<p>We present a model, the Parallel Semi-Asymmetric Model (PSAM), to analyze
algorithms in the setting, and run experiments on a 48-core NVRAM system to
validate the effectiveness of these algorithms. To this end, we study over a
dozen graph problems. We develop parallel algorithms for each that are
efficient, often work-optimal, in the model. Experimentally, we run all of the
algorithms on the largest publicly-available graph and show that our PSAM
algorithms outperform the fastest prior algorithms designed for DRAM or NVRAM.
We also show that our algorithms running on NVRAM nearly match the fastest
prior algorithms running solely in DRAM, by effectively hiding the costs of
repeatedly accessing NVRAM versus DRAM.
</p></div>
    </summary>
    <updated>2019-10-29T01:28:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12177</id>
    <link href="http://arxiv.org/abs/1910.12177" rel="alternate" type="text/html"/>
    <title>Computing a Geodesic Two-Center of Points in a Simple Polygon</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oh:Eunjin.html">Eunjin Oh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bae:Sang_Won.html">Sang Won Bae</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahn:Hee=Kap.html">Hee-Kap Ahn</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12177">PDF</a><br/><b>Abstract: </b>Given a simple polygon $P$ and a set $Q$ of points contained in $P$, we
consider the geodesic $k$-center problem where we want to find $k$ points,
called \emph{centers}, in $P$ to minimize the maximum geodesic distance of any
point of $Q$ to its closest center. In this paper, we focus on the case for
$k=2$ and present the first exact algorithm that efficiently computes an
optimal $2$-center of $Q$ with respect to the geodesic distance in $P$.
</p></div>
    </summary>
    <updated>2019-10-29T01:49:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12172</id>
    <link href="http://arxiv.org/abs/1910.12172" rel="alternate" type="text/html"/>
    <title>Near-Optimal Bounds for Online Caching with Machine Learned Advice</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohatgi:Dhruv.html">Dhruv Rohatgi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12172">PDF</a><br/><b>Abstract: </b>In the model of online caching with machine learned advice, introduced by
Lykouris and Vassilvitskii, the goal is to solve the caching problem with an
online algorithm that has access to next-arrival predictions: when each input
element arrives, the algorithm is given a prediction of the next time when the
element will reappear. The traditional model for online caching suffers from an
$\Omega(\log k)$ competitive ratio lower bound (on a cache of size $k$). In
contrast, the augmented model admits algorithms which beat this lower bound
when the predictions have low error, and asymptotically match the lower bound
when the predictions have high error, even if the algorithms are oblivious to
the prediction error. In particular, Lykouris and Vassilvitskii showed that
there is a prediction-augmented caching algorithm with a competitive ratio of
$O(1+\min(\sqrt{\eta/OPT}, \log k))$ when the overall $\ell_1$ prediction error
is bounded by $\eta$, and $OPT$ is the cost of the optimal offline algorithm.
</p>
<p>The dependence on $k$ in the competitive ratio is optimal, but the dependence
on $\eta/OPT$ may be far from optimal. In this work, we make progress towards
closing this gap. Our contributions are twofold. First, we provide an improved
algorithm with a competitive ratio of $O(1 + \min((\eta/OPT)/k, 1) \log k)$.
Second, we provide a lower bound of $\Omega(\log \min((\eta/OPT)/(k \log k),
k))$.
</p></div>
    </summary>
    <updated>2019-10-29T01:28:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12169</id>
    <link href="http://arxiv.org/abs/1910.12169" rel="alternate" type="text/html"/>
    <title>Computing the Center Region and Its Variants</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oh:Eunjin.html">Eunjin Oh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahn:Hee=Kap.html">Hee-Kap Ahn</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12169">PDF</a><br/><b>Abstract: </b>We present an $O(n^2\log^4 n)$-time algorithm for computing the center region
of a set of $n$ points in the three-dimensional Euclidean space. This improves
the previously best known algorithm by Agarwal, Sharir and Welzl, which takes
$O(n^{2+\epsilon})$ time for any $\epsilon &gt; 0$. It is known that the
combinatorial complexity of the center region is $\Omega(n^2)$ in the worst
case, thus our algorithm is almost tight. We also consider the problem of
computing a colored version of the center region in the two-dimensional
Euclidean space and present an $O(n\log^4 n)$-time algorithm.
</p></div>
    </summary>
    <updated>2019-10-29T01:49:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12085</id>
    <link href="http://arxiv.org/abs/1910.12085" rel="alternate" type="text/html"/>
    <title>On the Classical Hardness of Spoofing Linear Cross-Entropy Benchmarking</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aaronson:Scott.html">Scott Aaronson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gunn:Sam.html">Sam Gunn</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12085">PDF</a><br/><b>Abstract: </b>Recently, Google announced the first demonstration of quantum computational
supremacy with a programmable superconducting processor. Their demonstration is
based on collecting samples from the output distribution of a noisy random
quantum circuit, then applying a statistical test to those samples called
Linear Cross-Entropy Benchmarking (Linear XEB). This raises a theoretical
question: how hard is it for a classical computer to spoof the results of the
Linear XEB test? In this short note, we adapt an analysis of Aaronson and Chen
[2017] to prove a conditional hardness result for Linear XEB spoofing.
Specifically, we show that the problem is classically hard, assuming that there
is no efficient classical algorithm that, given a random n-qubit quantum
circuit C, estimates the probability of C outputting a specific output string,
say 0^n, with variance even slightly better than that of the trivial estimator
that always estimates 1/2^n. Our result automatically encompasses the case of
noisy circuits.
</p></div>
    </summary>
    <updated>2019-10-29T01:21:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12050</id>
    <link href="http://arxiv.org/abs/1910.12050" rel="alternate" type="text/html"/>
    <title>Facility Location Problem in Differential Privacy Model Revisited</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Yunus Esencayi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gaboardi:Marco.html">Marco Gaboardi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Shi.html">Shi Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Di.html">Di Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12050">PDF</a><br/><b>Abstract: </b>In this paper we study the uncapacitated facility location problem in the
model of differential privacy (DP) with uniform facility cost. Specifically, we
first show that, under the hierarchically well-separated tree (HST) metrics and
the super-set output setting that was introduced in Gupta et. al., there is an
$\epsilon$-DP algorithm that achieves an $O(\frac{1}{\epsilon})$(expected
multiplicative) approximation ratio; this implies an $O(\frac{\log
n}{\epsilon})$ approximation ratio for the general metric case, where $n$ is
the size of the input metric. These bounds improve the best-known results given
by Gupta et. al. In particular, our approximation ratio for HST-metrics is
independent of $n$, and the ratio for general metrics is independent of the
aspect ratio of the input metric. On the negative side, we show that the
approximation ratio of any $\epsilon$-DP algorithm is lower bounded by
$\Omega(\frac{1}{\sqrt{\epsilon}})$, even for instances on HST metrics with
uniform facility cost, under the super-set output setting. The lower bound
shows that the dependence of the approximation ratio for HST metrics on
$\epsilon$ can not be removed or greatly improved. Our novel methods and
techniques for both the upper and lower bound may find additional applications.
</p></div>
    </summary>
    <updated>2019-10-29T01:45:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.12026</id>
    <link href="http://arxiv.org/abs/1910.12026" rel="alternate" type="text/html"/>
    <title>On the Hardness of Energy Minimisation for Crystal Structure Prediction</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Duncan Adamson, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deligkas:Argyrios.html">Argyrios Deligkas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gusev:Vladimir.html">Vladimir Gusev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potapov:Igor.html">Igor Potapov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.12026">PDF</a><br/><b>Abstract: </b>Crystal Structure Prediction (csp) is one of the central and most challenging
problems in materials science and computational chemistry. In csp, the goal is
to find a configuration of ions in 3D space that yields the lowest potential
energy. Finding an efficient procedure to solve this complex optimisation
question is a well known open problem in computational chemistry. Due to the
exponentially large search space, the problem has been referred in several
materials-science papers as ''NP-Hard and very challenging'' without any formal
proof though. This paper fills a gap in the literature providing the first set
of formally proven NP-Hardness results for a variant of csp with various
realistic constraints. In particular, we focus on the problem of removal: the
goal is to find a substructure with minimal potential energy, by removing a
subset of the ions from a given initial structure. Our main contributions are
NP-Hardness results for the csp removal problem, new embeddings of
combinatorial graph problems into geometrical settings, and a more systematic
exploration of the energy function to reveal the complexity of csp. In a wider
context, our results contribute to the analysis of computational problems for
weighted graphs embedded into the three-dimensional Euclidean space.
</p></div>
    </summary>
    <updated>2019-10-29T01:22:05Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11993</id>
    <link href="http://arxiv.org/abs/1910.11993" rel="alternate" type="text/html"/>
    <title>Selection on $X_1+X_2+\cdots + X_m$ with layer-ordered heaps</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kreitzberg:Patrick.html">Patrick Kreitzberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lucke:Kyle.html">Kyle Lucke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Serang:Oliver.html">Oliver Serang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11993">PDF</a><br/><b>Abstract: </b>Selection on $X_1+X_2+\cdots + X_m$ is an important problem with many
applications in areas such as max-convolution, max-product Bayesian inference,
calculating most probable isotopes, and computing non-parametric test
statistics, among others. Faster-than-na\"{i}ve approaches exist for $m=2$:
Johnson \&amp; Mizoguchi (1978) find the smallest $k$ values in $A+B$ with runtime
$O(n \log(n))$. Frederickson \&amp; Johnson (1982) created a method for finding the
$k$ smallest values in $A+B$ with runtime $O(n +
\min(k,n)\log(\frac{k}{\min(k,n)}))$. In 1993, Frederickson published an
optimal algorithm for selection on $A+B$, which runs in $O(n+k)$. In 2018,
Kaplan \emph{et al.} described another optimal algorithm in terms Chazelle's of
soft heaps. No fast methods exist for $m&gt;2$. Johnson \&amp; Mizoguchi (1978)
introduced a method to compute the minimal $k$ terms when $m&gt;2$, but that
method runs in $O(m\cdot n^{\frac{m}{2}} \log(n))$ and is inefficient when $m
\gg 1$.
</p>
<p>In this paper, we introduce the first efficient methods for problems where
$m&gt;2$. We introduce the ``layer-ordered heap,'' a simple special class of heap
with which we produce a new, fast selection algorithm on the Cartesian product.
Using this new algorithm to perform $k$-selection on the Cartesian product of
$m$ arrays of length $n$ has runtime $\in o(m\cdot n + k\cdot m)$. We also
provide implementations of the algorithms proposed and their performance in
practice.
</p></div>
    </summary>
    <updated>2019-10-29T01:32:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11957</id>
    <link href="http://arxiv.org/abs/1910.11957" rel="alternate" type="text/html"/>
    <title>A Deterministic Linear Program Solver in Current Matrix Multiplication Time</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brand:Jan_van_den.html">Jan van den Brand</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11957">PDF</a><br/><b>Abstract: </b>Interior point algorithms for solving linear programs have been studied
extensively for a long time [e.g. Karmarkar 1984; Lee, Sidford FOCS'14; Cohen,
Lee, Song STOC'19]. For linear programs of the form $\min_{Ax=b, x \ge 0}
c^\top x$ with $n$ variables and $d$ constraints, the generic case $d =
\Omega(n)$ has recently been settled by Cohen, Lee and Song [STOC'19]. Their
algorithm can solve linear programs in $\tilde O(n^\omega \log(n/\delta))$
expected time, where $\delta$ is the relative accuracy. This is essentially
optimal as all known linear system solvers require up to $O(n^{\omega})$ time
for solving $Ax = b$. However, for the case of deterministic solvers, the best
upper bound is Vaidya's 30 years old $O(n^{2.5} \log(n/\delta))$ bound
[FOCS'89]. In this paper we show that one can also settle the deterministic
setting by derandomizing Cohen et al.'s $\tilde{O}(n^\omega \log(n/\delta))$
time algorithm. This allows for a strict $\tilde{O}(n^\omega \log(n/\delta))$
time bound, instead of an expected one, and a simplified analysis, reducing the
length of their proof of their central path method by roughly half.
Derandomizing this algorithm was also an open question asked in Song's PhD
Thesis.
</p>
<p>The main tool to achieve our result is a new data-structure that can maintain
the solution to a linear system in subquadratic time. More accurately we are
able to maintain $\sqrt{U}A^\top(AUA^\top)^{-1}A\sqrt{U}\:v$ in subquadratic
time under $\ell_2$ multiplicative changes to the diagonal matrix $U$ and the
vector $v$. This type of change is common for interior point algorithms.
Previous algorithms [e.g. Vaidya STOC'89; Lee, Sidford FOCS'15; Cohen, Lee,
Song STOC'19] required $\Omega(n^2)$ time for this task. [...]
</p></div>
    </summary>
    <updated>2019-10-29T01:45:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11921</id>
    <link href="http://arxiv.org/abs/1910.11921" rel="alternate" type="text/html"/>
    <title>Equivalence of Systematic Linear Data Structures and Matrix Rigidity</title>
    <feedworld_mtime>1572307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramamoorthy:Sivaramakrishnan_Natarajan.html">Sivaramakrishnan Natarajan Ramamoorthy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rashtchian:Cyrus.html">Cyrus Rashtchian</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11921">PDF</a><br/><b>Abstract: </b>Recently, Dvir, Golovnev, and Weinstein have shown that sufficiently strong
lower bounds for linear data structures would imply new bounds for rigid
matrices. However, their result utilizes an algorithm that requires an $NP$
oracle, and hence, the rigid matrices are not explicit. In this work, we derive
an equivalence between rigidity and the systematic linear model of data
structures. For the $n$-dimensional inner product problem with $m$ queries, we
prove that lower bounds on the query time imply rigidity lower bounds for the
query set itself. In particular, an explicit lower bound of
$\omega\left(\frac{n}{r}\log m\right)$ for $r$ redundant storage bits would
yield better rigidity parameters than the best bounds due to Alon, Panigrahy,
and Yekhanin. We also prove a converse result, showing that rigid matrices
directly correspond to hard query sets for the systematic linear model. As an
application, we prove that the set of vectors obtained from rank one binary
matrices is rigid with parameters matching the known results for explicit sets.
This implies that the vector-matrix-vector problem requires query time
$\Omega(n^{3/2}/r)$ for redundancy $r \geq \sqrt{n}$ in the systematic linear
model, improving a result of Chakraborty, Kamma, and Larsen. Finally, we prove
a cell probe lower bound for the vector-matrix-vector problem in the high error
regime, improving a result of Chattopadhyay, Kouck\'{y}, Loff, and
Mukhopadhyay.
</p></div>
    </summary>
    <updated>2019-10-29T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3222567344389850699</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3222567344389850699/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/09/random-non-partisan-thoughts-on-prez.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3222567344389850699" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3222567344389850699" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/09/random-non-partisan-thoughts-on-prez.html" rel="alternate" type="text/html"/>
    <title>Random non-partisan thoughts on the Prez Election</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
This post is non-partisan, but in the interest of full disclosure I disclose that I will almost surely be voting for the Democratic Nominee. And I say <i>almost surely</i> because very weird things could happen.I can imagine a republican saying, in 2015 <i>I will almost surely be voting for the Republican Nominee</i> and then later deciding to not vote for Trump. <br/>
<br/>
<br/>
<i>My Past Predictions</i>: Early on in 2007 I predicted it would be Obama vs McCain and that Obama would win. Was I smart or lucky? Early in 2011 I predicted Paul Ryan would be the Rep. Candidate. Early in 2015 and even into 2016 I predicted  that Trump would not get the nomination. After he got the nomination I predicted  he would not become president. So, in answer to my first question, I was lucky not smart. Having said all of this I predict that the Dem. candidate will be Warren. Note- this is an honest prediction, not one fueled by what I want to see happen. I predict Warren since she seems to be someone who can bridge the so-called establishment and the so-called left (I dislike the terms LEFT and RIGHT since issues and views change over time). Given my past record I would not take me too seriously. Also, since this prediction is not particularly unusual, if I am right this would NOT be impressive (My Obama prediction was impressive, and my Paul Ryan prediction would have been very impressive had I been right.)<br/>
<br/>
<i>Electability</i>: My spell checker doesn't think its a word. Actually it shouldn't be a word. It's a stupid concept. Recall<br/>
<br/>
JFK was unelectable since he was Catholic. <br/>
<br/>
Ronald Reagan was unelectable because he was too conservative.<br/>
<br/>
A draft dodging adulterer named Bill Clinton could not possible beat a sitting president who just won a popular war.<br/>
<br/>
Nobody named Barack Hussein Obama, who is half-black,  could possibly get the nomination, never mind the presidency. And Hillary had the nomination locked up in 2008--- she had no any serious challengers. <br/>
<br/>
(An article in <i>The New Republic</i> in 2007 predicted a brokered convention for the Republicans where Fred Thompson, Mitt Romney, and Rudy Guilliani would split the vote, and at the same time a cake walk for Hillary Clinton with<br/>
Barak Obama winning Illinois in the primaries but not much else. Recall that 2008 was McCain vs Obama.)<br/>
<br/>
Donald Trump will surely be stopped from getting the nomination because, in the end, <a href="https://www.amazon.com/Party-Decides-Presidential-Nominations-American/dp/0226112373/ref=cm_cr_arp_d_product_top?ie=UTF8">The Party Decides</a>.<br/>
<br/>
Republican voters in 2016  will prefer  Rubio to Trump since Marco is more electable AND more conservative. Hence, in the space of Rep. Candidates, Rubio dominates Trump. So, by simple game theory, Trump can't get the nomination.  The more electable Rubio, in the 2016 primaries, won Minnesota, Wash DC,  and Puerto Rico (Puerto Rico has a primary. Really!) One of my friends thought he also won Guam (Guam?) but I could not find evidence of that on the web. Okay, so why did Trump win? <i>Because voters are not game theorists.</i><br/>
<br/>
ANYWAY, my point is that how can anyone take the notion of electability seriously when unelectable people have gotten elected?<br/>
<br/>
<i>Primaries</i>: Dem primary  voters are torn between who they want to be president and who can beat Trump.  Since its so hard to tell who can beat who, I would recommend voting for who you like and not say stupid things like<br/>
<br/>
American would never elect  a 76 year old socialist whose recently had a heart attack.<br/>
<br/>
or<br/>
<br/>
Trump beat a women in 2016 so we can't nominate a women<br/>
<br/>
or<br/>
<br/>
America is not ready to elect a gay president yet. (America is never ready to do X until after it does X and then the pundits ret-con their opinions.For example, of course America is ready for Gay-Marriage. Duh.)<br/>
<br/>
<i>Who won the debate?<br/>
</i> Whoever didn't bother watching it :-). I think the question is stupid and has become who got out a clever sound bite. We need sound policy, not sound bites!</div>
    </content>
    <updated>2019-10-28T14:05:00Z</updated>
    <published>2019-10-28T14:05:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-10-28T18:54:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11850</id>
    <link href="http://arxiv.org/abs/1910.11850" rel="alternate" type="text/html"/>
    <title>Tight Running Time Lower Bounds for Strong Inapproximability of Maximum $k$-Coverage, Unique Set Cover and Related Problems (via $t$-Wise Agreement Testing Theorem)</title>
    <feedworld_mtime>1572220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manurangsi:Pasin.html">Pasin Manurangsi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11850">PDF</a><br/><b>Abstract: </b>We show, assuming the (randomized) Gap Exponential Time Hypothesis (Gap-ETH),
that the following tasks cannot be done in $T(k) \cdot N^{o(k)}$-time for any
function $T$ where $N$ denote the input size:
</p>
<p>- $\left(1 - \frac{1}{e} + \epsilon\right)$-approximation for Max
$k$-Coverage for any $\epsilon &gt; 0$,
</p>
<p>- $\left(1 + \frac{2}{e} - \epsilon\right)$-approximation for $k$-Median (in
general metrics) for any constant $\epsilon &gt; 0$.
</p>
<p>- $\left(1 + \frac{8}{e} - \epsilon\right)$-approximation for $k$-Mean (in
general metrics) for any constant $\epsilon &gt; 0$.
</p>
<p>- Any constant factor approximation for $k$-Unique Set Cover, $k$-Nearest
Codeword Problem and $k$-Closest Vector Problem.
</p>
<p>- $(1 + \delta)$-approximation for $k$-Minimum Distance Problem and
$k$-Shortest Vector Problem for some $\delta &gt; 0$.
</p>
<p>Since these problems can be trivially solved in $N^{O(k)}$ time, our running
time lower bounds are essentially tight. In terms of approximation ratios, Max
$k$-Coverage is well-known to admit polynomial-time $\left(1 -
\frac{1}{e}\right)$-approximation algorithms, and, recently, it was shown that
$k$-Median and $k$-Mean are approximable to within factors of $\left(1 +
\frac{2}{e}\right)$ and $\left(1 + \frac{8}{e}\right)$ respectively in FPT time
[Cohen-Addad et al. 2019]; hence, our inapproximability ratios are also tight
for these three problems. For the remaining problems, no non-trivial FPT
approximation algorithms are known.
</p>
<p>The starting point of all our hardness results mentioned above is the Label
Cover problem (with projection constraints). We show that Label Cover cannot be
approximated to within any constant factor in $T(k) \cdot N^{o(k)}$ time, where
$N$ and $k$ denote the size of the input and the number of nodes on the side
with the larger alphabet respectively. With this hardness, the above results
follow immediately from known reductions...
</p></div>
    </summary>
    <updated>2019-10-28T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11848</id>
    <link href="http://arxiv.org/abs/1910.11848" rel="alternate" type="text/html"/>
    <title>Finite Boolean Algebras for Solid Geometry using Julia's Sparse Arrays</title>
    <feedworld_mtime>1572220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paoluzzi:Alberto.html">Alberto Paoluzzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shapiro:Vadim.html">Vadim Shapiro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/DiCarlo:Antonio.html">Antonio DiCarlo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scorzelli:Giorgio.html">Giorgio Scorzelli</a>, Elia Onofri <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11848">PDF</a><br/><b>Abstract: </b>The goal of this paper is to introduce a new method in computer-aided
geometry of solid modeling. We put forth a novel algebraic technique to
evaluate any variadic expression between polyhedral d-solids (d = 2, 3) with
regularized operators of union, intersection, and difference, i.e., any CSG
tree. The result is obtained in three steps: first, by computing an independent
set of generators for the d-space partition induced by the input; then, by
reducing the solid expression to an equivalent logical formula between Boolean
terms made by zeros and ones; and, finally, by evaluating this expression using
bitwise operators. This method is implemented in Julia using sparse arrays. The
computational evaluation of every possible solid expression, usually denoted as
CSG (Constructive Solid Geometry), is reduced to an equivalent logical
expression of a finite set algebra over the cells of a space partition, and
solved by native bitwise operators.
</p></div>
    </summary>
    <updated>2019-10-28T23:28:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11826</id>
    <link href="http://arxiv.org/abs/1910.11826" rel="alternate" type="text/html"/>
    <title>Weighted Quasi Interpolant Spline Approximations: Properties and Applications</title>
    <feedworld_mtime>1572220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raffo:Andrea.html">Andrea Raffo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biasotti:Silvia.html">Silvia Biasotti</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11826">PDF</a><br/><b>Abstract: </b>Continuous representations are fundamental for modeling sampled data so that
computations and numerical simulations can be performed directly on the model
or its elements. To effectively and efficiently address the approximation of
perturbed point clouds we propose the Weighted Quasi Interpolant Spline
Approximation method (wQISA). We provide global and local bounds of the method
and discuss how it still preserves the shape properties of the classical
quasi-interpolation scheme. This approach is particularly useful when data
noise can be represented as a probabilistic distribution: from the point of
view of nonparametric regression, the wQISA estimator is robust to random data
perturbation such as noise and outliers. Finally, we show the effectiveness of
the method with several numerical simulations on real data, including curve
fitting on images, surface approximation and simulation of rainfall
precipitations.
</p></div>
    </summary>
    <updated>2019-10-28T23:28:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11782</id>
    <link href="http://arxiv.org/abs/1910.11782" rel="alternate" type="text/html"/>
    <title>Optimal Orthogonal Drawings of Planar 3-Graphs in Linear Time</title>
    <feedworld_mtime>1572220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Didimo:Walter.html">Walter Didimo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liotta:Giuseppe.html">Giuseppe Liotta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ortali:Giacomo.html">Giacomo Ortali</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Patrignani:Maurizio.html">Maurizio Patrignani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11782">PDF</a><br/><b>Abstract: </b>A planar orthogonal drawing $\Gamma$ of a planar graph $G$ is a geometric
representation of $G$ such that the vertices are drawn as distinct points of
the plane, the edges are drawn as chains of horizontal and vertical segments,
and no two edges intersect except at their common end-points. A bend of
$\Gamma$ is a point of an edge where a horizontal and a vertical segment meet.
$\Gamma$ is bend-minimum if it has the minimum number of bends over all
possible planar orthogonal drawings of $G$. This paper addresses a long
standing, widely studied, open question: Given a planar 3-graph $G$ (i.e., a
planar graph with vertex degree at most three), what is the best computational
upper bound to compute a bend-minimum planar orthogonal drawing of $G$ in the
variable embedding setting? In this setting the algorithm can choose among the
exponentially many planar embeddings of $G$ the one that leads to an orthogonal
drawing with the minimum number of bends. We answer the question by describing
an $O(n)$-time algorithm that computes a bend-minimum planar orthogonal drawing
of $G$ with at most one bend per edge, where $n$ is the number of vertices of
$G$. The existence of an orthogonal drawing algorithm that simultaneously
minimizes the total number of bends and the number of bends per edge was
previously unknown.
</p></div>
    </summary>
    <updated>2019-10-28T23:22:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11754</id>
    <link href="http://arxiv.org/abs/1910.11754" rel="alternate" type="text/html"/>
    <title>Overlay Indexes: Efficiently Supporting Aggregate Range Queries and Authenticated Data Structures in Off-the-Shelf Databases</title>
    <feedworld_mtime>1572220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pennino:Diego.html">Diego Pennino</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pizzonia:Maurizio.html">Maurizio Pizzonia</a>, Alessio Papi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11754">PDF</a><br/><b>Abstract: </b>Commercial off-the-shelf DataBase Management Systems (DBMSes) are highly
optimized to process a wide range of queries by means of carefully designed
indexing and query planning. However, many aggregate range queries are usually
performed by DBMSes using sequential scans, and certain needs, like storing
Authenticated Data Structures (ADS), are not supported at all. Theoretically,
these needs could be efficiently fulfilled adopting specific kinds of indexing,
which however are normally ruled-out in DBMSes design.
</p>
<p>We introduce the concept of overlay index: an index that is meant to be
stored in a standard database, alongside regular data and managed by regular
software, to complement DBMS capabilities. We show a data structure, that we
call DB-tree, that realizes an overlay index to support a wide range of custom
aggregate range queries as well as ADSes, efficiently. All DB-trees operations
can be performed by executing a small number of queries to the DBMS, that can
be issued in parallel in one or two query rounds, and involves a logarithmic
amount of data. We experimentally evaluate the efficiency of DB-trees showing
that our approach is effective, especially if data updates are limited.
</p></div>
    </summary>
    <updated>2019-10-28T23:22:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11753</id>
    <link href="http://arxiv.org/abs/1910.11753" rel="alternate" type="text/html"/>
    <title>Improved Approximation for Maximum Edge Colouring Problem</title>
    <feedworld_mtime>1572220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>L Sunil Chandran, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lahiri:Abhiruk.html">Abhiruk Lahiri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Nitin.html">Nitin Singh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11753">PDF</a><br/><b>Abstract: </b>The anti-Ramsey number, $ar(G, H)$ is the minimum integer $k$ such that in
any edge colouring of $G$ with $k$ colours there is a rainbow subgraph
isomorphic to $H$, i.e., a copy of $H$ with each of its edges assigned a
different colour. The notion was introduced by Erd{\"{o}}s and Simonovits in
1973. Since then the parameter has been studied extensively in combinatorics,
also the particular case when $H$ is a star graph. Recently this case received
the attention of researchers from the algorithm community because of its
applications in interface modelling of wireless networks. To the algorithm
community, the problem is known as maximum edge $q$-colouring problem.
</p>
<p>In this paper, we study the maximum edge $2$-colouring problem from the
approximation algorithm point of view. The case $q=2$ is particularly
interesting due to its application in real-life problems. Algorithmically, this
problem is known to be NP-hard for $q\ge 2$. For the case of $q=2$, it is also
known that no polynomial-time algorithm can approximate to a factor less than
$3/2$ assuming the unique games conjecture. Feng et al. showed a
$2$-approximation algorithm for this problem. Later Adamaszek and Popa
presented a $5/3$-approximation algorithm with the additional assumption that
the input graph has a perfect matching. Note that the obvious but the only
known algorithm issues different colours to the edges of a maximum matching
(say $M$) and different colours to the connected components of $G \setminus M$.
In this article, we give a new analysis of the aforementioned algorithm leading
to an improved approximation bound for triangle-free graphs with perfect
matching. We also show a new lower bound when the input graph is triangle-free.
The contribution of the paper is a completely new, deeper and closer analysis
of how the optimum achieves a higher number of colours than the matching based
algorithm, mentioned above.
</p></div>
    </summary>
    <updated>2019-10-28T23:27:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11564</id>
    <link href="http://arxiv.org/abs/1910.11564" rel="alternate" type="text/html"/>
    <title>Non-Rectangular Convolutions and (Sub-)Cadences with Three Elements</title>
    <feedworld_mtime>1572220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Funakoshi:Mitsuru.html">Mitsuru Funakoshi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pape=Lange:Julian.html">Julian Pape-Lange</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11564">PDF</a><br/><b>Abstract: </b>The discrete acyclic convolution computes the 2n-1 sums sum_{i+j=k; (i,j) in
[0,1,2,...,n-1]^2} (a_i b_j) in O(n log n) time. By using suitable offsets and
setting some of the variables to zero, this method provides a tool to calculate
all non-zero sums sum_{i+j=k; (i,j) in (P cap Z^2)} (a_i b_j) in a rectangle P
with perimeter p in O(p log p) time.
</p>
<p>This paper extends this geometric interpretation in order to allow arbitrary
convex polygons P with k vertices and perimeter p. Also, this extended
algorithm only needs O(k + p(log p)^2 log k) time.
</p>
<p>Additionally, this paper presents fast algorithms for counting sub-cadences
and cadences with 3 elements using this extended method.
</p></div>
    </summary>
    <updated>2019-10-28T23:28:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.11529</id>
    <link href="http://arxiv.org/abs/1910.11529" rel="alternate" type="text/html"/>
    <title>Manipulating Node Similarity Measures in Network</title>
    <feedworld_mtime>1572220800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dey:Palash.html">Palash Dey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Medya:Sourav.html">Sourav Medya</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.11529">PDF</a><br/><b>Abstract: </b>Node similarity measures quantify how similar a pair of nodes are in a
network. These similarity measures turn out to be an important fundamental tool
for many real world applications such as link prediction in networks,
recommender systems etc. An important class of similarity measures are local
similarity measures. Two nodes are considered similar under local similarity
measures if they have large overlap between their neighboring set of nodes.
Manipulating node similarity measures via removing edges is an important
problem. This type of manipulation, for example, hinders effectiveness of link
prediction in terrorists networks. Fortunately, all the popular computational
problems formulated around manipulating similarity measures turn out to be
NP-hard. We, in this paper, provide fine grained complexity results of these
problems through the lens of parameterized complexity. In particular, we show
that some of these problems are fixed parameter tractable (FPT) with respect to
various natural parameters whereas other problems remain intractable W[1]-hard
and W[2]-hard in particular). Finally we show the effectiveness of our proposed
FPT algorithms on real world datasets as well as synthetic networks generated
using Barabasi-Albert and Erdos-Renyi models.
</p></div>
    </summary>
    <updated>2019-10-28T23:28:14Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-28T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16320</id>
    <link href="https://rjlipton.wordpress.com/2019/10/27/quantum-supremacy-at-last/" rel="alternate" type="text/html"/>
    <title>Quantum Supremacy At Last?</title>
    <summary>What it takes to understand and verify the claim Cropped from 2014 Wired source John Martinis of U.C. Santa Barbara and Google is the last author of a paper published Wednesday in Nature that claims to have demonstrated a task executed with minimum effort by a quantum computer that no classical computer can emulate without […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>What it takes to understand and verify the claim</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/10/martinis1.png"><img alt="" class="alignright wp-image-16322" height="180" src="https://rjlipton.files.wordpress.com/2019/10/martinis1.png?w=153&amp;h=180" width="153"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from 2014 <i>Wired</i> <a href="https://www.wired.com/2014/09/martinis/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
John Martinis of U.C. Santa Barbara and Google is the <em>last</em> author of a <a href="https://www.nature.com/articles/s41586-019-1666-5">paper</a> published Wednesday in <em>Nature</em> that claims to have demonstrated a task executed with minimum effort by a quantum computer that no classical computer can emulate without expending Herculean—or Sisyphean—effort. </p>
<p>
Today we present a lay understanding of the claim and discuss degrees of establishing it.</p>
<p>
There are 76 other authors of the paper. The first 75 are alphabetical, then comes Hartmut Neven before Martinis. Usually pride of place goes to the first author, but that depends on size. Martinis is also the corresponding author. The cox in a rowing race rides at the rear. We have discussed aspects of papers with a huge number of authors <a href="https://rjlipton.wordpress.com/2014/02/13/seeing-atoms/">here</a>. </p>
<p>
Three planks of a quantum supremacy claim are:</p>
<ol>
<li>
<em>Build a physical device capable of a nontrivial sampling task.</em> <p/>
</li><li>
<em>Prove that it gains advantage over known classical approaches.</em> <p/>
</li><li>
<em>Prove that comparable classical hardware cannot gain such advantage.</em>
</li></ol>
<p>
Scott Aaronson not only has made <a href="https://www.scottaaronson.com/blog/?p=4317">two</a> great <a href="https://www.scottaaronson.com/blog/?p=4372">posts</a> on these and many other aspects of the claim, he independently proposed in 2015 the sampling task that was programmed, and he analyzed it in a foundational <a href="https://arxiv.org/abs/1612.05903">paper</a> with Lijie Chen of MIT. Researchers at Google had already been thinking along those lines, and they anchored the team composed from numerous other institutions as well. As if on cue—just a couple days before Wednesday’s announcement—a group from IBM put out a <a href="https://www.ibm.com/blogs/research/2019/10/on-quantum-supremacy/">post</a> and <a href="https://arxiv.org/abs/1910.09534">paper</a> taking issue with the argument for the third plank.</p>
<p>
We’ll start with the task and go in order 1-3-2.</p>
<p>
</p><p/><h2> The Task </h2><p/>
<p/><p>
Any <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-qubit quantum circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> and input <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> to <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> induces a probability distribution <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> on <img alt="{\{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{0,1\}^n}"/>. Because it will not matter if we prepend up to <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> NOT gates to <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>, we may suppose <img alt="{x = 0^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+0%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x = 0^n}"/>. Then <img alt="{C(0^n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%280%5En%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C(0^n)}"/> is a unit complex vector of length <img alt="{N = 2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N = 2^n}"/> with entries <img alt="{a_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_z}"/> corresponding to possible outputs <img alt="{z \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z \in \{0,1\}^n}"/>. Then the probability of getting <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> by a final measurement of all qubits is </p>
<p align="center"><img alt="\displaystyle  p_z = D_C(z) = |a_z|^2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_z+%3D+D_C%28z%29+%3D+%7Ca_z%7C%5E2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p_z = D_C(z) = |a_z|^2. "/></p>
<p>
Next we consider probability distributions <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/> that are generated uniformly at random by the following process, for some <img alt="{r \geq n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Cgeq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r \geq n}"/> and taking <img alt="{R = 2^r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%3D+2%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R = 2^r}"/>:</p>
<blockquote><p><b> </b> <em> for <img alt="{i = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%3D+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i = 1}"/> to <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{R}"/>:<br/>
   choose a <img alt="{z \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{z \in \{0,1\}^n}"/> uniformly at random;<br/>
   increment its probability <img alt="{D_1(z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%28z%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D_1(z)}"/> by <img alt="{\frac{1}{R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7BR%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\frac{1}{R}}"/>. </em>
</p></blockquote>
<p/><p>
Here we intend <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> to be the number of binary nondeterministic gates in the circuit. In place of Hadamard gates the experimental circuits get their nondeterminism from these three single-qubit gates (ignoring global phase for <img alt="{\mathbf{Y}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BY%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{Y}^{1/2}}"/> in particular): </p>
<p align="center"><img alt="\displaystyle  \mathbf{X}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; 1 - i \\ 1 - i &amp; 1 + i \end{bmatrix},~ \mathbf{Y}^{1/2} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1 \end{bmatrix},~ \mathbf{W}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; - i\sqrt{2} \\ \sqrt{2} &amp; 1 + i \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BX%7D%5E%7B1%2F2%7D+%3D+%5Cfrac%7B1%7D%7B2%7D%5Cbegin%7Bbmatrix%7D+1+%2B+i+%26+1+-+i+%5C%5C+1+-+i+%26+1+%2B+i+%5Cend%7Bbmatrix%7D%2C%7E+%5Cmathbf%7BY%7D%5E%7B1%2F2%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D+1+%26+-1+%5C%5C+1+%26+1+%5Cend%7Bbmatrix%7D%2C%7E+%5Cmathbf%7BW%7D%5E%7B1%2F2%7D+%3D+%5Cfrac%7B1%7D%7B2%7D%5Cbegin%7Bbmatrix%7D+1+%2B+i+%26+-+i%5Csqrt%7B2%7D+%5C%5C+%5Csqrt%7B2%7D+%26+1+%2B+i+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathbf{X}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; 1 - i \\ 1 - i &amp; 1 + i \end{bmatrix},~ \mathbf{Y}^{1/2} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1 \end{bmatrix},~ \mathbf{W}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; - i\sqrt{2} \\ \sqrt{2} &amp; 1 + i \end{bmatrix}. "/></p>
<p>Here <img alt="{\mathbf{W} = \frac{1}{\sqrt{2}}(\mathbf{X} + \mathbf{Y})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%5Cmathbf%7BX%7D+%2B+%5Cmathbf%7BY%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{W} = \frac{1}{\sqrt{2}}(\mathbf{X} + \mathbf{Y})}"/> where <img alt="{\mathbf{Y} = \begin{bmatrix} 0 &amp; -i \\ i &amp; 0 \end{bmatrix}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BY%7D+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+-i+%5C%5C+i+%26+0+%5Cend%7Bbmatrix%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{Y} = \begin{bmatrix} 0 &amp; -i \\ i &amp; 0 \end{bmatrix}}"/> and <img alt="{\mathbf{X}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BX%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{X}}"/> is another name for NOT. The difference from using Hadamard gates matters to technical analysis of the distributions <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> but the interplay between quantum nondeterministic gates and classical random coins remains in force. </p>
<p>
The choice of <img alt="{\mathbf{X}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BX%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{X}^{1/2}}"/>, <img alt="{\mathbf{Y}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BY%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{Y}^{1/2}}"/>, or <img alt="{\mathbf{W}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{W}^{1/2}}"/> is itself uniformly random at each point where a single-qubit gate is used, except for not repeating the same gate on the same qubit, and those choices determine <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. Now we can give an initial statement of the task tailored to what the paper achieves:</p>
<blockquote><p><b> </b> <em> Given randomly-generated quantum circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/> as inputs, distinguish <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D_C}"/> with high probability from any <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D_1}"/>. </em>
</p></blockquote>
<p/><p>
In more detail, the object is to take a number <img alt="{\delta &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta &gt; 0}"/> and moderately large integer <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, both dictated by practical elements of the experiment, and fulfill this task statement:</p>
<blockquote><p><b> </b> <em> Given randomly-generated <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/>, generate samples <img alt="{z_1,...,z_k \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_1%2C...%2Cz_k+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{z_1,...,z_k \in \{0,1\}^n}"/> such that <img alt="{\frac{1}{k}(D_C(z_1) + \cdots D_C(z_k)) \geq 1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Bk%7D%28D_C%28z_1%29+%2B+%5Ccdots+D_C%28z_k%29%29+%5Cgeq+1+%2B+%5Cdelta%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\frac{1}{k}(D_C(z_1) + \cdots D_C(z_k)) \geq 1 + \delta}"/>. </em>
</p></blockquote>
<p/><p>
It’s important to note that there are two <em>stages</em> of randomness: one over <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> which chooses <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/>, and then the stage of measuring after (perhaps imperfectly) executing <img alt="{C(0^n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%280%5En%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C(0^n)}"/>. The latter can be repeated to get a large sample of strings <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> for a given <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. The nature of the former stage matters most to justifying how to interpret tests of the samples and to closing loopholes. Our <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/> does not signify having uniform distribution in the latter sampling, but rather covers classical alternatives in the former stage that (with overwhelming probability) belong to a class we call <img alt="{\mathcal{D}_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_1}"/>. The <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> for random <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> will (again w.o.p.) belong to a class <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/> which we explain next.</p>
<p>
</p><p/><h2> The World Series of Quantum Computing </h2><p/>
<p/><p>
In honor of the baseball World Series, we offer a baseball analogy. To make differences sharper to see, we take <img alt="{r = n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = n}"/>, so <img alt="{R = N = 2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%3D+N+%3D+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R = N = 2^n}"/>. This is not what the experiment does: their biggest instance has 20 layers totaling <img alt="{r = 1,\!113}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+1%2C%5C%21113%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = 1,\!113}"/> nondeterministic single-qubit gates (plus <img alt="{430}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B430%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{430}"/> two-qubit gates) on the <img alt="{n = 53}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+53%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 53}"/> qubits. But let us continue.</p>
<p>
We are distributing <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> units of probability among <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> “batters” <img alt="{z \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z \in \{0,1\}^n}"/>. A batter who gets two units hits a double, three units makes a triple, and so on. The key distinction is between the familiar batting average and the <em>slugging average</em>, which averages all the bases scored with hits:</p>
<ul>
<li>
The chance of making an out—that is, getting no units—is <img alt="{(\frac{N-1}{N})^N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cfrac%7BN-1%7D%7BN%7D%29%5EN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\frac{N-1}{N})^N}"/> which is approximately <img alt="{\frac{1}{e} = 0.367879\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Be%7D+%3D+0.367879%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{e} = 0.367879\dots}"/> <p/>
</li><li>
The chance of hitting a single is also about <img alt="{\frac{1}{e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Be%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{e}}"/>, leaving <img alt="{1 - \frac{2}{e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cfrac%7B2%7D%7Be%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 - \frac{2}{e}}"/> as the frequency of getting an extra-base hit—which makes <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> a “heavy hitter.” <p/>
</li><li>
From <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> batters chosen uniformly at random, their expected batting average will be <img alt="{1 - \frac{1}{e} = 0.632\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cfrac%7B1%7D%7Be%7D+%3D+0.632%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 - \frac{1}{e} = 0.632\dots}"/>. <p/>
</li><li>
Their expected slugging average, however, will just be <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>: they expect <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> units to be distributed among them.
</li></ul>
<p>
Thus with respect to a random <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/>, and without any knowledge of <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/>, a chosen team of <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> hitters cannot expect to have a joint slugging average higher than <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. Moreover, for any fixed <img alt="{\delta &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta &gt; 0}"/>, the chance of getting a slugging average higher than <img alt="{1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 + \delta}"/> tails away exponentially in <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> (provided <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> also grows). </p>
<p>
With respect to <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/>, however, a quantum device can do better. Google’s device programs itself given <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> as the blueprint. So it just executes <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> and measures all qubits to sample the output. Finding its own heavy hitters is what a quantum circuit is good at. The probability of getting a hitter who hits a triple is magnified by <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/> compared to a uniform choice. Moreover, <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> will never output a string with zero hits—a “can’t miss” property denied to a classical reader of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. For large <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> the probability distribution approaches <img alt="{xe^{-x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bxe%5E%7B-x%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{xe^{-x}}"/> and the slugging expectation is approximately </p>
<p align="center"><img alt="\displaystyle  \int_0^\infty x^2 e^{-x} = 2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cint_0%5E%5Cinfty+x%5E2+e%5E%7B-x%7D+%3D+2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \int_0^\infty x^2 e^{-x} = 2. "/></p>
<p>That is, a team <img alt="{z_1,\dots,z_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_1%2C%5Cdots%2Cz_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_1,\dots,z_k}"/> drafted by sampling from random quantum circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> expects to have a slugging average near <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. This defines the class <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/>. If <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> works perfectly, the average will surpass <img alt="{1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 + \delta}"/> whenever <img alt="{0 &lt; \delta &lt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%3C+%5Cdelta+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0 &lt; \delta &lt; 1}"/> with near certainty as <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> grows. </p>
<p>
Google’s circuits have up to <img alt="{r = 20n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+20n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = 20n}"/>, so <img alt="{R \gg N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%5Cgg+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R \gg N}"/>. Then the “can’t miss” aspect of the quantum advantage is less sharp but the <img alt="{xe^{-x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bxe%5E%7B-x%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{xe^{-x}}"/> approximation is closer and the idea of <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/> is the same. The nature of <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/> can actually be <em>seen</em> from point intensities in <a href="https://en.wikipedia.org/wiki/Speckle_pattern">speckle</a> patterns of laser light:</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2019/10/375px-objective_speckle.jpg"><img alt="" class="aligncenter wp-image-16324" height="150" src="https://rjlipton.files.wordpress.com/2019/10/375px-objective_speckle.jpg?w=150&amp;h=150" width="150"/></a></p>
<p>
</p><p/><h2> Real-World Execution </h2><p/>
<p/><p>
The practical challenge is that the implementation of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> is not perfect. The consequence of an error in the final output is severe. The heavy-hitter outputs <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> of a random <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> are generally not bit-wise similar, so sampling their neighbors is like sampling uniform distribution. As the paper says, “A single bit or phase flip over the course of the algorithm will completely shuffle the speckle pattern and result in close to zero fidelity.”</p>
<p>
Their circuits are sufficiently random that effects of sporadic errors over millions of samples can be modeled by a simple equation using quantum mixed states. We shortcut the paper’s physical analysis by drawing on John Preskill’s illustration of a <em>de-polarizing channel</em> in <a href="http://www.theory.caltech.edu/~preskill/ph219/chap3_15.pdf">chapter 3</a> of his wonderful online <a href="http://www.theory.caltech.edu/~preskill/ph219/ph219_2018-19">notes</a> on quantum computation to reach the same equation (<a href="https://rjlipton.wordpress.com/feed/#F">1</a>). The modeling has informative symmetry when the errors of a <b>bit flip</b>, <b>phase flip</b>, or both are considered equally likely with probability <img alt="{\frac{p}{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bp%7D%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{p}{3}}"/>. The action on the entangled pair <img alt="{|\Phi^+\rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5CPhi%5E%2B%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|\Phi^+\rangle}"/> in the <a href="https://en.wikipedia.org/wiki/Bell_state#Bell_basis">Bell basis</a> is given by the density matrix evolution <img alt="{\rho = |\Phi^+\rangle\langle\Phi^+| \mapsto \rho'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho+%3D+%7C%5CPhi%5E%2B%5Crangle%5Clangle%5CPhi%5E%2B%7C+%5Cmapsto+%5Crho%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho = |\Phi^+\rangle\langle\Phi^+| \mapsto \rho'}"/> where </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}  \rho' &amp;=&amp; (1 - p)|\Phi^+\rangle\langle\Phi^+| \;+\; \frac{p}{3}\left(|\Psi^+\rangle\langle\Psi^+| \;+\; |\Phi^-\rangle\langle\Phi^-| \;+\; |\Psi^-\rangle\langle\Psi^-|\right)\\ ~~~\\ &amp;=&amp; (1 - p') |\Phi^+\rangle\langle\Phi^+| \;+\; p'\frac{I}{4}, \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Crho%27+%26%3D%26+%281+-+p%29%7C%5CPhi%5E%2B%5Crangle%5Clangle%5CPhi%5E%2B%7C+%5C%3B%2B%5C%3B+%5Cfrac%7Bp%7D%7B3%7D%5Cleft%28%7C%5CPsi%5E%2B%5Crangle%5Clangle%5CPsi%5E%2B%7C+%5C%3B%2B%5C%3B+%7C%5CPhi%5E-%5Crangle%5Clangle%5CPhi%5E-%7C+%5C%3B%2B%5C%3B+%7C%5CPsi%5E-%5Crangle%5Clangle%5CPsi%5E-%7C%5Cright%29%5C%5C+%7E%7E%7E%5C%5C+%26%3D%26+%281+-+p%27%29+%7C%5CPhi%5E%2B%5Crangle%5Clangle%5CPhi%5E%2B%7C+%5C%3B%2B%5C%3B+p%27%5Cfrac%7BI%7D%7B4%7D%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}  \rho' &amp;=&amp; (1 - p)|\Phi^+\rangle\langle\Phi^+| \;+\; \frac{p}{3}\left(|\Psi^+\rangle\langle\Psi^+| \;+\; |\Phi^-\rangle\langle\Phi^-| \;+\; |\Psi^-\rangle\langle\Psi^-|\right)\\ ~~~\\ &amp;=&amp; (1 - p') |\Phi^+\rangle\langle\Phi^+| \;+\; p'\frac{I}{4}, \end{array} "/></p>
<p>where <img alt="{p' = \frac{4}{3} p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%27+%3D+%5Cfrac%7B4%7D%7B3%7D+p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p' = \frac{4}{3} p}"/> and <img alt="{\frac{I}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BI%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{I}{4}}"/> is the density matrix of the completely mixed two-qubit state which is just a classical distribution. This presumes <img alt="{p \leq \frac{3}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%5Cleq+%5Cfrac%7B3%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p \leq \frac{3}{4}}"/>; note that <img alt="{p = \frac{3}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%3D+%5Cfrac%7B3%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p = \frac{3}{4}}"/> completely mixes the Bell basis already. The <b>fidelity</b> of <img alt="{\vec{\rho}'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7B%5Crho%7D%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{\rho}'}"/> to the original state is then given by </p>
<p align="center"><img alt="\displaystyle  F = \langle\Phi^+|\rho'|\Phi^+\rangle = 1 - p'. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F+%3D+%5Clangle%5CPhi%5E%2B%7C%5Crho%27%7C%5CPhi%5E%2B%5Crangle+%3D+1+-+p%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F = \langle\Phi^+|\rho'|\Phi^+\rangle = 1 - p'. "/></p>
<p>
This modeling already indicates that with <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> serial opportunities for error the fidelity will decay as <img alt="{(1 - p')^m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281+-+p%27%29%5Em%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1 - p')^m}"/>.  The Google team found low ‘crosstalk’ between qubits and they used exactly this expression in the form <img alt="{(1 - \frac{e_1}{1 - 1/D^2})^m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281+-+%5Cfrac%7Be_1%7D%7B1+-+1%2FD%5E2%7D%29%5Em%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1 - \frac{e_1}{1 - 1/D^2})^m}"/>, evidently with <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> being the native gate error rate they call <img alt="{e_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_1}"/> and <img alt="{D = 2^k,}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD+%3D+2%5Ek%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D = 2^k,}"/> where having <img alt="{k=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k=1}"/> for single-qubit gates supplies the factor <img alt="{\frac{2^{2k}}{2^{2k} - 1} = \frac{4}{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B2%5E%7B2k%7D%7D%7B2%5E%7B2k%7D+-+1%7D+%3D+%5Cfrac%7B4%7D%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{2^{2k}}{2^{2k} - 1} = \frac{4}{3}}"/>.<br/>
The error <img alt="{e_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_2}"/> for the two-qubit gates is similarly represented.  (The full modeling in the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1666-5/MediaObjects/41586_2019_1666_MOESM1_ESM.pdf">supplement</a>, section V, is more refined.)</p>
<p>
By observing their benchmarks (discussed below) for varying small <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> they could calculate the decay concretely and hence estimate values of <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> for the vast majority of runs with larger <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/>.  The random nature of the circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> evidently makes covariance of errors that could systematically upset this modeling negligible.  Thus they can conclude that their device effectively samples from the distribution <a name="F"/></p><a name="F">
<p align="center"><img alt="\displaystyle  F|\langle z \;|\; C \;|\; 0^n\rangle|^2 \;\;+\;\; (1 - F)\frac{1}{N}. \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%7C%5Clangle+z+%5C%3B%7C%5C%3B+C+%5C%3B%7C%5C%3B+0%5En%5Crangle%7C%5E2+%5C%3B%5C%3B%2B%5C%3B%5C%3B+%281+-+F%29%5Cfrac%7B1%7D%7BN%7D.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F|\langle z \;|\; C \;|\; 0^n\rangle|^2 \;\;+\;\; (1 - F)\frac{1}{N}. \ \ \ \ \ (1)"/></p>
</a><p><a name="F"/> Such distributions can be said to belong to the class <img alt="{\mathcal{D}_{1 + F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1+%2B+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1 + F}}"/>. The paper reports that their <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> is driven below <img alt="{0.01}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.01%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.01}"/> but stays above <img alt="{0.001}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.001%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.001}"/> in trials. This bounds the range of the <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> they can separate by. That <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> is separated from zero achieves the first plank and starts on the second. The third needs attention first, however. </p>
<p>
</p><p/><h2> The Third Plank </h2><p/>
<p/><p>
Both <em>concrete</em> and <em>asymptotic</em> complexity evidence matter for the third plank, the former for now and the latter for how <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> and everything else may scale up in the future. In asymptotic complexity, we still don’t know that <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/> and <img alt="{\mathsf{PSPACE}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPSPACE%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{PSPACE}}"/>, which sandwich the quantum feasible class <img alt="{\mathsf{BQP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{BQP}}"/>, are different. Thus asymptotic evidence about polynomial bounds must be conditional. Asymptotic evidence about linear time bounds can be sharper but then tends to be conditioned on forms of <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">SETH</a> in ways we still find <a href="https://rjlipton.wordpress.com/2015/06/01/puzzling-evidence/">puzzling</a>.</p>
<p>
Lower bounds in concrete complexity are less known and have a self-defeating aspect: We are trying to say that any program <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> run for less than an infeasible time <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> must fail. But we can’t run <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> for time <img alt="{T-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T-1}"/> to show that it fails because time <img alt="{T-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T-1}"/> is just as infeasible as time <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>. The best we can do is run <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> for a feasible <img alt="{T_0 \ll T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_0+%5Cll+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_0 \ll T}"/>, either (i) on a smaller task size, or (ii) on the original task but argue it doesn’t show <em>progress</em>. Neither is the same; we <a href="https://rjlipton.wordpress.com/2010/08/28/lower-bounds-and-progressive-algorithms/">made</a> some <a href="https://rjlipton.wordpress.com/2012/11/17/progress-on-progressive-algorithms/">attempts</a> on (ii). </p>
<p>
What the paper does instead is argue that a particular classical approach <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> (also from the Aaronson-Chen paper) would take 10,000 years on today’s hardware. This reminds us of a famous 1977 “Mathematical Games” <a href="https://simson.net/ref/1977/Gardner_RSA.pdf">column</a> by Martin Gardner, which quotes an estimate by Ron Rivest that for factoring a 126-digit number on then-current hardware, “the running time required would be about 40 quadrillion years!” It took only until <a href="https://en.wikipedia.org/wiki/The_Magic_Words_are_Squeamish_Ossifrage">1994</a> for this to be broken. Sure enough, IBM calculated that a more-clever implementation of <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> on the <a href="https://en.wikipedia.org/wiki/Summit_(supercomputer)">Summit</a> supercomputer would take under 3 days. The point is not so much that the Summit hardware is comparable as that estimates based on what are currently thought to be the best possible (classical) methods need asterisks.</p>
<p>
On the asymptotic side, the last section (XI) of the paper’s 66-page <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1666-5/MediaObjects/41586_2019_1666_MOESM1_ESM.pdf">supplement</a> proves a theorem toward showing that a classical simulation from <img alt="{\mathcal{D}_{1 + \delta}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1+%2B+%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1 + \delta}}"/> that scales polynomially with <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> would collapse <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{\#P}}"/> to <img alt="{\mathsf{AM}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{AM}}"/>, and similarly for sub-exponential running times. It does not get all the way there, however: improvements would need to be made in upper bounds for approximation and for worst-case to average-case equivalence. Moreover, there is a difference from what their statistical testing achieves that we try to explain next. </p>
<p>
</p><p/><h2> The Statistical Tests </h2><p/>
<p/><p>
We can cast the second plank in the general context of predictive modeling. Consider a forecaster who places estimates <img alt="{\{q_i\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Bq_i%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{q_i\}}"/> on the true probabilities <img alt="{\{p_i\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Bp_i%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{p_i\}}"/> of various events. In the quantum case, the <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> come from distributions in <img alt="{\mathcal{D}_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1+F}}"/>, where the <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> that applies to the latter sampling stage can be estimated based on the size and depth of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. The <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> come from the physical quantum device—that is to say, from the strings <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> that it outputs. What’s needed is to compute the corresponding outcome probability <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> analytically based on the given circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. This must be done <em>classically</em>, and incurs the “<img alt="{T_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_0}"/>-versus-<img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>” issue discussed above.  <b>[See Addendum below.]</b></p>
<p>
But before we get to that issue, let’s say more from the viewpoint of predictive modeling. We measure how well the forecasts <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> conform to the true <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> by applying a prediction scoring <a href="https://en.wikipedia.org/wiki/Scoring_rule">rule</a>. If outcome <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> happens, then the <em>log-likelihood rule</em> assesses a penalty of </p>
<p align="center"><img alt="\displaystyle  L_i = \log(\frac{1}{q_i}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_i+%3D+%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L_i = \log(\frac{1}{q_i}). "/></p>
<p>This is zero if the outcome was predicted with certainty but goes to infinity if the individual <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> is very low—which is an issue in the quantum case. The expected score based on the true probabilities is <a name="XE"/></p><a name="XE">
<p align="center"><img alt="\displaystyle  E[L_i] = \sum_i p_i \log(\frac{1}{q_i}). \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E%5BL_i%5D+%3D+%5Csum_i+p_i+%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  E[L_i] = \sum_i p_i \log(\frac{1}{q_i}). \ \ \ \ \ (2)"/></p>
</a><p><a name="XE"/> The log-likelihood rule is <b>strictly proper</b> insofar as the unique way to minimize <img alt="{E[L_i]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%5BL_i%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E[L_i]}"/> is to set <img alt="{q_i = p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i = p_i}"/> for each <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>. In human contexts this means the model has incentive to be as accurate as possible. For the quantum device, knowing the <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> that applies to its running of circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> suffices to calculate <img alt="{E[L_i]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%5BL_i%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E[L_i]}"/> as “<img alt="{E_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_{1+F}}"/>,” and hence to benchmark how accurately the device is conforming to the target.</p>
<p>
The formula (<a href="https://rjlipton.wordpress.com/feed/#XE">2</a>) is the <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a> between the <img alt="{\vec{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{p}}"/> and <img alt="{\vec{q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{q}}"/> distributions. It is advocated in several predecessor papers on quantum supremacy experiments, but in fact the team shifted to something simpler they call “linear cross-entropy.” They simply show that the <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> from their samples collectively beat the “<img alt="{E_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_1}"/>” that applies to <img alt="{\mathcal{D}_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_1}"/>—more simply put, that when summed over <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>-many trials <img alt="{z_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_t}"/>, </p>
<p align="center"><img alt="\displaystyle  \frac{1}{T} \sum_{t = 1}^T q_{z_t} &gt; \frac{1}{N} + \delta. \ \ \ \ \ (3)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B1%7D%7BT%7D+%5Csum_%7Bt+%3D+1%7D%5ET+q_%7Bz_t%7D+%3E+%5Cfrac%7B1%7D%7BN%7D+%2B+%5Cdelta.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{1}{T} \sum_{t = 1}^T q_{z_t} &gt; \frac{1}{N} + \delta. \ \ \ \ \ (3)"/></p>
<p>This just boils down to giving a <a href="https://en.wikipedia.org/wiki/Standard_score">z-score</a> based on the modeling for <img alt="{\mathcal{D}_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_1}"/>. It is analogous to how I (Ken writing this) test for cheating at chess. We are flagging the physical device as getting surreptitious input from quantum to achieve a strength of <img alt="{1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 + \delta}"/> compared to a “classical player” who is “rated” as having strength <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. </p>
<p>
The difference from showing that the device’s score from (<a href="https://rjlipton.wordpress.com/feed/#XE">2</a>) is within a hair of <img alt="{E_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_{1+F}}"/> is that this is based on <img alt="{E_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_1}"/>. To be sure, the paper shows that their <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-scores conform to those one would expect an “<img alt="{E_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_{1+F}}"/>-rated” device to achieve. But this is still not the same as (<a href="https://rjlipton.wordpress.com/feed/#XE">2</a>). Whether it is tantamount for enough purposes—including the theorem about <img alt="{\mathsf{AM}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{AM}}"/>—is where we’re most unsure, and we note distinctions between fully (classically) sampling and “spoofing” the statistical tests(s) raised by Scott (including directly in reply to me <a href="https://www.scottaaronson.com/blog/?p=4372#comment-1822570">here</a>) and others. The authors say that using “linear cross-entropy” gave sharper results and that they tried other (unspecified) measures. We wonder how much of the space of scoring rules familiar in predictive modeling has been tried, and whether rules having more gentle tail behavior for tiny <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> than <img alt="{L_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_i}"/> might do better.</p>
<p>
Finally, there is the issue that the team were able to verify <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> exactly only for circuits up to <img alt="{43}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B43%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{43}"/> qubits and/or with <img alt="{14}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B14%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{14}"/> levels, not <img alt="{53}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B53%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{53}"/> with <img alt="{20}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B20%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{20}"/> levels. This creates a dilemma in that IBM’s paper may push them toward <img alt="{n = 60}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 60}"/> or <img alt="{70}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{70}"/>, but that increases the gap from instance sizes they can verify. This also pushes away from the possibly of observing the <img alt="{\mathcal{D}_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1+F}}"/> nature of <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> more directly by finding repeated strings <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> in the second-stage sampling of a fixed <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. The “birthday paradox” threshold for repeats is roughly <img alt="{2^{n/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{n/2}}"/> samples, which might be feasible for <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> around <img alt="{50}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B50%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{50}"/> (given the classical work needed for each <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>, which IBM’s cleverness might speed) but not above <img alt="{60}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{60}"/>. The distinguishing power of repeats drops further with <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. We intend to say more about these last few points, and we are sure there are many chapters still to write about supremacy experiments. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Is the evidence so far convincing to you? Is enough being done on the third plank to exclude possible clever classical use of the fact that the circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> are given as “white boxes”? Are there possible loopholes? </p>
<p>
We would also be grateful to know where we may have oversimplified our characterization of the task and our analysis of the issues.</p>
<p/><p><br/>
<b>Addendum 10/28:</b> On further review, the “outcome probability” of a string <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> comes from first exhaustively computing the probability <img alt="{r_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r_z}"/> that would result from error-free operation of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> and plugging that in to make <img alt="{Fr_z + (1 - F)\frac{1}{N}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BFr_z+%2B+%281+-+F%29%5Cfrac%7B1%7D%7BN%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Fr_z + (1 - F)\frac{1}{N}.}"/>  Although derived from the estimate of <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> and taking <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> from the device, this seems better to regard as the “true probability” <img alt="{p_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_z}"/>, rather than “<img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/>” as stated above.  The actual quantity to regard as “<img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/>” is not calculable and estimating it would require observing repeats from the physical device.  Equation (2) remains correct on principle, but as explained in these <a href="https://www.cs.cmu.edu/~odonnell/quantum18/lecture25.pdf">notes</a> by Ryan O’Donnell, the reversed equation is used instead: </p>
<p align="center"><img alt="\displaystyle  E[L'_i] = \sum_z q_z \log(\frac{1}{p_z}). \ \ \ \ \ (2')" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E%5BL%27_i%5D+%3D+%5Csum_z+q_z+%5Clog%28%5Cfrac%7B1%7D%7Bp_z%7D%29.+%5C+%5C+%5C+%5C+%5C+%282%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  E[L'_i] = \sum_z q_z \log(\frac{1}{p_z}). \ \ \ \ \ (2')"/></p>
<p>
The difference is that <img alt="{\log(\frac{1}{p_z})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28%5Cfrac%7B1%7D%7Bp_z%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log(\frac{1}{p_z})}"/> can be calculated, and while <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> still cannot be, the act of sampling from the physical device estimates the idealized sum <img alt="{\sum_i q_i \log(\frac{1}{p_i})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+q_i+%5Clog%28%5Cfrac%7B1%7D%7Bp_i%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_i q_i \log(\frac{1}{p_i})}"/> closely enough.  This switches the roles of “forecaster” and “forecastee,” but the optimality of <img alt="{q_z = p_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z+%3D+p_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z = p_z}"/> remains valid and the target value is the same as before.  O’Donnell calls this inversion “slightly dicey” but (i) it was ultimately not used anyway, (ii) has an interpretation that regards the physical device as the ground truth, and (iii) may be equally amenable to asymptotic conditional hardness results.  Likewise “<img alt="{q_{z_t}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_%7Bz_t%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_{z_t}}"/>” should be re-named as “<img alt="{p_{z_t}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_%7Bz_t%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_{z_t}}"/>” in (3).]</p>
<p/><p><br/>
[Added more error-modeling details to the real-world section; some minor word changes; clarified how X,Y,W are chosen; addendum to clarify modeling issues.]</p></font></font></div>
    </content>
    <updated>2019-10-27T13:19:04Z</updated>
    <published>2019-10-27T13:19:04Z</published>
    <category term="All Posts"/>
    <category term="detection"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="decoherence"/>
    <category term="Google"/>
    <category term="IBM"/>
    <category term="John Martinis"/>
    <category term="Physics"/>
    <category term="quantum"/>
    <category term="quantum computer"/>
    <category term="quantum supremacy"/>
    <category term="Scott Aaronson"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-10-29T03:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=18384</id>
    <link href="https://gilkalai.wordpress.com/2019/10/27/starting-today-kazhdan-sunday-seminar-computation-quantumness-symplectic-geometry-and-information/" rel="alternate" type="text/html"/>
    <title>Starting today: Kazhdan Sunday seminar: “Computation, quantumness, symplectic geometry, and information”</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Sunday, 27 October, 2019 – 14:00 to 16:00 Repeats every week every Sunday until Sat Feb 01 2020 Location: Ross 70 See also: Seminar announcement; previous post Symplectic Geometry, Quantization, and Quantum Noise. The Google supremacy claims are discussed (with … <a href="https://gilkalai.wordpress.com/2019/10/27/starting-today-kazhdan-sunday-seminar-computation-quantumness-symplectic-geometry-and-information/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Sunday, 27 October, 2019 – 14:00 to 16:00</p>
<p>Repeats every week every Sunday until Sat Feb 01 2020</p>
<p>Location: Ross 70</p>
<p>See also: <a href="https://mathematics.huji.ac.il/event/kazhdan-sunday-seminar-computation-quantumness-symplectic-geometry-and-information-gil?delta=0">Seminar announcement</a>; previous post <a href="https://gilkalai.wordpress.com/2013/01/01/symplectic-geometry-quantization-and-quantum-noise/" rel="bookmark">Symplectic Geometry, Quantization, and Quantum Noise.</a></p>
<p>The Google supremacy claims are discussed (with updates from time to time) in <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">this earlier post</a>. Don’t miss <a href="https://gilkalai.wordpress.com/2019/10/13/gerard-cornuejolss-bakers-eighteen-5000-dollars-conjectures/">our previous post</a> on combinatorics.</p>
<h3>Tentative syllabus for “Computation, quantumness, symplectic geometry, and information”</h3>
<p>1. Mathematical models of classical and quantum mechanics.</p>
<p>2. Correspondence principle and quantization.</p>
<p>3. Classical and quantum computation: gates, circuits, algorithms (Shor, Grover). Solovay-Kitaev. Some ideas of cryptography</p>
<p>4. Quantum noise and measurement, and rigidity of the Poisson bracket.</p>
<p>5. Noisy classical and quantum computing and error correction, threshold theorem- quantum fault tolerance (small noise is good for quantum computation). Kitaev’s surface code.</p>
<p>6. Quantum speed limit/time-energy uncertainty vs symplectic displacement energy.</p>
<p>7. Time-energy uncertainty and quantum computation (Dorit or her student?)</p>
<p>8. Berezin transform, Markov chains, spectral gap, noise.</p>
<p>9. Adiabatic computation, quantum PCP (probabilistically checkable proofs) conjecture [? under discussion]</p>
<p>10. Noise stability and noise sensitivity of Boolean functions, noisy boson sampling</p>
<p>11. Connection to quantum field theory (Guy?).</p>
<p>Literature: Aharonov, D. Quantum computation, In “Annual Reviews of Computational Physics” VI, 1999 (pp. 259-346). <a href="https://arxiv.org/abs/quant-ph/9812037">https://arxiv.org/abs/quant-ph/9812037</a></p>
<p>Kalai, G., Three puzzles on mathematics computations, and games, Proc. Int Congress Math 2018, Rio de Janeiro, Vol. 1 pp. 551–606. <a href="https://arxiv.org/abs/1801.02602">https://arxiv.org/abs/1801.02602</a></p>
<p>Nielsen, M.A., and Chuang, I.L., Quantum computation and quantum information. Cambridge University Press, Cambridge, 2000.</p>
<p>Polterovich, L., Symplectic rigidity and quantum mechanics, European Congress of Mathematics, 155–179, Eur. Math. Soc., Zürich, 2018. <a href="https://sites.google.com/site/polterov/miscellaneoustexts/symplectic-rigidity-and-quantum-mechanics">https://sites.google.com/site/polterov/miscellaneoustexts/symplectic-rig…</a></p>
<p>Polterovich L., and Rosen D., Function theory on symplectic manifolds. American Mathematical Society; 2014. [Chapters 1,9] <a href="https://sites.google.com/site/polterov/miscellaneoustexts/function-theory-on-symplectic-manifolds">https://sites.google.com/site/polterov/miscellaneoustexts/function-theor…</a></p>
<p>Wigderson, A., Mathematics and computation, Princeton Univ. Press, 2019. <a href="https://www.math.ias.edu/files/mathandcomp.pdf">https://www.math.ias.edu/files/mathandcomp.pdf</a></p></div>
    </content>
    <updated>2019-10-27T06:09:19Z</updated>
    <published>2019-10-27T06:09:19Z</published>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Geometry"/>
    <category term="Physics"/>
    <category term="Teaching"/>
    <category term="Quantization"/>
    <category term="Quantum computation"/>
    <category term="Quantum information"/>
    <category term="Symplectic geometry"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-10-29T03:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/143</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/143" rel="alternate" type="text/html"/>
    <title>TR19-143 |  Equivalence of Systematic Linear Data Structures and Matrix Rigidity | 

	Sivaramakrishnan Natarajan Ramamoorthy, 

	Cyrus Rashtchian</title>
    <summary>Recently, Dvir, Golovnev, and Weinstein have shown that sufficiently strong lower bounds for linear data structures would imply new bounds for rigid matrices. However, their result utilizes an algorithm that requires an $NP$ oracle, and hence, the rigid matrices are not explicit. In this work, we derive an equivalence between rigidity and the systematic linear model of data structures. For the $n$-dimensional inner product problem with $m$ queries, we prove that lower bounds on the query time imply rigidity lower bounds for the query set itself. In particular, an explicit lower bound of $\omega\left(\frac{n}{r}\log m\right)$ for $r$ redundant storage bits would yield better rigidity parameters than the best bounds due to Alon, Panigrahy, and Yekhanin. We also prove a converse result, showing that rigid matrices directly correspond to hard query sets for the systematic linear model. As an application, we prove that the set of vectors obtained from rank one binary matrices is rigid with parameters matching the known results for explicit sets. This implies that the vector-matrix-vector problem requires query time $\Omega(n^{3/2}/r)$ for redundancy $r \geq \sqrt{n}$ in the systematic linear model, improving a result of Chakraborty, Kamma, and Larsen. Finally, we prove a cell probe lower bound for the vector-matrix-vector problem in the  high error regime, improving a result of Chattopadhyay, Koucký, Loff, and Mukhopadhyay.</summary>
    <updated>2019-10-25T19:43:46Z</updated>
    <published>2019-10-25T19:43:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-29T03:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/25/tenure-track-faculties-at-krannert-school-of-management-purdue-university-apply-by-december-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/25/tenure-track-faculties-at-krannert-school-of-management-purdue-university-apply-by-december-1-2019/" rel="alternate" type="text/html"/>
    <title>Tenure track faculties at Krannert School of Management, Purdue University (apply by December 1, 2019)</title>
    <summary>Krannert School of Management invites applicants for two tenure-track faculty positions at the assistant professor level in the Quantitative Methods area, to begin in the fall semester of 2020. We welcome applicants from all research areas represented within the Quantitative Methods area. Website: https://career8.successfactors.com/sfcareer/jobreqcareer?jobId=8013&amp;company=purdueuniv&amp;userna%20me= Email: nguye161@purdue.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Krannert School of Management invites applicants for two tenure-track faculty positions at the assistant professor level in the Quantitative Methods area, to begin in the fall semester of 2020. We welcome applicants from all research areas represented within the Quantitative Methods area.</p>
<p>Website: <a href="https://career8.successfactors.com/sfcareer/jobreqcareer?jobId=8013&amp;company=purdueuniv&amp;userna%20me=">https://career8.successfactors.com/sfcareer/jobreqcareer?jobId=8013&amp;company=purdueuniv&amp;userna%20me=</a><br/>
Email: nguye161@purdue.edu</p></div>
    </content>
    <updated>2019-10-25T18:11:05Z</updated>
    <published>2019-10-25T18:11:05Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-29T03:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/24/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/24/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019/" rel="alternate" type="text/html"/>
    <title>Simons-Berkeley Research Fellowship at Simons Institute for the Theory of Computing (apply by December 15, 2019)</title>
    <summary>The Simons Institute for the Theory of Computing invites applications for the Simons-Berkeley Research Fellowships to participate in one or more of the semester-long programs during the 2020-21 academic year: Probability, Geometry, and Computation in High Dimensions; Theory of Reinforcement Learning; Satisfiability: Theory, Practice, and Beyond; and Theoretical Foundations of Computer Systems Website: https://simons.berkeley.edu/fellows2020 Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Simons Institute for the Theory of Computing invites applications for the Simons-Berkeley Research Fellowships to participate in one or more of the semester-long programs during the 2020-21 academic year: Probability, Geometry, and Computation in High Dimensions; Theory of Reinforcement Learning; Satisfiability: Theory, Practice, and Beyond; and Theoretical Foundations of Computer Systems</p>
<p>Website: <a href="https://simons.berkeley.edu/fellows2020">https://simons.berkeley.edu/fellows2020</a><br/>
Email: simonsvisitorservices@berkeley.edu</p></div>
    </content>
    <updated>2019-10-24T23:20:27Z</updated>
    <published>2019-10-24T23:20:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-29T03:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1513</id>
    <link href="https://theorydish.blog/2019/10/24/shopping-for-grain-in-the-market-works-a-fine-job/" rel="alternate" type="text/html"/>
    <title>Shopping for Grain in the Market Works –   a Fine Job!</title>
    <summary>I’m excited to share the news of two upcoming workshops…   TCS Early Career Mentoring Workshop Yael Kalai, Matt Weinberg, and I are organizing a TCS mentoring workshop in upcoming FOCS with a focus on demystifying the job market. The program includes a senior panel featuring Shafi Goldwasser, Samir Khuller, Tim Roughgarden, and Eva Tardos, a junior panel starring Inbal Talgam-Cohen, Omri Weinstein, and Henry Yuen, and two exemplary job talks by Eshan Chattopadhyay and Pravesh Kothari. Visit our website to see the full program and most importantly suggest panel questions.   Fine-grained Complexity Workshop Amir Abboud and I are organizing a workshop on fine-grained complexity, to be held Jan 2nd 2020 at Tel-Aviv University, closing the first annual TAU Theory-Fest. The program includes a morning of plenary talks (Karl Bringmann, Seth Pettie, and Barna Saha) and shorter cutting-edge technical talks in the afternoon. (If you have something interesting to share with the fine-grained complexity community, and we haven’t contacted you yet about giving a talk, please let us know.)   Looking forward to seeing you in Baltimore and Tel-Aviv!</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I’m excited to share the news of two upcoming workshops…</p>
<p> </p>
<h3>TCS Early Career Mentoring Workshop</h3>
<p>Yael Kalai, Matt Weinberg, and I are organizing a TCS mentoring workshop in <a href="http://focs2019.cs.jhu.edu/">upcoming FOCS</a> with a focus on <strong>demystifying the job market</strong>.</p>
<p>The program includes a senior panel featuring Shafi Goldwasser, Samir Khuller, Tim Roughgarden, and Eva Tardos, a junior panel starring Inbal Talgam-Cohen, Omri Weinstein, and Henry Yuen, and two exemplary job talks by Eshan Chattopadhyay and Pravesh Kothari.</p>
<p>Visit our <a href="https://www.cs.princeton.edu/~smattw/FOCS19/index.html">website</a> to see the full program and most importantly <a href="https://forms.gle/5kw7Zydo4Cvw2D4r7">suggest panel questions</a>.</p>
<p> </p>
<h3>Fine-grained Complexity Workshop</h3>
<p>Amir Abboud and I are organizing a workshop on fine-grained complexity, to be held Jan 2nd 2020 at Tel-Aviv University, closing the first annual <a href="https://sites.google.com/view/tau-theory-fest/home">TAU Theory-Fest</a>.</p>
<p>The program includes a morning of plenary talks (Karl Bringmann, Seth Pettie, and Barna Saha) and shorter cutting-edge technical talks in the afternoon.</p>
<p>(If you have something interesting to share with the fine-grained complexity community, and we haven’t contacted you yet about giving a talk, please let us know.)</p>
<p> </p>
<p>Looking forward to seeing you in Baltimore and Tel-Aviv!</p></div>
    </content>
    <updated>2019-10-24T17:59:00Z</updated>
    <published>2019-10-24T17:59:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>aviad.rubinstein</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-10-29T03:21:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7561</id>
    <link href="https://windowsontheory.org/2019/10/24/boazs-inferior-classical-inferiority-faq/" rel="alternate" type="text/html"/>
    <title>Boaz’s inferior classical inferiority FAQ</title>
    <summary>(For better info, see Scott’s Supreme Quantum Superiority FAQ and also his latest post on the Google paper; also this is not really an FAQ but was inspired by a question about the Google paper from a former CS 121 student) “Suppose aliens invade the earth and threaten to obliterate it in a year’s time […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>(For better info, see <a href="https://www.scottaaronson.com/blog/?p=4317">Scott’s Supreme Quantum Superiority FAQ</a> and also his <a href="https://www.scottaaronson.com/blog/?p=4372">latest post on the Google paper</a>; also this is not really an FAQ but was inspired by a question about the Google paper from a former <a href="https://cs121.boazbarak.org/schedule/">CS 121 </a>student)</p>



<blockquote class="wp-block-quote"><p><em> “Suppose aliens invade the earth and threaten to obliterate it in a year’s time unless human beings can find the Ramsey number for red five and blue five. We could marshal the world’s best minds and fastest computers, and within a year we could probably calculate the value. If the aliens demanded the Ramsey number for red six and blue six, however, we would have no choice but to launch a preemptive attack.</em>“</p><cite>Paul Erdős (as quoted by Graham and Spencer, 1990, hat tip: <a href="https://windowsontheory.org/2017/10/30/the-different-forms-of-quantum-computing-skepticism/#comment-42659">Lamaze Tishallishmi</a>)</cite></blockquote>



<p>In a <a href="https://www.nature.com/articles/s41586-019-1666-5">Nature paper</a> published this week, a group of researchers from John Martinis’s lab at Google announced arguably the first demonstration of “quantum supremacy” – a computational task carried out by a 53 qubit quantum computer that would require a prohibitive amount of time to simulate classically. </p>



<p>Google’s calculations of the “classical computation time” might have been overly pessimistic (from the classical point of view), and there has been work from <a href="https://arxiv.org/abs/1910.09534">IBM</a> as well as some <a href="https://www.caltech.edu/campus-life-events/master-calendar/iqi-weekly-seminar-2019-10-01">work of Johnnie Gray</a> suggesting that there are significant savings to be made. Indeed, given the lessons that we learned from private key cryptography, where techniques such as linear and differential cryptanalysis were used to “shave factors from exponents”, we know that even if a problem requires exponential time in general, this does not mean that by being very clever we can’t make significant savings over the naive brute force algorithm. This holds doubly  so in this case, where, unlike the designers of block ciphers, the Google researchers were severely constrained by factors of geometry and the kind of gates they can reliably implement.</p>



<p>I would not be terribly surprised if we will see more savings and even an actual classical simulation of the same sampling task that Google achieved. In fact, I very much hope this happens, since it will allow us to independently verify the reliability of Google’s chip and whether it actually did in fact sample from the distribution it is supposed to have sampled from (or at least rule out some “null hypothesis”).  But this would not change the main point that the resources for classical simulation, as far as we know, scale exponentially with the number of qubits and their quality. While we could perhaps with great effort simulate a 53 qubit depth 20 circuit classically, once we reach something like 100 qubits and depth then all current approaches will be hopelessly behind.</p>



<p>In the language of <a href="https://windowsontheory.org/2017/10/30/the-different-forms-of-quantum-computing-skepticism/">my essay on quantum skepticism</a>, I think this latest result, and the rest of the significant experimental progress that has been going on, all but rules out the possibility of “Skepticland”  where there would be some fundamental physical reason why it is not possible to build quantum computers that offer exponential advantage in the amount of resources to achieve certain tasks over classical computers.</p>



<p>While the worlds of “Popscitopia”  (quantum computers can do everything) and “Classicatopia” (there is an efficient classical algorithm to simulate BQP) remain mathematical possiblities (just as P=NP is), most likely we live in <strong>“Superiorita”</strong> where quantum computers do offer exponential advantage for <em>some</em> computational problems.</p>



<p>Some people question whether these kind of “special purpose” devices that might be very expensive to build are worth the investment. First of all (and most importantly for me), as I argued in <a href="https://windowsontheory.org/2017/10/30/the-different-forms-of-quantum-computing-skepticism/">my essay</a>, exploring the limits of physically realizable computation is a grand scientific goal in its own right worthy of investment regardless  of applications. Second, technology is now a <a href="https://www.gartner.com/en/newsroom/press-releases/2019-01-28-gartner-says-global-it-spending-to-reach--3-8-trillio#targetText=Worldwide%20IT%20spending%20is%20projected,latest%20forecast%20by%20Gartner%2C%20Inc.">3.8 trillion dollar </a>per year industry, and quantum computers are in a very real sense the first qualitatively different computing devices since the days of Babbage and Turing. Spending a fraction of a percent of the industry’s worth to the economy on exploring the potential for quantum computing seems like a good investment, even if there will be no practical application in the next decade or two. (By the same token, spending a fraction of a percent on exploring algorithm design and the limitations of <em>classical </em>algorithms is a very good investment as well.)</p></div>
    </content>
    <updated>2019-10-24T13:32:18Z</updated>
    <published>2019-10-24T13:32:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-10-29T03:21:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/142</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/142" rel="alternate" type="text/html"/>
    <title>TR19-142 |  Semi-Algebraic Proofs, IPS Lower Bounds and the $\tau$-Conjecture: Can a Natural Number be Negative? | 

	Yaroslav Alekseev, 

	Dima Grigoriev, 

	Edward Hirsch, 

	Iddo  Tzameret</title>
    <summary>We introduce the `binary value principle' which is a simple subset-sum instance expressing that a natural number written in binary cannot be negative, relating it to central problems in proof and algebraic complexity. We prove conditional superpolynomial lower bounds on the Ideal Proof System (IPS) refutation size of this instance, based on a well-known hypothesis by Shub and Smale about the hardness of computing factorials, where IPS is the strong algebraic proof system introduced by Grochow and Pitassi (2018). Conversely, we show that short IPS refutations of this instance bridge the gap between sufficiently strong algebraic and semi-algebraic proof systems. Our results extend to full-fledged IPS the paradigm introduced in Forbes et al. (2016), whereby lower bounds against subsystems of IPS were obtained using restricted algebraic circuit lower bounds, and demonstrate that the binary value principle captures the advantage of semi-algebraic over algebraic reasoning, for sufficiently strong systems. Specifically, we show the following:

*Conditional IPS lower bounds:* The Shub-Smale hypothesis (1995) implies a superpolynomial lower bound on the size of IPS refutations of the binary value principle over the rationals defined as the unsatisfiable linear equation $\sum_{i=1}^{n} 2^{i-1}x_i = -1$, for boolean $x_i$'s. Further, the related $\tau$-conjecture (1995) implies a superpolynomial lower bound on the size of IPS refutations of a variant of the binary value principle over the ring of rational functions. No prior conditional lower bounds were known for IPS or for apparently much weaker propositional proof systems such as Frege.

*Algebraic vs. semi-algebraic proofs:* Admitting short refutations of the binary value principle is necessary for any algebraic proof system to fully simulate any known semi-algebraic proof system, and for strong enough algebraic proof systems it is also sufficient. In particular, we introduce a very strong proof system that simulates all known semi-algebraic proof systems (and most other known concrete propositional proof systems), under the name Cone Proof System (CPS), as a semi-algebraic analogue of the ideal proof system: CPS establishes the unsatisfiability of collections of polynomial equalities and inequalities over the reals, by representing sum-of-squares proofs (and extensions) as algebraic circuits. We prove that IPS is polynomially equivalent to CPS iff IPS admits polynomial-size refutations of the binary value principle (for the language of systems of equations that have no 0/1-solutions), over both $\mathbb{Z}$ and $\mathbb{Q}$.</summary>
    <updated>2019-10-24T00:45:43Z</updated>
    <published>2019-10-24T00:45:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-29T03:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/141</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/141" rel="alternate" type="text/html"/>
    <title>TR19-141 |  On Rich $2$-to-$1$ Games | 

	Mark Braverman, 

	Subhash Khot, 

	Dor Minzer</title>
    <summary>We propose a variant of the $2$-to-$1$ Games Conjecture that we call the Rich $2$-to-$1$ Games Conjecture and show that it is equivalent to the Unique Games Conjecture. We are motivated by two considerations. Firstly, in light of the recent proof of the $2$-to-$1$ Games Conjecture, we hope to understand how one might make further progress towards a proof of the Unique Games Conjecture. Secondly, the new variant along with perfect completeness in addition, might imply hardness of approximation results that necessarily require perfect completeness and (hence) are not implied by the Unique Games Conjecture.</summary>
    <updated>2019-10-24T00:42:15Z</updated>
    <published>2019-10-24T00:42:15Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-29T03:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4372</id>
    <link href="https://www.scottaaronson.com/blog/?p=4372" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4372#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4372" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Quantum supremacy: the gloves are off</title>
    <summary xml:lang="en-US">Links:Google paper in NatureNew York Times articleIBM paper and blog post responding to Google’s announcementBoaz Barak’s new post: “Boaz’s inferior classical inferiority FAQ”Lipton and Regan’s postMy quantum supremacy interview with the BBC (featuring some of my fewest “uhms” and “ahs” ever!)NEW: My preprint with Sam Gunn, On the Classical Hardness of Spoofing Linear Cross-Entropy Benchmarking […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Links:</strong><br/><a href="https://www.nature.com/articles/s41586-019-1666-5">Google paper in </a><em><a href="https://www.nature.com/articles/s41586-019-1666-5">Nature</a></em><br/><em><a href="https://www.nytimes.com/2019/10/23/technology/quantum-computing-google.html?action=click&amp;module=Top%20Stories&amp;pgtype=Homepage">New York Times</a></em><a href="https://www.nytimes.com/2019/10/23/technology/quantum-computing-google.html?action=click&amp;module=Top%20Stories&amp;pgtype=Homepage"> article</a><br/>IBM <a href="https://arxiv.org/abs/1910.09534">paper</a> and <a href="https://www.ibm.com/blogs/research/2019/10/on-quantum-supremacy/">blog post</a> responding to Google’s announcement<br/>Boaz Barak’s new post: <a href="https://windowsontheory.org/2019/10/24/boazs-inferior-classical-inferiority-faq/">“Boaz’s inferior classical inferiority FAQ”</a><br/><a href="https://rjlipton.wordpress.com/2019/10/27/quantum-supremacy-at-last/">Lipton and Regan’s post</a><br/><a href="https://www.bbc.co.uk/sounds/play/w3csym2f">My quantum supremacy interview with the BBC</a> (featuring some of my fewest “uhms” and “ahs” ever!)<br/><strong>NEW:</strong> My preprint with Sam Gunn, <a href="https://arxiv.org/abs/1910.12085">On the Classical Hardness of Spoofing Linear Cross-Entropy Benchmarking</a></p>



<p>When Google’s quantum supremacy paper leaked a month ago—not through Google’s error, but through NASA’s—I had a hard time figuring out how to cover the news here.  I had to say <em>something</em>; on the other hand, I wanted to avoid any detailed technical analysis of the leaked paper, because I was acutely aware that my colleagues at Google were still barred by <em>Nature</em>‘s embargo rules from publicly responding to anything I or others said.  (I was also one of the reviewers for the <em>Nature</em> paper, which put additional obligations on me.)</p>



<p>I ended up with <a href="https://www.scottaaronson.com/blog/?p=4317">Scott’s Supreme Quantum Supremacy FAQ</a>, which tried to toe this impossible line by “answering general questions about quantum supremacy, and the consequences of its still-hypothetical achievement, in light of the leak.”  It wasn’t an ideal solution—for one thing, because while I still regard Google’s sampling experiment as a historic milestone for our whole field, there <em>are</em> some technical issues, aspects that subsequent experiments (hopefully coming soon) will need to improve.  Alas, the ground rules of my FAQ forced me to avoid such issues, which caused some readers to conclude mistakenly that I didn’t think there were any.</p>



<p>Now, though, the Google paper has <a href="https://www.nature.com/articles/s41586-019-1666-5">come out as </a><em><a href="https://www.nature.com/articles/s41586-019-1666-5">Nature</a></em><a href="https://www.nature.com/articles/s41586-019-1666-5">‘s cover story</a>, at the same time as there have been new technical developments—most obviously, the <a href="https://arxiv.org/abs/1910.09534">paper from IBM</a> (see also their <a href="https://www.ibm.com/blogs/research/2019/10/on-quantum-supremacy/">blog post</a>) saying that they could simulate the Google experiment in 2.5 days, rather than the 10,000 years that Google had estimated.</p>



<p>(Yesterday I was deluged by emails asking me “whether I’d seen” IBM’s paper.  As a science blogger, I try to respond to stuff pretty quickly when necessary, but I don’t—can’t—respond in Twitter time.)</p>



<p>So now the gloves are off.  No more embargo.  Time to address the technical stuff under the hood—which is the purpose of this post.</p>



<p>I’m going to assume, from this point on, that you already understand the basics of sampling-based quantum supremacy experiments, and that I don’t need to correct beginner-level misconceptions about what the term “quantum supremacy” does and doesn’t mean (no, it doesn’t mean scalability, fault-tolerance, useful applications, breaking public-key crypto, etc. etc.).  If this is not the case, you could start (e.g.) with <a href="https://www.scottaaronson.com/blog/?p=4317">my FAQ</a>, or with John Preskill’s <a href="https://www.quantamagazine.org/john-preskill-explains-quantum-supremacy-20191002/">excellent </a><em><a href="https://www.quantamagazine.org/john-preskill-explains-quantum-supremacy-20191002/">Quanta</a></em><a href="https://www.quantamagazine.org/john-preskill-explains-quantum-supremacy-20191002/"> commentary</a>.</p>



<p>Without further ado:</p>



<p><strong>(1) So what about that IBM thing?  <em>Are</em> random quantum circuits easy to simulate classically?</strong></p>



<p>OK, so let’s carefully spell out what the IBM paper says.  They argue that, by commandeering the full attention of <a href="https://en.wikipedia.org/wiki/Summit_(supercomputer)">Summit</a> at Oak Ridge National Lab, the most powerful supercomputer that currently exists on Earth—one that fills the area of two basketball courts, and that (crucially) has <strong>250 petabytes</strong> of hard disk space—one could just barely store the entire quantum state vector of Google’s 53-qubit Sycamore chip in hard disk.  And once one had done that, one could simulate the chip in ~2.5 days, more-or-less just by updating the entire state vector by brute force, rather than the 10,000 years that Google had estimated on the basis of my and Lijie Chen’s <a href="https://arxiv.org/abs/1612.05903">“Schrödinger-Feynman algorithm”</a> (which can get by with less memory).</p>



<p>The IBM group understandably hasn’t actually done this yet—even though IBM set it up, the world’s #1 supercomputer isn’t just sitting around waiting for jobs!  But I see little reason to doubt that their analysis is basically right.  I don’t know why the Google team didn’t consider how such near-astronomical hard disk space would change their calculations; probably they wish they had.</p>



<p>I find this to be much, <em>much</em> better than IBM’s initial reaction to the Google leak, which was simply to <a href="https://www.ft.com/content/cede11e0-dd51-11e9-9743-db5a370481bc">dismiss</a> the importance of quantum supremacy as a milestone.  Designing better classical simulations is precisely how IBM and others <em>should</em> respond to Google’s announcement, and how I said a month ago that I hoped they <em>would</em> respond.  If we set aside the pass-the-popcorn PR war (or even if we don’t), this is how science progresses.</p>



<p>But does IBM’s analysis mean that “quantum supremacy” hasn’t been achieved?  No, it doesn’t—at least, not under any definition of “quantum supremacy” that I’ve ever used.  The Sycamore chip took about 3 minutes to generate the ~5 million samples that were needed to pass the “linear cross-entropy benchmark”—the statistical test that Google applies to the outputs of its device.</p>



<p>(<strong>Technical note added:</strong> Google’s samples are extremely noisy—the actual distribution being sampled from is something like 0.998U+0.002D, where U is the uniform distribution and D is the hard distribution that you want.  What this means, in practice, is that you need to take a number of samples that’s large compared to 1/0.002<sup>2</sup>, in order to extract a signal corresponding to D.  But the good news is that Google <em>can</em> take that many samples in just a few minutes, since once the circuit has been loaded onto the chip, generating each sample takes only about 40 microseconds.  And once you’ve done this, what hardness results we have for passing the linear cross-entropy test—to be discussed later in this post—apply basically just as well as if you’d taken a single noiseless sample.)</p>



<p>Anyway, you might notice that three minutes versus 2.5 days is still a quantum speedup by a factor of 1200.  But even more relevant, I think, is to compare the number of “elementary operations.”  Let’s generously count a FLOP (floating-point operation) as the equivalent of a quantum gate.  Then by my estimate, we’re comparing ~5×10<sup>9</sup> quantum gates against ~2×10<sup>20</sup> FLOPs—a quantum speedup by a factor of ~40 billion.</p>



<p>For me, though, the broader point is that neither party here—certainly not IBM—denies that the top-supercomputers-on-the-planet-level difficulty of classically simulating Google’s 53-qubit programmable chip really <em>is</em> coming from the exponential character of the quantum states in that chip, <em>and nothing else</em>.  That’s what makes this back-and-forth fundamentally different from the previous one between D-Wave and the people who sought to simulate <em>its</em> devices classically.  The skeptics, like me, didn’t much care what speedup over classical benchmarks there was or wasn’t today: we cared about the <em>increase</em> in the speedup as D-Wave upgraded its hardware, and the trouble was that we never saw a convincing case that there would be one.  I’m a theoretical computer scientist, and this is what I believe: that after the constant factors have come and gone, what remains are asymptotic growth rates.</p>



<p>In the present case, while increasing the circuit depth won’t evade IBM’s “store everything to hard disk” strategy, increasing the number of qubits will.  If Google, or someone else, upgraded from 53 to 55 qubits, that would apparently already be enough to exceed Summit’s 250-petabyte storage capacity.  At 60 qubits, you’d need 33 Summits.  At 70 qubits, enough Summits to fill a city … you get the idea.</p>



<p>From the beginning, it was clear that quantum supremacy would not be a milestone like the moon landing—something that’s achieved in a moment, and is then clear to everyone for all time.  It would be more like eradicating measles: it could be achieved, then temporarily unachieved, then re-achieved.  For by definition, quantum supremacy all about <em>beating</em> something—namely, classical computation—and the latter can, at least for a while, fight back.</p>



<p>As Boaz Barak put it to me, the current contest between IBM and Google is analogous to <a href="https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov">Kasparov versus Deep Blue</a>—<em>except with the world-historic irony that IBM is playing the role of Kasparov!</em>  In other words, Kasparov can put up a heroic struggle, during a “transitional period” that lasts a year or two, but the fundamentals of the situation are that he’s toast.  If Kasparov had narrowly beaten Deep Blue in 1997, rather than narrowly losing, the whole public narrative would likely have been different (“humanity triumphs over computers after all!”).  Yet as Kasparov himself well knew, the very fact that the contest was <em>close</em> meant that, either way, human dominance would soon end for good.</p>



<p>Let me leave the last word on this to friend-of-the-blog Greg Kuperberg, who graciously gave me permission to quote his comments about the IBM paper.</p>



<blockquote class="wp-block-quote"><p>I’m not entirely sure how embarrassed Google should feel that they overlooked this.   I’m sure that they would have been happier to anticipate it, and happier still if they had put more qubits on their chip to defeat it.   However, it doesn’t change their real achievement.</p><p>I respect the IBM paper, even if the press along with it seems more grouchy than necessary.   I tend to believe them that the Google team did not explore all avenues when they said that their 53 qubits aren’t classically simulable.   But if this is the best rebuttal, then you should still consider how much Google and IBM still agree on this as a proof-of-concept of QC.   This is still quantum David vs classical Goliath, in the extreme.   53 qubits is in some ways still just 53 bits, only enhanced with quantum randomness.  To answer those 53 qubits, IBM would still need entire days of computer time with the world’s fastest supercomputer, a 200-petaflop machine with hundreds of thousands of processing cores and trillions of high-speed transistors.   If we can confirm that the Google chip actually meets spec, but we need this much computer power to do it, then to me that’s about as convincing as a larger quantum supremacy demonstration that humanity can no longer confirm at all.</p><p>Honestly, I’m happy to give both Google and IBM credit for helping the field of QC, even if it is the result of a strange dispute. </p></blockquote>



<p>I should mention that, even before IBM’s announcement, Johnnie Gray, a postdoc at Imperial College, gave a talk (<a href="https://www.caltech.edu/campus-life-events/master-calendar/iqi-weekly-seminar-2019-10-01">abstract here</a>) at Caltech’s Institute for Quantum Information with a proposal for a <em>different</em> faster way to classically simulate quantum circuits like Google’s—in this case, by doing tensor network contraction more cleverly.  Unlike both IBM’s proposed brute-force simulation, and the Schrödinger-Feynman algorithm that Google implemented, Gray’s algorithm (as far as we know now) would need to be repeated k times if you wanted k independent samples from the hard distribution.  Partly because of this issue, Gray’s approach doesn’t currently look competitive for simulating thousands or millions of samples, but we’ll need to watch it and see what happens.</p>



<p><strong>(2) Direct versus indirect verification.</strong></p>



<p>The discussion of IBM’s proposed simulation brings us to a curious aspect of the Google paper—one that was already apparent when <em>Nature</em> sent me the paper for review back in August.  Namely, Google took its supremacy experiments well past the point <em>where even they themselves knew how to verify the results</em>, by any classical computation that they knew how to perform feasibly (say, in less than 10,000 years).</p>



<p>So you might reasonably ask: if they couldn’t even verify the results, then how did they get to claim quantum speedups from those experiments?  Well, they resorted to various gambits, which basically involved estimating the fidelity on quantum circuits that looked almost the same as the hard circuits, but happened to be easier to simulate classically, and then making the (totally plausible) assumption that that fidelity would be maintained on the hard circuits.  Interestingly, they also cached their outputs and put them online (as part of the supplementary material to their <em>Nature</em> paper), in case it became feasible to verify them in the future.</p>



<p>Maybe you can now see where this is going.  From Google’s perspective, IBM’s rainstorm comes with a big silver lining.  Namely, by using Summit, hopefully it will now be possible to verify Google’s hardest (53-qubit and depth-20) sampling computations directly!  This should provide an excellent test, since not even the Google group themselves would’ve known how to cheat and bias the results had they wanted to.</p>



<p>This whole episode has demonstrated the importance, when doing a sampling-based quantum supremacy experiment, of <em>going deep into the regime where you can no longer classically verify the outputs</em>, as weird as that sounds.  Namely, you need to leave yourself a margin, in the likely event that the classical algorithms improve!</p>



<p>Having said that, I don’t mind revealing at this point that the lack of direct verification of the outputs, for the largest reported speedups, was my single biggest complaint when I reviewed Google’s <em>Nature</em> submission.  It was because of my review that they added a paragraph explicitly pointing out that they <em>did</em> do direct verification for a smaller quantum speedup:</p>



<blockquote class="wp-block-quote"><p>The largest circuits for which the fidelity can still be directly verified have 53 qubits and a simplified gate arrangement. Performing random circuit sampling on these at 0.8% fidelity takes one million cores 130 seconds, corresponding to a million-fold speedup of the quantum processor relative to a single core. </p></blockquote>



<p>(An earlier version of this post misstated the numbers involved.)</p>



<p><strong>(3) The asymptotic hardness of spoofing Google’s benchmark.</strong></p>



<p>OK, but if Google thought that spoofing its test would take 10,000 years, using the best known classical algorithms running on the world’s top supercomputers, and it turns out instead that it could probably be done in more like 2.5 days, then how much else could’ve been missed?  Will we find out next that Google’s benchmark can be classically spoofed in mere milliseconds?</p>



<p>Well, no one can rule that out, but we do have some reasons to think that it’s unlikely—and crucially, that even if it turned out to be true, one would just have to add 10 or 20 or 30 more qubits to make it no longer true.  (We can’t be more definitive than that?  Aye, such are the perils of life at a technological inflection point—and of computational complexity itself.)</p>



<p>The key point to understand here is that we really are talking about simulating a <em>random</em> quantum circuit, with no particular structure whatsoever.  While such problems <em>might</em> have a theoretically efficient classical algorithm—i.e., one that runs in time polynomial in the number of qubits—I’d personally be much less surprised if you told me there was a polynomial-time classical algorithm for factoring.  In the universe where amplitudes of random quantum circuits turn out to be efficiently computable—well, you might as well just tell me that P=PSPACE and be done with it.</p>



<p>Crucially, if you look at IBM’s approach to simulating quantum circuits classically, <em>and</em> Johnnie Gray’s approach, <em>and</em> Google’s approach, they could all be described as different flavors of “brute force.”  That is, they all use extremely clever tricks to parallelize, shave off constant factors, make the best use of available memory, etc., but none involves any deep new mathematical insight that could roust BPP and BQP and the other complexity gods from their heavenly slumber.  More concretely, none of these approaches seem to have any hope of “breaching the 2<sup>n</sup> barrier,” where n is the number of qubits in the quantum circuit to be simulated (assuming that the circuit depth is reasonably large).  Mostly, they’re just trying to get down to that barrier, while taking the maximum advantage of whatever storage and connectivity and parallelism are there.</p>



<p>Ah, but at the end of the day, we only believe that Google’s Sycamore chip is solving a classically hard problem because of the statistical test that Google applies to its outputs: the so-called “Linear Cross-Entropy Benchmark,” which I described in Q3 of my <a href="https://www.scottaaronson.com/blog/?p=4317">FAQ</a>.  And even if we grant that calculating the output probabilities for a random quantum circuit is almost certainly classically hard, and sampling the output distribution of a random quantum circuit is almost certainly classically hard—still, couldn’t <em>spoofing Google’s benchmark</em> be classically easy?</p>



<p>This last question is where complexity theory can contribute something to the story.  A couple weeks ago, UT undergraduate Sam Gunn and I adapted the hardness analysis from my and Lijie Chen’s 2017 paper <a href="https://arxiv.org/abs/1612.05903">“Complexity-Theoretic Foundations of Quantum Supremacy Experiments,”</a> to talk directly about the classical hardness of spoofing the Linear Cross-Entropy benchmark.  Our short paper about this <s>should be on the arXiv later this week (or early next week, given that there are no arXiv updates on Friday or Saturday nights)</s> <a href="https://arxiv.org/abs/1910.12085">here it is</a>.</p>



<p>Briefly, Sam and I show that if you had a sub-2<sup>n</sup> classical algorithm to spoof the Linear Cross-Entropy benchmark, then you’d also have a sub-2<sup>n</sup> classical algorithm that, given as input a random quantum circuit, could estimate a <em>specific</em> output probability (for example, that of the all-0 string) with variance at least <em>slightly</em> (say, Ω(2<sup>-3n</sup>)) better than that of the trivial estimator that just always guesses 2<sup>-n</sup>.  Or in other words: we show that spoofing Google’s benchmark is no easier than the general problem of nontrivially estimating amplitudes in random quantum circuits.  Furthermore, this result automatically generalizes to the case of noisy circuits: all that the noise affects is the threshold for the Linear Cross-Entropy benchmark, and thus (indirectly) the number of samples one needs to take with the QC.  Our result helps to explain why, indeed, neither IBM nor Johnnie Gray nor anyone else suggested any attack that’s specific to Google’s Linear Cross-Entropy benchmark: they all simply attack the general problem of calculating the final amplitudes.</p>



<p><strong>(4) Why use Linear Cross-Entropy at all?</strong></p>



<p>In the comments of my FAQ, some people wondered why Google chose the Linear Cross-Entropy benchmark specifically—especially since they’d used a different benchmark (<em>multiplicative</em> cross-entropy, which unlike the linear version actually <em>is</em> a cross-entropy) in their earlier papers.  I asked John Martinis this question, and his answer was simply that linear cross-entropy had the lowest variance of any estimator they tried.  Since I <em>also</em> like linear cross-entropy—it turns out, for example, to be convenient for the analysis of my certified randomness protocol—I’m 100% happy with their choice.  Having said that, there are many other choices of benchmark that would’ve also worked fine, and with roughly the same level of theoretical justification.</p>



<p><strong>(5) Controlled-Z versus iSWAP gates.</strong></p>



<p>Another interesting detail from the Google paper is that, in their previous hardware, they could implement a particular 2-qubit gate called the Controlled-Z.  For their quantum supremacy demonstration, on the other hand, they modified their hardware to implement a different 2-qubit gate called the <s>iSWAP</s> some weird combination of iSWAP and Controlled-Z; see the comments section for more.  Now, this other gate has no known advantages over the Controlled-Z, for any applications like quantum simulation or Shor’s algorithm or Grover search.  Why then did Google make the switch?  Simply because, with certain classical simulation methods that they’d been considering, the simulation’s running time grows like 4 to the power of the number of these other gates, but only like 2 to the power of the number of Controlled-Z gates!  In other words, they made this engineering choice purely and entirely to make a classical simulation of their device sweat more.  This seems totally fine and entirely within the rules to me.  (Alas, this choice has no effect on a proposed simulation method like IBM’s.)</p>



<p><strong>(6) Gil Kalai’s objections.</strong></p>



<p>Over the past month, <em>Shtetl-Optimized</em> regular and noted quantum computing skeptic Gil Kalai has been posting one objection to the Google experiment after another on his <a href="https://gilkalai.wordpress.com/">blog</a>.  Unlike the IBM group and many of Google’s other critics, Gil completely accepts the centrality of quantum supremacy as a goal.  Indeed, he’s firmly predicted for years that quantum supremacy could never be achieved for fundamental reasons—and he agrees that the Google result, if upheld, would refute his worldview.  Gil also has no dispute with the exponential classical hardness of the problem that Google is solving.</p>



<p>Instead, Gil—if we’re talking not about <a href="https://en.wikipedia.org/wiki/Straw_man#Steelmanning">“steelmanning”</a> his beliefs, but about what he himself actually said—has taken the position that the Google experiment must’ve been done wrong and will need to be retracted.  He’s offered varying grounds for this.  First he said that Google never computed the full histogram of probabilities with a smaller number of qubits (for which such an experiment is feasible), which would be an important sanity check.  Except, it turns out they <em>did</em> do that, and it’s in their <a href="https://arxiv.org/abs/1709.06678">2018 <em>Science</em> paper</a>.  Next he said that the experiment is invalid because the qubits have to be calibrated in a way that depends on the specific circuit to be applied.  Except, this too turns out to be false: John Martinis explicitly confirmed for me that once the qubits are calibrated, you can run any circuit on them that you want.  In summary, unlike the objections of the IBM group, so far I’ve found Gil’s objections to be devoid of scientific interest or merit.</p>



<p><strong><font color="red">Update #1:</font></strong> Alas, I’ll have limited availability today for answering comments, since we’ll be grading the midterm exam for my Intro to Quantum Information Science course!  I’ll try to handle the backlog tomorrow (Thursday).</p>



<p><strong><font color="red">Update #2:</font></strong> Aaannd … timed to coincide with the Google paper, last night the group of Jianwei Pan and Chaoyang Lu put up a <a href="https://arxiv.org/abs/1910.09930">preprint on the arXiv</a> reporting a BosonSampling experiment with <s>20 photons</s> 14 photons observed out of 20 generated (the previous record had been 6 photons).   At this stage of the quantum supremacy race, many had of course written off BosonSampling—or said that its importance was mostly historical, in that it inspired Google’s random circuit sampling effort.  I’m thrilled to see BosonSampling itself take such a leap; hopefully, this will eventually lead to a demonstration that BosonSampling was (is) a viable pathway to quantum supremacy as well.  And right now, with fault-tolerance still having been demonstrated in <em>zero</em> platforms, we need all the viable pathways we can get.  What an exciting day for the field.</p></div>
    </content>
    <updated>2019-10-23T15:50:15Z</updated>
    <published>2019-10-23T15:50:15Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Bell's Theorem? But a Flesh Wound!"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-10-29T03:09:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=674</id>
    <link href="https://emanueleviola.wordpress.com/2019/10/22/newton-ma-votes-on-november-5-residents-vs-developers/" rel="alternate" type="text/html"/>
    <title>Newton MA votes on November 5: Residents vs. developers</title>
    <summary>Historically, progressive people have been understandably quite skeptical of big business, including developers. (I hesitated before using the word “progressive” because the meaning is obscure, and there are several related words, like “liberal” and so on. But the meaning on this post should be clear.) Recently, something shocking happened. Self-declared progressive people in Newton have […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Historically, progressive people have been understandably quite skeptical of big business, including developers. (I hesitated before using the word “progressive” because the meaning is obscure, and there are several related words, like “liberal” and so on. But the meaning on this post should be clear.)</p>
<p>Recently, something shocking happened. Self-declared progressive people in Newton have come to believe that the way to solve the world’s problems is to slash regulations, rewrite zoning documents, chop down forests, and give a free hand to developers (not residents in Newton) to build whatever they want, no questions asked.  (Wait, we are putting solar panels on the new roofs!)</p>
<p>As a consequence, there is now a heated  battle in Newton, ward for ward, to try to protect our city against this well-funded and politically well-connected assault.</p>
<p>And we are not even discussing if we should build a mega complex as opposed to creating new green spaces and protected bike lanes, or improving public transportation, or finally having a gym and a swimming pool — all things that would improve our health and the quality of life.  The discussion is just how big the mega complex should be.</p>
<p><a href="https://www.newtonvotes.org/">This website describes the issues and tells you who to vote for.</a></p>
<p><a href="https://rightsizenewton.org/">This one too (endorsements have strong overlap with above but are not identical).</a></p>
<p><a href="http://www.newtonma.gov/gov/elections/upcoming_elections.asp">Sample ballots are here.</a></p>
<p> </p></div>
    </content>
    <updated>2019-10-22T18:04:55Z</updated>
    <published>2019-10-22T18:04:55Z</published>
    <category term="Uncategorized"/>
    <category term="marijuana"/>
    <category term="Utopia"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2019-10-29T03:21:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/22/postdoc-at-the-euler-international-mathematical-institute-apply-by-november-30-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/22/postdoc-at-the-euler-international-mathematical-institute-apply-by-november-30-2019/" rel="alternate" type="text/html"/>
    <title>postdoc at The Euler International Mathematical Institute (apply by November 30, 2019)</title>
    <summary>The Euler International Mathematical Institute in St.Petersburg is seeking postdocs in Math, TCS, Mathematical and Theoretical Physics. St.Petersburg is the most beautiful city in the world and has multiple mathematical locations including Steklov Institute of Mathematics and Dept. of Mathematics and CS in St.Petersburg State Univ. The preference is given to applications sent before 11/30/19. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Euler International Mathematical Institute in St.Petersburg is seeking<br/>
postdocs in Math, TCS, Mathematical and Theoretical Physics. St.Petersburg is the most beautiful city in the world and has multiple mathematical locations including Steklov Institute of Mathematics and Dept. of Mathematics and CS in St.Petersburg State Univ. The preference is given to applications sent before 11/30/19.</p>
<p>Website: <a href="http://math-cs.spbu.ru/en/news/news-2019-10-22/">http://math-cs.spbu.ru/en/news/news-2019-10-22/</a><br/>
Email: euler.postdoc@gmail.com</p></div>
    </content>
    <updated>2019-10-22T08:52:30Z</updated>
    <published>2019-10-22T08:52:30Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-29T03:20:44Z</updated>
    </source>
  </entry>
</feed>
