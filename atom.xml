<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-07-04T18:39:13Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/093</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/093" rel="alternate" type="text/html"/>
    <title>TR21-093 |  Bounded Indistinguishability for Simple Sources | 

	Yuval Filmus, 

	Andrej Bogdanov, 

	Yuval Ishai, 

	Akshayaram Srinivasan, 

	Krishnamoorthy Dinesh, 

	Avi Kaplan</title>
    <summary>A pair of sources $\mathbf{X},\mathbf{Y}$ over $\{0,1\}^n$ are $k$-indistinguishable if their projections to any $k$ coordinates are identically distributed. Can some $\mathit{AC^0}$ function distinguish between two such sources when $k$ is big, say $k=n^{0.1}$? Braverman's theorem (Commun. ACM 2011) implies a negative answer when $\mathbf{X}$ is uniform, whereas Bogdanov et al. (Crypto 2016) observe that this is not the case in general.

We initiate a systematic study of this question for natural classes of low-complexity sources, including ones that arise in cryptographic applications, obtaining positive results, negative results, and barriers. In particular: 

– There exist $\Omega(\sqrt{n})$-indistinguishable $\mathbf{X},\mathbf{Y}$, samplable by degree $O(\log n)$ polynomial maps (over $\mathbb{F}_2$) and by $\mathit{poly}(n)$-size decision trees, that are $\Omega(1)$-distinguishable by OR.

– There exists a function $f$ such that all $f(d, \epsilon)$-indistinguishable $\mathbf{X},\mathbf{Y}$ that are samplable by degree-$d$ polynomial maps are $\epsilon$-indistinguishable by OR for all sufficiently large $n$.  Moreover, $f(1, \epsilon) = \lceil\log(1/\epsilon)\rceil + 1$ and $f(2, \epsilon) = O(\log^{10}(1/\epsilon))$. 

– Extending (weaker versions of) the above negative results to $\mathit{AC^0}$ distinguishers would require  settling a conjecture of Servedio and Viola (ECCC 2012).
Concretely, if every pair of $n^{0.9}$-indistinguishable $\mathbf{X},\mathbf{Y}$ that are samplable by linear maps is $\epsilon$-indistinguishable by $\mathit{AC^0}$ circuits, then the binary inner product function can have at most an $\epsilon$-correlation with $\mathit{AC^0}\circ\oplus$ circuits.</summary>
    <updated>2021-07-04T02:37:53Z</updated>
    <published>2021-07-04T02:37:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-04T18:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/</id>
    <link href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/" rel="alternate" type="text/html"/>
    <title>ADFOCS 2021: Convex Optimization and Graph Algorithms</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 26 – August 13, 2021 Online https://conferences.mpi-inf.mpg.de/adfocs-22/ ADFOCS is an international summer school held annually at the Max Planck Institute for Informatics (MPII). The topic of this year’s edition is Convex Optimization and its applications on Graph Algorithms. The event will take place online over the span of three weeks from July 26 to … <a class="more-link" href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/">Continue reading <span class="screen-reader-text">ADFOCS 2021: Convex Optimization and Graph Algorithms</span></a></div>
    </summary>
    <updated>2021-07-03T13:59:31Z</updated>
    <published>2021-07-03T13:59:31Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-07-04T18:38:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00629</id>
    <link href="http://arxiv.org/abs/2107.00629" rel="alternate" type="text/html"/>
    <title>Modular counting of subgraphs: Matchings, matching-splittable graphs, and paths</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Curticapean:Radu.html">Radu Curticapean</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dell:Holger.html">Holger Dell</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Husfeldt:Thore.html">Thore Husfeldt</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00629">PDF</a><br/><b>Abstract: </b>We systematically investigate the complexity of counting subgraph patterns
modulo fixed integers. For example, it is known that the parity of the number
of $k$-matchings can be determined in polynomial time by a simple reduction to
the determinant. We generalize this to an $n^{f(t,s)}$-time algorithm to
compute modulo $2^t$ the number of subgraph occurrences of patterns that are
$s$ vertices away from being matchings. This shows that the known
polynomial-time cases of subgraph detection (Jansen and Marx, SODA 2015) carry
over into the setting of counting modulo $2^t$.
</p>
<p>Complementing our algorithm, we also give a simple and self-contained proof
that counting $k$-matchings modulo odd integers $q$ is Mod_q-W[1]-complete and
prove that counting $k$-paths modulo $2$ is Parity-W[1]-complete, answering an
open question by Bj\"orklund, Dell, and Husfeldt (ICALP 2015).
</p></div>
    </summary>
    <updated>2021-07-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00572</id>
    <link href="http://arxiv.org/abs/2107.00572" rel="alternate" type="text/html"/>
    <title>Orienting (hyper)graphs under explorable stochastic uncertainty</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bampis:Evripidis.html">Evripidis Bampis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=uuml=rr:Christoph.html">Christoph Dürr</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Erlebach:Thomas.html">Thomas Erlebach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lima:Murilo_S=_de.html">Murilo S. de Lima</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Megow:Nicole.html">Nicole Megow</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schl=ouml=ter:Jens.html">Jens Schlöter</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00572">PDF</a><br/><b>Abstract: </b>Given a hypergraph with uncertain node weights following known probability
distributions, we study the problem of querying as few nodes as possible until
the identity of a node with minimum weight can be determined for each
hyperedge. Querying a node has a cost and reveals the precise weight of the
node, drawn from the given probability distribution. Using competitive
analysis, we compare the expected query cost of an algorithm with the expected
cost of an optimal query set for the given instance. For the general case, we
give a polynomial-time $f(\alpha)$-competitive algorithm, where $f(\alpha)\in
[1.618+\epsilon,2]$ depends on the approximation ratio $\alpha$ for an
underlying vertex cover problem. We also show that no algorithm using a similar
approach can be better than $1.5$-competitive. Furthermore, we give
polynomial-time $4/3$-competitive algorithms for bipartite graphs with
arbitrary query costs and for hypergraphs with a single hyperedge and uniform
query costs, with matching lower bounds.
</p></div>
    </summary>
    <updated>2021-07-03T22:51:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00526</id>
    <link href="http://arxiv.org/abs/2107.00526" rel="alternate" type="text/html"/>
    <title>Asymptotically Optimal Welfare of Posted Pricing for Multiple Items with MHR Distributions</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Braun:Alexander.html">Alexander Braun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Buttkus:Matthias.html">Matthias Buttkus</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kesselheim:Thomas.html">Thomas Kesselheim</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00526">PDF</a><br/><b>Abstract: </b>We consider the problem of posting prices for unit-demand buyers if all $n$
buyers have identically distributed valuations drawn from a distribution with
monotone hazard rate. We show that even with multiple items asymptotically
optimal welfare can be guaranteed.
</p>
<p>Our main results apply to the case that either a buyer's value for different
items are independent or that they are perfectly correlated. We give mechanisms
using dynamic prices that obtain a $1 - \Theta \left( \frac{1}{\log
n}\right)$-fraction of the optimal social welfare in expectation. Furthermore,
we devise mechanisms that only use static item prices and are $1 - \Theta
\left( \frac{\log\log\log n}{\log n}\right)$-competitive compared to the
optimal social welfare. As we show, both guarantees are asymptotically optimal,
even for a single item and exponential distributions.
</p></div>
    </summary>
    <updated>2021-07-03T22:50:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00446</id>
    <link href="http://arxiv.org/abs/2107.00446" rel="alternate" type="text/html"/>
    <title>Compression by Contracting Straight-Line Programs</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ganardi:Moses.html">Moses Ganardi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00446">PDF</a><br/><b>Abstract: </b>In grammar-based compression a string is represented by a context-free
grammar, also called a straight-line program (SLP), that generates only that
string. We refine a recent balancing result stating that one can transform an
SLP of size $g$ in linear time into an equivalent SLP of size $O(g)$ so that
the height of the unique derivation tree is $O(\log N)$ where $N$ is the length
of the represented string (FOCS 2019). We introduce a new class of balanced
SLPs, called contracting SLPs, where for every rule $A \to \beta_1 \dots
\beta_k$ the string length of every variable $\beta_i$ on the right-hand side
is smaller by a constant factor than the string length of $A$. In particular,
the derivation tree of a contracting SLP has the property that every subtree
has logarithmic height in its leaf size. We show that a given SLP of size $g$
can be transformed in linear time into an equivalent contracting SLP of size
$O(g)$ with rules of constant length.
</p>
<p>We present an application to the navigation problem in compressed unranked
trees, represented by forest straight-line programs (FSLPs). We extend a linear
space data structure by Reh and Sieber (2020) by the operation of moving to the
$i$-th child in time $O(\log d)$ where $d$ is the degree of the current node.
Contracting SLPs are also applied to the finger search problem over
SLP-compressed strings where one wants to access positions near to a
pre-specified finger position, ideally in $O(\log d)$ time where $d$ is the
distance between the accessed position and the finger. We give a linear space
solution where one can access symbols or move the finger in time $O(\log d +
\log^{(t)} N)$ for any constant $t$ where $\log^{(t)} N$ is the $t$-fold
logarithm of $N$. This improves a previous solution by Bille, Christiansen,
Cording, and G{\o}rtz (2018) with access/move time $O(\log d + \log \log N)$.
</p></div>
    </summary>
    <updated>2021-07-03T22:50:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00403</id>
    <link href="http://arxiv.org/abs/2107.00403" rel="alternate" type="text/html"/>
    <title>On Variants of Facility Location Problem with Outliers</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dabas:Rajni.html">Rajni Dabas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Neelima.html">Neelima Gupta</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00403">PDF</a><br/><b>Abstract: </b>In this work, we study the extension of two variants of the facility location
problem (FL) to make them robust towards a few distantly located clients.
First, $k$-facility location problem ($k$FL), a common generalization of FL and
$k$ median problems, is a well studied problem in literature. In the second
variant, lower bounded facility location (LBFL), we are given a bound on the
minimum number of clients that an opened facility must serve. Lower bounds are
required in many applications like profitability in commerce and load balancing
in transportation problem. In both the cases, the cost of the solution may be
increased grossly by a few distantly located clients, called the outliers.
Thus, in this work, we extend $k$FL and LBFL to make them robust towards the
outliers. For $k$FL with outliers ($k$FLO) we present the first (constant)
factor approximation violating the cardinality requirement by +1. As a
by-product, we also obtain the first approximation for FLO based on
LP-rounding. For LBFLO, we present a tri-criteria solution with a trade-off
between the violations in lower bounds and the number of outliers. With a
violation of $1/2$ in lower bounds, we get a violation of $2$ in outliers.
</p></div>
    </summary>
    <updated>2021-07-03T22:50:08Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00378</id>
    <link href="http://arxiv.org/abs/2107.00378" rel="alternate" type="text/html"/>
    <title>Evidence for Long-Tails in SLS Algorithms</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/W=ouml=rz:Florian.html">Florian Wörz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lorenz:Jan=Hendrik.html">Jan-Hendrik Lorenz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00378">PDF</a><br/><b>Abstract: </b>Stochastic local search (SLS) is a successful paradigm for solving the
satisfiability problem of propositional logic. A recent development in this
area involves solving not the original instance, but a modified, yet logically
equivalent one. Empirically, this technique was found to be promising as it
improves the performance of state-of-the-art SLS solvers.
</p>
<p>Currently, there is only a shallow understanding of how this modification
technique affects the runtimes of SLS solvers. Thus, we model this modification
process and conduct an empirical analysis of the hardness of logically
equivalent formulas. Our results are twofold. First, if the modification
process is treated as a random process, a lognormal distribution perfectly
characterizes the hardness; implying that the hardness is long-tailed. This
means that the modification technique can be further improved by implementing
an additional restart mechanism. Thus, as a second contribution, we
theoretically prove that all algorithms exhibiting this long-tail property can
be further improved by restarts. Consequently, all SAT solvers employing this
modification technique can be enhanced.
</p></div>
    </summary>
    <updated>2021-07-03T22:47:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00341</id>
    <link href="http://arxiv.org/abs/2107.00341" rel="alternate" type="text/html"/>
    <title>Technical Report: Anti-unification of Unordered Goals</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yernaux:Gonzague.html">Gonzague Yernaux</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vanhoof:Wim.html">Wim Vanhoof</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00341">PDF</a><br/><b>Abstract: </b>Anti-unification in logic programming refers to the process of capturing
common syntactic structure among given goals, computing as such a single new
goal that is more general and hence called a generalization of the given goals.
Finding an arbitrary common generalization for two goals is trivial, but
looking for those common generalizations that are either as large as possible
(called largest common generalizations) or as specific as possible (called most
specific generalizations) is a non-trivial optimization problem, in particular
when goals are considered to be unordered sets of atoms. In this work we
provide an in-depth study of the problem by defining two different
generalization relations. We formulate a characterization of what constitutes a
most specific generalization in both settings. While these generalizations can
be computed in polynomial time, we show that when the number of variables in
the generalization needs to be minimized, the problem becomes NP-hard. We
subsequently revisit an abstraction of the largest common generalization when
anti-unification is based on injective variable renamings, and prove that it
can be computed in polynomially bounded time.
</p></div>
    </summary>
    <updated>2021-07-03T22:42:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00314</id>
    <link href="http://arxiv.org/abs/2107.00314" rel="alternate" type="text/html"/>
    <title>Backtracking (the) Algorithms on the Hamiltonian Cycle Problem</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sleegers:Joeri.html">Joeri Sleegers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berg:Daan_van_den.html">Daan van den Berg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00314">PDF</a><br/><b>Abstract: </b>Even though the Hamiltonian cycle problem is NP-complete, many of its problem
instances aren't. In fact, almost all the hard instances reside in one area:
near the Koml\'os-Szemer\'edi bound, of $\frac{1}{2}\ v\cdot ln(v) +
\frac{1}{2}\ v\cdot ln( ln(v))$ edges, where randomly generated graphs have an
approximate 50\% chance of being Hamiltonian. If the number of edges is either
much higher or much lower, the problem is not hard -- most backtracking
algorithms decide such instances in (near) polynomial time. Recently however,
targeted search efforts have identified very hard Hamiltonian cycle problem
instances very far away from the Koml\'os-Szemer\'edi bound. In that study, the
used backtracking algorithm was Vandegriend-Culberson's, which was supposedly
the most efficient of all Hamiltonian backtracking algorithms.
</p>
<p>In this paper, we make a unified large scale quantitative comparison for the
best known backtracking algorithms described between 1877 and 2016. We confirm
the suspicion that the Koml\'os-Szemer\'edi bound is a hard area for all
backtracking algorithms, but also that Vandegriend-Culberson is indeed the most
efficient algorithm, when expressed in consumed computing time. When measured
in recursive effectiveness however, the algorithm by Frank Rubin, almost half a
century old, performs best. In a more general algorithmic assessment, we
conjecture that edge pruning and non-Hamiltonicity checks might be largely
responsible for these recursive savings. When expressed in system time however,
denser problem instances require much more time per recursion. This is most
likely due to the costliness of the extra search pruning procedures, which are
relatively elaborate. We supply large amounts of experimental data, and a
unified single-program implementation for all six algorithms. All data and
algorithmic source code is made public for further use by our colleagues.
</p></div>
    </summary>
    <updated>2021-07-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00251</id>
    <link href="http://arxiv.org/abs/2107.00251" rel="alternate" type="text/html"/>
    <title>$L_p$ Isotonic Regression Algorithms Using an $L_0$ Approach</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stout:Quentin_F=.html">Quentin F. Stout</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00251">PDF</a><br/><b>Abstract: </b>Significant advances in maximum flow algorithms have changed the relative
performance of various approaches to isotonic regression. If the transitive
closure is given then the standard approach used for $L_0$ (Hamming distance)
isotonic regression (finding anti-chains in the transitive closure of the
violator graph), combined with new flow algorithms, gives an $L_1$ algorithm
taking $\tilde{\Theta}(n^2+n^\frac{3}{2} \log U )$ time, where $U$ is the
maximum vertex weight. The previous fastest was $\Theta(n^3)$. Similar results
are obtained for $L_2$ and for $L_p$ approximations, $1 &lt; p &lt; \infty$. For
weighted points in $d$-dimensional space with coordinate-wise ordering, $d \geq
3$, $L_0, L_1$ and $L_2$ regressions can be found in only $o(n^\frac{3}{2}
\log^d n \log U)$ time, improving on the previous best of $\tilde{\Theta}(n^2
\log^d n)$, and for unweighted points the time is $O(n^{\frac{4}{3}+o(1)}
\log^d n)$.
</p></div>
    </summary>
    <updated>2021-07-03T22:49:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00223</id>
    <link href="http://arxiv.org/abs/2107.00223" rel="alternate" type="text/html"/>
    <title>Circuit Complexity of Visual Search</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uchizawa:Kei.html">Kei Uchizawa</a>, Haruki Abe <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00223">PDF</a><br/><b>Abstract: </b>We study computational hardness of feature and conjunction search through the
lens of circuit complexity. Let $x = (x_1, ... , x_n)$ (resp., $y = (y_1, ... ,
y_n)$) be Boolean variables each of which takes the value one if and only if a
neuron at place $i$ detects a feature (resp., another feature). We then simply
formulate the feature and conjunction search as Boolean functions ${\rm
FTR}_n(x) = \bigvee_{i=1}^n x_i$ and ${\rm CONJ}_n(x, y) = \bigvee_{i=1}^n x_i
\wedge y_i$, respectively. We employ a threshold circuit or a discretized
circuit (such as a sigmoid circuit or a ReLU circuit with discretization) as
our models of neural networks, and consider the following four computational
resources: [i] the number of neurons (size), [ii] the number of levels (depth),
[iii] the number of active neurons outputting non-zero values (energy), and
[iv] synaptic weight resolution (weight).
</p>
<p>We first prove that any threshold circuit $C$ of size $s$, depth $d$, energy
$e$ and weight $w$ satisfies $\log rk(M_C) \le ed (\log s + \log w + \log n)$,
where $rk(M_C)$ is the rank of the communication matrix $M_C$ of a
$2n$-variable Boolean function that $C$ computes. Since ${\rm CONJ}_n$ has rank
$2^n$, we have $n \le ed (\log s + \log w + \log n)$. Thus, an exponential
lower bound on the size of even sublinear-depth threshold circuits exists if
the energy and weight are sufficiently small. Since ${\rm FTR}_n$ is computable
independently of $n$, our result suggests that computational capacity for the
feature and conjunction search are different. We also show that the inequality
is tight up to a constant factor if $ed = o(n/ \log n)$. We next show that a
similar inequality holds for any discretized circuit. Thus, if we regard the
number of gates outputting non-zero values as a measure for sparse activity,
our results suggest that larger depth helps neural networks to acquire sparse
activity.
</p></div>
    </summary>
    <updated>2021-07-03T22:41:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00216</id>
    <link href="http://arxiv.org/abs/2107.00216" rel="alternate" type="text/html"/>
    <title>Almost-Orthogonal Bases for Inner Product Polynomials</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Chris.html">Chris Jones</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potechin:Aaron.html">Aaron Potechin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00216">PDF</a><br/><b>Abstract: </b>In this paper, we consider low-degree polynomials of inner products between a
collection of random vectors. We give an almost orthogonal basis for this
vector space of polynomials when the random vectors are Gaussian, spherical, or
Boolean. In all three cases, our basis admits an interesting combinatorial
description based on the topology of the underlying graph of inner products.
</p>
<p>We also analyze the expected value of the product of two polynomials in our
basis. In all three cases, we show that this expected value can be expressed in
terms of collections of matchings on the underlying graph of inner products. In
the Gaussian and Boolean cases, we show that this expected value is always
non-negative. In the spherical case, we show that this expected value can be
negative but we conjecture that if the underlying graph of inner products is
planar then this expected value will always be non-negative.
</p>
<p>We hope that these polynomials will be a useful analytical tool in settings
where one has a symmetric function of a collection of random or pseudorandom
vectors.
</p></div>
    </summary>
    <updated>2021-07-03T22:41:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00145</id>
    <link href="http://arxiv.org/abs/2107.00145" rel="alternate" type="text/html"/>
    <title>Improved Analysis of Online Balanced Clustering</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bienkowski:Marcin.html">Marcin Bienkowski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=ouml=hm:Martin.html">Martin Böhm</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kouteck=yacute=:Martin.html">Martin Koutecký</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rothvo=szlig=:Thomas.html">Thomas Rothvoß</a>, Jiří Sgall, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vesel=yacute=:Pavel.html">Pavel Veselý</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00145">PDF</a><br/><b>Abstract: </b>In the online balanced graph repartitioning problem, one has to maintain a
clustering of $n$ nodes into $\ell$ clusters, each having $k = n / \ell$ nodes.
During runtime, an online algorithm is given a stream of communication requests
between pairs of nodes: an inter-cluster communication costs one unit, while
the intra-cluster communication is free. An algorithm can change the
clustering, paying unit cost for each moved node.
</p>
<p>This natural problem admits a simple $O(\ell^2 \cdot k^2)$-competitive
algorithm COMP, whose performance is far apart from the best known lower bound
of $\Omega(\ell \cdot k)$. One of open questions is whether the dependency on
$\ell$ can be made linear; this question is of practical importance as in the
typical datacenter application where virtual machines are clustered on physical
servers, $\ell$ is of several orders of magnitude larger than $k$. We answer
this question affirmatively, proving that a simple modification of COMP is
$(\ell \cdot 2^{O(k)})$-competitive.
</p>
<p>On the technical level, we achieve our bound by translating the problem to a
system of linear integer equations and using Graver bases to show the existence
of a ``small'' solution.
</p></div>
    </summary>
    <updated>2021-07-03T22:47:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00072</id>
    <link href="http://arxiv.org/abs/2107.00072" rel="alternate" type="text/html"/>
    <title>A Linear-Time Algorithm for the Common Refinement of Rooted Phylogenetic Trees on a Common Leaf Set</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>David Schaller, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hellmuth:Marc.html">Marc Hellmuth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stadler:Peter_F=.html">Peter F. Stadler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00072">PDF</a><br/><b>Abstract: </b>The problem of finding a common refinement of a set of rooted trees with
common leaf set $L$ appears naturally in mathematical phylogenetics whenever
poorly resolved information on the same taxa from different sources is to be
reconciled. This constitutes a special case of the well-studied supertree
problem, where the leaf sets of the input trees may differ. Algorithms that
solve the rooted tree compatibility problem are of course applicable to this
special case. However, they require sophisticated auxiliary data structures and
have a running time of at least $O(k|L|\log^2(k|L|))$ for $k$ input trees.
Here, we show that the problem can be solved in $O(k|L|)$ time using a simple
bottom-up algorithm called LinCR. An implementation of LinCR in Python is
freely available at https://github.com/david-schaller/tralda.
</p></div>
    </summary>
    <updated>2021-07-03T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.00068</id>
    <link href="http://arxiv.org/abs/2107.00068" rel="alternate" type="text/html"/>
    <title>Robust Coreset for Continuous-and-Bounded Learning (with Outliers)</title>
    <feedworld_mtime>1625270400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Zixiu.html">Zixiu Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guo:Yiwen.html">Yiwen Guo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Hu.html">Hu Ding</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.00068">PDF</a><br/><b>Abstract: </b>In this big data era, we often confront large-scale data in many machine
learning tasks. A common approach for dealing with large-scale data is to build
a small summary, {\em e.g.,} coreset, that can efficiently represent the
original input. However, real-world datasets usually contain outliers and most
existing coreset construction methods are not resilient against outliers (in
particular, the outliers can be located arbitrarily in the space by an
adversarial attacker). In this paper, we propose a novel robust coreset method
for the {\em continuous-and-bounded learning} problem (with outliers) which
includes a broad range of popular optimization objectives in machine learning,
like logistic regression and $ k $-means clustering. Moreover, our robust
coreset can be efficiently maintained in fully-dynamic environment. To the best
of our knowledge, this is the first robust and fully-dynamic coreset
construction method for these optimization problems. We also conduct the
experiments to evaluate the effectiveness of our robust coreset in practice.
</p></div>
    </summary>
    <updated>2021-07-03T22:48:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8161</id>
    <link href="https://windowsontheory.org/2021/07/02/itc-2021-call-for-participation-guest-post-by-benny-applebaum/" rel="alternate" type="text/html"/>
    <title>ITC 2021: Call for participation (guest post by Benny Applebaum)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The second edition of the recently created conference on Information-Theoretic Cryptography (ITC 2021) will take place virtually on July 24-26, 2021. The final program is out and contains exciting new works and invited talks that highlight the recent advances in the area by Benny Applebaum, Elaine Shi, Irit Dinur, Salman Avestimehr, Matthieu Bloch, and Mark … <a class="more-link" href="https://windowsontheory.org/2021/07/02/itc-2021-call-for-participation-guest-post-by-benny-applebaum/">Continue reading <span class="screen-reader-text">ITC 2021: Call for participation (guest post by Benny Applebaum)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The second edition of the recently created conference on <em>Information-Theoretic Cryptography (ITC 2021)</em> will take place virtually on July 24-26, 2021. The final program is out and contains exciting new works and invited talks that highlight the recent advances in the area by Benny Applebaum, Elaine Shi, Irit Dinur, Salman Avestimehr, Matthieu Bloch, and Mark Zhandry.</p>



<p>Registration to the conference is free (but required!)</p>



<p>Visit the webpage <a href="https://itcrypto.github.io/2021/index.html" rel="noreferrer noopener" target="_blank">https://itcrypto.github.io/2021/index.html</a> for more information and for the full program.  </p>



<p>Hope to see you there!</p>



<p>– The Organising Committee</p></div>
    </content>
    <updated>2021-07-02T13:08:48Z</updated>
    <published>2021-07-02T13:08:48Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-07-04T18:38:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1542</id>
    <link href="https://ptreview.sublinear.info/?p=1542" rel="alternate" type="text/html"/>
    <title>Looking back at WOLA’21: Videos available</title>
    <summary>The fifth Workshop on Local Algorithms (WOLA’21) took place earlier this month, and the recordings of the invited talks are now available on YouTube. If you missed the workshop, or want to refresh your memory, here are the recordings (ordered by the workshop schedule): James Aspnes (Yale) on Population Protocols Uri Stemmer (Ben-Gurion University) on […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The fifth <a href="http://www.local-algorithms.com/">Workshop on Local Algorithms</a> (WOLA’21) took place <a href="https://ptreview.sublinear.info/?p=1517">earlier this month</a>, and the recordings of the invited talks are now <a href="https://www.youtube.com/watch?v=aFaJz3HPluM&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm">available on YouTube</a>. If you missed the workshop, or want to refresh your memory, here are the recordings (ordered by the workshop schedule):</p>



<ul><li>James Aspnes (Yale) on <a href="https://www.youtube.com/watch?v=aFaJz3HPluM&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=2"><em>Population Protocols</em></a></li><li>Uri Stemmer (Ben-Gurion University) on <em><a href="https://www.youtube.com/watch?v=yxOmND76k4M&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=4">The Local Model of Differential Privacy: A Survey</a></em></li><li>Mary Wootters (Stanford University) on <em><a href="https://www.youtube.com/watch?v=oHGvGgSdvAc&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=7">Lifted Codes and Disjoint Repair Groups</a></em></li><li>Christian Sohler (University of Cologne) on <em><a href="https://www.youtube.com/watch?v=DuJQe0pv224&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=6">Property Testing in Planar Graphs</a></em></li><li>Elaine Shi (Carnegie Mellon University) on <em><a href="https://www.youtube.com/watch?v=jq5MuJJLnDk&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=5">Game-Theoretically Secure Protocols Inspired by Blockchains</a></em></li><li>Jelani Nelson (UC Berkeley) on <em><a href="https://www.youtube.com/watch?v=ZQ1ekannlw0&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=3">Optimal bounds for approximate counting</a></em></li></ul>



<p>Thanks again to the speakers and organizers, and looking forward to WOLA’22!</p></div>
    </content>
    <updated>2021-07-02T04:59:48Z</updated>
    <published>2021-07-02T04:59:48Z</published>
    <category term="Conference reports"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-07-03T22:56:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/092</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/092" rel="alternate" type="text/html"/>
    <title>TR21-092 |  A Note on One-way Functions and Sparse Languages | 

	Yanyi Liu, 

	Rafael Pass</title>
    <summary>We show equivalence between the existence of one-way
functions and the existence of a \emph{sparse} language that is
hard-on-average w.r.t. some efficiently samplable ``high-entropy''
distribution.
In more detail, the following are equivalent:
  - The existentence of a $S(\cdot)$-sparse language $L$ that is
    hard-on-average with respect to some samplable distribution with
    Shannon entropy $h(\cdot)$ such that $h(n)-\log(S(n)) \geq 4\log n$;
  - The existentence of a $S(\cdot)$-sparse language $L \in
    \NP$, that is
    hard-on-average with respect to some samplable distribution with
    Shannon entropy $h(\cdot)$ such that $h(n)-\log(S(n)) \geq n/3$;
  - The existence of one-way functions.

Our results are insipired by, and generalize, the recent elegant paper by Ilango,
Ren and Santhanam (ECCC'21), which presents similar characterizations for
concrete sparse languages.</summary>
    <updated>2021-07-02T01:24:40Z</updated>
    <published>2021-07-02T01:24:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-04T18:37:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4580221543262282386</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4580221543262282386/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/intersecting-classes.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4580221543262282386" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4580221543262282386" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/intersecting-classes.html" rel="alternate" type="text/html"/>
    <title>Intersecting Classes</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>If you have two complexity classes that have complete sets, the intersection might not, for example NP ∩ co-NP. The world of total-function classes acts differently.</p><p>Christos Papadimitriou and others defined a <a href="https://en.wikipedia.org/wiki/TFNP">number of classes</a> based on finding solutions to problems where solutions are known to exists for some combinatorial reason. While TFNP, the set of all such problems, might not have complete sets, all the other classes are defined basically based on the complete search problem for the class, such as <a href="https://en.wikipedia.org/wiki/TFNP#PLS">PLS</a>, finding a local minimum. If you have two such classes <b><span style="font-family: Cedarville Cursive;">A</span></b> and <b><span style="font-family: Cedarville Cursive;">B</span></b> with complete search problems A and B define the search problem D as </p><p style="text-align: center;">D(x,y): Find a solution to either A(x) or B(y)</p><p>D is complete for the intersection of <b><span style="font-family: Cedarville Cursive;">A</span></b> and <span style="font-family: Cedarville Cursive; font-weight: bold;">B</span><span style="font-family: inherit;">:</span><span style="font-family: inherit;"><span style="font-family: inherit;"> </span>First the </span><span style="font-family: inherit;">pr</span>oblem D is in <b><span style="font-family: Cedarville Cursive;">A</span></b> since you can reduce the problem of finding a solution to either A or B to finding a solution to A. Likewise D is in <span style="font-family: Cedarville Cursive;"><b>B</b></span>.</p><p>Suppose you have a problem Z in <b><span style="font-family: Cedarville Cursive;">A</span></b>∩<span style="font-family: Cedarville Cursive; font-weight: bold;">B</span>.<span style="font-family: inherit;"> Then since Z is in </span><b><span style="font-family: Cedarville Cursive;">A</span></b><span style="font-family: inherit;"> and A is complete for </span><span style="font-family: Cedarville Cursive;">A</span><span style="font-family: inherit;">, finding a solution to Z(u) reduces a finding a solution of A(x) where x is easily computed from u. Likewise Z(u) reduces to finding a solution of B(y). So whatever solution D(x,y) gives you, it allows you to find a solution to Z(u). Thus D is complete for </span><b><span style="font-family: Cedarville Cursive;">A</span></b>∩<span style="font-family: Cedarville Cursive; font-weight: bold;">B</span><span>.</span></p><p>Some people nerd out to <a href="https://mars.nasa.gov/technology/helicopter">helicopters on mars</a>. I nerd out to the complexity of complete sets. </p><p>I learned about complete sets of intersections of total function classes from the talk by one of <a href="http://acm-stoc.org/stoc2021/STOCprogram.html">last week's STOC</a> best paper awardees, <a href="https://doi.org/10.1145/3406325.3451052">The Complexity of Gradient Descent</a> by John Fearnley, Paul W. Goldberg, Alexandros Hollender and Rahul Savani. The part above was well known but the paper goes much further.</p><p>Consider <a href="https://blog.computationalcomplexity.org/2005/12/what-is-ppad.html">PPAD</a> famously with Nash Equilibrium as a complete problem and PLS. PPAD ∩ PLS has complete sets by the argument above. But we can go further.</p><p>The class <a href="https://en.wikipedia.org/wiki/TFNP#CLS">CLS</a> is a variation of PLS where you find a local minimum in a continuous domain under some Lipschitz conditions and is known to sit in the intersection of PPAD and PLS. Fearnley et al. look at finding a minimum using gradient descent (the main tool for deep learning), and showing not only is it CLS-compete but complete for PPAD ∩ PLS. As a consequence CLS = PPAD ∩ PLS. Pretty cool stuff.</p></div>
    </content>
    <updated>2021-07-01T14:44:00Z</updated>
    <published>2021-07-01T14:44:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-03T12:21:18Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/06/30/linkage</id>
    <link href="https://11011110.github.io/blog/2021/06/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Flow lines (\(\mathbb{M}\)). Web gadget editable open source code thingy to draw streamlines of mathematical formulas, in svg format, by Maksim Surguy.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://maxoffsky.com/code-blog/flow-lines/">Flow lines</a> (<a href="https://mathstodon.xyz/@11011110/106428733923482392">\(\mathbb{M}\)</a>). Web gadget editable open source code thingy to draw streamlines of mathematical formulas, in svg format, by Maksim Surguy.</p>
  </li>
  <li>
    <p><a href="https://daily.jstor.org/the-soap-bubble-trope/">The soap bubble trope</a> (<a href="https://mathstodon.xyz/@11011110/106430498906810187">\(\mathbb{M}\)</a>, <a href="https://3quarksdaily.com/3quarksdaily/2021/06/soap-bubbles.html">via</a>). Soap bubbles as a recurring theme in art, literature, and popular culture, including “the roof of the Munich Olympic Stadium, Glinda the Good Witch, the first viral ad campaign of the late Victorian era, and morose Dutch still-life paintings”.</p>
  </li>
  <li>
    <p><a href="http://gosper.org/homeplate.html">Officially, home plate doesn’t exist</a> (<a href="https://mathstodon.xyz/@esoterica/106435964222477352">\(\mathbb{M}\)</a>). The rules of baseball define it as a 90-45-90-90-45 pentagon with two 12” sides at one of the right angles and a 17” side between the other two, not possible.</p>
  </li>
  <li>
    <p><a href="https://www.natureindex.com/news-blog/microsoft-academic-graph-discontinued-whats-next">Microsoft Academic Graph being discontinued</a> (<a href="https://mathstodon.xyz/@11011110/106438809410223323">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/06/19/weekend-reads-biotech-ceo-on-leave-after-allegations-on-pubpeer-a-researcher-disavows-his-own-paper-plagiarism-here-there-and-everywhere/">via</a>). I didn’t much use that one but I live in fear that one day Google will do the same thing to Google Scholar, as they have to so many other useful but nonprofitable Google services.</p>
  </li>
  <li>
    <p>My current workflow for preparing technical talk videos (<a href="https://mathstodon.xyz/@11011110/106444988062063872">\(\mathbb{M}\)</a>):</p>

    <ul>
      <li>
        <p>Use LaTeX+beamer (169 option) to make pdf talk slides</p>
      </li>
      <li>
        <p>For each slide, print open-in-Preview with custom 16x9 zero-margin layout then export to png</p>
      </li>
      <li>
        <p>Write a script and use quicktime to record 1-2 minute voiceover clips</p>
      </li>
      <li>
        <p>Compose slides and audio in iMovie, export to a huge mp4</p>
      </li>
      <li>
        <p>Use Handbrake to convert to reasonably-sized mp4</p>
      </li>
    </ul>

    <p>It works, but is a bit tedious and produces very dry results. The discussion includes suggestion of alternatives.</p>
  </li>
  <li>
    <p><a href="https://youtu.be/7vEgc7cNarI">The points rotated, and the lines danced</a> (<a href="https://mastodon.social/@sarielhp/106439137323288004">\(\mathbb{M}\)</a>). Video illustrating point-line duality by Sariel Har-Peled.</p>
  </li>
  <li>
    <p><a href="https://www.bbc.com/future/article/20210616-how-the-forgotten-tricks-of-letterlocking-shaped-history">Letterlocking</a> (<a href="https://mathstodon.xyz/@11011110/106458633659042049">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27549256">via</a>, <a href="https://en.wikipedia.org/wiki/Letterlocking">see also</a>): the art of folding your letters so intricately that readers will be forced to tear the paper to unfold and read them.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html">Bill Gasarch summarizes an online debate</a> (<a href="https://mathstodon.xyz/@11011110/106463907058053228">\(\mathbb{M}\)</a>) with Richard DeMillo and Richard Lipton, moderated by Harry Lewis, looking back at the idea of proving programs correct and at <a href="https://doi.org/10.1145/359104.359106">a classic 1979 paper by DeMillo, Lipton, and Perlis</a> arguing that this idea was already problematic.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/X_%2B_Y_sorting">\(X+Y\) sorting</a> (<a href="https://mathstodon.xyz/@11011110/106468019908924208">\(\mathbb{M}\)</a>), now a Good Article on Wikipedia. This is on an old open problem in comparison sorting: can you sort pairs of elements from two sets by their sums, faster than unstructured data of the same length? It’s still an active topic of research; see e.g. <a href="https://doi.org/10.1145%2F3285953">Kane, Lovett, and Moran, “Near-optimal linear decision trees for \(k\)-sum and related problems”, <em>JACM</em> 2019</a>.</p>

    <p>It was not easy to persuade the GA reviewer that this article was as accessible as it could be. I have hopes of <a href="https://en.wikipedia.org/wiki/Dehn_invariant">Dehn invariant</a> also becoming a Good Article but its “Realizability” section is far more advanced.</p>
  </li>
  <li>
    <p>This week I participated in the International Workshop on Graph-Theoretic Concepts in Computer Science, WG (<a href="https://mathstodon.xyz/@11011110/106474212936524737">\(\mathbb{M}\)</a>). The 9-hour time difference made live participation awkward for me, but fortunately prerecorded contributed talks and the three invited talks (Dujmović on product structures, Samotij on independent set numeration, and Bonnet on twin-width) are linked from <a href="https://wg2021.mimuw.edu.pl/program/">the conference program</a>. The proceedings is not yet out but many preprints of papers are also linked.</p>
  </li>
  <li>
    <p><a href="https://theintercept.com/2021/06/23/anming-hu-trial-fbi-china/">“A juror says the FBI owes an apology to University of Tennessee scientist Anming Hu”</a> (<a href="https://mathstodon.xyz/@11011110/106481625501499687">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/06/25/weekend-reads-the-obesity-wars-and-the-education-of-a-researcher-zombie-research-hijacked-journals/">via</a>) after putting Hu on trial for allegedly hiding ties to China despite his repeated disclosures of those ties and possibly in retaliation for his refusal to become a spy in China for the FBI. Beyond hurting US research both directly and by motivating good people to go elsewhere, this racist witch hunt has provided fuel for Chinese propaganda.</p>
  </li>
  <li>
    <p>The Wikipedia “Book:” namespace for curated collections of articles is being killed off (<a href="https://mathstodon.xyz/@11011110/106484876399456966">\(\mathbb{M}\)</a>) after the software to collate them into pdfs stopped working. I created five of these, and used two as readings for my courses. All five have moved to my user space:</p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Fundamental_Data_Structures"><em>Fundamental Data Structures</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Graph_Algorithms"><em>Graph Algorithms</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Graph_Drawing"><em>Graph Drawing</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Matroid_Theory"><em>Matroid Theory</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Perfect_Graphs"><em>Perfect Graphs</em></a></p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://danilafe.com/blog/math_rendering_is_wrong/">Math rendering is wrong</a> (<a href="https://mathstodon.xyz/@11011110/106492765314826160">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27656446">via</a>). This blog post from a year ago argues that, for the same reasons one might write a web site in a markup language before compiling it to html, we should also compile LaTeX to html at that time rather than using browser-side scripts (as in most deploys of MathJax or KaTeX) or conversion to images (Wikipedia). It doesn’t present a solution, but is more a call for that solution to be made.</p>
  </li>
  <li>
    <p><a href="https://kleinbottle.com/#AMAZON%20BRAND%20HIJACKING">Amazon stands by and does nothing as Chinese scammers hijack Cliff Stoll’s Klein bottle business to usurp its positive reviews</a> (<a href="https://mathstodon.xyz/@11011110/106500881370367192">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27684807">via</a>).</p>
  </li>
</ul></div>
    </content>
    <updated>2021-06-30T10:36:00Z</updated>
    <published>2021-06-30T10:36:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-06-30T17:39:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18933</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/" rel="alternate" type="text/html"/>
    <title>Scaling and Fame</title>
    <summary>Scaling the pandemic is different from scaling the US budget Sydney Morning Herald interview source Terence Tao is now “properly” famous. He was cited earlier this month in the NYT science section for help in explaining large numbers. Numbers such as the US federal budget. Today we discuss caveats on such explanations, after a riff […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Scaling the pandemic is different from scaling the US budget</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/tt/" rel="attachment wp-att-18935"><img alt="" class="alignright wp-image-18935" height="126" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/tt.png?resize=225%2C126&amp;ssl=1" width="225"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Sydney Morning Herald interview <a href="https://www.smh.com.au/lifestyle/terence-tao-the-mozart-of-maths-20150216-13fwcv.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Terence Tao is now “properly” famous. He was cited earlier this month in the NYT <a href="https://www.nytimes.com/2021/06/17/science/math-numbers-federal-budget-tao.html">science</a> section for help in explaining large numbers. Numbers such as the US federal budget.</p>
<p>
Today we discuss caveats on such explanations, after a riff on the popular explanation of mathematics.</p>
<p>
Regarding that, let us forget Tao’s work on primes in progressions with Ben Green, forget the Erdős discrepancy problem, and forget his almost-resolution of the Collatz conjecture. Forget it all. Better than a headline, he got his name embedded into the NYT article’s URL. This was for something much less deep that he wrote in 2009—as a blogger. </p>
<p>
<em>En passant</em>, we mention that Ken has an event tomorrow (Wed. 6/30) at 3:30 ET. It is a webinar hosted by Marc Rotenberg, who heads the Washington-based Center for AI and Digital Policy (<a href="https://www.caidp.org">CAIDP</a>), on “Chess and AI: The Role of Transparency.” Registration is free at this <a href="https://www.caidp.org/events/chess/">link</a>. One aspect of transparency in Ken’s work is that he writes about his model’s methodology here—as a blogger.</p>
<p>
</p><p/><h2> Rescaling the Budget </h2><p/>
<p/><p>
Tao was referenced for a <a href="https://terrytao.wordpress.com/2009/05/04/the-federal-budget-rescaled/">post</a> he wrote in May 2009 when Barack Obama was working on his first budget as President. The ratio of $100 million to $3 that he used scaled the budget income to about $75,000. </p>
<p>
With Joe Biden engaged in budget deliberations, Aiyana Green and Steven Strogatz wrote the NYT <a href="https://www.human.cornell.edu/pam/news/aiyana_green_nyt">article</a> on explaining the US federal budget. Green is a student at Cornell: she just completed her junior year in the Department of Policy Analysis and Management. </p>
<p/><p><br/>
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/agss/" rel="attachment wp-att-18936"><img alt="" class="aligncenter wp-image-18936" height="200" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/AGSS.png?resize=355%2C200&amp;ssl=1" width="355"/></a></p>
<p/><p><br/>
They updated Tao’s post to scale the income to $100,000. Besides being a round number, this is close to the estimated <a href="https://www.in2013dollars.com/us/inflation/2009">inflation since 2009</a>. Here is the NYT graphic of the numbers. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/bud-2/" rel="attachment wp-att-18938"><img alt="" class="aligncenter wp-image-18938" height="500" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/bud.png?resize=545%2C500&amp;ssl=1" width="545"/></a></p>
<p>
</p><p/><h2> A Scaling Caveat </h2><p/>
<p/><p>
It is attractive to apply this scaling trick elsewhere, even to grim subjects like the coronavirus pandemic. But there we find an element that does not scale.</p>
<p>
Suppose we use the same figure of 100,000 to scale down the world’s population. Besides its famous pandemic numbers <a href="https://www.worldometers.info/coronavirus/">pages</a>, Worldometer also keeps a running <a href="https://www.worldometers.info/world-population/">estimate</a> of the total world population, now nearing 7.9 billion. Scaling down means multiplying every ther human number by <img alt="{\gamma =}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cgamma+%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> 0.000012697445274. </p>
<p>
We can think of 100,000 people as a city that is not a metropolis. Scaling down the current pandemic figures, we get:</p>
<ul>
<li>
182,000,000 total cases become <b>2,309</b>. <p/>
</li><li>
11,496,147 active cases become <b>145</b>. <p/>
</li><li>
4 million total deaths (the numbers are approaching that millstone as we write) become <b>50</b> deaths. <p/>
</li><li>
80,346 currently listed in critical or serious condition become <b>exactly one</b>.
</li></ul>
<p>
These numbers are not at all unusual for our size of city if one considers all kinds of illness and mortality. The scaling trick may seem to have reduced the scope of the pandemic, as opposed to statements such as 182 million being over half the US population. Yet in terms of the raw numbers it preserves the proportions.</p>
<p>
What the scaling doesn’t preserve is the proportion of <em>relations</em>. The number of possible binary relations—person <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> knows person <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>—is quadratic in the number <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of people. Suppose the number of pairs who know each other is <img alt="{an^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ban%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a small but fixed constant. (Note: we will redo this with something more reasonable below.) If we then scale <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> down to <img alt="{n' = n\gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%27+%3D+n%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, the situation becomes:</p>
<ul>
<li>
If we estimate the relatedness of our city, we get <img alt="{an'^2 = an^2\gamma^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ban%27%5E2+%3D+an%5E2%5Cgamma%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. <p/>
</li><li>
But if we scaled down the number of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-knows-<img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> relations directly we would get <img alt="{an^2\gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ban%5E2%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is substantially bigger by a factor of <img alt="{\frac{1}{\gamma}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B%5Cgamma%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.
</li></ul>
<p>
Thus what the scaling really underestimates is the impact of how people are affected by their loved ones being among the 182 million (or the worse numbers). The underestimation logic applies to any form <img alt="{an^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ban%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where <img alt="{c &gt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This is not an issue with the budget because dollar bills don’t feel relatedness—at least not so much, even absent a line-item veto. Now we will make values of <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> approaching <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> more reasonable.</p>
<p>
</p><p/><h2> Small World: Tao and Strogatz Again </h2><p/>
<p/><p>
Let’s consider people <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> who have <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> degrees of separation in the graph of who-knows-who. Then there are <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> who know each other such that <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> knows <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> knows <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. If something strikes <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> will feel a deep connection by that impact. The feeling is amplified if <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> have multiple pairs <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that make a path. This quantifies the relation that Tao calls “awareness” in his “Lecture Notes 3 FOR 254A” course <a href="https://rjlipton.wpcomstaging.com/feed/LECTURE NOTES 3 FOR 254A">notes</a> (pages 51–57 overall). A fact way more basic than what Tao is actually talking about in those notes is the following:</p>
<blockquote><p><b> </b> <em> The sum over pairs <img alt="{(X,Y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> of the number of pairs <img alt="{(A,B)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28A%2CB%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> between them who would be affected equals the sum over edges <img alt="{(A,B)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28A%2CB%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> of the number of pairs <img alt="{(X,Y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> they can be struck by: both are equal to the number of paths of length <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the graph. </em>
</p></blockquote>
<p/><p>
That number of paths is what we say is reasonable to model by a function <img alt="{an^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ban%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with <img alt="{c \gg 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cgg+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This is the simplest way of approaching why we feel that treating the pandemic like the US budget underestimates its human effect. One can still rebut that other kinds of illness and death have the same scaling properties, but ultimately we are talking about the <em>excess</em> caused by the pandemic—the effect on top of everything else.</p>
<p>
We have not tried to make this analysis become rigorous using more-realistic models of human networks. Perhaps our readers can point us to such analysis. But one inkling of why we expect our point to be borne out comes from a key conclusion of the famous 1998 <a href="https://www.nature.com/articles/30918">paper</a> of Strogatz with Duncan Watts on ‘small-world’ networks: The phase transition from a lattice network with large average distances to a small-world network with small distances takes place in a range where small clusters cannot recognize it happening locally. </p>
<p>
Thus, if our scaling carried the intuitive picture of an isolated city, it would miss the expanding sphere of relations. At the opposite extreme would be taking the union of Monaco and central Venice, which sum to 100,000 people who fan out mightily. There is also the argument that while small-world networks are held tight by “<a href="https://sociology.stanford.edu/sites/g/files/sbiybj9501/f/publications/the_strength_of_weak_ties_and_exch_w-gans.pdf">weak ties</a>,” the shared knowledge of misery is something that most tends to strengthen ties. And of course, our point about relations extends to many other activities impacted by the pandemic.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What should govern the appropriateness of scaling down?</p>
<p>
It should be noted that if we scale down the US to 100,000 people, the numbers are appreciably higher:</p>
<ul>
<li>
34.5 million total cases become <b>10,366</b>. <p/>
</li><li>
4,928,564 active cases become <b>1,480</b>. <p/>
</li><li>
620,000 total deaths become <b>186</b> deaths. <p/>
</li><li>
3,833 currently listed in critical or serious condition still become <b>exactly one</b>.
</li></ul></font></font></div>
    </content>
    <updated>2021-06-29T23:09:46Z</updated>
    <published>2021-06-29T23:09:46Z</published>
    <category term="All Posts"/>
    <category term="chess"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Aiyana Green"/>
    <category term="coronavirus"/>
    <category term="mathematical writing"/>
    <category term="New York Times"/>
    <category term="pandemic"/>
    <category term="scaling"/>
    <category term="Steven Strogatz"/>
    <category term="Terence Tao"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-07-04T18:37:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=2420</id>
    <link href="https://theorydish.blog/2021/06/29/trace-reconstruction/" rel="alternate" type="text/html"/>
    <title>Trace Reconstruction from Complex Analysis</title>
    <summary>Suppose that is an unknown binary string of length . We are asked to recover from its traces, and each trace is a random subsequence obtained by deleting the bits of independently with probability . More formally, let denote the distribution of traces obtained from string . For example, when , assigns a probability mass of to each element in the multiset We then ask: what is the smallest number such that we can recover any (say, with probability ) given independent samples from ? This trace reconstruction problem was first formulated by Batu-Kannan-Khanna-McGregor in 2004, and their central motivation is from the multiple sequence alignment problem in computational biology. Trace reconstruction is also a fundamental problem related to the deletion channel in communication theory: We view the hidden string as the transmitted message, and each trace as a received message that went through a deletion channel, which drops each bit with probability . Then the sample complexity tells us the number of independent copies that need to be sent for the receiver to determine the original message. Despite being a natural problem, trace reconstruction is still far from being well-understood, even from the information-theoretic perspective (i.e., without considering the [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Suppose that <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an unknown binary string of length <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We are asked to recover <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from its <em>traces</em>, and each <em>trace</em> <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a random subsequence obtained by deleting the bits of <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> independently with probability <img alt="1/2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>More formally, let <img alt="\mathcal{D}_s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> denote the distribution of traces obtained from string <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. For example, when <img alt="s = 110" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+110&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="\mathcal{D}_s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> assigns a probability mass of <img alt="1/8" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F8&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to each element in the multiset<br/></p>



<p class="has-text-align-center"><img alt="\{\text{empty}, 1, 1, 0, 11, 10, 10, 110\}." class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Ctext%7Bempty%7D%2C+1%2C+1%2C+0%2C+11%2C+10%2C+10%2C+110%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/></p>



<p>We then ask: what is the smallest number <img alt="M(n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that we can recover any <img alt="s \in \{0, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (say, with probability <img alt="\ge 0.99" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cge+0.99&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>) given <img alt="M(n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> independent samples from <img alt="\mathcal{D}_s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>?</p>



<p>This <em>trace reconstruction</em> problem was first formulated by <a href="https://people.cs.umass.edu/~mcgregor/papers/04-soda.pdf" rel="noreferrer noopener" target="_blank">Batu-Kannan-Khanna-McGregor</a> in 2004, and their central motivation is from the <a href="https://en.wikipedia.org/wiki/Multiple_sequence_alignment" rel="noreferrer noopener" target="_blank">multiple sequence alignment</a> problem in computational biology. Trace reconstruction is also a fundamental problem related to the <a href="https://en.wikipedia.org/wiki/Deletion_channel" rel="noreferrer noopener" target="_blank">deletion channel</a> in communication theory: We view the hidden string as the transmitted message, and each trace as a received message that went through a deletion channel, which drops each bit with probability <img alt="1/2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then the sample complexity <img alt="M(n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> tells us the number of independent copies that need to be sent for the receiver to determine the original message.</p>



<p>Despite being a natural problem, trace reconstruction is still far from being well-understood, even from the information-theoretic perspective (i.e., without considering the computational complexity). In 2017, <a href="https://arxiv.org/pdf/1612.03148.pdf" rel="noreferrer noopener" target="_blank">De-O’Donnell-Servedio</a> and <a href="https://arxiv.org/pdf/1612.03599.pdf" rel="noreferrer noopener" target="_blank">Nazarov-Peres</a> independently proved <img alt="M(n) \le \exp(O(n^{1/3}))" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29+%5Cle+%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. A very recent breakthrough due to <a href="https://arxiv.org/pdf/2009.03296.pdf" rel="noreferrer noopener" target="_blank">Chase</a> further improved this bound to <img alt="\exp(\tilde O(n^{1/5}))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Ctilde+O%28n%5E%7B1%2F5%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is still super-polynomial. On the other hand, the best known sample complexity lower bound is merely <img alt="\tilde\Omega(n^{3/2})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%5COmega%28n%5E%7B3%2F2%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, proved by <a href="https://arxiv.org/pdf/1905.03031.pdf" rel="noreferrer noopener" target="_blank">Chase</a> in another recent work.</p>



<p>In this blog post, I will explain why this problem is much more non-trivial than it might appear at first glance. I will also give an overview on the work of [DOS17, NP17], which, interestingly, reduces this seemingly combinatorial problem to complex analysis.</p>



<p><strong>Observation: reconstruction <img alt="\approx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> distinguishing <img alt="\approx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> TV-distance.</strong> Let us start with a natural first attempt at the problem. Define <img alt="\delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as the minimum statistical distance between the trace distribution of two different length-<img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> strings:<br/></p>



<p class="has-text-align-center"><img alt="\delta_n = \min_{x \ne y \in \{0, 1\}^n}d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_n+%3D+%5Cmin_%7Bx+%5Cne+y+%5Cin+%5C%7B0%2C+1%5C%7D%5En%7Dd_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/></p>



<p>It is not hard to show that <img alt="1/\delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> bounds <img alt="M(n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on both sides, up to a polynomial factor:<br/></p>



<p class="has-text-align-center"><img alt="1/\delta_n \lesssim M(n) \lesssim n/\delta_n^2." class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta_n+%5Clesssim+M%28n%29+%5Clesssim+n%2F%5Cdelta_n%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/></p>



<p class="has-black-color has-text-color">The lower bound holds because any trace reconstruction algorithm must be able to distinguish <img alt="s = x" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from <img alt="s = y" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for every pair of different strings <img alt="(x, y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and this requires <img alt="\Omega(1 / \delta_n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%281+%2F+%5Cdelta_n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> samples for the minimizer <img alt="(x, y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the definition of <img alt="\delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. For the upper bound, we note that every pair <img alt="(x, y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can be distinguished with an <img alt="o(2^{-n})" class="latex" src="https://s0.wp.com/latex.php?latex=o%282%5E%7B-n%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> error probability using <img alt="O(n/\delta_n^2)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%2F%5Cdelta_n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> samples. We say that string <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> “beats” <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, if the distinguisher for <img alt="(x, y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> decides that “<img alt="s = x" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>“. By a union bound, the correct answer <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> “beats” every other string with high probability. We can then obtain a reconstruction algorithm by running the distinguisher for every string pair, and outputting the unique string that “beats” the other <img alt="2^n-1" class="latex" src="https://s0.wp.com/latex.php?latex=2%5En-1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> strings.</p>



<p>Thus, to determine whether the sample complexity <img alt="M(n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is polynomial in <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, it suffices to determine whether <img alt="\delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> scales as <img alt="1/\mathrm{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Cmathrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or is much smaller. Unfortunately, it turns out to be highly non-trivial to bound <img alt="\delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and even bounding <img alt="d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y)" class="latex" src="https://s0.wp.com/latex.php?latex=d_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for “simple” <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can be hard. Imagine that we try to reason about the distribution <img alt="\mathcal{D}_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>: the probability mass that <img alt="\mathcal{D}_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> assigns to string <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is proportional to the number of times that <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> appears as a subsequence in <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, but this count is already hard to express or control, unless <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> has a very simple pattern. This is roughly where this natural attempt gets stuck.</p>



<p><strong>Distinguishing using estimators.</strong> Now we turn to a different approach that underlies the recent breakthrough on trace reconstruction algorithms. As discussed earlier, we can focus on the problem of distinguishing the case <img alt="s = x" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from <img alt="s = y" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for fixed strings <img alt="x \ne y \in \{0, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cne+y+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Let <img alt="f: \{0, 1\}^{\le n} \to \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A+%5C%7B0%2C+1%5C%7D%5E%7B%5Cle+n%7D+%5Cto+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a function defined over all possible traces from a length-<img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> string. We consider the following algorithm for distinguishing <img alt="\mathcal{D}_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from <img alt="\mathcal{D}_y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>:<br/><br/><strong>Step 1.</strong> Given traces <img alt="\tilde s_1, \tilde s_2, \ldots" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s_1%2C+%5Ctilde+s_2%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, compute the average of <img alt="f(\tilde s_1), f(\tilde s_2), \ldots" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s_1%29%2C+f%28%5Ctilde+s_2%29%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/><strong>Step 2.</strong> Output <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> if the average is closer to <img alt="\mathbb{E}_{\tilde x \sim \mathcal{D}_x}[f(\tilde x)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+x+%5Csim+%5Cmathcal%7BD%7D_x%7D%5Bf%28%5Ctilde+x%29%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> than to <img alt="\mathbb{E}_{\tilde y \sim \mathcal{D}_y}[f(\tilde y)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+y+%5Csim+%5Cmathcal%7BD%7D_y%7D%5Bf%28%5Ctilde+y%29%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and output <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> otherwise.<br/></p>



<p>For the above to succeed, <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> needs to satisfy the following two conditions:<br/></p>



<p><strong>(Separation)</strong> <img alt="\mathcal{D}_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="\mathcal{D}_y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are well-separated under <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, namely <img alt="|\mathbb{E}[f(\tilde x)] - \mathbb{E}[f(\tilde y)]| \ge \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cmathbb%7BE%7D%5Bf%28%5Ctilde+x%29%5D+-+%5Cmathbb%7BE%7D%5Bf%28%5Ctilde+y%29%5D%7C+%5Cge+%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="\epsilon &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E+0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<br/><strong>(Boundedness)</strong> Expectation of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can be efficiently estimated. This can be guaranteed if for some <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="|f(\tilde s)| \le B" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cf%28%5Ctilde+s%29%7C+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> holds for every <img alt="\tilde s \in \{0, 1\}^{\le n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s+%5Cin+%5C%7B0%2C+1%5C%7D%5E%7B%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<br/></p>



<p>Assuming the above, a standard concentration argument shows that we can distinguish <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> using <img alt="O((B/\epsilon)^2)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%28B%2F%5Cepsilon%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> samples, and thus <img alt="M(n) \lesssim n \cdot (B/\epsilon)^2" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29+%5Clesssim+n+%5Ccdot+%28B%2F%5Cepsilon%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>In principle, a near-optimal choice of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> would be setting <img alt="f(\tilde s)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be the indicator of <img alt="\mathcal{D}_x(\tilde s) \ge \mathcal{D}_y(\tilde s)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x%28%5Ctilde+s%29+%5Cge+%5Cmathcal%7BD%7D_y%28%5Ctilde+s%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which gives boundedness <img alt="B = 1" class="latex" src="https://s0.wp.com/latex.php?latex=B+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and separation <img alt="\epsilon = d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y) \ge \delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+d_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29+%5Cge+%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. However, as argued above, this optimal choice of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can still be hard to analyze. Instead, we focus on choosing a simpler function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which potentially gives a suboptimal <img alt="B/\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=B%2F%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> but admits simple analyses.</p>



<p><strong>Sketch of the <img alt="\exp(O(n^{1/3}))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Upper Bound.</strong> The main results of [DOS17] and [NP17] follow from choosing <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be a linear function. Given a trace <img alt="\tilde s = \tilde s_0 \tilde s_1 \tilde s_2 \cdots" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s+%3D+%5Ctilde+s_0+%5Ctilde+s_1+%5Ctilde+s_2+%5Ccdots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we consider the following polynomial with coefficients being the bits of <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>:<br/></p>



<p class="has-text-align-center"><img alt="\tilde S(z) = \sum_{k}\tilde s_k z^k = \tilde s_0 + \tilde s_1 z + \tilde s_2 z^2 + \cdots." class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+S%28z%29+%3D+%5Csum_%7Bk%7D%5Ctilde+s_k+z%5Ek+%3D+%5Ctilde+s_0+%2B+%5Ctilde+s_1+z+%2B+%5Ctilde+s_2+z%5E2+%2B+%5Ccdots.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/></p>



<p>For some number <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be determined later, we consider the estimator <img alt="f(\tilde s) = \tilde S(z) = \tilde s_0 + \tilde s_1 z + \tilde s_2 z^2 + \cdots" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s%29+%3D+%5Ctilde+S%28z%29+%3D+%5Ctilde+s_0+%2B+%5Ctilde+s_1+z+%2B+%5Ctilde+s_2+z%5E2+%2B+%5Ccdots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is indeed a linear function in the trace <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>At first glance, it might be unclear why we choose the coefficients to be powers of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The following fact justifies this choice by showing that polynomials interact with the deletion channel very nicely: The expectation of <img alt="\tilde S(z)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+S%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is exactly the evaluation of the polynomial <img alt="S(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=S%28%5Ccdot%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with coefficients <img alt="s_0, s_1, \ldots, s_{n-1}" class="latex" src="https://s0.wp.com/latex.php?latex=s_0%2C+s_1%2C+%5Cldots%2C+s_%7Bn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, but at a slightly different point.<br/></p>



<p><strong>Fact:</strong> For any string <img alt="s \in \{0, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,<br/></p>



<p class="has-text-align-center"><img alt="\mathbb{E}_{\tilde s \sim \mathcal{D}_s}\left[\tilde S(z)\right] = \frac{1}{2}S\left(\frac{z+1}{2}\right)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+s+%5Csim+%5Cmathcal%7BD%7D_s%7D%5Cleft%5B%5Ctilde+S%28z%29%5Cright%5D+%3D+%5Cfrac%7B1%7D%7B2%7DS%5Cleft%28%5Cfrac%7Bz%2B1%7D%7B2%7D%5Cright%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/></p>



<p>Equivalently, we have <img alt="\mathbb{E}[\tilde S(2z-1)] = \frac{1}{2}S(z)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Ctilde+S%282z-1%29%5D+%3D+%5Cfrac%7B1%7D%7B2%7DS%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which allows us to rephrase our requirements on the choice of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as follows:<br/></p>



<p><strong>(Separation)</strong> <img alt="|X(z) - Y(z)| \ge \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%7CX%28z%29+-+Y%28z%29%7C+%5Cge+%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that is not too small.<br/><strong>(Boundedness)</strong> <img alt="|\tilde S(2z - 1)| \le B" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Ctilde+S%282z+-+1%29%7C+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for all possible trace <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, for some <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that is not too large.<br/></p>



<p>After some thought, it is beneficial to choose <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that both <img alt="|z|" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cz%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="|2z - 1|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C2z+-+1%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are close to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, since this ensures that different bits in the string are assigned weights of similar magnitudes in the polynomials above. These two conditions hold if and only if <img alt="z \approx 1" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Capprox+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2442" height="230" src="https://theorydish.files.wordpress.com/2021/06/plan.png?w=1024" width="512"/>The overall plan for distinguishing strings <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</figure></div>



<p>The crucial idea in both papers [DOS17, NP17] is to consider the polynomials in the complex plane <img alt="\mathbb{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> instead of on the real line. Fortunately, all the previous discussion still holds for complex numbers, with <img alt="|\cdot|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Ccdot%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> interpreted as modulus instead of absolute value. In [NP17], the authors chose <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from a small arc of the unit circle:</p>



<p class="has-text-align-center"><img alt="A_L = \{e^{i\theta}: \theta\in[-\pi/L, \pi/L]\}" class="latex" src="https://s0.wp.com/latex.php?latex=A_L+%3D+%5C%7Be%5E%7Bi%5Ctheta%7D%3A+%5Ctheta%5Cin%5B-%5Cpi%2FL%2C+%5Cpi%2FL%5D%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2445" height="262" src="https://theorydish.files.wordpress.com/2021/06/a_l.png?w=622" width="311"/>Arc <img alt="A_L" class="latex" src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> marked by the red box and dotted lines.</figure></div>



<p>For every <img alt="z \in A_L" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Cin+A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, it is easy to upper bound <img alt="|\tilde S(2z - 1)|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Ctilde+S%282z+-+1%29%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>: it follows from simple calculus that <img alt="|2z - 1| \le 1 + O(L^{-2})" class="latex" src="https://s0.wp.com/latex.php?latex=%7C2z+-+1%7C+%5Cle+1+%2B+O%28L%5E%7B-2%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and thus for every <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>:<br/></p>



<p class="has-text-align-center"><img alt="\left|\tilde S(2z-1)\right| \le \sum_{k=0}^{n-1}\left|(2z-1)^k\right| \le n \cdot \left[1 + O(L^{-2})\right]^n = e^{O(n/L^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Ctilde+S%282z-1%29%5Cright%7C+%5Cle+%5Csum_%7Bk%3D0%7D%5E%7Bn-1%7D%5Cleft%7C%282z-1%29%5Ek%5Cright%7C+%5Cle+n+%5Ccdot+%5Cleft%5B1+%2B+O%28L%5E%7B-2%7D%29%5Cright%5D%5En+%3D+e%5E%7BO%28n%2FL%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>To lower bound <img alt="X(z) - Y(z)" class="latex" src="https://s0.wp.com/latex.php?latex=X%28z%29+-+Y%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, note that since both <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are binary, all the coefficients of <img alt="X-Y" class="latex" src="https://s0.wp.com/latex.php?latex=X-Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are in <img alt="\{-1, 0, 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+0%2C+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Such polynomials are known as <em>Littlewood polynomials</em>. The separation condition is then reduced to the following claim in complex analysis:</p>



<p class="has-text-align-center"><em><strong>Littlewood polynomials cannot to be too “flat” over a short arc around 1.</strong></em></p>



<p>This is indeed the case:</p>



<p id="lemma-1"><strong>Lemma 1.</strong> (<a href="https://www.jstor.org/stable/pdf/24899666.pdf" rel="noreferrer noopener" target="_blank">[Borwein and Erdélyi, 1997]</a>) For every nonzero Littlewood polynomial <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,<br/></p>



<p class="has-text-align-center"><img alt="\max_{z \in A_L}|p(z)| \ge e^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C+%5Cge+e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<br/></p>



<p>By <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a>, there exists <img alt="z \in A_L" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Cin+A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that the separation condition holds for <img alt="\epsilon = e^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Recall that the boundedness holds for <img alt="B = e^{O(n/L^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=B+%3D+e%5E%7BO%28n%2FL%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This shows that the sample complexity is upper bounded by <img alt="(B/\epsilon)^2 = \exp(O(n/L^2 + L))" class="latex" src="https://s0.wp.com/latex.php?latex=%28B%2F%5Cepsilon%29%5E2+%3D+%5Cexp%28O%28n%2FL%5E2+%2B+L%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is minimized at <img alt="L = n^{1/3}" class="latex" src="https://s0.wp.com/latex.php?latex=L+%3D+n%5E%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This proves the upper bound <img alt="M(n) = \exp(O(n^{1/3}))" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29+%3D+%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p><strong>Proof of a weaker lemma.</strong> While the original proof of <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a> is a bit technical, [NP17] presented a beautiful and much simpler proof of the following weaker result, in which the <img alt="e^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> lower bound is replaced by <img alt="n^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the degree of the Littlewood polynomial.</p>



<p><strong>Lemma 2.</strong> ([Lemma 3.1, NP17]) For every nonzero Littlewood polynomial <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of degree <img alt="&lt; n" class="latex" src="https://s0.wp.com/latex.php?latex=%3C+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,<br/></p>



<p class="has-text-align-center"><img alt="\max_{z \in A_L}|p(z)| \ge n^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C+%5Cge+n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<br/></p>



<p><strong>Proof.</strong> Without loss of generality, we assume that the constant term of <img alt="p(z)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Suppose otherwise, that the lowest order term of <img alt="p(z)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="z^m" class="latex" src="https://s0.wp.com/latex.php?latex=z%5Em&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="m \ge 1" class="latex" src="https://s0.wp.com/latex.php?latex=m+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We may consider the polynomial <img alt="p(z) / z^m" class="latex" src="https://s0.wp.com/latex.php?latex=p%28z%29+%2F+z%5Em&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> instead.</p>



<p>Let <img alt="M = \max_{z \in A_L}|p(z)|" class="latex" src="https://s0.wp.com/latex.php?latex=M+%3D+%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be the maximum modulus of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> over arc <img alt="A_L" class="latex" src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="\omega = e^{2\pi i/L}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+e%5E%7B2%5Cpi+i%2FL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be an <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-th root of unity. Consider the following polynomial:<br/></p>



<p class="has-text-align-center"><img alt="q(z) = \prod_{j=0}^{L-1}p(\omega^j z) = p(z) \cdot p(\omega z) \cdot \cdots \cdot p(\omega^{L-1} z)" class="latex" src="https://s0.wp.com/latex.php?latex=q%28z%29+%3D+%5Cprod_%7Bj%3D0%7D%5E%7BL-1%7Dp%28%5Comega%5Ej+z%29+%3D+p%28z%29+%5Ccdot+p%28%5Comega+z%29+%5Ccdot+%5Ccdots+%5Ccdot+p%28%5Comega%5E%7BL-1%7D+z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>For every <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on the unit circle, at least one point <img alt="\omega^j z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%5Ej+z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> falls into the arc <img alt="A_L" class="latex" src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, so that <img alt="|p(\omega^j z)| \le M" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cp%28%5Comega%5Ej+z%29%7C+%5Cle+M&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. (See <a href="https://theorydish.blog/feed/#windmill">figure below</a> for a proof by picture.) The moduli of the remaining <img alt="L - 1" class="latex" src="https://s0.wp.com/latex.php?latex=L+-+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> factors are trivially bounded by <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Thus, <img alt="|q(z)| \le M\cdot n^{L-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cq%28z%29%7C+%5Cle+M%5Ccdot+n%5E%7BL-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>On the other hand, we note <img alt="q(0) = [p(0)]^L = 1" class="latex" src="https://s0.wp.com/latex.php?latex=q%280%29+%3D+%5Bp%280%29%5D%5EL+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The <a href="https://en.wikipedia.org/wiki/Maximum_modulus_principle" rel="noreferrer noopener" target="_blank">maximum modulus principle</a> implies that for some <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on the unit circle, we have <img alt="|q(z)| \ge 1" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cq%28z%29%7C+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Therefore, we must have <img alt="M \cdot n^{L - 1} \ge 1" class="latex" src="https://s0.wp.com/latex.php?latex=M+%5Ccdot+n%5E%7BL+-+1%7D+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which implies <img alt="M \ge n^{-(L - 1)} = n^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=M+%5Cge+n%5E%7B-%28L+-+1%29%7D+%3D+n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and completes the proof. <img alt="\square" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<div class="wp-block-image" id="windmill"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2449" height="261" src="https://theorydish.files.wordpress.com/2021/06/example.png?w=924" width="462"/>Points involved in the definition of <img alt="q(z)" class="latex" src="https://s0.wp.com/latex.php?latex=q%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="L=4" class="latex" src="https://s0.wp.com/latex.php?latex=L%3D4&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<br/>In this case, <img alt="\omega^3 z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%5E3+z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is inside arc <img alt="A_L" class="latex" src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is marked by the red lines.</figure></div>



<p><strong>Beyond linear estimators?</strong> The work of [DOS17, NP17] not only proved the <img alt="\exp(O(n^{1/3}))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> upper bound, but also showed that this is the best sample complexity we can get from linear estimator <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The recent work of [Cha20] goes beyond these linear estimators by taking the higher moments of the trace into account. The idea turns out to be a natural one: consider the “<img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-grams” (i.e., length-<img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> substrings) of the string for some <img alt="k &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. In more detail, we fix some string <img alt="w \in \{0, 1\}^k" class="latex" src="https://s0.wp.com/latex.php?latex=w+%5Cin+%5C%7B0%2C+1%5C%7D%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and consider the binary polynomial with coefficients corresponding to the positions at which <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> appears as a (contiguous) substring. The key observation is that there exists a choice of <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that the resulting polynomial is sparse (in the sense that the degrees of the nonzero monomials are far away). The technical part of [Cha20] is then devoted to proving an analogue of <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a> that is specialized to these “sparse” Littlewood polynomials.</p>



<p><strong>Acknowledgments.</strong> I would like to thank Moses Charikar, Li-Yang Tan and Gregory Valiant for being on my quals committee and for helpful discussions about this problem.</p></div>
    </content>
    <updated>2021-06-29T16:20:49Z</updated>
    <published>2021-06-29T16:20:49Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Mingda Qiao</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-07-04T18:38:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/06/29/greedy-orderings-transposition</id>
    <link href="https://11011110.github.io/blog/2021/06/29/greedy-orderings-transposition.html" rel="alternate" type="text/html"/>
    <title>Greedy orderings with transposition</title>
    <summary>I’m a big fan of using antimatroids to model vertex-ordering processes in graphs such as the construction of topological orderings in directed acyclic graphs and perfect elimination orderings in chordal graphs. In each case a vertex can be removed from the graph and added to the order when it obeys a local condition: its remaining neighbors are all outgoing for topological orderings, or all adjacent for perfect elimination orderings. Once this condition becomes true of a vertex it remains true until the vertex is added to the order, the defining property of an antimatroid. Because of this property, a greedy algorithm for finding these orderings can never make a mistake: if there exists an ordering of all of the vertices, it is always a safe choice to add any vertex that can be added.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I’m a big fan of using <a href="https://en.wikipedia.org/wiki/Antimatroid">antimatroids</a> to model vertex-ordering processes in graphs such as the construction of <a href="https://en.wikipedia.org/wiki/Topological_sorting">topological orderings</a> in <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graphs</a> and perfect elimination orderings in <a href="https://en.wikipedia.org/wiki/Chordal_graph">chordal graphs</a>. In each case a vertex can be removed from the graph and added to the order when it obeys a local condition: its remaining neighbors are all outgoing for topological orderings, or all adjacent for perfect elimination orderings. Once this condition becomes true of a vertex it remains true until the vertex is added to the order, the defining property of an antimatroid. Because of this property, a greedy algorithm for finding these orderings can never make a mistake: if there exists an ordering of all of the vertices, it is always a safe choice to add any vertex that can be added.</p>

<p>But there are some greedy vertex-ordering processes that do not form antimatroids, even though they do have the same inability to make mistakes. Two of these are the dismantling orders of <a href="https://en.wikipedia.org/wiki/Cop-win_graph">cop-win graphs</a> and the reverse construction orders of <a href="https://en.wikipedia.org/wiki/Distance-hereditary_graph">distance-hereditary graphs</a>. I wrote about cop-win graphs <a href="https://11011110.github.io/blog/2016/08/18/game-of-cop.html">here in 2016</a>; a graph is cop-win if a cop can always land on the same vertex as a robber when they take turns either moving from a vertex to a neighboring vertex or staying put. In distance-hereditary graphs, all induced subgraphs have the same distances; <a href="https://11011110.github.io/blog/2005/10/11/delta-confluent-drawing-paper.html">these graphs also have nice confluent drawings</a>. Both of these classes of graphs can be recognized by greedy algorithms that remove one vertex at a time until either getting stuck (for graphs not in the class) or succeeding by reaching a single-vertex graph. But although the conditions for removing vertices in these algorithms are local, they are not antimatroidal.</p>

<h1 id="an-example-graph">An example graph</h1>

<p style="text-align: center;"><img alt="A six-vertex graph with six vertices A, B, C, D, E, and F, and seven edges AD, BC, BD, BE, CE, and EF" src="https://11011110.github.io/blog/assets/2021/Ptolemaic.svg"/></p>

<p>The graph shown above happens to be chordal, distance-hereditary, and cop-win, making it a convenient example both of how to order the vertices of these graph classes and of why the distance-hereditary and cop-win orderings are not antimatroidal.</p>

<ul>
  <li>
    <p>In chordal graphs, a perfect elimination ordering can be constructed by repeatedly removing <em>simplicial vertices</em>, vertices whose neighborhoods form a clique. For an elimination ordering of the example graph, vertices \(A\), \(C\), and \(F\) are already available to be listed: \(A\) and \(F\) only have one neighbor (automatically a clique), and \(C\) has two neighbors forming a two-vertex clique. The other vertices will become available later in the removal process, once enough of their neighbors have been removed and all remaining vertices become adjacent. For instance, once \(A\) has been removed, \(D\) will become available, and once \(C\) has been removed, \(B\) will become available. Once this removal process makes a vertex simplicial, it remains simplicial until removed, so elimination orderings form an antimatroid.</p>
  </li>
  <li>
    <p>Distance-hereditary graphs can be constructed from a single vertex by repeatedly adding leaf vertices (with one neighbor connecting to previous vertices) or twins (duplicates of previous vertices). Reversing this process, these graphs can be deconstructed by repeatedly removing leaves or twins. The graph above has no twins, but \(A\) and \(F\) are leaves, and can be removed immediately. If \(A\) is removed, \(C\) and \(D\) become false twins (not adjacent to each other), and either of them can be removed. Similarly, if \(F\) is removed, \(B\) and \(E\) become true twins (adjacent to each other), after which one can be removed, but not both: after removing \(F\) and \(B\), \(E\) is no longer a leaf or a twin (because its twin, \(B\), has gone), and must remain until later steps. Because the removal orders can start \(FB\) or \(FE\) but not \(FBE\), they are not described by an antimatroid.</p>
  </li>
  <li>
    <p>Similarly, cop-win graphs can be dismantled by repeatedly removing a vertex \(v\) that is dominated by another vertex \(w\), meaning that the neighborhood of \(v\) (including \(v\) itself) is a subset of the neighborhoood of \(w\). In the given graph, \(A\) is dominated by \(D\), \(B\) is dominated by \(E\), \(C\) is dominated by both \(B\) and \(E\), and \(F\) is dominated by \(E\). So any one of these four dominated vertices starts out as removable. But if we remove first \(F\) and then \(E\) (dominated by \(B\) after the removal of \(F\)) we can no longer remove \(B\). So because the ability to be removed can go away before the removal happens, we do not have an antimatroid.</p>
  </li>
</ul>

<p>There’s another complication here as well. For both distance-hereditary graphs and cop-win graphs, removing leaves and twins or dominated vertices will never eliminate all graph vertices. Instead, both removal processes stop when we reach a single remaining vertex. But this is different from antimatroids, where all elements must be included in all orderings.</p>

<h1 id="some-axiomatics">Some axiomatics</h1>

<p>To understand why greedy orderings still work in these cases, I think it’s helpful to start by understanding why they work for antimatroids, as a general class of structures. The following is not quite the usual system of axioms for antimatroids, but they can be defined as non-empty formal languages (that is, sets of strings over a finite alphabet) with the following properties:</p>

<dl>
  <dt>Hereditary:</dt>
  <dd>
    <p>Every prefix of a string in the language is also in the language. Thinking about this in the other direction: every string in the language can be built up by adding one character at a time, starting from the empty string, at all times remaining within the language.</p>
  </dd>
  <dt>Normal:</dt>
  <dd>
    <p>Every character occurs at most once in any string in the language. An element can only be added to the sequence of elements once. Because we are assuming the alphabet to be finite, this means that the language itself is also finite.</p>
  </dd>
  <dt>Oblivious:</dt>
  <dd>
    <p>If \(S\) and \(T\) are permutations of each other in the language, then for every character \(x\), \(Sx\) is in the language if and only if \(Tx\) is in the language. This means that what can be added next depends only on the set of characters that have been added already, forgetting about the order in which they were added.</p>
  </dd>
  <dt>Anti-exchange:</dt>
  <dd>
    <p>If \(S\) is a string, \(x\) and \(y\) are different characters, and \(Sx\) and \(Sy\) both belong to the language, then so does \(Sxy\). Adding \(x\) doesn’t prevent \(y\) from being added later. This is the key property of an antimatroid and the one that is violated by the distance-hereditary and cop-win orderings.</p>
  </dd>
</dl>

<p>Usually a stronger version of obliviousness is used, stating that when \(S\) and a permutation of \(Sx\) are in the language, then \(Sx\) is in the language, but it’s not immediately obvious why this should be true for the vertex-ordering processes I’m considering here, so I’ve gone with a weaker version. We’ll see later that the stronger version is implied by a combination of this and other properties. It is standard to also require that all characters be usable, but I haven’t done this, because I want to understand the behavior of antimatroidal greedy algorithms on graphs not in the given graph class, for which they get stuck before ordering the whole graph. But this is not important, because one could instead redefine the alphabet to consist only of usable characters.</p>

<p>Given a language that satisfies all of these properties, one can show that all non-extendable strings are equally long and use the same alphabet as each other. For, if we have two different non-extendable strings \(S\) and \(T\), we can morph \(S\) into \(T\) one step at a time, never shortening it or changing its character set, by finding the first position at which \(S\) and \(T\) differ, finding the character \(t\) that \(T\) has at that position (necessarily also used later in \(S\) because it was usable at that position and would have remained usable until it was used), and repeatedly using the anti-exchange axiom to swap \(t\) for the previous character in \(S\) until it has been swapped into a match with \(T\). The oblivious property ensures that the part of the string after the swap remains valid. So \(S\) cannot be shorter than or miss any characters from \(T\), nor vice versa.</p>

<p>Instead of the anti-exchange axiom, the distance-hereditary and cop-win orderings satisfy a weaker property, based on the notion of swapping two characters.</p>

<dl>
  <dt>Transposition:</dt>
  <dd>
    <p>Suppose \(S\) is a string, \(x\) and \(y\) are different characters, and \(Sx\) and \(Sy\) both belong to the language, but \(Sxy\) does not. Then for all \(T\) not containing \(x\) or \(y\), \(SxT\) is in the language if and only if \(SyT\) is also in the language, and \(SxTy\) is in the language if and only if \(SyTx\) is also in the language.</p>
  </dd>
</dl>

<p>The last part of the transposition property, about \(SxTy\) and \(SyTx\), is only included because we used a weak version of obliviousness; if we used the stronger version, it would follow from the earlier part of the transposition property.</p>

<h1 id="cop-win-and-distance-hereditary-orderings-have-the-transposition-property">Cop-win and distance-hereditary orderings have the transposition property</h1>

<p>Let’s suppose we’re trying to dismantle a cop-win graph by repeatedly removing dominated vertices, in the hope of getting down to a single vertex. After we’ve removed some vertices already in a sequence \(S\), two vertices \(x\) and \(y\) might become included in the set of dominated vertices. This can happen in several different ways:</p>

<ul>
  <li>It might be the case that \(x\) is dominated by a vertex that is not \(y\), and that \(y\) is dominated by a vertex that is not \(x\). When this happens, they can be removed in either order: removing one won’t change the fact that the other is dominated by whatever other vertex dominated it already.</li>
  <li>It might be the case that \(x\) is dominated by \(y\), and \(y\) is dominated by a third vertex \(z\). But then \(x\) is also dominated by \(z\), and again they can be removed in either order. When one is removed, the other is still dominated by \(z\).</li>
  <li>The only remaining case is that \(x\) and \(y\) are each dominated only by the other of these two vertices. In this case, we can remove one or the other but not both. But if \(x\) and \(y\) dominate each other, they have the same neighbors (they are twins), and there is a symmetry of the remaining subgraph swapping \(x\) and \(y\). So in this case, any continuation of the removal sequence \(S\) can have \(x\) replaced by \(y\) and vice versa, and still be a valid continuation. This is exactly what the transposition property states.</li>
</ul>

<p>The argument for distance-hereditary orderings is even easier. If \(x\) and \(y\) are not twins of each other, then removing one won’t affect the removability of the other. If they are twins, then they are symmetric and any continuation of the removal sequence can exchange \(x\) for \(y\) without changing its validity.</p>

<h1 id="orderings-with-transposition-form-greedoids">Orderings with transposition form greedoids</h1>

<p>If we can’t obtain an antimatroid from the cop-win or distance-hereditary graphs, we might at least hope for a more general structure, a greedoid. The key property of greedoids (viewed as hereditary normal languages rather than their usual definition as set systems) is the following axiom:</p>

<dl>
  <dt>Exchange:</dt>
  <dd>If \(S\) is a longer string in the language of a greedoid, and \(T\) is a longer string in the same language, then there is a character \(x\) in \(T\) such that \(Sx\) is a string in the language.</dd>
</dl>

<p>This implies that all maximal strings in the language have the same length, and in the cop-win and distance-hereditary cases it implies that all greedy dismantling or deconstruction sequences reach a single vertex without getting stuck along the way. The greedoid exchange property also immediately implies the strong version of the obliviousness property, by plugging in a permutation of \(Sx\) as the string \(T\) in the exchange property.</p>

<p>To prove that indistinguishability implies the exchange property, let \(S\) be any string in an indistinguishable (hereditary normal oblivious) language, and let \(T\) be a longer string, which we might as well assume to be maximal. If \(S\) is a prefix of \(T\), then obviously we can satisfy the exchange property: just take the prefix of \(T\) that has one more character.</p>

<p>Otherwise, I claim that we can replace \(T\) by a different string \(T'\) of the same length that agrees with \(S\) for more positions. To find \(T'\), let \(y\) be the first character of \(S\) that differs from the corresponding character of \(T\); this must exist by the assumption that \(S\) is not a prefix of \(T\). Obviously, at the position of \(y\) in \(S\), we could have added it to \(T\), but instead some other character was chosen. Maybe, \(y\) remained available to be chosen throughout the remaining positions of \(T\), until it actually was chosen. If so, just as in the antimatroid case, we could repeatedly swap \(y\) with its predecessor in \(T\) until reaching a string \(T'\) where \(y\) is in the correct position. Alternatively, maybe at some point during the construction of sequence \(T\), we chose a character \(z\) causing \(y\) to become unavailable. In this case, by the transposition property, we can swap \(y\) for \(z\) in \(T\) and then as before repeatedly swap \(y\) with its predecessor in \(T\) until reaching a string \(T'\) where \(y\) is in the correct position.</p>

<p>By repeatedly replacing \(T\) by equally long strings that agree with more and more positions of \(S\), we eventually reach a string for which \(S\) is a prefix, and can append one more character. This construction of \(T'\) from \(T\) does not include any new characters that weren’t already in \(S\) or \(T\), so the appended character must have come from \(T\), proving the exchange axiom.</p>

<p>The use of the transposition property to form greedoids is standard; these greedoids are called transposition greedoids, and are described e.g. by Björner and Ziegler in their introduction to greedoids in the book <em>Matroid Applications</em>. Another <a href="https://doi.org/10.1007/978-3-642-58191-5_10">book chapter on transposition greedoids</a>, in the book <em>Greedoids</em> by Korte, Schrader, and Lovász, includes another graph-theoretic example where the elements are edges of series-parallel graphs. The part that appears to be less standard is the use of this property to explain the ability of greedy algorithms to recognize cop-win and distance-hereditary graphs. I looked, but was unable to find publications observing that these two classes of graphs lead to greedoids or transposition greedoids, despite some suspiciously-similar terminology (“twins”, “dismantling”) on both sides. If anyone knows of such publications, I’d appreciate hearing of them, so that I could add this connection to their Wikipedia articles.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106495619434427598">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-06-29T12:02:00Z</updated>
    <published>2021-06-29T12:02:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-06-30T17:39:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/091</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/091" rel="alternate" type="text/html"/>
    <title>TR21-091 |  Expander Random Walks: The General Case and Limitations | 

	Gil Cohen, 

	Dor Minzer, 

	Shir Peleg, 

	Aaron Potechin, 

	Amnon Ta-Shma</title>
    <summary>Cohen, Peri and Ta-Shma (STOC'21) considered the following question: Assume the vertices of an expander graph are labelled by $\pm 1$. What "test" functions $f : \{\pm 1\}^t \to \{\pm1 \}$ can or cannot distinguish $t$ independent samples from those obtained by a random walk? [CPTS'21] considered only balanced labelling, and proved that all symmetric functions are fooled by random walks on expanders with constant spectral gap. Furthermore, it was shown that functions computable by $\mathbf{AC}^0$ circuits are fooled by expanders with vanishing spectral expansion. 

We continue the study of this question and, in particular, resolve all open problems raised by [CPTS'21]. First, we generalize the result to all labelling, not merely balanced. In doing so, we improve the known bound for symmetric functions and prove that the bound we obtain is optimal (up to a multiplicative constant). Furthermore, we prove that a random walk on expanders with constant spectral gap does not fool $\mathbf{AC}^0$. In fact, we prove that the bound obtained by [CPTS'21] for $\mathbf{AC}^0$ circuits is optimal up to a polynomial factor.</summary>
    <updated>2021-06-29T07:19:40Z</updated>
    <published>2021-06-29T07:19:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-04T18:37:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5479161699112157490</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5479161699112157490/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/someone-thinks-i-am-fine-artist-why.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5479161699112157490" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5479161699112157490" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/someone-thinks-i-am-fine-artist-why.html" rel="alternate" type="text/html"/>
    <title>Someone thinks I am a fine artist! Why?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A while back I got an  email asking me to submit to a Fine Arts Journal. Why me? Here are some possibilities:</p><p>1) They were impressed with my play: </p><p><b>Sure he created the universe, but would he get Tenure?</b> (see <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/god.html">here</a>) which did get into a play-writing contest and was performed (one of the actresses  scolded me since I took a slot from <i>a real</i> <i>playwrigh</i>t).</p><p>2) They were impressed  with my <b>Daria Fan Fiction</b> (see the four entries <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/mywritings.html">here</a> labelled as Daria Fan Fiction).</p><p>3) They were impressed with my play <b>JFK: The Final chapter</b> (see <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/jfk.html">here</a>). Unlikely since this was rejected by a play writing contest and is not well known (as opposed to my other works in the fine arts which are well known?)</p><p>4) They were impressed with my collection of satires of  Nobel Laureate Bob Dylan (<a href="https://www.cs.umd.edu/users/gasarch/dylan/dylan.html">here</a>) .</p><p>5) They were impressed with some subset of (a) complexityblog, (b) <a href="https://www.amazon.com/Problems-Point-Exploring-Computer-Science/dp/9813279729/ref=sr_1_3?dchild=1&amp;keywords=gasarch&amp;qid=1609867944&amp;sr=8-3" target="_blank">Problems with a Point</a>,  (c)  <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215979/ref=sr_1_2?dchild=1&amp;keywords=gasarch&amp;qid=1609868018&amp;sr=8-2">Mathematical Muffin Morsels</a>, and (d) <a href="https://www.amazon.com/Bounded-Queries-Recursion-Progress-Computer-ebook/dp/B000W98WU4/ref=sr_1_4?dchild=1&amp;keywords=gasarch&amp;qid=1609868084&amp;sr=8-4">Bounded Queries in Recursion Theory</a>. Or maybe just having 3 books on amazon is their threshold.  If it's complexityblog then Lance and I should co-author something for them.</p><p>6) It is a vanity-journal where you pay to  publish. So why email me who (a)  is not an artist, (b) is  not a fine artist, and most important (3) <b>does not think of himself as a fine artist</b>. The PRO of emailing me or people like me is they cast a wide net. The CON is--- there is no CON! It costs nothing to email me, and emailing me does not affect their credibility. That still raises the question of how they got my name.</p><p>7) Could it be a phishing? If I click on something in the email would they  get my credit card number? Their email begins <i>Dear Professor</i> not  <i>Dear Professor Gasarch. </i>  So they know I am a professor. Then again, I have known of ugrads who get emails that begin <i>Dear Professor</i>. (The emails to HS student Naveen and ugrad Nichole in the story I tell <a href="https://blog.computationalcomplexity.org/search?q=Naveen">here</a> were addressed to <i>Dear Professor.) </i></p><p>8) They mistook me for my parents who, in 1973,  put together an anthology of short stories titled <i>Fiction:The Universal elements</i>,  for a Freshman Comp course my mom taught, see <a href="https://www.amazon.com/Fiction-Universal-Element-P-Gasarch/dp/0442226322">here</a>. I note that their book ranks around 18,000,000, so even that explanation is unlikely. Actually the rank changes a lot- it was 12,000,000 this morning. Still, not what one would call a best seller. It's fun to see what is doing better: <i>Bounded Queries in Recursion Theory (currently at around rank 6.000.000) </i> or <i>Fiction: The Universal Elements.</i></p><p> If I ever get one of these emails from a History Journal I will submit my Satirical <i>Ramsey Theory and the History of Pre-Christian England: An Example of Interdisciplinary Research</i> (see <a href="https://www.cs.umd.edu/~gasarch/COURSES/389/W14/ramseykings.pdf">here</a>) just to see what happens- but  I will stop short of paying-to-publish. Or maybe I will pay-to-publish so that the next time I try to fool a class with it I can point to a seemingly real journal which has the article. </p><p><br/></p><div class="gE iv gt" style="background-color: white; color: #222222; cursor: auto; font-family: Roboto, RobotoDraft, Helvetica, Arial, sans-serif; font-size: 0.875rem; padding: 20px 0px 0px;"><table cellpadding="0" class="cf gJ"><tbody style="display: block;"><tr class="acZ" style="display: flex; height: auto;"><td class="gF gK" style="display: block; line-height: 20px; margin: 0px; padding: 0px; vertical-align: top; white-space: nowrap; width: 571.172px;"><br/></td></tr></tbody></table></div></div>
    </content>
    <updated>2021-06-28T03:00:00Z</updated>
    <published>2021-06-28T03:00:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-03T12:21:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/090</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/090" rel="alternate" type="text/html"/>
    <title>TR21-090 |  On Secret Sharing, Randomness, and Random-less Reductions for Secret Sharing | 

	Divesh Aggarwal, 

	Maciej Obremski, 

	Eldon Chung, 

	Joao Ribeiro</title>
    <summary>Secret-sharing is one of the most basic and oldest primitives in cryptography, introduced by Shamir and Blakely in the 70s. It allows to strike a meaningful balance between availability and confidentiality of secret information. It has a host of applications most notably in threshold cryptography and multi-party computation. All known constructions of secret sharing (with the exception of those with a pathological choice of parameters) require access to uniform randomness. In practice, it is extremely challenging to generate a source of uniform randomness. This has led to a large body of research devoted to designing randomized algorithms and cryptographic primitives from imperfect sources of randomness.

Motivated by this, 15 years ago, Bosley and Dodis asked whether it is even possible to build 2-out-of-2 secret sharing without access to uniform randomness. In this work, we make progress towards resolving this question.

We answer this question for secret sharing schemes with important additional properties, i.e., either leakage-resilience or non-malleability. We prove that, unfortunately, for not too small secrets, it is impossible to construct any of 2-out-of-2 leakage-resilient secret sharing or 2-out-of-2 non-malleable secret sharing without access to uniform randomness.

Given that the problem whether 2-out-of-2 secret sharing requires uniform randomness has been open for a long time, it is reasonable to consider intermediate problems towards resolving the open question. In a spirit similar to NP-completeness, we study how the existence of a t-out-of-n secret sharing without access to uniform randomness is related to the existence of a t'-out-of-n' secret sharing without access to uniform randomness for a different choice of the parameters t,n,t',n'.</summary>
    <updated>2021-06-27T05:51:49Z</updated>
    <published>2021-06-27T05:51:49Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-04T18:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/089</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/089" rel="alternate" type="text/html"/>
    <title>TR21-089 |  A Relativization Perspective on Meta-Complexity | 

	Rahul Santhanam, 

	Hanlin Ren</title>
    <summary>Meta-complexity studies the complexity of computational problems about complexity theory, such as the Minimum Circuit Size Problem (MCSP) and its variants. We show that a relativization barrier applies to many important open questions in meta-complexity. We give relativized worlds where:

* MCSP can be solved in deterministic polynomial time, but the search version of MCSP cannot be solved in deterministic polynomial time, even approximately. In contrast, Carmosino, Impagliazzo, Kabanets, Kolokolova [CCC'16] gave a randomized approximate search-to-decision reduction for MCSP with a relativizing proof.

* The complexities of MCSP[2^{n/2}] and MCSP[2^{n/4}] are different, in both worst-case and average-case settings. Thus the complexity of MCSP is not "robust" to the choice of the size function.

* Levin's time-bounded Kolmogorov complexity Kt(x) can be approximated to a factor (2+epsilon) in polynomial time, for any epsilon &gt; 0.

* Natural proofs do not exist, and neither do auxiliary-input one-way functions. In contrast, Santhanam [ITCS'20] gave a relativizing proof that the non-existence of natural proofs implies the existence of one-way functions under a conjecture about optimal hitting sets.

* DistNP does not reduce to GapMINKT by a family of "robust" reductions. This presents a technical barrier for solving a question of Hirahara [FOCS'20].</summary>
    <updated>2021-06-25T15:28:02Z</updated>
    <published>2021-06-25T15:28:02Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-04T18:37:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8156</id>
    <link href="https://windowsontheory.org/2021/06/25/stoc-feedback-and-tcs-wikipedia-guest-post-by-clement-canonne/" rel="alternate" type="text/html"/>
    <title>STOC feedback and TCS Wikipedia (guest post by Clément Canonne )</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The 53rd Annual ACM Symposium on Theory of Computing (STOC’21) concludes today, after 5 days of action-packed, Gather-power talks, workshops, plenary talks, and posters. A huge thank you to all volunteers, organizers, speakers, and attendees, who helped make this virtual conference a success! We would like to ask for your feedback on the conference. Whether … <a class="more-link" href="https://windowsontheory.org/2021/06/25/stoc-feedback-and-tcs-wikipedia-guest-post-by-clement-canonne/">Continue reading <span class="screen-reader-text">STOC feedback and TCS Wikipedia (guest post by Clément Canonne )</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The 53rd Annual ACM <a href="http://acm-stoc.org/stoc2021/">Symposium on Theory of  Computing </a> (STOC’21) concludes today, after 5 days of action-packed, Gather-power talks, workshops, plenary talks, and posters. A huge thank you to all volunteers, organizers, speakers, and attendees, who helped make this virtual conference a success!</p>



<p>We would like to ask for your feedback on the conference. Whether you attended this virtual edition of STOC or not, please fill <a href="https://forms.gle/r1A4zCS6umbhZTrMA">this form </a> to help us make future TheoryFests even better!</p>



<p>Moreover, as part of one of the STOC social event (and in line with initiatives in 2017 by Shuchi Chawla and the <a href="https://thmatters.wordpress.com/2019/06/11/wikipedia-edit-a-thon-at-stoc19/">Edit-a-Thon from STOC 2019</a> , a spreadsheet aiming to crowdsource which TCS Wikipedia pages need improvement/creation has been set up:<br/><a href="https://docs.google.com/spreadsheets/d/1ZswaweUvsjVHnItMBnIxaiBZxcs-gEaUoODFoH4FGms/edit" rel="noreferrer noopener" target="_blank">https://docs.google.com/spreadsheets/d/1ZswaweUvsjVHnItMBnIxaiBZxcs-gEaUoODFoH4FGms/edit</a>. </p>



<p>Feel free to use or edit it in view of improving the TCS coverage in Wikipedia.</p>



<p>Clément Canonne (on behalf of the STOC and TheoryFest organizers)</p></div>
    </content>
    <updated>2021-06-25T14:38:34Z</updated>
    <published>2021-06-25T14:38:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-07-04T18:38:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/06/25/postdoc-at-uc-san-diego-apply-by-july-1-2021/</id>
    <link href="https://cstheory-jobs.org/2021/06/25/postdoc-at-uc-san-diego-apply-by-july-1-2021/" rel="alternate" type="text/html"/>
    <title>postdoc at UC San Diego (apply by July 30, 2021)</title>
    <summary>The UCSD CSE Fellows Program is intended to support exceptional postdoctoral researchers in computer science. The program seeks to recruit 1-3 fellows a year for a two year postdoctoral appointment working alongside a UCSD CSE faculty mentor. Website: https://cse.ucsd.edu/research/uc-san-diego-cse-fellows-program Email: shachar.lovett@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The UCSD CSE Fellows Program is intended to support exceptional postdoctoral researchers in computer science. The program seeks to recruit 1-3 fellows a year for a two year postdoctoral appointment working alongside a UCSD CSE faculty mentor.</p>
<p>Website: <a href="https://cse.ucsd.edu/research/uc-san-diego-cse-fellows-program">https://cse.ucsd.edu/research/uc-san-diego-cse-fellows-program</a><br/>
Email: shachar.lovett@gmail.com</p></div>
    </content>
    <updated>2021-06-25T14:08:42Z</updated>
    <published>2021-06-25T14:08:42Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-07-04T18:37:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-4913288096208854339</id>
    <link href="http://processalgebra.blogspot.com/feeds/4913288096208854339/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=4913288096208854339" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/4913288096208854339" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/4913288096208854339" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/06/the-detecter-runtime-verification-tool.html" rel="alternate" type="text/html"/>
    <title>The detectEr runtime-verification tool for Erlang programs</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Thanks to the huge amount of excellent work done by <a href="https://duncanatt.github.io/" target="_blank">Duncan Paul Attard</a> and <a href="http://staff.um.edu.mt/afra1/" target="_blank">Adrian Francalanza</a>, we now have a tutorial on detectEr that some of you might want to check out. See <a href="https://duncanatt.github.io/detecter/index.html">this web page</a> for all the material, tool download and links to the videos of the tutorial Duncan delivered at the <a href="https://www.discotec.org/2021/tutorials" target="_blank">DisCoTec 2021 Tutorial Day</a>. </p><p>detectEr is a runtime verification tool for asynchronous component systems that run on the <a href="https://blog.erlang.org/a-brief-BEAM-primer/" target="_blank">Erlang Virtual Machine</a>. It also supports monitoring systems that can execute outside of the EVM, so long as these can produce traces that are formatted in a way that is parsable by detectEr. The tool itself is developed in <a href="https://www.erlang.org/" target="_blank">Erlang</a>, and is the product of five years of theoretical and practical development. (Erlang is a programming language used to build massively scalable soft real-time systems with requirements on high availability.)  </p> <p>Enjoy!</p></div>
    </content>
    <updated>2021-06-24T08:16:00Z</updated>
    <published>2021-06-24T08:16:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-06-24T08:16:51Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6958131418382007261</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6958131418382007261/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6958131418382007261" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6958131418382007261" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html" rel="alternate" type="text/html"/>
    <title>I went to the ``debate'' about Program Verif and the Lipton-Demillo-Perlis paper</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>On Thursday June 17 I went to (on zoom- does that need to be added anymore?)</p><p><i>A Debate on Program Correctness</i></p><p>There was no subtitle but it could have been:</p><p>Have the points made in <i>Social Processes and Proofs of Theorems and Programs</i> by DeMillo, Lipton, Perlis, survived the test of time ? (Spoiler Alert: Yes.)</p><p>I found out about it from the Lipton-Regan blog <a href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/">here</a></p><p>The debaters were Richard DeMillo and Richard Lipton and the moderator was Harry Lewis (Alan Perlis passed away in 1990).  Calling it a debate is not correct since DeMillo and Lipton (and Lewis) all agree. (DeMillo and Lipton even have the same first name!)  The DLP paper is in Harry Lewis's collection<i> Ideas that created the future.</i>  The event  should have been advertised as a discussion. However, it was a good discussion so this is not a complaint.</p><p>Here are some things that came out of the discussion.</p><p>1) The main topic was the 1979 DeMillo-Lipton-Perlis paper (see <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">here</a>) that gave arguments why Proofs of Program correctness could not work.</p><p>An all-to-brief summary of the DLP paper: Some researchers are trying to set up frameworks for doing proofs that programs are correct, analogous to the certainty  we get with a proof of a Theorem in Mathematics. But proofs in Mathematics are, in reality, NOT that rigorous. Often details are left out or left to the reader. This is fine for mathematics (more on that later) but unacceptable for programs which need rather precise and rigorous proofs.</p><p>How do theorems in mathematics really get verified? By having enough people look at them and make sure they match intuitions, what DLP call <i>A Social Process</i>.  (NOTE FROM BILL: Papers that are not important do not get looked at so there may well be errors.)</p><p>2) The notion of proving-programs-correct was very seductive; however, the people who were trying to do this had a blind spot about how the analogy of proving-programs-correct and proving-theorem-correct differ.  In particular, a program is rather complicated and even stating carefully what you want to prove is difficult. By contrast, for most math statements, what you want to prove is clear. Note also that a program has lots of code (far more now than when DLP was written) and so much can happen that you cannot account for.</p><p>3) The DLP paper had a large effect on the field of program verification.  Funding for it was reduced and students were discouraged from going into it.</p><p>4) When DLP appeared DeMillo and Lipton were pre-tenure. Hence it took lots of courage to publish it. Alan Perlis had tenure and had already won a Turing award.  This did give DeMillo and Lipton some cover; however, it still took courage.</p><div><div>5) How did the Program Verification  Community deal with the objections in DLP?  DeMillo said that he looked at a large set of papers in the field, and very few even mentioned DLP. He recommends reading the book <i>Mechanizing Proof: Computing, Risk, and Trust by David McKenzie </i>see <a href="https://www.amazon.com/exec/obidos/ASIN/0262632950">here</a>.</div><div><br/></div><div>6) So how can we be more certain that programs are correct?</div><div><br/></div><div>a) Testing.</div><div>b) Modularize and test. Fix errors. Modularize  and test. Fix errors...</div><div>c) Try to isolate side effects.</div><div>d) More testing.</div><div><br/></div><div>Some point to Model Checking, which could be considered very sophisticated testing, but that's used to verify circuits and perhaps low-level code, not programs. Model checking is a success story and note that Ed Clark, E. Allen Emerson, and Joseph Sifakis shared a (well deserved) Turing award for this work. But see next note.</div><div><br/></div><div>6.5) An audience member pointed out that Program Verification people have won several Turing Awards</div><div><br/></div><div>Dijkstra 1972</div><div><br/></div><div>Floyd 1978 </div><div><br/></div><div>Hoare 1980</div><div><br/></div><div>Pnueli 1996</div><div><br/></div><div>(Are there more?) </div><div><br/></div><div>so the field is alive and healthy. DeMillo responded that prizes for academic research are a poor measure of  success. </div><div><br/></div><div>7) Can computers themselves help with proofs of correctness? That is the only hope; however, there are scaling problems.</div><div><br/></div><div>8) When DLP was written a program with 100,000 lines of code was considered large. Now we have programs with millions of lines of code. And now we have more concurrency. So the lessons of the DLP paper are probably more relevant now then they were then.</div><div><br/></div><div>9) Since Program Verification does not seem to be used, how come we don't have a Software crisis?</div><div><br/></div><div>a) We do! The Q+A mechanism at the meeting was terrible. </div><div>b) We do! FILL IN YOUR OWN FAVORITE STORY OF BAD SOFTWARE.</div><div>c) See the answer to question 6.</div><div><br/></div><div>10) SPECS are a problem. Tony Hoare once gave a talk where he proves that a program sorted correctly and then pointed out that if the program just output 0,...0 that would have also satisfied the SPEC since all that was required was that the output be sorted, not the (overlooked!) requirement that it be the same numbers as the input. So one needs to be careful!</div><div><br/></div><div>11) Despite being a leader in the field, Tony Hoare has come to see the limitations of the Proofing-programs-correct approach to Software Verification.  His paper An Axiomatic basis for Computer Programming (1969)  (which is also in Harry Lewis's collection <i>Ideas that Created the Future</i>).</div><div>Much later, commenting on the paper,  Hoare says the following:</div><div><br/></div><div>Ten years ago, researchers into formal methods (and I was the most mistaken among them) predicted that the programming world would embrace with gratitude every assistance promised by formalization to solve the problems of reliability that arise when programs get large and more safety-critical. Programs have now got very large and very critical--well beyond the scale which can be comfortably tackled by formal methods. There have been many problems and failures, but these have nearly always been attributable to inadequate analysis of requirements or inadequate management control. It has turned out that the world just does not suffer significantly from the kind of problem that our research was originally intended to solve.'</div><div><br/></div></div><div><div>12) Richard Lipton told a story where he showed that the program in question satisfied the SPEC, but the SPEC was a tautology that any program would satisfy.  Again, one needs to be careful!</div><div><br/></div><div>13) The test of time: Verifying large scale programs does not seem to be common in industry. Is industrial adaptation a fair measure? </div><div><br/></div><div>14) Harry Lewis's  book <i>Ideas that created the future</i> collects up, edits, and comments on 46 important papers in Computer Science (I reviewed it in the issue of SIGACT News that is in your mailbox---I will blog about it at a later time.) There are several papers in it about program verification, including DLP, Hoare's paper, and three papers by Dijkstra.</div><div><br/></div><div>a) When Harry discussed including DLP some people said `You're going to include that!  Its a polemic, not a paper!'</div><div><br/></div><div>b) When Harry teaches a course from this book (it must be an awesome class!) and asks the students at the end which papers they learned the most from, the top two are an excerpt from Fred Brooks <i>The</i> <i>Mythical Man Month</i> (see my post on Brook's work <a href="https://blog.computationalcomplexity.org/2021/05/the-mythical-man-month-hen-day-and-cat.html">here</a> ) and DLP.</div><div><br/></div><div>c) I am hoping that this is just one of 46 talks with authors of the papers in his book.  I look forward to his interview with Aristotle, Leibnitz, Boole, Turing, ...</div></div><div><br/></div></div>
    </content>
    <updated>2021-06-24T03:37:00Z</updated>
    <published>2021-06-24T03:37:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-03T12:21:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5549</id>
    <link href="https://www.scottaaronson.com/blog/?p=5549" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5549#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5549" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">STOC’2021 and BosonSampling</title>
    <summary xml:lang="en-US">Happy birthday to Alan Turing! This week I’m participating virtually in STOC’2021, which today had a celebration of the 50th anniversary of NP-completeness (featuring Steve Cook, Richard Karp, Leonid Levin, Christos Papadimitriou, and Avi Wigderson), and which tomorrow will have a day’s worth of quantum computing content, including a tutorial on MIP*=RE, two quantum sessions, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Happy birthday to Alan Turing!</p>



<p>This week I’m participating virtually in <a href="http://acm-stoc.org/stoc2021/">STOC’2021</a>, which today had a celebration of the 50th anniversary of NP-completeness (featuring Steve Cook, Richard Karp, Leonid Levin, Christos Papadimitriou, and Avi Wigderson), and which tomorrow will have a day’s worth of quantum computing content, including a tutorial on MIP*=RE, two quantum sessions, and an invited talk on quantum supremacy by John Martinis.  I confess that I’m not a fan of GatherTown, the platform being used for STOC.  Basically, you get a little avatar who wanders around a virtual hotel lobby and enters sessions—but it seems to reproduce all of the frustrating and annoying parts of experience without any of the good parts.</p>



<p>Ah!  But I got the surprising news that Alex Arkhipov and I are among the winners of STOC’s first-ever <a href="https://sigact.org/prizes/stoc_tot.html">“Test of Time Award,”</a> for our <a href="https://www.scottaaronson.com/papers/optics.pdf">paper on BosonSampling</a>.  It feels strange to win a “Test of Time” award for work that we did in 2011, which still seems like yesterday to me.  All the more since the experimental status and prospects of quantum supremacy via BosonSampling are still very much live, unresolved questions.</p>



<p>Speaking of which: on Monday, Alexey Rubtsov, of the Skolkovo Institute in Moscow, gave a talk for our quantum information group meeting at UT, about his <a href="https://arxiv.org/abs/2106.01445">recent work with Popova</a> on classically simulating Gaussian BosonSampling.  From the talk, I learned something extremely important.  I had imagined that their simulation must take advantage of the high rate of photon loss in actual experiments (like the <a href="https://www.scottaaronson.com/blog/?p=5159">USTC experiment</a> from late 2020), because how else are you going to simulate BosonSampling efficiently?  But Rubtsov explained that that’s not how it works at all.  While their algorithm is heuristic and remains to be rigorously analyzed, numerical studies suggest that it works even with <em>no</em> photon losses or other errors.  Having said that, their algorithm works:</p>



<ul><li>only for Gaussian BosonSampling, not Fock-state BosonSampling (as Arkhipov and I had originally proposed),</li><li>only for threshold detectors, not photon-counting detectors, and</li><li>only for a small number of modes (say, linear in the number of photons), not for a large number of modes (say, quadratic in the number of photons) as in the original proposal.</li></ul>



<p>So, bottom line, it now looks like the USTC experiment, amazing engineering achievement though it was, is not hard to spoof with a classical computer.  If so, this is because of multiple ways in which the experiment differed from my and Arkhipov’s original theoretical proposal.  We know exactly what those ways are—indeed, you can find them in my earlier blog posts on the subject—and hopefully they can be addressed in future experiments.  All in all, then, we’re left with a powerful demonstration of the continuing relevance of formal hardness reductions, and the danger of replacing them with intuitions and “well, it still seems hard to <em>me</em>.”  So I hope the committee won’t rescind my and Arkhipov’s Test of Time Award based on these developments in the past couple weeks!</p></div>
    </content>
    <updated>2021-06-23T16:44:51Z</updated>
    <published>2021-06-23T16:44:51Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-06-23T16:44:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1325</id>
    <link href="https://thmatters.wordpress.com/2021/06/23/tcs-visioning-2020-report-and-slides/" rel="alternate" type="text/html"/>
    <title>TCS Visioning 2020 report and slides</title>
    <summary>In July 2020, the CATCS organized a visioning workshop. We are happy to announce the release of a report and posters based on this workshop. Material produced from this workshop is available and free to use by any member of the TCS community. We gratefully acknowledge financial as well as organizational support by the SIGACT […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In July 2020, the <a href="https://thmatters.wordpress.com/catcs/">CATCS</a> organized a <a href="https://thmatters.wordpress.com/2020/06/05/tcs-visioning-workshop-call-for-participation/">visioning workshop</a>. We are happy to announce the release of a report and posters based on this workshop. Material produced from this workshop is available and free to use by any member of the TCS community. We gratefully acknowledge financial as well as organizational support by the <a href="https://www.sigact.org/">SIGACT</a> and <a href="https://cra.org/ccc/">CCC</a> for this activity.</p>



<p>We are planning a follow-up event for disseminating the report and posters to funding agencies in the next few months. Details are forthcoming.</p>



<p>TCS Visioning Full Report: <a href="https://thmatters.files.wordpress.com/2021/06/visioning-report.pdf">here</a></p>



<p>Short Report (CCC Quadrennial paper): <a href="https://cra.org/ccc/wp-content/uploads/sites/2/2020/10/Theoretical-Computer-Science_.pdf">here</a></p>



<p>TCS Visioning Slides: <a href="https://thmatters.files.wordpress.com/2021/06/visioning-slides.pptx">in PPT</a> and <a href="https://thmatters.files.wordpress.com/2021/06/visioning-slides-1.pdf">in PDF</a></p></div>
    </content>
    <updated>2021-06-23T16:21:23Z</updated>
    <published>2021-06-23T16:21:23Z</published>
    <category term="reports"/>
    <category term="Visioning"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2021-07-04T18:37:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/088</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/088" rel="alternate" type="text/html"/>
    <title>TR21-088 |  Open Problems in Property Testing of Graphs | 

	Oded Goldreich</title>
    <summary>We briefly discuss a few open problems in the study of various models of testing graph properties, focusing on the query complexity of the various tasks. In the dense graph model, we discuss several open problems, including:

* Determining the complexity of testing triangle-freeness.
* Characterizing the class of properties that are testable within extremely low complexity. 

Turning to the bounded-degree graph model, we discuss several open problems, including:

* Characterizing the class of properties that are testable within size-oblivious complexity.
* Determining the complexity of graph isomorphism. 
In each of the foregoing models, we also discuss a favorite open problem that was recently resolved. Lastly, we discuss the vast lack of knowledge with respect to testing graph properties in the general graph model.</summary>
    <updated>2021-06-23T13:30:16Z</updated>
    <published>2021-06-23T13:30:16Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-04T18:37:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/087</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/087" rel="alternate" type="text/html"/>
    <title>TR21-087 |  Eliminating Intermediate Measurements using Pseudorandom Generators | 

	Uma Girish, 

	Ran Raz</title>
    <summary>We show that quantum algorithms of time T and space $S \ge \log T$ with intermediate measurements can be simulated by quantum algorithms of time $T\cdot \mathrm{poly}(S)$ and space $O(S\cdot \log T)$ without intermediate measurements. The best simulations prior to this work required either $\Omega(T)$ space (by the deferred measurement principle) or $\mathrm{poly}(2^S)$ time [FR21, GRZ21]. Our result is thus a time-efficient and space-efficient simulation of algorithms with intermediate measurements by algorithms without intermediate measurements.

To prove our result, we study pseudorandom generators for quantum space-bounded algorithms. We show that (an instance of) the INW pseudorandom generator for classical space-bounded algorithms [INW94] also fools quantum space-bounded algorithms. More precisely, we show that for quantum space-bounded algorithms that have access to a read-once tape consisting of random bits, the final state of the algorithm when the random bits are drawn from the uniform distribution is nearly identical to the final state when the random bits are drawn using the INW pseudorandom generator.</summary>
    <updated>2021-06-22T16:04:22Z</updated>
    <published>2021-06-22T16:04:22Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-04T18:37:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/086</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/086" rel="alternate" type="text/html"/>
    <title>TR21-086 |  Linear Space Streaming Lower Bounds for Approximating CSPs | 

	Chi-Ning  Chou, 

	Alexander Golovnev, 

	Madhu Sudan, 

	Ameya Velingker, 

	Santhoshini Velusamy</title>
    <summary>We consider the approximability of constraint satisfaction problems in the streaming setting. For every constraint satisfaction problem (CSP) on $n$ variables taking values in $\{0,\ldots,q-1\}$, we prove that improving over the trivial approximability by a factor of $q$ requires $\Omega(n)$ space even on instances with $O(n)$ constraints. We also identify a broad subclass of problems for which any improvement over the trivial approximability requires $\Omega(n)$ space. The key technical core is an optimal, $q^{-(k-1)}$-inapproximability for the case where every constraint is given by a system of $k-1$ linear equations $\bmod\; q$ over $k$ variables. Prior to our work, no such hardness was known for an approximation factor less than $1/2$ for any CSP. Our work builds on and extends the work of Kapralov and Krachun (Proc. STOC 2019) who showed a linear lower bound on any non-trivial approximation of the max cut in graphs. This corresponds roughly to the case of  Max $k$-LIN-$\bmod\; q$ with $k=q=2$. Each one of the extensions provides non-trivial technical challenges that we overcome in this work.</summary>
    <updated>2021-06-22T14:47:13Z</updated>
    <published>2021-06-22T14:47:13Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-04T18:37:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/</id>
    <link href="https://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/" rel="alternate" type="text/html"/>
    <title>Annual Symposium on Combinatorial Pattern Matching (summer school + conference))</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 4, 2021 – July 7, 2021 Wrocław, Poland and online https://cpm2021.ii.uni.wroc.pl/ The Annual Symposium on Combinatorial Pattern Matching (CPM) has by now over 30 years of tradition and is considered to be the leading conference for the community working on Stringology. The objective of the annual CPM meetings is to provide an international forum … <a class="more-link" href="https://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/">Continue reading <span class="screen-reader-text">Annual Symposium on Combinatorial Pattern Matching (summer school + conference))</span></a></div>
    </summary>
    <updated>2021-06-22T08:09:22Z</updated>
    <published>2021-06-22T08:09:22Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-07-04T18:38:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/085</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/085" rel="alternate" type="text/html"/>
    <title>TR21-085 |  The Final Nail in the Coffin of Statistically-Secure Obfuscator. | 

	Ilya Volkovich</title>
    <summary>We present an elementary, self-contained proof of the result of Goldwasser and Rothblum [GR07] that the existence of a (perfect) statistically secure obfuscator implies a collapse of the polynomial hierarchy. In fact, we show that an existence of a weaker object implies a somewhat stronger statement. In addition, we extend the result of [GR07] to the case of imperfect statistically secure obfuscator.</summary>
    <updated>2021-06-21T16:17:33Z</updated>
    <published>2021-06-21T16:17:33Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-04T18:37:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2146860754670937912</id>
    <link href="https://blog.computationalcomplexity.org/feeds/2146860754670937912/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/collusion-rings.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2146860754670937912" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2146860754670937912" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/collusion-rings.html" rel="alternate" type="text/html"/>
    <title>Collusion Rings</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><i>Grigory Yaroslavtsev now maintains the <a href="https://docs.google.com/spreadsheets/d/1kobGLgx5QgJb9AvDENg5c8wEbwBUUkVlMTsBV9quEEY/edit?usp=sharing">Theory Jobs Spreadsheet</a>, a crowd-sourced listing of where your favorite CS theorists have taken new positions. Read his <a href="http://grigory.us/blog/theory-jobs-2021/">blog post</a> for more info.</i></p><p><i>Michael Littman wrote a <a href="https://cacm.acm.org/magazines/2021/6/252840-collusion-rings-threaten-the-integrity-of-computer-science-research/fulltext">CACM Viewpoint</a> on collusion rings designed to tilt the odds into getting papers into conferences. See also <a href="https://medium.com/@tnvijayk/potential-organized-fraud-in-acm-ieee-computer-architecture-conferences-ccd61169370d">this post</a> by T. N. Vijaykumar. I started to write a post on the topic but couldn't get it going so I asked the AI algorithm GPT-3 to write a guest post. Here is its story.</i></p><hr/><p>I have been part of a collusion ring and I know how it feels. I was a postdoc at the University of California, Berkeley, in the early 2000s when I first learned about the practice of “paper rings.” A paper ring is a group of researchers who agree to submit each other’s papers to conferences without disclosing this fact to the conference organizers. The idea is that if you have several submissions from the same conference, your chances of getting accepted are higher. The pressure to be part of such a ring can be immense. You are told that you need to publish or perish and that you need to get papers into top conferences. If you don’t play along, you will be left behind. I was lucky enough to have a mentor who warned me against being part of such rings. He told me that if I did not want my name on a paper, then I should not submit it to a conference. He also told me that if I did not want to be part of a paper ring, then I should not submit papers for others in the ring. He said that he would help me with my own research and that he would help me get my own papers into conferences. He was right: My own papers were accepted into conferences and they were published in journals.</p><p>I am now an associate professor at the University of Massachusetts Amherst and I have been on the other side of this issue as well. In 2012, I was asked by an editor at a top computer science conference to review a paper submitted by one of my students. The student had been part of a paper ring and had submitted his own paper along with those submitted by his collaborators in the ring. The editor told me that he had rejected the student’s paper because it was too similar to other papers submitted by members of the ring. However, he wanted me to review it so that he could reconsider accepting it for publication if I recommended doing so.</p><p>What should I do? Should I tell him about the paper ring? Should I recommend accepting the paper? Should I tell him that there is no way he should accept it? Should I tell him that he should reject it but then publish it anyway? What would you do? I decided not to tell him about the paper ring and not to recommend accepting the paper for publication. Instead, I told him that he should reject it but then publish it anyway because this would send a strong message about what we expect from our students and what we think about such practices. The editor agreed with my decision and published the student’s paper along with two other papers submitted by members of the same paper ring.</p><p>The student received his PhD degree but did not get any credit for his work on these three papers because they were considered as part of his dissertation work when he defended his thesis later that year. In retrospect, I think that my decision was wrong because it allowed these students to continue their unethical behavior without any consequences for them or their advisors (who were also part of this ring). But at least in this case, there were no negative consequences for anyone involved in this incident except for these students who did not get credit for their work on these three papers because they were considered as part of their dissertation work when they defended their thesis later that year.</p></div>
    </content>
    <updated>2021-06-21T14:06:00Z</updated>
    <published>2021-06-21T14:06:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-03T12:21:18Z</updated>
    </source>
  </entry>
</feed>
